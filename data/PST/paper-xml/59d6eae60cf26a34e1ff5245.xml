<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Domain Generalization With Structured Low-Rank Constraint</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Zhengming</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engi-neering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<postCode>02115</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<postCode>02115</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Yun</forename><surname>Fu</surname></persName>
							<email>yunfu@ece.neu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engi-neering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<postCode>02115</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Ding</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<postCode>02115</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Domain Generalization With Structured Low-Rank Constraint</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9AD31717E188A0F87440F45E8193D613</idno>
					<idno type="DOI">10.1109/TIP.2017.2758199</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Domain generalization</term>
					<term>deep learning</term>
					<term>low-rank constraint</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Domain adaptation nowadays attracts increasing interests in pattern recognition and computer vision field, since it is an appealing technique in fighting off weakly labeled or even totally unlabeled target data by leveraging knowledge from external well-learned sources. Conventional domain adaptation assumes that target data are still accessible in the training stage. However, we would always confront such cases in reality that the target data are totally blind in the training stage. This is extremely challenging since we have no prior knowledge of the target. In this paper, we develop a deep domain generalization framework with structured low-rank constraint to facilitate the unseen target domain evaluation by capturing consistent knowledge across multiple related source domains. Specifically, multiple domain-specific deep neural networks are built to capture the rich information within multiple sources. Meanwhile, a domaininvariant deep neural network is jointly designed to uncover most consistent and common knowledge across multiple sources so that we can generalize it to unseen target domains in the test stage. Moreover, structured low-rank constraint is exploited to align multiple domain-specific networks and the domaininvariant one in order to better transfer knowledge from multiple sources to boost the learning problem in unseen target domains. Extensive experiments are conducted on several cross-domain benchmarks and the experimental results show the superiority of our algorithm by comparing it with state-of-the-art domain generalization approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>D OMAIN adaptation [1], [2] has already attracted con- siderable attentions in pattern recognition and computer vision field, since it well tackles the tasks with no or limited labeled target data. Generally, domain adaptation borrows well-established knowledge from source domains to alleviate the learning problem in the target domain. Conventional domain adaptation methods <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b7">[8]</ref> consider seeking domain-invariant representation of the data or adapting classifiers, or both of them to mitigate the marginal or conditional Fig. <ref type="figure">1</ref>. Illustration of domain generalization problem, where we have multiple sources (3 sources here) while we are not accessible to the target domain during the training stage. Note that multiple sources and unseen target share the same labels. The key is to uncover the consistent information across multiple sources through deep structure to alleviate the unseen target learning. distribution differences between source and target domains. In traditional domain adaptation scenarios, we still assume we have access to the target data in the training stage. In reality, however, we usually confront such challenges that the target data are totally unavailable in the training stage (Fig. <ref type="figure">1</ref>). Thus, conventional domain adaptation techniques <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref> would fail in such cases, since they all need target data to guide knowledge transfer during model training.</p><p>Fortunately, we could always find some related source domains to extract valid knowledge to build a classifier for unseen targets. For example, we desire to evaluate image classification on VOC2007, and we can have the images from ImageNet, Caltech-256 and LabelMe databases available ahead of time. That is, we have multiple sources for training but without any knowledge about the target so that it is very difficult to guide the knowledge transfer. Actually, it is different from multi-source domain adaptation <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b14">[15]</ref>, which has access to the target data in the training stage. Since these multiple sources have different distributions with each other as well as the target, it is very important to mitigate the domain shift between multiple sources and target by exploring more consistent information from multiple sources.</p><p>Most recently, domain generalization <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b22">[23]</ref> has been well exploited to fight off the challenge through capturing knowledge from multiple source domains and generalizing to the unseen target domains. Along this line, there are mainly two strategies: one is domain-invariant feature learning, while the other is multiple classifiers adaptation. For example, Xu et al. exploited the low-rank structure from multiple latent source domains by extending an exemplar-SVM in order to capture the likelihoods of all positive samples <ref type="bibr" target="#b18">[19]</ref>. However, existing domain generalization research efforts all employ shallow structures, so it is difficult for them to well uncover the rich information within the complex data. Therefore, it is easy to ignore the useful knowledge shared by multiple sources and hard to adapt the knowledge to the unseen target domains in the test stage.</p><p>Recently, deep learning <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> has been extensively explored, since it could capture more effective knowledge in hierarchical feature extraction structure. That is, more discriminative and powerful knowledge underlying the data would be captured through multi-layer non-linear transformations <ref type="bibr" target="#b25">[26]</ref>. Targeting at solving the distribution difference, deep learning has been well explored in domain adaptation, and it presents powerful ability in domain-invariant feature learning <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b29">[30]</ref>. Specifically, deep domain adaptation technique manages to uncover common information across different domains. With deeper structures, the domain shift can be well mitigated so that the well-learned knowledge in source can be adapted to alleviate the target learning. In this way, deep structure learning is promising to uncover more discriminative features across multiple sources and generalize to the unseen target in the test stage.</p><p>In this paper, we present a Deep Domain Generalization framework (DDG) through structured low-rank constraint by leveraging the knowledge between multiple sources and the unseen target domains (Fig. <ref type="figure" target="#fig_0">2</ref>). The core idea of DDG is to build a domain-invariant end-to-end deep structure by uncovering shared knowledge across multiple source domains so that the domain-invariant deep structure could well generalize to unseen target domains in the test stage. To our best knowledge, this is the first work for deep learning to fight off domain generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Our Contributions</head><p>To explore more consistent knowledge across multiple sources, the idea of deep structure learning has been incorporated to build an effective end-to-end architecture to facilitate the unseen target learning. Here we summarize the main contributions of this work in two folds as follows:</p><p>• Multiple domain-specific deep structures and one domaininvariant deep structure are jointly built to uncover more useful information from each domain and shared by different domains, respectively. With multi-layer networks, the rich knowledge within sources can be learned to facilitate the unseen target learning. Aiming at coupling two types of networks, we adopt a difference loss to guide the deep learning. • To better couple multiple domain-specific structures and the domain-invariant one, we deploy a structured lowrank reconstruction scheme to transfer the knowledge between domain-specific and the domain-invariant one. Specifically, the output from domain-specific networks would be only reconstructed by the output from domaininvariant network with the same class label under lowrank constraint. To this end, the learned domain-invariant deep structure can be more effective for the unseen target domains. The remaining parts of this paper are presented as follows. We show a brief discussion of the related works in Section II. Then we provide our novel deep domain generalization algorithm in Section III, as well as its optimization solution. Experimental evaluations on several different benchmarks are reported in Section IV, which is followed with the conclusion in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this part, we briefly discuss two lines of related work and further highlight the differences between our proposed algorithm and the existing works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Domain Adaptation &amp; Generalization</head><p>Domain adaption aims to tackle with the problem that the source distribution is different from the target one <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. However, traditional domain adaptation assumes the target data is accessible during the training procedure.</p><p>In reality, we may confront such cases that we cannot have any evaluation data available in the training stage.</p><p>To address the case where target data cannot be accessible for training, domain generalization <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b20">[21]</ref> has recently attracted increasing attentions, since it is promising to extend the training model from multiple sources to the unseen target. Multi-source adaptation problem <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b11">[12]</ref> is similar to domain generalization, since they both manage to extract the source knowledge and handle the distribution difference across sources. Therefore, their proposed techniques have many similar aspects. However, domain generalization is more challenging as there is no target data achievable in the training procedure for help.</p><p>Along this line, there are mainly two strategies: one is domain-invariant feature learning and the other is multiple classifiers adaptation. In the first line, Muandet et al. developed a kernel-based optimization algorithm to build a domaininvariant projection by minimizing the divergence across different domains <ref type="bibr" target="#b16">[17]</ref>. Fang et al. designed Unbiased Metric Learning (UML) algorithm through learning to rank framework, which produced a less biased distance metric with better domain generalization performance in weakly-labeled web images <ref type="bibr" target="#b17">[18]</ref>. Furthermore, Ghifary et al. developed a multitask auto-encoder by encoding with a common layer while decoding with domain-specific layers in order to uncover the shared information across multiple domains for the unseen target domains <ref type="bibr" target="#b19">[20]</ref>. Along the second line, Khosla et al. designed a multi-task max-marginal classifier on multiple source domains, where the learned weights that are shared to all source domains can be utilized for knowledge generalization <ref type="bibr" target="#b15">[16]</ref>. Most recently, Niu et al. extended multiple instance learning and constructed one classifier per class per latent domain, and hence, multiple classifiers could be effectively integrated to obtain promising generalization <ref type="bibr" target="#b20">[21]</ref>.</p><p>However, current work on domain generalization ignored the rich information from the hierarchical structure within the data. Differently, we develop two types of deep neural networks to uncover more effective information across multiple sources to alleviate the unseen target learning. Furthermore, we adopt structured low-rank constraint to build a more effective domain-invariant deep structure for unseen target learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep Learning</head><p>In the recent years, deep Learning has been well explored and increased great attention in a lot of real-world applications, due to its promising ability in different learning tasks, e.g., face recognition <ref type="bibr" target="#b31">[32]</ref>, image super-resolution <ref type="bibr" target="#b24">[25]</ref>, action classification <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, feature selection <ref type="bibr" target="#b34">[35]</ref>, image clustering <ref type="bibr" target="#b35">[36]</ref> and multi-view learning <ref type="bibr" target="#b36">[37]</ref>. In general, deep learning attempts to build hierarchical structures to extract features with the input of raw data. Most deep learning approaches assume that deep architecture is able to extract invariant features which can be transferable across various learning tasks. However, the domain discrepancy, unfortunately, cannot be removed, but only alleviated with the deep structure. Thus, domain mismatch is one of the bottlenecks to the adaptability of deep architecture.</p><p>Most recently, a lot of deep domain adaptation methods are proposed to address the domain discrepancy based on the deep structure by making merit of both deep learning and domain adaptation techniques <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>. The core idea is to improve the representation adaptability in the higher layers of the deep structure through explicitly mitigating the domain distribution mismatch. However, current deep domain adaptation algorithms all assume the target data are achievable for model training, that is, none of them are designed for domain generalization.</p><p>In this paper, we are the first to explore the deep structures to address the domain generalization challenge. To transfer more effective information from the multiple sources to the domain-invariant network, which is further extended to the unseen target domains, we propose a structured constraint to build a bridge across them under low-rank assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED MODEL</head><p>In this part, we will introduce our proposed Deep Domain Generalization learning algorithm for unseen target domains by constructing multiple domain-specific and one domaininvariant deep neural networks from several related sources. Then, we provide the solution to the proposed algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Preliminaries</head><p>Domain can be defined as probability distribution P XY on X × Y, in which X and Y represent the data and label spaces, respectively. For simplicity, we denote P XY as P. Assume D = {x j ; y j } n j =1 as an independent and identically distributed (i.i.d.) sample from a domain. In domain generalization, assume = {P s,1 ; • • • ; P s,m } as a combination of m source domains and P t / ∈ as a target domain. Assume i -th source domain</p><formula xml:id="formula_0">D s,i = {x j i ; y j i } n i j =1</formula><p>with n i labeled samples. In the test stage, we aim to evaluate on some unseen domains D t with the same categories.</p><p>The main difference between multi-source domain adaptation and domain generalization is on the accessibility of the target data during the training stage. Both manage to seek a labeling function θ : X → Y that can achieve good performance on the target data. Note that domain generalization would definitely become to multi-source domain adaptation, when P t ∈ . Although these two are highly related problems, domain adaptation approaches generally cannot be directly exploited in domain generalization scenario. Thus, it is significantly desirable to propose models efficiently for domain generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Motivation</head><p>When dealing with a learning problem that the evaluation data are only available in the test stage, domain generalization attracts increasing attentions by exploring more consistent knowledge across multiple related sources domains, which are all drawn from different distributions with the unseen target. Along the line of multi-source domain adaptation <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b11">[12]</ref> and domain generalization <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, most researchers assume there is a latent domain-free space, where sources and target could align well. Thus, it is very essential to find the latent space so that multi-source knowledge could be transferred to the unseen target to boost the learning problem. However, current domain generalization <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b20">[21]</ref> works all adopted shallow structures to extract domain-invariant features, which actually cannot uncover enough effective features to facilitate the target learning.</p><p>To this end, we desire to explore deep structure learning in domain generalization to uncover more effective knowledge across multiple sources. Considering the distribution divergence among multiple sources, we design multiple domainspecific deep structures, each for one source domain to transform multiple sources into the latent space. Specifically, each domain-specific network can be treated as one task in multi-task scheme, and Bengio et al. mentioned that good representation should have such prior as shared factors across tasks <ref type="bibr" target="#b23">[24]</ref>. Therefore, we assume different networks should have similar activation values given the same class of objects from different domains, and our domain-invariant network is designed to capture most shared factors across tasks. Such domain-invariant deep structure is treated as an approximation of the specific target domain deep structure when we cannot have access to any target data during the training procedure. Finally, we further learn a classifier based on the domaininvariant deep structure, thus, we can evaluate on the unseen target data in the test stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Deep Domain Generalization Model</head><p>To build a deep domain generalization model, we extend the well-known architectures <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b39">[40]</ref>, which are comprised of several convolutional layers and a few fully-connected layers. Generally, each fully-connected (fc) layer l learns a nonlinear transformation</p><formula xml:id="formula_1">h l i = f l (W l h l-1 i + b l ), in which h l</formula><p>i is the l-th layer hidden representation of sample x i (W l and b l are the weights and bias for the l-th layer), while f l (•) is the activation function, for example, rectifier function f l (x) = max(0, x) for hidden layers or Softmax function f l (x) = exp(x)/ |X | j =1 exp(x j ) for the output layer. Current deep domain adaptation approaches <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b37">[38]</ref> suggest that high-layer features eventually transition from general to specific along the deep architecture, the feature adaptability gap increase with the domain mismatch particularly enlarged. Here we do not focus on how to calculate the convolutional layers, since we do not enforce distributionadaptation constraint in those layers, provided that the convolutional layers are able to generate generic features <ref type="bibr" target="#b40">[41]</ref>.</p><formula xml:id="formula_2">Define E i = {W l i , b l i }| L l=1</formula><p>as the set of all CNN parameters for each source-specific network, E c = {W l c , b l c }| L l=1 as the set of all CNN parameters for the domain-invariant network and θ c as the parameter for standard Softmax classifier.</p><p>For our model, inference is provided with ȳ = θ c (E c (x)), in which ȳ is the task-specific label prediction. In this paper, we attempt to minimize the following loss function with respect to parameters = {E i , E c , θ c }:</p><formula xml:id="formula_3">L = L g + γ L d + βL r ,<label>(1)</label></formula><p>which includes three parts, i.e., classification loss L g , crossnetwork difference loss L d and reconstruction loss L r .</p><p>First of all, the classification loss L g tends to guide the model training to assign the output labels. Specifically, we attempt to minimize the negative log-likelihood of the ground truth class, provided every labeled source sample:</p><formula xml:id="formula_4">L g = - N j =0</formula><p>y s, j log(ȳ s, j ), <ref type="bibr" target="#b1">(2)</ref> in which N = i n i , and y s, j is the class label for source sample x s, j , while ȳs, j is the label prediction as ȳs, j = θ c (E c (x s, j )).</p><p>The cross-network difference loss L d is also exploited to all source domains and encourages the domain-invariant networks to preserve most information across multiple domain-specific networks. Since multiple sources all share the same categories but in different distributions, we assume the domain-specific deep networks share most information in the latent space, where the knowledge can be extended to the unseen target domain. Specifically, each domain-specific network can be treated as one task. It has been suggested in <ref type="bibr" target="#b23">[24]</ref> that good representations should have such prior as Shared Factors Across Tasks. Therefore, we assume that different networks should have similar activation values given the same class of objects from different domains, and our domain-invariant network is designed to identify shared factors across tasks, which is an approximation for unseen target domain. An intuitive strategy is to couple the weights in each layer between the domainspecific networks and the domain-invariant one. To this end, we minimize the following connections across them as:</p><formula xml:id="formula_5">L d = m i=1 L l=1 W l i -W l c 2 F + b l i -b l c 2 2 ,<label>(3)</label></formula><p>In this way, the common domain-invariant network can uncover shared factors across multiple domain-specific networks so that it can be better extended to alleviate the unseen target learning in the real test stage. The cross-network constraint (3) aims to uncover more shared knowledge for domain-invariant deep network from multiple sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Structured Low-rank Reconstruction</head><p>The reconstruction loss L r is also applied to encourage the shared and private networks to capture more discriminate information. When the domain distribution difference is mitigated with deep architecture, we could observe that the within-class data would gather together although they are from different domains. That is, each sample would be highly similar to other samples in the same class. In this way, the conditional distribution difference could be addressed. Therefore, we explore the structured low-rank constraint could help transfer knowledge from domain-specific networks to domain-invariant networks.</p><p>Let H s,i and H c,i be matrices whose rows are the domainspecific representation and domain-invariant representations H s,i = E i (X s,i ) and H c,i = E c (X s,i ) from samples of i -th source, respectively. To guide two types of networks learning and adapt more effective information to the unseen target data, we adopt low-rank reconstruction to couple the output of each domain-specific network and the output of the domaininvariant network in the following:</p><formula xml:id="formula_6">min Z rank(Z ), s.t. H s = H c Z , (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>where</p><formula xml:id="formula_8">H c = [H c,1 , • • • , H c,m ] and H s = [H s,1 , • • • , H s,m ].</formula><p>Z ∈ R N×N is the reconstruction coefficient matrix and rank(•) is the rank operator <ref type="bibr" target="#b41">[42]</ref>. The rank minimization issue can be addressed by the trace norm • * as a good surrogate <ref type="bibr" target="#b41">[42]</ref>.</p><p>Actually, low-rank reconstruction matrix Z tends to be block-diagonal in the ideal case, since only the within-class data are correlated across two types of deep networks. Similar ideas were proposed in the literature <ref type="bibr" target="#b30">[31]</ref>, which adopted multi-view data to reconstruct each view under linear transformations with low-rank constraint. However, such shallow structures with linear transformations would fail to represent complex data. Our algorithm joints deep structure learning and low-rank constraint into a unified framework in order to capture more effective information shared by multiple sources. In this way, the domain shift across multiple sources could be further mitigated to boost the unseen target learning.</p><p>However, previous model (4) works in an unsupervised way, which still treats the dataset as whole and ignores the discriminative information during low-rank reconstruction. In other word, it is hard to recover block-diagonal low-rank matrices. Motivated by previous structured low-rank learning <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, we propose to guide the low-rank reconstruction through structured matrix across two types of deep networks as follows:</p><formula xml:id="formula_9">min Z Z * + α Z -Q 2 F , s.t. H s = H c Z , (<label>5</label></formula><formula xml:id="formula_10">)</formula><p>where α &gt; 0 is the trade-off parameter and Q is the structured low-rank matrix (Figure <ref type="figure" target="#fig_1">3</ref>), which is defined as follows:</p><formula xml:id="formula_11">Q[ j, k] = 1, if H j s</formula><p>and H k c belong to the same class 0, otherwise Clearly, Q is a block-diagonal structured matrix, because it is constructed based on within-class relationship. Such structured low-rank reconstruction would involve more label information during the deep structures learning so that the conditional distribution difference across two types of deep structures could be reduced. To that end, we develop a novel loss function for deep domain generalization with the constraint H s = H c Z :</p><formula xml:id="formula_12">L r = Z * + α Z -Q 2 F . (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Model Training</head><p>Eq. ( <ref type="formula" target="#formula_3">1</ref>) is challenging to be addressed because of the nonlinearity and non-convexity. Thus, we present an alternating optimization method to iteratively update two subproblems: 1) optimize the low-rank coefficients; 2) learn two types of deep structures.</p><p>Low-Rank Coefficient Optimization: we propose to update low-rank coefficients, then provide the deep structure optimization. When the deep structures are fixed, the objective function in Eq. ( <ref type="formula" target="#formula_3">1</ref>) degenerates to a traditional low-rank modeling problem. Then, we derive the corresponding augmented Lagrangian function of Eq. ( <ref type="formula" target="#formula_9">5</ref>) w.r.t Z :</p><formula xml:id="formula_13">Lr = Z * + α Z -Q 2 F + R, H s -H c Z + μ 2 H s -H c Z 2 F , (<label>7</label></formula><formula xml:id="formula_14">)</formula><p>where R is the Lagrange multiplier and μ &gt; 0 is the penalty parameter. , denotes the inner product operator of two matrices. Specifically, we first define</p><formula xml:id="formula_15">Lr (Z , Q, R τ , μ τ ) = α Z -Q 2 F + R τ , H s -H c Z + μ τ 2 H s -H c Z 2 F</formula><p>, then have following updating rules of Z variable at time τ :</p><formula xml:id="formula_16">Z = arg min Z Z * + Lr (Z τ , Q, R τ , μ τ ) = arg min Z 1 ξμ τ Z * + Z -Z τ + ∇ Z τ Lr 2 F (<label>8</label></formula><formula xml:id="formula_17">)</formula><p>where</p><formula xml:id="formula_18">∇ Z τ Lr = ∇ Z τ Lr (Z τ , Q, R τ , μ τ ) = 2α(Z τ -Q) - H s R τ -μ τ H c (H s -H c Z τ ) and ξ = H c 2 F</formula><p>. Problem (8) can be effectively addressed with the singular value thresholding (SVT) operator <ref type="bibr" target="#b44">[45]</ref>. Suppose U Z Z V Z are the SVD of matrix (Z τ -∇ Z τ Lr ), in which Z = diag({σ } 1≤i≤r ) with singular value σ . Thus, we could achieve the optimal of Z at time τ + 1 as</p><formula xml:id="formula_19">Z τ +1 = U Z ( 1 μτ ) ( Z )V Z , in which ( 1 μτ ) = diag({σ -1 μ τ } + )</formula><p>, and a + represents the positive part of a <ref type="bibr" target="#b44">[45]</ref>.</p><p>The multiplier R and the penalty parameter μ are updated as</p><formula xml:id="formula_20">R τ +1 = R τ +μ τ (H s -H c Z τ +1</formula><p>) and μ τ +1 = min(ρμ τ , μ max ), respectively, where ρ = 1.2 and μ max = 10 6 . We obtain the low-rank coefficient Z until it converges with the condition of</p><formula xml:id="formula_21">H s -H c Z ∞ &lt; ( = 10 -6 ).</formula><p>Learning Deep Weights: after Z updated, we could adopt conventional gradient descent strategy to optimize parameters of neural networks individually. Specially, we first calculate the gradient descant of each variable, w.r.t., ∂L/∂ . Then, is updated through</p><formula xml:id="formula_22">= + η ∂L ∂ , (<label>9</label></formula><formula xml:id="formula_23">)</formula><p>where η is updated with Adagrad Rule <ref type="bibr" target="#b45">[46]</ref>. Large-Scale Data Extension: When dealing with large-scale dataset, our deep domain generalization model with structured low-rank constraints can still adopt mini-batch stochastic gradient descent (SGD) strategy. Specifically, we need to do some modification on the optimization procedure. For every random sampling, we randomly select a certain number of samples per class from different domains, i.e., n r samples each class. In this way, we can ensure the low-rank property to be preserved. Thus, in each sampling, we have subset</p><formula xml:id="formula_24">A s = [A s,1 , • • • , A s,m</formula><p>] from m domains, each A s,i with n r ×C samples. For each mini-batch, we could iteratively update the above two subproblems. Since our structured low-rank constraint is exploited to transfer more effective knowledge across two types of networks, hence, for each mini-batch, we can still achieve this goal if we sample the data with lowrank property. To this end, we conclude that our model can be quite scalable to large-scale dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Model Comparison</head><p>In this part, we will discuss three most related works to ours.</p><p>The first related work in domain generalization is MTAE <ref type="bibr" target="#b19">[20]</ref>, which exploits the common encoder to transform multiple sources to the hidden layer, then adopted domainspecific decoders. In this way, the common domain-invariant encoder could be generalized to unseen target domains. Their reconstructions provide a class-level correspondence, which constrains the number of samples in one class to be the same for each domain.</p><p>The second related works are DSN <ref type="bibr" target="#b38">[39]</ref> and WSDTN <ref type="bibr" target="#b46">[47]</ref>, which are both designed for domain adaptation. Specifically, DSN is designed for domain adaptation by building two types of networks, i.e., domain-specific and domain-invariant networks. Furthermore, DSN adopts orthogonal constraints to enforce source and target with more shared knowledge. WSDTN proposes two networks for two modalities with weakly-shared constraint, which is the same to our constraint in Eq. (3). However, both are developed for traditional domain adaptation, which assume target data are available during training.</p><p>However, we adopt structured low-rank reconstruction to adapt the two types of deep networks for domain generalization, where test data are totally blind in the training stage. Our low-rank reconstruction would make the same class data in two types of networks correlate to mitigate the withinclass variance. Moreover, we explore the similar strategy to WSDTN to enforce multiple domains with more shared knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>In this part, we evaluate our proposed model on several cross-domain benchmarks by comparing state-of-the-art domain generalization algorithms. We also analyze the convergence and each component individually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets &amp; Experimental Setting</head><p>In the experiments, we mainly use four cross-domain databases, which are Office-31, Office-10 + Caltech-10 (Fig. <ref type="figure" target="#fig_2">4</ref>), VLCS-5 and MUS-10 (Fig. <ref type="figure" target="#fig_3">5</ref>). The statistic of four datasets are shown in Table <ref type="table" target="#tab_0">I</ref>.  Example from cross-domain digit dataset of three subsets, i.e., MNIST, USPS and SVHN.</p><p>Office-31: is a widely-adopted cross-domain benchmark, which consists of three real-world visual object datasets, i.e., Amazon (images downloaded online), DSLR (highresolution images collected with a digital SLR camera) and Webcam (low-resolution images captured with a web camera). In total, there 4,652 images from 31 categories.</p><p>Office-10 + Caltech-10: dataset contains Office-31 and which is a standard for visual classification. <ref type="foot" target="#foot_0">1</ref> There are 10 common categories shared with them. Generally, each category has 8 to 151 samples, and in total it includes 2,533 samples. Here we build evaluation cases selecting sources and targets.</p><p>VLCS-5: is another cross-domain object benchmark, which contains the images from PASCAL VOC2007 (V), LabelMe (L), Caltech-101 (C), and SUN09 (S), with each as a domain. Specifically, V, L, and S are scene-centric datasets, while C is an object-centric dataset. There are five shared object categories, i.e., "car", "bird", "chair", "dog", and "person". We follow the setting in <ref type="bibr" target="#b19">[20]</ref> to divide each domain of VLCS into a training set (70%) and a test set (30%) through random selection.</p><p>MUS-10: digit dataset includes three subsets, i.e., MNIST (M), USPS (U) and SVHN (S). Each dataset contains digits belonging to 10 classes (0-9), each captured under different conditions. Specifically, MNIST and USPS are large datasets of handwritten digits captured under constrained conditions. Both these domains are visually very similar and this makes adaptation relatively easy. SVHN has significant variations (See Fig. <ref type="figure" target="#fig_3">5</ref>) in many aspects, which makes it quite different from MNIST and USPS.</p><p>In this section, we mainly evaluate our proposed approach with the following comparisons:</p><p>• DCNN: We directly apply labeled sources to only train one deep convolutional neural network (DCNN), then exploit to predict the labels of unseen targets.</p><p>• LRCS <ref type="bibr" target="#b30">[31]</ref>: a multi-view common subspace learning algorithm, which seeks a shared low-rank subspace for view-unknown test data. • Undo-Bias <ref type="bibr" target="#b15">[16]</ref>: a multi-task SVM-based model to address the dataset bias.</p><p>• UML <ref type="bibr" target="#b17">[18]</ref>: a structural metric learning-based approach, targeting at building a domain-invariant distance metric for target learning.</p><p>• DICA <ref type="bibr" target="#b16">[17]</ref>: a kernel-based learning approach that seeks an domain-invariant projection by minimizing the discrepancy across domains. • LRE-SVM <ref type="bibr" target="#b18">[19]</ref>: a non-linear exemplar-SVM algorithm with a low-rank constraint on the likelihood matrix.</p><p>• MTAE <ref type="bibr" target="#b19">[20]</ref>: multi-task auto-encoder seeks to convert the raw data into analogs in multiple related sources.</p><p>It thereby extracts new representations robust to domain mismatch.</p><p>• DSN <ref type="bibr" target="#b38">[39]</ref>: a deep domain adaptation model by learning a shared encoding network.</p><p>• SCA <ref type="bibr" target="#b22">[23]</ref>: a linear subspace approach by maximizing the discriminability of classes and minimizing the mismatch between domains. The first two, DCNN and LRCS, are two baselines while the middle five algorithms are the state-of-the-art domain generalization approaches. DSN is previously designed for unsupervised domain adaptation, since it also attempts to seek a common deep structure shared by different two domains. Applying it to domain generalization, we do some modification that the classifier is learned on all the labeled sources. Specifically, we adopt the MMD as the similarity loss. We evaluate on two-source domain generalization challenges, thus, we can still use the DSN code easily.</p><p>Throughout the experiments, we implement CNN topologies based on the deep structures <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>. Detailedly, for Office-31, Office-10 + Caltech-10 and VLCS-5, we exploit the AlexNet structure <ref type="bibr" target="#b39">[40]</ref>. Since these three datasets are rather small-scale and the size of available training data is essential to build a powerful deep model, hence we opted for the finetuning of the CNN pre-trained on the ImageNet (AlexNet with the Caffe toolbox <ref type="bibr" target="#b47">[48]</ref>) <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b37">[38]</ref>. For cross-domain digit dataset (MUS-10), we use the CNN structure <ref type="bibr" target="#b48">[49]</ref>.</p><p>Besides, for shallow learning algorithms, we adopt the DeCAF 6 features extracted from the first three datasets (Office-31, Office-10 + Caltech-10 and VLCS-5) as inputs to the methods. These representations have dimensionality of 4,096 and are public to download. Office-31 is publicly available, <ref type="foot" target="#foot_1">2</ref> Office-10 + Caltech-10 can be downloaded from the website. <ref type="foot" target="#foot_2">3</ref> VLCS-5 features are available on the website. <ref type="foot" target="#foot_3">4</ref>While for the last digit dataset, we extract the features with the structure <ref type="bibr" target="#b48">[49]</ref>.</p><p>In the experiments, we report the model performance in terms of the classification accuracy (%) <ref type="bibr" target="#b18">[19]</ref>. Here, we tune the hyper-parameters for each model through 10-fold crossvalidation on labeled sources, since we have no labeled target data available for training. Note that "{Webcam, DSLR, Amazon} → Caltech" represents that the sources are Webcam, DSLR and Amazon, while the unseen target is Caltech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Domain Generalization Results</head><p>We testify the domain generalization performance of each algorithm, and the results are shown in Tables II, III, IV and Fig. <ref type="figure" target="#fig_4">6</ref>.</p><p>Generally, we observe that our proposed algorithm could outperform all the other comparisons in all cases, since our proposed algorithm deploys two types of deeper structures and incorporates structured low-rank reconstruction fashion. Therefore, more discriminative knowledge could be transferred to facilitate the learning problem to unseen target domains. Besides, domain generalization algorithms could achieve better results than others in most cases. That shows domain generalization techniques definitely facilitate the unseen target  Secondly, compared with DCNN, our model builds two types of networks, thus our model can have a better flexibility in dealing with unseen target domains. DCNN can only capture the knowledge from sources while cannot well uncover the shared knowledge across sources for unseen targets. LRCS also adopts the low-rank reconstruction to further learn a view-invariant linear projection, however, it is an unsupervised and shallow model so that it cannot well adapt the source knowledge from sources to unseen target. For other domain generalization methods, most of them can achieve better performance based on deep features, compared with DCNN. This indicates that deep structure learning is not enough to handle the unseen domain challenge, and they cannot build a transferable network. However, domain generalization approaches still can improve the performance.</p><p>Thirdly, 1) from the classification performance in Tables II, III and IV, we could observe deep learning features, i.e., DeCAF 6 features, could help the unseen target learning. The reason is that all the domains are extracted features through a common deep structure, which is trained on large-scale dataset, so that the domain mismatch across sources and target could be mitigated to some extent. We can also notice that all the algorithms work worse on {D, W} → A than other two cases, since A has a larger distribution difference with W and D. In handcrafted features, domain generalization techniques cannot help a lot for this challenge case. 2) from the results in Table <ref type="table" target="#tab_1">III</ref>, A and C have higher similarity while D and W share higher similarity, therefore, we can witness that the first four cases achieve better results, especially {A, C, D} → W. However, domain generalization methods can still improve the performance a little bit. 3) For VLCS-5, the object-centric dataset, Caltech-101, appears to show the best performance, as scene-centric datasets usually tend to have a good generalization over object-centric datasets <ref type="bibr" target="#b49">[50]</ref>. 4) For MUS-10, MNIST and USPS have similar distributions, thus, we can achieve better performance when unseen target is MNIST or USPS. However, when SVHN is the unseen target, the performance is worse since SVHN is much different from MNIST and USPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Empirical Analysis</head><p>In this part, we mainly testify some properties of the proposed model.</p><p>First of all, we evaluate several variants of our model to to understand our model deeply.</p><p>• DDG_l: we remove the structured term, i.e., Z -Q 2 F (set α = 0).   • DDG_s: we further remove the shared factor constraint, i.e., (3), from DDG_p (set γ = 0). From the results, we notice that our model outperforms other variants, which indicates that each component contributes to domain generalization. Since we remove each term step by step, we could have a clear view that each component has an improvement.</p><p>Secondly, we testify the convergence of our proposed model. Actually, our model has two subproblems, one is traditional deep convolution neural networks, the other is low-rank modeling. In fact, for each subproblem, researchers has well shown their convergence. However, it is still hard to theoretically prove the convergence of deep learning models, especially for our model with two subproblems. Thus, researchers usually empirically testify the convergence of the deep structure. We found our model converges well for all the cases. Here we adopt the test cases {A, D} → W of Office-31, {V, L, C} → S of VLCS, and {U, S} → M of cross-domain digit dataset, respectively. The convergence curves are shown in Fig. <ref type="figure" target="#fig_6">7</ref>, where we could observe that our model converges well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we developed a deep domain generalization framework with structured low-rank constraint, which aimed to seek most shared discriminative knowledge within multiple sources to facilitate the unseen target learning. Specifically, we built two types of deep structures, domain-specific and domain-invariant, to capture most common discriminative information shared by multiple sources so that the knowledge could be transferred to the unseen target domains. The structured low-rank reconstructions were adopted to mitigate the gap between domain-specific and domain-invariant structure. Experimental evaluation on several popular cross-domain benchmarks verified the superiority of our proposed model, by comparing with the state-of-the-art domain generalization methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Framework of our proposed algorithm. (a) multiple domain-specific deep structures E i (•) tend to be learned to capture the rich information from each source. (b) A domain-invariant deep structure E c (•) is built for all the domains, and further generalize to the unseen domain in the test stage with learned classifier θ c (•). (c) To couple the outputs of multiple domain-specific networks and domain-invariant one, low-rank reconstruction is adopted to align two types of networks in structured low-rank fashion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of structure matrix Q for three sources (S1, S2, S3) with three categories (C1,C2, C3). Q only has positive values at the positions in which sources share the same labels, otherwise Q is 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Example images from the Backpack in Caltech-256 Office-31 (i.e., Amazon, DSLR, and Webcam). Specifically, Caltech-256 and Amazon images are mostly from online merchants, while DSLR (high resolution) and Webcam (low resolution) images are from offices. (Best viewed in color).</figDesc><graphic coords="6,395.03,298.61,83.78,92.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5.Example from cross-domain digit dataset of three subsets, i.e., MNIST, USPS and SVHN.</figDesc><graphic coords="6,314.51,299.57,80.30,91.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The cross-domain recognition accuracy % of 7 algorithms on the MUS-10 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>• DDG_p: we further remove the low-rank reconstruction based on DDG_l by replacing pair-wise reconstruction (set β = 0). That is, Z = I N , where I N is the identity matrix with size of N × N.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Convergence curves of our model on three cases, i.e., { A, D} → W , {V, L , C} → S and {U, S} → M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I STATISTICS</head><label>I</label><figDesc>OF THE THREE BENCHMARK DATASETS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II THE</head><label>II</label><figDesc>CROSS-RECOGNITION ACCURACY % OF 9 ALGORITHMS ON THE OFFICE-31 DATASET. RED COLOR DENOTES THE BEST RECOGNITION RATES. BLUE COLOR DENOTES THE SECOND BEST TABLE III THE CROSS-RECOGNITION ACCURACY % OF 9 ALGORITHMS ON THE OFFICE-10 + CALTECH-10 DATASET. RED COLOR DENOTES THE BEST RECOGNITION RATES. BLUE COLOR DENOTES THE SECOND BEST</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV THE</head><label>IV</label><figDesc>CROSS-RECOGNITION ACCURACY % OF 9 ALGORITHMS ON THE VLCS DATASET. RED COLOR DENOTES THE BEST RECOGNITION RATES. BLUE COLOR DENOTES THE SECOND BEST learning by borrowing the knowledge from related multiple sources.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V RECOGNITION</head><label>V</label><figDesc>RATE OF 4 ALGORITHMS ON DIFFERENT EVALUATION CASES</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www-scf.usc.edu/ boqinggo/domainadaptation.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://vc.sce.ntu.edu.sg/transfer_learning_domain_adaptation/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://people.eecs.berkeley.edu/ jhoffman/domainadapt/#datasets_code</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>http://www.cs.dartmouth.edu/ chenfang/proj_page/FXR_iccv13/index.php</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the NSF IIS under Award 1651902, in part by the ONR Young Investigator under Award N00014-14-1-0484, and in part by the U.S. Army Research Office under Award W911NF-17-1-0367. The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Catarina Brites.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010-10">Oct. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Transfer learning for visual categorization: A survey</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1019" to="1034" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="2066" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust visual domain adaptation with low-rank reconstruction</title>
		<author>
			<persName><forename type="first">I.-H</forename><surname>Jhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="2168" to="2175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transfer joint matching for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
			<biblScope unit="page" from="1410" to="1417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning transferrable representations for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2110" to="2118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">I have seen enough: Transferring parts across categories</title>
		<author>
			<persName><forename type="first">D</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Brit. Mach. Vis. Conf</title>
		<meeting>Brit. Mach. Vis. Conf</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Missing modality transfer learning via latent low-rank constraint</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4322" to="4334" />
			<date type="published" when="2015-11">Nov. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Latent low-rank transfer subspace learning for missing modality recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th AAAI Conf</title>
		<meeting>28th AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1192" to="1198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminative transfer subspace learning via low-rank and sparse representation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="850" to="863" />
			<date type="published" when="2016-02">Feb. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust transfer metric learning for image classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="660" to="670" />
			<date type="published" when="2017-02">Feb. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generalized domain-adaptive dictionaries</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="361" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-transfer: Transfer learning with multiple views and multiple sources</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statist. Anal. Data Mining</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="282" to="293" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Collaborative multi-domain sentiment classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Data Mining</title>
		<meeting>IEEE Int. Conf. Data Mining</meeting>
		<imprint>
			<date type="published" when="2015-11">Nov. 2015</date>
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-source domain adaptation: A causal view</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Assoc</title>
		<meeting>Assoc</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3150" to="3157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Undoing the damage of dataset bias</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="158" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unbiased metric learning: On the utilization of multiple datasets and Web images for softening bias</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Rockmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2013-12">Dec. 2013</date>
			<biblScope unit="page" from="1657" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploiting low-rank structure from latent domains for domain generalization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="628" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Domain generalization for object recognition with multi-task autoencoders</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
			<biblScope unit="page" from="2551" to="2559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visual recognition by learning from Web data: A weakly supervised domain generalization approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="2774" to="2783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning attributes equals multisource domain generalization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="87" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scatter component analysis: A unified framework for domain adaptation and domain generalization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1414" to="1430" />
			<date type="published" when="2017-07">Jul. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013-08">Aug. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-scale orderless pooling of deep convolutional activation features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="392" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
			<biblScope unit="page" from="4068" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">59</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Low-rank common subspace for multi-view learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Data Mining</title>
		<meeting>IEEE Int. Conf. Data Mining</meeting>
		<imprint>
			<date type="published" when="2014-12">Dec. 2014</date>
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deeply learned view-invariant features for cross-view action recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3028" to="3037" />
			<date type="published" when="2017-06">Jun. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning a non-linear knowledge transfer model for cross-view action recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="2458" to="2466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Feature selection guided auto-encoder</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artif. Intell</title>
		<meeting>AAAI Conf. Artif. Intell</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2725" to="2731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep linear coding for fast graph clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Artif. Intell</title>
		<meeting>Int. Joint Conf. Artif. Intell</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3798" to="3804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-view clustering via deep matrix factorization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artif. Intell</title>
		<meeting>AAAI Conf. Artif. Intell</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2921" to="2927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 32nd Int. Conf. Mach. Learn</title>
		<meeting>32nd Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Robust recovery of subspace structures by low-rank representation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="171" to="184" />
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning structured low-rank representations for image classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="676" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Structure-constrained low-rank representation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2167" to="2179" />
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A singular value thresholding algorithm for matrix completion</title>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1956" to="1982" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-02">Feb. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Weakly-shared deep transfer networks for heterogeneous-domain knowledge propagation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd ACM Int. Conf. Multimedia</title>
		<meeting>23rd ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd ACM Int. Conf. Multimedia</title>
		<meeting>22nd ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
