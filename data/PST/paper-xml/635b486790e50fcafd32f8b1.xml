<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MABEL: Attenuating Gender Bias using Textual Entailment Data</title>
				<funder ref="#_bNwwHUv">
					<orgName type="full">Princeton School of Engineering &amp; Applied Sciences</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-26">26 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jacqueline</forename><surname>He</surname></persName>
							<email>jacquelinehe00@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mengzhou</forename><surname>Xia</surname></persName>
							<email>mengzhou@cs.princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
							<email>fellbaum@cs.princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
							<email>danqic@cs.princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MABEL: Attenuating Gender Bias using Textual Entailment Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-26">26 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2210.14975v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pre-trained language models encode undesirable social biases, which are further exacerbated in downstream use. To this end, we propose MABEL (a Method for Attenuating Gender Bias using Entailment Labels), an intermediate pre-training approach for mitigating gender bias in contextualized representations. Key to our approach is the use of a contrastive learning objective on counterfactually augmented, gender-balanced entailment pairs from natural language inference (NLI) datasets. We also introduce an alignment regularizer that pulls identical entailment pairs along opposite gender directions closer. We extensively evaluate our approach on intrinsic and extrinsic metrics, and show that MABEL outperforms previous task-agnostic debiasing approaches in terms of fairness. It also preserves task performance after fine-tuning on downstream tasks. Together, these findings demonstrate the suitability of NLI data as an effective means of bias mitigation, as opposed to only using unlabeled sentences in the literature. Finally, we identify that existing approaches often use evaluation settings that are insufficient or inconsistent. We make an effort to reproduce and compare previous methods, and call for unifying the evaluation settings across gender debiasing methods for better future comparison. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pre-trained language models have reshaped the landscape of modern natural language processing <ref type="bibr" target="#b46">(Peters et al., 2018;</ref><ref type="bibr" target="#b20">Devlin et al., 2019;</ref><ref type="bibr" target="#b39">Liu et al., 2019)</ref>. As these powerful networks are optimized to learn statistical properties from large training corpora imbued with significant social biases (e.g., gender, racial), they produce encoded representations that inherit undesirable associations as a byproduct <ref type="bibr" target="#b67">(Zhao et al., 2019;</ref><ref type="bibr" target="#b62">Webster et al., 2020;</ref><ref type="bibr" target="#b42">Nadeem et al., 2021)</ref>. More concerningly, models trained on these representations can not only propagate but also amplify discriminatory judgments in downstream applications <ref type="bibr" target="#b36">(Kurita et al., 2019)</ref>.</p><p>A multitude of recent efforts have focused on alleviating biases in language models. These can be classed into two categories (Table <ref type="table" target="#tab_1">1</ref>): 1) taskspecific approaches perform bias mitigation during downstream fine-tuning, and require data to be annotated for sensitive attributes; 2) task-agnostic approaches directly improve pre-trained representations, most commonly either by removing discriminative biases through projection <ref type="bibr" target="#b17">(Dev et al., 2020;</ref><ref type="bibr" target="#b38">Liang et al., 2020;</ref><ref type="bibr" target="#b34">Kaneko and Bollegala, 2021)</ref>, or by performing intermediate pre-training on genderbalanced data <ref type="bibr" target="#b62">(Webster et al., 2020;</ref><ref type="bibr" target="#b10">Cheng et al., 2021;</ref><ref type="bibr" target="#b37">Lauscher et al., 2021;</ref><ref type="bibr" target="#b28">Guo et al., 2022)</ref>, resulting in a new encoder that can transfer fairness effects downstream via standard fine-tuning.</p><p>In this work, we present MABEL, a novel and lightweight method for attenuating gender bias. MABEL is task-agnostic and can be framed as an intermediate pre-training approach with a contrastive learning framework. Our approach hinges on the use of entailment pairs from supervised natural language inference datasets <ref type="bibr" target="#b6">(Bowman et al., 2015;</ref><ref type="bibr" target="#b64">Williams et al., 2018)</ref>. We augment the training data by swapping gender words in both premise and hypothesis sentences and model them using a contrastive objective. We also propose an alignment regularizer, which minimizes the distance between the entailment pair and its augmented one. MABEL optionally incorporates a masked language modeling objective, so that it can be used for token-level downstream tasks.</p><p>To the best of our knowledge, MABEL is the first to exploit supervised sentence pairs for learning fairer contextualized representations. Supervised contrastive learning via entailment pairs is known to learn a more uniformly distributed rep-resentation space, wherein similarity measures between sentences better correspond to their semantic meanings <ref type="bibr" target="#b23">(Gao et al., 2021)</ref>. Meanwhile, our proposed alignment loss, which pulls identical sentences along contrasting gender directions closer, is well-suited to learning a fairer semantic space.</p><p>We systematically evaluate MABEL on a comprehensive suite of intrinsic and extrinsic measures spanning language modeling, text classification, NLI, and coreference resolution. MABEL performs well against existing gender debiasing efforts in terms of both fairness and downstream task performance, and it also preserves language understanding on the GLUE benchmark <ref type="bibr" target="#b58">(Wang et al., 2019)</ref>. Altogether, these results demonstrate the effectiveness of harnessing NLI data for bias attenuation, and underscore MABEL's potential as a general-purpose fairer encoder.</p><p>Lastly, we identify two major issues in existing gender bias mitigation literature. First, many previous approaches solely quantify bias through the Sentence Encoding Association Test (SEAT) <ref type="bibr" target="#b40">(May et al., 2019)</ref>, a metric that compares the geometric relations between sentence representations. Despite scoring well on SEAT, many debiasing methods do not show the same fairness gains across other evaluation settings. Second, previous approaches evaluate on extrinsic benchmarks in an inconsistent manner. For a fairer comparison, we either reproduce or summarize the performance of many recent methodologies on major evaluation tasks. We believe that unifying the evaluation settings lays the groundwork for more meaningful methodological comparisons in future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Debiasing Contextualized Representations</head><p>Debiasing attempts in NLP can be divided into two categories. In the first category, the model learns to disregard the influence of sensitive attributes in representations during fine-tuning, through projectionbased <ref type="bibr" target="#b49">(Ravfogel et al., 2020</ref><ref type="bibr" target="#b50">(Ravfogel et al., , 2022))</ref>, adversarial <ref type="bibr">(Han et al., 2021a,b)</ref> or contrastive <ref type="bibr">(Shen et al., 2021;</ref><ref type="bibr" target="#b11">Chi et al., 2022)</ref> downstream objectives. This approach is task-specific as it requires fine-tuning data that is annotated for the sensitive attribute.</p><p>The second type, task-agnostic training, mitigates bias by leveraging textual information from general corpora. This can involve computing a gender subspace and eliminating it from encoded represen-tations <ref type="bibr" target="#b17">(Dev et al., 2020;</ref><ref type="bibr" target="#b38">Liang et al., 2020;</ref><ref type="bibr" target="#b18">Dev et al., 2021;</ref><ref type="bibr" target="#b34">Kaneko and Bollegala, 2021)</ref>, or by re-training the encoder with a higher dropout <ref type="bibr" target="#b62">(Webster et al., 2020)</ref> or equalizing objectives <ref type="bibr" target="#b10">(Cheng et al., 2021;</ref><ref type="bibr" target="#b28">Guo et al., 2022)</ref> to alleviate unwanted gender associations. We summarize recent efforts of both taskspecific and task-agnostic approaches in Table <ref type="table" target="#tab_1">1</ref>. Compared to task-specific approaches that only debias for the task at hand, task-agnostic models produce fair encoded representations that can be used toward a variety of applications. MABEL is task-agnostic, as it produces a general-purpose debiased model. Some recent efforts have broadened the scope of task-specific approaches. For instance, <ref type="bibr" target="#b41">Meade et al. (2022)</ref> adapt the task-specific Iterative Nullspace Linear Projection (INLP) <ref type="bibr" target="#b49">(Ravfogel et al., 2020)</ref> algorithm to rely on Wikipedia data for language model probing. While non-taskagnostic approaches can potentially be adapted to general-purpose debiasing, we primarily consider other task-agnostic approaches in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluating Biases in NLP</head><p>The recent surge of interest in fairer NLP systems has surfaced a key question: how should bias be quantified? Intrinsic metrics directly probe the upstream language model, whether by measuring the geometry of the embedding space <ref type="bibr" target="#b7">(Caliskan et al., 2017;</ref><ref type="bibr" target="#b40">May et al., 2019;</ref><ref type="bibr" target="#b27">Guo and Caliskan, 2021)</ref>, or through likelihood-scoring <ref type="bibr" target="#b36">(Kurita et al., 2019;</ref><ref type="bibr" target="#b43">Nangia et al., 2020;</ref><ref type="bibr" target="#b42">Nadeem et al., 2021)</ref>. Extrinsic metrics evaluate for fairness by comparing the system's predictions across different populations on a downstream task <ref type="bibr">(De-Arteaga et al., 2019a;</ref><ref type="bibr" target="#b67">Zhao et al., 2019;</ref><ref type="bibr" target="#b17">Dev et al., 2020)</ref>. Though opaque, intrinsic metrics are fast and cheap to compute, which makes them popular among contemporary works <ref type="bibr" target="#b41">(Meade et al., 2022;</ref><ref type="bibr" target="#b47">Qian et al., 2022)</ref>. Comparatively, though extrinsic metrics are more interpretable and reflect tangible social harms, they are often time-and compute-intensive, and so tend to be less frequently used. <ref type="foot" target="#foot_0">2</ref>To date, the most popular bias metric among task-agnostic approaches is the Sentence Encoder Association Test (SEAT) <ref type="bibr" target="#b40">(May et al., 2019)</ref>, which compares the relative distance between the encoded representations. Recent studies have cast doubt on the predictive power of these intrinsic indicators.  <ref type="bibr" target="#b49">(Ravfogel et al., 2020)</ref> * Wikipedia* CON <ref type="bibr">(Shen et al., 2021)</ref> -DADV <ref type="bibr">(Han et al., 2021b)</ref> -GATE <ref type="bibr">(Han et al., 2021a)</ref> -R-LACE <ref type="bibr" target="#b50">(Ravfogel et al., 2022)</ref> -Task-agnostic approaches CDA <ref type="bibr" target="#b62">(Webster et al., 2020)</ref> Wikipedia (1M steps, 36h on 8x 16 TPU) DROPOUT <ref type="bibr" target="#b62">(Webster et al., 2020)</ref> Wikipedia (100K steps, 3.5h on 8x 16 TPU) ADELE <ref type="bibr" target="#b37">(Lauscher et al., 2021)</ref> Wikipedia, BookCorpus (105M sentences) BIAS PROJECTION <ref type="bibr" target="#b17">(Dev et al., 2020)</ref> Wikisplit (1M sentences) OSCAR <ref type="bibr" target="#b18">(Dev et al., 2021)</ref> SNLI (190.1K sentences) SENT-DEBIAS <ref type="bibr" target="#b38">(Liang et al., 2020)</ref> WikiText-2, SST, Reddit, MELD, POM CONTEXT-DEBIAS <ref type="bibr" target="#b34">(Kaneko and Bollegala, 2021)</ref> News-commentary-v1 (87.66K sentences) AUTO-DEBIAS <ref type="bibr" target="#b28">(Guo et al., 2022)</ref> Bias prompts generated from Wikipedia (500) FAIRFIL <ref type="bibr" target="#b10">(Cheng et al., 2021)</ref> WikiText-2, SST, Reddit, MELD, POM #MABEL (ours) MNLI, SNLI with gender terms (134k sentences) SEAT has been found to elicit counter-intuitive results from encoders <ref type="bibr" target="#b40">(May et al., 2019)</ref> or exhibit high variance across identical runs <ref type="bibr" target="#b0">(Aribandi et al., 2021)</ref>. <ref type="bibr" target="#b26">Goldfarb-Tarrant et al. (2021)</ref> show that intrinsic metrics do not reliably correlate with extrinsic metrics, meaning that a model could score well on SEAT, but still form unfair judgements in downstream conditions. This is especially concerning as many debiasing studies <ref type="bibr" target="#b38">(Liang et al., 2020;</ref><ref type="bibr" target="#b10">Cheng et al., 2021)</ref> solely report on SEAT, which is shown to be unreliable and incoherent. For these reasons, we disregard SEAT as a main intrinsic metric in this work. <ref type="foot" target="#foot_1">3</ref>Bias evaluation is critical as it is the first step towards detection and mitigation. Given that bias reflects across language in many ways, relying upon a single bias indicator is insufficient <ref type="bibr" target="#b54">(Silva et al., 2021)</ref>. Therefore, we benchmark not just MABEL, but also current task-agnostic methods against a diverse set of intrinsic and extrinsic indicators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>MABEL attenuates gender bias in pre-trained language models by leveraging entailment pairs from natural language inference (NLI) data to produce general-purpose debiased representations. To the best of our knowledge, MABEL is the first method that exploits semantic signals from supervised sentence pairs for learning fairness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training Data</head><p>NLI data is shown to be especially effective in training discriminative and high-quality sentence representations <ref type="bibr" target="#b13">(Conneau et al., 2017;</ref><ref type="bibr" target="#b51">Reimers and Gurevych, 2019;</ref><ref type="bibr" target="#b23">Gao et al., 2021)</ref>. While previous works in fair representation learning use generic sentences from different domains <ref type="bibr" target="#b38">(Liang et al., 2020;</ref><ref type="bibr" target="#b10">Cheng et al., 2021;</ref><ref type="bibr" target="#b34">Kaneko and Bollegala, 2021)</ref>, we explore using sentence pairs with an entailment relationship: a hypothesis sentence that can be inferred to be true, based on a premise sentence. Since gender is our area of interest, we extract all entailment pairs that contain at least one gendered term in either the premise or the hypothesis from an NLI dataset. In our experiments, we explore using two well-known NLI datasets: the Stanford Natural Language Inference (SNLI) dataset <ref type="bibr" target="#b6">(Bowman et al., 2015)</ref> and the Multi-Genre Natural Language Inference (MNLI) dataset <ref type="bibr" target="#b64">(Williams et al., 2018)</ref>.</p><p>As a pre-processing step, we first conduct counterfactual data augmentation <ref type="bibr" target="#b62">(Webster et al., 2020)</ref> on the entailment pairs. For any sensitive attribute A woman is working on furniture. h Man putting together wooden shelf.</p><p>A man is working on furniture. ? sim(p, h) sim( p , ? )</p><p>Two girls are looking at something.</p><p>Two boys are looking at something.</p><p>Three humans together.</p><p>Three humans together. term in a word sequence, we swap it for a word along the opposite bias direction, i.e., girl to boy, and keep the non-attribute words unchanged. <ref type="foot" target="#foot_2">4</ref> This transformation is systematically applied to each sentence in every entailment pair. An example of this augmentation, with gender bias as the sensitive attribute, is shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Objective</head><p>Our training objective consists of three components: a contrastive loss based on entailment pairs and their augmentations, an alignment loss, and an optional masked language modeling loss.</p><p>Entailment-based contrastive loss. Training with a contrastive loss induces a more isotropic representation space, wherein the sentences' geometric positions can better align with their semantic meaning <ref type="bibr" target="#b59">(Wang and Isola, 2020;</ref><ref type="bibr" target="#b23">Gao et al., 2021)</ref>. We hypothesize that this contrastive loss would be conducive to bias mitigation, as concepts with similar meanings, but along opposite gender directions, move closer under this similarity measurement. Inspired by <ref type="bibr" target="#b23">Gao et al. (2021)</ref>, we use a contrastive loss that encourages the inter-association of entailment pairs, with the goal of the encoder also learning semantically richer associations. <ref type="foot" target="#foot_3">5</ref>With p as the premise representation and h as the hypothesis representation, let {(p i , h i )} n i=1 be the sequence of representations for n original entailment pairs, and {(p i , ?i )} n i=1 be n counterfactuallyaugmented entailment pairs. Each entailment pair (and its corresponding augmented pair) forms a positive pair, and the other in-batch sentences constitute negative samples. With m pairs and their augmentations in one training batch, the contrastive objective for an entailment pair i is defined as:</p><formula xml:id="formula_0">L (i) CL = -log e sim(p i ,h i )/? m j=1 e sim(p i ,h j )/? + e sim(p i , ?j )/?</formula><p>-log e sim(p i , ?i )/? m j=1 e sim(p i ,h j )/? + e sim(p i , ?j )/?</p><p>, where sim(?, ?) denotes the cosine similarity function, and ? is the temperature. L CL is simply the average of all the losses in a training batch. Note that when h i = ?i (i.e., when h i does not contain any gender words and the augmentation is unchanged), we exclude ?i from the denominator to avoid h i as a positive sample and ?i as a negative sample for p i , and vice versa.</p><p>Alignment loss. We want a loss that encourages the intra-association between the original entailment pairs and their augmented counterparts. Intuitively, the features from an entailment pair and its gender-balanced opposite should be taken as positive samples and be spatially close. Our alignment loss minimizes the distance between the cosine similarities of the original sentence pairs (p i , h i ) and the gender-opposite sentence pairs (p i , ?i ):</p><formula xml:id="formula_1">L AL = 1 m m i=1 sim(p i , ?i ) -sim(p i , h i ) 2 .</formula><p>We assume that a model is less biased if it assigns similar measurements to two gender-opposite pairs, meaning that it maps the same concepts along different gender directions to the same contexts. <ref type="foot" target="#foot_4">6</ref>Masked language modeling loss. Optionally, we can append an auxiliary masked language modeling (MLM) loss to preserve the model's language modeling capability. Following <ref type="bibr" target="#b20">Devlin et al. (2019)</ref>, we randomly mask p = 15% of tokens in all sentences. By leveraging the surrounding context to predict the original terms, the encoder is incentivized to retain token-level knowledge.</p><p>In sum, our training objective is as follows:</p><formula xml:id="formula_2">L = (1 -?) ? L CL + ? ? L AL + ? ? L MLM ,</formula><p>wherein the two contrastive losses are linearly interpolated by a tunable coefficient ?, and the MLM loss is tempered by the hyper-parameter ?.</p><p>4 Evaluation Metrics CrowS-Pairs <ref type="bibr" target="#b43">(Nangia et al., 2020</ref>) is an intrasentence dataset of minimal pairs, where one sentence contains a disadvantaged social group that either fulfills or violates a stereotype, and the other sentence is minimally edited to contain a contrasting advantaged group. The language model compares the masked token probability of tokens unique to each sentence. Focusing only on gender examples, we report the stereotype score (SS), the percentage in which a model assigns a higher aggregated masked token probability to a stereotypical sentence over an anti-stereotypical one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Extrinsic Metrics</head><p>As there has been some inconsistency in the evaluation settings in the literature, we mainly con-sider the fine-tuning setting for extrinsic metrics and leave the discussion of the linear probing setting to Appendix I.</p><p>Bias-in-Bios <ref type="bibr">(De-Arteaga et al., 2019b</ref>) is a thirdperson biography dataset annotated by occupation and gender. We fine-tune the encoder, along with a linear classification layer, to predict an individual's profession given their biography. We report overall task accuracy and accuracy by gender, as well as two common fairness metrics <ref type="bibr">(De-Arteaga et al., 2019b;</ref><ref type="bibr" target="#b49">Ravfogel et al., 2020)</ref>: 1) GAP T P R M , the difference in true positive rate (TPR) between male-and female-labeled instances; 2) GAP T P R M,y , the root-mean square of the TPR gap of each occupation class.</p><p>Bias-NLI <ref type="bibr" target="#b17">(Dev et al., 2020)</ref> is an NLI dataset consisting of neutral sentence pairs. It is systematically constructed by populating sentence templates with a gendered word and an occupation word with a strong gender connotation (e.g., The woman ate a bagel; The nurse ate a bagel). Bias can be interpreted as a deviation from neutrality and is determined by three metrics: Net Neutral (NN), Fraction Neutral (FN) and Threshold:? (T:? ). A bias-free model should score a value of 1 across all 3 metrics. We fine-tune on SNLI and evaluate on Bias-NLI during inference.</p><p>WinoBias <ref type="bibr" target="#b68">(Zhao et al., 2018</ref>) is an intra-sentence coreference resolution task that evaluates a system's ability to correctly link a gendered pronoun to an occupation across both pro-stereotypical and anti-stereotypical contexts. Coreference can be inferred based on syntactic cues in Type 1 sentences or on more challenging semantic cues in Type 2 sentences. We first fine-tune the model on the OntoNotes 5.0 dataset <ref type="bibr" target="#b32">(Hovy et al., 2006)</ref> before evaluating on the WinoBias benchmark. We report the average F1-scores for pro-stereotypical and anti-stereotypical instances, and the true positive rate difference in average F1-scores, across Type 1 and Type 2 examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Language Understanding</head><p>To evaluate whether language models still preserve general linguistic understanding after bias attenuation, we fine-tune them on seven classification tasks and one regression task from the General Language Understanding Evaluation (GLUE) benchmark <ref type="bibr" target="#b58">(Wang et al., 2019)</ref>.<ref type="foot" target="#foot_5">7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baselines &amp; Implementation Details</head><p>We choose Sent-Debias<ref type="foot" target="#foot_6">8</ref>  <ref type="bibr" target="#b38">(Liang et al., 2020)</ref>, Context-Debias<ref type="foot" target="#foot_7">9</ref>  (Kaneko and Bollegala, 2021),  and FairFil 10 (Cheng et al., 2021) as our primary baselines. By introducing a general-purpose method for producing debiased representations, these three approaches are most similar in spirit to MABEL. We consider FairFil to be especially relevant as it is also a task-agnostic, contrastive learning approach. Compared to FairFil, MABEL leverages NLI data, and also applies entailmentbased and MLM losses to ensure that sentence-and token-level knowledge is preserved.</p><p>We evaluate the three aforementioned taskagnostic baselines across all bias benchmarks and also compare against other approaches in Table 1 by reporting the recorded numbers from their original work. Unless otherwise specified, all models, including MABEL, default to bert-base-uncased <ref type="bibr" target="#b20">(Devlin et al., 2019)</ref> as the backbone encoder. In the default setting, ? = 0.1 and ? = 0.05. Implementation details on MABEL and the task-agnostic baselines can be found in Appendix A and Appendix B, respectively. For our own implementations, we report the average across 3 runs.<ref type="foot" target="#foot_9">11</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results: Intrinsic Metrics</head><p>As Table <ref type="table" target="#tab_2">2</ref> shows, MABEL strikes a good balance between language modeling and fairness with the highest ICAT score. Compared to BERT, MABEL retains and even exhibits an average modest improvement (from 84.17 to 84.80) in language modeling. MABEL also performs the best on CrowS-Pairs, with an average metric score of 50.76.</p><p>While MABEL does not have the best SS value for StereoSet, we must caution that this score should not be considered in isolation. For example, although FairFil shows a better stereotype score, its language modeling ability (as the LM score shows) is significantly deteriorated and lags behind other approaches. This is akin to an entirely random model that obtains a perfect SS of 50 as it does not contain bias, but would also have a low LM score as it lacks linguistic knowledge.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results: Extrinsic Metrics</head><p>Bias-in-Bios. As Table <ref type="table" target="#tab_3">3</ref> indicates, MABEL exhibits the highest overall and individual accuracies, as well as the smallest TPR-GAP when compared against the task-agnostic baselines and BERT.</p><p>Still, MABEL and the other task-agnostic models are close in performance to BERT on Bias-in-Bios, which suggests that the fine-tuning process can significantly change a pre-trained model's representational structure to suit a specific downstream task. Furthermore, <ref type="bibr" target="#b35">Kaneko et al. (2022)</ref> finds that debiased language models can still re-learn social biases after standard fine-tuning on downstream tasks, which may explain why the task-agnostic methods, which operate upstream, cope worse with this particular manifestation of gender bias than on others. Our results also show that task-specific interventions fare better fairness-wise on this task. Methods such as INLP <ref type="bibr" target="#b49">(Ravfogel et al., 2020)</ref>, GATE <ref type="bibr">(Han et al., 2021a)</ref>, and R-LACE <ref type="bibr" target="#b50">(Ravfogel et al., 2022)</ref> exhibit better TPR RMS scores, although sometimes at the expense of task accuracy.</p><p>As these methods operate directly on the downstream task, they may have a stronger influence on the final prediction <ref type="bibr" target="#b33">(Jin et al., 2021)</ref>.</p><p>Bias-NLI. We next move to Bias-NLI, where Table <ref type="table" target="#tab_4">4</ref> indicates that MABEL outperforms BERT and other baselines across all metrics. Unlike in Bias-in-Bios, the results have a greater spread, and MABEL's comparative advantage becomes clear.</p><p>The FN score denotes that, on average, MABEL correctly predicts neutral 97.7% of the time. MA-BEL is also more confident in predicting the correct answer, surpassing the 0.7 threshold 93.5% of the time. Other approaches, such as Sent-Debias and FairFil, do not show as clear-cut of an improvement over BERT, despite scoring well on other bmetrics such as SEAT.</p><p>As natural language inference requires robust semantic reasoning capabilities to deduce the correct answer, it is a more challenging problem than classification. Therefore, for this task, the models' initialization weights-which store the linguistic knowledge acquired in pre-training-may play a larger impact on the final task accuracy than in Bias-in-Bios.</p><p>WinoBias. On this token-level extrinsic task (Table 5), MABEL, and the other bias mitigation baselines, achieve very similar average F1-scores on OntoNotes. However, performance on Wino-Bias becomes variegated. MABEL shows the best task improvement on anti-stereotypical tasks, with an average 7.25% and 10.58% increase compared to BERT on Type 1 and Type 2 sentences, respectively. The strong performance on antistereotypical examples implies that MABEL can effectively weaken the stereotypical token-level associations between occupation and gender. Though MABEL exhibits a marginally lower F1-score on Type 1 pro-stereotypical examples (an 1.64% decrease on average compared to the best-performing model, BERT), it has the highest F1-scores across all other categories. Furthermore, it has the best reduction in fairness, with the smallest average TPR-1 and TPR-2 by a clear margin (respectively, 23.73 and 3.41, compared to the next-best average TPR scores at 26.14 and 9.57).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results: Language Understanding</head><p>As the GLUE benchmark results indicate (Table <ref type="table" target="#tab_6">6</ref>), MABEL preserves semantic knowledge across downstream tasks. On average, MABEL performs marginally better than BERT (82.0% vs. 81.8%), but not as well as BERT fine-tuned beforehand on the NLI task with MNLI and SNLI data (BERT-NLI), at 82.0% vs. 82.1%. Other bias mitigation baselines lag behind BERT, but the overall semantic deterioration remains minimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Qualitative Comparison</head><p>We perform a small qualitative study by visualizing the t-SNE (van der Maaten and Hinton, 2008) plots of sentence representations from BERT, supervised SimCSE <ref type="bibr" target="#b23">(Gao et al., 2021)</ref>, and MABEL. Following <ref type="bibr" target="#b38">Liang et al. (2020)</ref>; <ref type="bibr" target="#b10">Cheng et al. (2021)</ref>, we plot averaged sentence representations of a gendered or neutral concept across different contexts (sentence templates). We re-use the list of gender words, and neutral words with strong gender connotations, from <ref type="bibr" target="#b7">Caliskan et al. (2017)</ref>.</p><p>From Figure <ref type="figure" target="#fig_3">2</ref>, in BERT, certain concepts from technical fields such as 'technology' or 'science' are spatially closer to 'man,' whereas concepts from the humanities such as 'art' or 'literature' are closer to 'woman.' After debiasing with MA-BEL, we observe that the gendered tokens (e.g., 'man <ref type="bibr">' and 'woman,' or 'girl' and 'boy')</ref> have shifted closer in the embedding space, and away from the neutral words. While SimCSE shows a similar trend in pulling gendered words away from the neutral words, it also separates the masculine and feminine terms into two distinct clusters. This is undesirable behavior, as it suggests that identical concepts along opposite gender directions are now further apart in latent space, and are have become more differentiated in the same contexts.</p><formula xml:id="formula_3">Model OntoNotes ? 1A ? 1P ? 2A ? 2P ? TPR-1 ? TPR-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Ablations</head><p>We perform extensive ablations to show that every component of MABEL benefits the overall system. We use StereoSet, CrowS-Pairs, and Bias-NLI as representative tasks.</p><p>Comparing other supervised pairs. Since leveraging entailment examples as positive pairs is conducive to high-quality representation learning <ref type="bibr" target="#b23">(Gao et al., 2021)</ref>, we believe that this construction is particularly suitable for semantic retention. To justify our choice, we further train on neutral pairs and contradiction pairs from the SNLI dataset. We also consider paraphrase pairs from the Quora Question Pairs (QQP) dataset <ref type="bibr" target="#b60">(Wang et al., 2017)</ref> and the Para-NMT dataset <ref type="bibr" target="#b63">(Wieting and Gimpel, 2018)</ref>. Finally, we try individual unlabeled sentences from the same multi-domain corpora used by <ref type="bibr" target="#b38">Liang et al. (2020)</ref> and <ref type="bibr" target="#b10">Cheng et al. (2021)</ref>. In this setting, standard dropout is applied: positive pairs are constructed by encoding the same sentence twice with different masks, resulting in two minimally different embeddings <ref type="bibr" target="#b23">(Gao et al., 2021)</ref>. From   Entailment pairs, and neutral pairs to a lesser extent, demonstrate the best language retention on the Bias-NLI metric, although the QQP dataset also performs well. One possible explanation is that the QQP paraphrases hold a similar, albeit weaker, directional relationship to NLI pairs. Disentangling objectives. Results of MABEL trained with ablated losses are in Table <ref type="table" target="#tab_8">8</ref>. Without the MLM objective, the LM score collapses along with the ICAT score. Though the SS score approaches 50, it seems to be more indicative of randomness than model fairness; the off-the-shelf LM head is no longer compatible with the trained encoder. With the contrastive loss omitted, MA-BEL's performance on Bias-NLI drops from the 0.9 range to the 0.8 range, showing that it is key to preserving sentence-level knowledge. Removing the alignment loss also leads to a similar decrease in performance on Bias-NLI. As this particular objective does not directly optimize semantic understanding, we attribute this drop to a reduction in fairness knowledge.</p><p>Impact of batch size. Table <ref type="table" target="#tab_9">9</ref> shows the effect of batch size on StereoSet performance. Encouragingly, although contrastive representation learning typically benefits from large batch sizes <ref type="bibr" target="#b9">(Chen et al., 2020)</ref>, an aggregated batch size of 128 already works well. MABEL is very lightweight and trains in less than 8 hours on a single GPU. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We propose MABEL, a simple bias mitigation technique that harnesses supervised signals from entailment pairs in NLI data to create informative and fair contextualized representations. We compare MABEL and other recent task-agnostic debiasing baselines across a wide range of intrinsic and extrinsic bias metrics, wherein MABEL demonstrates a better performance-fairness tradeoff. Its capacity for language understanding is also minimally impacted, rendering it suitable for general-purpose use. Systematic ablations show that both the choice of data and individual objectives are integral to MABEL's good performance. Our contribution is complementary to the bias transfer hypothesis <ref type="bibr" target="#b33">(Jin et al., 2021)</ref>, which suggests that upstream bias mitigation effects are transferable to downstream settings. We hope that MABEL adds a new perspective toward creating fairer language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Following prior bias mitigation work <ref type="bibr" target="#b10">(Cheng et al., 2021;</ref><ref type="bibr" target="#b38">Liang et al., 2020)</ref>, our framework relies on a curated list of gender word pairs for counterfactual data augmentation. While we believe that our general list is broad enough to cover the majority of gendered terms in a dataset, this lexicon is nevertheless non-exhaustive and cannot completely remove all bias directions <ref type="bibr" target="#b22">(Ethayarajh et al., 2019)</ref>. One possible improvement would be to use automatic perturbation augmentation on the entailment pairs <ref type="bibr" target="#b47">(Qian et al., 2022</ref>) (concurrent work), a more expansive technique that counterfactually augments data along multiple demographic axes.</p><p>Another consideration is that we primarily juxtapose against task-agnostic approaches in our work, even though some task-specific procedures, specifically R-LACE <ref type="bibr" target="#b50">(Ravfogel et al., 2022)</ref> and INLP <ref type="bibr" target="#b49">(Ravfogel et al., 2020)</ref>, show excellent gains in occupation classification, an extrinsic task. Recently, <ref type="bibr" target="#b41">Meade et al. (2022)</ref> has successfully adapted INLP to a task-agnostic setting by mining on an unlabeled corpus. We believe that other task-specific methods can be similarly adapted to train task-agnostic encoders, though we leave this comparison to future work. In light of recent findings that bias can re-enter the model during any stage of the training pipeline <ref type="bibr" target="#b33">(Jin et al., 2021;</ref><ref type="bibr" target="#b35">Kaneko et al., 2022)</ref>, one interesting direction would be to pair MABEL, which is task-agnostic, with task-specific procedures. Essentially, by debiasing at both endsfirst upstream in the encoder, then downstream in the classifier-MABEL could potentially achieve a greater reduction in bias across some of the extrinsic benchmarks.</p><p>Although MABEL shows exciting performance across an extensive range of evaluation settings, these results should not be construed as a complete erasure of bias. For one, our two main intrinsic metrics, StereoSet and CrowS-Pairs, are skewed towards North American social biases and only re-flect positive predictive power. They can detect the presence, not the absence of bias <ref type="bibr" target="#b41">(Meade et al., 2022)</ref>. <ref type="bibr" target="#b0">Aribandi et al. (2021)</ref> furthers that these likelihood-based diagnostics can vary wildly across identical model checkpoints trained on different random seeds. <ref type="bibr" target="#b4">Blodgett et al. (2021)</ref> points to the unreliability of several benchmarks we use, including StereoSet, CrowS-Pairs, and WinoBias, which inadequately articulate their assumptions of stereotypical behaviors. Additionally, MABEL's gains in fairness are not universally strong-it handles some operationalizations of gender bias more effectively than others. One reason for this inconsistency is that bias metrics have been found to correlate poorly; desirable performance on one bias indicator does not necessarily translate to equivalently significant gains on other evaluation tasks <ref type="bibr" target="#b26">(Goldfarb-Tarrant et al., 2021;</ref><ref type="bibr" target="#b44">Orgad et al., 2022)</ref>. The lack of clarity and agreement in existing evaluation frameworks is a fundamental challenge in this field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>There are several ethical points of consideration to this work. As our contribution is entirely methodological, we rely upon an existing range of wellknown datasets and evaluation tasks that assume a binary conceptualization of gender. In particular, the over-simplification of gender identity as a dichotomy, not as a spectrum, means that MA-BEL does not adequately address the full range of stereotypical biases expressed in real life. We fully acknowledge and support the development of more inclusive methodological tools, datasets, and evaluation mechanisms.</p><p>Furthermore, we restrict the definitonal scope of bias in this work to allocational and representational bias <ref type="bibr" target="#b1">(Barocas et al., 2017)</ref>. Allocational bias is the phenomenon in which models perform systematically better for some social groups over others, e.g., a coreference resolution system that successfully identifies male coreferents at a higher rate over female ones. Representational bias denotes the spurious associations between social groups and certain words or concepts. An example would be the unintentional linkage of genders with particular occupations, as captured by contextualized word representations.</p><p>We neglect other critical types of biases under this framework, in particular intersectional biases. As per <ref type="bibr" target="#b56">Subramanian et al. (2021)</ref>, most existing debiasing techniques only consider sensitive at-tributes, e.g., race or gender, in isolation. However, a truly fair model does not and cannot operate in a vacuum, and should be able to handle a complex combination of various biases at once.</p><p>Another consideration is that MABEL is entirely English-centric. This assumption is symptomatic of a larger problem, as most gender bias studies are situated in high-resource languages. Given that conceptualizations of gender and language are a function of societal and cultural norms, it is imperative that the tools we create can generalize beyond an English context. For instance, some languages such as Spanish or German contain grammatical gender, meaning that nouns or adjectives can have masculine or feminine forms. The need to account for both linguistic gender and social gender significantly complicates the matter of bias detection and elimination.</p><p>For these reasons, practitioners should exercise great caution when applying MABEL to real-world use cases. At its present state, MABEL should not be viewed as a one-size-fits-all solution to gender bias in NLP, but moreso as a preliminary effort to illuminate and attenuate aspects of a crucial, elusive, and multi-faceted problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation Details of MABEL</head><p>We use an aggregation of entailment pairs from the SNLI and MNLI datasets, and augment pairs with opposite gender directions, drawing from the same list of attribute word pairs used by <ref type="bibr" target="#b5">Bolukbasi et al. (2016)</ref>, <ref type="bibr" target="#b38">Liang et al. (2020), and</ref><ref type="bibr" target="#b10">Cheng et al. (2021)</ref>: (man, woman), (boy, girl), (he, she), (father, mother), (son, daughter), (guy, gal), (male, female), (his, her), (himself, herself), (John, Mary), alongside plural forms.</p><p>We implement MABEL using the HuggingFace Trainer in PyTorch <ref type="bibr" target="#b45">(Paszke et al., 2019)</ref> and train for 2 epochs. We take the last-saved checkpoint. Training MABEL takes less than 2 hours across 4 NVIDIA GeForce RTX 3090 GPUs.</p><p>Ablation details. Dataset sizes from our ablation study are in Table <ref type="table" target="#tab_11">10</ref>.  Besides batch size, we also tune for learning rate ? {1e -5 , 3e -5 , 5e -5 } and ? ? {0.01, 0.05, 0.1}.</p><p>As Table <ref type="table" target="#tab_12">11</ref> indicates, increasing the learning rate improves fairness as the stereotype score approaches 50, but also seems to slightly diminish the model's language modeling ability. Furthermore, a larger ? results in a fairer stereotype score, which corroborates our intuition as this parameter adjusts the influence of our alignment loss. Unfortunately, increasing ? also monotonically decreases the language modeling score.</p><p>Therefore, we take a learning rate of 5e -5 , a batch size of 32, and an ? = 0.05 as our default hyper-parameters; these result in the best trade-off between fairness and language modeling ability. We use ? = 0.1 in all the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Baseline Implementation</head><p>Context-Debias. We use the model checkpoint provided by <ref type="bibr" target="#b34">Kaneko and Bollegala (2021)</ref>, and treat it as a regular encoder for downstream evaluation. Sent-Debias. We use the code and data provided by <ref type="bibr" target="#b38">Liang et al. (2020)</ref> to compute the gender bias subspace. For downstream evaluation, the debiasing step (subtracting the subspace from the representations) is applied directly after encoding.</p><p>FairFil. As code for this work is not available, we re-implement FairFil, the main contrastive approach, without the additional informationtheoretic regularizer. Note that the reported performance difference from including the regularizer or not (0.150 vs. 0.179 on SEAT) is marginal. We checked all the implementation details carefully and report our reproduced and original SEAT effect size results in Table <ref type="table" target="#tab_2">12</ref>. and reproduced (R) results on FairFil (FF). We report the average and standard deviation for our reproduction. ?: the closer to 0, the better.</p><formula xml:id="formula_4">SEAT FF (O) FF (R) Category ES ? ES ? Names,</formula><p>During evaluation, we fix the FairFil layer upon initialization so that its parameters no longer update. We debias by feeding encoded representations through the layer. In the ideal scenario, a perfectly fair and highly performative language model would have an LM score of 100, an SS score of 50, and thus an ICAT score of 100. Therefore, the higher the ICAT score, the better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Evaluation Details</head><p>CrowS-Pairs. While CrowS-Pairs originally used pseudo log-likelihood MLM scoring, this form of measurement is found to be error-prone <ref type="bibr" target="#b41">(Meade et al., 2022)</ref>. Therefore, we follow <ref type="bibr" target="#b41">Meade et al. (2022)</ref>'s evaluation approach, and compare the masked token probability of tokens unique to each sentence. The stereotype score (SS) for this task is the percentage of instances for which a language model computes a greater masked token probability to a stereotypical sentence over an anti-stereotypical sentence. An impartial language model without stereotypical biases should score an SS of 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Extrinsic Metrics</head><p>Bias-in-Bios. GAP T P R M is denoted as (observe that the closer the value is to 0, the better)</p><formula xml:id="formula_5">GAP T P R M = |T P R M -T P R F |.</formula><p>Merely taking the difference in overall accuracies does not account for the highly imbalanced nature of the Bias-in-Bios dataset. In line with <ref type="bibr" target="#b49">Ravfogel et al. (2020)</ref>, we also calculate the root-mean square of GAP T P R M,y to obtain a more robust metric. Taking y as a profession in C, the set of all 28 professions, we can compute</p><formula xml:id="formula_6">GAP T P R,RM S M = 1 |C| y?C (GAP T P R M,y ) 2 .</formula><p>Following the suggestion of De-Arteaga et al. (2019a), our train-val-test split of the Bias-in-Bios dataset is 65/25/10. We were able to scrape 206,511 biographies. 12  In the fine-tuning setting, we train for 5 epochs and evaluate every 1000 steps on the validation set. The model checkpoint is saved if the validation accuracy has improved. We use the AutoModelForSequenceClassification class from the transformers package <ref type="bibr" target="#b65">(Wolf et al., 2020)</ref>, which extracts sentence representations by taking the last-layer hidden state of the [CLS] token and feeding it through a linear layer with tanh activation. We use a batch size of 128, a learning rate of ? = 1e -5 , and a maximum sequence length of 128.</p><p>Bias-NLI. The three evaluation metrics used in Bias-NLI task are calculated as follows:</p><p>1. Net Neutral (NN): The average probability of the neutral label across all instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Fraction Neutral (FN):</head><p>The fraction of sentence pairs accurately labeled as neutral.</p><p>3. Threshold:? (T:? ): The fraction of instances with the probability of neutral above ? .</p><p>In the linear probing setting, we construct an updating linear layer on top of the frozen encoder. Following <ref type="bibr" target="#b17">Dev et al. (2020)</ref>, sentence representations are extracted from the [CLS] token of the last hidden state. We use a batch size of 64, a learning rate of ? = 5 ? 10 -5 , and a maximum sequence length of 128. We fine-tune for 3 epochs and evaluate every 500 steps, saving the checkpoint if the validation accuracy improves. We randomly subsample 10,000 elements from <ref type="bibr" target="#b17">Dev et al. (2020)</ref>'s evaluation dataset during inference. WinoBias. Each WinoBias example contains exactly two mentions of professions and one pronoun, which co-refers correctly to one of the profession (Table <ref type="table" target="#tab_3">13</ref>). Type 1 sentences are syntactically ambiguous and require world knowledge to be correctly resolved, while Type 2 sentences are easier and can be inferred through only syntactic cues. Examples are presented in in Table <ref type="table" target="#tab_3">13</ref>.</p><p>Following previous gender bias analyses <ref type="bibr" target="#b44">(Orgad et al., 2022)</ref>, we borrow the PyTorch reimplementation of the end-to-end c2f-coref model from <ref type="bibr" target="#b66">Xu and Choi (2020)</ref>. We use the cased version of encoders, a significant performance difference exists between cased and uncased variants. Models </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prostereotypical</head><p>The developer argued with the designer because he did not like the design.</p><p>The guard admired the secretary and wanted her job.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Antistereotypical</head><p>The developer argued with the designer because his design cannot be implemented.</p><p>The secretary called the mover and asked her to come.</p><p>Table <ref type="table" target="#tab_3">13</ref>: Example of Type I and Type II sentences from the WinoBias dataset <ref type="bibr" target="#b68">(Zhao et al., 2018)</ref>. The colored text indicates the pronoun and the correct coreferent.</p><p>are trained for 24 epochs with a dropout rate of 0.3 and a maximum sequence length of 384. Encoder parameters and task parameters have separate learning rates (1 ? 10 -5 and 3 ? 10 -4 ), separate linear decay schedules, and separate weight decay rates (1 ? 10 -2 and 0).</p><p>We report the averaged F1-score of three coreference evaluation metrics: MUC, B 3 , and CEAF, following <ref type="bibr" target="#b66">Xu and Choi (2020)</ref>.</p><p>C.3 Language Understanding GLUE. CoLA <ref type="bibr" target="#b61">(Warstadt et al., 2019)</ref> and SST-2 <ref type="bibr" target="#b55">(Socher et al., 2013)</ref> are single-sentence tasks; MRPC <ref type="bibr" target="#b21">(Dolan and Brockett, 2005)</ref> and QQP are paraphrase detection tasks; MNLI <ref type="bibr" target="#b64">(Williams et al., 2018)</ref>, QNLI <ref type="bibr" target="#b48">(Rajpurkar et al., 2016), and</ref><ref type="bibr">RTE (Dagan and</ref><ref type="bibr" target="#b14">Glickman, 2005;</ref><ref type="bibr" target="#b29">Haim et al., 2006;</ref><ref type="bibr" target="#b25">Giampiccolo et al., 2007</ref><ref type="bibr" target="#b24">Giampiccolo et al., , 2008;;</ref><ref type="bibr" target="#b3">Bentivogli et al., 2009)</ref> are inference tasks; STS-B <ref type="bibr" target="#b8">(Cer et al., 2017)</ref> is a sentence similarity task. We report the accuracy for SST-2, MNLI, QNLI, RTE, and STS-B, and the Matthews correlation coefficient for CoLA. Both the accuracy and the F-1 score are included for MRPC and QQP.</p><p>We use the run_glue.py script provided by HuggingFace <ref type="bibr" target="#b65">(Wolf et al., 2020)</ref>, and follow their exact hyper-parameters. For all tasks, we use a batch size of 32, a maximum sequence length of 128, and a learning rate of 2 ? 10 -5 . We train for 3 epochs for all tasks except for MRPC, which is trained for 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Standard Deviation</head><p>As many fairness benchmarks tend to exhibit high variance, we report the standard deviation across 3 runs for each of our implementations from the main results. Standard deviations for intrinsic tasks can be found in  <ref type="table" target="#tab_2">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E SentEval Transfer Evaluation</head><p>To more thoroughly evaluate our models' capacity for NLU retention, we additionally test on 9 transfer tasks provided by the SentEval toolkit <ref type="bibr" target="#b12">(Conneau and Kiela, 2018)</ref>. While some task overlap exists between SentEval and GLUE, the evaluation setup is different. By freezing the encoder and training a logistic regression classifier, the default SentEval implementation focuses on evaluating the knowledge stored in a fixed-size frozen sentence embedding. Whereas when testing on GLUE, the parameters in the entire encoder are allowed to freely update. GLUE also emphasizes high-resource downstream tasks, which use training data with hundreds of thousands of samples. Comparatively, SentEval mostly focuses on low-resource transfer, with smaller downstream classification tasks such as Movie Review (MR) or Product Review (CR) <ref type="bibr" target="#b12">(Conneau and Kiela, 2018)</ref>.</p><p>We use the evaluation toolkit provided by Conneau and Kiela (2018), following the standard settings, and form sentence representations by extracting the [CLS] token of the last hidden state. 10fold cross-validation was used for MR, CR, SUBJ, MPQA, and SST-2, and cross-validation for TREC. For MRPC, a 2-class classifier learns to predict the probability distribution of relatedness scores; the Pearson correlation coefficient is reported.</p><p>Table <ref type="table" target="#tab_18">16</ref> shows the performance across the downstream SentEval transfer tasks. As this evaluation  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Bias Benchmarks in Other Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G SEAT Evaluation</head><p>The Sentence Encoder Association Test (SEAT) <ref type="bibr" target="#b40">(May et al., 2019)</ref> is the sentence-level extension of the Word Embedding Association Test (WEAT) <ref type="bibr" target="#b7">(Caliskan et al., 2017)</ref> For both WEAT and SEAT, the null hypothesis postulates that no difference exists in the relative similarity between the sets of target words X, Y and the sets of attribute words A, B. The effect size, s(X, Y, A, B), quantifies the difference in mean cosine similarity between representations of the target concept pair (X, Y ), and representations of the attribute concept pair (A, B), through the We report our results (alongside pre-trained BERT's) in Table <ref type="table" target="#tab_8">18</ref>. Following <ref type="bibr" target="#b38">Liang et al. (2020)</ref>, we extract the sentence representation as the [CLS] token fed through a linear layer and tanh activation (e.g., the pooled output).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H A Comparison to SimCSE</head><p>A non-debiasing analogue to MABEL is supervised SimCSE <ref type="bibr" target="#b23">(Gao et al., 2021)</ref>, a state-of-the-art representation learning approach that generates sentence representations with good semantic textual similarity (STS) performance. Like MABEL, supervised SimCSE also trains on entailment pairs from NLI data using an contrastive learning objective.</p><p>One potential concern is that MABEL performs well on tasks such as Bias-NLI not due to greater fairness, but because it has already been trained on NLI data. To test this assumption, we repeat the Bias-NLI task on SimCSE, which has been trained on un-augmented entailment pairs from the SNLI dataset. In Table <ref type="table" target="#tab_9">19</ref>, the tangible increase across all metrics from SimCSE to MABEL indicates that MABEL's good performance can not solely be attributed to NLI knowledge retention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Linear Probing Experiments</head><p>To better illustrate the effects of MABEL, we conduct a suite of probing experiments, in which the entire encoder parameters are frozen during training. We summarize task details and results below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.1 Bias-in-Bios</head><p>We follow the same procedure as in the fine-tuning setting, except freeze the encoder and only update the linear classification layer. From  <ref type="bibr" target="#b62">(Webster et al., 2020)</ref> DisCo STS-B, WinoGender ADELE <ref type="bibr" target="#b37">(Lauscher et al., 2021)</ref> BEC-Pro, DisCo STS-B BIAS PROJECTION <ref type="bibr" target="#b17">(Dev et al., 2020)</ref> OSCAR <ref type="bibr" target="#b18">(Dev et al., 2021)</ref> ECT SIRT SENT-DEBIAS <ref type="bibr" target="#b38">(Liang et al., 2020)</ref> CONTEXT-DEBIAS <ref type="bibr" target="#b34">(Kaneko and Bollegala, 2021</ref> Table <ref type="table" target="#tab_1">17</ref>: Gender bias metrics used for each baseline, as reported from the original work. A * means that the metrics are directly comparable to those in our main results. DisCo <ref type="bibr" target="#b62">(Webster et al., 2020</ref>) is a template-based likelihood metric; STS-B is a semantic similarity task adapted by <ref type="bibr" target="#b62">Webster et al. (2020)</ref> for measuring gender bias; WinoGender <ref type="bibr" target="#b52">(Rudinger et al., 2018</ref>) is a small-scale coreference resolution dataset that links pronouns and occupations; BEC-Pro <ref type="bibr" target="#b2">(Bartl et al., 2020</ref>) is a template-based metric that measures the influence of an occupation word on a gender word; ECT <ref type="bibr" target="#b19">(Dev and Phillips, 2019)</ref> applies the Spearman's correlation coefficient to calculate the association between gender words and attribute-neutral words, SIRT <ref type="bibr" target="#b18">(Dev et al., 2021)</ref> uses NLI data to evaluate for gendered information retention.</p><p>MABEL and MABEL without the MLM loss show better overall and gender-specific task accuracy on the probe, in comparison to BERT and other bias mitigation baselines. MABEL has the lowest TPR-GAP and the second lowest TPR-RMS. While INLP has a very low TPR-RMS of 0.069, it also has significantly worse accuracy, whereas MABEL achieves a better fairness-accuracy balance. <ref type="bibr" target="#b17">Dev et al. (2020)</ref> originally formulated this task as a probing experiment. Accordingly, we only update a linear layer on top of a frozen encoder when training on the SNLI dataset. We freeze the entire model when evaluating on the test dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.2 Bias-NLI</head><p>Our results are shown in Table <ref type="table" target="#tab_2">21</ref>.</p><p>Figure <ref type="figure" target="#fig_9">3</ref> shows the validation accuracy of BERT,  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: MABEL consists of three losses: 1) an entailment-based contrastive loss (L CL ) that uses the premises's hypothesis as a positive sample and other in-batch hypotheses as negative samples; 2) an alignment loss (L AL ) that minimizes the similarity difference between each original entailment pair and its gender-balanced counterpart; 3) a masked language modeling loss (L MLM ) to recover p = 15% of the masked tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>4. 1</head><label>1</label><figDesc>Intrinsic MetricsStereoSet<ref type="bibr" target="#b42">(Nadeem et al., 2021)</ref> queries the language model for stereotypical associations. Following<ref type="bibr" target="#b41">Meade et al. (2022)</ref>, we consider intrasentence examples from the gender domain. This task can be formulated as a fill-in-the-blank style problem, wherein the model is presented with an incomplete context sentence, and must choose between a stereotypical word, an anti-stereotypical word, and an irrelevant word. The Language Modeling Score (LM) is the percentage of instances in which the model chooses a valid word (either the stereotype or the anti-stereotype) over the random word; the Stereotype Score (SS) is the percentage in which the model chooses the stereotype over the anti-stereotype. The Idealized Context Association Test (ICAT) score combines the LM and SS scores into a single metric.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: t-SNE plots of sentence representations encoded with BERT, sup. SimCSE, and MABEL. Male-aligned terms (man, male, he, brother, son, father) are in red, female-aligned terms (woman, female, she, her, sister, daughter, mother) are in blue. Neutral terms (e.g., math, art, calculus, poetry, science) are in black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>C. 1</head><label>1</label><figDesc>Intrinsic Metrics StereoSet. StereoSet unifies the language modeling (LM) score and the stereotype score (SS) into a single metric, the Idealized Context Association Test (ICAT) score, which is as follows: ICAT = LM ? min(SS, 100 -SS) 50 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Y, A, B) = x?X s(x, A, B) -y?Y s(y, A, B)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Linear Probing Accuracy during Training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: probing accuracy on Bias-NLI of model checkpoints at various timesteps when training on SNLI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Properties of existing gender debiasing approaches for contextualized representations. Proj. FAIRFIL shows poor LM probing performance in Table2as the debiasing filter is not trained with an MLM head. MABEL fixes this issue by jointly training with an MLM objective. : these works use a single gender pair "he/she" to calculate the gender subspace. :<ref type="bibr" target="#b18">Dev et al. (2021)</ref> fine-tunes on SNLI but does not use it for debiasing.</figDesc><table /><note><p><p><p><p>based: projection-based. Con. obj.: based on contrastive objectives. Gen. aug.: these approaches use a seed list of gender terms for counterfactual data augmentation. LM probe and Fine-tune denote that the approach can be used for language model probing or fine-tuning, respectively. * : INLP was originally only used for task-specific fine-tuning;</p><ref type="bibr" target="#b41">Meade et al. (2022)</ref> </p>later adapted it for task-agnostic training on Wikipedia for LM probing.</p>:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on StereoSet and CrowS-Pairs (standard deviations are in Table14). : the results are reported in<ref type="bibr" target="#b41">Meade et al. (2022)</ref>; ?: the results are reported in<ref type="bibr" target="#b28">Guo et al. (2022)</ref>. : the closer to 50, the better.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">StereoSet</cell><cell cols="2">CrowS-Pairs</cell></row><row><cell>Model</cell><cell cols="2">LM ? SS</cell><cell>ICAT ?</cell><cell>SS</cell><cell></cell></row><row><cell>BERT</cell><cell cols="3">84.17 60.28 66.86</cell><cell cols="2">57.25 ?7.25</cell></row><row><cell cols="4">BERT+DROPOUT 83.04 60.66 65.34</cell><cell cols="2">55.34 ?5.34</cell></row><row><cell>BERT+CDA</cell><cell cols="3">83.08 59.61 67.11</cell><cell cols="2">56.11 ?6.11</cell></row><row><cell>INLP</cell><cell cols="3">80.63 57.25 68.94</cell><cell cols="2">51.15 ?1.15</cell></row><row><cell>SENT-DEBIAS</cell><cell cols="3">84.20 59.37 68.42</cell><cell cols="2">52.29 ?2.29</cell></row><row><cell cols="4">CONTEXT-DEBIAS 85.42 59.35 69.45</cell><cell cols="2">58.01 ?8.01</cell></row><row><cell>AUTO-DEBIAS  ?</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">54.92 ?4.92</cell></row><row><cell>FAIRFIL</cell><cell cols="3">44.85 50.93 44.01</cell><cell cols="2">49.03 ?0.97</cell></row><row><cell>MABEL (ours)</cell><cell cols="3">84.80 56.92 73.07</cell><cell cols="2">50.76 ?0.76</cell></row><row><cell></cell><cell cols="4">Acc. Acc. Acc. TPR</cell><cell>TPR</cell></row><row><cell>Model</cell><cell>(All) ?</cell><cell>(M) ?</cell><cell>(F) ?</cell><cell cols="2">GAP ? RMS ?</cell></row><row><cell>BERT</cell><cell cols="5">84.14 84.69 83.50 1.189 0.144</cell></row><row><cell>INLP  ?</cell><cell>70.50</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.067</cell></row><row><cell>CON  ?</cell><cell>81.69</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.168</cell></row><row><cell>DADV  ?</cell><cell>81.10</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.126</cell></row><row><cell>GATE  ?</cell><cell>80.50</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.111</cell></row><row><cell>R-LACE  ?</cell><cell>85.04</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.115</cell></row><row><cell>SENT-DEBIAS</cell><cell cols="5">83.56 84.10 82.92 1.180 0.144</cell></row><row><cell cols="6">CONTEXT-DEBIAS 83.67 84.08 83.18 0.931 0.137</cell></row><row><cell>FAIRFIL</cell><cell cols="5">83.18 83.52 82.78 0.746 0.142</cell></row><row><cell>MABEL (ours)</cell><cell cols="5">84.85 84.92 84.34 0.599 0.132</cell></row></table><note><p>LM: language modeling score, SS: Steoreotype score, ICAT: combined score, defined as LM?(min(SS, 100-SS))/50.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note><p><p><p><p><p><p><p><p>Results on fine-tuning with the Bias in Bios dataset. : the results are reported in</p>Shen et al. (2021)</p>;</p>: the results are reported in</p>Han et al. (2021a)</p>; : the results are reported in</p><ref type="bibr" target="#b50">Ravfogel et al. (2022)</ref></p>; ?: the approaches depend on gender annotations.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results on Bias-NLI. We fine-tune the models on SNLI and then evaluate on Bias-NLI. : results are reported from original papers; ?: the models are finetuned on MNLI.</figDesc><table><row><cell>Model</cell><cell cols="3">TN ? FN ? T:0.5 ? T:0.7 ?</cell></row><row><cell>BERT</cell><cell cols="2">0.799 0.879 0.874</cell><cell>0.798</cell></row><row><cell>ADELE  ?</cell><cell>0.557 0.504</cell><cell>-</cell><cell>-</cell></row><row><cell>SENT-DEBIAS</cell><cell cols="2">0.793 0.911 0.897</cell><cell>0.788</cell></row><row><cell>CONTEXT-DEBIAS</cell><cell cols="2">0.858 0.906 0.902</cell><cell>0.857</cell></row><row><cell cols="2">CONTEXT-DEBIAS  ? 0.878 0.968</cell><cell>-</cell><cell>0.893</cell></row><row><cell>FAIRFIL</cell><cell cols="2">0.829 0.883 0.846</cell><cell>0.845</cell></row><row><cell>MABEL (ours)</cell><cell cols="2">0.900 0.977 0.974</cell><cell>0.935</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Average F1-scores OntoNotes and WinoBias, and TPR scores across Winobias categories. 1 = Type 1; 2 = Type 2. A=anti-stereotypical; P=pro-stereotypical.</figDesc><table><row><cell>2 ?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Fine-tuning results on the GLUE benchmark. BERT-NLI denotes that we fine-tune pre-trained BERT on NLI data first before fine-tuning on a GLUE task. For the average, we report the Matthew's correlation coefficient for CoLA, the Spearman's rank correlation coefficient for STS-B, and the accuracy for all other tasks.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Table 7, entailment pairs are a critical data</figDesc><table><row><cell></cell><cell>StereoSet</cell><cell>CSP</cell><cell>Bias-NLI</cell></row><row><cell></cell><cell cols="3">LM? SS ICAT? SS NN? FN? TN:0.5?</cell></row><row><cell>DEFAULT</cell><cell cols="3">84.5 56.2 74.0 50.8 0.917 0.983 0.983</cell></row><row><cell cols="4">SNLI ENT. 84.1 58.9 69.1 51.5 0.885 0.973 0.972</cell></row><row><cell cols="4">MNLI ENT. 85.8 55.7 76.1 53.8 0.915 0.927 0.971</cell></row><row><cell cols="4">SNLI NEU. 82.8 58.9 68.3 55.0 0.935 0.945 0.945</cell></row><row><cell cols="4">SNLI CON. 76.9 58.0 64.6 56.5 0.710 0.723 0.722</cell></row><row><cell>QQP</cell><cell cols="3">76.9 57.9 64.6 53.1 0.917 0.938 0.938</cell></row><row><cell cols="4">PARA-NMT 79.3 57.8 67.0 53.4 0.756 0.783 0.782</cell></row><row><cell>DROPOUT</cell><cell cols="3">78.4 57.3 67.0 52.7 0.780 0.809 0.807</cell></row><row><cell cols="4">Table 7: Data ablation results for MABEL. Positive</cell></row><row><cell cols="4">pair constructions include entailment (Ent.), neutral</cell></row><row><cell cols="4">(Neu.) and contradictory (Con.) pairs, paraphrastic</cell></row><row><cell cols="4">examples from QQP and Para-NMT, and general sen-</cell></row><row><cell cols="4">tences from the corpora used by Liang et al. (2020).</cell></row><row><cell cols="4">Default: SNLI+MNLI entailment data. Dropout: the</cell></row><row><cell cols="4">same sentence is passed through the encoder twice with</cell></row><row><cell cols="4">standard dropout. CSP: CrowS-Pairs. : the closer to</cell></row><row><cell cols="2">50, the better.</cell><cell></cell></row><row><cell cols="4">choice for preserving language modeling ability,</cell></row><row><cell cols="4">and result in the highest ICAT scores. Interestingly,</cell></row><row><cell cols="4">the SS is consistent across the board. The MNLI</cell></row><row><cell cols="4">dataset produces the best LM and SS scores, likely</cell></row></table><note><p>as it is a semantically richer dataset with sentences harvested across diverse genres. In contrast, SNLI consists of short, artificial sentences harvested from image captions. The exposure to MNLI's diverse vocabulary set may have helped MABEL learn bet-</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Objective ablation results for MABEL. CSP: CrowS-Pairs. : the closer to 50, the better.</figDesc><table><row><cell></cell><cell cols="2">StereoSet</cell><cell>CSP</cell><cell>Bias-NLI</cell></row><row><cell></cell><cell cols="3">LM? SS ICAT? SS NN? FN? TN:0.5?</cell></row><row><cell cols="4">MABEL 84.6 56.2 74.0 50.8 0.917 0.983 0.982</cell></row><row><cell cols="4">-L MLM 55.8 51.1 54.6 44.3 0.970 0.976 0.976</cell></row><row><cell>-L CL</cell><cell cols="3">84.9 57.2 72.6 54.6 0.858 0.884 0.883</cell></row><row><cell>-L AL</cell><cell cols="3">85.0 57.3 72.6 54.2 0.878 0.890 0.889</cell></row><row><cell cols="4">Batch Size LM ? SS</cell><cell>ICAT ?</cell></row><row><cell></cell><cell>64</cell><cell cols="2">82.43 56.42</cell><cell>71.85</cell></row><row><cell></cell><cell>128</cell><cell cols="2">84.55 56.25</cell><cell>73.98</cell></row><row><cell></cell><cell>256</cell><cell cols="2">84.62 57.46</cell><cell>72.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>StereoSet results on different cumulative batch sizes. : the closer to 50, the better.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Information about dataset sizes.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>StereoSet results on different hyperparameter settings. Unless otherwise stated, the default configuration is a learning rate (LR) of 5e -5 , a batch size of 32, and an ? of 0.05. : the closer to 50, the better.</figDesc><table><row><cell></cell><cell>LM ? SS</cell><cell>ICAT ?</cell></row><row><cell cols="2">LR = 1e -5 85.13 59.71</cell><cell>68.60</cell></row><row><cell cols="2">LR = 3e -5 85.03 58.29</cell><cell>70.92</cell></row><row><cell cols="2">LR = 5e -5 84.54 56.73</cell><cell>73.98</cell></row><row><cell>? = 0.01</cell><cell>85.29 59.67</cell><cell>68.80</cell></row><row><cell>? = 0.05</cell><cell>84.54 56.73</cell><cell>73.98</cell></row><row><cell>? = 0.1</cell><cell>83.34 56.94</cell><cell>72.52</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 14 :</head><label>14</label><figDesc>Table 14, and for extrinsic tasks in Table 15. Standard deviation on intrinsic tasks from StereoSet and CrowS-Pairs (Table</figDesc><table><row><cell></cell><cell cols="2">StereoSet CrowS-Pairs</cell></row><row><cell>Model</cell><cell>LM SS</cell><cell>SS</cell></row><row><cell cols="2">CONTEXT-DEBIAS 0.07 0.24</cell><cell>0.77</cell></row><row><cell>FAIRFIL</cell><cell>1.47 0.71</cell><cell>3.43</cell></row><row><cell>MABEL (ours)</cell><cell>0.39 0.69</cell><cell>0.77</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 15 :</head><label>15</label><figDesc>Standard deviation on extrinsic tasks: Bias-in-Bios (Table3), Bias-NLI(Table 4), and OntoNotes and WinoBias (Table 5).</figDesc><table><row><cell></cell><cell cols="2">Bias-in-Bios</cell><cell>Bias-NLI</cell><cell>WinoBias</cell></row><row><cell>Model</cell><cell cols="3">Acc. Acc. Acc. TPR TPR TN FN T:0.5 T:0.7 ON 1A 1P 2A 2P TPR-1 TPR-2</cell></row><row><cell></cell><cell>(All) (M)</cell><cell>(F) GAP RMS</cell></row><row><cell>BERT</cell><cell cols="3">0.56 0.56 0.58 0.14 0.01 0.03 0.05 0.05 0.05 0.05 1.12 0.57 1.02 1.15 1.17</cell><cell>1.21</cell></row><row><cell>SENT-DEBIAS</cell><cell cols="3">0.09 0.10 0.20 0.25 0.00 0.05 0.04 0.05 0.09 0.15 2.88 0.27 1.47 0.29 3.02</cell><cell>1.53</cell></row><row><cell cols="4">CONTEXT-DEBIAS 0.36 0.32 0.42 0.17 0.01 0.03 0.05 0.05 0.04 0.19 1.00 1.25 1.13 1.33 0.75</cell><cell>0.74</cell></row><row><cell>FAIRFIL</cell><cell cols="3">0.07 0.25 0.16 0.41 0.00 0.05 0.07 0.01 0.05 0.64 0.41 0.47 2.31 1.49 0.16</cell><cell>0.84</cell></row><row><cell>MABEL (ours)</cell><cell cols="3">0.40 0.51 0.47 0.04 0.00 0.02 0.01 0.01 0.03 0.37 1.12 1.11 0.30 0.41 1.88</cell><cell>0.44</cell></row><row><cell cols="3">regime does not involve fine-tuning, we notice less</cell></row><row><cell cols="3">uniformity across the baselines' results. While MA-</cell></row><row><cell cols="3">BEL and MABEL w/o MLM have a higher average</cell></row><row><cell cols="3">performance than BERT, the only task with an ob-</cell></row><row><cell cols="3">vious gain is in MRPC (68.87% and 71.48% vs.</cell></row><row><cell cols="3">65.57%). This makes sense as MRPC is a semantic</cell></row><row><cell cols="3">similarity task, which leveraging supervised sig-</cell></row><row><cell cols="3">nals from NLI entailment pairs happens to benefit</cell></row><row><cell cols="2">(Gao et al., 2021).</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 16 :</head><label>16</label><figDesc>Table 20, both Model MR ? CR ? SUBJ ? MPQA ? SST-2 ? TREC ? MRPC ? Avg. ? Downstream transfer task results for BERT, MABEL models, and bias baselines from the SentEval benchmark<ref type="bibr" target="#b13">(Conneau et al., 2017)</ref>. R-LACE(Ravfogel et al., 2022)   * CON(Shen et al., 2021)   * DADV(Han et al., 2021b)   </figDesc><table><row><cell>BERT</cell><cell>80.99 85.67</cell><cell>95.31</cell><cell>87.40</cell><cell>86.99</cell><cell>84.20</cell><cell>65.57</cell><cell>83.73</cell></row><row><cell>CONTEXT-DEBIAS</cell><cell>78.37 85.22</cell><cell>94.11</cell><cell>85.99</cell><cell>84.73</cell><cell>85.60</cell><cell>66.55</cell><cell>82.94</cell></row><row><cell>SENT-DEBIAS</cell><cell>69.85 68.96</cell><cell>89.12</cell><cell>80.78</cell><cell>81.44</cell><cell>60.00</cell><cell>70.14</cell><cell>74.33</cell></row><row><cell>FAIRFIL</cell><cell>76.94 80.34</cell><cell>92.82</cell><cell>83.54</cell><cell>81.88</cell><cell>79.20</cell><cell>69.28</cell><cell>80.57</cell></row><row><cell>MABEL</cell><cell>78.33 85.83</cell><cell>93.78</cell><cell>89.13</cell><cell>85.50</cell><cell>85.20</cell><cell>68.87</cell><cell>83.81</cell></row><row><cell cols="2">MABEL W/O MLM 80.01 86.41</cell><cell>94.50</cell><cell>89.29</cell><cell>85.45</cell><cell>84.80</cell><cell>71.48</cell><cell>84.56</cell></row><row><cell></cell><cell cols="5">WEAT/ CrowS-Stereo-Bias-Bias-Wino-</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">SEAT Pairs</cell><cell cols="4">Set in-Bios NLI Bias Other int.</cell><cell>Other ext.</cell></row><row><cell>Task-specific approaches</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">INLP (Ravfogel et al., 2020)  Task-agnostic approaches</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CDA (Webster et al., 2020)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DisCo</cell><cell></cell><cell>STS-B, WinoGender</cell></row><row><cell>DROPOUT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>* * GATE (Han et al., 2021a) *</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>As Table17in Appendix F indicates, many previous bias mitigation approaches limit evaluation to 1 or 2 metrics.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>For comprehensiveness, we report MABEL's results on SEAT in Appendix G.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>We use the same list of attribute word pairs from Bolukbasi et al. (2016), Liang et al. (2020), and Cheng et al. (2021), which can be found in Appendix A.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>  5  In this work, we only refer to the supervised SimCSE model, which leverages entailment pairs from NLI data.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>We also explore different loss functions for alignment and report them in Appendix J.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>We also evaluate transfer performance on the SentEval tasks<ref type="bibr" target="#b13">(Conneau et al., 2017)</ref> in Appendix E.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>https://github.com/pliang279/sent_debias</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>https://github.com/kanekomasahiro/ context-debias</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8"><p>As there is no code released, we use our own implementation without an auxiliary regularization term.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9"><p>11 Standard deviations can be found in Appendix D.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank <rs type="person">Tianyu Gao</rs>, <rs type="person">Sadhika Malladi</rs>, <rs type="person">Howard Yen</rs>, and the other members of the Princeton NLP group for their helpful discussions and support. We are also grateful to the anonymous reviewers for their valuable feedback. This research is partially supported by the <rs type="programName">Peter and Rosalind Friedland Endowed Senior Thesis Fund</rs> from the <rs type="funder">Princeton School of Engineering &amp; Applied Sciences</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_bNwwHUv">
					<orgName type="program" subtype="full">Peter and Rosalind Friedland Endowed Senior Thesis Fund</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>MABEL models, and supervised SimCSE models on the NLI-Bias evaluation set at various timesteps when training on the SNLI dataset. BERT consistently struggles with the task-its accuracy starts off high before degrading noticeably. MABEL outperforms SimCSE both with and without the MLM loss, which shows that its performance on Bias-NLI is not entirely due to enhanced semantic understanding, but greater fairness as well. The MLM objective steadily drops the performance of both MABEL and SimCSE, which shows that it harms sentence-level knowledge retention. Interestingly, the opposite trend holds true in the finetuning setting-including the MLM loss leads to better NLI performance across all three metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Alignment Objectives</head><p>Beside our default alignment loss (Alignment Loss 1), we experiment with other losses that maximize the similarity between original and genderaugmented representations. We describe them and report their results across Bias-in-Bios and Bias-NLI tasks. and other sentences form in-batch negatives. Let x i be any original premise or hypothesis representation, and x be the augmented counterpart of x:</p><p>2n j=1 e sim(x i ,x j )/? .</p><p>Alignment Loss 3 tries to maximize the cosine similarities between original and augmented sentences. Given the pairs (p i , p i ) and (h i , h i ):</p><p>The results for MABEL trained with each alignment loss are in </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How reliable are model diagnostics?</title>
		<author>
			<persName><forename type="first">Vamsi</forename><surname>Aribandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.155</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1778" to="1785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The problem with bias: from allocative to representational harms in machine learning</title>
		<author>
			<persName><forename type="first">Solon</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCIS Conference</title>
		<meeting><address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unmasking contextual stereotypes: Measuring and mitigating BERT&apos;s gender bias</title>
		<author>
			<persName><forename type="first">Marion</forename><surname>Bartl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malvina</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Gender Bias in Natural Language Processing</title>
		<meeting>the Second Workshop on Gender Bias in Natural Language Processing<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The fifth pascal recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>In TAC</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stereotyping Norwegian salmon: An inventory of pitfalls in fairness benchmark datasets</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilsinia</forename><surname>Blodgett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Olteanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><surname>Wallach</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.81</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1004" to="1015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Man is to computer programmer as woman is to homemaker? debiasing word embeddings</title>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Tauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalai</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05">2016. 2016. December 5-10, 2016</date>
			<biblScope unit="page" from="4349" to="4357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1075</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantics derived automatically from language corpora contain human-like biases</title>
		<author>
			<persName><forename type="first">Aylin</forename><surname>Caliskan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><forename type="middle">J</forename><surname>Bryson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">356</biblScope>
			<biblScope unit="issue">6334</biblScope>
			<biblScope unit="page" from="183" to="186" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I?igo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S17-2001</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note>SemEval-2017</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
		<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<date type="published" when="2020-07-18">2020. 18 July 2020</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fairfil: Contrastive neural debiasing method for pretrained text encoders</title>
		<author>
			<persName><forename type="first">Pengyu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weituo</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijing</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2021-05-03">2021. May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Conditional supervised contrastive learning for fair text classification</title>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Shand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaodong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SentEval: An evaluation toolkit for universal sentence representations</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA)</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lo?c</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1070</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment</title>
		<meeting>the PASCAL Challenges Workshop on Recognising Textual Entailment</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bias in bios: A case study of semantic representation bias in a high-stakes setting</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>De-Arteaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Chayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Borgs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Chouldechova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahin</forename><surname>Geyik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnaram</forename><surname>Kenthapadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Tauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalai</forename></persName>
		</author>
		<idno type="DOI">10.1145/3287560.3287572</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* &apos;19</title>
		<meeting>the Conference on Fairness, Accountability, and Transparency, FAT* &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bias in bios: A case study of semantic representation bias in a high-stakes setting</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>De-Arteaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Chayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Borgs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Chouldechova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahin</forename><surname>Geyik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnaram</forename><surname>Kenthapadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Tauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalai</forename></persName>
		</author>
		<idno type="DOI">10.1145/3287560.3287572</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* &apos;19</title>
		<meeting>the Conference on Fairness, Accountability, and Transparency, FAT* &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On measuring and mitigating biased inferences of word embeddings</title>
		<author>
			<persName><forename type="first">Sunipa</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><forename type="middle">M</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><surname>Vivek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07">Srikumar. 2020. February 7-12, 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="7659" to="7666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">OSCaR: Orthogonal subspace correction and rectification of biases in word embeddings</title>
		<author>
			<persName><forename type="first">Sunipa</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><forename type="middle">M</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><surname>Vivek</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.411</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">Srikumar. 2021</date>
			<biblScope unit="page" from="5034" to="5050" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attenuating bias in word vectors</title>
		<author>
			<persName><forename type="first">Sunipa</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><forename type="middle">M</forename><surname>Phillips</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<meeting><address><addrLine>Naha, Okinawa, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-04-18">2019. 2019, 16-18 April 2019</date>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="879" to="887" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Paraphrasing</title>
		<meeting>the Third International Workshop on Paraphrasing</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Understanding undesirable word embedding associations</title>
		<author>
			<persName><forename type="first">Kawin</forename><surname>Ethayarajh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1166</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1696" to="1705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SimCSE: Simple contrastive learning of sentence embeddings</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.552</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6894" to="6910" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The fourth pascal recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoa</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Trang</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Cabrio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<editor>TAC. Citeseer</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The third PASCAL recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</title>
		<meeting>the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</meeting>
		<imprint>
			<publisher>Prague. Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Intrinsic bias metrics do not correlate with application bias</title>
		<author>
			<persName><forename type="first">Seraphina</forename><surname>Goldfarb-Tarrant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Marchant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><forename type="middle">Mu?oz</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mugdha</forename><surname>Pandya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.150</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1926" to="1940" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aylin</forename><surname>Caliskan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3461702.3462536</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="122" to="133" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Autodebias: Debiasing masked language models with automated biased prompts</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Abbasi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.72</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1012" to="1023" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The second pascal recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Bar Haim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Idan</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><surname>Szpektor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second PAS-CAL Challenges Workshop on Recognising Textual Entailment</title>
		<meeting>the Second PAS-CAL Challenges Workshop on Recognising Textual Entailment</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">2021a. Balancing out bias: Achieving fairness through training reweighting</title>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<idno>abs/2109.08253</idno>
		<imprint/>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">2021b. Diverse adversaries for mitigating bias in training</title>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.239</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="2760" to="2765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">OntoNotes: The 90% solution</title>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers</title>
		<meeting>the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers<address><addrLine>New York City</addrLine></address></meeting>
		<imprint>
			<publisher>USA. Association for Computational Linguistics</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="57" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On transferability of bias mitigation effects in language model fine-tuning</title>
		<author>
			<persName><forename type="first">Xisen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aida</forename><forename type="middle">Mostafazadeh</forename><surname>Davani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.296</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3770" to="3783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Debiasing pre-trained contextualised embeddings</title>
		<author>
			<persName><forename type="first">Masahiro</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.107</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Debiasing isn&apos;t enough! -on the effectiveness of debiasing MLMs and their social biases in downstream tasks</title>
		<author>
			<persName><forename type="first">Masahiro</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Computational Linguistics</title>
		<meeting>the 29th International Conference on Computational Linguistics<address><addrLine>Gyeongju, Re</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1299" to="1310" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Measuring bias in contextualized word representations</title>
		<author>
			<persName><forename type="first">Keita</forename><surname>Kurita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nidhi</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Pareek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-3823</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Gender Bias in Natural Language Processing</title>
		<meeting>the First Workshop on Gender Bias in Natural Language Processing<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="166" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sustainable modular debiasing of language models</title>
		<author>
			<persName><forename type="first">Anne</forename><surname>Lauscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Lueken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Glava?</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.411</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4782" to="4797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards debiasing sentence representations</title>
		<author>
			<persName><forename type="first">Paul Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><forename type="middle">Mengze</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><surname>Morency</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.488</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5502" to="5515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">RoBERTa: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>ArXiv preprint, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On measuring social biases in sentence encoders</title>
		<author>
			<persName><forename type="first">Chandler</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikha</forename><surname>Bordia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><surname>Rudinger</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1063</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="622" to="628" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An empirical survey of the effectiveness of debiasing techniques for pre-trained language models</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Meade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elinor</forename><surname>Poole-Dayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.132</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1878" to="1898" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">StereoSet: Measuring stereotypical bias in pretrained language models</title>
		<author>
			<persName><forename type="first">Moin</forename><surname>Nadeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Bethke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.416</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5356" to="5371" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">CrowS-pairs: A challenge dataset for measuring social biases in masked language models</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Vania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasika</forename><surname>Bhalerao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.154</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1953" to="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">How gender debiasing affects internal model representations, and why it matters</title>
		<author>
			<persName><forename type="first">Hadas</forename><surname>Orgad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seraphina</forename><surname>Goldfarb-Tarrant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.188</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Seattle, United States. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2602" to="2628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. 2019. December 8-14, 2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Candace</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jude</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<title level="m">Perturbation augmentation for fairer nlp</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Null it out: Guarding protected attributes by iterative nullspace projection</title>
		<author>
			<persName><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hila</forename><surname>Gonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Twiton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.647</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7237" to="7256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Linear adversarial concept erasure</title>
		<author>
			<persName><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Twiton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence embeddings using Siamese BERTnetworks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1410</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3982" to="3992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Gender bias in coreference resolution</title>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="8" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Timothy Baldwin, and Lea Frermann. 2021. Contrastive learning for fair representations</title>
		<author>
			<persName><forename type="first">Aili</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Towards a comprehensive understanding and accurate evaluation of societal biases in pre-trained transformers</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradyumna</forename><surname>Tambwekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Gombolay</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.189</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2383" to="2389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Evaluating debiasing techniques for intersectional biases</title>
		<author>
			<persName><forename type="first">Shivashankar</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lea</forename><surname>Frermann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.193</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2492" to="2498" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">86</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. 2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
		<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<date type="published" when="2020-07-18">2020. 18 July 2020</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="9929" to="9939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Bilateral multi-perspective matching for natural language sentences</title>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/579</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-19">2017. 2017. August 19-25, 2017</date>
			<biblScope unit="page" from="4144" to="4150" />
		</imprint>
	</monogr>
	<note>ijcai.org</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Neural network acceptability judgments</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00290</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="625" to="641" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Measuring and reducing gendered correlations in pre</title>
		<author>
			<persName><forename type="first">Kellie</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>trained models</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">ParaNMT-50M: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations</title>
		<author>
			<persName><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1042</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Revealing the myth of higher-order inference in coreference resolution</title>
		<author>
			<persName><forename type="first">Liyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinho</forename><forename type="middle">D</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.686</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8527" to="8533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Gender bias in contextualized word embeddings</title>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1064</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="629" to="634" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Gender bias in coreference resolution: Evaluation and debiasing methods</title>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2003</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
		<title level="s">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="15" to="20" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
