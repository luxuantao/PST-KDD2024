<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Entity Matching: How Similar Is Similar</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiannan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guoliang</forename><surname>Li</surname></persName>
							<email>liguoliang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">#</forename><surname>Jeffrey</surname></persName>
							<email>jefflee@abc.com</email>
						</author>
						<author>
							<persName><forename type="first">Xu</forename><surname>Yu</surname></persName>
							<email>yu@se.cuhk.edu.hk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Systems Engineering and Engineering Management</orgName>
								<orgName type="institution">Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianhua</forename><surname>Feng</surname></persName>
							<email>fengjh@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jeffrey</forename><surname>Yi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jeffery</forename><surname>Yi</surname></persName>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">CUHK</orgName>
								<address>
									<addrLine>852 Jeff Room 567B, HongKong Male r2 Jeff Lee</addrLine>
									<postCode>852-333333</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">852 Jeff Computer Science Department</orgName>
								<address>
									<country>CUHK F</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Entity Matching: How Similar Is Similar</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0DC93C5E1D08D174397B75D5A8C2C3F8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>{rp1</term>
					<term>6</term>
					<term>rp1</term>
					<term>7</term>
					<term>rp2</term>
					<term>5</term>
					<term>rp3</term>
					<term>4</term>
					<term>rp6</term>
					<term>7} Negative-example set D : {rp1</term>
					<term>2</term>
					<term>rp1</term>
					<term>3</term>
					<term>rp1</term>
					<term>4</term>
					<term>rp1</term>
					<term>5</term>
					<term>rp2</term>
					<term>3</term>
					<term>rp2</term>
					<term>4</term>
					<term>rp2</term>
					<term>6</term>
					<term>rp2</term>
					<term>7</term>
					<term>rp3</term>
					<term>5</term>
					<term>rp3</term>
					<term>6</term>
					<term>rp3</term>
					<term>7</term>
					<term>rp4</term>
					<term>5</term>
					<term>rp4</term>
					<term>6</term>
					<term>rp4</term>
					<term>7</term>
					<term>rp5</term>
					<term>6</term>
					<term>rp5</term>
					<term>7}</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Entity matching that finds records referring to the same entity is an important operation in data cleaning and integration. Existing studies usually use a given similarity function to quantify the similarity of records, and focus on devising index structures and algorithms for efficient entity matching. However it is a big challenge to define "how similar is similar" for real applications, since it is rather hard to automatically select appropriate similarity functions. In this paper we attempt to address this problem. As there are a large number of similarity functions, and even worse thresholds may have infinite values, it is rather expensive to find appropriate similarity functions and thresholds. Fortunately, we have an observation that different similarity functions and thresholds have redundancy, and we have an opportunity to prune inappropriate similarity functions. To this end, we propose effective optimization techniques to eliminate such redundancy, and devise efficient algorithms to find the best similarity functions. The experimental results on both real and synthetic datasets show that our method achieves high accuracy and outperforms the baseline algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In the fields of data cleaning and integration, entity matching that identifies records referring to the same entity is an important operation. This problem is also known as duplicate detection, record linkage and merge/purge, and it has been extensively studied by different communities such as statistics, databases, and artificial intelligence (see <ref type="bibr" target="#b8">[10]</ref> for a recent survey). For example, Figure <ref type="figure" target="#fig_0">1</ref> gives a set of records integrated from multiple sources. As there may have inconsistences in the data, we want to find the records that refer to the same person from the data.</p><p>In statistics and artificial intelligence, existing studies formulate this operation as a classification problem <ref type="bibr" target="#b10">[12]</ref>. They enumerate all record pairs, represent each record pair as a feature vector, and classify these pairs as matching or nonmatching. While these methods typically have good accu-racy, they do not scale well for a large amount of data, since they need to classify n 2 feature vectors if there are n records.</p><p>In the database community, existing methods usually employ a rule-based method to find entities <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b16">18]</ref>. They define a set of record-matching rules to accommodate different representations of the same entity. Consider a record-matching rule "if two records have similar name and the same tel, they refer to the same entity" in Figure <ref type="figure" target="#fig_0">1</ref>. For instance, records r1 and r7 have similar names and the same telephone numbers, they will be considered as the same entity. Then they utilize these rules to find entities.</p><p>The rule-based method has the following problems. Firstly, it is not straightforward to generate record-matching rules, and Fan et al. <ref type="bibr" target="#b9">[11]</ref> proposed to use schema information to effectively generate record-matching rules, which may contain exact-match conditions (e.g., tels are the same) and approximate-match conditions (e.g., names are similar). Secondly although it can be efficient to support exact-match conditions, it is not straightforward to support approximatematch conditions. To address this problem, existing methods <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b5">7]</ref> focus on devising effective index structures and algorithms to efficiently support the approximate-match conditions. Thirdly, it is a big challenge to define "how similar is similar" for the approximate-match conditions. Traditional methods <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b12">14]</ref> usually suppose a similarity function (e.g., Edit Similarity and Jaccard) is given to decide whether two values are similar. However this method cannot be adapted to different applications and it calls for a new method to automatically select appropriate similarity functions in recordmatching rules.</p><p>In this paper we address the third problem. Given a table with a set of records and a set of record-matching rules with unknown similarity functions and thresholds, we want to identify the best similarity functions and thresholds for effectively finding entities from the table. Note that it is rather hard to find the best similarity functions by only using record-matching rules <ref type="bibr" target="#b4">[6]</ref>, since there could be larger numbers of similarity functions and thresholds. To address this problem, we suppose that users can provide a set of positive examples (e.g., which records are known to be the same entity) and negative examples (e.g., which records are known to be not the same entity). We utilize these examples to identify the best similarity functions.</p><p>This problem has the following challenges. Firstly there are a large number of similarity functions, and even worse thresholds may have infinite values. It is rather expensive to find appropriate similarity functions and thresholds. Secondly, users may have different preferences (e.g., preferring to a high recall or a high precision), and it is hard to devise an adaptive algorithm for different preferences. Thirdly, as there are large numbers of record-matching rules and examples, and different record-matching rules are not independent, it is a big challenge to fully utilize these rules and examples. Fortunately, we have an observation that different similarity functions and thresholds have redundancy, and we have an opportunity to prune inappropriate similarity functions. To this end, we propose effective optimization techniques to eliminate such redundancy, and devise efficient algorithms to find the best similarity functions and thresholds. To summarize, we make the following contributions.</p><p>(1) We formalize the problem of similarity-function identification in record-matching rules for entity matching. (2) We observe that different similarity functions and thresholds have redundancy, and devise efficient techniques to eliminate the redundancy. <ref type="bibr" target="#b1">(3)</ref> We devise efficient algorithms to select the best similarity functions and thresholds. <ref type="bibr" target="#b2">(4)</ref> We have conducted an extensive experiment study on both real and synthetic data sets. The experimental result shows that our algorithm achieves high accuracy and significantly outperforms the baseline algorithm.</p><p>Related Work. Entity Matching has been extensively studied in different communities <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b20">22]</ref>. In the artificial intelligence community, machinelearning based techniques <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b19">21]</ref> usually learned a classifier (e.g., decision tree or SVM) from a given example set, and used the classifier to classify each record pair as matching or non-matching. However, these methods are rather expensive since they need to enumerate n 2 pairs. Although some heuristic techniques (e.g., blocking <ref type="bibr" target="#b2">[4]</ref> and canopy clustering <ref type="bibr" target="#b17">[19]</ref>) have been applied to filter "non-matching" pairs, they are at expense of decreasing the accuracy. In addition, these methods (e.g., SVM) usually employ black-box-based techniques and the results are not explainable.</p><p>As an alternative, the rule-based method <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b16">18]</ref> proposed from the database community has the advantages that the algorithm scales well and the obtained results are explainable. Typically, the rules can be specified based on domain knowledge. Recently, Fan et al. <ref type="bibr" target="#b9">[11]</ref> proposed an effective algorithm for deducing record-matching rules from a small set of matching dependencies. As early mentioned, to accommodate errors in data, the rules may contain approximate-match conditions. However these methods neglect the problem of selecting optimal similarity functions and thresholds. It is challenging to select good similarity functions and thresholds, and inappropriate selections will lead to rather worse results (see Section 6.1). To address this problem, Chaudhuri et al. <ref type="bibr" target="#b4">[6]</ref> proposed an operator tree for selecting similarity functions and thresholds. They modeled record-matching rules as a tree structure with the root node as a union operator, intermediate </p><formula xml:id="formula_0">(b) RRs φ1 λ i 1 ∧ λ e 2 φ2 λ e 3 ∧ λ i 4 φ3 λ i 1 ∧ λ i 4 ∧ λ e 5</formula><p>Figure <ref type="figure">2</ref>: Attribute-matching rules and recordmatching rules ("f=" denotes exact matching).</p><p>join operators and leaf nodes as relations. They identified the best operator tree based on a given set of positive and negative examples. However the method is rather inefficient for large numbers of similarity functions, since it does not consider the redundancy among similarity functions. There were also many studies on similarity functions <ref type="bibr" target="#b15">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROBLEM FORMULATION</head><p>Let R[a1, a2, • • • , an] be a relation with a set of attributes ai (i ∈ [1, n]). Let r be a record in R and r[a] be the value of the attribute a in record r. Given two records r and r ′ , a similarity function f (r[a], r ′ [a]) computes a similarity score in [0, 1]. A larger score indicates r[a] and r ′ [a] have a higher similarity. As an example, we consider the following three similarity functions. (1) Edit Similarity fe. The edit similarity between two strings s1 and s2 is defined as 1 -</p><formula xml:id="formula_1">ed(s 1 ,s 2 ) max(|s 1 |,|s 2 |)</formula><p>, where ed(s1, s2) is the edit distance between s1 and s2, and |s1|(|s2|) is the length of string s1(s2). For example, given s1 = "Jeffrey Yi" and s2 = "Jeffery Yi". fe(s1, s2) = 1 -2 10 = 0.8. (2) Jaccard Similarity fj . Given two strings s1 and s2, we first tokenize them into two token sets token(s1) and token(s2). Their jaccard similarity is |token(s 1 )∩token(s 2 )| |token(s 1 )∪token(s 2 )| . For example, token(s1)={Jeffrey,Yi } and token(s2)={Jeffery, Yi }. fj(s1, s2) = 1 3 . (3) Grambased Similarity fg. Given two strings s1 and s2, we first generate their gram sets gram(s1) and gram(s2), where a q-gram is a substring with length q. For example, gram(s1) = {"Je", "ef", "ff", "fr", "re", "ey", "y ", " Y", "Yi"} (q = 2 in all running examples). The gram similarity is defined as</p><formula xml:id="formula_2">|gram(s 1 )∩gram(s 2 )| |gram(s 1 )∪gram(s 2 )| .</formula><p>For example, fg(s1, s2)= 6  12 . In order to decide whether two records r and r ′ in R refer to the same entity, we employ a matching-rule-based scheme to define their similarity. We below introduce a concept, "explicit attribute-matching rule." Definition 1. An explicit attribute-matching rule (eAR) is a triple λ e (a, f, θ), where a is an attribute name, f is a similarity function, and θ is a threshold. r[a] and r ′ [a] are considered to be the same, if</p><formula xml:id="formula_3">f (r[a], r ′ [a]) ≥ θ. We use (r[a], r ′ [a]) λ e to denote that r[a] and r ′ [a] satisfy λ e , (r[a], r ′ [a]) λ e to denote that r[a] and r ′ [a] dissatisfy λ e .</formula><p>The explicit attribute-matching rule includes an explicit similarity function and a threshold. We can use them to eas-ily deduce whether two attribute values refer to the same entity. However in most cases users do not know how to define the similarly functions and how to determine the appropriate thresholds. Users may want to provide multiple similarity functions and a threshold range in the attribute-matching rule, and the system can find the best similarity function and a threshold in the range. We call such attribute-matching rules "implicit attribute-matching rules," defined as follows.</p><p>Definition 2. An implicit attribute-matching rule (iAR) is a triple λ i (a, F, Θ), where a is an attribute name, F = {f1, f2, • • • } is a set of similarity functions, and Θ is a range. r[a] and r ′ [a] are considered to be the same if there exists a similarity function f ∈ F and f (r[a], r ′ [a]) ≥ θ where θ is the lower bound of Θ.</p><p>Given an iAR (a, F, Θ) and an eAR (a, f, θ), the eAR is an instance of the iAR (or iAR is a generalization of the eAR) if f ∈ F and θ ∈ Θ. We can generate many instances of the iAR by enumerating the similarity functions in F and selecting a threshold in the threshold range.</p><p>For example, Figure <ref type="figure">2</ref>(a) gives 2 iARs and 3 eARs. λ i 1 and λ i 4 are iARs. λ e 2 , λ e 3 , and λ e 5 are eARs. λ e : (name, fe, 0.8) is an instance of λ i 1 . We use "f=" to denote exact matching, that is r</p><formula xml:id="formula_4">[a] are similar to r ′ [a] if and only if r[a] = r ′ [a].</formula><p>Based on these two concepts, we can define the concept of "record-matching rule (RR)".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3. A record-matching rule (RR) is a set of (explicit or implicit) attribute-matching rules, denoted by</head><formula xml:id="formula_5">φ = k i=1 λi. A record pair (r, r ′ ) satisfies the record- matching rule, denoted by (r, r ′ ) φ, if (r[a], r ′ [a]</formula><p>) satisfies every attribute-matching rule λi for 1 ≤ i ≤ k. We also use (r, r ′ ) φ to denote that (r, r ′ ) dissatisfies φ. 1 ∧λ e 2 which indicates that two records refer to the same entity if two records have the similar names (where users want to select a similarity function from fe and fg, and the threshold can be any value in [0, 1]) and the same telephone number. To help readers better understand, we summarizes the notations used in this paper in Appendix A.</p><p>Traditional methods usually use explicit record-matching rules to quantify similarity of records. In this paper, we employ implicit record-matching rules to quantify the similarity and study how to select the best instances (eRRs) from a given set of iRRs. Note that iRRs can be gotten using existing methods, such as the schema-based methods <ref type="bibr" target="#b9">[11]</ref>.</p><p>Next we discuss how to evaluate the quality of an instance eRR. To address this problem, we suppose that users can give a set of examples, denoted by E, including positive examples, e.g., which records are known to be the same entity, denoted by M , and negative examples, e.g., which records are known to be not the same entity, denoted by D. Obviously M ∪ D = E and M ∩ D = ∅.</p><p>Consider a set of eRRs Ψ. Given an eRR, ψ ∈ Ψ, let M ψ = {(r, r ′ )|(r, r ′ ) ∈ E, (r, r ′ ) ψ} be the set of record pairs generated by ψ (record pairs that satisfy ψ). Let M Ψ = ψ∈Ψ M ψ be the set of record pairs generated by Ψ. Ideally, we hope that M Ψ is exactly equal to M . However, in reality M Ψ may contain negative record pairs. To evaluate the quality of Ψ, in this paper we focus on a general case of objective functions ̥(Ψ, M, D): the larger |M Ψ ∩ M |, the larger ̥(Ψ, M, D); the smaller |M Ψ ∩ D|, the larger ̥(Ψ, M, D). Many functions belong to this general class. For example, the well-know F-measure function in information retrieval is a general objective function 2   3 . This value may not be the maximum one. In this paper we study how to efficiently find the best Ψ to maximize the value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">FROM INFINITE THRESHOLDS TO FI-NITE THRESHOLDS</head><p>A naive method to address SiFi problem is to enumerate all possible eRRs and select the best eRRs to maximize the objective function. However, as an iAR may contain a threshold range, there are infinite values. For example in Figure <ref type="figure">2</ref>, we can get many eRRs from λ i 1 ∧ λ e 2 as there are infinite values in the threshold range of λ i 1 (Θ = [0, 1]). As we are only interested in those eRRs which can maximize the objective function, we want to generate finite eRRs which can also maximize the objective function.</p><p>Consider λ i (a, F, Θ) and its two instances, λ e 1 (a, f, θ1) and λ e 2 (a, f, θ2). Without loss of generality, suppose θ1 &lt; θ2. Obviously the set of examples that satisfy λ e 2 is a subset of that of λ e 1 , that is, M λ e 2 ⊆ M λ e 1 (where M λ e denotes the record pairs that satisfy λ e ). If there is no positive example in M λ e 1 -M λ e 2 , for any iRRs that contain λ i , we will not use the eRRs with λ e 1 since they cannot get a better objective value than those with λ e 2 . The following Theorem shows the correctness of this idea (all the proofs are in Appendix B).  </p><formula xml:id="formula_6">in M λ e 1 -M λ e 2 , we have ̥(Ψ1, M, D) ≤ ̥(Ψ2, M, D).</formula><p>Based on Theorem 1, we have an observation that a large number of eRRs can be pruned since they cannot provide a better objective value. For example, consider an iAR λ i 1 : (name, {fe, fg}, [0, 1]) in Figure <ref type="figure">2</ref>. It has two similarity functions, edit similarity fe and gram-based similarity fg. Table <ref type="table" target="#tab_2">1</ref> </p><formula xml:id="formula_7">shows fe(r[name], r ′ [name]) and fg(r[name], r ′ [name]) of each record pair (r, r ′ ) in E.</formula><p>The positive examples are marked by the gray background color (e.g. rp1,6). Consider two eARs λ e 1 : (a, fe, 0.6) and λ e 2 : (a, fe, 0.7), M λ e 1 = {rp1,3, rp1,5, rp1,6, rp1,7, rp2,5, rp3,5, rp3,6, rp3,7, rp5,7, rp6,7}, M λ e 2 = {rp1,3, rp1,5, rp1,6, rp1,7, rp2,5, rp3,7, rp6,7}, M λ e 1 -M λ e 2 ={rp3,5, rp3,6, rp5,7}. As there is no positive example in M λ e 1 -M λ e 2 , we can prune λ e 1 . Using this idea, we can generate a finite set of eARs, and take them as a candidate eAR set, which can maximize the object function. Given an iAR λ i : (a, F, Θ), we construct a finite candidate eAR set of λ i , denoted by P(λ i ), as follows. For each similarity function f ∈ F, we add λ e : (a, f, θmax) into P(λ i ) where θmax is the upper bound of Θ (as it cannot be pruned due to it is the maximal value); for each positive example (r, r ′ ), we compute θ = f (r[a], r ′ [a]), if θ ∈ Θ, we add λ e : (a, f, θ) into P(λ i ). For all other thresholds we can prune them as proved in Corollary 1.</p><p>Corollary 1. Given an iAR λ i : (a, F, Θ) and a set of positive examples M , we can use the candidate eAR set P(λ i ) to generate the best eRR set.</p><p>Example 1. Consider the relation in Figure <ref type="figure" target="#fig_0">1</ref>. We show how the algorithm computes P(λ i 1 ) for λ i 1 : (name, {fe, fg}, [0, 1]). Firstly, for fe and fg we add two eRRs with the threshold θmax = 1 into P(λ i 1 ) = {(name, fe, 1), (name, fg, 1)}. Then we enumerate each record pair in M : {rp1,6, rp1,7, rp2,5, rp3,4, rp6,7}. For the first record pair rp1,6, we compute the similarity fe("Jeffrey Yi", "Jeffery Yi") = 0.8 and fg("Jeffrey Yi", "Jeffery Yi") = 0.5 as shown in Table <ref type="table" target="#tab_2">1</ref>, and then add (name, fe, 0.8) and (name, fg, 0.5) into P(λ i 1 ). We repeat these steps for the rest of record pairs in M . Finally, we get P(λ i 1 ) with 12 eARs as shown in Table <ref type="table" target="#tab_3">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ELIMINATING REDUNDANCY</head><p>The size of candidate eAR set generated in Section 3 may be quite large. In this section, we show that a large number of eARs, called redundant eARs, can not lead to an optimal objective-function value. In other words, we can eliminate such redundancy to reduce the size of candidate eAR set. Given a candidate eAR set P(λ i ), we first divide it into |F| groups, where |F| is the size of λ i 's similarity function set F. Let G f denote a group of candidate eARs in P(λ i ) whose similarity function is f . For simplicity, in one group we say λ e 1 &gt; λ e 2 iff λ e 1 has a larger threshold than λ e 2 . λ e is maximal (minimal) means it has the maximal (minimal) threshold. For example, in Table <ref type="table" target="#tab_3">2</ref> λ i 1 has two similarity functions.</p><formula xml:id="formula_8">P(λ i 1 ) is divided into two groups G fe = {λ e 1 , λ e 2 , • • • , λ e 6 } and G fg = {λ e 7 , λ e 8 , • • • , λ e 12 }.</formula><p>We say λ e 3 &gt; λ e 4 since the threshold of λ e 3 (0.8) is larger than that of λ e 4 (0.73). Note that we cannot say λ e 3 &gt; λ e 9 since they are not in the same group. We observe that in each group there may exist threshold redundancy (Section 4.1), and between two different groups there may exist similarity-function redundancy (Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Threshold Redundancy</head><p>Given an iAR λ i : (a, F, Θ), there may be still many eARs of λ i in P(λ i ), we can remove some of them based on the following observation. Consider two eARs λ e 1 : (a, f, θ1) and </p><formula xml:id="formula_9">λ e 2 : (a, f, θ2) in P(λ i ). Suppose θ1 &gt; θ2, then M λ e 1 ⊂ M λ e 2 (If M λ e 1 = M λ e 2 ,</formula><formula xml:id="formula_10">= log(|F| • |G f |) (See analysis in Appendix C).</formula><p>We use a compressed inverted index CIX to detect threshold redundancy. An inverted index over P(λ i ) maps each λ e ∈ P(λ i ) to a list of record pairs that satisfy λ e (i.e. M λ e ). We can compress the inverted index since if</p><formula xml:id="formula_11">λ e 1 &gt; λ e 2 , M λ e 1 ⊆ M λ e 2 .</formula><p>Suppose λ e 1 is the maximal eAR and λ e 2 is the second maximal eAR in P(λ i 1 ). The compressed inverted list CIX(λ e 2 ) of λ e 2 only stores the record pairs in M λ e 2 -M λ e 1 . Iteratively we construct the compressed inverted index for all eARs. Figure <ref type="figure" target="#fig_7">3</ref> shows the compressed inverted index over the two groups of P(λ i 1 ). For example, CIX(λ e 3 ) = M λ e 3 -M λ e 2 = {rp1,6, rp1,3, rp3,7}. We partition CIX into two lists: CIXM (λ e ) and CIXD(λ e ), which respectively denote the positive and the negative record pairs in CIX(λ e ). For example, CIXM (λ e 3 ) = {rp1,6} and CIXD(λ e 3 ) = {rp1,3, rp3,7}. Recall Theorem 2, to verify whether λ e 1 is redundant, we need to check M λ e 2 -M λ e 1 ⊆ M for every λ e 2 ∈ P(λ i ) that is smaller than λ e 1 . However, note that we only need to check the maximal eAR λ e 2 such that λ e 2 &lt; λ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Similarity-Function Redundancy</head><p>In this section, we study redundancy among different similarity functions. We observe that 1) Similarity functions tend to return a higher score to a similar pair and a lower score to a dissimilar pair, and they may give nearly the same scores for some similar pairs; 2) Similarity functions may share a common inherent coherence. For example, consider some token-based similarity functions such as Jaccard and Cosine. They all share an inherent coherence: if two strings share many tokens, any token-based similarity functions will return a higher value <ref type="bibr" target="#b5">[7]</ref>. Based on these observations, we discuss how to find redundancy among similarity functions.</p><p>Consider two eARs λ e 1 : (a, f1, θ1) and λ e 2 : (a, f2, θ2) in P(λ i ) where f1 = f2. We first consider a special case such that, for each record pair, λ e 1 and λ e 2 always generate the same record pairs, i.e. M λ e 1 = M λ e 2 . From the viewpoint of RR set, they have no difference and we can only keep one of them. To make this observation more general, we find that 1) for each positive pair (r, r ′ ) (i.e. (r, r ′ ) ∈ M ), if λ e 1 takes this pair as similar, λ e 2 will also takes it as similar, and 2) for each negative pair (r, r ′ ) (i.e. (r, r ′ ) ∈ D), if λ e 1 takes this pair as dissimilar, λ e 2 will also takes it as dissimilar, then λ e 1 is redundant w.r.   The basic idea is as follows. Consider two groups G f 1 , G f 2 with similarity functions f1, f2. Since similarity-function redundancy exists between two different groups, we only need to study how to detect redundant (1) Eliminating threshold redundancy has no effect on similarity-function redundancy. Therefore, for a candidate eAR set, we can first eliminate its threshold redundancy, then eliminate its similarity-function redundancy. (2) The eliminating order of function redundancy and threshold redundancy will not change the final results. That is we get the same results for the two cases (a) eliminating function redundancy first and then threshold redundancy; and (b) eliminating threshold redundancy first and then function redundancy. (Appendix E shows the correctness).</p><formula xml:id="formula_12">eARs in G f 1 w.r.t G f 2 . Consider an eAR λ e 1 ∈ G f 1 . Next we show for each eAR λ e 1 ∈ G f 1 we only need O(1) time to verify whether λ e 1 is redundant w.r.t G f 2 . Let λ e M be the maximal eAR in G f 2 such that M λ e 1 ∩ M ⊆ M λ e M ∩ M . For other eARs λ e ∈ G f 2 , if λ e &gt; λ e M , since λ e M is maximal, M λ e 1 ∩ M M λ e ∩ M ; if λ e ≤ λ e M , then M λ e M ⊆ M λ e , thus M λ e 1 ∩ M ⊆ M λ e ∩ M . Therefore, to validate whether λ e 1 is redundant w.r.t G f 2 , we only need to check if there exists an eAR λ e in C = {λ e ∈ G f 2 |λ e ≤ λ e M } such that M λ e 1 ∩ D ⊇ M λ e ∩ D. Since λ e M is the maximal eAR in C, we have M λ e M ∩ D ⊆ M λ e ∩ D for other λ e in C, thus we only need to check M λ e 1 ∩ D ⊇ M λ e M ∩ D. Let λ e D be the maximal eAR in G f 1 such that M λ e D ∩ D ⊇ M λ e M ∩ D. If λ e 1 &gt; λ e D , since λ e D is maximal, M λ e 1 ∩ D M λ e M ∩ D; if λ e 1 ≤ λ e D , since M λ e 1 ⊇ M λ e D , M λ e 1 ∩ D ⊇ M λ e M ∩ D. Therefore,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ALGORITHMS FOR SiFi PROBLEM</head><p>A brute-force algorithm to solve the SiFi problem is to first enumerate all candidates and then select the one with the maximal value as follows. Given a set of RRs Φ, we first generate P(λ i ) for each iAR λ i in Φ and reduce P(λ i ) to P n (λ i ) by eliminating redundancy. Then we enumerate all the combinations of eRRs and the number of candidates is λ i ∈Φ |P n (λ i )|. For each candidate, we compute its objective-function value, and select the one with the maximal value. However as there are large numbers of candidates, this method is very expensive. Suppose there are 6 RRs, each RR contains 2 iARs, and there are 5 functions and 10 possible thresholds. Then there will be 50 12 candidates.</p><p>In addition, the SiFi problem is N P -hard as formalized in Theorem 4, which can be proved using a reduction from the maximum-coverage problem <ref type="bibr" target="#b13">[15]</ref>. We propose three heuristic algorithms SiFi-Greedy, SiFi-Gradient and SiFi-Hill.</p><p>Theorem 4. The SiFi problem is N P -hard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SiFi-Greedy:</head><p>We propose SiFi-Greedy inspired by the greedy algorithm for maximum-coverage problem <ref type="bibr" target="#b13">[15]</ref>. Intuitively, for a given set Φ of RRs, SiFi-Greedy picks the best iAR in Φ at each time and terminates until all the iARs are picked. Specifically, the algorithm first evaluates the quality of each iAR in Φ based on the example set E, and then picks the iAR with the highest quality, denoted by λ i max . Next it updates Φ by changing λ i max to the eAR with the highest quality in P n (λ i max ), and updates the example set E by removing the examples that satisfy the eRRs in Φ. The algorithm terminates when there is no iAR in Φ. The quality of an eAR λ e w.r.</p><formula xml:id="formula_13">t E = M ∪ D is defined as Q(λ e ) = ̥(λ e , M, D)</formula><p>where ̥ is the same objective function used to quantity eRRs (λ e is taken as a special eRR with a single eAR). For an iAR λ i , since any eAR in P n (λ i ) can become its instance, we define its quality Q(λ i ) as the maximum of Q(λ e ) for λ e ∈ P n (λ i ). Example 3 shows how SiFi-Greedy works.  </p><formula xml:id="formula_14">M , M λ e 2 ∩ M = {rp1,7} and M λ e 2 ∩ D = {}. There- fore, Q(λ e 2 ) = |M λ e 2 ∩ M | -0.5 • |M λ e 2 ∩ D| = 1. Similarly, Q(λ e 3 ) = 1, Q(λ e 5 ) = 2.5 and Q(λ e 12 ) = 0.5. Since Q(λ e 5 ) is maximum, Q(λ i 1 ) = 2.5. Using the same method, we can compute Q(λ i 4 ) = 3.5. As Q(λ i 4 ) &gt; Q(λ i 1 ), λ i max = λ i 4 . Let λ e ′</formula><p>4 denote the eAR with the highest quality in P n (λ i 4 ). Thus, λ e ′ 4 is the instance of λ i 4 . We update Φ by changing λ i 4 to λ e ′ 4 , and update the example set by removing the examples that satisfy the eRRs in Φ, i.e. λ e 3 ∧ λ e ′ 4 . At the second step, based on the new example set, we recompute Q(λ i 1 ) and update Φ by changing λ i 1 to the eAR with the highest quality in P n (λ i 1 ). Finally, we obtain the instances of λ i 1 and λ i 4 .</p><p>SiFi-Gradient: The major problem of SiFi-Greedy is that iARs are optimized independently, and the interaction among different iARs is neglected. For example, in Figure <ref type="figure">2</ref>, due to the occurrence of both λ i 1 and λ i 4 in φ3, the best instance of λ i 1 may change for different instances of λ i 4 , and vice versa. To address this problem, we devise an iterative algorithm, namely SiFi-Gradient, which is based on the idea of gradient descent. Intuitively, SiFi-Gradient iteratively adjusts the instances of iARs for reaching a higher objective value. Initially, given a set Φ of RRs, for each λ i ∈ Φ, the algorithm takes λ e ∈ P n (λ i ) with the highest Q(λ e ) as λ i 's instance, and computes the objective value of the corresponding eRRs. At each iteration, the algorithm changes the instance of each iAR to one of its neighbors which results in the largest objective value. SiFi-Gradient terminates the iteration when the objective value can not be larger. Consider the instance λ e : (a, f, θ) of an iAR λ i . Intuitively, the neighbors of λ e consists of eARs whose thresholds are close to θ. Formally, we say λ e 1 : (a, f1, θ1) is λ e 's neighbor if and only if λ e 1 ∈ P n (λ i ) and there does not exist λ e 2 : (a, f1, θ2) ∈ P n (λ i ) such that θ2 ∈ (θ, θ1) or θ2 ∈ (θ1, θ). SiFi-Hill: SiFi-Gradient terminates the iteration when the objective value can not be larger by changing current instances to their neighbors. In each iteration it only considers neighbors of current instances, and it only uses a subset of P n (λ i ) and may lead to local optimal solution. The objective value may become larger by changing current instances to other eARs. To address this problem, we devise another iterative algorithm, namely SiFi-Hill, which is based on the idea of hill climbing. SiFi-Hill uses the same method as SiFi-Gradient to initialize the instances of iARs. At each iteration, different from SiFi-Gradient, the algorithm adjusts the instance of a single iAR and allows to change the instance of λ i to any eAR in P n (λ i ). SiFi-Hill terminates when the objective value can not be larger. Example 5 shows how SiFi-Hill works.</p><p>Example 5. Recall Example 4, initially SiFi-Hill takes λ e 5 as λ i 1 's instance and λ e ′ 4 as λ i 4 's instance, and computes the objective value op. Next we iteratively adjust instances of λ i 1 and λ i 4 . We first fix λ e 5 and try to replace λ e ′ 4 with the other eAR in P n (λ i 4 ), then fix λ e ′ 4 and try to replace λ e 5 with the other eAR in P n (λ i 1 ). SiFi-Hill chooses the case that results in the largest objective value, and updates op to the new objective value, then goes to the next iteration. If the objective value can not be larger than op, SiFi-Hill terminates and returns the current instances of λ i 1 and λ i 4 .</p><p>We also explore the practical applicability of our approach to support missing attribute values, combination of similarity values and majority votes in Appendix F and provide the complexity analysis of three algorithms in Appendix G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENTS</head><p>We have conducted experiment evaluation on both real and synthetic data sets: Cora, Restaurant and DBGen. We compared with the baseline methods and state-of-theart methods, OpTrees <ref type="bibr" target="#b4">[6]</ref> and SVM <ref type="bibr" target="#b3">[5]</ref>. Appendix H.1 gives detailed data set descriptions and experimental settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Comparison with Baseline Methods</head><p>We compared SiFi-Gradient and SiFi-Hill with baseline methods SiFi-Greedy, SiFi-Equal, SiFi-Expert, where SiFi-Equal uses f= for each iAR and SiFi-Expert used the eARs formulated by experts (see Table <ref type="table">5</ref> in Appendix H.1). We asked for three experts to formulate eARs who are familiar with the datasets, SiFi-Expert-1, SiFi-Expert-2 and SiFi-Expert-3. We used K -fold cross-validation to evaluate all methods.</p><p>We compared these methods on three data sets where the objective function is F-measure (We evaluated different objective functions in Appendix H.2). Figure <ref type="figure" target="#fig_13">5</ref> reports F-measure values by varying the number of folds (K ). We can see SiFi-Hill and SiFi-Gradient outperform the baseline methods on two real data sets Cora and Restaurant. For example, in Figure <ref type="figure" target="#fig_13">5</ref>(a) the values of SiFi-Hill are around 0.9 while the best baseline method SiFi-Greedy is around 0.7. This is because iARs are interdependent, and SiFi-Greedy neglects the interaction among different iARs. But for DBGen, SiFi-Greedy almost got the same objective value as SiFi-Hill since the data set contains errors in different attributes independently. SiFi-Hill performs better than SiFi-Gradient on Cora since SiFi-Gradient only enumerates neighbors at each iteration while SiFi-Gradient enumerates all possible eARs.</p><p>From the performance of SiFi-Equal, we can see it is important to match some attribute values approximately. For example, in Figure <ref type="figure" target="#fig_13">5</ref>(a) the values of SiFi-Equal are below 0.1. The performance of SiFi-Expert shows the necessity of studying the SiFi problem. Firstly, experts can not select appropriate similarity functions and thresholds for iARs. For example, in Figure <ref type="figure" target="#fig_13">5</ref>(a) the values of SiFi-Expert-1, SiFi-Expert-2, SiFi-Expert-3 can not even reach 0.5 on Cora data set. Secondly, it is hard for human to tell the difference of similarity functions and thresholds, so the eARs formulated by experts may lead to different results. For example, in Figure <ref type="figure" target="#fig_13">5</ref>(b) the F-measure of SiFi-Expert-3 is around 0.8 while that of SiFi-Expert-1 are only around 0.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation of Eliminating Redundancy</head><p>We first compared the efficiency of eliminating redundancy on three data sets. In Figure <ref type="figure">6</ref>, ER denotes the simple algorithm and ER-Op denotes the optimized algorithm in Section 4. Note that ER-Op contains the time of precomputing λ e M and λ e D . We can see ER-Op is faster than ER by several orders of magnitude. For example, in Figure <ref type="figure">6</ref>(a) when the number of record pairs is 10 5 , ER took 8492 seconds to eliminate redundancy, but ER-Op only took 5.5 seconds. Next we evaluated the effect of eliminating redundancy on SiFi-Hill. In Figure <ref type="figure">6</ref>, SiFi-Hill, SiFi-Hill+ER and SiFi-Hill+ER-Op respectively denote the SiFi-Hill algorithm without eliminating redundancy, using ER algorithm to eliminate redundancy and using optimized ER algorithm to eliminate redundancy. We can see SiFi-Hill+ER-Op performs the best among the three algorithms. In Figure <ref type="figure">6(c</ref>) for 5K record pairs, SiFi-Hill took 53.6 seconds and SiFi-Hill+ER took 121.1 seconds but SiFi-Hill+ER-Op only took 14.3 seconds. Figure <ref type="figure">6</ref> also illustrates two important find-ings. The first one is eliminating redundancy can improve the performance of SiFi-Hill. From Figure <ref type="figure">6</ref>(a)-(c), we can see SiFi-Hill+ER-Op always outperforms SiFi-Hill. The second one is that optimizing the eliminating-redundancy algorithm is quite necessary. In Figure <ref type="figure">6</ref>(a), SiFi-Hill+ER performs much worse than SiFi-Hill as ER is inefficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Comparison with existing techniques</head><p>We compared our methods with OpTrees and SVM for record matching. OpTrees is another efficient and explainable technique for record matching <ref type="bibr" target="#b4">[6]</ref>. It needs a set of positive and negative examples to construct an executable operator tree and also three input parameters β, d and K, where β is used to adjust the precision, d denotes the maximal number of similarity functions in all RRs and K denotes the maximal number of RRs in the operator tree. In the experiment, we set β to the value such that the precision is no larger than 0.9, d = 2<ref type="foot" target="#foot_0">1</ref> and K = 4. SVM is a machine-learning technique for record matching that has been shown to significantly outperform other machine-learning techniques such as decision trees <ref type="bibr" target="#b3">[5]</ref>. Given a data set with n attributes, we represent each record pair as a n|F|-dimensional vector where each component denotes similarity between two attribute values of the record that is calculated using one of the |F| similarity functions. We implemented OpTrees by ourselves and obtained the implementation of SVM from Bilenko et al <ref type="bibr" target="#b3">[5]</ref>.</p><p>We first compared the effectiveness with existing techniques. Figure <ref type="figure">7</ref> reports F-measure values by varying the number of folds. We see that SiFi-Hill can get higher values than OpTrees. For example, in Figure <ref type="figure">7</ref>(a) the values of SiFi-Hill are around 0.9 while those of OpTrees are around 0.8. This is because OpTrees does not consider the redundancy among similarity functions. In our experiments, there are a lot of optional similarity functions and OpTrees fails to find the optimal similarity functions. We also see that SVM can get the highest values while it consumes the most time for record matching as shown in the following experiment.</p><p>Next we compared the efficiency with existing techniques. We used the whole data set as training and testing data. Table <ref type="table" target="#tab_10">3</ref> reports the results. The training time of SiFi-Hill, OpTrees, SVM respectively refers to the time of computing the optimal eARs, the time of constructing the operator tree, and the time of learning classifier. The testing time respectively refers to the time of executing eRRs, the time of executing the operator tree, and the time of running the classifier. In the training process, we see that SiFi-Hill consumes the least time. For example, on Cora SiFi-Hill needs 2018 seconds which is about half of the time of OpTrees and SVM. In the testing process, SiFi-Hill and OpTrees are much more efficient than SVM. For example, on Restaurant the elapsed time for SiFi-Hill and OpTrees are less than 8 seconds while SVM needs 221.8 seconds. This is because similarityjoin operators can use some filter techniques to efficiently join similar pairs <ref type="bibr" target="#b5">[7]</ref>. Thus, SiFi-Hill is more efficient and interpretable for record matching and can also achieve comparable accuracy with machine-learning techniques (SVM).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>In this paper we have studied the problem of similarityfunction identification in record-matching rules for effective entity matching. We proposed to identify the best similarity functions and thresholds to maximize a given objective function. We proposed to detect and eliminate redundancy among similarity functions and thresholds. We also devised efficient algorithms to find the best similarity functions to maximize the eliminated redundancy. The experimental results on both real and synthetic datasets show that our method achieves high performance and result quality. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A. SUMMARY OF NOTATIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. PROOF</head><p>Proof of Theorem 1 Proof. As θ1 &lt; θ2, we have</p><formula xml:id="formula_15">M λ e 1 ⊇ M λ e 2 . Thus M λ e 1 ∩ M ⊇ M λ e 2 ∩ M, M λ e 1 ∩ D ⊇ M λ e 2 ∩ D. As there is no positive example in M λ e 1 -M λ e 2 , we have M λ e 1 ∩ M = M λ e 2 ∩ M. For any ψ1 ∈ Ψ1, if ψ1 does not contain λ e</formula><p>1 , Ψ2 also contains ψ1; otherwise Ψ2 replaces it with another eRR ψ2 by substituting λ e 1 for λ e 2 . As M ψ = ∩M λ i , we have</p><formula xml:id="formula_16">M ψ 1 ∩ M = M ψ 2 ∩ M, M ψ 1 ∩ D ⊇ M ψ 2 ∩ D.</formula><p>As M Ψ = ∪M ψ i , we have</p><formula xml:id="formula_17">M Ψ 1 ∩ M = M Ψ 2 ∩ M, M Ψ 1 ∩ D ⊇ M Ψ 2 ∩ D. That is ̥(Ψ1, M, D) ≤ ̥(Ψ2, M, D).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Corollary 1</head><p>Proof. Consider an eRR λ e 1 (a, f, θ1) of this iAR. Suppose λ e 1 / ∈ P(λ i ). We prove that λ e 1 can be pruned.</p><formula xml:id="formula_18">Let V = {θmax} ∪ {f (r[a], r ′ [a])|f ∈ F, (r, r ′ ) ∈ M }. As λ e 1 / ∈ P(λ i ), θ1 / ∈ V . As θ1 &lt; θmax, we find θ2 ∈ V , which is the minimal value in V that is larger than θ1. Let λ e 2 = (a, f, θ2). There is no positive examples in M λ i 1 -M λ i 2</formula><p>, thus we can prune λ e 1 based on Theorem 1; otherwise, suppose there is a positive example in</p><formula xml:id="formula_19">M λ i 1 -M λ i 2 with similarity θ ′ . Obviously θ1 &lt; θ ′ &lt; θ2.</formula><p>Based on the definition of V , θ ′ ∈ V , which conflicts that θ2 is the minimal value in V that is larger than θ1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Theorem 2</head><p>Proof. Consider a set of RRs with an iRR which contains λ i . Suppose Ψ1 is a specificized eRR set, Ψ2 is another specificized eRR set transformed from Ψ1 by replacing λ e 1 in Ψ1 with λ e 2 . Since θ1 &gt; θ2 we have </p><formula xml:id="formula_20">M λ e 1 ⊆ M λ e 2 , thus M λ e 1 ∩ M ⊆ M λ e 2 ∩ M, M λ e 1 ∩ D ⊆ M λ e 2 ∩ D.</formula><formula xml:id="formula_21">M Ψ 1 ∩ M ⊆ M Ψ 2 ∩ M M Ψ 1 ∩ D = M Ψ 2 ∩ D.</formula><p>That is ̥(Ψ1, M, D) ≤ ̥(Ψ2, M, D). λ e 1 is redundant w.r.t λ e 2 since it cannot get a better objective value than λ e 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Theorem 3</head><p>Proof. Consider a set of RRs with an iRR which contains λ i . Suppose Ψ1 is a specificized eRR set, Ψ2 is another specificized eRR set transformed from Ψ1 by replacing λ e 1 in Ψ1 with λ e 2 . Since θ1 &gt; θ2 we have</p><formula xml:id="formula_22">M λ e 1 ⊆ M λ e 2 , thus M λ e 1 ∩ M ⊆ M λ e 2 ∩ M, M λ e 1 ∩ D ⊇ M λ e 2 ∩ D.</formula><p>With the same idea as the proof in Theorem 1, we have</p><formula xml:id="formula_23">M Ψ 1 ∩ M ⊆ M Ψ 2 ∩ M, M Ψ 1 ∩ D ⊇ M Ψ 2 ∩ D.</formula><p>That is ̥(Ψ1, M, D) ≤ ̥(Ψ2, M, D). λ e 1 is redundant w.r.t λ e 2 since it cannot get a better objective value than λ e 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Theorem 4</head><p>Proof. We prove that the SiFi problem is N P -hard using a reduction from maximum coverage problem <ref type="bibr" target="#b13">[15]</ref>. Given a universal set</p><formula xml:id="formula_24">U = {e1, e2, • • • , en} of n elements, a collec- tion C = {S1, S2, • • • , Sm} where Si ⊆ U (i ∈ [1, m])</formula><p>, and an integer k, the maximum-coverage problem is to select k sets from C such that their union has the maximum cardinality. We consider a variant of the maximum-coverage problem that given k collections C1, • • • , C k where each collection is the same as C = {S1, S2, • • • , Sm}, the goal is to select one set from each collection such that their union has the maximum cardinality. A minor difference from the original problem is that duplicate sets are allowed, e.g. we can select S1 from both C1 and C2. Obviously, the difference can not lead to a larger objective value. Therefore, we can solve the maximum-coverage problem via its variant. Next we reduce the variant to the SiFi problem. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. TIME COMPLEXITY ANALYSIS OF RE-DUNDANCY DETECTION</head><p>We first analyze the time complexity of the algorithm for threshold-redundancy detection. It contains two steps to detect redundant eARs in P(λ i ). The first step is to construct the compressed inverted index CIX and the second step is to use CIX to detect redundancy. We first construct a forward index F X over P(λ i ) from E. Each row represents a record pair rp in E and each column represents a similarity function f in F . F X(rp, f ) stores the maximal eAR λ e in G f such that rp λ e . Figure <ref type="figure">8</ref> shows the forward index for the running example. For instance, F X(rp1,5, fg) = λ e 9 as rp1,5 λ e 9 but rp1,5 λ e 8 . Specially F X(rp4,5, fe) = '-' means there is no eAR λ e in G fe such that rp4,5 λ e . We analyze the time complexity of constructing F X. It has |E| rows and |F| columns. Consider one cell F X(rp, f ). We need O(log|P(λ i )|) time to find the maximal eAR λ e in G f such that rp λ e . Therefore, the total time is</p><formula xml:id="formula_25">O(|F| • |E| • log|P(λ i )|).</formula><p>Suppose we want to compute T LM (λ e , f Using this equation, we can compute T LM (λ e , f ) for all λ e ∈ G f i incrementally. Let λ e(k) be the k-th largest eAR in G f i . Initially, let λ e(0) M be the maximal eAR in G f . Suppose we have obtained λ  </p><formula xml:id="formula_26">) (λ e ∈ G f i ) that is the maximal eAR λ e M in G f such that M λ e ∩ M ⊆ M λ e M ∩ M .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. ELIMINATING REDUNDANCY</head><p>We find that eliminating threshold redundancy has no effect on similarity-function redundancy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. PRACTICAL APPLICABILITY</head><p>Missing Attribute Values: Our approach can support the case of missing attribute values. This is because given multiple RRs, a record pair is taken as matching if it satisfies one of RRs. A missing attribute value can affect some RRs while others may still work. Consider the RRs in Figure <ref type="figure">2</ref>. If a value of "tel" attribute is missing, φ1 will be affected while φ2 and φ3 still work. Thus if r6[tel] is missing, the matching pair rp6,7 can still be returned based on φ2 (if email attributes are same and addr attributes are similar).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combination of Similarity Values:</head><p>The record matching rules allow a combination of similarity values of a single attribute. This can be achieved by adding a combination of similarity functions into the similarity-function set F. For example, consider λ i = {name, F, [0, 1]}. Adding 0.7fe + 0.3fj into F makes name attribute allow a combination of Edit Similarity and Jaccard Similarity. However, for the record-matching rules with a combination of similarity values of multiple attributes, there is no method that can efficiently find record pairs satisfying such rules <ref type="bibr" target="#b4">[6]</ref>. Since we focus on efficient record-matching methods, such recordmatching rules are not allowed.</p><p>Majority Votes: To make RRs support majority votes, we can define that a record pair satisfies a set Φ of RRs if and only if it satisfies more than a half of the RRs in Φ. As formalized in Theorem 5, Theorems 1, 2, 3 and Corollary 1 also hold for majority votes. Therefore, the algorithms of computing candidate eAR set and eliminating redundancy are applicable for majority votes. where t ′ is the number of iterations. In the worst case, SiFi-Gradient and SiFi-Hill need |M | iterations, but in our experiment, the algorithms can converge with smaller than 10 iterations on both real and synthetic data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. ADDITIONAL EXPERIMENTS H.1 Experiment Setup</head><p>Datasets: We used three data sets to evaluate our method.</p><p>Cora<ref type="foot" target="#foot_2">2</ref> is a collection of citation entries. The data consists of 1875 distinct citations of 191 papers. We selected 9 frequent attributes in our experiment: author, title, venue, address, publisher, editor, date, volume, pages. It had 1875 * 1874 2 = 1, 756, 875 record pairs. To eliminate this quadratic cost, we used a similar idea with the canopy method <ref type="bibr" target="#b17">[19]</ref>. We concatenated attribute values of each citations to a string and used jaccard similarity based on tokens to quantify the similarity of two strings. We eliminated record pairs whose similarity are either equal to 1 or no larger than 0.1. After this process, the total number of record pairs was 184,738 with 14,358 positive pairs.</p><p>Restaurant<ref type="foot" target="#foot_3">3</ref> is a collection of restaurant records. The data contains 864 distinct records of 752 restaurants. Each record has five attributes: name, addr (restaurant address), phone, city, and type. It had 864 * 863 2 = 372, 816 record pairs. We used the same method as Cora to reduce the number of record pairs to 87,492 with 106 positive pairs. DBGen 3 is a random mailing-list generator. The generated mailing-list has 10 attributes, ssn (social security number), fname (first name), minit (middle initial), lname (last name), stnum (street number), stadd (street name), apmt (apartment number), city, state and zip. We generate data set by setting Number of Records to 1000 and Number of Clusters to 100, and keeping other parameters by default. Using the same method as Cora, we construct a data set of 5,497 record pairs with 3,071 positive pairs. Similarity Metrics: A large number of similarity functions are proposed to quantity string similarity. SecondString [1] and SimMetrics <ref type="bibr">[2]</ref> are two open-source Java packages that implement a large collection of string functions. In particular, SimMetrics provides a consistent interface layer that returns a normalized similarity measure from 0 to 1, 0 being entirely different, 1 being identical. We used SimMetrics package in our experiment. For token-based similarity functions, we need to consider different ways of tokenization, such as tokenizing by space, q-gram, etc. Totally we selected 26 similarity functions. In the case of null values, we</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A sample data of relation R (RP i,j denotes the record pair (ri, rj)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>A</head><label></label><figDesc>record-matching rule φ = k i=1 λi is called an explicit record-matching rule (eRR) if every λi is an explicit attribute-matching rule for 1 ≤ i ≤ k; otherwise it is called an implicit record-matching rule (iRR). For examples, Figure 2(b) shows 3 iRRs. Consider φ1 = λ i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Definition 4 .</head><label>4</label><figDesc>Ψ ∩M |+|M Ψ ∩D| is the precision, and r = |M Ψ ∩M | |M | is the recall. Users can tune the weights of precision or recall to select their preferences: preferring to recall or precision. Now we are ready to formalize the problem of si milarity f unction i dentification in implicit record-matching rules for effective entity matching (called SiFi). Given a set of RRs Φ, a set of positive examples M , and a set of negative examples D, SiFi finds a set of instances (eRRs) Ψ from Φ to maximize a pre-defined objective function ̥(Ψ, M, D). Consider the relation R in Figure 1. The positive-example set M = {rp1,6, rp1,7, • • • } contains five record pairs and the negative-example set D = {rp1,2, rp1,3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>|M Ψ ∩M | |M Ψ ∩M |+|M Ψ ∩D| = 33+1 and the recall is r = |M Ψ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Theorem 1 .</head><label>1</label><figDesc>Consider a set of RRs Φ, a set of positive examples M , a set of negative examples D, and an objective</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>is redundant w.r.t :(name, f e , 0.73) :(name, f g , 0.55) :(name, f g , 0.5 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Verify similarity-function redundancy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Example 3 .</head><label>3</label><figDesc>Suppose the objective function ̥(Ψ, M, D) = |M Ψ ∩ M | -0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :Figure 6 :Figure 7 :</head><label>567</label><figDesc>Comparing F-measure of different methods. Comparison for the total running time of different methods. Comparison of F-measure with exsiting methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>For any record pair (r, r′ ) ∈ M λ e 2 ∩ D, (r, r ′ ) should be in M λ e 1 ∩ D; otherwise if (r, r ′ ) ∈ D, (r, r ′ ) / ∈ M λ e 1 , and as (r, r ′ ) ∈ M λ e 2 , (r, r ′ ) ∈ M λ e 2 -M λ e 1 ⊆ M , which conflicts with (r, r ′ ) / ∈ M (as (r, r ′ ) ∈ D). Therefore, M λ e 1 ∩ D = M λ e 2 ∩ D.With the same idea as the proof in Theorem 1, we have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>1. 11 Figure 8 :</head><label>118</label><figDesc>Figure 8: A forward index over P(λ i 1 ) in Table2.[1, m]} can not lead to the maximum objective value. Otherwise, if λ e j+m is chosen as an instance of λ i , then |M Ψ ∩M | ≤ n and |M Ψ ∩D| = n, thus ̥(Ψ, M, D) ≤ 0. But for the case that only eARs in {λ e j : (a, fj , θ1)|j ∈ [1, m]} are chosen, we have |M Ψ ∩ M | &gt; 0 and |M Ψ ∩ D| = 0, thus ̥(Ψ, M, D) &gt; 0. Therefore we can reduce P(λ i ) to {λ e j : (a, fj , θ1)|j ∈ [1, m]}.The above SiFi problem aims to choose an eAR from eachP(λ i ) such that |M Ψ ∩ M | -|M Ψ ∩ D| is maximum. Since P(λ i ) = {λ e j : (a, fj, θ1)|j ∈ [1, m]}, then for λ e j ∈ P(λ i ), we have M λ e j = Sj ⊆ M , thus |M Ψ ∩ M | -|M Ψ ∩ D| = |( λ e ∈Ψ M λ e ) ∩ M | -|( λ e ∈Ψ M λ e ) ∩ D| = λ e ∈Ψ M λ e .An equivalent statement of the SiFi problem is that given k collections where each one is {M λ e j |j ∈ [1, m]}, the goal is to select one set from each collection such that λ e ∈Ψ M λ e is maximum. Since M λ e j = Sj (j ∈ [1, m]), the problem is the same as the variant of the maximum coverage problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>For the first step, as there are |E| record pairs and each pair needs O(|F| • log|P(λ i )|) time to insert it into CIX (as it needs to do a binary search to insert λ i into corresponding position), the total time to construct CIX is O(|F| • log|P(λ i )| • |E|). For the second step, for each λ e ∈ P(λ i ) the algorithm needs O(log|P(λ i )|) to check whether λ e is redundant, thus the total time of this step is O(|P(λ i )| • log|P(λ i )|). Based on the algorithm of computing candidate eAR set in Section 3, |P(λ i )| is no larger than |F| • |E|, thus the whole algorithm to detect threshold redundancy needs O |F| • log|P(λ i )| • |E| , i.e. O |F| • |K| • |E| where |K| = log(|F| • |G f |). We analyze the time complexity of the algorithm for function redundancy detection. It needs O(|F| • log|P(λ i )| • |E|) time to construct the compressed inverted index and the forward index, and O(|F| 2 • |E|) time to compute T LM and T LD. When detecting similarity-function redundancy, it enumerates |P(λ i )| eARs and needs O(1) to check whether each eAR is redundant w.r.t another group. Since there are |F| groups, the total time is O(|F| • |P(λ i )|). Since |P(λ i )| is no larger than |F| • |E|, it needs O |F| • |K| • |E| where |K| = min |F|, log(|P(λ i )|) to eliminate similarityfunction redundancy. D. PRE-COMPUTING T L M AND T L D To avoid the expensive computation of finding λ e M and λ e D on the fly, we pre-compute λ e M and λ e D and store them into two tables T LM and T LD. T LM (λ e , f ) stores the maximal eAR λ e M in G f such that M λ e ∩ M ⊆ M λ e M ∩ M . T LD(λ e , f ) stores the maximal eAR λ e D in G f such that M λ e ∩ D ⊆ M λ e D ∩ D. Next we focus on obtaining T LM and T LD efficiently.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>for λ e(k) . Then we compute λ e(k+1) M for λ e(k+1) incrementally. As M λ e(k+1) ∩ M = (M λ e(k) ∩ M ) ∪ CIXM (λ e(k+1) ), we have λ rp∈CIX M (λ e(k+1) ) F X(rp, f ) . This equation shows that λ e(k+1) M can be obtained by comparing λ e(k) M and F X(rp, f ) for each rp ∈ CIXM (λ e(k+1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Theorem 5 .</head><label>5</label><figDesc>Theorems 1, 2, 3 and Corollary 1 hold for changingM Ψ = ψ∈Ψ M ψ to MΨ = Ψ ′ ⊆Ψ,|Ψ ′ |&gt; |Ψ| 2 ψ∈Ψ ′ M ψ .Proof Sketch. To prove Theorem 1, we only change the line of its proof "As M Ψ = ∪M ψ i , we have" to "As M Ψ =Ψ ′ ⊆Ψ,|Ψ ′ |&gt; |Ψ| 2ψ∈Ψ ′ M ψ , we have". For Theorems 2, 3 and Corollary 1, we do not change their proofs.G. TIME COMPLEXITY ANALYSIS OFSiFi-Greedy, SiFi-Gradient AND SiFi-Hill Let n denote the number of iARs in Φ. All the algorithms need O(n • |F| • |M |) time to obtain candidate eAR sets and O n • |F| • |K| • |E| time to eliminate redundancy where |K| = min |F|, log(|P(λ i )|) . Next we analyze the time complexity of finding the best instance. Let P n (λ i ) denote the candidate eAR set of λ i without redundancy. For SiFi-Greedy, to compute Q(λ i ), we need enumerate |P n (λ i )| eARs and compute the quality of each eAR with O(|E|) time. At the beginning, there are n iARs in Φ. The number of iARs decreases one at each time. Therefore, the time complexity of SiFi-Greedy is O( n k=1 k • |P n (λ i )| • |E|). For SiFi-Gradient, at each iteration we need enumerate n • 2|F| neighbors and compute the new objective value with O(|Φ| • |E|) time. Therefore, the time complexity of SiFi-Gradient is O(t • n • |F| • |Φ| • |E|) where t is the number of iterations. For SiFi-Hill, to check whether the objective value can become larger, we need enumerate n • |P n (λ i )| eARs and compute the new objective value with O(|Φ| • |E|) times. Therefore, the time complexity of SiFi-Hill is O(t ′ • n • |P n (λ i )| • |Φ| • |E|)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Similarity for iAR λ i 1 : (name, {fe, fg}, [0, 1]). Ψ2 is another eRR set transformed from Ψ1 by replacing λ e 1 in Ψ1 with λ e 2 . If θ1 &lt; θ2 and there is no positive example</figDesc><table><row><cell cols="2">rid pairs fe</cell><cell>fg</cell><cell cols="2">rid pairs fe</cell><cell>fg</cell><cell cols="2">rid pairs fe</cell><cell>fg</cell></row><row><cell>rp1,2</cell><cell cols="3">0.4 0.23 rp2,4</cell><cell>0.13</cell><cell>0</cell><cell>rp3,7</cell><cell cols="2">0.8 0.78</cell></row><row><cell>rp1,3</cell><cell cols="2">0.8 0.55</cell><cell>rp2,5</cell><cell cols="3">0.73 0.55 rp4,5</cell><cell>0.09</cell><cell>0</cell></row><row><cell>rp1,4</cell><cell>0.1</cell><cell>0</cell><cell>rp2,6</cell><cell cols="2">0.4 0.23</cell><cell>rp4,6</cell><cell>0.1</cell><cell>0</cell></row><row><cell>rp1,5</cell><cell cols="3">0.73 0.58 rp2,7</cell><cell cols="2">0.44 0.25</cell><cell>rp4,7</cell><cell>0</cell><cell>0</cell></row><row><cell>rp1,6</cell><cell cols="2">0.8 0.5</cell><cell>rp3,4</cell><cell cols="3">0.1 0.09 rp5,6</cell><cell cols="2">0.55 0.27</cell></row><row><cell>rp1,7</cell><cell cols="2">0.9 0.7</cell><cell>rp3,5</cell><cell cols="2">0.64 0.5</cell><cell>rp5,7</cell><cell cols="2">0.64 0.5</cell></row><row><cell>rp2,3</cell><cell cols="3">0.4 0.25 rp3,6</cell><cell cols="2">0.6 0.21</cell><cell>rp6,7</cell><cell cols="2">0.7 0.31</cell></row></table><note><p>function ̥(Ψ, M, D). Consider an iRR in Φ which contains an iAR λ i : (a, F, Θ), and two instances of λ i , λ e 1 : (a, f, θ1) and λ e 2 (a, f, θ2). Suppose Ψ1 is an eRR set,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Candidate eAR Set of λ i</figDesc><table><row><cell>λ e 1 λ e 2 λ e 3 λ e 4 λ e 5 λ e 6</cell><cell>(name,fe, 1) (name,fe, 0.9) (name,fe, 0.8) (name,fe, 0.73) (name,fe, 0.7) (name,fe, 0.1)</cell><cell>λ e 7 λ e 8 λ e 9 λ e 10 λ e 11 λ e 12</cell><cell>(name,fg, 1) (name,fg, 0.7) (name,fg, 0.55) (name,fg, 0.5) (name,fg, 0.31) (name,fg, 0.09)</cell></row></table><note><p>1 : (name, {fe, fg}, [0, 1]).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>A simple algorithm to detect such redundancy is to enumerate |F| • |G f | candidate eARs, and takes O |G f | • |E| to verify the redundancy of each candidate. The algorithm needs O |F|•|G f | 2 •|E| time. Next we propose an efficient algorithm that reduces the time complexity to O |F|•|K|•|E| where |K|</figDesc><table /><note><p>λ e 2 will be removed based on Theorem 1). Obviously, λ e 1 and λ e 2 generate the same record pairs M λ e 1 , and λ e 2 can identify more record pairs M λ e 2 -M λ e 1 . If these pairs are all positive examples, that is M λ e 2 -M λ e 1 ⊆ M , then λ e 2 will be better than λ e 1 . We say λ e 1 is redundant w.r.t λ e 2 . The correctness is proved in Theorem 2. Theorem 2. Given a candidate eAR set P(λ i ) and a set M of positive examples, λ e 1 : (a, f, θ1) ∈ P(λ i ) is redundant w.r.t λ e 2 : (a, f, θ2) ∈ P(λ i ) if M λ e 2 -M λ e 1 ⊆ M and θ1 &gt; θ2.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>t λ e 2 since it can not lead to a better objective value than λ e 2 . The first statement implies M λ e 1 ∩M ⊆ M λ e 2 ∩M , and the second statement is equivalent to M λ e 1 ∩ D ⊇ M λ e 2 ∩ D. Theorem 3 shows the correctness. Theorem 3. Given a candidate eAR set P(λ i ), a set M of positive examples and a set D of negative examples, λ e )| candidate eARs, and takes O |P(λ i )| • |E| to verify the redundancy of each candidate. The algorithm needs O |P(λ i )| 2 • |E| time. It is expensive since |P(λ i )| may be quite large. Next we propose an efficient algorithm that reduces the time complexity to O |F| • |K| • |E| where</figDesc><table><row><cell>1 : 2 : (a, f2, θ2) ∈ P(λ i ) (a, f1, θ1) ∈ P(λ i ) is redundant w.r.t λ e</cell></row><row><cell>if M λ e 1 ∩ M ⊆ M λ e 2 ∩ M and M λ e 1 ∩ D ⊇ M λ e 2 ∩ D.</cell></row><row><cell>A simple algorithm to detect such redundancy is to enu-</cell></row><row><cell>merate |P(λ i</cell></row></table><note><p>|K| = min |F|, log(|P(λ i )|) (See analysis in Appendix C).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>λ e 1 is redundant w.r.t G f 2 if and only if λ e 1 ≤ λ e D . Note that we can pre-compute λ e M and λ e D , and store them into two tables in advance (See details in Appendix D). Then given λ e , we can get λ e M and λ e D with O(1) time. Example 2 shows how this idea works. Example 2. In Figure 4, consider λ e 10 : (name, fg, 0.5) in G fg . We want to check whether λ e 10 is redundant w.r.t G fe . Firstly, we find the maximal λ e M in G fe such that M λ e 10 ∩ M ⊆ M λ e M ∩ M . From Table 1, we can compute M λ e</figDesc><table><row><cell></cell><cell></cell><cell>10 ∩ M</cell></row><row><cell cols="3">= {rp1,6, rp1,7, rp2,5}. Since M λ e 10 ∩ M ⊆ M λ e 4 ∩ M =</cell></row><row><cell cols="3">{rp1,6, rp1,7, rp2,5} and M λ e 10 ∩ M M λ e 3 ∩ M = {rp1,6, rp1,7}, the maximal λ e M is λ e</cell></row><row><cell cols="3">From Table 1, we can compute M λ e 4 ∩ D = {rp1,3, rp1,5,</cell></row><row><cell>rp3,7}. Since M λ e 4</cell><cell>∩ D ⊆ M λ e 9</cell><cell>∩ D = {rp1,3, rp1,5,</cell></row><row><cell cols="3">rp3,7} and M λ e 4 ∩ D M λ e 8 ∩ M = {rp1,3}, the maximal λ e D is λ e 9 : (name, fg, 0.55). Thirdly, we compare λ e 10 with λ e 9 . Since λ e 10 ≤ λ e 9 , λ e 10 is redundant w.r.t G fe .</cell></row><row><cell cols="3">Eliminating Redundancy: Our method has two salient</cell></row><row><cell>features.</cell><cell></cell><cell></cell></row></table><note><p>4 :(name, fe, 0.73). Secondly, we find the maximal λ e D in G fg such that M λ e 4 ∩ D ⊆ M λ e D ∩ D.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>5 • |M Ψ ∩ D|. In our running example, there are two iARs λ i 1 and λ i 4 , thus SiFi-Greedy needs two steps. At the first step, we first compute the quality of λ i</figDesc><table><row><cell>1 and λ i 4 . 12 } after elimi-5 , λ e 3 , λ e 2 , λ e 1 ) = {λ e 1 , we obtain P n (λ i For λ i</cell></row><row><cell>nating redundancy from the candidate eAR set in Table 2.</cell></row><row><cell>As only rp1,7 satisfies λ e 2 , M λ e 2 = {rp1,7}. As rp1,7 is</cell></row><row><cell>in</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Example 4  shows how SiFi-Gradient works.</figDesc><table><row><cell>Example 4. Recall Example 3, λ e 5 and λ e ′ 4 are the eARs with the highest quality in P n (λ i 1 ) and P n (λ i 4 ), respectively. Initially, SiFi-Gradient takes λ e 5 as λ i 1 's instance and λ e ′ 4 as λ i 4 's instance, and computes the corresponding objective</cell></row><row><cell>value, denoted as op. Next we iteratively adjust instances of</cell></row><row><cell>λ i 1 and λ i 4 . Consider the current instance λ e 5 : (name, fe, 0.7) of λ i 1 . We first identify the neighbors of λ e 5 from P n (λ i 1 ) = {λ e 2 , λ e 3 , λ e 5 , λ e 12 }. We can see λ e 3 is a neighbor of λ e 5 since for other eARs with the same similarity function as λ e 3 , i.e. λ e 2 : (name, fe, 0.9), the threshold of λ e ∈ (0.7, 0.8). 2 is 0.9 / Similarly, we can identify another neighbor λ e 12 . We change λ e 5 to one of its neighbors, λ e 3 or λ e 12 and choose the neigh-</cell></row><row><cell>bor, which can result in the largest objective value, as the</cell></row><row><cell>new instance of λ i</cell></row></table><note><p>1 . Using the similar method, we can identify a new instance for λ i 4 . SiFi-Gradient changes λ e 5 and λ e ′ 4 to new instances, and updates op to the new objective value, then goes to the next iteration. If the new objective value can not be larger than op, SiFi-Gradient terminates the iteration and returns the current instances of λ i 1 and λ i 4 .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 3 :</head><label>3</label><figDesc>Efficiency Comparison (seconds).</figDesc><table><row><cell></cell><cell cols="2">Cora</cell><cell cols="2">Restaurant</cell><cell cols="2">DBGen</cell></row><row><cell></cell><cell>train</cell><cell>test</cell><cell>train</cell><cell>test</cell><cell>train</cell><cell>test</cell></row><row><cell>SiFi-Hill</cell><cell>2018</cell><cell>35</cell><cell>43.2</cell><cell>4.9</cell><cell>17.4</cell><cell>0.85</cell></row><row><cell>OpTrees</cell><cell>4555</cell><cell>47</cell><cell>1362</cell><cell>7.6</cell><cell>20.4</cell><cell>0.73</cell></row><row><cell>SVM</cell><cell>3610</cell><cell>3035</cell><cell>263.4</cell><cell>221.8</cell><cell>23.6</cell><cell>20.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4 :</head><label>4</label><figDesc>Summary of notations used in this paper.</figDesc><table><row><cell>Notations</cell><cell>Descriptions</cell></row><row><cell>R</cell><cell>a relation</cell></row><row><cell>a</cell><cell>an attribute in R</cell></row><row><cell>r</cell><cell>a record in R</cell></row><row><cell>λ</cell><cell>an attribute-matching rule (AR)</cell></row><row><cell>λ i</cell><cell>an implicit attribute-matching rule (iAR)</cell></row><row><cell>λ e</cell><cell>an explicit attribute-matching rule (eAR)</cell></row><row><cell>φ</cell><cell>a record-matching rule (RR)</cell></row><row><cell>ψ</cell><cell>an explicit record-matching rule (eRR)</cell></row><row><cell>Φ</cell><cell>a set of record-matching rules (RRs)</cell></row><row><cell>Ψ</cell><cell>a set of explicit record-matching rules (eRRs)</cell></row><row><cell>f</cell><cell>a similarity function</cell></row><row><cell>F</cell><cell>a set of similarity functions</cell></row><row><cell>θ</cell><cell>a similarity-function threshold</cell></row><row><cell>Θ</cell><cell>a threshold range</cell></row><row><cell>E</cell><cell>a set of examples</cell></row><row><cell>M</cell><cell>a set of positive examples</cell></row><row><cell>D</cell><cell>a set of negative examples</cell></row><row><cell>MΨ</cell><cell>a set of record pairs generated by Ψ</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Then for each record pair rp in M λ e ∩ M , we have rp λ e M . And since F X(rp, f ) is the maximal eAR in G f</figDesc><table><row><cell>such that rp each rp. As λ e M should be maximal, we have F X(rp, f ), we have λ e M ≤ F X(rp, f ) for λ e min M = rp∈M λ e ∩M</cell></row></table><note><p>F X(rp, f ).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>). Since |M | = λ e(k) ∈G f i CIXM (λ e(k) ), we only need O(|M |) time to obtain T LM (λ e , f ) for all λ e ∈ G f i . Since the numbers of f and groups are both |F|, we need O(|F| 2 • |M |) time to obtain the table T LM . With the same idea, we can obtain the table T LD in O(|F| 2 • |D|) time. Therefore, the total time of computing T LM and T LD is O(|F| 2 • |E|).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>The eliminating order of function redundancy and threshold redundancy will not change the final results. That is we get the same results for the two cases (1) eliminating function redundancy first and then threshold redundancy; and(2) eliminating threshold redundancy first and then function redundancy. The correctness is shown in Lemma 2 Lemma 2. Consider two possible sequences of eliminating redundancy. Seq1: Eliminating threshold redundancy first, then function redundancy; Seq2: Eliminating function redundancy first, then threshold redundancy. Given a candidate eAR set P(λ i ), a set M of positive examples and a set D of negative examples, for any λ e ∈ P(λ i ), we have (1) both of Seq2 and Seq2 will eliminate λ e ; (2) neither Seq1 nor Seq2 will eliminate λ e .</figDesc><table><row><cell>That is if λ e 1 is threshold redundant w.r.t P(λ i ), after eliminating λ e 1 , for any λ e 2 that is similarity-function redundant w.r.t P(λ i ), λ e 2 is still similarity-function redundant w.r.t P(λ i ) -{λ e 1 }.</cell></row><row><cell>Therefore, we can first eliminate its threshold redundancy,</cell></row><row><cell>then eliminate its similarity-function redundancy. The cor-</cell></row><row><cell>rectness is shown in Lemma 1.</cell></row><row><cell>Lemma 1. Given a candidate eAR set P(λ i ), a set M of</cell></row><row><cell>positive examples and a set D of negative examples, suppose</cell></row><row><cell>λ e 1 ∈ P(λ</cell></row></table><note><p>i ) is threshold redundant w.r.t P(λ i ) and λ e 2 ∈ P(λ i ) is similarity-function redundant w.r.t P(λ i ). If λ e 1 is eliminated from P(λ i ), then λ e 2 is still similarity-function redundant w.r.t P(λ i ) -{λ e 1 }.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In our settings, there are large numbers of candidate functions. When d</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>&gt; 2, OpTrees cannot finish in 3 days.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>http://www.cs.umass.edu/∼mccallum/data/cora-refs.tar.gz</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>http://www.cs.utexas.edu/users/ml/riddle/data</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. The authors thank the anonymous reviewers for their insightful suggestions. This work was partly supported by the Research Grants Council of the Hong Kong SAR, China, under Grant No. CUHK/419008 and CUHK/419109, the National Natural Science Foundation of China under Grant No. 61003004 and 60873065, the National Grand Fundamental Research 973 Program of China under Grant No. 2011CB302206, National S&amp;T Major Project of China under Grant No. 2011ZX01042-001-002, and the "NExT Research Center" funded by MDA, Singapore, under the research Grant No. WBS:R-252-300-001-490.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>RR set for the Cora data set 1 : author i ∧ title i ∧ venue e 2 : author i ∧ title e ∧ venue i 3 : author e ∧ title i ∧ venue i 4 : author i ∧ title i ∧ venue i ∧ date e 5 : author i ∧ title i ∧ venue i ∧ volumn e ∧ pages e 6 : author i ∧ title i ∧ publisher e ∧ editor e ∧ date e 7 : author i ∧ title e ∧ publisher e ∧ editor e ∧ volume e ∧ pages e RR set for the Restaurant data set 1 : name e ∧ addr i 2 : name i ∧ addr e 3 : name i ∧ addr i ∧ city e 4 : name i ∧ addr i ∧ city i ∧ type e</p><p>RR set for the DBGen data set 1 : f name i ∧ lname e ∧ zip e 2 : f name e ∧ lname i ∧ zip e 3 : f name e ∧ lname e ∧ zip i 4 : f name e ∧ minit e ∧ lname e 5 : f name i ∧ stnum e ∧ stadd i ∧ city e 6 : f name i ∧ stnum i ∧ stadd e ∧ city e 7 : lname i ∧ stnum e ∧ stadd i ∧ city e 8 : lname i ∧ stnum i ∧ stadd e ∧ city e 9 : f name i ∧ stnum e ∧ stadd i ∧ city i ∧ state e 10 : f name i ∧ stnum i ∧ stadd e ∧ city i ∧ state e 11 : lname i ∧ stnum e ∧ stadd i ∧ city i ∧ state e 12 : lname i ∧ stnum i ∧ stadd e ∧ city i ∧ state e    suppose the similarity between two null values is 1 and the similarity between a null value and a non-null value is 0.</p><p>Record-matching Rule Set: To reduce user effort, all iARs are in the form of (a, F, [0, 1]) where F consists of 26 similarity functions. Figure <ref type="figure">9</ref> shows three RR sets for Cora, Restaurant, DBGen respectively. To save space, we denote (a, f=, 1) as a e and (a, F, [0, 1]) as a i . The RR set for Cora consists of seven RRs with three iARs, author i , title i and venue i . The RR set for Restaurant consists of four RRs with three iARs, name i , addr i and city i . The RR set for DB-Gen consists of twelve RRs with six iARs, f name i , lname i , stnum i , stadd i , city i and zip i . Note that we ignore the attribute phone and the attribute ssn in the RR set of Restaurant and DBGen respectively since we found that simply returning record pairs that share the same phone (ssn) can lead to an acceptable result. The RR sets of Restaurant and DBGen are formulated by experts based on domain knowledge. The RR set of Cora is deduced from an initial set of matching dependencies <ref type="bibr" target="#b9">[11]</ref>. A matching dependency over one relation, λ1 ∧ λ2 A contains all the attributes of the relation, then we can deduce the record-matching rule author i ∧title i ∧venue e → A.</p><p>When the number of deduced rules is large, the algorithm can select the top-k rules based on some heuristic metrics such as the diversity of rules.</p><p>User Study: We chose three students, called Expert 1, Expert 2, Expert 3 respectively, from our research group to do user study. They are quite familiar with similarity metrics and data sets. We asked them to formulate the eARs for the iARs in Figure <ref type="figure">9</ref>. Table <ref type="table">5</ref> shows the result. Note that there are only six candidate functions. Experts tend to select edit-based similarity functions such as Edit Similarity and Soundex for the iAR whose attribute value consists of a small number of tokens (e.g. Expert 1 selects Edit Similarity fe for author i ) and select token-based similarity functions such as Jaccard Similarity and Cosine Similarity for the iAR whose attribute values consists of a large number of tokens (Expert 2 selects Jaccard Similarity fj for title i ). We see eARs differ a lot in both similarity functions and thresholds.</p><p>Table <ref type="table">5</ref>: The eARs formulated by three experts for iARs in Figure <ref type="figure">9</ref> (fe: Edit Similarity, fj : Jaccard Similarity, fg: Gram-based Similarity, fc: Cosine Similarity <ref type="bibr">[8]</ref>, fw: Jaro Winkler <ref type="bibr">[8]</ref>, fs: Soundex <ref type="bibr" target="#b15">[17]</ref>). All the algorithms were implemented in C++ and compiled using GCC 4.2.3 with -O3 flag. All the experiments were run on a Ubuntu machine with an Intel Core 2 Quad X5450 3.00GHz processor and 4 GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2 Evaluation of Objective Functions</head><p>We evaluated the effectiveness of our methods with different objective functions. We used a family of objective functions, i.e. (α ∈ (0, 1)) where α is a parameter to tune the importance of precision and recall. In the case that we require the returned record pairs have higher precision, we can specify a larger α; on the contrary, if we require the returned record pairs miss fewer matching record pairs, we can specify a smaller α. Figure <ref type="figure">10</ref> shows the 5-cross validation results on three data sets. We can see SiFi-Hill is always superior to other methods, and has more stable values with changes of objective functions.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient exact set-similarity joins</title>
		<author>
			<persName><forename type="first">A</forename><surname>Arasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ganti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kaushik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="918" to="929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A comparison of fast blocking methods for record linkage</title>
		<author>
			<persName><forename type="first">R</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Christen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Churches</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 ACM SIGKDD Workshop on Data Cleaning, Record Linkage, and Object Consolidation</title>
		<meeting>the 2003 ACM SIGKDD Workshop on Data Cleaning, Record Linkage, and Object Consolidation</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="25" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive duplicate detection using learnable string similarity measures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bilenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Example-driven design of efficient record matching queries</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ganti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kaushik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="327" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A primitive operator for similarity joins in data cleaning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ganti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kaushik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="5" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A comparison of string distance metrics for name-matching tasks</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Fienberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IIWEB</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="73" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to match and cluster large high-dimensional data sets for data integration</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Richman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="475" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Duplicate record detection: A survey</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Elmagarmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Verykios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reasoning about record matching rules</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="407" to="418" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A theory for record linkage</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">P</forename><surname>Fellegi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Sunter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">328</biblScope>
			<biblScope unit="page" from="1183" to="1210" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Declarative data cleaning: Language, model, and algorithms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Galhardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Florescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shasha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-A</forename><surname>Saita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="371" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The merge/purge problem for large databases</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Stolfo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="127" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Approximation algorithms for NP-hard problems</title>
		<editor>D. S. Hochbaum</editor>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>PWS Publishing Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Advances in record-linkage methodology as applied to matching the 1985 census of tampa, florida</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Jaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">406</biblScope>
			<biblScope unit="page" from="414" to="420" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Record linkage: similarity measures and algorithms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Koudas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="802" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Entity identification in database integration</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Prabhakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="294" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient clustering of high-dimensional data sets with application to reference matching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interactive deduplication using active learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhamidipaty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="269" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning domain-independent string transformation weights for high accuracy object identification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tejada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Knoblock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Minton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="350" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Methods for record linkage and bayesian networks</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Series RRS2002/05, U.S. Bureau of the Census</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
