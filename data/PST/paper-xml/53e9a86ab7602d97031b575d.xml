<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast and Extensible Building Modeling from Airborne LiDAR Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
							<email>qianyizh@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
							<email>uneumann@graphics.usc.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Fast and Extensible Building Modeling from Airborne LiDAR Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B7EAAE64BE979A9D6A2465416813BC06</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.4.8 [Image Processing And Computer Vision]: Scene Analysis-Range data Algorithm</term>
					<term>Experimentation</term>
					<term>Performance LiDAR(light detection and ranging)</term>
					<term>building modeling</term>
					<term>segmentation</term>
					<term>building footprints</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents an automatic algorithm which reconstructs building models from airborne LiDAR (light detection and ranging) data of urban areas. While our algorithm inherits the typical building reconstruction pipeline, several major distinct features are developed to enhance efficiency and robustness: 1) we design a novel vegetation detection algorithm based on differential geometry properties and unbalanced SVM; 2) after roof patch segmentation, a fast boundary extraction method is introduced to produce topology-correct water tight boundaries; 3) instead of making assumptions on the angles between roof boundary lines, we propose a data-driven algorithm which automatically learns the principal directions of roof boundaries and uses them in footprint production. Furthermore, we show the extendability of our algorithm by supporting non-flat object patterns with the help of only a few user interactions. We demonstrate the efficiency and accuracy of our algorithm by showing experiment results on urban area data of several different data sets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>3D building models have long been useful in a variety of applications in geographic information systems, such as urban planning and virtual city tourism. In these applications, Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. ACM GIS '08, <ref type="bibr">November 5-7, 2008</ref>. Irvine, CA, USA (c) 2008 ACM ISBN 978-1-60558-323-5/08/11 ... $5.00. building models are preferred to be composed of several simple parts and organized in a meaningful way.</p><p>One way to obtain such data is to let skillful workers create building models manually based on building blueprints, aerial images and other data sources. This solution, however, requires a large amount of manual work, thus is both slow and expensive. Airborne LiDAR (Light Detection and Ranging) data, on the other hand, provides a faster and lower cost means to gather first-hand data of selected urban areas. With laser scanners equipped on aeroplanes, a 3D point cloud is collected, which samples the surface of an urban site in an accurate and fast manner. Directly converting LiDAR data into a DSM (Digital Surface Model) provides us another way of obtaining building models. These models, however, consist of millions of triangles and contain undesired noise and vegetation. Therefore they satisfy neither the simple requirement nor the meaningful requirement.</p><p>To bridge this large gap between LiDAR data and 3D building models, much research effort has focused on the building reconstruction problem. There are two major differences between our work and the existing approaches. First, we show that exploiting shared properties of specific types  of areas will lead to better reconstruction performance. In this paper we concentrate on the urban areas. Our algorithm can adapt to properties such as vegetation patterns and buildings layouts, therefore yielding better result based on the context knowledge. Second, experiments are done on several data-sets of two cities and one industrial site to show the generality and extendability of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Algorithm overview</head><p>In this paper, we present a fully automatic algorithm which creates simple and meaningful building models from urban LiDAR data. Our algorithm requires only the LiDAR data as input and works directly on the original point cloud without rasterization. This provides us with more information especially at the multi-layered area,<ref type="foot" target="#foot_0">1</ref> e.g. roof edges and trees. Our output, on the other hand, is a set of watertight models, each of which is composed of a few triangles. The boundaries of neighbor building components are aligned together to reflect the relationship between them.</p><p>Most existing works on building reconstruction from Li-DAR data follow a three-stage pipeline (Figure <ref type="figure" target="#fig_2">2</ref>): first, irrelevant parts such as trees and ground are removed from the LiDAR data; second, roof patches are separated and extracted from the remaining point cloud, and significant features (e.g. boundaries) are detected; finally, with these features, each roof patch is fitted to a pre-defined parametric model which is further used in creating polygonal models.</p><p>While sharing a similar pipeline, we introduce novel mechanisms to adapt to urban properties and to enhance efficiency. Our contribution mainly includes:</p><p>• For classification of vegetation from other urban objects, we introduce a SVM (Support Vector Machine) algorithm which takes several differential geometry properties as features, instead of using global-aware features such as height and intensity. Therefore, the learning algorithm only needs to be trained once, and the solution could be reused in other data sets without retraining.</p><p>• We propose an efficient and robust boundary extraction algorithm by extending the 3D OcTree contouring algorithm in the field of volumetric geometry processing <ref type="bibr" target="#b16">[17]</ref>. Compared with existing boundary extraction algorithm, our method is topology-preserving, faster and much easier to implement.</p><p>• For the last stage of the pipeline, we design a datadriven algorithm which learns the principal directions from original data, and aligns roof boundary segments along these directions automatically. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, this mechanism enables us to handle arbitrary angles and avoid the pre-assumption or preference for specific angles between neighboring roof boundary segments which is often assumed in previous works (e.g. 90 • , 45 • and 135 • in <ref type="bibr" target="#b14">[15]</ref>).</p><p>• We show our algorithm can be easily extended to handle non-flat object patterns by adding minimal user interaction.</p><p>The rest part of this paper is organized as follows: Section 2 summarizes the related literature to this paper; Section 3 presents the algorithm to detect and eliminate vegetation (mainly trees) from original data; Section 4 describes our boundary extraction algorithm; in Section 5, we illustrate the final step of our pipeline -generation of final building models; Section 6 presents an extension to our approach for handling non-flat object patterns, as well as some implementation details; Section 7 gives the experiment results and Section 8 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Building modeling</head><p>Pioneer building modeling algorithms, e.g. <ref type="bibr">[2][12]</ref>[16] <ref type="bibr" target="#b10">[11]</ref> [5] <ref type="bibr" target="#b0">[1]</ref>, convert LiDAR point cloud into a DSM (Digital Surface Model) as their first steps, and then apply image processing algorithms on the depth images to detect building footprints, fit parametric models and reconstruct polygons. Although not explicitly mentioned in all papers, most works follow the pipeline we summarized in Section 1.1. This pipeline is proved to be both useful and efficient in previous research.</p><p>Some recent research such as <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b14">[15]</ref> have introduced the direction of constructing building models from the original LiDAR point cloud. These algorithms also follow the common pipeline, and are similar to our algorithm in structure. Verma et al. <ref type="bibr" target="#b13">[14]</ref> focus on automatic building topology detection by searching pre-defined roof patterns in the rooftopology graph; while Wang et al. <ref type="bibr" target="#b14">[15]</ref> concentrate on build-ing footprint extraction problem; both are different from our approach.</p><p>Another related but orthogonal research direction to our approach is to combine LiDAR data with other data sources. For instance, Früh and Zakhor <ref type="bibr" target="#b3">[4]</ref> introduce both ground based and aerial LiDAR data. By integrating them, complete building models are constructed to contain not only detailed roofs, but also subtle facades. You et al. <ref type="bibr" target="#b15">[16]</ref>, on the other hand, allow users to make a few clicks and use these inputs as a heuristic guide in model reconstruction. As a subsequent work, Hu et al. <ref type="bibr" target="#b5">[6]</ref> employ aerial image and ground images to achieve a higher-resolution solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Vegetation detection</head><p>As some researchers have pointed out, vegetation (trees) are the most difficult part to be separated from building roofs. Therefore, there are several works addressing this problem such as <ref type="bibr" target="#b12">[13]</ref>[9][14] <ref type="bibr" target="#b14">[15]</ref>. In general, these works employ both geometry and reflection properties at each sample point, and then use a classification algorithm to assign each point to corresponding class. The classification algorithm could be a simple thresholding <ref type="bibr" target="#b13">[14]</ref>, an SVM classifier <ref type="bibr" target="#b12">[13]</ref>, or an Adaboost classifer <ref type="bibr" target="#b8">[9]</ref>[15].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Building footprint detection</head><p>Besides tree detection, another well-studied subproblem is building footprint extraction. Haithcoat et al. <ref type="bibr" target="#b4">[5]</ref> detect building footprints from a depth image, then use an orthogonal simplification algorithm to turn these footprints into rectangle corners. Wang et al. <ref type="bibr" target="#b14">[15]</ref> raise a bayesian approach in building footprint extraction, which shows a preference to specific corner angles. Alharthy and Bethel <ref type="bibr" target="#b0">[1]</ref>'s algorithm is the most similar to ours. They also make the assumption that boundary segments in a local area fall into a couple of dominant directions. However, their algorithm simply detects two orthogonal principal directions, thus limiting their application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">VEGETATION DETECTION</head><p>As we have discussed in section 1.1, vegetation, mainly in the form of trees in urban area, is an irrelevant part in our task of building reconstruction and should therefore be eliminated in the first step. Assume that p ∈ P is a sample point in the original LiDAR point cloud, Np = {q|q ∈ P, pq &lt; δ} denotes the set of points within a small neighborhood of point p, and p is the centroid of all points in Np, we compute five features based on the local differential geometry properties:</p><p>1. Regularity: we first measure if the point distribution around a point is regular by calculating</p><formula xml:id="formula_0">F1 = p -p .<label>(1)</label></formula><p>Intuitively, sample distribution around a roof point is more likely to be regular, thus the distance between original point and centroid point should be smaller than that of vegetation points.</p><p>2. Horizontality: LiDAR captures roof from a top view. Therefore, the normal at a roof point is more likely to be vertical. In this case, we compute: where ez = (0, 0, 1) is the vertical direction, and np is obtained through covariance analysis <ref type="bibr" target="#b9">[10]</ref>. <ref type="foot" target="#foot_1">2</ref> In particular, we solve the eigenvector problem for the covariance matrix</p><formula xml:id="formula_1">F2 = 1 -|np • ez|,<label>(2)</label></formula><formula xml:id="formula_2">Cp = 1 |Np| X q∈Np (q -p)(q -p) T .</formula><p>The three eigenvalues are sorted in ascending order, i.e. λ0 ≤ λ1 ≤ λ2; and the eigenvector corresponding to the smallest eigenvalue, i.e. v0, is the approximated normal at point p.</p><p>3. Flatness: with covariance analysis results, the flatness at this point, also known as surface variation <ref type="bibr" target="#b9">[10]</ref>, could be estimated as</p><formula xml:id="formula_3">F3 = λ0 λ0 + λ1 + λ2 .<label>(3)</label></formula><p>Similar to the former features, a smaller flatness value indicates more possibility for a point to be a roof point.</p><p>4. Normal distribution: once the normal at each point is estimated, we could further apply another covariance analysis on these normals, but within a different neighborhood N n p = {q|q ∈ P, pq &lt; η}. We solve the eigenvector problem for a normal covariance matrix</p><formula xml:id="formula_4">C n p = 1 |N n p | X q∈N n p n T q • nq,</formula><p>and get corresponding eigenvalues</p><formula xml:id="formula_5">λ n 0 ≤ λ n 1 ≤ λ n 2 .</formula><p>As Pauly <ref type="bibr" target="#b9">[10]</ref> has pointed out, λ n 1 measures the maximum variation of these normals on the Gauss sphere, hence could be regarded as a kind of normal variation. Therefore we define</p><formula xml:id="formula_6">F4 = λ n 1 .<label>(4)</label></formula><p>A roof point prefers a smaller normal variation than a tree point. Moreover, we observe that λ n 0 also reflects some kind of regularity in normal distribution. An example is shown in figure <ref type="figure" target="#fig_3">3</ref>. In this case, normals of points around a roof ridge (which is a common pattern in buildings) form two clusters on the Gauss sphere. Hence λ n 1 (illustrated as the length of the green vector in Figure <ref type="figure" target="#fig_3">3(b)</ref>) is large while λ n 0 (length of the blue vector in figure 3(b)) is very small. In contrast, normal distribution around a tree point is fairly irregular and exhibits large λ n 0 and λ n 1 (shown in figure <ref type="figure" target="#fig_3">3(c)</ref>). Therefore, our last feature is defined as</p><formula xml:id="formula_7">F5 = λ n 0 .<label>(5)</label></formula><p>To this end, we use a linear discriminative function to classify trees from ground and roof area, denoted as:</p><formula xml:id="formula_8">K = ω0 + ω1F1 + ω2F2 + ω3F3 + ω4F4 + ω5F5, (6)</formula><p>where ω0,1,2,3,4,5 are undetermined parameters which are then learned using an unbalanced soft margin SVM algorithm from a small urban area with manual labeling. We use a third-party library SVM Light <ref type="bibr" target="#b7">[8]</ref> as our implementation. Once these parameters are determined, the classification algorithm simply computes K for each point and determine its category with sgn(K).</p><p>To further improve the classification results, we introduce a voting algorithm as a post-processing step. The idea is based on the fact that points of same category usually occur together in space. Hence, for each point, we let all points in its neighborhood Np vote for the category, and point p belongs to tree category only if the percentage of tree votes is greater than a certain value ω, which is also learned from the labeled data set.</p><p>Note that all of the features are designed based on a local geometry analysis, thus we avoid absolute variants such as height and intensity to enhance the generalization ability of the learned function. In our experiments, all the parameters are learned from a 100m × 100m labeled area. They show great adaptability on our testing data sets.</p><p>The only parameter we need to set for each data set is δ and η. Empirically, we set δ to the value which makes the average point number in Np between 13 ∼ 15, and η to two times the value of δ.</p><p>Our classification algorithm achieves an accuracy above 95% on all testing set. The feature values and final classification results are shown in Figure <ref type="figure" target="#fig_4">4</ref>. Note that our algorithm could precisely classify trees and non-trees even at areas where points of both categories have similar height and are connected together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ROOF BOUNDARY GENERATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Planar roof patch extraction</head><p>Once trees are eliminated from the LiDAR data, we need to identify ground points and separate different roof patches. This is achieved using a distance-based region growing algorithm. Starting from a random seed point p, the algorithm searches its neighborhood Np, and assigns all untouched points with its distance to p less than a certain parameter α. 3 By iteratively running this algorithm, the LiDAR point cloud is divided into different patches. We accept the largest patch as ground. To further improve the results, we introduce a post-processing algorithm which merges low-height 3 We choose α = 1m in our experiments. large patches into grounds. This is reasonable as in some area ground might be divided into several pieces due to the cutoffs on roads caused by occlusion. Finally, we combine roof patches with neighboring x and y coordinates into one roof for certain building. Figure <ref type="figure" target="#fig_4">4</ref>(c) shows such a result. Note that our method for this mainly focuses on planar roof patterns. Hence, we need to detect planar shapes from roof patches. We use a clustering algorithm based on the similarity between normals of neighboring points. In particular, the algorithm is similar to the region growing algorithm presented in the previous paragraph: it also searches the neighborhood Np of point p, but the classification is based on normal difference, i.e. whether 1 -|np • nq| &lt; β. 4   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Boundary production</head><p>The next step in our algorithm is to mark boundary points on each plane roof patch, which are later used in footprint generation and polygon construction. Some previous works, e.g. <ref type="bibr" target="#b14">[15]</ref>, find boundary points by measuring certain characteristics of roof points. These kinds of methods, although efficient, cannot guarantee a watertight manifold boundary, hence it limits the subsequent processing. Other works use 2D delaunay triangulation to find polygonal boundaries, e.g. Verma et al. <ref type="bibr" target="#b13">[14]</ref> introduce a plane ball-pivoting algorithm to triangulate roof points and detect boundaries. These algorithms are able to generate perfect boundaries, however, the efficiency is sacrificed. In addition, a robust implementation for such algorithm is hard to achieve.</p><p>We propose an algorithm which combines the benefits of these two kinds of algorithm. Our method is inspired by the contouring algorithm in <ref type="bibr" target="#b16">[17]</ref>.</p><p>Initially, for each plane roof patch, all of the roof points are projected to the same plane. Then a uniform grid is applied onto this plane and all the grid cells containing roof points are marked. These grid cells are defined as object cells, which are illustrated as grey cells in Figure <ref type="figure" target="#fig_5">5</ref>.</p><p>Note that each connected component of these object cells is surrounded by a watertight polygonal boundary composed of grid lines and corners, shown as the red grid lines and corners in the figure. Therefore, we construct a closed boundary for the LiDAR points as follows:</p><p>• For each boundary grid line l, which separates an ob- ject cell cin and a background cell cout, we take the nearest LiDAR point to l in cin as a boundary point p(l), shown as red circles in Figure <ref type="figure" target="#fig_5">5</ref>. <ref type="foot" target="#foot_2">5</ref>• For each boundary grid corner c, which connects two boundary grid lines l1 and l2, we add a boundary edge (p(l1), p(l2)), shown as red thin edges in Figure <ref type="figure" target="#fig_5">5</ref>.</p><p>In this way, we construct a watertight manifold boundary for every component of object cells. Our method is easy to implement, at the same time guarantees the correctness of topology. A detailed correctness proof could refer to the 2D version of proofs in <ref type="bibr" target="#b16">[17]</ref>. The completeness of geometry, on the other hand, cannot be guaranteed by our method. For example, in Figure <ref type="figure" target="#fig_5">5</ref>, there is one point outside the extracted boundary However, we find this not a common case and our boundary is a good approximation for later processing.</p><p>Another advantage of our algorithm is that it could support morphological operations in an easy manner. After marking object and background cells, one could treat this grid as a monochrome image and apply any morphological operation onto it. Figure <ref type="figure" target="#fig_7">6</ref> illustrates an example from part of an industrial site. Directly extracting boundaries from the object patches produces numerous artifacts, shown in (b). Hence, we introduce an opening morphological operation to remove the unimportant information as in (c).  Only one parameter is involved in our boundary extraction algorithm, which is the unit length of the uniform grid. This parameter gives us the control of the connectivity of the signed to different boundary grid lines, we duplicate it and assign different copies of this point to different lines. These copies might be merged together in later processing if the correctness of topology is not violated.  patch, similar to the ball radius parameter in ball-pivoting algorithm used by <ref type="bibr" target="#b13">[14]</ref>. Based on our experience, we find that a unit length equal to δ performs well on most of our data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">BUILDING MODELING</head><p>In this section, we present a data driven algorithm which generates simple and correct polygonal mesh models from the extracted roof patch boundaries. Our main idea is based on the following observations. First, most boundary line segments in a local area fall into a couple of directions, known as principal directions. Second, if two roof planes are neighbors when projected to the xy plane, they are very likely to share a same boundary line segment or be connected by a vertical wall. In the latter case, the two line segments connected by the wall are also overlapping on the xy plane. Based on these two observations, we design a 3-step algorithm as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Find principal directions</head><p>For each boundary point, we first estimate the tangent direction across this point, by applying a 2D covariance analysis on seven nearby boundary points. <ref type="foot" target="#foot_3">6</ref> The estimated tan-gent directions of a building example is shown as the blue lines across red boundary points in Figure <ref type="figure" target="#fig_12">7(d)</ref>.</p><p>Although in general these tangent directions should follow the principal directions, yet due to the noise and insufficient sampling, boundaries often exhibit zigzags in detail and hence the tangents do not follow the principal directions very well in micro scope. As in Figure <ref type="figure" target="#fig_9">7</ref>(d), blue tangent lines do not follow the principal directions (illustrated as the dark blue cross arrow) precisely.</p><p>To overcome the difficulty, we introduce statistical analysis. In particular, we record the tangent directions for all the boundary points in a local area, say a 1km × 1km area, and build a histogram of direction angles. In this histogram, each peak represents a principal direction, to which numerous boundary points shows preference. Note that a histogram may have several peaks. Hence, we use an iterative algorithm to find all the peaks, as follows:</p><p>1. Find the highest peak in the histogram; take it as a principal direction, as d peak .</p><p>2. Remove this peak from the histogram. Note that a peak is similar to a Gauss distribution, hence we remove a whole section [d peak -φ0, d peak + φ1] from the histogram, where φ0 and φ1 are found by following the falling trend along x axis until reaching a local minimum.</p><p>3. Repeat step 1 and step 2, until the highest peak does not contain enough samples. • respectively, which could be separated into two orthogonal pairs. This phenomenon fits into the observation made by the previous works that orthogonal corners are a common pattern in building footprints. In addition, we notice the 0 • and 90 • directions contain less samples than the other two. In fact, this LiDAR point cloud piece is a square cut from the original data set, hence these two directions represent the directions at the area boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Snapping</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Snap to principal directions</head><p>Once the principal direction set D is determined, we divide the boundaries into line segments and align as many segments to the principal directions as possible. This process is performed for each boundary loop B through a greedy algorithm as follows:</p><p>For each boundary point bi ∈ B, we test for every principal direction d k ∈ D, and count the number of continuous boundary points of bi which agree with line l : (pbi) × d k = 0, denoted as Count(bi, d k ). The "agree" criteria is defined based on the distance from point p to line l, i.e.</p><formula xml:id="formula_9">distance(p, l) = |(p -bi) × d k | |d k | &lt; ε.<label>(7)</label></formula><p>We We ignore all the projected points, recalculate Count(bi, d k ), and find the next (b max i , d max k ) pair. The whole process is done iteratively, until the maximal number of agreement falls under a certain value (e.g. 10 points).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Neighbor segments snapping</head><p>The former snapping algorithm provides us line segments aligning to the principal directions. However, as we have pointed out, we could further improve the results by aligning neighbor line segments of different plane patches onto the same line. This is achieved through a neighbor segments snapping algorithm, as follows:</p><p>• For two neighbor boundary segments on the same plane patch, if they point toward the same direction and the distance between them is under a certain value; then we snap these two segments onto the same line. This operation solves the case in which a line segment is broken into two neighbor segments.</p><p>• For two boundary segments from different plane patch on same building roof, if they point toward the opposite direction (i.e. are of the opposite orientation), and the distance between them is under a certain value; then we snap these two segments onto the same line in xy plane, while keep the z coordinates for each segment. This operation eliminates the gaps between neighbor plane patches due to the noise and insufficient sampling. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Generate polygonal mesh</head><p>Finally, we finish the whole pipeline by constructing a polygonal mesh from the extracted boundary line segments. Typically, we construct a polygon for each boundary loops on the plane patches, and further connect them to the ground by adding vertical walls. Note that the boundaries we have on each plane patch are already composed of topology-correct loops. Thus, for each loop, we generate a polygon based on the line segments. Consider a pair of neighbor segments of a same boundary loop, if their end points are close enough, we create a new footprint as the intersection point of these two segments. Otherwise, we find the best approximation line to the interior boundary points and thus link all the segments into a complete polygon.</p><p>Figure <ref type="figure" target="#fig_9">7</ref>(c) shows a whole pipeline to create a building model. Starting from a building roof patch, plane patches are detected and boundaries are extracted. Boundary points are then snapped and form up the line segments. Finally, a polygonal model is constructed from these segments, which is shown in the last figure. The final result, produced from 9,778 roof points, only contains 72 triangles, yet representing this building in a precise manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">POST PROCESSING AND EXTENSIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Ground modeling</head><p>To construct the complete city model, we still need a ground model which could be combined with the models. We use the terrain geometry modeling algorithm presented in <ref type="bibr" target="#b13">[14]</ref>. Briefly speaking, a uniform grid is aligned onto the ground points of our urban patch. If a grid cell contains some ground points, its ground height is computed as the average height of them. We then solve a Laplace's equation ∇ 2 z = 0 taking the heights in all empty grid cells as unknowns. In particular, for each empty cell at (i, j), we have, 4zi,j = zi-1,j + zi+1,j + zi,j-1 + zi,j+1.</p><p>With this equation array, all the heights could be calculated and a DSM is generated from them as the ground mesh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Non-flat objects support</head><p>One limitation of the automatic pipeline presented in former sections is that it only supports flat roofs. However, in some cases, there are some non-flat objects in our data set. For example, figure <ref type="figure" target="#fig_13">8</ref>(a) shows an industrial site in which the most interesting objects are the oil tins and tanks represented in cylinder and cone shapes. These shapes increase the complexity of the object reconstruction problem. However, fortunately, we find that these non-flat shapes usually fall into only a few specific patterns. In this case, once the pattern type is known, we could apply typical pattern recognition algorithms such as RANSAC <ref type="bibr" target="#b2">[3]</ref>.</p><p>Therefore, we provide a user input mechanism to help with this task. The initial LiDAR data is first segmented into different patches and the noise and ground points are removed from the data set. Now the object patches are shown to the user as illustrated in Figure <ref type="figure" target="#fig_13">8(b)</ref>. The user could move his mouse onto the object patch, click to select this patch, and press a key to specify its pattern type, e.g. cone, cylinder, or plane-roof object. Since we could extract boundary loops for each object, as presented in the previous sections, now we perform a RANSAC algorithm <ref type="bibr" target="#b2">[3]</ref> based on the specified pattern type input by the user. Finally, we construct the complete model as shown in Figure <ref type="figure" target="#fig_13">8</ref>(c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">EXPERIMENTS</head><p>We have tested our algorithm on several types of LiDAR data taken from 3 data sets -Denvor city, Oakland city, and an industrial site. The resolution of our data sets varies from 6 samples/sq.m. to 17 samples/sq.m. Our experiments show that the time cost of our approach is proportional to the number of points. We test all the data sets on a consumerlevel laptop (Intel Core2 1.83GHz CPU, 2G memory) with an external hard disk, and find the ratio be around 8 minutes/million points.</p><p>Figure <ref type="figure" target="#fig_15">9</ref> shows the reconstruction results of a part of Denver city. Our building models are composed of simple and clean triangular meshes, and thus are ideal to some applications such as digital city visualization. In addition, the basic structures of these building models are aligned tightly together and form buildings of good shape. As shown in closeups of Figure <ref type="figure" target="#fig_15">9</ref>(b), these building models fit our initial LiDAR point cloud in an excellent manner.</p><p>To further demonstrate the ability of supporting multiple principal directions, we test our program on part of Oakland city, as shown in Figure <ref type="figure" target="#fig_0">1</ref>. Seven principal directions are automatically detected from the original data sets, illustrated as the arrows in Figure <ref type="figure" target="#fig_0">1</ref>(a). Since our program has no pre-assumptions on the corner angles, correct models are generated with corners of angles between any two principal directions, which may be 90 • , 45 • , or any other angle; shown in Figure <ref type="figure" target="#fig_0">1(c</ref>).</p><p>As we have pointed out in Section 6.2, there are cases where our automatic flat roof detection algorithm cannot work well. Hence, we propose a semi-automatic algorithm which benefits from a few user inputs. Figure <ref type="figure" target="#fig_13">8</ref> demonstrates an example of an industrial site. In this example, we define three object patterns, namely, a standing cylinder with a cone on top of it, representing a tin; a lying cylinder representing a tank; and plane shaped roofs. Using our algorithm, objects of all three patterns could be detected and reconstructed, as shown in closeup of figure <ref type="figure" target="#fig_13">8(c)</ref>.</p><p>Finally, we would like to point out one limitation of our algorithm. In this paper, we use airborne LiDAR data as the whole input. However, this kind of data contains depth information almost only from the top view, thus occlusion becomes a serious problem in case when the "vertical wall assumption" does not hold true. For example, in our industrial site data set, we lack point samples under tanks and pipes, hence we experience difficulties in reconstructing such objects. Another observation is that, when we try to apply our algorithm to the residential areas of some cities, trees become the majority objects and house roofs are usually partial occluded by the trees. In such cases, it is much more difficult for our algorithm to extract the precise boundaries for these roofs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSION</head><p>This paper presents an automatic building modeling algorithm for airborne LiDAR data. The roof and ground points are first separated from tree points by applying an SVM method based on local geometry property analysis. Once the plane roof patches are generated through clusters of roof points, a novel boundary detection algorithm is applied to extract a watertight manifold boundary for each planar patch. These boundaries are later analyzed to find principal directions of a local area, which are used as the guidance of a snapping algorithm. After all these steps, most roof points are snapped onto certain boundary line segments, and polygonal building models are produced from them.  Possible future work lies in the following directions. First, since we do not have ground-truth building models to our data set, it is a difficult task for us to analyze our results quantitatively. However, we notice the lack of quantitative error analysis is a common problem in this area. Hence, it may be necessary to investigate approaches for measuring the error of reconstructing results without ground-truth.</p><p>Second, since LiDAR data is normally a huge data set containing several millions of points, it is a common solution to divide the initial data set into several different pieces, each of which could be loaded into memory and processed at a time. Our implementation also follows this solution, however, we find that additional processing is needed at the area boundaries. On the other hand, we notice that our algorithm runs locally, i.e. any operation in our algorithm is a local operation which only involves points in a small neighborhood. Therefore it is suitable to be made into an out-of-core implementation, inspired from the streaming techniques <ref type="bibr" target="#b6">[7]</ref>. In addition, in our current implementation, we assume that the principal directions remain the same in the same local patch -this assumption may not hold for a large urban area. Hence, it would be an interesting topic to study a global principal direction model for the whole urban area.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Building modeling of Oakland city: (a) input Li-DAR data, point height is illustrated as intensity; (b) a histogram learned from original data showing the seven principal directions of this area, which are also illustrated as arrows of the same color in (a); (c) some building modeling results, both orthogonal corners and non-orthogonal corners are reconstructed correctly.</figDesc><graphic coords="1,384.55,304.06,68.33,70.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Building modeling pipeline: classification separates vegetation points from roofs and ground; planes are extracted from roof patches and boundaries of each plane are detected; finally, building models are reconstructed from boundary points.</figDesc><graphic coords="2,481.72,66.33,58.60,57.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of normal variation measurements: (a) normals of points around a roof ridge; (b) normals of points from (a), distributed in a Gauss sphere. Red/green/blue arrows point to the eigenvectors of C n p with length equal to corresponding eigenvalues; (c) normals of points in the neighborhood of a tree point. Both λ n 1 and λ n 2 are large due to the irregularity of normals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Tree detection and roof patch extraction of a piece of Oakland city: (a) original LiDAR data, color intensity represents the height at each point; (b) green points are detected as tree; (c) different roof patches are labeled with different colors; (d) five features at each point within a highlight area.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: An illustration of the boundary extraction algorithm. Circles represent the LiDAR points projected on the plane. The red circles and red edges connecting them form up the boundary.</figDesc><graphic coords="5,309.98,151.89,64.71,69.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Boundary extraction for part of an industrial site: (b) without any morphological operation; (c) with an opening morphological operation, most artifacts are removed.</figDesc><graphic coords="5,74.04,500.98,64.36,71.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: An illustration for the snapping algorithm: (a) Li-DAR input of a square piece of Denver city; (b) histogram of tangent directions, an interesting observation is that 0 • and 90 • are also counted as principal directions due to the cut area boundary; (c) from left to right, initial point cloud, extracted boundary loops, snapped boundaries, and constructed polygonal model; (d) closeups of boundaries before and after snapping.</figDesc><graphic coords="5,365.56,236.41,64.80,73.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 (</head><label>7</label><figDesc>Figure 7(b) illustrates the direction histogram of a Denver city piece shown in Figure 7(a). The four principal directions detected are 0 • , 43 • , 90 • , 133• respectively, which could be separated into two orthogonal pairs. This phenomenon fits into the observation made by the previous works that orthogonal corners are a common pattern in building footprints. In addition, we notice the 0 • and 90 • directions contain less samples than the other two. In fact, this LiDAR point cloud piece is a square cut from the original data set, hence these two directions represent the directions at the area boundary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>now pick the (b max i , d max k ) pair which maximizes Count(bi, d k ), take line l max : (pb max i ) × d max k = 0 as a boundary segment, and project the continuous boundary points which agree with l max onto l max .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 (</head><label>7</label><figDesc>Figure 7(d) shows a case of snapping. The boundary points shown in the left figure are rather irregular, while in the right figure, points have been snapped to the correct line segments. Note that a red segment and a yellow segment are snapped together in the right figure. This exposes as a vertical wall in the final reconstructed polygonal mesh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: An example of an industrial site: (a) input LiDAR data; (b) detected object patches, different patches are rendered with different colors, ground and noise are dark-grey and black respectively; (c) the reconstructed models, different types of objects are generated in different ways.</figDesc><graphic coords="7,178.83,129.31,94.03,62.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Building modeling from a 1km × 1km piece of Denver city: (a) input LiDAR data; (b) reconstructed building models overlaying with initial point cloud; (c)(d) modeling result viewed from different perspectives.</figDesc><graphic coords="8,80.84,154.43,121.50,101.42" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that LiDAR is composed of several pieces of scans from different perspectives, thus may contain multiple layers at the same x-y coordinate.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Note that the covariance analysis needs enough sample points in the neighborhood to make the matrix nonsingular. Hence, if the number of points in Np is less than some specified number (experimentally, we require at least 10 points), we make the reasonable assumption that this point belongs to the noise category.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>To guarantee the correctness of topology, if a LiDAR point is as-</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>Since we have a closed manifold boundary, we hereby find the nearby boundary points by traveling on the boundary through two opposite directions respectively.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">ACKNOWLEDGEMENT</head><p>We would like to thank the anonymous reviewers for their valuable comments. We gratefully acknowledge the sources of our data sets: Airborne 1 Corp. for Oakland, Sanborn Corp. for Denver, and Chevron Corp. for the industrial site. We thank Suya You and Yuan Li for helpful discussion. This work is partially supported by a Provost's Fellowship from USC.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Heuristic filtering and 3d feature extraction from lidar data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alharthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bethel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPRS Commission III, Symposium</title>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="29" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reconstructing 3d buildings from lidar data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elaksher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bethel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPRS Commission III, Symposium</title>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="102" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Constructing 3d city models by merging aerial and ground views</title>
		<author>
			<persName><forename type="first">C</forename><surname>Früh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zakhor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="52" to="61" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Building footprint extraction and 3-d reconstruction from lidar data</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Haithcoat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Hipple</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ISPRS Joint Workshop on Remote Sensing and Data Fusion over Urban Areas</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="74" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Integrating lidar, aerial image and ground images for complete urban building modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DPVT&apos;06</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="184" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating raster dem from mass points via tin streaming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Isenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shewchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snoeyink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Thirion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, Geographic Information Science</title>
		<meeting>Geographic Information Science</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="186" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<ptr target="http://svmlight.joachims.org/" />
		<title level="m">Svm light</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Aerial lidar data classification using adaboost</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Lodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Fitzpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Helmbold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DIM&apos;07</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="413" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Point primitives for interactive modeling and processing of 3d geometry</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>ETH Zurich</publisher>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Extracting urban features from lidar digital surface models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Priestnall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jaafar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers, Environment and Urban Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="65" to="78" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic generation of high-quality building models from lidar data</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rottensteiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="42" to="50" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tree detection in urban regions using aerial lidar and image data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Secord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zakhor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="196" to="200" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3d building detection and modeling from aerial lidar data</title>
		<author>
			<persName><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2006</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2213" to="2220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A bayesian approach to building footprint extraction from aerial lidar data</title>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Lodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Helmbold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DPVT&apos;06</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="192" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Urban site modeling from lidar</title>
		<author>
			<persName><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, Part III</title>
		<meeting>Part III</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="579" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Topology repair of solid models using skeletons</title>
		<author>
			<persName><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="675" to="685" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
