<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">i-MIX: A DOMAIN-AGNOSTIC STRATEGY FOR CONTRASTIVE REPRESENTATION LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kibok</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yian</forename><surname>Zhu</surname></persName>
							<email>yianz@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
							<email>kihyuks@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
							<email>chunliang@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
							<email>jinwoos@kaist.ac.kr</email>
						</author>
						<author>
							<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
							<email>honglak@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">LG AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amazon</forename><forename type="middle">Web</forename><surname>Services</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Google</forename><surname>Cloud</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ai</forename><forename type="middle">4</forename><surname>Kaist</surname></persName>
						</author>
						<title level="a" type="main">i-MIX: A DOMAIN-AGNOSTIC STRATEGY FOR CONTRASTIVE REPRESENTATION LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Contrastive representation learning has shown to be effective to learn representations from unlabeled data. However, much progress has been made in vision domains relying on data augmentations carefully designed using domain knowledge. In this work, we propose i-Mix, a simple yet effective domain-agnostic regularization strategy for improving contrastive representation learning. We cast contrastive learning as training a non-parametric classifier by assigning a unique virtual class to each data in a batch. Then, data instances are mixed in both the input and virtual label spaces, providing more augmented data during training. In experiments, we demonstrate that i-Mix consistently improves the quality of learned representations across domains, including image, speech, and tabular data. Furthermore, we confirm its regularization effect via extensive ablation studies across model and dataset sizes. The code is available at https://github.com/kibok90/imix.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Representation learning <ref type="bibr" target="#b7">(Bengio et al., 2013</ref>) is a fundamental task in machine learning since the success of machine learning relies on the quality of representation. Self-supervised representation learning (SSL) has been successfully applied in several domains, including image recognition <ref type="bibr" target="#b35">(He et al., 2020;</ref><ref type="bibr" target="#b12">Chen et al., 2020a)</ref>, natural language processing <ref type="bibr" target="#b53">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b17">Devlin et al., 2018)</ref>, robotics <ref type="bibr" target="#b66">(Sermanet et al., 2018;</ref><ref type="bibr" target="#b49">Lee et al., 2019)</ref>, speech recognition <ref type="bibr" target="#b64">(Ravanelli et al., 2020)</ref>, and video understanding <ref type="bibr" target="#b46">(Korbar et al., 2018;</ref><ref type="bibr" target="#b60">Owens &amp; Efros, 2018)</ref>. Since no label is available in the unsupervised setting, pretext tasks are proposed to provide self-supervision: for example, context prediction <ref type="bibr" target="#b21">(Doersch et al., 2015)</ref>, inpainting <ref type="bibr" target="#b63">(Pathak et al., 2016)</ref>, and contrastive learning <ref type="bibr" target="#b81">(Wu et al., 2018b;</ref><ref type="bibr" target="#b40">Hjelm et al., 2019;</ref><ref type="bibr" target="#b35">He et al., 2020;</ref><ref type="bibr" target="#b12">Chen et al., 2020a)</ref>. SSL has also been used as an auxiliary task to improve the performance on the main task, such as generative model learning <ref type="bibr" target="#b11">(Chen et al., 2019)</ref>, semi-supervised learning <ref type="bibr" target="#b84">(Zhai et al., 2019)</ref>, and improving robustness and uncertainty <ref type="bibr" target="#b37">(Hendrycks et al., 2019)</ref>.</p><p>Recently, contrastive representation learning has gained increasing attention by showing state-ofthe-art performance in SSL for large-scale image recognition <ref type="bibr" target="#b35">(He et al., 2020;</ref><ref type="bibr" target="#b12">Chen et al., 2020a)</ref>, which outperforms its supervised pre-training counterpart <ref type="bibr" target="#b34">(He et al., 2016)</ref> on downstream tasks. However, while the concept of contrastive learning is applicable to any domains, the quality of learned representations rely on the domain-specific inductive bias: as anchors and positive samples are obtained from the same data instance, data augmentation introduces semantically meaningful variance for better generalization. To achieve a strong, yet semantically meaningful data augmentation, domain knowledge is required, e.g., color jittering in 2D images or structural information in video understanding. Hence, contrastive representation learning in different domains requires an effort to develop effective data augmentations. Furthermore, while recent works have focused on largescale settings where millions of unlabeled data is available, it would not be practical in real-world applications. For example, in lithography, acquiring data is very expensive in terms of both time and cost due to the complexity of manufacturing process <ref type="bibr" target="#b50">(Lin et al., 2018;</ref><ref type="bibr" target="#b68">Sim et al., 2019)</ref>.</p><p>Meanwhile, MixUp <ref type="bibr" target="#b85">(Zhang et al., 2018)</ref> has shown to be a successful data augmentation for supervised learning in various domains and tasks, including image classification <ref type="bibr" target="#b85">(Zhang et al., 2018)</ref>, generative model learning <ref type="bibr" target="#b52">(Lucas et al., 2018)</ref>, and natural language processing <ref type="bibr" target="#b31">(Guo et al., 2019;</ref><ref type="bibr" target="#b30">Guo, 2020)</ref>.</p><p>In this paper, we explore the following natural, yet important question: is the idea of MixUp useful for unsupervised, self-supervised, or contrastive representation learning across different domains?</p><p>To this end, we propose instance Mix (i-Mix), a domain-agnostic regularization strategy for contrastive representation learning. The key idea of i-Mix is to introduce virtual labels in a batch and mix data instances and their corresponding virtual labels in the input and label spaces, respectively. We first introduce the general formulation of i-Mix, and then we show the applicability of i-Mix to state-ofthe-art contrastive representation learning methods, SimCLR <ref type="bibr" target="#b12">(Chen et al., 2020a)</ref> and MoCo <ref type="bibr" target="#b35">(He et al., 2020)</ref>, and a self-supervised learning method without negative pairs, BYOL <ref type="bibr" target="#b29">(Grill et al., 2020)</ref>.</p><p>Through the experiments, we demonstrate the efficacy of i-Mix in a variety of settings. First, we show the effectiveness of i-Mix by evaluating the discriminative performance of learned representations in multiple domains. Specifically, we adapt i-Mix to the contrastive representation learning methods, advancing state-of-the-art performance across different domains, including image <ref type="bibr" target="#b47">(Krizhevsky &amp; Hinton, 2009;</ref><ref type="bibr" target="#b16">Deng et al., 2009)</ref>, speech <ref type="bibr" target="#b78">(Warden, 2018)</ref>, and tabular <ref type="bibr" target="#b3">(Asuncion &amp; Newman, 2007)</ref> datasets. Then, we study i-Mix in various conditions, including when 1) the model and training dataset is small or large, 2) domain knowledge is limited, and 3) transfer learning.</p><p>Contribution. In summary, our contribution is three-fold:</p><p>• We propose i-Mix, a method for regularizing contrastive representation learning, motivated by MixUp <ref type="bibr" target="#b85">(Zhang et al., 2018)</ref>. We show how to apply i-Mix to state-of-the-art contrastive representation learning methods <ref type="bibr" target="#b12">(Chen et al., 2020a;</ref><ref type="bibr" target="#b35">He et al., 2020;</ref><ref type="bibr" target="#b29">Grill et al., 2020)</ref>.</p><p>• We show that i-Mix consistently improves contrastive representation learning in both vision and non-vision domains. In particular, the discriminative performance of representations learned with i-Mix is on par with fully supervised learning on CIFAR-10/100 <ref type="bibr" target="#b47">(Krizhevsky &amp; Hinton, 2009)</ref> and Speech Commands <ref type="bibr" target="#b78">(Warden, 2018)</ref>.</p><p>• We verify the regularization effect of i-Mix in a variety of settings. We empirically observed that i-Mix significantly improves contrastive representation learning when 1) the training dataset size is small, or 2) the domain knowledge for data augmentations is not enough.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Self-supervised representation learning (SSL) aims at learning representations from unlabeled data by solving a pretext task that is derived from self-supervision. Early works on SSL proposed pretext tasks based on data reconstruction by autoencoding <ref type="bibr" target="#b6">(Bengio et al., 2007)</ref>, such as context prediction <ref type="bibr" target="#b21">(Doersch et al., 2015)</ref> and inpainting <ref type="bibr" target="#b63">(Pathak et al., 2016)</ref>. Decoder-free SSL has made a huge progress in recent years. Exemplar CNN <ref type="bibr" target="#b22">(Dosovitskiy et al., 2014)</ref> learns by classifying individual instances with data augmentations. SSL of visual representation, including colorization <ref type="bibr" target="#b34">(Zhang et al., 2016)</ref>, solving jigsaw puzzles <ref type="bibr" target="#b56">(Noroozi &amp; Favaro, 2016)</ref>, counting the number of objects <ref type="bibr" target="#b57">(Noroozi et al., 2017)</ref>, rotation prediction <ref type="bibr" target="#b26">(Gidaris et al., 2018)</ref>, next pixel prediction <ref type="bibr" target="#b59">(Oord et al., 2018;</ref><ref type="bibr" target="#b36">Hénaff et al., 2019)</ref>, and combinations of them <ref type="bibr" target="#b20">(Doersch &amp; Zisserman, 2017;</ref><ref type="bibr" target="#b44">Kim et al., 2018;</ref><ref type="bibr" target="#b58">Noroozi et al., 2018)</ref> often leverages image-specific properties to design pretext tasks. Meanwhile, alhough deep clustering <ref type="bibr" target="#b8">(Caron et al., 2018;</ref><ref type="bibr">2019;</ref><ref type="bibr" target="#b2">Asano et al., 2020)</ref> is often distinguished from SSL, it also leverages unsupervised clustering assignments as self-supervision for representation learning.</p><p>Contrastive representation learning has gained lots of attention for SSL <ref type="bibr" target="#b35">(He et al., 2020;</ref><ref type="bibr" target="#b12">Chen et al., 2020a)</ref>. As opposed to early works on exemplar CNN <ref type="bibr" target="#b22">(Dosovitskiy et al., 2014;</ref><ref type="bibr">2015)</ref>, contrastive learning maximizes similarities of positive pairs while minimizes similarities of negative pairs instead of training an instance classifier. As the choice of negative pairs is crucial for the quality of learned representations, recent works have carefully designed them. Memory-based approaches <ref type="bibr" target="#b81">(Wu et al., 2018b;</ref><ref type="bibr" target="#b40">Hjelm et al., 2019;</ref><ref type="bibr" target="#b4">Bachman et al., 2019;</ref><ref type="bibr" target="#b54">Misra &amp; van der Maaten, 2020;</ref><ref type="bibr" target="#b73">Tian et al., 2020a</ref>) maintain a memory bank of embedding vectors of instances to keep negative samples, where the memory is updated with embedding vectors extracted from previous batches. In addition, MoCo <ref type="bibr" target="#b35">(He et al., 2020)</ref> showed that differentiating the model for anchors and positive/negative samples is effective, where the model for positive/negative samples is updated by the exponential moving average of the model for anchors. On the other hand, recent works <ref type="bibr" target="#b82">(Ye et al., 2019;</ref><ref type="bibr" target="#b54">Misra &amp; van der Maaten, 2020;</ref><ref type="bibr" target="#b12">Chen et al., 2020a;</ref><ref type="bibr" target="#b73">Tian et al., 2020a)</ref> showed that learning invariance to different views is important in contrastive representation learning. The views can be generated through data augmentations carefully designed using domain knowledge <ref type="bibr" target="#b12">(Chen et al., 2020a)</ref>, splitting input channels <ref type="bibr" target="#b73">(Tian et al., 2020a)</ref>, or borrowing the idea of other pretext tasks, such as creating jigsaw puzzles or rotating inputs <ref type="bibr" target="#b54">(Misra &amp; van der Maaten, 2020)</ref>. In particular, SimCLR <ref type="bibr" target="#b12">(Chen et al., 2020a)</ref> showed that a simple memory-free approach with a large batch size and strong data augmentations has a comparable performance to memory-based approaches. InfoMin <ref type="bibr" target="#b74">(Tian et al., 2020b)</ref> further studied a way to generate good views for contrastive representation learning and achieved state-of-the-art performance by combining prior works. Different from other contrastive representation learning methods, BYOL <ref type="bibr" target="#b29">(Grill et al., 2020)</ref> does not require negative pairs, where the proposed pretext task aims at predicting latent representations of one view from another. While prior works have focused on SSL on large-scale visual recognition tasks, our work focuses on contrastive representation learning in both small-and large-scale settings in different domains.</p><p>Data augmentation is a technique to increase the diversity of data, especially when training data are not enough for generalization. Since the augmented data must be understood as the original data, data augmentations are carefully designed using the domain knowledge about images <ref type="bibr" target="#b19">(DeVries &amp; Taylor, 2017b;</ref><ref type="bibr" target="#b14">Cubuk et al., 2019a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b87">Zhong et al., 2020)</ref>, speech <ref type="bibr" target="#b0">(Amodei et al., 2016;</ref><ref type="bibr" target="#b62">Park et al., 2019)</ref>, or natural languages <ref type="bibr" target="#b86">(Zhang et al., 2015;</ref><ref type="bibr" target="#b79">Wei &amp; Zou, 2019)</ref>.</p><p>Some works have studied data augmentation with less domain knowledge: DeVries &amp; Taylor (2017a) proposed a domain-agnostic augmentation strategy by first encoding the dataset and then applying augmentations in the feature space. MixUp <ref type="bibr" target="#b85">(Zhang et al., 2018)</ref> is an effective data augmentation strategy in supervised learning, which performs vicinal risk minimization instead of empirical risk minimization, by linearly interpolating input data and their labels on the data and label spaces, respectively. On the other hand, MixUp has also shown its effectiveness in other tasks and non-vision domains, including generative adversarial networks <ref type="bibr" target="#b52">(Lucas et al., 2018)</ref>, improved robustness and uncertainty <ref type="bibr" target="#b38">(Hendrycks et al., 2020)</ref>, and sentence classification in natural language processing <ref type="bibr" target="#b30">(Guo, 2020;</ref><ref type="bibr" target="#b31">Guo et al., 2019)</ref>. Other variations have also been investigated by interpolating in the feature space <ref type="bibr" target="#b76">(Verma et al., 2019)</ref> or leveraging domain knowledge <ref type="bibr" target="#b83">(Yun et al., 2019)</ref>. MixUp would not be directly applicable to some domains, such as point clouds, but its adaptation can be effective <ref type="bibr" target="#b33">(Harris et al., 2020)</ref>. i-Mix is a kind of data augmentation for better generalization in contrastive representation learning, resulting in better performances on downstream tasks.</p><p>Concurrent works have leveraged the idea of MixUp for contrastive representation learning. As discussed in Section 3.3, only input data can be mixed for improving contrastive representation learning <ref type="bibr" target="#b67">(Shen et al., 2020;</ref><ref type="bibr" target="#b77">Verma et al., 2020;</ref><ref type="bibr" target="#b88">Zhou et al., 2020)</ref>, which can be considered as injecting data-driven noises. <ref type="bibr" target="#b42">Kalantidis et al. (2020)</ref> mixed hard negative samples on the embedding space. <ref type="bibr" target="#b45">Kim et al. (2020)</ref> reported similar observations to ours but focused on small image datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head><p>In this section, we review MixUp <ref type="bibr" target="#b85">(Zhang et al., 2018)</ref> in supervised learning and present i-Mix in contrastive learning <ref type="bibr" target="#b35">(He et al., 2020;</ref><ref type="bibr" target="#b12">Chen et al., 2020a;</ref><ref type="bibr" target="#b29">Grill et al., 2020)</ref>. Throughout this section, let X be a data space, R D be a D-dimensional embedding space, and a model f : X → R D be a mapping between them. For conciseness,</p><formula xml:id="formula_0">f i = f (x i ) and fi = f (x i ) for x i , xi ∈ X</formula><p>, and model parameters are omitted in loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MIXUP IN SUPERVISED LEARNING</head><p>Suppose an one-hot label y i ∈ {0, 1} C is assigned to a data x i , where C is the number of classes. Let a linear classifier predicting the labels consists of weight vectors {w 1 , . . . , w C }, where w c ∈ R D .<ref type="foot" target="#foot_0">1</ref> Then, the cross-entropy loss for supervised learning is defined as:</p><formula xml:id="formula_1">Sup (x i , y i ) = − C c=1 y i,c log exp(w c f i ) C k=1 exp(w k f i )</formula><p>.</p><p>(1)</p><p>While the cross-entropy loss is widely used for supervised training of deep neural networks, there are several challenges of training with the cross-entropy loss, such as preventing overfitting or networks being overconfident. Several regularization techniques have been proposed to alleviate these issues, including label smoothing <ref type="bibr" target="#b71">(Szegedy et al., 2016)</ref>, adversarial training <ref type="bibr" target="#b55">(Miyato et al., 2018)</ref>, and confidence calibration <ref type="bibr" target="#b48">(Lee et al., 2018)</ref>.</p><p>MixUp <ref type="bibr" target="#b85">(Zhang et al., 2018)</ref> is an effective regularization with negligible computational overhead. It conducts a linear interpolation of two data instances in both input and label spaces and trains a model by minimizing the cross-entropy loss defined on the interpolated data and labels. Specifically, for two labeled data (x i , y i ), (x j , y j ), the MixUp loss is defined as follows:</p><p>MixUp Sup</p><formula xml:id="formula_2">(x i , y i ), (x j , y j ); λ = Sup (λx i + (1 − λ)x j , λy i + (1 − λ)y j ),<label>(2)</label></formula><p>where λ ∼ Beta(α, α) is a mixing coefficient sampled from the beta distribution. MixUp is a vicinal risk minimization method <ref type="bibr" target="#b10">(Chapelle et al., 2001)</ref> that augments data and their labels in a data-driven manner. Not only improving the generalization on the supervised task, it also improves adversarial robustness <ref type="bibr" target="#b61">(Pang et al., 2019)</ref> and confidence calibration <ref type="bibr" target="#b72">(Thulasidasan et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">i-MIX IN CONTRASTIVE LEARNING</head><p>We introduce instance mix (i-Mix), a data-driven augmentation strategy for contrastive representation learning to improve the generalization of learned representations. Intuitively, instead of mixing class labels, i-Mix interpolates their virtual labels, which indicates their identity in a batch.</p><p>Let B = {(x i , xi )} N i=1 be a batch of data pairs, where N is the batch size, x i , xi ∈ X are two views of the same data, which are usually generated by different augmentations. For each anchor x i , we call xi and xj =i positive and negative samples, respectively.<ref type="foot" target="#foot_1">2</ref> Then, the model f learns to maximize similarities of positive pairs (instances from the same data) while minimize similarities of negative pairs (instances from different data) in the embedding space. The output of f is L2-normalized, which has shown to be effective <ref type="bibr" target="#b80">(Wu et al., 2018a;</ref><ref type="bibr" target="#b35">He et al., 2020;</ref><ref type="bibr" target="#b12">Chen et al., 2020a)</ref>. Let v i ∈ {0, 1} N be the virtual label of x i and xi in a batch B, where v i,i = 1 and v i,j =i = 0. For a general sample-wise contrastive loss with virtual labels (x i , v i ), the i-Mix loss is defined as follows:</p><formula xml:id="formula_3">i-Mix (x i , v i ), (x j , v j ); B, λ = (Mix(x i , x j ; λ), λv i + (1 − λ)v j ; B),<label>(3)</label></formula><p>where λ ∼ Beta(α, α) is a mixing coefficient and Mix is a mixing operator, which can be adapted depending on target domains: for example, MixUp(x i , x j ; λ) = λx i + (1−λ)x j <ref type="bibr" target="#b85">(Zhang et al., 2018)</ref> when data values are continuous, and CutMix(x i , x j ; λ) = M λ x i + (1−M λ ) x j <ref type="bibr" target="#b83">(Yun et al., 2019)</ref> when data values have a spatial correlation with neighbors, where M λ is a binary mask filtering out a region whose relative area is (1−λ), and is an element-wise multiplication. Note that some mixing operators might not work well for some domains: for example, CutMix would not be valid when data values and their spatial neighbors have no correlation. However, the MixUp operator generally works well across domains including image, speech, and tabular; we use it for i-Mix formulations and experiments, unless otherwise specified. In the following, we show how to apply i-Mix to contrastive representation learning methods.</p><p>SimCLR <ref type="bibr" target="#b12">(Chen et al., 2020a</ref>) is a simple contrastive representation learning method without a memory bank, where each anchor has one positive sample and (2N −2) negative samples. Let</p><p>x N +i = xi for conciseness. Then, the (2N −1)-way discrimination loss is written as follows:</p><formula xml:id="formula_4">SimCLR (x i ; B) = − log exp s(f i , f (N +i) mod 2N )/τ 2N k=1,k =i exp s(f i , f k )/τ , (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where τ is a temperature scaling parameter and s(f, f ) = (f f )/ f f is the inner product of two L2-normalized vectors. In this formulation, i-Mix is not directly applicable because virtual labels are defined differently for each anchor. <ref type="foot" target="#foot_2">3</ref> To resolve this issue, we simplify the formulation of SimCLR by excluding anchors from negative samples. Then, with virtual labels, the N -way discrimination loss is written as follows:</p><formula xml:id="formula_6">N-pair (x i , v i ; B) = − N n=1 v i,n log exp s(f i , fn )/τ N k=1 exp s(f i , fk )/τ ,<label>(5)</label></formula><p>where we call it the N-pair contrastive loss, as the formulation is similar to the N-pair loss in the context of metric learning <ref type="bibr" target="#b69">(Sohn, 2016)</ref>. <ref type="foot" target="#foot_3">4</ref> For two data instances (x i , v i ), (x j , v j ) and a batch of data </p><formula xml:id="formula_7">pairs B = {(x i , xi )} N i=1</formula><p>, the i-Mix loss is defined as follows:</p><formula xml:id="formula_8">i-Mix N-pair (x i , v i ), (x j , v j ); B, λ = N-pair (λx i + (1 − λ)x j , λv i + (1 − λ)v j ; B).<label>(6)</label></formula><p>Algorithm 1 provides the pseudocode of i-Mix on N-pair contrastive learning for one iteration.<ref type="foot" target="#foot_4">5</ref> </p><p>Pair relations in contrastive loss. To use contrastive loss for representation learning, one needs to properly define a pair relation {(x i , xi )} N i=1 . For contrastive representation learning, where semantic class labels are not provided, the pair relation would be defined in that 1) a positive pair, x i and xi , are different views of the same data and 2) a negative pair, x i and xj =i , are different data instances. For supervised representation learning, x i and xi are two data instances from the same class, while x i and xj =i are from different classes. Note that two augmented versions of the same data also belong to the same class, so they can also be considered as a positive pair. i-Mix is not limited to self-supervised contrastive representation learning, but it can also be used as a regularization method for supervised contrastive representation learning <ref type="bibr" target="#b43">(Khosla et al., 2020)</ref> or deep metric learning <ref type="bibr" target="#b69">(Sohn, 2016;</ref><ref type="bibr">Movshovitz-Attias et al., 2017)</ref>.</p><p>MoCo <ref type="bibr" target="#b35">(He et al., 2020)</ref>. In contrastive representation learning, the number of negative samples affects the quality of learned representations <ref type="bibr" target="#b1">(Arora et al., 2019)</ref>. Because SimCLR mines negative samples in the current batch, having a large batch size is crucial, which often requires a lot of computational resources <ref type="bibr" target="#b12">(Chen et al., 2020a)</ref>. For efficient training, recent works have maintained a memory bank M = {µ k } K k=1 , which is a queue of previously extracted embedding vectors, where K is the size of the memory bank <ref type="bibr" target="#b81">(Wu et al., 2018b;</ref><ref type="bibr" target="#b35">He et al., 2020;</ref><ref type="bibr" target="#b73">Tian et al., 2020a;</ref><ref type="bibr">b)</ref>. In addition, MoCo introduces an exponential moving average (EMA) model to extract positive and negative embedding vectors, whose parameters are updated as θ f EMA ← mθ f EMA + (1 − m)θ f , where m ∈ [0, 1) is a momentum coefficient and θ is model parameters. The loss is written as follows:</p><formula xml:id="formula_9">MoCo (x i ; B, M) = − log exp s(f i , f EMA i )/τ exp s(f i , f EMA i )/τ + K k=1 exp s(f i , µ k )/τ . (<label>7</label></formula><formula xml:id="formula_10">)</formula><p>The memory bank M is then updated with { f EMA i } in the first-in first-out order. In this (K+1)-way discrimination loss, data pairs are independent to each other, such that i-Mix is not directly applicable because virtual labels are defined differently for each anchor. To overcome this issue, we include the positive samples of other anchors as negative samples, similar to the N-pair contrastive loss in Eq. ( <ref type="formula" target="#formula_6">5</ref>). Let ṽi ∈ {0, 1} N +K be a virtual label indicating the positive sample of each anchor, where ṽi,i = 1 and ṽi,j =i = 0. Then, the (N +K)-way discrimination loss is written as follows:</p><formula xml:id="formula_11">MoCo (x i , ṽi ; B, M) = − N n=1 ṽi,n log exp s(f i , f EMA n )/τ N k=1 exp s(f i , f EMA k )/τ + K k=1 exp s(f i , µ k )/τ . (<label>8</label></formula><formula xml:id="formula_12">)</formula><p>As virtual labels are bounded in the same set in this formulation, i-Mix is directly applicable: for two data instances (x i , ṽi ), (x j , ṽj ), a batch of data pairs B = {(x i , xi )} N i=1 , and the memory bank M, the i-Mix loss is defined as follows:</p><formula xml:id="formula_13">i-Mix MoCo (x i , ṽi ), (x j , ṽj ); B, M, λ = MoCo (λx i + (1 − λ)x j , λṽ i + (1 − λ)ṽ j ; B, M). (9)</formula><p>BYOL <ref type="bibr" target="#b29">(Grill et al., 2020)</ref>. Different from other contrastive representation learning methods, BYOL is a self-supervised representation learning method without contrasting negative pairs. For two views of the same data x i , xi ∈ X , the model f learns to predict a view embedded with the EMA model f EMA i from its embedding f i . Specifically, an additional prediction layer g is introduced, such that the difference between g(f i ) and f EMA i is learned to be minimized. The BYOL loss is written as follows:</p><formula xml:id="formula_14">BYOL (x i , xi ) = g(f i )/ g(f i ) − fi / fi 2 = 2 − 2 • s(g(f i ), fi ). (<label>10</label></formula><formula xml:id="formula_15">)</formula><p>This formulation can be represented in the form of the general contrastive loss in Eq. ( <ref type="formula" target="#formula_3">3</ref>), as the second view xi can be accessed from the batch B with its virtual label v i . To derive i-Mix in BYOL, let F = [ f1 / f1 , . . ., fN / fN ] ∈ R D×N be the collection of L2-normalized embedding vectors of the second views, such that fi / fi = F v i . Then, the BYOL loss is written as follows:</p><formula xml:id="formula_16">BYOL (x i , v i ; B) = g(f i )/ g(f i ) − F v i 2 = 2 − 2 • s(g(f i ), F v i ).<label>(11)</label></formula><p>For two data instances (x i , v i ), (x j , v j ) and a batch of data pairs B = {(x i , xi )} N i=1 , the i-Mix loss is defined as follows:</p><formula xml:id="formula_17">i-Mix BYOL (x i , v i ), (x j , v j ); B, λ = BYOL (λx i + (1 − λ)x j , λv i + (1 − λ)v j ; B).</formula><p>(12)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">INPUTMIX</head><p>The contribution of data augmentations to the quality of learned representations is crucial in contrastive representation learning. For the case when the domain knowledge about efficient data augmentations is limited, we propose to apply InputMix together with i-Mix, which mixes input data but not their labels. This method can be viewed as introducing structured noises driven by auxiliary data to the principal data with the largest mixing coefficient λ, and the label of the principal data is assigned to the mixed data <ref type="bibr" target="#b67">(Shen et al., 2020;</ref><ref type="bibr" target="#b77">Verma et al., 2020;</ref><ref type="bibr" target="#b88">Zhou et al., 2020)</ref>. We applied InputMix and i-Mix together on image datasets in Table <ref type="table" target="#tab_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we demonstrate the effectiveness of i-Mix. In all experiments, we conduct contrastive representation learning on a pretext dataset and evaluate the quality of representations via supervised classification on a downstream dataset. We report the accuracy averaged over up to five runs. In the first stage, a convolutional neural network (CNN) or multilayer perceptron (MLP) followed by the two-layer MLP projection head is trained on an unlabeled dataset. Then, we replace the projection head with a linear classifier and train only the linear classifier on a labeled dataset for downstream task. Except for transfer learning, datasets for the pretext and downstream tasks are the same. For i-Mix, we sample a mixing coefficient λ ∼ Beta(α, α) for each data, where α = 1 unless otherwise stated. <ref type="foot" target="#foot_5">6</ref>Additional details for the experimental settings and more experiments can be found in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EXPERIMENTAL SETUP</head><p>Baselines and datasets. We consider 1) N-pair contrastive learning as a memory-free contrastive learning method,<ref type="foot" target="#foot_6">7</ref> 2) MoCo v2 <ref type="bibr" target="#b35">(He et al., 2020;</ref><ref type="bibr" target="#b13">Chen et al., 2020b)</ref> <ref type="foot" target="#foot_7">8</ref> as a memory-based contrastive learning method, and 3) BYOL <ref type="bibr" target="#b29">(Grill et al., 2020)</ref>, which is a self-supervised learning method without negative pairs. We apply i-Mix to these methods and compare their performances. To show the effectiveness of i-Mix across domains, we evaluate the methods on datasets from multiple domains, including image, speech, and tabular datasets.</p><p>CIFAR-10/100 <ref type="bibr" target="#b47">(Krizhevsky &amp; Hinton, 2009)</ref>   random resized cropping, horizontal flipping, color jittering, gray scaling, and Gaussian blurring for ImageNet, which has shown to be effective <ref type="bibr" target="#b12">(Chen et al., 2020a;</ref><ref type="bibr">b)</ref>. We use ResNet-50 <ref type="bibr" target="#b34">(He et al., 2016)</ref> as a backbone network. Models are trained with a batch size of 256 (i.e., 512 including augmented data) for up to 4000 epochs on CIFAR-10 and 100, and with a batch size of 512 for 800 epochs on ImageNet. For ImageNet experiments, we use the CutMix <ref type="bibr" target="#b83">(Yun et al., 2019)</ref> version of i-Mix.</p><p>The Speech Commands dataset <ref type="bibr" target="#b78">(Warden, 2018)</ref>  For Higgs, we use a subset of 100k and 1M training data to experiment at a different scale. Since the domain knowledge for data augmentations on tabular data is limited, only a masking noise with the probability 0.2 is considered as a data augmentation. We use a 5-layer MLP with batch normalization <ref type="bibr" target="#b41">(Ioffe &amp; Szegedy, 2015)</ref> as a backbone network. Models are trained with a batch size of 512 for 500 epochs. We use α = 2 for CovType and Higgs100k, as it is slightly better than α = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MAIN RESULTS</head><p>Table <ref type="table" target="#tab_0">1</ref> shows the wide applicability of i-Mix to state-of-the-art contrastive representation learning methods in multiple domains. i-Mix results in consistent improvements on the classification accuracy, e.g., up to 6.5% when i-Mix is applied to MoCo v2 on CIFAR-100. Interestingly, we observe that linear classifiers on top of representations learned with i-Mix without fine-tuning the pre-trained part often yield a classification accuracy on par with simple end-to-end supervised learning from random initialization, e.g., i-Mix vs. end-to-end supervised learning performance is 96.3% vs. 95.5% on CIFAR-10, 78.6% vs. 78.9% on CIFAR-100, and 98.2% vs. 98.0% on Speech Commands.<ref type="foot" target="#foot_8">9</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">REGULARIZATION EFFECT OF i-MIX</head><p>A better regularization method often benefits from longer training of deeper models, which is more critical when training on a small dataset. To investigate the regularization effect of i-Mix, we first  <ref type="figure" target="#fig_0">1</ref> shows the performance of MoCo v2 (solid box) and i-Mix (dashed box). The improvement by applying i-Mix to MoCo v2 is consistent over the different architecture size and the number of training epochs. Deeper models benefit from i-Mix, achieving 96.7% on CIFAR-10 and 79.1% on CIFAR-100 when the backbone network is ResNet-152.</p><p>On the other hand, models trained without i-Mix start to show decrease in performance, possibly due to overfitting to the pretext task when trained longer. The trend clearly shows that i-Mix results in better representations via improved regularization.</p><p>Next, we study the effect of i-Mix with varying dataset sizes for the pretext tasks. Table <ref type="table" target="#tab_2">2</ref> shows the effect of i-Mix on large-scale datasets<ref type="foot" target="#foot_9">10</ref> from image and tabular domains. We observe that i-Mix is particularly effective when the amount of training data is reduced, e.g., ImageNet-100 consists of images from 100 classes, thus has only 10% of training data compared to ImageNet-1k. However, the performance gain is reduced when the amount of training data is large. we further study representations learned with different pretext dataset sizes from 1% to 100% of the ImageNet training data in Figure <ref type="figure">2</ref>. Here, different from ImageNet-100, we reduce the amount of data for each class, but maintain the number of classes the same. We observe that the performance gain by i-Mix is more significant when the size of the pretext dataset is small. Our study suggests that i-Mix is effective for regularizing self-supervised representation learning when training from a limited amount of data. We believe that this is aligned with findings in <ref type="bibr" target="#b85">Zhang et al. (2018)</ref> for MixUp in supervised learning. Finally, when a large-scale unlabeled dataset is available, we expect i-Mix would still be useful in obtaining better representations when trained longer with deeper and larger models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">CONTRASTIVE LEARNING WITHOUT DOMAIN-SPECIFIC DATA AUGMENTATION</head><p>Data augmentations play a key role in contrastive representation learning, and therefore it raises a question when applying them to domains with a limited or no knowledge of such augmentations.</p><p>In this section, we study the effectiveness of i-Mix as a domain-agnostic strategy for contrastive representation learning, which can be adapted to different domains. Table <ref type="table" target="#tab_3">3</ref> shows the performance of MoCo v2 and i-Mix with and without data augmentations. We observe significant performance gains with i-Mix when other data augmentations are not applied. For example, compared to the accuracy of 93.5% on CIFAR-10 when other data augmentations are applied, contrastive learning achieves 47.7% when trained without any data augmentations. This suggests that data augmentation is an essential part for the success of contrastive representation learning <ref type="bibr" target="#b12">(Chen et al., 2020a)</ref>. However, i-Mix is able to learn meaningful representations without other data augmentations and achieves the accuracy of 83.4% on CIFAR-10.  In Table <ref type="table" target="#tab_3">3</ref>, InputMix is applied together with i-Mix to further improve the performance on image datasets. For each principal data, we mix two auxiliary data, with mixing coefficients (0.5λ 1 + 0.5, 0.5λ 2 , 0.5λ 3 ), where λ 1 , λ 2 , λ 3 ∼ Dirichlet(1, 1, 1).<ref type="foot" target="#foot_10">11</ref> In the above example, while i-Mix is better than baselines, adding InputMix further improves the performance of i-Mix, i.e., from 75.1% to 83.4% on CIFAR-10, and from 50.7% to 54.0% on CIFAR-100. This confirms that InputMix can further improve the performance when domain-specific data augmentations are not available, as discussed in Section 3.3.</p><p>Moreover, we verify its effectiveness on other domains beyond the image domain. For example, the performance improves from 76.9% to 92.8% on the Speech Commands dataset when we assume no other data augmentations are available. We also observe consistent improvements in accuracy for tabular datasets, even when the training dataset size is large. Although the domain knowledge for data augmentations is important to achieve state-of-the-art results, our demonstration shows the potential of i-Mix to be used for a wide range of application domains where domain knowledge is particularly limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">TRANSFERABILITY OF i-MIX</head><p>In this section, we show the improved transferability of the representations learned with i-Mix. The results are provided in Table <ref type="table" target="#tab_5">4</ref>. First, we train linear classifiers with downstream datasets different from the pretext dataset used to train backbone networks and evaluate their performance, e.g., CIFAR-10 as pretext and CIFAR-100 as downstream datasets or vice versa. We observe consistent performance gains when learned representations from one dataset are evaluated on classification tasks of another dataset. Next, we transfer representations trained on ImageNet to the PASCAL VOC object detection task <ref type="bibr" target="#b24">(Everingham et al., 2010)</ref>. We follow the settings in prior works <ref type="bibr" target="#b35">(He et al., 2020;</ref><ref type="bibr" target="#b13">Chen et al., 2020b)</ref>: the parameters of the pre-trained ResNet-50 are transferred to a Faster R-CNN detector with the ResNet50-C4 backbone <ref type="bibr" target="#b65">(Ren et al., 2015)</ref>, and fine-tuned end-to-end on the VOC 07+12 trainval dataset and evaluated on the VOC 07 test dataset. We report the average precision (AP) averaged over IoU thresholds between 50% to 95% at a step of 5%, and AP 50 and AP 75 , which are AP values when IoU threshold is 50% and 75%, respectively. Similar to Table <ref type="table" target="#tab_2">2</ref>, we observe small but consistent performance gains in all metrics. Those results confirm that i-Mix improves the quality of learned representations, such that performances on downstream tasks are improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We propose i-Mix, a domain-agnostic regularization strategy applicable to a class of self-supervised learning. The key idea of i-Mix is to introduce a virtual label to each data instance, and mix both inputs and the corresponding virtual labels. We show that i-Mix is applicable to state-of-the-art self-supervised representation learning methods including SimCLR, MoCo, and BYOL, which consistently improves the performance in a variety of settings and domains. Our experimental results indicate that i-Mix is particularly effective when the training dataset size is small or data augmentation is not available, each of which are prevalent in practice.</p><p>Then, the head of the CNN is replaced with a linear classifier, and only the linear classifier is trained with the labeled downstream dataset. For the second stage, we use a batch size of 256 with the SGD optimizer with a momentum of 0.9 and an initial learning rate chosen among {1, 3, 5, 10, 30, 50, 70} over 100 epochs, where the learning rate is decayed by 0.2 after 80, 90, 95 epochs. No weight decay is used at the second stage. The quality of representation is evaluated by the top-1 accuracy on the downstream task. We sample a single mixing coefficient λ ∼ Beta(1, 1) for each training batch. The temperature is set to τ = 0.2. Note that the optimal distribution of λ and the optimal value of τ varies over different architectures, methods, and datasets, but the choices above result in a reasonably good performance. The memory bank size of MoCo is 65536 for ImageNet and 4096 for other datasets, and the momentum for the exponential moving average (EMA) update is 0.999 for MoCo and BYOL. We do not symmetrize the BYOL loss, as it does not significantly improve the performance while increasing computational complexity.</p><p>For data augmentation, we follow Chen et al. (2020a): We apply a set of data augmentations randomly in sequence including resized cropping <ref type="bibr" target="#b70">(Szegedy et al., 2015)</ref>, horizontal flipping with a probability of 0.5, color jittering,<ref type="foot" target="#foot_14">15</ref> and gray scaling with a probability of 0.2. A Gaussian blurring with σ ∈ [0.1, 2] and kernel size of 10% of the image height/width is applied for ImageNet. For evaluation on downstream tasks, we apply padded cropping with the pad size of 4 and horizontal flipping for CIFAR-10 and 100, and resized cropping and horizontal flipping for ImageNet.</p><p>Speech. In the experiments on Speech Commands <ref type="bibr" target="#b78">(Warden, 2018)</ref>, the network is the same with the image domain experiments, except that the number of input channels is one instead of three. The temperature is set to τ = 0.5 for the standard setting and τ = 0.2 for the no augmentation setting. 10% of silence data (all zero) are added when training. At the first stage, the model is trained with the SGD optimizer with a momentum of 0.9 and an initial learning rate of 0.125 over 500 epochs, where the learning rate decays by 0.1 after 300 and 400 epochs and the weight decay is 0.0001. The other settings are the same with the experiments on CIFAR.</p><p>For data augmentation, <ref type="foot" target="#foot_15">16</ref> we apply a set of data augmentations randomly in sequence including changing amplitude, speed, and pitch in time domain, stretching, time shifting, and adding background noise in frequency domain. Each data augmentation is applied with a probability of 0.5. Augmented data are then transformed to the mel spectogram in the size of 32 × 32.</p><p>Tabular. In the experiments on CovType and Higgs <ref type="bibr" target="#b3">(Asuncion &amp; Newman, 2007)</ref>, we take a fivelayer MLP with batch normalization as a backbone network. The output dimensions of layers are <ref type="bibr">(2048-2048-4096-4096-8192)</ref>, where all layers have batch normalization followed by ReLU except for the last layer. The last layer activation is maxout <ref type="bibr" target="#b27">(Goodfellow et al., 2013)</ref> with 4 sets, such that the output dimension is 2048. On top of this five-layer MLP, we attach two-layer MLP (2048-128) as a projection head. We sample a single mixing coefficient λ ∼ Beta(α, α) for each training batch, where α = 2 for CovType and Higgs100k, and α = 1 for Higgs1M. The temperature is set to τ = 0.1. The other settings are the same with the experiments on CIFAR, except that the batch size is 512 and the number of training epochs is 500. At the second stage, the MLP head is replaced with a linear classifier. For Higgs, the classifier is computed by linear regression from the feature matrix obtained without data augmentation to the label matrix using the pseudoinverse. Since the prior knowledge on tabular data is very limited, only the masking noise with a probability of 0.2 is considered as a data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 VARIATIONS OF i-MIX</head><p>We compare the MixUp <ref type="bibr" target="#b85">(Zhang et al., 2018)</ref> and CutMix <ref type="bibr" target="#b83">(Yun et al., 2019)</ref> variation of i-Mix on N-pair contrastive learning and SimCLR. To distinguish them, we call them i-MixUp and i-CutMix, respectively. To be fair with the memory usage in the pretext task stage, we reduce the batch size of i-MixUp and i-CutMix by half (256 to 128) for SimCLR. Following the learning rate adjustment strategy in <ref type="bibr" target="#b28">Goyal et al. (2017)</ref>, we also decrease the learning rate by half (0.125 to 0.0625) when the batch size is reduced. We note that i-MixUp and i-CutMix on SimCLR take approximately 2.5 times more training time to achieve the same number of training epochs. The results are provided in Table <ref type="table">C</ref>.1. We first verify that the N-pair formulation results in no worse performance than that of SimCLR. This justifies to conduct experiments using the N-pair formulation instead of that of</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>Loss computation for i-Mix on N-pair contrastive learning in PyTorch-like style. a, b = aug(x), aug(x) # two different views of input x lam = Beta(alpha, alpha).sample() # mixing coefficient randidx = randperm(len(x)) a = lam * a + (1-lam) * a[randidx] logits = matmul(normalize(model(a)), normalize(model(b)).T) / t loss = lam * CrossEntropyLoss(logits, arange(len(x))) + \(1-lam) * CrossEntropyLoss(logits, randidx)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of performance gains by applying i-Mix to MoCo v2 with different model sizes and number of epochs on CIFAR-10 and 100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>consist of 50k training and 10k test images, and ImageNet<ref type="bibr" target="#b16">(Deng et al., 2009)</ref> has 1.3M training and 50k validation images, where we use them for evaluation. For ImageNet, we also use a subset of randomly chosen 100 classes out of 1k classes to experiment at a different scale. We apply a set of data augmentations randomly in sequence including ± 0.1 95.6 ± 0.2 93.5 ± 0.2 96.1 ± 0.1 94.2 ± 0.2 96.3 ± 0.2 CIFAR-100 70.8 ± 0.4 75.8 ± 0.3 71.6 ± 0.1 78.1 ± 0.3 72.7 ± 0.4 78.6 ± 0.2 Speech Commands 94.9 ± 0.1 98.3 ± 0.1 96.3 ± 0.1 98.4 ± 0.0 94.8 ± 0.2 98.3 ± 0.0 Comparison of contrastive representation learning methods and i-Mix in different domains.</figDesc><table><row><cell>Domain</cell><cell>Dataset</cell><cell>N-pair</cell><cell>+ i-Mix MoCo v2 + i-Mix</cell><cell>BYOL</cell><cell>+ i-Mix</cell></row><row><cell cols="6">Image CIFAR-10 93.3 Tabular CovType 68.5 ± 0.3 72.1 ± 0.2 70.5 ± 0.2 73.1 ± 0.1 72.1 ± 0.2 74.1 ± 0.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of MoCo v2 and i-Mix on large-scale datasets. ± 0.2 96.1 ± 0.1 71.6 ± 0.1 78.1 ± 0.3 96.3 ± 0.1 98.4 ± 0.0 70.5 ± 0.2 73.1 ± 0.1</figDesc><table><row><cell cols="2">Domain Image Domain Tabular</cell><cell>Dataset ImageNet-100 ImageNet-1k Dataset Higgs100k Higgs1M</cell><cell cols="2">MoCo v2 + i-Mix 84.1 87.0 70.9 71.3 MoCo v2 + i-Mix 72.1 72.9 74.9 74.5</cell><cell>Accuracy (%)</cell><cell>15 20 25 30 35 40 45 50 55 60 65 70 75</cell><cell>0.01 Method MoCo v2 0.02 i-Mix</cell><cell cols="2">0.05 Relative Dataset Size 0.1 0.2</cell><cell>0.5</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Figure 2: Comparison of MoCo v2 and i-Mix trained</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">on the different size of ImageNet.</cell></row><row><cell>Aug</cell><cell></cell><cell>CIFAR-10</cell><cell>CIFAR-100</cell><cell cols="3">Speech Commands</cell><cell>CovType</cell><cell></cell><cell>Higgs100k</cell><cell>Higgs1M</cell></row><row><cell></cell><cell cols="9">MoCo v2 + i-Mix * MoCo v2 + i-Mix * MoCo v2 + i-Mix MoCo v2 + i-Mix MoCo v2 + i-Mix MoCo v2 + i-Mix</cell></row><row><cell>-</cell><cell cols="8">47.7 ± 1.3 83.4 ± 0.4 24.7 ± 0.7 54.0 ± 0.5 76.9 ± 1.7 92.8 ± 0.5 69.6 ± 0.3 73.1 ± 0.1</cell><cell>64.2</cell><cell>71.8</cell><cell>65.5</cell><cell>72.9</cell></row><row><cell></cell><cell cols="9">93.5 72.1</cell><cell>72.9</cell><cell>74.9</cell><cell>74.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of MoCo v2 and i-Mix with and without data augmentations. a comparison between MoCo v2 and i-Mix by training with different model sizes and number of training epochs on the pretext task. We train ResNet-18, 50, 101, and 152 models with varying number of training epochs from 200 to 2000.Figure</figDesc><table /><note>* InputMix is applied when no other data augmentations are used. make</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>± 0.2 96.1 ± 0.1 85.9 ± 0.3 90.0 ± 0.4 CIFAR-100 64.1 ± 0.4 70.8 ± 0.4 71.6 ± 0.1 78.1 ± 0.3</figDesc><table><row><cell>Pretext Downstream MoCo v2 + i-Mix MoCo v2 + i-Mix CIFAR-10 CIFAR-100</cell><cell>VOC Object Detection</cell><cell>ImageNet MoCo v2 + i-Mix</cell></row><row><cell>CIFAR-10 93.5 (a) CIFAR-10 and 100 as the pretext dataset</cell><cell cols="2">AP AP 50 AP 75 (b) ImageNet as the pretext dataset 57.3 ± 0.1 57.5 ± 0.4 82.5 ± 0.2 82.7 ± 0.2 63.8 ± 0.3 64.2 ± 0.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison of MoCo v2 and i-Mix in transfer learning.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We omit bias terms for presentation clarity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Some literature<ref type="bibr" target="#b35">(He et al., 2020;</ref><ref type="bibr" target="#b13">Chen et al., 2020b)</ref> refers to them as query and positive/negative keys.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">We present the application of i-Mix to the original SimCLR formulation in Appendix A.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"> InfoNCE (Oord et al., 2018)  is a similar loss inspired by the idea of noise-contrastive estimation<ref type="bibr" target="#b32">(Gutmann &amp; Hyvärinen, 2010)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">For losses linear with respect to labels (e.g., the cross-entropy loss), they are equivalent to λ (λxi + (1 − λ)xj, vi)+(1−λ) (λxi +(1−λ)xj, vj), i.e., optimization to the mixed label is equivalent to joint optimization to original labels. The proof for losses in contrastive learning methods is provided in Appendix B.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"> Beta(α, α)  is the uniform distribution when α = 1, bell-shaped when α &gt; 1, and bimodal when α &lt; 1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">We use the N-pair formulation in Eq. (5) instead of SimCLR as it is simpler and more efficient to integrate i-Mix. As shown in Appendix C.2, the N-pair formulation results in no worse performance than SimCLR.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7">MoCo v2 improves the performance of MoCo by cosine learning schedule and more data augmentations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8">Supervised learning with improved methods, e.g., MixUp, outperforms i-Mix. However, linear evaluation on top of self-supervised representation learning is a proxy to measure the quality of representations learned without labels, such that it is not supposed to be compared with the performance of supervised learning.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9">Here, "scale" corresponds to the amount of data rather than image resolution.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10">This guarantees that the mixing coefficient for the principal data is larger than 0.5 to prevent from training with noisy labels. Note that<ref type="bibr" target="#b5">Beckham et al. (2019)</ref> also sampled mixing coefficients from the Dirichlet distribution for mixing more than two data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11">The j-th data can be excluded from the negative samples, but it does not result in a significant difference.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_12">https://github.com/HobbitLong/SupContrast</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_13">For small resolution data from CIFAR and Speech Commands, we replaced the kernal, stride, and padding size from (7,2,3) to (3,1,1) in the first convolutional layer, and removed the first max pooling layer, following<ref type="bibr" target="#b12">Chen et al. (2020a)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_14">Specifically, brightness, contrast, and saturation are scaled by a factor uniformly sampled from [0.6, 1.4] at random, and hue is rotated in the HSV space by a factor uniformly sampled from [−0.1, 0.1] at random.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_15">https://github.com/tugstugi/pytorch-speech-commands</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A MORE APPLICATIONS OF i-MIX</head><p>In this section, we introduce more variations of i-Mix. For conciseness, we use v i to denote virtual labels for different methods. We make the definition of v i for each application clear.</p><p>A.1 i-MIX FOR SIMCLR For each anchor, SimCLR takes other anchors as negative samples such that the virtual labels must be extended. Let x N +i = xi for conciseness, and v i ∈ {0, 1} 2N be the virtual label indicating the positive sample of each anchor, where v i,N +i = 1 and v i,j =N +i = 0. Note that v i,i = 0 because the anchor itself is not counted as a positive sample. Then, Eq. ( <ref type="formula">4</ref>) can be represented in the form of the cross-entropy loss:</p><p>The application of i-Mix to SimCLR is straightforward: for two data instances (x i , v i ), (x j , v j ) and a batch of data B = {x i } 2N i=1 , the i-Mix loss is defined as follows: 12</p><p>i-Mix</p><p>SimCLR (x i , v i ), (x j , v j ); B, λ = SimCLR (λx i + (1 − λ)x j , λv i + (1 − λ)v j ; B). (A.2)</p><p>Note that only the input data of Eq. (A.2) is mixed, such that f i in Eq. (A.1) is an embedding vector of the mixed data while the other f n 's are the ones of clean data. Because both clean and mixed data need to be fed to the network f , i-Mix for SimCLR requires twice more memory and training time compared to SimCLR when the same batch size is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 i-MIX FOR SUPERVISED CONTRASTIVE LEARNING</head><p>Supervised contrastive learning has recently shown to be effective for supervised representation learning and it often outperforms the standard end-to-end supervised classifier learning <ref type="bibr" target="#b43">(Khosla et al., 2020)</ref>. Suppose an one-hot label y i ∈ {0, 1} C is assigned to a data x i , where C is the number of classes. Let x N +i = xi and y N +i = y i for conciseness. For a batch of data pairs and their labels B = {(x i , y i )} 2N i=1 , let v i ∈ {0, 1} 2N be the virtual label indicating the positive samples of each anchor, where v i,j = 1 if y i = y j =i , and otherwise v i,j = 0. Intuitively, 2N j=1 v i,j = 2N yi − 1 where N yi is the number of data with the label y i . Then, the supervised learning version of the SimCLR (SupCLR) loss function is written as follows:</p><p>The application of i-Mix to SupCLR is straightforward: for two data instances (x i , v i ), (x j , v j ) and a batch of data B = {x i } 2N i=1 , the i-Mix loss is defined as follows:</p><p>Note that i-Mix in Eq. (A.4) is not as efficient as SupCLR in Eq. (A.3) due to the same reason in the case of SimCLR. To overcome this, we reformulate SupCLR in the form of the N-pair loss <ref type="bibr" target="#b69">(Sohn, 2016)</ref>. Suppose an one-hot label y i ∈ {0, 1} C is assigned to a data x i , where C is the number of classes. For a batch of data pairs and their labels B = {(x i , xi , y i )} N i=1 , let v i ∈ {0, 1} N be the virtual label indicating the positive samples of each anchor, where v i,j = 1 if y i = y j =i , and otherwise v i,j = 0. Then, the supervised version of the N-pair (Sup-N-pair) contrastive loss function is written as follows:</p><p>Then, the i-Mix loss for Sup-N-pair is defined as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PROOF OF THE LINEARITY OF LOSSES WITH RESPECT TO VIRTUAL LABELS</head><p>Cross-entropy loss. The loss used in contrastive representation learning works, which is often referred to as InfoNCE <ref type="bibr" target="#b59">(Oord et al., 2018)</ref>, can be represented in the form of the cross-entropy loss as we showed for N-pair contrastive learning, SimCLR <ref type="bibr" target="#b12">(Chen et al., 2020a)</ref>, and MoCo <ref type="bibr" target="#b35">(He et al., 2020)</ref>. Here we provide an example in the case of N-pair contrastive learning. Let</p><p>L2 loss between L2-normalized feature vectors. The BYOL <ref type="bibr" target="#b29">(Grill et al., 2020)</ref> loss is in this type.</p><p>Because F is not backpropagated, it can be considered as a constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C MORE ON EXPERIMENTS</head><p>We describe details of the experimental settings and more experimental results. For additional experiments below, we adapted the code for supervised contrastive learning <ref type="bibr" target="#b43">(Khosla et al., 2020)</ref>. 13   C.1 SETUP</p><p>In this section, we describe details of the experimental settings. Note that the learning rate is scaled by the batch size <ref type="bibr" target="#b28">(Goyal et al., 2017)</ref>: ScaledLearningRate = LearningRate × BatchSize/256.</p><p>Image. The experiments on CIFAR-10 and 100 <ref type="bibr" target="#b47">(Krizhevsky &amp; Hinton, 2009)</ref> and ImageNet <ref type="bibr" target="#b16">(Deng et al., 2009)</ref> are conducted in two stages: following Chen et al. (2020a), the convolutional neural network (CNN) part of ResNet-50 <ref type="bibr" target="#b34">(He et al., 2016)</ref> 14 followed by the two-layer multilayer perceptron (MLP) projection head (output dimensions are 2048 and 128, respectively) is trained on the unlabeled pretext dataset with a batch size of 256 (i.e., 512 augmented data) with the stochastic gradient descent (SGD) optimizer with a momentum of 0.9 over up to 4000 epochs. BYOL has an additional prediction head (output dimensions are the same with the projection head), which follows the projection head, only for the model updated by gradient. 10 epochs of warmup with a linear schedule to an initial learning rate of 0.125, followed by the cosine learning rate schedule <ref type="bibr" target="#b51">(Loshchilov &amp; Hutter, 2017)</ref> is used. We use the weight decay of 0.0001 for the first stage. For ImageNet, we use the same hyperparameters except that the batch size is 512 and the initial learning rate is 0.03.  <ref type="figure">Table C</ref>.1: Comparison of N-pair contrastive learning and SimCLR with i-MixUp and i-CutMix on them with ResNet-50 on CIFAR-10 and 100. We run all experiments for 1000 epochs. i-MixUp improves the accuracy on the downstream task regardless of the data distribution shift between the pretext and downstream tasks. i-CutMix shows a comparable performance with i-MixUp when the pretext and downstream datasets are the same, but it does not when the data distribution shift occurs.  <ref type="table">C</ref>.2: Comparison of the N-pair self-supervised and supervised contrastive learning methods and i-Mix on them with ResNet-50 on CIFAR-10 and 100. We also provide the performance of formulations proposed in prior works: SimCLR <ref type="bibr" target="#b12">(Chen et al., 2020a)</ref> and its supervised version <ref type="bibr" target="#b43">(Khosla et al., 2020)</ref>. We run all experiments for 1000 epochs. i-Mix improves the accuracy on the downstream task regardless of the data distribution shift between the pretext and downstream tasks, except the case that the pretest task has smaller number of classes than that of the downstream task. The quality of representation depends on the pretext task in terms of the performance of transfer learning: self-supervised learning is better on CIFAR-10, while supervised learning is better on CIFAR-100.</p><p>SimCLR, which is simpler and more efficient, especially when applying i-Mix, while not losing the performance. When pretext and downstream tasks share the training dataset, i-CutMix often outperforms i-MixUp, though the margin is small. However, i-CutMix shows a worse performance in transfer learning.</p><p>Table <ref type="table">C</ref>.2 compares the performance of SimCLR, N-pair contrastive learning, and i-Mix on N-pair contrastive learning when the pretext task is self-supervised and supervised contrastive learning. We confirm that the N-pair formulation results in no worse performance than that of SimCLR in supervised contrastive learning as well. i-Mix improves the performance of supervised contrastive learning from 95.7% to 97.0% on CIFAR-10, similarly to improvement achieved by MixUp for supervised learning where it improves the performance of supervised classifier learning from 95.5% to 96.6%. On the other hand, when the pretext dataset is CIFAR-100, the performance of supervised contrastive learning is not better than that of supervised learning: MixUp improves the performance of supervised classifier learning from 78.9% to 82.2%, and i-Mix improves the performance of supervised contrastive learning from 74.6% to 78.4%. While supervised i-Mix improves the classification accuracy on CIFAR-10 when trained on CIFAR-10, the representation does not transfer well to CIFAR-100, possibly due to overfitting to 10 class classification. When pretext dataset is CIFAR-100, supervised contrastive learning shows a better performance than self-supervised contrastive learning regardless of the distribution shift, as it learns sufficiently general representation for linear classifier to work well on CIFAR-10 as well.   <ref type="bibr" target="#b25">(Fréchet, 1957;</ref><ref type="bibr" target="#b75">Vaserstein, 1969)</ref> between the set of training and test embedding vectors under the Gaussian distribution assumption. For conciseness, let fi = f (x i )/ f (x i ) be an 2 normalized embedding vector; we normalize embedding vectors as we do when we measure the cosine similarity. Then, with the estimated mean m = 1  <ref type="table">C</ref>.3, i-Mix improves FED over contrastive learning, regardless of the distribution shift. Note that the distance is large when the training dataset of the downstream task is the same with that of the pretext task. This is because the model is overfit to the training dataset, such that the distance from the test dataset, which is unseen during training, has to be large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 QUALITATIVE EMBEDDING ANALYSIS</head><p>On the other hand, Table <ref type="table">C</ref>.3 shows that i-Mix reduces the gap between the training and test accuracy. This implies that i-Mix is an effective regularization method for pretext tasks, such that the learned representation is more generalizable on downstream tasks.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishita</forename><surname>Sundaram Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingliang</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A theoretical analysis of contrastive unsupervised representation learning</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrishikesh</forename><surname>Khandeparkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orestis</forename><surname>Plevrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikunj</forename><surname>Saunshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName><forename type="first">Yuki</forename><surname>Markus Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Uci machine learning repository</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">On adversarial mixup resynthesis</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sina</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">M</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farnoosh</forename><surname>Ghadiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Pal</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised pre-training of image features on non-curated data</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Léon Bottou, and Vladimir Vapnik. Vicinal risk minimization</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-supervised gans via auxiliary rotation loss</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<title level="m">A simple framework for contrastive learning of visual representations</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation policies from data</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Ekin D Cubuk</surname></persName>
		</author>
		<author>
			<persName><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Practical data augmentation with no separate search</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Ekin D Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Randaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13719</idno>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05538</idno>
		<title level="m">Dataset augmentation in feature space</title>
				<imprint>
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017">2017b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-task self-supervised visual learning</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Discriminative unsupervised feature learning with exemplar convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">PAMI</biblScope>
			<biblScope unit="page" from="1734" to="1747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sur la distance de deux lois de probabilité</title>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Fréchet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COMPTES RENDUS HEBDO-MADAIRES DES SEANCES DE L ACADEMIE DES SCIENCES</title>
				<imprint>
			<date type="published" when="1957">1957</date>
			<biblScope unit="volume">244</biblScope>
			<biblScope unit="page" from="689" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Maxout networks</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet in 1 hour</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Pierre H Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohan</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Daniel Guo</surname></persName>
		</author>
		<author>
			<persName><surname>Gheshlaghi Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Nonlinear mixup: Out-of-manifold data augmentation for text classification</title>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Augmenting data with mixup for sentence classification: An empirical study</title>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08941</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonia</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Painter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahesan</forename><surname>Niranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Prügel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Hare</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12047</idno>
		<title level="m">Fmix: Enhancing mixed sample data augmentation</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Using self-supervised learning can improve model robustness and uncertainty</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Augmix: A simple data processing method to improve robustness and uncertainty</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Ekin D Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaji</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><surname>Lakshminarayanan</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Hard negative mixing for contrastive learning</title>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bulent</forename><surname>Mert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noe</forename><surname>Sariyildiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Pion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diane</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName><surname>Larlus</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11362</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Supervised contrastive learning</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning image representations by completing damaged jigsaw puzzles</title>
		<author>
			<persName><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donggeun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">In</forename><surname>So Kweon</surname></persName>
		</author>
		<editor>WACV</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Sungnyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gihun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangmin</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Se-Young</forename><surname>Yun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.06300</idno>
		<title level="m">Mixco: Mix-up contrastive learning for visual representation</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Cooperative learning of audio and video models from self-supervised synchronization</title>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Training confidence-calibrated classifiers for detecting out-of-distribution samples</title>
		<author>
			<persName><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Making sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks</title>
		<author>
			<persName><forename type="first">Michelle</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnan</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parth</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeannette</forename><surname>Bohg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Data efficient lithography modeling with residual neural networks and transfer learning</title>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuki</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taiki</forename><surname>Kimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tetsuaki</forename><surname>Matsunawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shigeki</forename><surname>Nojima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">Z</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 International Symposium on Physical Design</title>
				<meeting>the 2018 International Symposium on Physical Design</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="82" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Mixed batches and symmetric discriminators for gan training</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Ollivier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin-Ichi</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin</forename><surname>Ishii ; Alexander Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Yair Movshovitz-Attias</title>
				<imprint>
			<date type="published" when="2017">2018. 2017</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1979" to="1993" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Representation learning by learning to count</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Boosting self-supervised learning via knowledge transfer</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananth</forename><surname>Vinjimoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11515</idno>
		<title level="m">Mixup inference: Better exploiting mixup to defend adversarial attacks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName><forename type="first">William</forename><surname>Daniel S Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekin</forename><forename type="middle">D</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08779</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Multi-task self-supervised learning for robust speech recognition</title>
		<author>
			<persName><forename type="first">Mirco</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pawel</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Trmal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICASSP</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Time-contrastive networks: Self-supervised learning from video</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corey</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yevgen</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasmine</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Google</forename><surname>Brain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Rethinking image mixture for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05438</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Automatic correction of lithography hotspots with a deep generative model</title>
		<author>
			<persName><forename type="first">Woojoo</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingdong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaeseung</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Suk</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sooryong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optical Microlithography XXXII</title>
				<imprint>
			<publisher>International Society for Optics and Photonics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">10961</biblScope>
			<biblScope unit="page">1096105</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">On mixup training: Improved calibration and predictive uncertainty for deep neural networks</title>
		<author>
			<persName><forename type="first">Gopinath</forename><surname>Sunil Thulasidasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><forename type="middle">A</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanmoy</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><surname>Michalak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10243</idno>
		<title level="m">What makes for good views for contrastive learning</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Markov processes over denumerable products of spaces, describing large systems of automata</title>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Nisonovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaserstein</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Problemy Peredachi Informatsii</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="64" to="72" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Manifold mixup: Better representations by interpolating hidden states</title>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.04419</idno>
		<title level="m">Towards domainagnostic contrastive learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11196</idno>
		<title level="m">Eda: Easy data augmentation techniques for boosting performance on text classification tasks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Incremental classifier learning with generative adversarial networks</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuancheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00853</idno>
		<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Unsupervised embedding learning via invariant and spreading instance feature</title>
		<author>
			<persName><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><surname>Joon Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">S4l: Self-supervised semisupervised learning</title>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><surname>Lopez-Paz</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2016. Xiang Zhang, Junbo Zhao, and Yann LeCun</title>
				<editor>
			<persName><surname>Neurips</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Colorful image colorization</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Comparing to learn: Surpassing imagenet pretraining on radiographs by comparing image representations</title>
		<author>
			<persName><forename type="first">Hong-Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yefeng</forename><surname>Zheng</surname></persName>
		</author>
		<editor>MICCAI</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
