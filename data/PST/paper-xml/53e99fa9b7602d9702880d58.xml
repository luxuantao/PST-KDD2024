<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ant Colony Optimization and Stochastic Gradient Descent</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Meuleau</surname></persName>
							<email>nmeuleau@iridia.ulb.ac.be</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IRIDIA</orgName>
								<orgName type="institution" key="instit2">Université</orgName>
								<address>
									<addrLine>Libre de Bruxelles Avenue Franklin Roosevelt 50, CP 194/6</addrLine>
									<postCode>B-1050</postCode>
									<settlement>Brussels</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">°2002 Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><surname>Dorigo</surname></persName>
							<email>mdorigo@ulb.ac.be</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IRIDIA</orgName>
								<orgName type="institution" key="instit2">Université</orgName>
								<address>
									<addrLine>Libre de Bruxelles Avenue Franklin Roosevelt 50, CP 194/6</addrLine>
									<postCode>B-1050</postCode>
									<settlement>Brussels</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">°2002 Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Ant Colony Optimization and Stochastic Gradient Descent</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">54D7B1ADEA5D79DEDEB16737F1CA27B6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>heuristic</term>
					<term>ant system</term>
					<term>ant colony optimization</term>
					<term>combinatorial optimization</term>
					<term>stochastic gradient descent</term>
					<term>reinforcement learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this article, we study the relationship between the two techniques known as ant colony optimization (ACO) and stochastic gradient descent. More precisely, we show that some empirical ACO algorithms approximate stochastic gradient descent in the space of pheromones, and we propose an implementation of stochastic gradient descent that belongs to the family of ACO algorithms. We then use this insight to explore the mutual contributions of the two techniques.</p><p>1 A notable exception is the ABC algorithm for routing that, although belonging to ACO, was developed independently of AS <ref type="bibr" target="#b30">[31]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The study of self-organization in social insects as a source of inspiration for novel distributed forms of computation is a promising area of AI known as ant algorithms (or sometimes as swarm intelligence ) that is experiencing growing popularity <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13]</ref>. A particularly successful form of ant algorithm is that inspired by ant colony foraging behavior. In these algorithms, applied to combinatorial optimization problems, a number of arti cial ants are given a set of simple rules that take inspiration from the behavior of real ants. Arti cial ants are then left free to move on an appropriate graph representation of the considered problem: they probabilistically build a solution to the problem and then deposit on the graph some arti cial pheromones that will bias the probabilistic solution construction activity of future ants. The amount of pheromone deposited and the way it is used to build solutions are such that the overall search process is biased toward the generation of approximate solutions of improving quality.</p><p>The historic rst example of an algorithm inspired by ant foraging behavior is the ant system (AS) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref> and its rst application was to the traveling salesman problem (TSP), a well known NP-hard problem <ref type="bibr" target="#b20">[21]</ref>. As a follow-up of AS, a number of similar algorithms, each one trying either to improve performance or to make AS better t a particular class of problems, were developed. Currently, many successful applications of such algorithms exist for N P-hard academic combinatorial optimization problems such as quadratic assignment <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23]</ref>, sequential ordering <ref type="bibr" target="#b15">[16]</ref>, resource-constrained project scheduling <ref type="bibr" target="#b23">[24]</ref>, vehicle routing with time windows <ref type="bibr" target="#b16">[17]</ref>, routing in packet-switched networks <ref type="bibr" target="#b5">[6]</ref>, shortest common supersequence <ref type="bibr" target="#b24">[25]</ref>, and frequency assignment <ref type="bibr" target="#b21">[22]</ref>. Applications to real-world combinatorial optimization problems are starting to appear: For example, a gasoline distribution company in Switzerland is using ACO algorithms to choose routes of its trucks <ref type="bibr" target="#b16">[17]</ref>, while Fujitsu-Siemens Computers in Germany is testing ant colony optimization (ACO) for an important logistic problem [R. <ref type="bibr">Palm, personal communication]</ref>. As a consequence, the ACO metaheuristic was recently de ned <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> to put in a common framework all the algorithms that can be considered as offspring of AS. 1  Despite these successes, the basic mechanisms at work in ACO are still loosely understood, and there is usually no analytical tool to explain the observed effectiveness of an ACO algorithm. In this article, we make a step in the direction of providing such tools by formally relating the ACO metaheuristic and the technique known as stochastic gradient descent (SGD), which has been extensively used in machine learning <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref>. More precisely, we show that some ACO algorithms approximate gradient descent of the expected value of the solution produced by an ant, in the space of pheromone trails. Moreover, we present an algorithm for combinatorial optimization that is, at the same time, an SGD algorithm working in the space of pheromones, and an instance of the ACO metaheuristic. This algorithm is an instance of the gradient-based reinforcement learning algorithm known as REINFORCE <ref type="bibr" target="#b34">[35]</ref>. It can be seen both as a distributed, stigmergic <ref type="foot" target="#foot_0">2</ref> implementation of SGD, or as an ACO algorithm where the overall effect of ants' activity is to descend the gradient of a given function in the space of pheromones.</p><p>The interest of establishing connections between ACO and SGD is that it offers many opportunities of cross-fertilization. On one side, many questions asked in the study of ACO algorithms receive a second look under the gradient-descent interpretation. For instance, a new way of understanding and proving convergence of ACO algorithms is proposed. Moreover, some classical acceleration techniques for gradient-based reinforcement learning can be easily transposed to ACO algorithms. On the other side, ACO algorithms show how to implement effectively the technique of SGD for solving large combinatorial optimization problems. Several improvements to the basic trial-and-error search of arti cial ants developed in the best ACO algorithms suggest, in turn, new ways of using gradient descent in the framework of combinatorial optimization.</p><p>In this introductory article, we present our main arguments using the example of the asymmetric TSP (ATSP). First (Sect. 2.1), we brie y review the AS algorithm using the ATSP as the example problem, and we show that AS is indeed closely related to the technique of SGD (Sects. 2.2 and 2.3). We then describe an implementation of SGD-or, more precisely, of William's REINFORCE <ref type="bibr" target="#b34">[35]</ref>-that belongs to the family of ACO algorithms (Sect. 2.4). In Section 2.5, we show how to generalize this reasoning to any combinatorial optimization problem that can be solved by an ACO algorithm. Finally, we comment on these results and outline some future research directions in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Ant System and Stochastic Gradient Descent</head><p>Ant system is a simple distributed algorithm that can be applied to any (constrained) minimum cost path problem on a graph. Throughout this article, we use the application of AS to the ATSP as a basic example to present our arguments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Ant System</head><p>The ATSP can be de ned as follows. Let X be a set of cities, |X | D n, and D D [d (x, y )] be a distance matrix, with d (x , y ) 2 R C for all (x, y ) 2 X 2 . We will denote by N X t µX t the set of acyclic paths of length t 2 f1, . . . , ng in terms of the number of cities crossed ( N X 1 D X and N X 2 D f(x , y) 2 X 2 : x 6 D yg). ATSP can be de ned as the problem of nding a path</p><formula xml:id="formula_0">h n D (x 1 , x 2 , . . . , x n ) 2 N</formula><p>X n that minimizes the length of the corresponding tour, de ned as</p><formula xml:id="formula_1">L (h n ) D n¡1 X t D1 d (x t , x t C1 ) C d (x n , x 1 ).</formula><p>The main variables of the AS algorithm are the pheromone trails t (x, y ) associated with each pair of cities (x , y ) 2 N X 2 . Let T be the bidimensional vector gathering all the t (x, y )'s. The basic principle of AS is to simulate arti cial ants that use the pheromone trails to build a random tour. Once its tour is completed, each ant makes a backward trip following the same path and updates the pheromones on its way back. Finally, the pheromone trails partially evaporate, that is, they decrease by a constant factor r, 0 &lt; r • 1, called the evaporation rate. The behavior of each ant can be summarized as follows:</p><p>Forward: Draw the start city x 1 at random uniformly; At each step t 2 f1, . . . , n ¡ 1g, after following the path</p><formula xml:id="formula_2">h t D (x 1 , x 2 , . . . , x t ) 2 N</formula><p>X t , draw the next city at random following</p><formula xml:id="formula_3">Pr(x t C1 D x | T , h t ) D ( 0 if x 2 h t , t (x t , x ) / P y 2X y / 2h t t (x t , y ) otherwise,<label>(1)</label></formula><p>where x 2 h t means that the acyclic path h t traverses x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backward:</head><p>After generating the path</p><formula xml:id="formula_4">h n D (x 1 , x 2 , . . . , x n ) 2 N</formula><p>X n , reinforce the pheromone trails t (x t , x t C1 ) for each t 2 f1, . . . , n ¡ 1g and t (x n , x 1 ) by the amount 1/L (h n ).</p><p>There are many ways of implementing the algorithm. In the original implementation of AS, a set of m arti cial ants synchronously built m solutions as follows: First, all the ants perform their forward trip without updating the pheromones, and then all of them execute their backward trip and update the pheromones for the next "generation" of m ants. A pheromone evaporation stage takes place at each generation, before sending the ants backward. The total update at each generation of each pheromone t (x, y ), (x, y ) 2 N X 2 is then</p><formula xml:id="formula_5">Dt (x, y ) D Á m X i D1 d x, y (h i n ) L (h i n ) ! ¡ r t (x, y),</formula><p>where h i n 2 N X n is the path followed by the ith ant during its forward trip (i 2 f1, 2, . . . , mg), and d x, y (h n ) D 1 if y is the immediate successor of x in the tour associated to h n 2 N X n and 0 otherwise. When m D 1, the different ants are sent one after the other in a fully sequential way, waiting for the previous ant to complete its backward trip before sending a new one. In this case, the pheromone update implemented by each ant is, for all (x, y ) 2 N X 2 ,</p><formula xml:id="formula_6">Dt (x, y ) D d x,y (h n ) L (h n ) ¡ rt (x, y ),<label>(2)</label></formula><p>where h n D fx 1 , x 2 , . . . , x n g 2 N X n is the path followed by the ant during its forward trip. This pheromone update rule may also be used in a fully asynchronous and parallel implementation of ant system in which ants act completely independently of each other, and a pheromone evaporation stage is associated with each ant. Equation 2, originally introduced in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref>, is often replaced by the following rule, introduced for the rst time in <ref type="bibr" target="#b11">[12]</ref>, in which the reinforcement of pheromone trails is multiplied by the evaporation rate r:</p><formula xml:id="formula_7">Dt (x, y ) D r d x, y (h n ) L (h n ) ¡ rt (x, y ) D r d x,y (h n ) L (h n ) ¡ t (x, y ) ´.<label>(3)</label></formula><p>Ant system was extensively tested together with a few other algorithms inspired by real ants' behavior on the TSP <ref type="bibr" target="#b13">[14]</ref>. Although AS did not compete with the best known algorithms for TSP, its relative success inspired a great number of algorithms for different combinatorial optimization problems (cf. Introduction). Often the AS-based algorithms provide state-of-the-art performance. These algorithms have recently been put in a unifying framework called an ACO metaheuristic <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. ACO is composed of three main procedures. In the rst one, arti cial ants probabilistically construct feasible solutions to the considered problem by moving on a proper graph representation. In this phase the construction process is biased by previous experience memorized in the form of pheromone trails, and, in some implementations, by heuristic information available about the considered problem (see discussion in Sect. 3.1.1 ). The second phase, brie y discussed in Section 3.1.2, is optional: Here the solutions generated by the arti cial ants can be taken to their local optima by a suitable local search routine. In the last phase, pheromone trails are updated by the ants, pheromone evaporation, and/or other suitable processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Stochastic Gradient Descent</head><p>ACO algorithms are usually regarded as optimization techniques working in the solution set of the combinatorial problem at hand. For instance, the ant system for ATSP is usually seen as an algorithm that tries to nd a tour of minimal length (i.e., an optimal solution of the combinatorial problem). However, we adopt in this work a different approach and we look at ACO algorithms as working in the space of pheromone trails. In other words, we aim at nding an optimal set of pheromones, which can be de ned in different ways. In this article, we focus on a particular form of optimality for pheromone values. We will call an optimal set of pheromones a con guration that optimizes the expected value of the solution produced by an ant during its forward trip. We then study how this problem may be solved using gradient descent in the continuous space of pheromone trails.</p><p>In the case of ATSP, we aim at maximizing the expected value of the inverse length of an ant's forward trip, given the current pheromone trails and the city-selection rule of Equation 1. That is, we will climb stochastically the gradient of the "error" E de ned as<ref type="foot" target="#foot_1">3</ref> </p><formula xml:id="formula_8">E def D E µ 1 L (h n ) | T ¶ D X h n 2 N X n Pr(h n | T ) 1 L(h n ) .</formula><p>Note that the expectation is conditional on T because the probability of a given tour happening depends on the current pheromone trail vector T , while the "local error"</p><p>1/L (h n ) does not depend on the weights t (x, y). Then we have</p><formula xml:id="formula_9">@E @t (x, y ) D X h n 2 N X n @ Pr(h n | T ) @t (x , y ) 1 L (h n ) ,</formula><p>for each pair of cities (x , y) 2 N X 2 . The probability of a given path is equal to the product of the probabilities of all the elementary events that compose it: if</p><formula xml:id="formula_10">h n D (x 1 , x 2 , . . . , x n ) 2 N X n , then Pr(h n | T ) D n Y t D1 Pr(x t | T , h t ¡1 ),</formula><p>where h t is equal to h n truncated after step t :</p><formula xml:id="formula_11">h t D (x 1 , x 2 , . . . , x t ) 2 N X t</formula><p>, and h 0 is the empty sequence. Therefore, <ref type="figure">,</ref><ref type="figure">y</ref> ) .</p><formula xml:id="formula_12">@ Pr(h n | T ) @t (x , y ) D Pr(h n | T ) n X t D1 @ ln (Pr(x t | T , h t ¡1 )) @t (x</formula><p>Here we have supposed that Pr(x t | T , h t ¡1 ) &gt; 0, which is always true because x t / 2 h t ¡1 , as h t is an acyclic path; and because the pheromone trails never fall to 0 in the original AS algorithm (however, we will see later that this is a problem for the new algorithm). De ne the eligibility trace<ref type="foot" target="#foot_2">4</ref> of (x , y) in path h n as</p><formula xml:id="formula_13">T x, y (h n ) def D @ ln(Pr(h n | T )) @t (x, y ) D n X t D1 @ ln (Pr (x t | T , h t ¡1 )) @t (x, y) ,<label>(4)</label></formula><p>then</p><formula xml:id="formula_14">@E @t (x, y ) D X h n 2 N X n Pr(h n | T ) T x, y (h n ) L(h n ) D E µ T x, y (h n ) L (h n ) | T ¶ . (<label>5</label></formula><formula xml:id="formula_15">)</formula><p>We will see later how to calculate the traces T x,y . We can already outline the basis of the SGD algorithm. Climbing the gradient of E corresponds to updating T iteratively in the direction of the gradient of E :</p><formula xml:id="formula_16">DT D ar T E , that is, Dt (x, y ) D a @E @t (x , y ) ,</formula><p>for each individual "weight" t (x , y ), where a &gt; 0 is the step-size parameter or learn- ing rate. Following Equation <ref type="formula" target="#formula_14">5</ref>, we could do exact gradient ascent in the space of pheromone trails by enumerating all possible paths h n and calculating, for each of them, the probability Pr(h n | T ) that an ant follows this path during its forward trip (given the current pheromone trails), the length of the corresponding tour L(h n ), and the variable T x,y (h n ) for each (x , y) 2 N X 2 . Obviously, this approach would make no sense in practice: Once we have enumerated all possible paths we can solve our original problem simply by taking the best. Moreover, the size of N X n grows exponentially with the number of cities, so that this approach quickly becomes infeasible. Finally, the exact gradient descent performs poorly in many complex domains because it gets trapped in the rst local optimum on its way.</p><p>In stochastic gradient descent (SGD), an unbiased random estimate of the gradient is used instead of the true gradient. In the case of our application to ATSP, Equation <ref type="formula" target="#formula_14">5</ref>shows that the gradient of E is the expected value of the random variable T x,y /L given the current pheromones (and the selection rule of Equation <ref type="formula" target="#formula_3">1</ref>). Therefore, if we draw independently m paths h 1 n , h 2 n , . . . , h m n in N X n following the probability Pr(h n | T ), and average their contributions T x,y</p><formula xml:id="formula_17">(h i n ) /L (h i n ) to the gradient, then the result 1 m m X iD1 T x, y (h i n ) L (h i n )</formula><p>is a random vector whose expected value is equal to the gradient. In other words, it is an unbiased estimate of the gradient. This is true regardless of the number of paths sampled, even if only one sample is used to estimate the gradient (i.e., m D 1). The resulting stochastic algorithm has a reasonable complexity. <ref type="foot" target="#foot_3">5</ref> Moreover, it may escape from some low-value local optima on its path. It sometimes makes bad moves because the gradient estimate is wrong, but these moves may allow jumping out of a bad local optimum. Therefore SGD usually performs better than the exact gradient descent in large, highly multimodal search spaces. The basis of our comparison between ACO and SGD is the analogy between the actions of sending an ant forward and sampling a tour h n from Pr(h n | T ). During its forward trip, the action of an ant is precisely to sample a solution following this probability distribution. Therefore, the forward component of AS can be used in an SGD algorithm as well, and we just have to change the weight update rules. We show below that the updates associated with a given sampled tour are very similar in the two algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">A rst ACO/SGD Algorithm</head><p>In our ACO/SGD algorithm, each arti cial ant samples a tour h n 2 N</p><p>X n using the current pheromones and then updates every pheromone t (x , y ) following</p><formula xml:id="formula_18">Dt (x, y ) D a T x,y (h n ) L(h n )<label>(6)</label></formula><p>In a synchronous implementation, arti cial ants are sent by groups of m ants that sample m tours without updating the pheromones (i.e., following the same probability distribution Pr(h n | T )). Then, each of them updates the pheromones adding a quantity of pheromone function of the quality of the solution it generated during its forward trip. The total amount of pheromone added by the m ants is then</p><formula xml:id="formula_19">Dt (x, y ) D a m X iD1 T x, y (h i n ) L (h i n )</formula><p>, for all (x , y), where h i n 2 N X n is the path followed by the ith ant. This corresponds to using m independent samples h n to calculate the gradient estimate (note that the factor 1/ m has been absorbed in the learning rate a). The particular case m D 1 that is based on Equation 6 corresponds to a fully sequential implementation where only one sample is used to estimate the gradient and make a step in the space of pheromones. Equation <ref type="formula" target="#formula_18">6</ref>may also be used as the basis of a fully asynchronous and parallel implementation of the algorithm where the arti cial ants act completely independently of each other. In this case, the gradient estimates may be slightly biased by the simultaneous read and write activity of the different ants. However, this bias will probably be negligible in many applications, provided that the learning rate a stays within reasonable bounds. <ref type="foot" target="#foot_4">6</ref>Given a path h n D (x 1 , x 2 , . . . , x n ) 2 N X n and a pair of cities (x, y ) 2 N X 2 , the problem is now to calculate T x, y (h n ) as de ned by Equation <ref type="formula" target="#formula_13">4</ref>. Using Equation <ref type="formula" target="#formula_3">1</ref>, we see that</p><formula xml:id="formula_20">if x 6 D x t ¡1 or y 2 h t ¡1 , then Pr(x t | T , h t ¡1 ) is independent of t (x , y) and @ ln(Pr(x t | T , h t ¡1 )) @t (x , y ) D 0I if x D x t ¡1 and y D x t then @ ln(Pr(x t | T , h t ¡1 )) @t (x , y ) D @ ln ± t (x , y ) / P y 0 / 2h t ¡1 t (x , y 0 ) ² @t (x, y ) , D @ ln(t (x , y )) @t (x, y ) ¡ @ ln ± P y 0 / 2h t ¡1 t (x, y 0 ) ² @t (x, y ) , D 1 t (x, y ) ¡ 1 P y 0 / 2h t ¡1 t (x, y 0 ) , D 1 ¡ Pr(y | T , h t ¡1 ) t (x, y ) I if x D x t ¡1 , y 6 D x t , and y / 2 h t ¡1 then, similarly, @ ln(Pr(x t | T , h t ¡1 )) @t (x , y ) D ¡ Pr(y | T , h t ¡1 ) t (x , y ) .</formula><p>Therefore, the SGD algorithm can be implemented using distributed, local information, as done in AS. The weight update corresponding to a sampled tour can be performed by the ant that sampled this tour during a backward trip. When returning to the tth city of the tour, <ref type="foot" target="#foot_5">7</ref> the ant updates the pheromone trail t (x t , x ) for all x 2 X following</p><formula xml:id="formula_21">Dt (x t , x ) D a 1 L (h n ) 1 t (x t , x ) (d (x t C1 D x ) ¡ Pr(x | T , h t )),<label>(7)</label></formula><p>where</p><formula xml:id="formula_22">d (x t C1 D x ) D 1 if x t C1 D</formula><p>x and 0 otherwise, and Pr(x | T , h t ) represents the probability that the ant chooses x 2 X as the next city when it stands in x t and it has already followed the path h t 2 N X t . Note that the ants need a little bit more memory than in the original AS. They need to remember not only the tour they followed, but also the probability of choosing each city at each step of the forward trip. If they do not have such a memory, they can always recompute the probabilities using the pheromone trails, but the trails must not have changed in between due to another ant updating pheromones. Therefore, this solution works exactly only in a synchronous implementation of the algorithm.</p><p>The previous results show that the SGD of the error E[1/L (h n ) | T ] is close to the original AS algorithm. This appears clearly when we compare Equation <ref type="formula" target="#formula_21">7</ref>to the analogous step-by-step update rule used in an asynchronous implementation of AS based on Equation <ref type="formula" target="#formula_7">3</ref>:</p><formula xml:id="formula_23">Dt (x t , x ) D r d (x t C1 D x ) L (h n ) ¡ t (x t , x ) ´.<label>(8)</label></formula><p>The step-size a of SGD plays the role of the evaporation rate r in AS. The main differences are <ref type="foot" target="#foot_6">8</ref>the pheromone value t (x t , x ) in AS update rule is replaced in SGD by the probability Pr(x | T , h t ) of moving from x t to x;</p><p>the decrease of pheromones (through the term ¡t (x t , x ) in Equation <ref type="formula" target="#formula_23">8</ref>, and through the term ¡ Pr(x | T , h t ) in Equation <ref type="formula" target="#formula_21">7</ref>) is proportional to the reward 1/L (h n ) in our algorithm, whereas it is independent of it in the original AS;</p><p>the presence of the factor 1/t (x t , x ) in the update rule (Equation <ref type="formula" target="#formula_21">7</ref>) of the gradient-based algorithm.</p><p>It is important to note that, in our algorithm, the pheromones that are not used during an ant's forward trip are not modi ed during the ant's backward trip. If we had already visited y when we were in x , then t (x, y ) was not used to choose the next city after x , and hence, it was not used at all during the whole forward trip. As a consequence, t (x, y ) is left unchanged during the backward trip (that was not the case in the original AS where each pheromone trail evaporates). This makes sense, because if t (x, y ) is not used during the generation of a tour, then the value of this tour provides no information about the good way to change t (x , y ). This will be true in any application of SGD. It implies that the weight update associated with a forward trip (i.e., a sampled solution), can always be performed in a backward trip following the same path. This is the basis of "ant" implementations of SGD presented in this article.</p><p>There are a few problems with the algorithm we just de ned. First, the update rule may bring the weights t (x, y ) at or below 0. Negative pheromone trails do not really make sense. Moreover, we supposed the pheromones to be (strictly) positive when calculating the gradient. When some pheromones are 0, the analytical expression of the gradient is more complex. An empirical solution to this problem consists of arti cially preventing the weights from falling below a given value 2 &gt; 0. However, there is another problem with this algorithm: the contribution T x, y (h n ) /L (h n ) of a sequence h n 2 N</p><p>X n may tend to in nity when some pheromone trails t (x, y ) tend to 0, which induces a very unstable behavior of the algorithm in some regions of the search space.</p><p>For instance, if t (x , y ) ¼ 0 for some (x , y ) 2 N X 2 and an ant unluckily goes through this edge during its forward trip, then the subsequent weight update, which is proportional to 1/t (x, y), may bring the algorithm very far from its original state. This is a case of unstable behavior due to unboundedness of the gradient estimate variance <ref type="bibr" target="#b1">[2]</ref>: Although the expected value of the gradient estimate (i.e., the gradient itself) is always nite, the variance of the gradient estimate tends to in nity when some weights t (x , y ) tend to 0. In the next section, we present a new implementation of SGD that does not exhibit this instability and still belongs to the family of ACO algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">A Stable ACO/SGD Algorithm</head><p>A classical solution to the problem of unbounded variance and unstable behavior is to use Boltzmann's law instead of the proportional selection rule of Equation 1 (e.g., <ref type="bibr" target="#b0">[1]</ref>). In our case, the city-selection rule takes the form</p><formula xml:id="formula_24">Pr(x t C1 D x | T , h t ) D 8 &gt; &lt; &gt; : 0 if x 2 h t , e t (x t ,x) / X y 2X y / 2h t e t (x t , y ) otherwise. (<label>9</label></formula><formula xml:id="formula_25">)</formula><p>The derivation presented in Section 2.2 is still valid; the only changes are in the calculation of the traces (Sect. 2.3):</p><formula xml:id="formula_26">if x 6 D x t ¡1 or y 2 h t ¡1 , then @ ln(Pr(x t | T , h t ¡1 )) @t (x , y ) D 0I if x D x t ¡1 and y D x t then @ ln(Pr(x t | T , h t ¡1 )) @t (x , y ) D @ ln Á e t (x,y ) / X y 0 / 2h t ¡1 e t (x,y 0 ) ! @t (x, y ) , D @ ln(e t (x, y ) ) @t (x , y ) ¡ @ ln Á X y 0 / 2h t ¡1 e t (x, y 0 ) ! @t (x , y) , D 1 ¡ e t (x, y ) X y 0 / 2h t ¡1 e t (x,y 0 ) , D 1 ¡ Pr(y | T , h t ¡1 )I if x D x t ¡1 , y 6 D x t and y / 2 h t ¡1 then, similarly, @ ln(Pr(x t | T , h t ¡1 )) @t (x , y ) D ¡ Pr(y | T , h t ¡1 ).</formula><p>As in the previous case, the gradient-descent weight updates may be performed by the arti cial ants during backward trips when they retrace their path backward. The new pheromone update rule, which replaces Equation <ref type="formula" target="#formula_21">7</ref>, is</p><formula xml:id="formula_27">Dt (x t , x ) D a 1 L (h n ) (d (x t C1 D x ) ¡ Pr(x | T , h t )).<label>(10)</label></formula><p>Note that the factor 1/t (x t , x ) has disappeared from the right-hand side, making this rule very similar to the original AS update rule (Equation <ref type="formula" target="#formula_23">8</ref>). Ants need the same memory capacity as in the previous SGD algorithm. This new algorithm does not have the drawbacks of the previous one. The weights t (x , y) are unconstrained and can take any real value while keeping the probability of each path (strictly) positive. Moreover, the gradient estimate is uniformly bounded (i.e., bounded by the same bound for all h n 2 N X n ) by the value 1/L ¤ , where L ¤ is the length of the shortest path, and so its variance is bounded by (1 /L ¤ ) 2 . Therefore, this algorithm is stable in any region of the search space. Bounded variance is a necessary condition for convergence to a local optimum, but it is not suf cient <ref type="bibr" target="#b1">[2]</ref>. As a matter of fact, it can be shown that stochastic gradient algorithms such as ours may exhibit unbounded behavior, that is, some weights may tend to in nity <ref type="bibr" target="#b27">[28]</ref>. A typical case of unbounded behavior is when the weights that generate an optimal solution tend to plus in nity, while all the other weights tend to minus in nity. The problem is that an optimal set of pheromone values (i.e., a set that generates an optimal tour with probability 1) is obtained when some weights are positively or negatively in nite. Therefore, the algorithm may continue to climb a given choice forever, so that some weights diverge to in nity. However, this limitation has no consequence in our combinatorial optimization framework because we are not interested in having all the ants follow an optimal path; we just want an optimal solution to be generated (at least) once. Moreover, some classical tricks may be used to derive a formally convergent variant of our algorithm. Notably, we can either arti cially bound the weights away from in nity (i.e., they are enforced to stay in a compact subset of R l , where l is the total number of weights), as in <ref type="bibr" target="#b27">[28]</ref>; or add to the objective function a penalty term in the form ¡c ¢ kT k, where c is a constant and kT k is a norm of T , so that the objective function tends to ¡1 when the weights tend to in nity <ref type="bibr" target="#b1">[2]</ref>.</p><p>We conjecture that either of these two solutions may be used to design a variant of our algorithm that converges with probability 1 to a local optimum of the error function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Extensions</head><p>It is easy to modify the algorithm so that it optimizes other criteria than E</p><formula xml:id="formula_28">[1/L(h n ) | T ].</formula><p>For instance, if we want to minimize the expected tour length E[L (h n ) | T ], then the update rule of the (stable) SGD algorithm becomes</p><formula xml:id="formula_29">Dt (x t , x ) D ¡aL (h n ) (d (x t C1 D x ) ¡ Pr(x | T , h t )).</formula><p>Here, the algorithm may be understood as maximizing the reward ¡L (h n ), which is always negative. <ref type="foot" target="#foot_7">9</ref> It is important to note that, because the shape of the objective function strongly determines the behavior of a gradient-following algorithm with constant stepsize such as ours, the performance of SGD may vary with different objective functions, even if these functions have the same local and global optima.</p><p>More generally, the same approach could be applied to every combinatorial optimization problem for which an ACO algorithm can be designed. <ref type="foot" target="#foot_8">10</ref> The generic ACO/SGD approach to a given maximization problem with solution set S and objective function f : S ! R can be summarized as follows: First, design a stochastic controller that generates solutions in an iterative way using a set of weights T . The controller is represented as a construction graph G such that the generation of a solution corresponds to some sort of path in this graph, and the weights T are attached to the arcs (or vertices) of G . The weights, called pheromones, determine the transition probabilities in G during the random generation of a solution. This rst stage, which is called the choice of a problem representation in <ref type="bibr" target="#b9">[10]</ref>, is crucial. It transforms the static combinatorial problem maxf f (s ): s 2 Sg into the problem of optimizing a trajectory, that is, a dynamic problem.</p><p>The next step is to de ne the "error" function as the expected value of a solution produced by the controller, given the current weights:</p><formula xml:id="formula_30">E D E[F (s) | T ] D X s2S Pr(s | T )F (s)</formula><p>where</p><formula xml:id="formula_31">F : S ! R strictly increases with f (that is, f (s ) &gt; f (s 0 ) H) F (s) &gt; F (s 0 )).</formula><p>Solutions s are generated by an iterative stochastic process that follows a nite number of steps. Let H be the set of trajectories in G that the controller may follow when generating a solution, and g: H ! S be the function that assigns the solution produced to a given trajectory. The error may be rewritten as:</p><formula xml:id="formula_32">E D E[F (g (h)) | T ] D X h2H Pr(h | T )F (g (h)).</formula><p>The gradient of the error is then the expectation over all possible trajectories of the value of the solution produced multiplied by an eligibility trace T t :</p><formula xml:id="formula_33">@E @t D X h2H Pr(h | T )F (g (h))T t (h) D E[F (g (h))T t (h) | T ],</formula><p>for each individual weight t . The trace T t (s ) is the sum of the partial derivatives of the log of every elementary event that composes the trajectory s. Note that this decomposable structure of the gradient derives from the fact that solution generation is an iterative process, that is, from the very nature of the ACO approach. Stochastic gradient descent can thus be implemented by sampling a few trajectories h-which can be seen as sending a few ants forward in the construction graph Gand calculating their contribution to the gradient. In general, the weights that are not used when sampling a trajectory h have zero update according to the contribution of h. Therefore, the weight updates can be performed by arti cial ants during backward trips in G . <ref type="foot" target="#foot_9">11</ref> Finally, if the ants use Boltzmann's law to make random choices during forward trips, then the gradient estimate is uniformly bounded and the algorithm is stable in any part of the search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Discussion</head><p>Technically speaking, the ACO/SGD algorithm described above is not new. It is an instance of the generalized learning automaton <ref type="bibr" target="#b26">[27]</ref> and, more precisely, of the gradient-based reinforcement learning algorithm known as REINFORCE <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b34">35]</ref>. The originality of our work is to apply REINFORCE in the framework of ACO for combinatorial optimization, instead of the traditional Markov decision process (MDP) or partially observable MDP (POMDP) used in the reinforcement learning literature <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b32">33]</ref>. Therefore, this work establishes connections between "classical" reinforcement learning and the less classical ACO learning. It suggests a general approach for applying reinforcement learning to combinatorial optimization that can be resumed as follows:</p><p>1. Design a parametrized stochastic controller that generates solutions in an incremental way, which turns the original static (optimization) problem into a dynamic (control) problem;</p><p>2. Use a reinforcement learning algorithm to (learn to) optimize the controller.</p><p>It is tempting to over-generalize the previous results and see in any ACO algorithm a more or less accurate approximation of the mechanism of gradient descent in the space of pheromones. In a sense, SGD is a very intuitive trial-and-error Monte-Carlo technique that samples solutions, increases the probability of the best sampled solutions, and decreases the probability of the worst. Its particularity is to be grounded on a solid theoretical framework so that it is possible to give a sense to the updates performed by the algorithm, but the basic intuition is the same as in ACO algorithms. Researchers in the eld of ACO algorithms might nd this position a little bit reductive. In fact, the best ACO algorithms are not limited to the simple trial-and-error ant search but also use other optimization techniques such as constructive heuristics and local search routines <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">32]</ref>. However, these features may also be grounded in the gradientdescent framework and improve the algorithm's performance (see Sect. 3.1). It is also interesting to note that the assimilation of ACO to approximate SGD allows us to draw a parallel with arti cial neural networks (ANNs), because SGD is the basic principle behind the well-known backpropagation algorithm <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30]</ref>. Accordingly, we suggest in this work that the basic mechanisms at work in ACO and ANNs could be the same. In the following, we discuss some opportunities of cross-fertilization between ACO and SGD and then survey some important issues in the study of ACO algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Mutual Contributions</head><p>There are many opportunities for cross-fertilization between ACO and SGD (or, more generally, gradient-based reinforcement learning). On one side, several ef cient acceleration techniques for gradient-based reinforcement learning can easily be implemented in our ACO/SGD algorithm and then generalized to other ACO algorithms (this is the subject of ongoing research). On the other side, existing ACO algorithms suggest different ways of implementing the technique of SGD in the context of combinatorial optimization. The most successful applications of the metaheuristic are not limited to the simple trial-and-error ant search but also use some "external" information in the form of constructive heuristics or (discrete) local search routines. They suggest different ways of merging SGD and these two combinatorial optimization techniques. In this section, we examine how our simple ACO/SGD algorithm can be used in combination with these techniques, as inspired by previous ACO algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Using Constructive Heuristics</head><p>Many successful implementations of the ACO metaheuristic combine information from the pheromone trails and heuristic information when generating solutions. In the case of AS, the city selection rule (1) is replaced by</p><formula xml:id="formula_34">Pr(x t C1 D x | T , g, h t ) D 8 &gt; &lt; &gt; : 0 if x 2 h t , t (x t , x ) a g (x t , x ) b / X y2X y / 2h t t (x t , y ) a g (x t , y) b otherwise,</formula><p>where g ¸0 is a (constructive) heuristic function of (x , y) 2 N X 2 , and a and b are two (positive) parameters that determine the relative in uence of pheromone trails and heuristic information. The function g re ects heuristic information about the good way to complete a partial solution. For instance, in the case of ATSP, a common choice is g (x , y ) D 1/d (x , y ) for all (x, y ) 2 N X 2 . In this case, the closest unvisited cities have larger probability to be chosen than without heuristic information. Moreover, in the successful applications of ACO to nonstationary (time-varying) problems, such as data packet routing in AntNet <ref type="bibr" target="#b5">[6]</ref>, the function g is used to provide information to the algorithm about the current state of the problem.</p><p>There are several ways of integrating a similar mechanism in our algorithm. A particularly simple and elegant formulation is obtained when we replace the exponential selection rule (9) of our gradient algorithm with the following equation:</p><formula xml:id="formula_35">Pr(x t C1 D x | T , g, h t ) D 8 &gt; &lt; &gt; : 0 if x 2 h t , e at (x t ,x ) Cbg (x t ,x ) / X y2X y / 2h t e at (x t , y )Cbg(x t , y ) otherwise,<label>(11)</label></formula><p>where a and b ¸0 are two external parameters that play the same role as in the previous equation. <ref type="foot" target="#foot_10">12</ref> Equation 9 is obtained when a D 1 and b D 0. When a D 0 and b &gt; 0, the algorithm does not use the pheromone trails at all. It is then an iterative stochastic heuristic search similar to the rst stage of GRASP <ref type="bibr" target="#b14">[15]</ref>. Therefore, we dispose of a whole range of algorithms that extend from pure (heuristic-free) gradient-based reinforcement learning to simple (constant probability) stochastic heuristic search.</p><p>The next step is to recalculate the gradient to take into account the new selection rule. Once again, only the last part of the calculation is modi ed. We have the following:</p><formula xml:id="formula_36">if x 6 D x t ¡1 or y 2 h t ¡1 , then @ ln(Pr(x t | T , g, h t ¡1 )) @t (x, y ) D 0I if x D x t ¡1 and y D x t then @ ln(Pr(x t | T , g, h t ¡1 )) @t (x, y ) D @ ln Á e at (x, y )Cbg(x,y ) / X y 0 / 2h t ¡1</formula><p>e at (x, y 0 ) Cbg (x, y 0 ) ! @t (x, y ) , D @ ln(e at (x,y ) Cbg (x, y ) ) @t (x, y )</p><formula xml:id="formula_37">¡ @ ln Á X y 0 / 2h t ¡1</formula><p>e at (x,y 0 ) Cbg (x,y 0 ) ! @t (x , y ) ,</p><formula xml:id="formula_38">D a 0 B B @ 1 ¡ e at (x, y ) Cbg (x, y ) X y 0 / 2h t ¡1</formula><p>e at (x, y 0 ) Cbg (x, y 0 ) This leads to the following pheromone update rule, after absorbing the constant factor a in the learning rate a:</p><formula xml:id="formula_39">Dt (x t , x ) D a 1 L (h n ) (d (x t C1 D x ) ¡ Pr(x | T , g, h t ))</formula><p>The only difference with the update rule of the previous algorithm (Equation <ref type="formula" target="#formula_27">10</ref>) is that the heuristic-independent probability Pr (x | T , h t ) calculated following Equation <ref type="formula" target="#formula_24">9</ref>is replaced by the heuristic-dependent probability Pr(x | T , g, h t ) de ned by Equation <ref type="formula" target="#formula_35">11</ref>. Therefore, the basic principle of the heuristic-free algorithm generalizes to the new selection rule: Each ant has to memorize the probability distribution it uses at each step of its forward trip and then decrease the pheromones on its way back proportionally to these distributions.</p><p>It is important to note that, by varying the values of the external parameters a and b, we change two factors that strongly in uence the effectiveness of the algorithm. The rst is the shape of the "error" function E (de ned as a function from pheromone vectors to real number) that the algorithm is approximately "descending." It is not clear to us what is exactly the effect of the new selection rule on the "landscape" we are exploring. A globally optimum set of pheromones is still obtained by putting in nite weights on the best paths (that do not change when changing the parameter values), but some important aspects, such as the steepness of some peaks, are modi ed. The second and probably most important feature that is modi ed by varying parameters a and b is the sampling process used to estimate the gradient. The probability of sampling different paths and the update of the corresponding pheromones changes as a function of the parameter values. In the case of ATSP, the overall effect of using heuristic information (i.e., having b &gt; 0) is that when an unvisited city y is close to the current city x , it has a greater chance to be chosen than without heuristic information. On the other side, the pheromone trail t (x , y) is decreased by a larger amount each time y is considered as a candidate successor of x. Further research is needed to understand how better performance may be obtained using heuristic information appropriately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Using Discrete Local Search</head><p>The ACO metaheuristic is often used in conjunction with local search algorithms <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">32]</ref>. In this approach, an ACO algorithm generates starting points for a discrete local search routine. <ref type="foot" target="#foot_11">13</ref> Each ant produces a solution, say s 1 , which is then transformed into another solution, say s 2 , by the local search. Then the pheromones are updated. As our goal is to maximize the quality of the nal solution s 2 , pheromone updates must be proportional to the quality of s 2 , not s 1 . Given this, there are still two ways of updating the pheromones: either we reinforce the pheromones corresponding to the nal solution s 2 -in other words, we do as if the solution s 2 was generated directly by the ant algorithm, without the help of the local search (in this approach, we suppose that there is a mapping between the solution set and the set of possible forward trajectories); or we reinforce the pheromones corresponding to the intermediate solution s 1 .</p><p>By analogy with similar procedures in the area of genetic algorithms <ref type="bibr" target="#b33">[34]</ref>, we call the rst alternative the Lamarckian approach, and the second the Darwinian approach.</p><p>The main argument supporting the Lamarckian approach is that it is reasonable to think that, if the ant algorithm can be trained directly using the better solution s 2 , then it would be stupid to train it using the worse solution s 1 . In fact, in published ACO implementations, only the Lamarckian alternative has been used. In the case of SGD, however, the Darwinian approach may make more sense. It is easy to show that, if we try to maximize the expected value of the solution s 2 produced by the local search algorithm, then the update rule of an SGD algorithm is to reinforce the pheromones corresponding to the intermediate solution s 1 proportionally to the value of the nal solution s 2 . The formal framework developed in Section 2.5 can be used for this calculation, the effect of the local search being modeled in the function F . Having understood this, we can derive qualitative arguments in favor of the Darwinian approach. For instance, if the good starting points of the local search are very far from the corresponding local optima in the topology of the gradient algorithm, then the Darwinian approach could outperform the Lamarckian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Important Issues</head><p>By establishing connections with ANNs on one side, and reinforcement learning on the other side, we also show that ACO algorithms are concerned with two important issues paradigmatically illustrated in these techniques. They are, respectively, the issue of generalization and the exploration versus exploitation dilemma. In this section, we examine how these problems arise in ACO algorithms. It is clear that any reinforcement learning algorithm for combinatorial optimization has to deal with these two issues simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Generalization</head><p>The most famous application of SGD is surely the algorithm known as backpropagation in ANNs, and the issue the most studied in backpropagation is probably the ability of the algorithm to generalize over inputs <ref type="bibr" target="#b29">[30]</ref>. Stated simply, the problem is to nd a set of weights W D [w] for a network encoding a function F from an input set I to an output set O (say, O D R), so that F approximates as much as possible a target function F ¤ : I ! O. Backpropagation learns an optimal con guration of weights by observing a set of training examples, that is, pairs (i, F ¤ (i)) with i 2 I , and memorizing and generalizing these observations. In general, the input set I is a huge combinatorial set, if not an in nite set. Therefore, it is not possible to present every instance i 2 I in the training set. However, backpropagation is able to generalize the observed data to unseen instances. That is, it assumes that any unseen input i has a value F ¤ (i) that is close to the value of the observed examples that are similar to i in some sense. It is well known that the ability of an ANN to generalize and its ef ciency in generalization strongly depend on the network structure <ref type="bibr" target="#b4">[5]</ref>.</p><p>As we said, the backpropagation algorithm is an instance of SGD. More precisely, its overall effect is to descend the gradient of the mean square error</p><formula xml:id="formula_40">E MS D X i2I p i (F (i) ¡ F ¤ (i)) 2 D E[(F ¡ F ¤ ) 2 | W ],</formula><p>where [p i ] i2I is a given probability distribution on instances ( P i2I p i D 1). Note that, differently from the case of our ACO/SGD algorithm, the expectation in this equation is conditional on the weights W because the local error e MS def D (F ¡ F ¤ ) 2 depends on the weights, whereas the probabilities p i do not. Therefore,</p><formula xml:id="formula_41">@E MS @w D X i2I p i @e MS (i) @w D E µ @e MS @w | W ¶ .</formula><p>This result suggests an exact gradient algorithm that enumerates all possible inputs i 2 I for each step of gradient descent. Conversely, backpropagation uses an unbiased estimate of the gradient obtained by sampling a unique input i 2 I . After sampling input i and comparing the actual output F (i) and the desired output F ¤ (i), backpropagation updates the weights of the network following Dw D ¡a @e MS (i) @w .</p><p>for each weight w. The values of F (i) and F ¤ (i) are used here to calculate the partial derivative @e MS (i) /@w.</p><p>We see that the basic principles of backpropagation and of our algorithm are the same. It is, in both cases, a Monte-Carlo estimation of the gradient of a given error function, with respect to a set of weights attached to the components of a graph (the construction graph G in one case, and the ANN itself in the other case). Also, both algorithms are distributed and parallel implementation of this principle. Therefore, our algorithm should have, at least partially, the same ability to generalize observed data over unseen instances as backpropagation.</p><p>Actually, it is not dif cult to convince oneself that generalization is as big an issue in ACO in general as in ANNs. For instance, it is clear that the application of AS to the ATSP works by generalizing the observed solutions: If a majority of the sampled tours that traverse a given arc (x, y ) 2 N X 2 are of good quality, then the algorithm increases the probability of traversing this arc. In a sense, it assumes that, in general, the tours that traverse (x , y ) are of good quality. That is, it generalizes the observations. As in the case of backpropagation, the ability and ef ciency of an ACO algorithm to generalize is mostly determined by the structure of the graph, that is, the problem representation used by the arti cial ants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Exploration versus Exploitation</head><p>As we stressed above, the ACO metaheuristic can be seen as a way of applying reinforcement learning to combinatorial optimization problems. Thus, every ACO algorithm has to deal with one of the main issues in reinforcement learning: the exploration versus exploitation dilemma <ref type="bibr" target="#b18">[19]</ref>. This is the problem of nding an optimal compromise between obtaining additional information about the least known solutions (exploration), and maximizing rewards by taking the estimated best actions (exploitation). This problem is characteristic of real-time on-line learning, where one motivation is to learn as fast as possible, and another is to maximize the reward received during each experience.</p><p>In the case of ACO algorithms, the mechanisms of the exploration versus exploitation dilemma are intimately linked with those of generalization. For instance, in the application to the ATSP, since the pheromones are attached to pairs of cities (x , y ) 2 N X 2 , the algorithm is confronted with questions like: "Is it necessary to try again a given pair of cities that seems to be nonoptimal?" Clearly, different questions arise with different representations. In fact, it appears that an optimal solution to the exploration versus exploitation dilemma in the framework of ACO depends intimately on the problem representation, that is, the structure of the construction graph.</p><p>In practice, our ACO/SGD algorithm does not really address the issue of exploration, although it does face the dilemma. Its behavior is dictated by the trial-and-error search of SGD, independent of any consideration about exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this article, we explored the connections between the two techniques of ACO and SGD. First, we showed that the empirical designed AS algorithm is very similar to SGD in the space of pheromones, and we proposed a stable implementation of gradientbased reinforcement learning that belongs to the framework of ACO algorithms. Then we outlined a general ACO/SGD algorithm for combinatorial optimization. The performance of this algorithm depends crucially on some basic choices such as the problem representation and the objective function. This insight may be used to develop simple acceleration techniques for ACO algorithms, by transposing previous work on gradientbased reinforcement learning. Moreover, the most successful applications of the ACO metaheuristic suggest new ways of merging gradient descent with other optimization techniques for combinatorial optimization.</p><p>In conclusion, we believe that this work constitutes a signi cant step toward understanding the mechanisms at work in ACO algorithms, shedding new light on some important issues in the theory of these algorithms.</p></div>			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Stigmergy is a particular form of indirect communication used by social insects to coordinate their activities. Its role in the de nition of ant algorithms is discussed at length in<ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Although we are dealing with maximization problems using stochastic gradient ascent, we use in this article the vocabulary associated with gradient descent algorithms, which is much more common in the machine learning literature (see the example of arti cial neural networks in Sect. 3.2.1). The variable E represents the objective function of the gradient ascent algorithm (and not of the original combinatorial optimization problem), and it must be maximized. However, it plays the same role as the error function used in gradient descent algorithms.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>We borrow this vocabulary from reinforcement learning literature. It is used in a similar gradient-based algorithm for optimal control of Markov decision processes<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b34">35]</ref>. The eligibility of a weight is a measure of how much this weight will be involved in the next update. The Tx,y variables are called traces because they keep track of the eligibility of the weights at each step t.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>We may actually in uence the complexity by xing the number m of samples drawn to calculate the estimate of the gradient. More samples allow a more accurate estimate.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>Typically, the learning speed of a constant step-size SGD algorithm increases with the learning rate, up to a certain limit where the quality of the solution obtained decreases and the algorithm becomes unstable. By a "reasonable" learning rate value, we mean any value that is small enough to preserve the quality of the solution found and avoid instability.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>We assume here that the pheromone variables are stored as a set of tables T x D [t (x, y)] y2X , each T x being accessible to the arti cial ants (or "physically located") in city x 2 X.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>Another difference between the two algorithms is that the pheromone t (x n , x 1 ) between the last and rst city of the tour is reinforced by AS, whereas it is left untouche d by SGD.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>Conversely to the original AS, our gradient algorithm does not assume that the objective function is positive.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8"><p>That is, for which a constructive heuristic can be de ned<ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9"><p>Note that the construction process does not necessarily have the same "taboo" aspect as in ATSP, where each weight is used at most once during an ant's forward trip. The calculation of the SGD update is not more dif cult in this case. The general rule is that if an ant traverses a vertex of G (and uses the associated pheromone values) several times during its forward trip, it must traverse that vertex (and update the associated pheromone values) the same number of times during its backward trip.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_10"><p>Note that this formalism is compatible with negative heuristic functions. Therefore, we could choose g(x, y) D ¡d(x, y) in the case of ATSP.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_11"><p>Gradient descent is itself a local search procedure, but it operates in the continuous space of pheromones, whereas the discrete local search used here operates in the discrete set of solutions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_12"><p>Arti cial Life Volume 8, Number 2</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by a Marie Curie Fellowship awarded to Nicolas Meuleau (Contract No. HPMFCT-2000-00230 ). Marco Dorigo acknowledges support from the Belgian FNRS, of which he is a Senior Research Associate. This work was also partially supported by the "Metaheuristics Network," a Research Training Network funded by the Improving Human Potential programme of the CEC, grant HPRN-CT-1999-00106. The information provided is the sole responsibility of the authors and does not re ect the Community's opinion. The Community is not responsible for any use that might be made of data appearing in this publication.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Copyright of Artificial Life is the property of MIT Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gradient descent for general reinforcement learning</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Baird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Kearns</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Solla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Cohn</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="968" to="974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Nonlinear programming</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Athena Scienti c</publisher>
			<pubPlace>Belmont, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Swarm intelligence: From natural to arti cial systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bonabeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Theraulaz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Oxford Universit y Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Inspiration for optimization from social insect behavior</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bonabeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Theraulaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">406</biblScope>
			<biblScope unit="page" from="39" to="42" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Parallel and distributed processing</title>
		<author>
			<persName><forename type="first">K</forename><surname>Boyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>North-Holland</publisher>
			<pubPlace>Amsterdam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">AntNet: Distributed stigmergetic control for communications networks</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Caro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dorigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Arti cial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="317" to="365" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Optimization, learning and natural algorithms (in Italian). Unpublished doctoral dissertation, Dipartimento di Elettronica</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<pubPlace>Milano, Italy</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ant algorithms and stigmergy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bonabeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Theraulaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="851" to="871" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The ant colony optimization meta-heuristic</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Di Caro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">New ideas in optimization</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Corne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Glover</surname></persName>
		</editor>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="11" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ant algorithms for discrete optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Di Caro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arti cial Life</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="137" to="172" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Special issue on &quot;ant algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caro</forename><surname>Di</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Stützle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="851" to="956" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ant colony system: A cooperative learning approach to the traveling salesman problem</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="66" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Middendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ant algorithms and Swarm intelligence</title>
		<editor>
			<persName><forename type="first">T</forename><surname>St Ützle</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note>in press. Special issue</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The ant system: Optimization by a colony of cooperating agents</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Maniezzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Colorni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part B</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="41" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A probabilistic heuristic for a computationally dif cult set covering problem</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Feo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G C</forename><surname>Resende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research Letters</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="67" to="71" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ant colony system hybridized with a new local search for the sequential ordering problem</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INFORMS Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="237" to="255" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">MACS-VRPTW: A multiple ant colony system for vehicle routing problems with time windows</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">È</forename><forename type="middle">D</forename><surname>Taillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Agazzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">New ideas in optimization</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Corne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Glover</surname></persName>
		</editor>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="63" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ant colonies for the quadratic assignment problem</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">È</forename><forename type="middle">D</forename><surname>Taillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Operational Research Society</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="176" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning in embedded systems</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reinforcement learning: A survey</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Arti cial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="237" to="285" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The travelling salesman problem</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Lawler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Lenstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H G</forename><surname>Rinnooy Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Shmoys</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<publisher>Wiley</publisher>
			<pubPlace>Chichester, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An ANTS heuristic for the frequency assignment problem</title>
		<author>
			<persName><forename type="first">V</forename><surname>Maniezzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carbonaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="927" to="935" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The ant system applied to the quadratic assignment problem</title>
		<author>
			<persName><forename type="first">V</forename><surname>Maniezzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Colorni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Data and Knowledge Engineering</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="769" to="778" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ant colony optimization for resource-constraine d project scheduling</title>
		<author>
			<persName><forename type="first">D</forename><surname>Merkle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Middendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schmeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Genetic and Evolutionary Computation Conference</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Whitley</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Goldberg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Cantú-Paz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Spector</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Parmee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">&amp; H.-G</forename><surname>Beyer</surname></persName>
		</editor>
		<meeting>the Genetic and Evolutionary Computation Conference<address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="893" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An ACO algorithm for the shortest supersequenc e problem</title>
		<author>
			<persName><forename type="first">R</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Middendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">New ideas in optimization</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Corne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Glover</surname></persName>
		</editor>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="51" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Machine learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>McGraw-Hill</publisher>
			<pubPlace>Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning automata: An introduction</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Narendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A L</forename><surname>Thathachar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Local and global optimization algorithms for generalized learning automata</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Phansalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A L</forename><surname>Thathachar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="950" to="973" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<author>
			<persName><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Monroe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A stochastic approximation method</title>
		<imprint>
			<date type="published" when="1951">1951</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="400" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning internal representations by error backpropagation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel distributed processing: Explorations in the microstructures of cognition</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="318" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ant-based load balancing in telecommunications networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Schoonderwoerd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rothkrantz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adaptive Behavior</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="169" to="207" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The MAX -MIN ant system and local search for the traveling salesman problem</title>
		<author>
			<persName><forename type="first">T</forename><surname>Stützle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Hoos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1997 IEEE International Conference on Evolutionary Computation (ICEC&apos;97)</title>
		<meeting>the 1997 IEEE International Conference on Evolutionary Computation (ICEC&apos;97)<address><addrLine>Piscataway, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="309" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Lamarckian evolution, the Baldwin effect and function optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Whitley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mathias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of PPSN-III, Third International Conference on Parallel Problem Solving from Nature</title>
		<meeting>PPSN-III, Third International Conference on Parallel Problem Solving from Nature<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="6" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
