<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Power of Scale for Parameter-Efficient Prompt Tuning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
							<email>brianlester@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
							<email>nconstant@google.com</email>
						</author>
						<title level="a" type="main">The Power of Scale for Parameter-Efficient Prompt Tuning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we explore "prompt tuning", a simple yet effective mechanism for learning "soft prompts" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's "few-shot" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method "closes the gap" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed "prefix tuning" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the wide success of pre-trained large language models, a range of techniques have arisen to adapt these general-purpose models to downstream tasks. <ref type="bibr">ELMo (Peters et al., 2018)</ref> proposed freezing the pre-trained model and learning a taskspecific weighting of its per-layer representations. However, since BERT <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref>, the dominant adaptation technique has been "model tuning" (or "fine-tuning"), where all model parameters are tuned during adaptation. * Work done as a Google AI Resident. Figure <ref type="figure">1</ref>: Standard model tuning of T5 achieves strong performance, but requires storing separate copies of the model for each end task. Our prompt tuning of T5 matches the quality of model tuning as size increases, while enabling the reuse of a single frozen model for all tasks. Our approach significantly outperforms few-shot prompt design using GPT-3.</p><p>More recently, <ref type="bibr" target="#b5">Brown et al. (2020)</ref> showed that "prompt design" (or "priming") is surprisingly effective at modulating a frozen GPT-3 model's behavior through short text prompts. Prompts are typically composed of a task description and/or several canonical examples. The choice of description is important <ref type="bibr" target="#b19">(Jiang et al., 2020)</ref>, and performance improves as more examples are included in the prompt <ref type="bibr" target="#b5">(Brown et al., 2020)</ref>. This return to "freezing" pre-trained models is appealing, especially as model size continues to increase. Rather than requiring a separate copy of the model for each downstream task, a single generalist model can simultaneously serve many different tasks.</p><p>Unfortunately, prompt-based adaptation has several key drawbacks. First, task description is errorprone and requires human involvement. Second, the effectiveness of the prompt is limited by the number of conditioning examples that can fit in the model's maximum input length. Third, and most importantly, downstream task quality still Figure <ref type="figure">2</ref>: Model tuning requires making a task-specific copy of the entire pre-trained model for each downstream task, and inference must be performed in separate batches. Prompt tuning only requires storing a small taskspecific prompt for each task, and enables mixed-task inference using the original pre-trained model. Using a T5 "XXL" model, each additional copy of the tuned model would require 11 billion parameters. By contrast, our tuned prompts would only require 81,920 parameters per task-a reduction of over five orders of magnitude-given a prompt length of 20 tokens and embedding dimension 4,096.</p><p>lags far behind that of tuned models. For instance, GPT-3 175B few-shot performance on SuperGLUE is 17.5 points below fine-tuned T5-XXL <ref type="bibr">(Raffel et al., 2020) (71.8 vs. 89</ref>.3), despite using 16 times more parameters.</p><p>Several efforts to automate prompt design have been recently proposed. <ref type="bibr" target="#b44">Shin et al. (2020)</ref> propose a search algorithm over the discrete space of words, guided by the downstream application training data. While this technique outperforms manual prompt design, there is still a gap relative to model tuning. This may be because the loss as a function of the prompt tokens is non-differentiable and hard to optimize reliably in practice. <ref type="bibr" target="#b30">Li and Liang (2021)</ref> propose "prefix tuning" and show impressive results on generative tasks. This method freezes the language model parameters and backpropagates the error during tuning to prefix activations prepended to each transformer layer in the encoder stack, including the input layer. <ref type="bibr" target="#b15">Hambardzumyan et al. (2021)</ref> simplify this recipe by restricting the trainable parameters to the input and output sub-networks of a masked language model, and show reasonable results on classifications tasks.</p><p>In this paper, we propose prompt tuning as a further simplification for adapting language models. We freeze the entire pre-trained model and only allow an additional k tunable tokens per downstream task to be prepended to the input text. This "soft prompt" is trained end-to-end and can condense the signal from a full labeled dataset, allowing our method to outperform "few-shot" prompts, and close the quality gap with model tuning. At the same time, because a single pre-trained model is recycled for all downstream tasks, we retain the efficient serving benefits of frozen models, as illustrated in Figure <ref type="figure">2</ref>.</p><p>While we developed our method concurrently with Li and Liang (2021) and <ref type="bibr" target="#b15">Hambardzumyan et al. (2021)</ref>, we are the first to show that prompt tuning alone (with no further complexity such as intermediate-layer prefixes or task-specific output layers) is sufficient to be competitive with model tuning. Through detailed experiments in Sections 2-3, we demonstrate that language model capacity is a key ingredient for these approaches to succeed. As Figure <ref type="figure">1</ref> shows, prompt tuning becomes more competitive with scale. We offer a comparison with similar approaches in Section 4.</p><p>Explicitly separating task-specific parameters from "generalist" parameters (those needed for general language-understanding) has a range of additional benefits. In Section 5, we show that by capturing the task definition in the prompt while keeping the generalist parameters fixed, we are able to achieve better transfer learning when adapting to new domains. In Section 6, we show that "prompt ensembling" (learning multiple prompts for the same task) can increase quality and is more efficient than traditional model ensembling.</p><p>To summarize, our key contributions are: 1. Proposing prompt tuning and showing its competitiveness with model tuning in the regime of large language models. 2. Ablating many design choices, and showing quality and robustness improve with scale. 3. Showing prompt tuning outperforms model tuning on domain shift problems. 4. Proposing "prompt ensembling" and showing its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Prompt Tuning</head><p>Following the "text-to-text" approach of T5 <ref type="bibr" target="#b36">(Raffel et al., 2020)</ref>, we cast all tasks as text generation.</p><p>Instead of modeling classification as the probability of an output class given some input, p(y|X), where X is a series of tokens and y is a single class label, we now model it as conditional generation, where Y is a sequence of tokens that represent a class label. This is a natural fit for T5, as it is an encoder-decoder model. T5 models classification as p θ (Y |X), parameterized by the weights, θ, of the transformers <ref type="bibr" target="#b45">(Vaswani et al., 2017)</ref> that make up its encoder and decoder.</p><p>Prompting is the approach of adding extra information for the model to condition on during its generation of Y . Normally, prompting is done by prepending a series of tokens, P , to the input X, such that the model maximizes the likelihood of the correct Y , p θ (Y |[P ; X]), while keeping the model parameters, θ, fixed. In GPT-3, the representations of the prompt tokens, P = {p 0 , p 1 , . . . , p n }, are part of the model's embedding table, parameterized by the frozen θ. Finding an optimal prompt thus requires the selection of prompt tokens, through either manual search or non-differentiable search methods <ref type="bibr" target="#b19">(Jiang et al., 2020;</ref><ref type="bibr" target="#b44">Shin et al., 2020)</ref>. Prompt tuning removes the restriction that the prompt P be parameterized by θ; instead the prompt has its own dedicated parameters θ P that can be updated. While prompt design involves selecting prompt tokens from a fixed vocabulary of frozen embeddings, prompt tuning can be thought of as using a fixed prompt of special tokens, where only the embeddings of these prompt tokens can be updated. Our new conditional generation is now p θ;θ P (Y |[P ; X]) and can be trained by maximizing the likelihood of Y via backpropagation, while only applying gradient updates to θ P .</p><p>Given a series of n tokens, {x 0 , x 1 , . . . , x n }, the first thing T5 does is embed the tokens, forming a matrix X e ∈ R n×e where e is the dimension of the embedding space. Our soft-prompts are represented as a parameter P e ∈ R p×e , where p is the length of the prompt. Our prompt is then concatenated to the embedded input forming a single matrix [P e ; X e ] ∈ R (p+n)×e which then flows though the encoder-decoder as normal. Our models are trained to maximize the probability of Y , but only the prompt parameters P e are updated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Design Decisions</head><p>There are many possible ways to initialize the prompt representations. The simplest is to train from scratch, using random initialization. A more sophisticated option is to initialize each prompt token to an embedding drawn from the model's vocabulary. Conceptually, our soft-prompt modulates the frozen network's behavior in the same way as textual context preceding the input, so it follows that a word-like representation might serve as a good initialization spot. For classification tasks, a third option is to initialize the prompt with embeddings that represent an enumeration of the output classes, similar to the "verbalizers" of <ref type="bibr" target="#b41">Schick and Schütze (2020)</ref>. Since we want the model to produce these tokens in the output, initializing the prompt with the embeddings of the valid target tokens could "prime" the model to restrict its output to the legal output classes.</p><p>Another design consideration is the length of the prompt. The parameter cost of our method is EP , where E is the token embedding dimension and P is the length of the prompt. The shorter the prompt, the fewer new parameters must be tuned, so we aim to find a minimal length of prompt that still has strong performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Unlearning Span Corruption</head><p>Unlike autoregressive language models like GPT-3, the T5 models we experiment with use an encoderdecoder architecture and pre-train on a span corruption objective. Specifically, T5 is tasked with "reconstructing" masked spans in an input text string, which are marked with unique sentinel tokens, indicated here with X , Y , and Z . The target output text consists of all the masked content, separated by sentinels, plus a final sentinel. For instance, from the text "Thank you for inviting me to your party last week" we might construct a pretraining example where the input is "Thank you X me to your party Y week" and the target output is " X for inviting Y last Z ".</p><p>While <ref type="bibr" target="#b36">Raffel et al. (2020)</ref> find this architecture and pre-training objective to be more effective than traditional language modeling, we hypothesize that this setup is not a good fit for producing a "frozen" model that can be readily controlled through prompt tuning. In particular, a T5 model pre-trained exclusively on span corruption, such as T5.1.1, has never seen truly natural input text (free of sentinel tokens), nor has it ever been asked to predict truly natural targets. In fact, due to the details of T5's span corruption preprocessing, every pre-training target will begin with the sentinel X . While this "unnatural" tendency to output sentinels is easy to overcome through fine-tuning, we suspect that it would be much harder to override through a prompt alone, as the decoder priors cannot be adjusted.</p><p>Given these concerns, we experiment with T5 models in three settings. In the first case ("Span Corruption"), we use pre-trained T5 off-the-shelf as our frozen model, and test to what degree we can learn prompts that result in the natural text output expected for downstream tasks. In the second case ("Span Corruption + Sentinel"), we use the same model, but prepend all downstream task target text with the sentinel X , so as to more closely resemble the targets seen in pre-training. Finally, in the third case ("LM Adaptation"), we continue T5's self-supervised training for a small number of addition steps, but using the "prefix LM" objective discussed by <ref type="bibr" target="#b36">Raffel et al. (2020)</ref>: given a natural text prefix as input, the model must produce the natural text continuation as output. Crucially, this adaptation step is independent of the downstream task, and happens only once, producing a single frozen model that we can reuse for prompt tuning across a variety of tasks.</p><p>Through LM adaptation, we hope to be able to "quickly" transform T5 into a model more similar to GPT-3, which always outputs realistic text, and is known to respond well to prompts as a "fewshot learner". It is not immediately apparent how successful such a late-stage transformation will be (compared to pre-training from scratch), and it has not been investigated previously to our knowledge. As such, we experiment with various lengths of adaptation, between 10K and 100K steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>Our frozen models are built on top of pre-trained T5 checkpoints of all sizes (Small, Base, Large, XL, XXL). We leverage the public T5.1.1 checkpoints, which include a few improvements over the original T5: (i) the removal of all supervised data from pretraining, (ii) adjustments to hyperparameters d model and d ff , and (iii) the use of GeGLU <ref type="bibr" target="#b42">(Shazeer, 2020)</ref> over ReLU <ref type="bibr" target="#b33">(Nair and Hinton, 2010)</ref> activations.</p><p>We select an LM adapted version of T5 (trained for an additional 100K steps) as the default setting for our experiments, as we found it to have less variance and yield higher performing models. We set our default prompt size to 100 tokens and prepend these to the embedded input. While this is longer than the default 10-token prefix used by Li and Liang (2021), our method still uses fewer task-specific parameters, as we only tune the input layer, as opposed to overwriting activations in all network layers. See Figure <ref type="figure" target="#fig_3">4</ref> for a detailed comparison. We will also see shortly that even much shorter prompts are viable as model size increases.</p><p>We measure performance on the SuperGLUE benchmark <ref type="bibr" target="#b46">(Wang et al., 2019a;</ref><ref type="bibr" target="#b6">Clark et al., 2019;</ref><ref type="bibr" target="#b8">De Marneff et al., 2019;</ref><ref type="bibr" target="#b39">Roemmele et al., 2011;</ref><ref type="bibr" target="#b21">Khashabi et al., 2018;</ref><ref type="bibr" target="#b48">Zhang et al., 2018;</ref><ref type="bibr" target="#b7">Dagan et al., 2005;</ref><ref type="bibr" target="#b2">Bar-Haim et al., 2006;</ref><ref type="bibr" target="#b13">Giampiccolo et al., 2007;</ref><ref type="bibr" target="#b3">Bentivogli et al., 2009;</ref><ref type="bibr" target="#b34">Pilehvar and Camacho-Collados, 2018;</ref><ref type="bibr" target="#b27">Levesque et al., 2012)</ref>, a collection of eight challenging language understanding tasks designed to be summarized into a single metric. We report metrics on the development set associated with each dataset.</p><p>Each of our models train on a single SuperGLUE task; there was no multi-task setup or mixing of training data across tasks. We translate each Super-GLUE dataset into a text-to-text format following <ref type="bibr" target="#b36">Raffel et al. (2020)</ref>, except that we omit the task names prepended to inputs indicating which Super-GLUE task an example belongs to.</p><p>We train our prompts for 30,000 steps with a constant learning rate of 0.3 and a batch size of 32. The best checkpoints are selected via early stopping on the development set, where the stopping metric is either the default metric for that dataset (e.g. Accuracy for BoolQ), or the average of metrics in the cases where a dataset is evaluated on multiple metrics. The models were trained in JAX (Bradbury et al., 2018) using the Adafactor <ref type="bibr" target="#b43">(Shazeer and Stern, 2018)</ref> optimizer with weight decay 1e−5, β 2 decay 0.8, and parameter scaling turned off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Closing the Gap</head><p>To compare our method against standard model tuning, we tune the public T5.1.1 checkpoints on Su-perGLUE using the default hyperparameters specified in the T5 library (2 20 tokens per batch, learning rate 0.001, and Adafactor optimizer with pretraining parameter states restored). We use T5's multi-task fine-tuning setup to achieve a competi- tive baseline. 1 In this setup, a single model is tuned on all tasks jointly, with a text prefix indicating the task name. The performance of our model-tuned XXL baseline on the SuperGLUE dev set is one point higher (90.3 vs. 89.3) than T5's published 1 The T5 SuperGLUE submission used a more complex setup, first mixing multi-task supervised training data into pretraining, and then performing single-task fine-tuning. Since we use T5.1.1 throughout, this setup is unavailable, as the pretraining phase is fully self-supervised. In preliminary experiments, we found multi-task tuning to outperform single-task tuning, so we opt to use the stronger setting for our baseline. We follow <ref type="bibr" target="#b36">Raffel et al. (2020)</ref> in including DPR training in the multi-task mixture, which is known to improve performance on the WSC task <ref type="bibr" target="#b22">(Kocijan et al., 2019)</ref>. results on the SuperGLUE test set, supporting the view that this is a fairly strong baseline. <ref type="foot" target="#foot_0">2</ref>In our original Figure <ref type="figure">1</ref>, we see that prompt tuning becomes more competitive with the model tuning baseline as scale increases. At the XXL size (11 billion parameters), prompt tuning matches the performance of model tuning, despite having over 20,000 times fewer task-specific parameters.</p><p>To make a rough comparison with prefix design, we include GPT-3 few-shot performance on the SuperGLUE dev split, as reported by <ref type="bibr" target="#b5">Brown et al. (2020)</ref>. While the pre-training data and model architecture differ from T5, we can still take GPT-3 as a good representative of prompt design and observe overall trends. From Figure <ref type="figure">1</ref>, we see that prompt tuning beats GPT-3 prompt design by a large margin, with prompt-tuned T5-Small matching GPT-3 XL (over 16 times larger), and prompttuned T5-Large beating GPT-3 175B (over 220 times larger).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Prompt Initialization</head><p>We ablate the effect of prompt initialization by training models at all sizes while fixing the rest of the hyperparameters, using a prompt length of 100.</p><p>When using random initialization, we draw from a uniform distribution from −0.5 to 0.5. When initializing from the embeddings of sampled vocabulary items, we restrict the selection to only the 5,000 most "common" tokens in T5's Senten-cePiece vocabulary <ref type="bibr" target="#b23">(Kudo and Richardson, 2018)</ref>, which is ordered by likelihood in the pre-training corpus. For "class label" initialization, we take the embedding values for the string representations of each class in the downstream task and use them to initialize one of the tokens in the prompt. In cases where the class label is multi-token, we use the mean of the token embeddings. At longer prompt lengths, we often run out of class labels before we have initialized all of the prompt tokens. In this case we fall back to our embedding initialization strategy and sample from the most common 5,000 tokens to finish filling in the prompt. T5, and our work by extension, doesn't treat all of the Super-GLUE tasks as classification prompts: notably, the ReCoRD and WSC datasets involve the generation of short, free-form text. In these cases we initialize the prompts with words related to the task, "commonsense", "reasoning", "reading", and "comprehension" for ReCoRD and "commonsense", "pronoun", and "resolution" for WSC. Figure <ref type="figure" target="#fig_2">3</ref>(a) shows our ablation of initialization strategy across model sizes, where we find that the class based initialization performs best. At smaller model sizes, there are large gaps between the different initializations, but once the model is scaled to XXL size, those differences disappear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Prompt Length</head><p>We train prompts for each model size while varying the prompt length in {1, 5, 20, 100, 150}, while fix-ing the rest of the model hyperparameters. Specifically, we use the 100K-step LM-adapted frozen model, and class-label initialization. Figure <ref type="figure" target="#fig_2">3(b)</ref> shows that for most model sizes, increasing prompt length beyond a single token is critical for achieving good performance. Notably, the XXL model still gives strong results with a single-token prompt, suggesting that the larger the model, the less conditioning signal is needed to achieve the target behavior. Across all models, increasing beyond 20 tokens only yields marginal gains. Going past 100 tokens appears to be mildly detrimental for the larger models. A similar pattern of diminishing performance past a certain prompt length is observed by Li and Liang (2021) in the case of prefix tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Pre-training Objective</head><p>In Figures <ref type="figure" target="#fig_2">3(c</ref>) and 3(d), we see that pre-training objectives have a profound effect on prompt tuning quality. As hypothesized in Section 2.2, T5's default "span corruption" objective is generally not well-suited for training frozen models to be later conditioned by prompts. Intuitively, models pretrained to read and write sentinel tokens are hard to apply directly to tasks of reading and writing text without sentinels. As seen in Figure <ref type="figure" target="#fig_2">3</ref>(c), even the "workaround" of adding a sentinel to the downstream targets has little benefit. While LM adaptation (of 100K steps) adds value across all model sizes, we note that our largest XXL model is the most forgiving, and can achieve strong results even with span corruption.</p><p>Given the clear benefit of LM adaptation, we also explore how long of an adaptation is helpful. Figure <ref type="figure" target="#fig_2">3(d)</ref> shows that generally longer adaptation provides additional gains, up to 100K steps. This suggests that the "transition" from span corruption to a language modeling objective is not a trivial change, and making an effective switch takes an investment of training resources (10% of the steps of the original T5 pre-training). At the same time, as in our other ablations, we observe that the XXL model is robust to even non-ideal configurations. At this size, the gains from additional adaptation are quite modest.</p><p>One interesting side note is that in the (nonoptimal) "span corruption" setting, the Small model outperforms the larger Base, Large, and XL models. This is not due to random variance in the prompt tuning process, as we observed low variance across 3 runs for each data point. It could be that there is random luck in which pre-trained checkpoints are able to "overcome" span corruption through prompting alone. This hypothesis could be tested by pre-training new models from scratch. However, since LM adaptation provides an easy fix, we opt to go forward with that instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Comparison to Similar Approaches</head><p>Following the success of GPT-3, there has been a marked increase in the study of "prompting". Due to the impracticality of hand-designing prompts and the challenge of automatically searching for optimal prompts in the discrete text space, several recent works have explored learning a continuous version of prompts.</p><p>Li and Liang (2021) propose "prefix tuning": learning a sequence of prefixes that are prepended to the representation at every layer of the transformer. This is akin to learning transformer activations that are fixed across examples at every network layer. In contrast, prompt tuning uses a single prompt representation that is prepended to the embedded input. In addition to requiring less parameters, this choice allows the transformer to update the intermediate-layer task representations, as contextualized by an input example. Their work builds on GPT-2 <ref type="bibr" target="#b35">(Radford et al., 2019)</ref> and BART <ref type="bibr" target="#b29">(Lewis et al., 2020)</ref>, while ours focuses on T5, and examines changes in performance and robustness to design choices as model size increases. When using BART, prefix tuning includes learnable prefixes on both the encoder and decoder network, while prompt tuning requires prompts on only the encoder. As a final difference, Li and Liang (2021) rely on a reparameterization of the prefix in order to stabilize learning, which adds a large number of parameters during training, while we have found a set of hyperparameters that do not require this reparameterization and are robust across SuperGLUE tasks and model sizes. <ref type="bibr" target="#b15">Hambardzumyan et al. (2021)</ref> propose "WARP", a similar method where prompt parameters are only specified in the input layer, not at each layer of the transformer. However, their model is only designed to work with masked language models, given that it requires the inclusion of a [MASK] token and a learnable output layer that projects the mask to class logits. This formulation restricts the model to producing a single output, limiting the tasks it can perform to classification. Prompt tuning does not require any changes to the input, or a task-specific head, as it is a generative text-to-text model. The performance of prompt tuning is also considerably closer to the strong performance of model-tuning. <ref type="bibr" target="#b31">Liu et al. (2021)</ref> propose "P-tuning", a similar approach where continuous prompts are learned by inserting a trainable variable into the embedded input. These prompts are interleaved throughout the input, following patterns based on human design, while our approach removes this complication by only prepending the prompt to the beginning of the input. To achieve strong results on SuperGLUE, P-tuning has to be used in conjugation with model tuning: that is, their models jointly update both the prompt variables and the main model parameters, while our models keep the original language model frozen. In order to enforce a sequential nature to the prompts, <ref type="bibr" target="#b31">Liu et al. (2021)</ref> represent the prompts as the output of a BiLSTM <ref type="bibr" target="#b14">(Graves et al., 2013)</ref>, while we represent our prompts with a simple embedding table. Finally, P-tuning requires the addition of "anchor" tokens in the input (e.g. a question mark following the hypothesis in the RTE task) to achieve strong performance, while prompt tuning leaves the input untouched. <ref type="bibr" target="#b32">Logeswaran et al. (2020)</ref> use a similar learnable, prepended token to adapt transformer based models to a variety of tasks, but their work focuses on small synthetic datasets designed to accommodate a compositional task representation, as opposed to larger real-world datasets. Additionally, their base models are small transformers which are trained from scratch in tandem with the task representations, whereas we keep the base model frozen and investigate scaling laws using larger transformers.</p><p>Beyond the differences covered above, one important axis of comparison is the number of taskspecific parameters each method requires. We compare various approaches in Figure <ref type="figure" target="#fig_3">4</ref>. To aid comparison, we normalize to a fixed network architecture (T5.1.1), and fixed prompt/prefix lengths in the range 1-100. Among the methods that tune network parameters or activations, prompt tuning is the most parameter efficient. For models over a billion parameters, the task-specific parameters needed are less than 0.01% of the pre-trained model size. For prompt design, we count each token ID in the prompt as a parameter, and assume a longer prompt of between 500-2000 tokens to match the GPT-3 setting. Even with these longer prompts, this approach is by far the most parameter efficient, but this comes at the cost of task quality.</p><p>More generally, this line of work is closely aligned with work on "adapters" <ref type="bibr" target="#b38">(Rebuffi et al., 2017;</ref><ref type="bibr" target="#b17">Houlsby et al., 2019)</ref>, small bottleneck layers inserted between frozen pre-trained network layers. Adapters offer another means of reducing the number of task-specific parameters, with <ref type="bibr" target="#b17">Houlsby et al. (2019)</ref> achieving GLUE performance close to full model tuning when freezing BERT-Large and only adding 2-4% additional parameters. However, this still has over 100 times more parameters than our approach, which adds around 0.01% parameters for similarly-sized models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Resilience to Domain Shift</head><p>By freezing the parameters of the core language model, prompt tuning prevents the model from modifying its general understanding of language. Instead, prompt representations indirectly modulate the representation of the input. This reduces the model's ability to overfit to a dataset by memorizing specific lexical cues and spurious correlations. This restriction suggests that prompt tuning may help a model be more robust to domain shifts, where the distribution of inputs differs between training and evaluation, even though the underlying task is the same.</p><p>We test prompt tuning's ability to facilitate domain transfer in a zero-shot setting. We train a prompt on a source dataset, and then evaluate it on a target dataset that represents the same task, but in a different domain. No training is done on the target dataset. Our first experiment in domain transfer uses the two paraphrase detection datasets from GLUE <ref type="bibr" target="#b47">(Wang et al., 2019b)</ref>. The first dataset is the Quora Question Pairs (QQP) dataset <ref type="bibr" target="#b18">(Iyer et al., 2017)</ref>. For this task, models receive two questions as input, and must predict if they are "duplicates" of each other. The domain of this dataset is webtext from the community Q&amp;A site, Quora, and the examples are all drawn from questions asked on the site. The second dataset is the Microsoft Research Paraphrase Corpus (MRPC) <ref type="bibr" target="#b10">(Dolan and Brockett, 2005)</ref>. This dataset is similar to QQP in that a model must predict if one sentence is a re-phrasing of the other, but the data is drawn from news articles. We select the model that is to be run on the target dataset by picking the checkpoint that has the strongest performance on the validation set of the source dataset.</p><p>Table <ref type="table">1</ref> shows that training a lightweight prompt on the QQP data and evaluating on MRPC gives much better performance than the "heavyweight" approach of tuning the entire model (+13.9 accuracy and +0.7 F1). Prompt tuning wins in the other direction as well (transferring from the smaller MRPC dataset to the larger QQP), showing a +3.8 point improvement in accuracy, while maintaining similar F1. These results suggest that model tuning may be over-parameterized and more prone to overfit the training task, to the detriment of similar tasks in different domains.</p><p>We also test the generalization capability of prompt tuning using the 2019 Machine Reading for Question Answering (MRQA) shared task <ref type="bibr" target="#b12">(Fisch et al., 2019)</ref> on generalization. For this task, several different question answering datasets <ref type="bibr" target="#b11">(Dua et al., 2019;</ref><ref type="bibr" target="#b40">Saha et al., 2018;</ref><ref type="bibr" target="#b24">Lai et al., 2017;</ref><ref type="bibr" target="#b28">Levy et al., 2017;</ref><ref type="bibr" target="#b20">Kembhavi et al., 2017;</ref><ref type="bibr" target="#b37">Rajpurkar et al., 2016)</ref> are converted into a singular, extractive format. Furthermore, the datasets are divided into in-domain and out-of-domain groups. In-domain datasets are provided for model training, while outof-domain validation datasets are used to measure generalization to new domains. The model is still doing a single task (extractive question answering), but the distributions, patterns, and domain of the input change underneath it. Beyond providing the data itself, the MRQA shared task categorizes each dataset according to the domain of the input text. We train two models on SQuAD <ref type="bibr" target="#b37">(Rajpurkar et al., 2016)</ref>, the most widely used of the in-domain datasets, one using model tuning and the other using prompt tuning. After training on SQuAD, we select the best checkpoint, based on the F1 scores on the SQuAD validation set, and evaluate it on each of the out-of-domain datasets.</p><p>In Table <ref type="table" target="#tab_1">2</ref>, we see that prompt tuning gives stronger zero-shot performance than model tuning on the majority of out-of-domain datasets, with a remarkable 17.1 point gap in F1 scores between the two approaches on TextbookQA <ref type="bibr" target="#b20">(Kembhavi et al., 2017)</ref>. Of the out-of-domain datasets where model tuning is better, we see that DROP <ref type="bibr" target="#b11">(Dua et al., 2019)</ref> shares a domain (Wikipedia) with SQuAD and is thus one of the smallest domain transfers. In transfers with larger domain shifts (e.g. to Biomedical in BioASQ 3 or to Textbooks in TextbookQA), we see larger gains from prompt tuning. Interestingly, we observe that the prompt tuned model has stronger performance on the SQuAD development set for both exact match (+0.9) and F1 (+0.2). This again suggests that prompt tuning is better able to learn the essential task at hand without overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Prompt Ensembling</head><p>Ensembles of neural models trained from different initializations on the same data are widely observed to improve task performance <ref type="bibr" target="#b16">(Hansen and Salamon, 1990)</ref>, and are useful for estimating model uncertainty <ref type="bibr" target="#b25">(Lakshminarayanan et al., 2017)</ref> as model size increases, ensembling can become impractical. Beyond the space required to store N models (e.g. 42 GiB for each copy of T5-XXL), there is a substantial inference cost to running N distinct models, whether in parallel or in series.</p><p>Prompt tuning provides a more efficient way to ensemble multiple adaptations of a pre-trained language model. By training N prompts on the same task, we create N separate "models" for a task, while still sharing the core language modeling parameters throughout. Beyond drastically reducing storage costs, the prompt ensemble makes inference more efficient. To process one example, rather than computing forward passes of N different models, we can execute a single forward pass with a batch size of N , replicating the example across the batch and varying the prompt. These savings mirror those seen for multi-tasking in Figure <ref type="figure">2</ref>.</p><p>To demonstrate the viability of prompt ensembling, we train five prompts for each SuperGLUE task, using a single frozen T5-XXL model with 100K steps of LM adaptation and a prompt length of 100. We use simple majority voting to compute predictions from the ensemble. The results in Table 3 show that the prompt ensemble outperforms the single-prompt average on all tasks, and, with the exception of the RTE and WSC, outperforms any individual prompt. Our ensemble of prompts also produces a stronger overall SuperGLUE score than using the best performing prompt for each dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>In this paper, we showed that prompt tuning is a competitive technique for adapting frozen pretrained language models to downstream tasks. On the popular SuperGLUE benchmark, its task performance rivals that of traditional model tuning, with the gap vanishing as model size increases. On zero-shot domain transfer, we saw that prompt tuning leads to improved generalization ability. This is plausibly an indication that freezing generalpurpose language understanding parameters and restricting downstream learning to a lightweight parameter footprint can help to avoid overfitting to a specific domain.</p><p>Beyond task quality metrics, we discussed the appeal of moving to frozen pre-trained models in terms of storage and serving costs. This move enables both efficient multi-task serving, as well as efficient high-performing prompt ensembling.</p><p>The ability of our prompts to match the performance of model tuning suggests that task definitions exist in their own subspace of parameters. Looking forward, we believe that factoring out task-defining parameters as distinct from general language-modeling parameters is an exciting step that opens up several avenues for new research.</p><p>Task Complexity In the task subspace we can measure the capacity required to capture a specific task by varying the prompt length in the spirit of experiments conducted by <ref type="bibr" target="#b1">Aghajanyan et al. (2020)</ref>. This task complexity metric will enable us to understand better which categories of tasks require either more training data or higher capacity pre-trained language models. Task Similarity Measuring the similarity of tasks utilizing their prompt representations, similar to <ref type="bibr" target="#b0">Achille et al. (2019)</ref>, could provide an efficient means of searching the growing number of publicly available labeled datasets, and identifying which tasks could benefit each other.</p><p>Meta-Learning Relations in the task subspace might be deducible from their descriptions. For example, could we learn to translate from German to English by manipulating the task prompt for an English to German task with a learned operator that flips the order of the arguments? Learning these operators is active area of research <ref type="bibr" target="#b26">(Lampinen and McClelland, 2020)</ref> that would benefit from simple methods to learn task representations such as prompt tuning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Ablation study on the effect of various hyperparameters on prompt tuning performance. In our "best" (green '×') configuration, quality improves stably with model size. Across all ablations, the largest (XXL) model shows the most robustness to hyperparameter choice. Lines show mean and standard deviation across 3 runs. (a) Prompt initialization scheme: Random uniform initialization lags behind more "advanced" initializations using sampled vocabulary or class label embeddings, but the difference vanishes at XXL size. (b) Prompt length: Increasing to 20 or more tokens generally confers a large boost, but XXL performs well even with single-token prompts. (c) Pre-training objective: LM adaptation outperforms span corruption, even when a sentinel is added to downstream task targets, but XXL works well with any method. (d) LM adaptation: As little as 10K steps of LM adaptation boosts performance dramatically across all model sizes. Longer adaptation generally gives additional gains, but XXL is robust to even very short adaptation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of how many parameters worth of task-specific conditioning information is used by each method. To aid comparison, we fix the network architecture to T5.1.1 and fix the prefix/prompt lengths to between 1-100 tokens, with bands showing mean and standard deviation. Model Tuning: All parameters are task-specific. Prefix Tuning: Activations are tuned in the prefix of each network layer, resulting in between 0.1-1% task-specific parameters during inference, though more are used in training. WARP: Task parameters are reduced to under 0.1% by only tuning input and output layers. Prompt Tuning: Only prompt embeddings are tuned, reaching under 0.01% for most model sizes. Prompt Design: Only a sequence of prompt IDs (500-2000 tokens) is required.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Models trained on SQuAD, evaluated on the out-of-domain datasets from the MRQA 2019 shared task in a zero-shot fashion. We see that prompt tuning has stronger zero-shot performance than model tuning. The datasets where model tuning has an advantage are some of the smallest domain transfers (e.g. Wikipedia to Wikipedia), while prompt tuning really shines on large domain shifts like TextbookQA.</figDesc><table><row><cell>. However,</cell></row><row><cell>3 http://bioasq.org/</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance of a five-prompt ensemble, using a single frozen T5-XXL model throughout. Prompt ensmebling outperforms the strongest prompt on all datasets except for RTE and WSC. The ensemble Su-perGLUE score also outperforms using the best prompt from each dataset.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Metric Avg. Best Ensemble</cell></row><row><cell>BoolQ</cell><cell>Acc.</cell><cell>91.0 91.2</cell><cell>91.6</cell></row><row><cell>COPA</cell><cell>Acc.</cell><cell>97.2 98.0</cell><cell>99.0</cell></row><row><cell>CB</cell><cell>Acc.</cell><cell>92.1 98.2</cell><cell>100.0</cell></row><row><cell></cell><cell>F1a</cell><cell>92.5 96.4</cell><cell>100.0</cell></row><row><cell>RTE</cell><cell>Acc.</cell><cell>92.4 93.5</cell><cell>92.8</cell></row><row><cell>ReCoRD</cell><cell>EM</cell><cell>88.4 90.2</cell><cell>90.6</cell></row><row><cell></cell><cell>F1</cell><cell>90.5 91.8</cell><cell>92.0</cell></row><row><cell>MultiRC</cell><cell>EM</cell><cell>65.7 66.3</cell><cell>67.2</cell></row><row><cell></cell><cell>F1</cell><cell>89.0 89.0</cell><cell>89.4</cell></row><row><cell>WiC</cell><cell>Acc.</cell><cell>76.2 76.8</cell><cell>77.9</cell></row><row><cell>WSC</cell><cell>Acc.</cell><cell>91.7 93.3</cell><cell>93.3</cell></row><row><cell>SuperGLUE</cell><cell></cell><cell>88.5 89.8</cell><cell>90.5</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">For comparison, the SuperGLUE baseline "BERT++" is</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">.1 points higher on dev than test, suggesting that while the dev set is somewhat easier, our baseline is still fairly close to the best-reported T5 performance.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Lucas Dixon, Waleed Ammar, Slav Petrov and Sebastian Ruder for comments on an earlier draft, and the following people for helpful discussion: Colin Raffel, Adam Roberts, and Noam Shazeer. We thank Linting Xue for help with the LM adaptation training.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Task2vec: Task embedding for meta-learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00653</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
				<meeting><address><addrLine>CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6429" to="6438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Intrinsic dimensionality explains the effectiveness of language model fine-tuning</title>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<idno>ArXiv, abs/2012.13255</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The second PASCAL recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Roy</forename><surname>Bar-Haim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second PASCAL challenges workshop on recognising textual entailment</title>
				<meeting>the second PASCAL challenges workshop on recognising textual entailment</meeting>
		<imprint>
			<publisher>Venice</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="6" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The fifth PASCAL recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<editor>TAC</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">James</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Necula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skye</forename><surname>Wanderman-Milne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">JAX: composable transformations of Python+NumPy programs</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Alec Radford, Ilya Sutskever, and Dario Amodei</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>Language models are few-shot learners</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BoolQ: Exploring the surprising difficulty of natural yes/no questions</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1300</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2924" to="2936" />
		</imprint>
	</monogr>
	<note>Minneapolis</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The PASCAL recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges Workshop</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The CommitmentBank: Investigating projection in naturally occurring discourse. proceedings of Sinn und Bedeutung 23</title>
		<author>
			<persName><forename type="first">Marie-Catherine De</forename><surname>Marneff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandy</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judith</forename><surname>Tonhauser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Paraphrasing (IWP2005)</title>
				<meeting>the Third International Workshop on Paraphrasing (IWP2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs</title>
		<author>
			<persName><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1246</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2368" to="2378" />
		</imprint>
	</monogr>
	<note>Minneapolis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">MRQA 2019 shared task: Evaluating generalization in reading comprehension</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2nd Machine Reading for Reading Comprehension (MRQA) Workshop at EMNLP</title>
				<meeting>2nd Machine Reading for Reading Comprehension (MRQA) Workshop at EMNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The third PASCAL recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing</title>
				<meeting>the ACL-PASCAL workshop on textual entailment and paraphrasing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2013.6638947</idno>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Karen</forename><surname>Hambardzumyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrant</forename><surname>Khachatrian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00121</idno>
		<title level="m">WARP: Word-level adversarial reprogramming</title>
				<imprint>
			<date type="published" when="2021-05">May. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural network ensembles</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Salamon</surname></persName>
		</author>
		<idno type="DOI">10.1109/34.58871</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="993" to="1001" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for NLP</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
				<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="2790" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">First quora dataset release: Question pairs</title>
		<author>
			<persName><forename type="first">Shankar</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Dandekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kornel</forename><surname>Csernai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">How can we know what language models know? Transactions of the</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00324</idno>
		<imprint>
			<date type="published" when="2020-06">Jun Araki, and Graham Neubig. 2020</date>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="423" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.571</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5376" to="5384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Looking beyond the surface: A challenge set for reading comprehension over multiple sentences</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of North American Chapter</title>
				<meeting>North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A surprisingly robust trick for the Winograd schema challenge</title>
		<author>
			<persName><forename type="first">Ana-Maria</forename><surname>Vid Kocijan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oana-Maria</forename><surname>Cretu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yordan</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Yordanov</surname></persName>
		</author>
		<author>
			<persName><surname>Lukasiewicz</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1478</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4837" to="4842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-2012</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">RACE: Large-scale ReAding comprehension dataset from examinations</title>
		<author>
			<persName><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1082</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Balaji Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Transforming task representations to perform novel tasks</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">L</forename><surname>Andrew K Lampinen</surname></persName>
		</author>
		<author>
			<persName><surname>Mcclelland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">52</biblScope>
			<biblScope unit="page" from="32970" to="32981" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The winograd schema challenge</title>
		<author>
			<persName><forename type="first">Hector</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Zero-shot relation extraction via reading comprehension</title>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K17-1034</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
				<meeting>the 21st Conference on Computational Natural Language Learning<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017. CoNLL 2017</date>
			<biblScope unit="page" from="333" to="342" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">Marjan Ghazvininejad,. 2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Prefixtuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00190</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10385</idno>
		<title level="m">GPT understands, too</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">'</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09543</idno>
		<title level="m">Few-shot Sequence Learning with Transformers</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<editor>
			<persName><forename type="first">Icml</forename><forename type="middle">Matthew</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</editor>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010. 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
	<note>Deep contextualized word representations</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">WiC: 10,000 example pairs for evaluating context-sensitive representations</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pilehvar</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<idno>CoRR, abs/1808.09121</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-totext transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<title level="m">SQuAD: 100,000+ questions for machine comprehension of text</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning multiple visual domains with residual adapters</title>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Choice of plausible alternatives: An evaluation of commonsense causal reasoning</title>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Roemmele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cosmin</forename><surname>Adrian Bejan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">S</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 AAAI Spring Symposium Series</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">DuoRC: Towards complex language understanding with paraphrased reading comprehension</title>
		<author>
			<persName><forename type="first">Amrita</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Aralikatte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName><surname>Sankaranarayanan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1156</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1683" to="1693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Few-shot text generation with pattern-exploiting training</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.11926</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05202</idno>
		<title level="m">GLU variants improve transformer</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Adafactor: Adaptive learning rates with sublinear memory cost</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
				<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4596" to="4604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.346</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4222" to="4235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00537</idno>
		<title level="m">Super-GLUE: A stickier benchmark for general-purpose language understanding systems</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note>In the Proceedings of ICLR</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12885</idno>
		<title level="m">Record: Bridging the gap between human and machine commonsense reading comprehension</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
