<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ReST-Net: Diverse Activation Modules and Parallel Subnets-Based CNN for Spatial Image Steganalysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Weiha</forename><surname>Wei</surname></persName>
							<idno type="ORCID">0000-0003-1738-2984</idno>
							<affiliation key="aff0">
								<orgName type="laboratory">Guangdong Key Laboratory of Intelligent In-formation Processing and Shenzhen Key Laboratory of Media Security</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<postCode>518060</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><roleName>Member, IEEE</roleName><forename type="first">Anselmo</forename><surname>Ferreira</surname></persName>
							<email>anselmo@szu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Guangdong Key Laboratory of Intelligent In-formation Processing and Shenzhen Key Laboratory of Media Security</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<postCode>518060</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ReST-Net: Diverse Activation Modules and Parallel Subnets-Based CNN for Spatial Image Steganalysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6784E25331D743CA72C3A440BE9FF36A</idno>
					<idno type="DOI">10.1109/LSP.2018.2816569</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convolutional neural networks</term>
					<term>diverse activation</term>
					<term>steganalysis</term>
					<term>steganography</term>
					<term>wide structure</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent steganalytic schemes reveal embedding traces in a promising way by using convolutional neural networks (CNNs). However, further improvements, such as exploring complementary data processing operations and using wider structures, were not extensively studied so far. In this letter, we design a new CNN in these aspects in order to better capture embedding artifacts. Specifically, on the one hand, we propose to process information diversely with a module called diverse activation module. On the other hand, we build a wide structure with parallel subnets using several filter groups for preprocessing. To accelerate the training process, we pretrain the subnets independently. Extensive experiments show that the proposed method is effective in detecting content-adaptive steganographic schemes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Covert communication with steganography <ref type="bibr" target="#b0">[1]</ref> has attracted considerable attention in recent years. Modern image steganographic schemes, such as <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b5">[6]</ref>, are considered safer as they hide information in an adaptive way so that it is difficult to detect their embedding traces. Steganography can be used for criminal purpose <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Consequently, detecting steganography, also called steganalysis <ref type="bibr" target="#b8">[9]</ref>, is an important task.</p><p>Most of steganalytic approaches were based on hand-crafted features allied to machine-learning classifiers <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b13">[14]</ref>. This reality started to change since the rise of convolutional neural networks (CNNs) <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b17">[18]</ref>. Such data-driven methods have achieved satisfactory performance in image classification. Several CNN solutions tried to address the spatial image steganalysis problem <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b23">[24]</ref>. However, most of these schemes use only one learning pipeline. Further improvements, such as using wider structures, were not extensively explored so far.</p><p>In this letter, we propose a more effective data-driven CNN structure for steganalysis. On the one hand, inspired by the inception module <ref type="bibr" target="#b16">[17]</ref> which increases the width of CNN architecture with convolutional kernels of different sizes, we propose a CNN architecture composed of diverse activation modules (DAMs), which activate the convolution outputs differently and then concatenate their outputs for the subsequent layers. This way, more clues of data embedding are passed through the network, making the detection of embedding traces more accurate. Since ReLU, Sigmoid, and TanH activation functions are used, we name our network as ReST-Net. On the other hand, inspired by the diverse submodels used in Spatial Rich Models (SRMs) <ref type="bibr" target="#b10">[11]</ref>, we construct a structure with multiple subnets, whose inputs are preprocessed with several groups of high-pass filters. Consequently, the embedding traces are highlighted in a complementary way. In the experiments using the BOSSBase image set <ref type="bibr" target="#b24">[25]</ref>, we show that the proposed approach outperforms the established methods.</p><p>The remaining of this letter is organized as follows. In Section II, we review some data-driven approaches for image steganalysis. We present our CNN architecture with multiple activation units and parallel sub-nets in Section III. In Section IV, we introduce the experimental settings and report the results. This letter is concluded in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The first work to consider deep learning structures for image steganalysis was done by Tan and Li with convolutional autoencoders <ref type="bibr" target="#b18">[19]</ref>. Qian et al. <ref type="bibr" target="#b19">[20]</ref> proposed a CNN structure comprised of a preprocessing layer equipped with a high-pass filter. A breakthrough was achieved by Xu et al. <ref type="bibr" target="#b20">[21]</ref>. Their method (called Xu-CNN in this letter) is comparable with the hand-crafted SRM feature-based method for the first time. The authors considered several advanced CNN techniques used in image classification tasks, such as batch normalization (BN) <ref type="bibr" target="#b25">[26]</ref>, 1×1 convolution, and global average pooling <ref type="bibr" target="#b26">[27]</ref>. They also applied steganalytic domain knowledge-based operations, such as preprocessing with a high-pass filter and using an absolute activation layer. By using an ensemble of a modified version of Xu-CNN, a more stable performance can be achieved <ref type="bibr" target="#b21">[22]</ref>. By mimicking the process of another effective hand-crafted feature-based steganalytic method, called Projection SRM <ref type="bibr" target="#b11">[12]</ref>, Sedighi and Fridrich <ref type="bibr" target="#b22">[23]</ref> proposed a CNN structure with histogram layers, formed by a set of mean-shifted Gaussian activation functions. Ye et al. <ref type="bibr" target="#b23">[24]</ref> proposed a CNN structure with a group of high-pass filters for preprocessing and adopted a new activation function, called truncated linear unit (TLU), to better capture the embedding signals. It has been shown that a wider architecture may improve the performance of CNNs. However, it has not been extensively explored in steganalysis so far. This motivates us to design a wider CNN, with more parallel activation units and processing pipelines. We will discuss our proposed method in the next section. Fig. <ref type="figure">2</ref>. Proposed pretrained subnet with DAM. A convolutional group is represented by a blue box. N is the number of filter residuals. Conv(x 1 , a × a, x 2 ) denotes the convolution layer with the kernel size a × a for x 1 input feature maps and x 2 output feature maps. Batch normalization is abbreviated as BN. Absolute activation is abbreviated as ABS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architecture Overview</head><p>The proposed CNN structure, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, is composed of three parallel convolutional subnets and a fully connected classification module. Each subnet accepts an input image of size 512 × 512 and outputs a 256-D feature vector. These subnets act as data-driven feature extractors, and are built based on Xu-CNN <ref type="bibr" target="#b20">[21]</ref> but equipped with DAMs in some convolutional groups, which are further described in Section III-B. The only difference among subnets is the way they preprocess the input, which are explained in Section III-C.</p><p>The proposed CNN is trained in two phases. In the first phase, each subnet is pretrained independently with a fully connected layer and a Softmax function, as shown in Fig. <ref type="figure">2</ref>, to classify cover and stego images. Once the pretraining is done, the parameters in the subnets are frozen without further training, and the fully connected layers are discarded. In the second phase, a new fully connected layer with 768 (256 × 3) input neurons is fed with the concatenated output feature vectors from the final convolutional groups of all three subnets. Such a fully connected layer with a Softmax function is trained and acts as the final classification module. This training process with two phases, i.e., training the subnets for feature extraction, and training the fully connected layer for classification, can ensure stable and efficient convergence compared with training the proposed CNN as a whole without pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Diverse Activation Modules</head><p>It has been widely accepted that wider networks can carry more significant information through CNN layers. For example, the inception module <ref type="bibr" target="#b16">[17]</ref> increases the width by using convolutional kernels of different sizes. Inspired by the idea of increasing the width to boost performance, we adapt Xu-CNN <ref type="bibr" target="#b20">[21]</ref> by using what we call the DAMs. In Xu-CNN, TanH is used for activation in the first two convolutional groups, and ReLU is used in the last three. From the results reported in [21], when the activation functions are replaced, the performance degrades. In order to learn the steganography artifacts differently, we form DAMs by using ReLU, Sigmoid, and TanH activation functions simultaneously in the second and the fourth convolutional groups, and concatenating the resulting feature maps for the subsequent convolutional groups. The proposed subnet architecture can be seen in Fig. <ref type="figure">2</ref>. According to the initial letters of the activation units, we call the network as ReST-Net. It is expected that each activation unit in DAMs may have different responses to the embedding traces. The diversity may help to boost classification performance, as shown in the experiment in the next section. Note that the DAM is not used in all convolutional groups because the number of the weights in convolutional kernels may increase, making such a wide network less efficient for convergence. The slim structure for the third and the fifth convolutional groups in the proposed network is similar to the "bottleneck block" in ResNet <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Parallel Subnets with Different Preprocessing Operations</head><p>The structure of the three subnets are identical, except for their preprocessing layers with high-pass filtering operations, as shown in Fig. <ref type="figure">2</ref>, where N is the number of the filtered residuals. The high-pass filtering technique is pervasively used in steganalysis <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b12">[13]</ref> to capture embedding artifacts. CNNbased methods <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b23">[24]</ref> also benefits from such a technique. One may speculate that involving more high-pass filters could boost the steganalytic performance. However, as shown in our experiments in Section IV-B, simply stacking a large number of high-pass filters cannot perform the best. It is more effective when we use subnets equipped with different sets of high-pass filters. Linear filters and nonlinear filters from SRM <ref type="bibr" target="#b10">[11]</ref>, together with Gabor filters <ref type="bibr" target="#b27">[28]</ref>, are, respectively, employed in the three subnets. The SRM linear filters can capture complicated linear relationships among image pixels, and were effectively used in <ref type="bibr" target="#b23">[24]</ref>. The SRM nonlinear filters introduce nonlinearity and diversity to capture stego abnormality, but were not tried in previous CNN designs. The Gabor filters are complementary to SRM filters, for that they can analyze the image with a specific frequency in a specific direction. The details of the preprocessing layer for each subnet are explained as follows:</p><p>1) Subnet #1: The input image is preprocessed by filtering with a set of 6 × 6 Gabor filters <ref type="bibr" target="#b27">[28]</ref>, and the resulting images are the input of the first convolutional block. The Gabor filter is defined as the product of a Gaussian function and a cosine function as shown in the following equation:</p><formula xml:id="formula_0">g(x, y) = exp - x 2 + γ 2 y 2 2σ 2 cos 2π x λ (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where x = x cos θ + y sin θ, y = -x sin θ + y cos θ, θ is the orientation of the filter, σ is the scale parameter,  λ = σ 0.56 is the wavelength of the cosine function, and γ = 0.5 is the spatial aspect ratio to specify the Gaussian ellipticity. We use eight orientations (θ from 0 to 7π/8) with two scales (σ = 1 and σ = 0.5); consequently, the number of filters and residual maps yielded are 16. As done before in <ref type="bibr" target="#b27">[28]</ref>, all the filters are made zero-mean by subtracting the mean of the filter elements. The filters are illustrated in Fig. <ref type="figure">3</ref> for better understanding.</p><p>2) Subnet #2: The input image is preprocessed by linear filtering with a set of high-pass filters from SRM <ref type="bibr" target="#b10">[11]</ref>. We pad the filters with zeros to obtain a unified size of 5 × 5.</p><p>The filters are renamed and grouped into nine classes. As shown in Fig. <ref type="figure">4</ref>, there are four filters with different angles in the first seven filter classes. We selected some filters for use as follows:</p><formula xml:id="formula_2">D 0 • 1 , D 90 • 1 , D 0 • 2 , D 90 • 2 , D 0 • 3 , D 90 • 3 , D 0 • 4 , D 90 • 4 , D 0 • 5 , D 90 • 5 , D 0 • 6 , D 90 • 6 , D 0 • 7 , D 90 • 7 , D 8</formula><p>, and D 9 . As a result, 16 linear residual images are obtained and used as the preprocessed inputs for subnet #2.</p><p>3) Subnet #3: In order to introduce nonlinearity, the input image is first preprocessed by filtering with some SRM high-pass filters discussed before, and then the resultant residual images are nonlinearly processed with "max" or "min" operation, as done in <ref type="bibr" target="#b10">[11]</ref>. Denote R D the residual image by convolving an image with a filter D. The "max" (or "min") operator computes the maximum (or minimum) values among the residual images within a filter class. The output residual is denoted by</p><formula xml:id="formula_3">R max D (or R min D ). For example, R max D 1 = max R D 0 • 1 , R D 9 0 • 1 , R D 1 8 0 • 1 , R D 2 7 0 • 1 (2) R min D 1 = min R D 0 • 1 , R D 9 0 • 1 , R D 1 8 0 • 1 , R D 2 7 0 • 1 .<label>(3)</label></formula><p>As shown in Fig. <ref type="figure">4</ref>, the filters in the first seven classes are processed in this manner. As a result, 14 nonlinear residual images are obtained and used as the preprocessed inputs for subnet #3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Setup</head><p>We used the BOSSBase v1.01 dataset <ref type="bibr" target="#b24">[25]</ref>, which contains 10 000 uncompressed images of size 512 × 512 and S- UNIWARD <ref type="bibr" target="#b3">[4]</ref>, HILL <ref type="bibr" target="#b4">[5]</ref>, and CMD-HILL <ref type="bibr" target="#b5">[6]</ref> for data embedding, with the payload from 0.1 to 0.5 bit per pixel. The images were randomly split into a training set with 4 000 cover and stego image pairs, a validation set with 1 000 image pairs, and a testing set containing 5 000 image pairs. We used Tensorflow v1.1 for implementation. The network weights were initialized using a normal distribution with zero mean and standard deviation 0.01. The learning algorithm was the minibatch gradient descent with a 0.9 momentum. The initial learning rate was set to 0.001 and the learning decay was set to 90% every 5 000 training steps. The batch size was set to 40, with 20 cover images and their corresponding stego counterparts. The training decay was set to 0.05. The epsilon in BN layer was 0.001. We used 1 000 and 50 training epochs to train subnets and the classification module, respectively. The performance was evaluated by the testing accuracy, where the best validation model obtained during training was selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Comparison to Existing Methods:</head><p>We conducted experiments to compare the proposed approach with Xu-CNN <ref type="bibr" target="#b20">[21]</ref> and the method in <ref type="bibr" target="#b23">[24]</ref> without the selection-channel information (which is called TLU-CNN). Table <ref type="table" target="#tab_0">I</ref> shows the results. First, it can be observed that in most cases all three individual subnets outperformed the considered baseline methods. Considering that 30 high-pass filters are used in TLU-CNN <ref type="bibr" target="#b23">[24]</ref> and only 14 to 16 filters are used in the proposed subnets, we may conclude that using the DAMs rather than increasing the number of high-pass filters takes effect. Second, ReST-Net had the best improvement in all the experiments. On S-UNIWARD, HILL, and CMD-HILL with different payloads, ReST-Net got an averaged accuracy improvement of 5.14%, 3.32%, and 3.83% over Xu-CNN, and 5.77%, 5.27%, and 2.83% over TLU-CNN, respectively. The fact that the overall ReST-Net performed better than any of the subnets justifies the effectiveness of the proposed parallel subnets structure.</p><p>2) Further Investigation on DAM: We would like to find out the benefits of using the proposed DAMs. For that, we performed experiments by comparing how the individual subnets and the ensemble of these subnets were affected by removing the DAM and only using one kind of activation function selected from ReLU, Sigmoid, and TanH. Results in Table <ref type="table" target="#tab_1">II</ref> show significant decreases in detection accuracy, indicating the benefits of simultaneously using several activation functions. In addition, we performed experiments by computing the correlation coefficients between the averaged gradient values of each activation unit in the second and the fourth convolutional group for subnet #2 during the training stage.  units perform differently. Similar to ensemble learning systems where independence makes benefits, the diversity of the activation units improves the performance.</p><p>3) Further Investigation on Parallel Structure: We would like to find out how the parallel structure affects the performance. To this end, we show experimental results with the following six cases. Case I: the use of only one subnet by using Gabor, SRM linear, and SRM nonlinear preprocessing filters all together (N = 44, where N is defined in Section III-C). Case II: the use of only one subnet by using SRM linear and nonlinear preprocessing filters together (N = 28). Case III: the use of subnet #1 and #2. Case IV: the use of subnet #1 and #3. Case V: the use of subnet #2 and #3. Case VI: the use of four subnets by replacing subnet #1 with two new subnets, where Gabor filters (N = 8) with scales σ = 0.5 and σ = 1 are, respectively, employed in the new subnets. The results for detecting S-UNIWARD are shown in Table <ref type="table" target="#tab_0">IV</ref>. We can observe that, when only one subnet is used, even though we increase the number of filters for preprocessing, it is not as effective as using more subnets with less number of filters. Besides, as the number of subnets increases, better performance can be obtained. To balance detection performance and model complexity, ReST-Net with three subnets is recommended.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>Performing steganalysis using data-driven models have gradually evolved in the past few years. In this letter, we propose a new CNN architecture adapted from Xu-CNN with three parallel subnets in order to consider more preprocessed information. Our CNN also uses the DAMs to activate the convolved data differently. The contributions of this letter rely on two aspects: (i) proposing DAMs to learn steganographic artifacts in a diverse way; and (ii) showing that high-pass preprocessing operations in a parallel structure is better than simply stacking several filters together. As the proposed network is inspired by the inception module <ref type="bibr" target="#b16">[17]</ref>, other effective structures for image classification, such as ResNet <ref type="bibr" target="#b17">[18]</ref> and DenseNet <ref type="bibr" target="#b28">[29]</ref>, may help for the design of new steganalysis architectures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Proposed CNN architecture with three parallel subnets and a classification module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Illustration of Gabor filters with different orientations and scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I DETECTION</head><label>I</label><figDesc>ACCURACIES (IN %) FOR S-UNIWARD, HILL, AND CMD-HILL</figDesc><table><row><cell>CNN scheme</cell><cell></cell><cell cols="3">S-UNIWARD (bpp)</cell><cell></cell><cell></cell><cell></cell><cell>HILL (bpp)</cell><cell></cell><cell></cell><cell></cell><cell cols="3">CMD-HILL (bpp)</cell><cell></cell></row><row><cell></cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell></row><row><cell>Xu-CNN [21]</cell><cell>59.43</cell><cell>66.67</cell><cell>73.68</cell><cell>80.12</cell><cell>83.54</cell><cell>58.93</cell><cell>66.75</cell><cell>73.14</cell><cell>78.69</cell><cell>81.82</cell><cell>55.64</cell><cell>61.73</cell><cell>66.75</cell><cell>71.61</cell><cell>74.75</cell></row><row><cell cols="2">TLU-CNN [24] 59.71</cell><cell>66.49</cell><cell>74.38</cell><cell>77.36</cell><cell>82.36</cell><cell>56.45</cell><cell>65.35</cell><cell>72.02</cell><cell>76.92</cell><cell>78.86</cell><cell>54.95</cell><cell>60.24</cell><cell>68.71</cell><cell>74.38</cell><cell>77.25</cell></row><row><cell>Subnet #1</cell><cell>64.62</cell><cell>69.58</cell><cell>76.13</cell><cell>84.62</cell><cell>85.87</cell><cell>60.54</cell><cell>68.52</cell><cell>74.57</cell><cell>80.44</cell><cell>83.29</cell><cell>56.71</cell><cell>63.43</cell><cell>68.49</cell><cell>74.36</cell><cell>77.25</cell></row><row><cell></cell><cell>+5.19</cell><cell>+2.91</cell><cell>+2.45</cell><cell>+4.50</cell><cell>+2.32</cell><cell>+1.61</cell><cell>+1.77</cell><cell>+1.43</cell><cell>+1.75</cell><cell>+1.47</cell><cell>+1.07</cell><cell>+1.70</cell><cell>+1.74</cell><cell>+2.75</cell><cell>+2.50</cell></row><row><cell></cell><cell cols="4">+4.91 +3.09 +1.75 +7.26</cell><cell cols="11">+3.51 +4.09 +3.17 +2.55 +3.52 +4.43 +1.76 +3.19 -0.22 -0.02 +0.00</cell></row><row><cell>Subnet #2</cell><cell>64.15</cell><cell>68.73</cell><cell>76.44</cell><cell>84.28</cell><cell>86.17</cell><cell>60.88</cell><cell>69.13</cell><cell>75.16</cell><cell>80.25</cell><cell>83.47</cell><cell>56.93</cell><cell>64.15</cell><cell>69.66</cell><cell>74.89</cell><cell>78.37</cell></row><row><cell></cell><cell>+4.72</cell><cell>+2.06</cell><cell>+2.76</cell><cell>+4.16</cell><cell>+2.63</cell><cell>+1.95</cell><cell>+2.38</cell><cell>+2.02</cell><cell>+1.56</cell><cell>+1.65</cell><cell>+1.29</cell><cell>+2.42</cell><cell>+2.91</cell><cell>+3.28</cell><cell>+3.62</cell></row><row><cell></cell><cell cols="4">+4.44 +2.24 +2.06 +6.92</cell><cell cols="9">+3.81 +4.43 +3.78 +3.14 +3.33 +4.61 +1.98 +3.91 +0.95</cell><cell>+0.51</cell><cell>+1.12</cell></row><row><cell>Subnet #3</cell><cell>60.24</cell><cell>66.37</cell><cell>74.75</cell><cell>78.54</cell><cell>83.02</cell><cell>61.23</cell><cell>68.56</cell><cell>74.67</cell><cell>79.59</cell><cell>82.87</cell><cell>56.49</cell><cell>62.94</cell><cell>68.11</cell><cell>73.06</cell><cell>76.52</cell></row><row><cell></cell><cell cols="6">+0.81 -0.30 +1.07 -1.58 -0.52 +2.30</cell><cell>+1.81</cell><cell>+1.53</cell><cell>+0.90</cell><cell>+1.05</cell><cell>+0.85</cell><cell>+1.21</cell><cell>+1.36</cell><cell>+1.45</cell><cell>+1.77</cell></row><row><cell></cell><cell cols="4">+0.53 -0.12 +0.37 +1.18</cell><cell cols="11">+0.66 +4.78 +3.21 +2.65 +2.67 +4.01 +1.54 +2.70 -0.60 -1.32 -0.73</cell></row><row><cell>ReST-Net</cell><cell>65.67</cell><cell>71.35</cell><cell>78.78</cell><cell>85.44</cell><cell>87.93</cell><cell>62.38</cell><cell>70.64</cell><cell>76.74</cell><cell>81.66</cell><cell>84.54</cell><cell>58.92</cell><cell>65.14</cell><cell>70.28</cell><cell>76.17</cell><cell>79.16</cell></row><row><cell></cell><cell>+6.24</cell><cell>+4.68</cell><cell>+5.10</cell><cell>+5.32</cell><cell>+4.39</cell><cell>+3.45</cell><cell>+3.89</cell><cell>+3.60</cell><cell>+2.97</cell><cell>+2.72</cell><cell>+3.28</cell><cell>+3.41</cell><cell>+3.53</cell><cell>+4.56</cell><cell>+4.41</cell></row><row><cell></cell><cell cols="4">+5.96 +4.86 +4.40 +8.08</cell><cell cols="9">+5.57 +5.93 +5.29 +4.72 +4.74 +5.68 +3.97 +4.90 +1.57</cell><cell>+1.79</cell><cell>+1.91</cell></row></table><note><p>Differences compared with Xu-CNN and TLU-CNN are shown in bold and italic fonts, respectively.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II DETECTION</head><label>II</label><figDesc>ACCURACIES FOR S-UNIWARD, HILL, AND CMD-HILL(%) BY REPLACING THE DAM IN OUR PROPOSED APPROACH WITH RELU, SIGMOID,</figDesc><table><row><cell></cell><cell></cell><cell cols="3">AND TANH, RESPECTIVELY</cell><cell></cell><cell></cell></row><row><cell>CNN scheme</cell><cell cols="2">S-UNIWARD</cell><cell cols="2">HILL</cell><cell cols="2">CMD-HILL</cell></row><row><cell></cell><cell>0.1</cell><cell>0.4</cell><cell>0.1</cell><cell>0.4</cell><cell>0.1</cell><cell>0.4</cell></row><row><cell>Subnet #1</cell><cell>59.81</cell><cell>82.84</cell><cell>58.76</cell><cell>79.25</cell><cell>55.76</cell><cell>73.18</cell></row><row><cell>with ReLU</cell><cell>-4.81</cell><cell>-1.78</cell><cell>-1.78</cell><cell>-1.19</cell><cell>-0.95</cell><cell>-1.18</cell></row><row><cell>Subnet #1</cell><cell>63.8</cell><cell>83.65</cell><cell>59.06</cell><cell>77.07</cell><cell>55.12</cell><cell>71.18</cell></row><row><cell>with Sigmoid</cell><cell>-0.82</cell><cell>-0.97</cell><cell>-1.48</cell><cell>-3.37</cell><cell>-1.59</cell><cell>-3.18</cell></row><row><cell>Subnet #1</cell><cell>62.33</cell><cell>83.39</cell><cell>59.31</cell><cell>78.2</cell><cell>56.22</cell><cell>71.15</cell></row><row><cell>with TanH</cell><cell>-2.29</cell><cell>-1.23</cell><cell>-1.23</cell><cell>-2.24</cell><cell>-0.49</cell><cell>-3.21</cell></row><row><cell>Subnet #2</cell><cell>62.37</cell><cell>83.62</cell><cell>59.43</cell><cell>78.86</cell><cell>55.41</cell><cell>72.74</cell></row><row><cell>with ReLU</cell><cell>-1.78</cell><cell>-0.62</cell><cell>-1.45</cell><cell>-1.39</cell><cell>-1.52</cell><cell>-2.15</cell></row><row><cell>Subnet #2</cell><cell>63.44</cell><cell>82.86</cell><cell>59.12</cell><cell>77.84</cell><cell>56.04</cell><cell>71.42</cell></row><row><cell>with Sigmoid</cell><cell>-0.71</cell><cell>-1.42</cell><cell>-1.76</cell><cell>-2.38</cell><cell>-0.89</cell><cell>-3.47</cell></row><row><cell>Subnet #2</cell><cell>63.72</cell><cell>83.81</cell><cell>59.92</cell><cell>78.71</cell><cell>56.43</cell><cell>72.75</cell></row><row><cell>with TanH</cell><cell>-0.43</cell><cell>-0.49</cell><cell>-0.96</cell><cell>-1.52</cell><cell>-0.50</cell><cell>-2.14</cell></row><row><cell>Subnet #3</cell><cell>57.45</cell><cell>77.29</cell><cell>58.54</cell><cell>78.23</cell><cell>56.17</cell><cell>72.42</cell></row><row><cell>with ReLU</cell><cell>-2.79</cell><cell>-1.25</cell><cell>-2.69</cell><cell>-1.36</cell><cell>-0.32</cell><cell>-0.64</cell></row><row><cell>Subnet #3</cell><cell>59.02</cell><cell>76.98</cell><cell>58.32</cell><cell>76.46</cell><cell>55.25</cell><cell>69.77</cell></row><row><cell>with Sigmoid</cell><cell>-1.22</cell><cell>-1.56</cell><cell>-2.91</cell><cell>-3.13</cell><cell>-1.24</cell><cell>-3.29</cell></row><row><cell>Subnet #3</cell><cell>59.02</cell><cell>76.98</cell><cell>58.32</cell><cell>76.46</cell><cell>55.25</cell><cell>69.77</cell></row><row><cell>with TanH</cell><cell>-1.22</cell><cell>-1.56</cell><cell>-2.91</cell><cell>-3.13</cell><cell>-1.24</cell><cell>-3.29</cell></row><row><cell>Ensemble</cell><cell>62.93</cell><cell>84.37</cell><cell>60.35</cell><cell>79.86</cell><cell>57.32</cell><cell>73.95</cell></row><row><cell>with ReLU</cell><cell>-2.74</cell><cell>-1.07</cell><cell>-2.03</cell><cell>-1.80</cell><cell>-1.60</cell><cell>-2.22</cell></row><row><cell>Ensemble</cell><cell>65.13</cell><cell>84.59</cell><cell>60.11</cell><cell>78.87</cell><cell>57.16</cell><cell>75.08</cell></row><row><cell>with Sigmoid</cell><cell>-0.54</cell><cell>-0.85</cell><cell>-2.27</cell><cell>-2.79</cell><cell>-1.76</cell><cell>-1.09</cell></row><row><cell>Ensemble</cell><cell>64.53</cell><cell>84.68</cell><cell>60.6</cell><cell>79.63</cell><cell>58.02</cell><cell>74.83</cell></row><row><cell>with TanH</cell><cell>-1.14</cell><cell>-0.76</cell><cell>-1.78</cell><cell>-2.03</cell><cell>-0.90</cell><cell>-1.34</cell></row></table><note><p>Differences compared with the scheme using DAM are shown in bold.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III CORRELATION</head><label>III</label><figDesc>COEFFICIENTS BETWEEN THE MEAN GRADIENT VALUES OF DIFFERENT ACTIVATION UNITS</figDesc><table><row><cell></cell><cell cols="3">Subnet #2 with DAM</cell><cell cols="3">Subnet #2 with ReLU</cell><cell cols="3">Subnet #2 with Sigmoid</cell><cell cols="3">Subnet #2 with TanH</cell></row><row><cell>Activation #1</cell><cell>ReLU</cell><cell>ReLU</cell><cell>TanH</cell><cell>ReLU1</cell><cell>ReLU1</cell><cell>ReLU2</cell><cell>Sigmoid1</cell><cell>Sigmoid1</cell><cell>Sigmoid2</cell><cell>TanH1</cell><cell>TanH1</cell><cell>TanH2</cell></row><row><cell>Activation #2</cell><cell>TanH</cell><cell>Sigmoid</cell><cell>Sigmoid</cell><cell>ReLU2</cell><cell>ReLU3</cell><cell>ReLU3</cell><cell>Sigmoid2</cell><cell>Sigmoid3</cell><cell>Sigmoid3</cell><cell>TanH2</cell><cell>TanH3</cell><cell>TanH3</cell></row><row><cell>Second conv</cell><cell>0.7890</cell><cell>0.8202</cell><cell>0.9190</cell><cell>0.9759</cell><cell>0.9874</cell><cell>0.9675</cell><cell>0.9613</cell><cell>0.9520</cell><cell>0.9854</cell><cell>0.9584</cell><cell>0.9435</cell><cell>0.9762</cell></row><row><cell>Fourth conv</cell><cell>0.8575</cell><cell>0.8836</cell><cell>0.9401</cell><cell>0.9886</cell><cell>0.9938</cell><cell>0.9897</cell><cell>0.9740</cell><cell>0.9849</cell><cell>0.9816</cell><cell>0.9730</cell><cell>0.9902</cell><cell>0.9841</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Table III shows the results, where we can observe that when two different activation functions are used, the correlation is lower. It indicates different activation</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the NSFC under Grants 61572329, 61772349, and U1636202; and in part by the Shenzhen R&amp;D Program under Grant JCYJ20160328144421330. The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Antonio Paiva.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bloom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kalker</surname></persName>
		</author>
		<title level="m">Digital Watermarking and Steganography</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Using high-dimensional image models to perform highly undetectable steganography</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pevný</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Filler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Inf. Hiding Workshop</title>
		<meeting>12th Inf. Hiding Workshop</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="161" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Designing steganographic distortion using directional filters</title>
		<author>
			<persName><forename type="first">V</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Workshop Inf. Forensic Secur</title>
		<meeting>IEEE Int. Workshop Inf. Forensic Secur</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="234" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Universal distortion function for steganography in an arbitrary domain</title>
		<author>
			<persName><forename type="first">V</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Denemark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Assoc. Signal Process. J. Inf. Secur</title>
		<imprint>
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A new cost function for spatial image steganography</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="4206" to="4210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A strategy of clustering modification directions in spatial image steganography</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Secur</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1905" to="1917" />
			<date type="published" when="2015-09">Sep. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Japanese cartoon icon hello kitty&apos;s hello to big money</title>
		<author>
			<persName><forename type="first">T</forename><surname>Baker</surname></persName>
		</author>
		<ptr target="http://www.couriermail.com.au/news/kittys-hello-to-big-cash/" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>sv=4483df43372c5701b 19f74bec3d2f3a1</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Why are jihadis so obsessed with porn?</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chesler</surname></persName>
		</author>
		<ptr target="http://nypost.com/2015/02/17/why-are-jihadis-so-obsessed-with-porn" />
	</analytic>
	<monogr>
		<title level="s">Middle East Quarterly</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A survey on image steganography and steganalysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Inf. Hiding Multimedia Signal Process</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="142" to="172" />
			<date type="published" when="2011-04">Apr. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Steganalysis by subtractive pixel adjacency matrix</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pevny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Secur</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="224" />
			<date type="published" when="2010-06">Jun. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich models for steganalysis of digital images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kodovský</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Secur</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="868" to="882" />
			<date type="published" when="2012-06">Jun. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Random projections of residuals for digital image steganalysis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Secur</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="1996">1996-2006, Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Textural features for steganalysis</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sutthiwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Workshop Inf. Hiding</title>
		<meeting>Int. Workshop Inf. Hiding</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="63" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">New steganalytic features for spatial image steganography based on derivative filters and threshold LBP operator</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Secur</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1242" to="1257" />
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998-11">Nov. 1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Neural Inf</title>
		<meeting>Int. Conf. Neural Inf</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Int. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stacked convolutional auto-encoders for steganalysis of digital images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. Asia-Pacific Signal Inf. Process. Assoc. Annu. Summit Conf.</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning for steganalysis via convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IS&amp;T/SPIE Electron</title>
		<meeting>IS&amp;T/SPIE Electron</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="94" to="090J" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Structural design of convolutional neural networks for steganalysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Q</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="708" to="712" />
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ensemble of CNNs for steganalysis: An empirical study</title>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Q</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th ACM Inf. Hiding Multimedia Secur</title>
		<meeting>4th ACM Inf. Hiding Multimedia Secur</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="103" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Histogram layer, moving convolutional neural networks towards feature-based steganalysis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sedighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Media Watermarking Secur</title>
		<meeting>Media Watermarking Secur</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="50" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning hierarchical representations for image steganalysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Secur</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2545" to="2557" />
			<date type="published" when="2017-11">Nov. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Break our steganographic system-the ins and outs of organizing BOSS</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Filler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pevný</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th Inf. Hiding Workshop</title>
		<meeting>13th Inf. Hiding Workshop</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="59" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 32nd Int. Conf</title>
		<meeting>32nd Int. Conf</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
		<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Steganalysis of adaptive JPEG steganography using 2D Gabor filters</title>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd ACM Inf. Hiding Multimedia Secur</title>
		<meeting>3rd ACM Inf. Hiding Multimedia Secur</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="15" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
