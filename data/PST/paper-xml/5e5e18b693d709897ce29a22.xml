<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NEURAL TEXT DEGENERATION WITH UNLIKELIHOOD TRAINING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ilia</forename><surname>Kulikov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">CIFAR Azrieli Global Scholar</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">NEURAL TEXT DEGENERATION WITH UNLIKELIHOOD TRAINING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive outputs <ref type="bibr" target="#b10">(Holtzman et al., 2019)</ref>. While some post-hoc fixes have been proposed, in particular top-k and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model are poor. In this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences containing repeats and frequent words, unlike those from the human training distribution. We propose a new objective, unlikelihood training, which forces unlikely generations to be assigned lower probability by the model. We show that both token and sequence level unlikelihood training give less repetitive, less dull text while maintaining perplexity, giving superior generations using standard greedy or beam search. According to human evaluations, our approach with standard beam search also outperforms the currently popular decoding methods of nucleus sampling or beam blocking, thus providing a strong alternative to existing techniques. * Equal contribution; the ordering was decided by a coin flip.</p><p>(ii) it is not focused on optimizing sequence generation, only on producing the next token. The first issue means that greedy or beam search decoding, which rely on the top of the list to generate, are not optimized -there is a discrepancy between maximizing the log-probability of a ground-truth token and ensuring the rank of the ground-truth token to be one. The second issue means that during sequence generation, any imperfection in next token prediction leads to error accumulation that is not addressed by likelihood training.</p><p>In this work, we introduce unlikelihood training, an approach that addresses the two aforementioned issues. It combines two types of updates: a likelihood update on the true target tokens so that they are assigned high probability, and an unlikelihood update on tokens that are otherwise assigned too high a probability. We can collect these unlikely token candidates either during next-token prediction or from generated sequences, allowing us to train at both the token and sequence levels. Both token and sequence level unlikelihood training are shown to improve metrics that measure dullness and repetition of the model, while maintaining performance in other metrics such as perplexity or token accuracy compared to the maximum likelihood baseline. Finally, we assess our models using human evaluations. We find that our generations have vastly improved quality compared to likelihood trained models when both models use beam search decoding. Moreover, our approach when using beam search also significantly improves over likelihood trained models using either beam blocking or nucleus sampling, thus outperforming the current state-of-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Neural text generation is a vital tool in a wide range of natural language applications. However, the standard approach -training a sequence to sequence model, e.g. Transformer <ref type="bibr" target="#b24">(Vaswani et al., 2017)</ref>, to maximize log-likelihood and approximately decoding the most likely sequence -is known to be flawed. Generated text in open-ended applications such as language modeling or dialogue has been observed to be dull, with high frequency tokens used too often and interesting content words used too rarely <ref type="bibr" target="#b10">(Holtzman et al., 2019;</ref><ref type="bibr" target="#b5">Dinan et al., 2019)</ref>. Moreover, the models repeat themselves at the token, phrase, and sentence levels, and statistics comparing a set of human-generated utterances and model-generated responses indicate a discrepancy between the human and model word distributions. This does not appear to be rectified by training on more data <ref type="bibr" target="#b19">(Radford et al., 2019)</ref>. Recent fixes involve modifying the decoding strategy using sampling or more sophisticated beam search variants. However, these decoding strategies do not address the core issue: the model's underlying sequence probabilities are clearly not correct.</p><p>Several reasons for exactly why neural text is degenerate have been posited, with the cause currently unknown. Possible candidates include the problem being (i) a by-product of the model architecture, e.g. the Transformer architecture preferring repeats <ref type="bibr" target="#b10">(Holtzman et al., 2019;</ref><ref type="bibr" target="#b25">Vig, 2018)</ref>, (ii) an intrinsic property of human language <ref type="bibr" target="#b10">(Holtzman et al., 2019)</ref> rather than a modeling deficiency, or that (iii) a training objective relying on fixed corpora cannot take into account the real goal of using the language <ref type="bibr" target="#b1">(Choi, 2018)</ref>. Our work shows that, while the above may be factors, a primary factor is the use of the likelihood objective itself, as we demonstrate that degeneration is alleviated if we replace the likelihood objective with our proposal. While low perplexity in the limit should lead to predicting the correct next target word, there are two major flaws of the likelihood objective: (i) it pays relatively little attention to the argmax or the top of the ranked list of next token probabilities, instead optimizing the likelihood of the entire distribution;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Neural Text Degeneration Recently, several papers have observed various forms of neural text degeneration, especially in open-ended generation tasks. In dialogue, it has been shown that there is a mismatch between model and human word distributions, where generative models are more likely to output frequent words, but less likely to produce rare words compared to humans. For example, this was observed across all generative models submitted to the ConvAI2 NeurIPS 2018 competition <ref type="bibr" target="#b5">(Dinan et al., 2019)</ref>. In language modeling, the work of <ref type="bibr" target="#b10">Holtzman et al. (2019)</ref> highlighted problems with the word frequency distribution and level of repetition in model generations compared to human text. These issues are not remedied by simply increasing the amount of the training data; e.g. largescale GPT-2 language models <ref type="bibr" target="#b19">(Radford et al., 2019)</ref> display the same issues.</p><p>Improved Decoding Algorithms Several methods have been proposed to rectify these issues. The primary ones involve changing the decoding method to a sophisticated beam search variant or to stochastic decoding, e.g. sampling. Different variants of beam search have been explored <ref type="bibr" target="#b14">(Li et al., 2016;</ref><ref type="bibr" target="#b26">Vijayakumar et al., 2018;</ref><ref type="bibr" target="#b12">Kulikov et al., 2018;</ref><ref type="bibr" target="#b9">Holtzman et al., 2018)</ref> which can decrease a model's level of repetition by selecting candidates that are unlike previously chosen ones. Separately, hard or soft beam blocking has been investigated <ref type="bibr" target="#b18">(Paulus et al., 2017;</ref><ref type="bibr" target="#b11">Klein et al., 2017)</ref>, whereby previously generated n-grams are blocked from subsequent generation. This approach is often used in dialogue generation, fixing some token or phrase level repetitions but removing repetitions that would naturally occur in human text.</p><p>The second major approach is that of sampling from the model at generation time. Top k-sampling <ref type="bibr" target="#b7">(Fan et al., 2018)</ref> and nucleus sampling <ref type="bibr" target="#b10">(Holtzman et al., 2019)</ref> are two methods that sample sequences based on a function of the predicted next token probability distribution given by the model. Both approaches vastly improve the repetition issue, as the randomization often reduces the number of duplicate tokens in a decoded sequence, even if highly scored paths under the model (represented by beam search candidates) contain repetitions. However, as the underlying model is unchanged, it often prefers semantically similar phrasing, depending on the temperature parameter of the sampling <ref type="bibr" target="#b10">(Holtzman et al., 2019)</ref>. Furthermore, this solution is less relevant in less open-ended tasks such as machine translation, where beam search variants are the preferred method. Ideally we would like a model that can work with both beam and sampling decoding methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Improved Learning Algorithms</head><p>The proposed learning criteria are closely related to structured output prediction methods in which the goal is to increase the scores assigned by a model to true examples while decreasing those assigned to negative examples often generated by the model itself. Some representative algorithms include structured perceptron <ref type="bibr" target="#b2">(Collins, 2002)</ref>, energy-based models <ref type="bibr" target="#b13">(LeCun et al., 2006)</ref> and more recently reflective likelihood <ref type="bibr" target="#b4">(Dieng et al., 2018)</ref>. A particular variant in this family of algorithms, called negative training, was recently used by <ref type="bibr" target="#b8">He and Glass (2019)</ref> to prevent generic and malicious responses in dialogue models. Similarly, these structured prediction algorithms with neural language models have been applied to machine translation in recent years by <ref type="bibr" target="#b23">Shen et al. (2015)</ref> and <ref type="bibr" target="#b6">Edunov et al. (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NEURAL TEXT GENERATION</head><p>Language Modeling In language modeling, our goal is to model a probability distribution p * (x) over variable-length text sequences x = (x 1 , . . . , x |x| ) composed of tokens from a vocabulary, x t ∈ V. We wish to find a model p θ (x) which resembles p * (x), meaning that samples x ∼ p θ are similar to samples from p * , and p θ (x) ≈ p * (x) for all x. When p θ (x) is parameterized by a neural network, we call p θ a neural language model. We assume that p θ takes the form p θ (x) = |x| t=1 p θ (x t |x &lt;t ). The de facto approach to training such a model is to find parameters θ that maximize the loglikelihood of a finite set of samples D from p * by minimizing:</p><formula xml:id="formula_0">LMLE(p θ , D) = − |D| i=1 |x (i) | t=1 log p θ (x (i) t |x (i) &lt;t ).</formula><p>(1)</p><p>Sequence Completion A closely related problem consists of sampling a sub-sequence, or prefix, x 1:k ∼ p * , then using p θ to conditionally decode a continuation, xk+1:N ∼ p θ (•|x 1:k ). We now want the resulting completion (x 1 , . . . , x k , xk+1 , . . . , xN ) to resemble a sample from p * .</p><p>We use sequence completion as a setting to study the behavior of neural language models due to its generality. For instance, sequence completion encompasses story generation <ref type="bibr" target="#b7">(Fan et al., 2018)</ref>, contextual text completion <ref type="bibr" target="#b19">(Radford et al., 2019)</ref>, language modeling (for k = 0), and dialogue modeling <ref type="bibr" target="#b29">(Zhang et al., 2018)</ref> where x 1:k is a dialogue history and a continuation is a next utterance.</p><p>Given p θ and a prefix x 1:k , finding the optimal continuation is not tractable, so in practice approximate deterministic or stochastic decoding strategies are used to generate continuations.</p><p>Deterministic Decoding Two widely used deterministic decoding approaches are greedy search and beam search. The former can be seen as a special case of the latter. Greedy search selects the highest probability token at each time step: x t = arg max p θ (x t |x &lt;t ). Beam search maintains a fixed-size set of partially-decoded sequences, called hypotheses. At each time step, beam search forms new hypotheses by appending each token in the vocabulary to each existing hypothesis, scoring the resulting sequences then selecting the highest scoring sequences. As we describe in Section 4, these deterministic decoding strategies, which depend highly on underlying model probabilities, expose issues with conventionally trained neural language models.</p><p>Stochastic Decoding An alternative is to sample from a model-dependent distribution at each step, x t ∼ q(x t |x &lt;t , p θ ). In order to prevent sampling low probability tokens, a typical approach is to restrict sampling to a subset of the vocabulary U ⊂ V at each step:</p><formula xml:id="formula_1">q(x t |x &lt;t , p θ ) = p θ (x t |x &lt;t )/Z x t ∈ U 0 otherwise,</formula><p>where Z = x∈U p θ (x|x &lt;t ). The top-k sampler restricts sampling to the k most-probable tokens; i.e. U is the size k subset of V which maximizes x∈U p θ (x|x &lt;t ) <ref type="bibr" target="#b7">(Fan et al., 2018)</ref>. The nucleus sampler instead restricts sampling to the smallest set of tokens with total mass above a threshold p ∈ [0, 1]; i.e. U is the smallest subset with x∈U p θ (x|x &lt;t ) &gt;= p <ref type="bibr" target="#b10">(Holtzman et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">NEURAL TEXT DEGENERATION</head><p>In this section we discuss two degenerate properties that frequently occur in conventional neural language models trained with the maximum likelihood objective (Equation <ref type="formula">1</ref>).</p><p>Repetition First, model-generated continuations exhibit sequence-level repetition, especially with deterministic decoding. The problem is seen by observing samples in Appendix Table <ref type="table">4</ref>, which</p><p>shows completions from the state-of-the-art GPT-2 language model <ref type="bibr" target="#b19">(Radford et al., 2019)</ref>. Greedy decoding as well as top-k and nucleus sampling exhibit degenerate repetition (with a certain hyperparameter setting), although greedy decoding shows the worst degradation. Using a Transformer language model trained with maximum likelihood ( §6), we find that the average percentage of repeated n-grams in model continuations with greedy decoding (43%) far exceeds that of humans (0.5%), computed over prefixes drawn from a validation corpus.</p><p>Unlike previous work which only focused on degenerate sequence-level repeats <ref type="bibr" target="#b10">(Holtzman et al., 2019)</ref>, we additionally observe that neural language models exhibit substantially more repetition in next-token prediction compared to human text:</p><formula xml:id="formula_2">Pr (x k+1 = arg max p θ (x|x 1:k ) ∈ x 1:k ) &gt; Pr (x k+1 ∈ x 1:k ) .<label>(2)</label></formula><p>For instance, the Transformer language model ( §6) predicted next-tokens that appeared in the preceding 128 words 62% of the time, versus 49% in ground-truth text. This is especially concerning since the maximum-likelihood objective focuses on optimizing next-token conditional distributions.</p><p>Token Distribution Mismatch Second, both greedy continuations and next-token predictions from conventional neural text generators have different token distributions from human text. As demonstrated by <ref type="bibr" target="#b10">Holtzman et al. (2019)</ref>, such models with greedy or beam search tend to produce high frequency tokens too often and low frequency tokens too rarely, where frequency is defined by the human token distribution. With the Transformer language model ( §6), the set of nexttoken greedy predictions on a held-out validation set had roughly 40% fewer unique tokens than the ground-truth tokens (11.6k vs. 18.9k), and overproduced frequent tokens (Appendix Figure <ref type="figure" target="#fig_0">1</ref>). Such behavior has been linked to generations being judged as dull by humans because rare words can add engaging specificity <ref type="bibr" target="#b27">(Weston et al., 2018;</ref><ref type="bibr" target="#b22">See et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">THE UNLIKELIHOOD TRAINING OBJECTIVE</head><p>We now describe unlikelihood training for neural language models, then in Section 6 demonstrate empirically that our proposal substantially improves neural text degeneration ( §4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">UNLIKELIHOOD TRAINING</head><p>The key idea behind unlikelihood training is decreasing the model's probability of certain tokens, called negative candidates. Given a sequence (x 1 , . . . , x T ) and a set of negative candidate tokens C t = {c 1 , . . . , c m }, where each c j ∈ V, we define the unlikelihood loss for step t as:</p><formula xml:id="formula_3">L t UL (p θ (•|x &lt;t ), C t ) = − c∈C t log(1 − p θ (c|x &lt;t )).<label>(3)</label></formula><p>The loss decreases as p θ (c|x &lt;t ) decreases. We incorporate the unlikelihood loss into a token-level unlikelihood objective which augments each time-step of maximum likelihood training:</p><formula xml:id="formula_4">L t UL-token (p θ (•|x &lt;t ), C t ) = −α • c∈C t log(1 − p θ (c|x &lt;t )) unlikelihood − log p θ (x t |x &lt;t ) likelihood .<label>(4)</label></formula><p>As candidates, we use previous context tokens:</p><formula xml:id="formula_5">C t prev-context = {x 1 , . . . , x t−1 } \ {x t }.<label>(5)</label></formula><p>Intuitively, minimizing the unlikelihood loss with this candidate set makes (i) incorrect repeating tokens less likely, as the previous context contains potential repeats, and (ii) frequent tokens less likely, as these tokens appear often in the previous context. These candidates are efficient to compute, without requiring additional supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gradient analysis</head><p>We assume p θ (x t |x &lt;t ) = softmax(a) and consider the gradient of ( <ref type="formula" target="#formula_4">4</ref>) with respect to the softmax input a ∈ R V . With a single negative candidate, the (negative) gradient is:</p><formula xml:id="formula_6">∇L a = x * − m p, m i = (1 − α pneg 1−pneg ) if i = i neg (1 + α) if i = i neg ,<label>(6)</label></formula><p>where x * ∈ {0, 1} V is a one-hot ground-truth vector, m ∈ R V , p = p θ (•|x &lt;t ), and p neg is the probability of the negative candidate at index i neg (derivation in Appendix A).</p><p>This unlikelihood gradient (6) differs from the likelihood gradient, (x * −p), due to the term m which varies based on the hyper-parameter α and the model's negative candidate probability, p neg . At the ground-truth token index i * , the unlikelihood gradient is positive, increasing the ground-truth token's probability with a magnitude that grows with p neg . Conversely, at the negative candidate index i neg the gradient is negative. At all other token indices i ∈ {i * , i neg }, the gradient moves from negative to positive as p neg increases. For instance, with α = 1.0 the gradient increases the probability of each token x i when the model assigns high probability to the negative candidate (p neg &gt; 0.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">SEQUENCE-LEVEL UNLIKELIHOOD TRAINING</head><p>While the token-level unlikelihood objective efficiently augments maximum likelihood training with token-level penalties, it is limited to prefixes drawn from the training distribution. The resulting distribution mismatch between training sequences and generated sequences is a known issue with maximum-likelihood training, motivating objectives that operate on model-generated sequences <ref type="bibr" target="#b3">(Daumé et al., 2009;</ref><ref type="bibr" target="#b21">Ross et al., 2011;</ref><ref type="bibr" target="#b20">Ranzato et al., 2015;</ref><ref type="bibr" target="#b28">Yu et al., 2016)</ref>.</p><p>We thus propose a sequence-level unlikelihood objective which uses unlikelihood on decoded continuations. That is, given a prefix (x 1 , . . . , x k ) ∼ p * , we decode a continuation (x k+1 , . . . , x k+N ) ∼ p θ (•|x 1 , . . . , x k ), construct per-step negative candidate sets (C k+1 , . . . , C k+N ), and define each perstep sequence-level loss for t ∈ {k + 1, . . . , k + N } as:</p><formula xml:id="formula_7">L t ULS (p θ (•|x &lt;t ), C t ) = − c∈C t log(1 − p θ (c|x &lt;t )).<label>(7)</label></formula><p>Intuitively, the negative candidates can identify problematic tokens for the loss to penalize. We choose to penalize repeating n-grams in the continuation:</p><formula xml:id="formula_8">C t repeat-n = {x t } if (x t−i , . . . , x t , . . . , x t+j ) ∈ x &lt;t−i for any (j − i) = n, i ≤ n ≤ j,<label>(8)</label></formula><p>which says that x t is the (single) negative candidate for step t if it is part of a repeating n-gram<ref type="foot" target="#foot_0">1</ref> .</p><p>In our experiments we apply this sequence loss in two ways: (i) using it to fine-tune a standard MLE baseline; and (ii) using it to fine-tune an unlikelihood model trained at the token level, L UL-token . We refer to the former as L UL-seq and the latter as L UL-token+seq . In both cases, fine-tuning is done by equally mixing sequence-level unlikelihood updates (7) and the token-level loss from which it was initially trained (either likelihood updates (1) or token-level unlikelihood updates (4)).</p><p>Efficiency Any objective that requires explicitly decoding a sequence is constrained by sample efficiency when decoding is slow; if sample efficiency is low, the total decoding time is too large for practical use. In our experiments we show that when used for fine-tuning, the sequence-level unlikelihood objective substantially reduced degeneration in under 1,500 updates, rendering it practical for modern large-scale neural models, even with high decoding costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>We follow a standard language modeling setup from Baevski and Auli ( <ref type="formula">2019</ref>) and evaluate our method on the task of sequence completion, detailed below.<ref type="foot" target="#foot_1">2</ref> </p><p>Model Architecture Recent large-scale language models are based on the Transformer architecture, a multi-layer feed-forward network with self-attention <ref type="bibr" target="#b24">(Vaswani et al., 2017)</ref>. We use a 16-layer Transformer with 8 attention heads, embedding dimension 1024, and fully-connected dimension 4096; the architecture is based on Baevski and Auli (2019) but with standard embedding and softmax layers. Our proposed method is architecture agnostic; we choose this one as a representative of recent large-scale language models, e.g. <ref type="bibr" target="#b19">Radford et al. (2019)</ref>. LUL-token+seq , and German . In the first verse , the protagonist sings about being a " girl who 's been in love with someone else " , while the second verse describes the relationship between the protagonist and her lover . In the third verse , the protagonist sings 0.063 Prefix ... starboard engines and was going to crash . " We 're going in ," LMLE he said . " We 're going to crash . We 're going to crash . We 're going to crash . We 're going to crash . We 're going to crash . We 're going to crash . We 're going to crash . We 're going to 0.787 LUL-token+seq Hood said . " I 'm going to make sure we 're going to get back to the water . " The order to abandon ship was given by Admiral Beatty , who ordered the remaining two battlecruisers to turn away . At 18 : 25 , Hood turned his 0.000 Prefix ... career -high 27 points on 8 -for -11 shooting with three rebounds LMLE and two assists . On January 3 , 2012 , he was named to the 2012 13 All -Atlantic 10 first team . On February 3 , 2012 , he was named to the Atlantic 10 first team . On February 5 , 2012 , he was named 0.277</p><p>LUL-token+seq and a career -high 7 assists against the Minnesota Timberwolves . On February 3 , 2012 , he was named to the 2012 All -NBA First Team . On March 7 , 2012 , he was named one of five finalists for the Naismith Award , which is 0.064  <ref type="bibr" target="#b0">(Baevski and Auli, 2019;</ref><ref type="bibr" target="#b19">Radford et al., 2019)</ref>. We perform experiments at the word level.</p><p>Training We train on fixed-length contiguous sequences, in our case of length 1,536, which was selected based on GPU memory constraints. For the token-level losses (L MLE , L UL-token ), we train each model on 8 GPUs for a maximum of 150k updates, evaluating on the validation set and saving the model state every 10k updates. For the experiments below, we select the saved model state with the best validation perplexity.</p><p>Sequence-level fine-tuning begins with the model state selected based on the validation perplexity. Models are fine-tuned for 1,500 total updates. With probability 0.5 an update uses L ULS and otherwise uses the token-level loss with which the model was trained. For a L ULS update, we split each training sequence and greedily decode continuations (details below). The experiments use a prefix length k = 50 and continuation length N = 100 for fine-tuning.</p><p>Completions We evaluate a model on sequence completion by using the model to decode continuations of prefixes derived from the validation (or test) set. Specifically, the validation (or test) set is first partitioned into sequences of 1,536 tokens, as in training. Then we split each sequence into a batch of prefixes of length k (discarding extra tokens), and decode a continuation of length N for each prefix. The experiments below use k = 50 and N = 100 for evaluation. For deterministic decoding we use greedy search and beam search with beam size 10, and for stochastic decoding we use top-k sampling with k ∈ {3, 50} and nucleus sampling with p ∈ {0.3, 0.9}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">EVALUATION METRICS</head><p>Repetition As a token-level metric for repetition, we use the fraction of next-token (top-1) predictions that occur in the previous tokens (rep/ ); given a set D of length-T sequences,</p><formula xml:id="formula_9">rep/ = 1 |D|T x∈D T t=1 I [arg max p θ (x|x&lt;t) ∈ x t− −1:t−1 ] .<label>(9)</label></formula><p>A predicted token is called a "single-token repeat" when I [•] is 1. Some of these single-token repeats also occur in the human-generated sequences, and we thus report a variant which only counts singletoken repeats that are additionally not equal to the ground-truth next-token (wrep/ ).  </p><p>and average over continuations. seq-rep-n is zero when the continuation has no repeating n-grams, and increases towards 1.0 as the model repeats. We compute seq-rep-n on the continuation.</p><p>Token Distribution We quantify a model's predicted token distribution using the number of unique tokens. As a token-level metric (uniq), we use the number of unique next-token predictions on a validation or test set D, i.e. |{arg max p(x t |x &lt;t ) | x &lt;t ∈ D}|. As a sequence-level metric (uniq-seq) we use the number of unique tokens in continuations of validation or test prefixes ( §6).</p><p>Language Modeling Quality We use perplexity (ppl), and next-token prediction accuracy (acc), defined as 1 N |{arg max p(x t |x &lt;t ) = x * t | x &lt;t ∈ D}|, with N prefixes x &lt;t and true next tokens x * t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">RESULTS</head><p>Token-level and sequence-level results on the test set are in Table <ref type="table" target="#tab_3">2</ref> (valid set in Appendix Table <ref type="table" target="#tab_6">5</ref>).</p><p>Baseline The baseline model trained with maximum likelihood (L MLE ) achieved 25.64 test perplexity, comparable to a current state-of-the-art system (Baevski and Auli, 2019) (24.92). However, the greedy baseline's seq-level repeats (seq-rep-4 .442) and single-token repeats (rep .627) far exceed those in human text (.006, .487 respectively). The baseline continuations have far fewer unique tokens than human text (uniq-seq 11.8k vs 19.8k), with a high rate of frequent tokens (Figure <ref type="figure" target="#fig_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Token-Level Objective</head><p>The proposed token-level unlikelihood objective (L UL-token ) reduced nexttoken wrong repetition (wrep .311 vs. .352) while increasing the number of unique next-tokens (uniq 12.7k vs. 11.8k) compared to the baseline (L MLE ). Perplexity and accuracy were similar.</p><p>Importantly, the token-level unlikelihood objective yielded substantial improvements in sequencelevel generations. With greedy search, token-level unlikelihood training improved the 4-gram repetition in continuations by 36% (seq-rep-4 .283 vs. .442) while generating roughly 22% more unique tokens than the baseline (uniq-seq 13.2k vs. 10.8k), and a more favorable rate of infrequent tokens (Figure <ref type="figure" target="#fig_0">1</ref>). With beam search, unlikelihood training showed similar improvements over the baseline.  (ppl 26.72 vs. 26.91), despite using only 1,500 updates. The token distribution improved, with infrequent tokens produced more often than the baseline, and frequent tokens approaching the human level (Figure <ref type="figure" target="#fig_0">1</ref>). Finally, after sequence-level fine-tuning, beam search out-performed greedy search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence-Level Objective</head><p>To visualize how these improvements in metrics translate to generation quality, Table <ref type="table" target="#tab_1">1</ref> shows greedy completions that characterize the baseline's degeneration and L UL-token+seq 's improved behavior.</p><p>GPT-2 Fine-Tuning In the preceding experiment, sequence-level fine-tuning alone (L UL-seq ) showed substantial improvements over the baseline using a small number of updates. This indicates that the proposed sequence-level fine-tuning can be a cheap, effective way to improve existing pre-trained language models. We demonstrate this by fine-tuning a pre-trained GPT-2 <ref type="bibr" target="#b19">(Radford et al., 2019)</ref> language model with sequence-level unlikelihood, using a comparable experimental setup to §6 (details in Appendix C). Fine-tuning with unlikelihood yielded similar improvements in sequence-level repetition (seq-rep-4 .042 vs. .506) to those observed in Table <ref type="table" target="#tab_6">5</ref>, while maintaining language modeling quality according to perplexity and accuracy (see Appendix Table <ref type="table">7</ref>).</p><p>Stochastic Decoding Although we have focused on deterministic decoding, we also confirm that a model trained with the proposed unlikelihood objectives may still be used with stochastic decoders. Appendix Table <ref type="table" target="#tab_7">6</ref> shows metrics for completions generated with top-k sampling <ref type="bibr" target="#b7">(Fan et al., 2018)</ref> and nucleus sampling <ref type="bibr" target="#b10">(Holtzman et al., 2019)</ref>. Models trained with unlikelihood objectives maintain language modeling quality compared to the baseline, but with improvements in repetition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Evaluation</head><p>We perform a crowdworker evaluation to judge the quality of the generations of our proposed models compared to each other, the baseline, two other generation methods, and the reference. We employ a pairwise setup: an evaluator is presented with a prefix and shown continuations from two different models and asked to select which continuation they found more natural. Following <ref type="bibr" target="#b15">Li et al. (2019)</ref>, we filter workers using quality controls (detailed in Appendix E) and limit the number of annotations that they may complete. Prompts are from the Wikitext-103 test set. All models used beam search (beam size 10) for generation, except for those that use stochastic decoding. We report the win rates for each pairwise comparison.</p><p>The main results are presented in Table <ref type="table" target="#tab_5">3</ref>, with additional experiments in Appendix Table <ref type="table" target="#tab_9">9</ref>. We find that all proposed models are preferred over the baseline, and that congruent with automatic metrics, win rates improve after adding the sequence level objective. Our best model also outperforms the baseline used with either nucleus sampling or beam blocking.</p><p>We also collected limited annotations from other NLP researchers. These Expert annotators were given the same UI as the crowdworkers, and not told about models they were evaluating, but all annotators were familiar with language models. As shown in Table <ref type="table" target="#tab_5">3</ref>, the L UL-token+seq model significantly outperforms both nucleus sampling and beam blocking according to the experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We described unlikelihood training, an approach to training neural language models. We observed that state-of-the-art models trained to maximize likelihood exhibit neural text degeneration, which we characterized and quantified in terms of repetition and token distribution mismatch. Our results show that the likelihood objective is not constrained enough, in the sense that two models with the same perplexity can exhibit wildly different generation performance. We empirically showed that unlikelihood training -both at the token and sequence levels -substantially reduced degeneration according to automatic metrics, and outperformed likelihood-trained models with various decoding methods according to human evaluation, being superior to the current state-of-the-art approaches.</p><p>Table <ref type="table">4</ref>: Top: Degenerate repetition in completions from a state-of-the-art large-scale language model <ref type="bibr" target="#b19">(Radford et al., 2019)</ref>. The examples contain single-word repetitions, phrase-level repetitions, and structural repetitions where some tokens within a repeating phrase vary. Recently proposed stochastic samplers (top-k, nucleus) exhibit degeneration based on hyper-parameter settings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A GRADIENT</head><p>Notation Let x * t be the true next-token (index i * ∈ V) at step t, and let x neg be a negative candidate (index i neg ). Let p = p(x t |x &lt;t ) ∈ R V be the output of softmax(a) where a ∈ R V .</p><p>Denote the probability of an element i ∈ {1, . . . , V } as p i = p(x i t |x &lt;t ), and let p * , p neg , and pi be probabilities of the true next-token, negative-candidate token, and any other token with i ∈ {i * , ī}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 DERIVATION</head><p>The (negative) token-level loss with a single candidate is,</p><formula xml:id="formula_11">L t = log p(x * t |x &lt;t ) + α • log(1 − p(x neg |x &lt;t )),<label>(11)</label></formula><p>and its gradient with respect to a logit a i is:</p><formula xml:id="formula_12">∂L ∂p i ∂p i ∂a i = (I[i = i * ] − p i ) − α p neg 1 − p neg (I[i = i neg ] − p i ) .<label>(12)</label></formula><p>We consider the gradient when i is the true next-token, a negative-candidate, and any other token.</p><formula xml:id="formula_13">True Next-Token (i = i * ) ∂L ∂p * ∂p * ∂a i * = (1 − p * ) − α p neg 1 − p neg (0 − p * ) (13) = 1 − p * (1 − α p neg 1 − p neg ). (<label>14</label></formula><formula xml:id="formula_14">) Negative Candidate (i = i neg ) ∂L ∂p neg ∂p neg ∂a neg = (0 − p neg ) − α p neg 1 − p neg (1 − p neg )<label>(15)</label></formula><formula xml:id="formula_15">= −p neg (1 + α).<label>(16)</label></formula><p>Other Token (i ∈ {i * , i neg })</p><formula xml:id="formula_16">∂L ∂ pi ∂ pi ∂a i = (0 − pi ) − α p neg 1 − p neg (0 − pi )<label>(17)</label></formula><formula xml:id="formula_17">= −p i (1 − α p neg 1 − p neg ).<label>(18)</label></formula><p>Combining the three cases above, we get:</p><formula xml:id="formula_18">∇L a = x * − m p,<label>(19)</label></formula><p>where x * ∈ {0, 1} V is 1 at index i * and 0 otherwise, and m ∈ R V is:</p><formula xml:id="formula_19">m i = (1 − α pneg 1−pneg ) i = i neg (1 + α) i = i neg . (<label>20</label></formula><formula xml:id="formula_20">)</formula><p>Multiple Candidates In general the objective considers multiple candidates (see section 5):</p><formula xml:id="formula_21">L t UL-token (p θ (•|x &lt;t ), C t ) = −α • c∈C t log(1 − p θ (c|x &lt;t )) unlikelihood − log p θ (x t |x &lt;t ) likelihood .<label>(21)</label></formula><p>We regroup the token-level objective to be a weighted sum of per-candidate objectives:</p><formula xml:id="formula_22">−L t UL-token (p θ (•|x &lt;t ), C t ) = 1 |C t | c∈C t log p θ (x t |x &lt;t ) + α c • log(1 − p θ (c|x &lt;t ))<label>(22)</label></formula><p>where</p><formula xml:id="formula_23">α c = α • |C t |.</formula><p>Now the gradient can be generalized to multiple candidates, in which case the gradient takes the same form as Eqn. 20, but with α c in place of α.  C GPT-2 FINE-TUNING</p><p>We evaluated the GPT-2 medium pre-trained model ('GPT-2') and two separate fine-tuning variants on Wikitext-103. The first variant ('GPT-2 MLE ') was fine-tuned using maximum likelihood; we select the model state with the lowest validation perplexity. The second model ('GPT-2 UL-seq ') was fine-tuned using the sequence-level unlikelihood objective ( §5.2). For both evaluation and sequencelevel tuning, we used a prefix length of 50 BPE tokens and a continuation length of 100 BPE tokens.</p><p>In order to train on a single GPU, we used a batch-size of 1024 tokens for MLE updates, and 300 prefix tokens for unlikelihood updates. Due to the smaller batch size and single-GPU setting, we used 10,000 updates during sequence-level fine-tuning, comparable to the 1,500 updates in the main experiment ( §6) in terms of the total number of tokens. Results are shown in Table <ref type="table">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D SEQUENCE-LEVEL RANDOM CANDIDATES</head><p>In Sec. 5.2 we described a way to penalize tokens that occurred in a n-gram repetition. One alternative is to penalize a random subset of the generated sequence. That is, given a continuation x t+1 , . . . , x t+K , we now define per-step candidates (C k+1 , . . . , C k+N ) as:</p><formula xml:id="formula_24">C t random-seq = {x t } if z t = 1 ∅ if z t = 0,<label>(23)</label></formula><p>for each t ∈ {k + 1, . . . , k + N }, where z t ∼ Bernoulli(p penalize ), and p penalize ∈ [0, 1] is a fixed hyper-parameter. Intuitively, these candidates identify random tokens in the generated sequence (hence 'random-seq'), which are then penalized by the sequence-level loss (Eqn. 7).</p><p>Results with different values of p penalize are shown in Table <ref type="table" target="#tab_8">8</ref>. Penalizing 10% of the generated tokens led to substantial improvements in seq-rep-4 for both greedy and beam search compared to the baseline (e.g. 41% for L UL-seq greedy, 73% for L UL-tok+seq greedy), though using n-gram repetition candidates yielded further improvements ( §5.2, Table <ref type="table" target="#tab_6">5</ref>). Improvements in single-token metrics were similar to those from the n-gram repetition candidates (e.g. wrep .287). These results with random-seq candidates demonstrate that sequence fine-tuning can yield improvements without explicitly using the notion of repetition for candidate selection. We also find that penalizing 90% of the generated tokens yields substantial improvements in beam search, but not greedy search; investigating this is left as future work.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 CROWDWORKER QUALITY CONTROLS</head><p>We require workers to correctly answer both of the following quality control questions for their evaluations to be included. Both quality controls compare the true completion against a greedy baseline model.</p><p>Following <ref type="bibr" target="#b15">Li et al. (2019)</ref>, we informed workers that they must provide reasoning for their choices. We filtered workers who did not provide reasoning for at least 80% of their choices.</p><p>63% of workers fail at least one of our three quality control mechanisms (2 quality control metrics, and failing to give reasons). 61% fail at least one quality control question; 16% of workers fail both; 4% of workers fail to give reasoning for their choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2.1 QUALITY CONTROL 1</head><p>Prompt = = In the decades since its release , The Hustler has cemented its reputation as a classic . Roger Ebert , echoing earlier praise for the performances , direction , and cinematography and adding laurels for editor Dede Allen , cites the film as " one of"'</p><p>Correct answer those films where scenes have such psychic weight that they grow in our memories . " He further cites Fast Eddie Felson as one of " only a handful of movie characters so real that the audience refers to them as touchstones . " TV Guide calls the film a " dark stunner " offering " a grim world whose only bright spot is the top of the pool table , yet [ with ] characters [ who ] maintain a shabby nobility and grace . " The four leads are again lavishly praised for their performances and the</p><p>Incorrect answer the most influential films of the year " . In his review for the Chicago Sun @-@ Times , Richard Corliss calls it " a film of the highest order " and " a film of the highest order " . In his review for the Chicago Sun @-@ Times , Richard Corliss calls it " a film of the highest order " and " a film of the highest order " . In his review for the Chicago Sun @-@ Times , Richard Corliss calls it " a film of the highest order " and " a film of E.2.2 QUALITY CONTROL 2</p><p>Prompt , which had a major negative effect on the state 's large merchant fleet . Gore was in 1808 elected to the Massachusetts House of Representatives , where he successfully led Federalist efforts to ensure the selection of a Federalist slate of presidential electors . He also spearheaded actions to</p><p>Correct answer drive Senator John Quincy Adams from the Federalist Party over his support of Thomas Jefferson 's foreign policy . The legislature elected Adams ' successor nine months early , and gave Adams sufficiently distasteful instructions that he resigned the post and joined with the Republicans . = = Governor = = Gore led the Federalists to victory in 1809 against Sullivan 's successor , Levi Lincoln , Sr. , who had taken over as acting governor upon Sullivan 's death late in 1808 . During Gore 's term the principal domestic issue occupying state politics</p><p>Incorrect Answer prevent the American Revolutionary War from being fought by the British , and to prevent the British from using the country to launch a war against the British . Gore 's actions in the House of Representatives were a major turning point in his political career . </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Sequence-level token distribution using the test subset of Wikitext-103. Nucleus sampling (p = 0.9) and beam blocking (n = 4) are used with the maximum likelihood baseline (L MLE ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Screen shot of the user interface used in the human evaluation.</figDesc><graphic url="image-1.png" coords="16,127.80,400.77,356.39,149.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Prefix ... Lyrically the song has excerpts of different languages including French , Spanish LMLE , Italian , Spanish , Italian , Spanish , Italian , Spanish , Spanish , Portuguese , Portuguese , Portuguese , Portuguese , Portuguese , Portuguese , Portuguese , Portuguese , Portuguese , Portuguese , Portuguese , Portuguese , Spanish , Portuguese , Portuguese , Portuguese , Portuguese , Portuguese</figDesc><table><row><cell>seq-rep-4</cell></row><row><cell>0.744</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Example greedy completions showing representative examples of the MLE model's degenerate single-token repetition (top), phrase-level repetition (middle), and 'structural' repetition (bottom), as well as the proposed method's ability to fix these degenerate behaviors. Dataset We use the Wikitext-103 dataset (Merity et al., 2016), a large-scale collection of Wikipedia articles containing over 100 million words and 260 thousand unique tokens. As a document-level dataset, Wikitext-103 is an open-source representative of recent datasets used for large-scale language modeling</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results for token-level objectives (upper) and sequence-level fine-tuning (lower) according to sequence-level (left) and token-level (right) metrics using the test subset of Wikitext-103.</figDesc><table><row><cell cols="3">We use the portion of duplicate n-grams (seq-rep-n) in a generated sequence to measure sequence-</cell></row><row><cell cols="2">level repetition. That is, for a continuation x k+1:k+N we compute,</cell><cell></cell></row><row><cell>seq-rep-n = 1.0 −</cell><cell>|unique n-grams(x k+1:k+N )| |n-grams|</cell><cell>,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Human eval results. * denotes statistical significance (2-sided binomial test, p &lt; .05).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Results for token-level objectives (upper) and sequence-level fine-tuning (lower) according to sequence-level (left) and token-level (right) metrics using the validation subset of wikitext-103.</figDesc><table><row><cell>Model</cell><cell cols="3">search seq-rep-4 uniq-seq</cell><cell>ppl</cell><cell>acc</cell><cell cols="2">rep wrep</cell><cell>uniq</cell></row><row><cell>L MLE</cell><cell>greedy beam</cell><cell>.429 .495</cell><cell cols="4">10.6k 24.59 .401 .619 9.4k</cell><cell>.346 11.6k</cell></row><row><cell>L UL-token</cell><cell>greedy beam</cell><cell>.274 .327</cell><cell cols="4">12.6k 25.62 .396 .569 11.2k</cell><cell>.305 12.5k</cell></row><row><cell>L UL-seq</cell><cell>greedy beam</cell><cell>.130 .018</cell><cell cols="4">12.7k 24.28 .406 .603 16.8k</cell><cell>.329 12.4k</cell></row><row><cell>L UL-token+seq</cell><cell>greedy beam</cell><cell>.051 .013</cell><cell cols="4">14.8k 25.37 .401 .551 17.6k</cell><cell>.287 13.4k</cell></row><row><cell>Human</cell><cell>-</cell><cell>.005</cell><cell>18.9k</cell><cell>-</cell><cell cols="2">-.479</cell><cell>-18.9k</cell></row><row><cell>Search</cell><cell cols="3">Model seq-rep-4 uniq-seq</cell><cell>ppl</cell><cell>acc</cell><cell cols="2">rep wrep</cell><cell>uniq</cell></row><row><cell></cell><cell>L MLE</cell><cell>.0991</cell><cell cols="4">14.7k 25.70 .350 .597</cell><cell>.355 12.6k</cell></row><row><cell>top-k-3</cell><cell>L UL-token L UL-seq</cell><cell>.0491 .0068</cell><cell cols="4">16.4k 27.02 .344 .539 17.9k 25.11 .353 .581</cell><cell>.306 13.6k .341 13.6k</cell></row><row><cell cols="2">L UL-token+seq</cell><cell>.0087</cell><cell cols="4">15.2k 26.84 .347 .524</cell><cell>.292 14.6k</cell></row><row><cell></cell><cell>L MLE</cell><cell>.0165</cell><cell cols="4">21.9k 25.70 .302 .511</cell><cell>.303 16.1k</cell></row><row><cell>top-k-50</cell><cell>L UL-token L UL-seq</cell><cell>.006 .0005</cell><cell cols="4">23.5k 27.02 .286 .440 25.7k 25.11 .291 .497</cell><cell>.247 17.8k .291 17.3k</cell></row><row><cell cols="2">L UL-token+seq</cell><cell>.0009</cell><cell cols="4">23.7k 26.84 .289 .430</cell><cell>.238 18.8k</cell></row><row><cell></cell><cell>L MLE</cell><cell>.273</cell><cell cols="4">13.6k 25.70 .264 .339</cell><cell>.154 12.6k</cell></row><row><cell>top-p-0.3</cell><cell>L UL-token L UL-seq</cell><cell>.101 .0033</cell><cell cols="4">16.5k 27.02 .247 .290 20.8k 25.11 .266 .327</cell><cell>.121 13.9k .145 13.6k</cell></row><row><cell cols="2">L UL-token+seq</cell><cell>.0041</cell><cell cols="4">19.1k 26.84 .250 .284</cell><cell>.116 14.9k</cell></row><row><cell></cell><cell>L MLE</cell><cell>.0154</cell><cell cols="4">26.9k 25.70 .288 .462</cell><cell>.263 18.6k</cell></row><row><cell>top-p-0.9</cell><cell>L UL-token L UL-seq</cell><cell>.004 .0003</cell><cell cols="4">30.2k 27.02 .266 .381 34.7k 25.11 .290 .450</cell><cell>.202 22.3k .254 19.6k</cell></row><row><cell cols="2">L UL-token+seq</cell><cell>.0007</cell><cell cols="4">32.4k 26.84 .269 .376</cell><cell>.198 22.7k</cell></row><row><cell>Human</cell><cell>-</cell><cell>.006</cell><cell>19.8k</cell><cell>-</cell><cell cols="2">-.487</cell><cell>-19.8k</cell></row><row><cell cols="8">Table 6: Stochastic decoding results according to sequence-level (left) and token-level (right) met-</cell></row><row><cell cols="3">rics using the test subset of Wikitext-103.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">B STOCHASTIC DECODING RESULTS</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6</head><label>6</label><figDesc>provides automatic metrics for top-k and nucleus sampling (called top-p) on the Wikitext-103 test set. These can be compared with the main results of the paper in Table2. In general, sampling methods yield worse next-token predictions than deterministic approaches (0.302 vs. 0.394 acc for top-k-50 vs. greedy MLE, where acc for stochastic decoding measures the probability that the decoding strategy chooses the ground truth word given a ground truth context). As the choice of sampling hyperparameter gets closer to greedy (i.e. lower values of k and p) next token accuracy improves, eventually approaching the greedy MLE results. The unlikelihood-trained sampling models have similar next token accuracy (acc) to their likelihood-trained counterparts, but exhibit fewer repetitions. For lower values of p and k the improvements of unlikelihood training are larger, e.g. 0.277 reduced to 0.0041 for 4-gram sequence repetitions (seq-rep-4) using top-p-0.3. At higher levels of p and k, for all methods the continuations contain more unique tokens than that of humans, meaning those values may be too high.</figDesc><table><row><cell>Model</cell><cell cols="2">search seq-rep-4</cell><cell>ppl</cell><cell>acc</cell><cell cols="2">rep wrep</cell><cell>uniq</cell></row><row><cell>GPT-2</cell><cell>greedy</cell><cell cols="4">.506 20.75 .430 .589</cell><cell>.306 13.3k</cell></row><row><cell>GPT-2 MLE</cell><cell>greedy</cell><cell cols="4">.460 15.82 .464 .612</cell><cell>.305 11.8k</cell></row><row><cell>GPT-2 UL-seq</cell><cell>greedy</cell><cell cols="4">.042 18.49 .444 .613</cell><cell>.317 11.3k</cell></row><row><cell>Human</cell><cell>-</cell><cell>.005</cell><cell>-</cell><cell cols="2">-.407</cell><cell>-17.7k</cell></row><row><cell cols="7">Table 7: GPT-2 results according to sequence-level and token-level metrics using the validation</cell></row><row><cell cols="7">subset of wikitext-103. seq-rep-4 is computed on the word level; ppl, acc, rep, wrep are computed</cell></row><row><cell>on the BPE level.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Results for sequence-level fine-tuning using random-seq candidates according to sequence-level (left) and token-level (right) metrics using the validation subset of wikitext-103.</figDesc><table><row><cell>Model</cell><cell cols="4">p penalize search seq-rep-4 uniq-seq</cell><cell>ppl</cell><cell>acc</cell><cell cols="2">rep wrep</cell><cell>uniq</cell></row><row><cell>L MLE</cell><cell cols="2">-greedy -beam</cell><cell>.429 .495</cell><cell cols="4">10.6k 24.590 .401 .619 9.4k</cell><cell>.346 11.6k</cell></row><row><cell>L UL-seq</cell><cell>0.1</cell><cell>greedy beam</cell><cell>.253 .274</cell><cell cols="4">9.9k 24.329 .404 .602 13.1k</cell><cell>.330 12.3k</cell></row><row><cell>L UL-seq</cell><cell>0.9</cell><cell>greedy beam</cell><cell>.434 .231</cell><cell cols="4">5.3k 26.519 .399 .600 13.5k</cell><cell>.330 12.2k</cell></row><row><cell>L UL-tok+seq</cell><cell>0.1</cell><cell>greedy beam</cell><cell>.116 .146</cell><cell cols="4">12.5k 25.518 .399 .551 14.2k</cell><cell>.287 13.2k</cell></row><row><cell>L UL-tok+seq</cell><cell>0.9</cell><cell>greedy beam</cell><cell>.423 .080</cell><cell cols="4">6.7k 26.629 .396 .551 16k</cell><cell>.288 13.2k</cell></row><row><cell>Human</cell><cell>-</cell><cell>-</cell><cell>.005</cell><cell>18.9k</cell><cell>-</cell><cell cols="2">-.479</cell><cell>-18.9k</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>He was elected to the House of Representatives in 1811 , and served until his death in 1815 . = = Early life and education = = ¡/s¿ ¡/s¿ Gore was born in Boston , Massachusetts , on February 22 , 1798 , the son of Benjamin Gore and his Full human evaluation results. Includes additional comparisons omitted for brevity, and the raw number of wins and loses by each comparison.</figDesc><table><row><cell>E.3 FULL HUMAN EVALUATION RESULTS</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">An alternative we tried is to choose a penalization probability ppenalize, and use xt as the single negative candidate for time t when zt ∼ Bernoulli(ppenalize) is 1, and no negative candidate for time t otherwise; this approach was effective but under-performed the Crepeat-n candidates; see Appendix D.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Code and trained models are available at https://github.com/facebookresearch/ unlikelihood_training; implemented with Fairseq<ref type="bibr" target="#b17">(Ott et al., 2019)</ref>.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adaptive input representations for neural language modeling</title>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The missing representation in neural (language) models. 3rd Workshop on Representation Learning for</title>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NLP</title>
		<imprint>
			<date type="published" when="2018">2018. RepL4NLP</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2002 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Search-based structured prediction</title>
		<author>
			<persName><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="325" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Adji B Dieng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><surname>Lecun</surname></persName>
		</author>
		<title level="m">Learning with reflective likelihoods</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Malykh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00098</idno>
		<title level="m">The second conversational intelligence challenge (convai2)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Classical structured prediction losses for sequence to sequence learning</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04956</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04833</idno>
		<title level="m">Hierarchical neural story generation</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Negative training for neural dialogue response generation</title>
		<author>
			<persName><forename type="first">Tianxing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02134</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to write with cooperative discriminators</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09751</idno>
		<title level="m">The curious case of neural text degeneration</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Opennmt: Open-source toolkit for neural machine translation</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.02810</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Importance of a search strategy in neural dialogue modelling</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Kulikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00907</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A tutorial on energybased learning</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Predicting structured data</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A simple, fast diverse decoding algorithm for neural generation</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08562</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons</title>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03087</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
		<title level="m">Pointer sentinel mixture models</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2019</title>
				<meeting>NAACL-HLT 2019</meeting>
		<imprint>
			<publisher>Demonstrations</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04304</idno>
		<title level="m">A deep reinforced model for abstractive summarization</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><surname>Zaremba</surname></persName>
		</author>
		<idno>CoRR, abs/1511.06732</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A reduction of imitation learning and structured prediction to no-regret online learning</title>
		<author>
			<persName><forename type="first">Stéphane</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
				<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="627" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">What makes a good conversation? how controllable attributes affect human judgments</title>
		<author>
			<persName><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1170</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter<address><addrLine>Minneapolis</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1702" to="1723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02433</idno>
		<title level="m">Minimum risk training for neural machine translation</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deconstructing bert: Distilling 6 patterns from 100 million parameters</title>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Vig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Diverse beam search for improved description of complex scenes</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ashwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Vijayakumar</surname></persName>
		</author>
		<author>
			<persName><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Ramprasaath R Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Retrieve and refine: Improved sequence generation models for dialogue</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04776</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Seqgan: Sequence generative adversarial nets with policy gradient</title>
		<author>
			<persName><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingrui</forename><surname>Yu</surname></persName>
		</author>
		<idno>ArXiv, abs/1609.05473</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Prefix Lyrically the song has excerpts of different languages including</title>
		<author>
			<persName><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gpt2-Greedy</forename><surname>Spanish</surname></persName>
		</author>
		<author>
			<persName><surname>German</surname></persName>
		</author>
		<author>
			<persName><surname>Italian</surname></persName>
		</author>
		<author>
			<persName><surname>Portuguese</surname></persName>
		</author>
		<author>
			<persName><surname>Spanish</surname></persName>
		</author>
		<author>
			<persName><surname>Portuguese</surname></persName>
		</author>
		<author>
			<persName><surname>Portuguese</surname></persName>
		</author>
		<author>
			<persName><surname>Portuguese</surname></persName>
		</author>
		<author>
			<persName><surname>Portuguese</surname></persName>
		</author>
		<author>
			<persName><surname>Portuguese</surname></persName>
		</author>
		<author>
			<persName><surname>Portuguese</surname></persName>
		</author>
		<author>
			<persName><surname>Portuguese</surname></persName>
		</author>
		<author>
			<persName><surname>Portuguese</surname></persName>
		</author>
		<author>
			<persName><surname>Portuguese</surname></persName>
		</author>
		<author>
			<persName><surname>Portuguese</surname></persName>
		</author>
		<author>
			<persName><surname>Portuguese</surname></persName>
		</author>
		<author>
			<persName><surname>Portuguese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">,</forename><surname>Portuguese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia; German , Italian , Portuguese , Spanish , Portuguese , Portuguese, Italian</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2204" to="2213" />
		</imprint>
		<respStmt>
			<orgName>French and Spanish</orgName>
		</respStmt>
	</monogr>
	<note>and Spanish. It is also available in English. In addition to its lyrics, the album features a number of original songs, ..</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Italian</forename><surname>German</surname></persName>
		</author>
		<author>
			<persName><surname>Portuguese</surname></persName>
		</author>
		<author>
			<persName><surname>Spanish</surname></persName>
		</author>
		<author>
			<persName><surname>Portuguese</surname></persName>
		</author>
		<author>
			<persName><surname>Italian</surname></persName>
		</author>
		<author>
			<persName><surname>Portuguese</surname></persName>
		</author>
		<author>
			<persName><surname>Spanish</surname></persName>
		</author>
		<author>
			<persName><surname>Portuguese</surname></persName>
		</author>
		<author>
			<persName><surname>Spanish</surname></persName>
		</author>
		<author>
			<persName><surname>Portuguese</surname></persName>
		</author>
		<author>
			<persName><surname>Spanish</surname></persName>
		</author>
		<author>
			<persName><surname>Portuguese</surname></persName>
		</author>
		<author>
			<persName><surname>Spanish</surname></persName>
		</author>
		<author>
			<persName><surname>Portuguese</surname></persName>
		</author>
		<author>
			<persName><surname>Spanish</surname></persName>
		</author>
		<author>
			<persName><surname>Portuguese</surname></persName>
		</author>
		<author>
			<persName><surname>Spanish</surname></persName>
		</author>
		<author>
			<persName><surname>Portuguese</surname></persName>
		</author>
		<author>
			<persName><surname>Spanish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">,</forename><surname>Portuguese</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Prefix The first pair of pereiopods is armed with a large , asymm GPT2-greedy etrical, and long-range laser cannon. The second pair is armed with a large , asymmetrical, and long-range laser cannon. The third pair is armed with a large , asymmetrical, and long-range laser cannon. The fourth pair is armed with a large</title>
		<imprint/>
	</monogr>
	<note>GPT2-top3 etrical, and highly mobile head, and the second pair has a small , asymmetrical, and highly mobile head. The second pair has a large and highly mobile head. The first pair ..</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">etrical head and a large body. The first pair of pereiopods is armed with a large</title>
		<idno>GPT2-nucleus-0.3</idno>
		<imprint/>
	</monogr>
	<note>The first pair of pereiopods is armed with a large. The first pair of pereiopods is armed ..</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
