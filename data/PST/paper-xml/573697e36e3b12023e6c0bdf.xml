<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">S$A: A Shared Cache Attack that Works Across Cores and Defies VM Sandboxing-and its Application to AES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Gorka</forename><surname>Irazoqui</surname></persName>
							<email>girazoki@wpi.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Worcester Polytechnic Institute Worcester</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Eisenbarth</surname></persName>
							<email>teisenbarth@wpi.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Worcester Polytechnic Institute Worcester</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Berk</forename><surname>Sunar</surname></persName>
							<email>sunar@wpi.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Worcester Polytechnic Institute Worcester</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">S$A: A Shared Cache Attack that Works Across Cores and Defies VM Sandboxing-and its Application to AES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B278C3F974493A08B846EB9835961A2C</idno>
					<idno type="DOI">10.1109/SP.2015.42</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cross-VM</term>
					<term>huge pages</term>
					<term>memory deduplication</term>
					<term>prime and probe</term>
					<term>flush+reload</term>
					<term>cache attacks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The cloud computing infrastructure relies on virtualized servers that provide isolation across guest OS's through sandboxing. This isolation was demonstrated to be imperfect in past work which exploited hardware level information leakages to gain access to sensitive information across co-located virtual machines (VMs). In response virtualization companies and cloud services providers have disabled features such as deduplication to prevent such attacks.</p><p>In this work, we introduce a fine-grain cross-core cache attack that exploits access time variations on the last level cache. The attack exploits huge pages to work across VM boundaries without requiring deduplication. No configuration changes on the victim OS are needed, making the attack quite viable. Furthermore, only machine co-location is required, while the target and victim OS can still reside on different cores of the machine. Our new attack is a variation of the prime and probe cache attack whose applicability at the time is limited to L1 cache. In contrast, our attack works in the spirit of the flush and reload attack targeting the shared L3 cache instead. Indeed, by adjusting the huge page size our attack can be customized to work virtually at any cache level/size. We demonstrate the viability of the attack by targeting an OpenSSL1.0.1f implementation of AES. The attack recovers AES keys in the cross-VM setting on Xen 4.1 with deduplication disabled, being only slightly less efficient than the flush and reload attack. Given that huge pages are a standard feature enabled in the memory management unit of OS's and that besides co-location no additional assumptions are needed, the attack we present poses a significant risk to existing cloud servers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The end of exponential growth of single core performance in the past decade has helped creating a new industry selling computing infrastructure as a service (IaaS) popularly referred to as cloud computing. Instead of financing and maintaining expensive workstations and servers, companies can rent the resources from cloud providers just when needed and only for the duration of the need, thereby significantly cutting IT costs. A number of well-known tech companies such as Google, Amazon AWS, EMC come to mind when mentioning cloud computing. Popular user-oriented examples include cloud backed storage service providers like Dropbox in the personal computing space and Box.net in the enterprise. These are just a couple of examples among numerous businesses enabled by cloud backed compute and storage offerings such as Amazon's EC2 compute and S3 storage solutions, respectively. Nevertheless, like any emerging technology, cloud services have also encountered their unique security challenges. The problem stems from the fact that most security technologies were developed for a world of isolated servers. These servers were subsequently transferred to virtualized servers hosting a number of guest OS's without any adjustments.</p><p>A new class of security vulnerabilities arises due to one of the most important principles that cloud systems are based on: co-residency and multi-tenancy. The benefit of cloud computing comes from resource sharing, implying that multiple customers will utilize the same hardware of the same physical machine instead of assigning a dedicated server per user. Despite the benefits that co-residency bestows, namely maintenance and electricity cost reduction, it also implies that users run their virtual machines (VM) in the same hardware only separated by the virtualization layer provided by the Virtual Machine Manager (VMM). In theory sandboxing techniques should provide the required isolation between VMs, but of course the devil is in the details.</p><p>A serious threat to VM isolation (and therefore the customer's privacy) comes from side channel attacks which exploit subtle information leakage channels at the microarchitectural level. If side channel attacks can circumvent the logical isolation provided by the hypervisor, critical pieces of information such as cryptographic keys might be stolen. In particular, co-residency creates a scenario where microarchitectural side channels can potentially be exploited. A large number of microarchitectural attacks targeting cryptographic keys have already been extensively studied and successfully applied in non-virtualized scenarios. For instance, cache attacks are based on access time variations when retrieving data from the cache and from the memory, as proposed by Bernstein <ref type="bibr" target="#b0">[1]</ref> or Osvik et al. <ref type="bibr" target="#b1">[2]</ref>. Both studies manage to recover AES secret keys by monitoring the cache utilization. Modern memory saving features like Kernel Samepage Merging (KSM) <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> have also been shown to threaten the security of cryptographic processes as proven by Gullasch et.al <ref type="bibr" target="#b4">[5]</ref>, recovering AES keys with as few as 100 encryptions. However, despite the success of these attacks in non-virtualized scenarios, still very little research has been done aiming in securing the implementation of cryptosystems in the virtualized setting.</p><p>It was not until 5 years ago, when motivated by the work done by Ristenpart et al. <ref type="bibr" target="#b5">[6]</ref>, that the first successful implementations of side channel attacks inside VMs started to appear in the community. In fact, <ref type="bibr">Ristenpart et al.</ref> were not only able to co-locate two virtual machines hosted by Amazon EC2 on the same physical hardware, but also managed to recover key strokes used by a victim VM. In consequence, they showed for the first time that side channel attacks can be implemented in the cloud to break through the isolation provided by sandboxing techniques.</p><p>From that point on, researches have been focusing on recovering fine grain information with new and known side channel techniques targeting weak cryptographic implementations inside VMs, e.g. El Gamal <ref type="bibr" target="#b6">[7]</ref> or AES <ref type="bibr" target="#b7">[8]</ref>. The Flush+Reload technique has proven to be particularly effective when memory deduplication features are enabled by the VMM. Indeed, Yarom et al. <ref type="bibr" target="#b8">[9]</ref> demonstrated attack that recovered RSA keys across VMs running in different cores and hosted by KVM and VMware. Later Irazoqui et al. <ref type="bibr" target="#b10">[10]</ref> used the same technique to recover AES keys across VMware VMs. The relevance of these studies is highlighted by the prompt security update by VMware, making memory deduplication an opt-in feature that was formerly enabled by default. Recognizing the potential for a security compromise, Amazon never enabled deduplication on their EC2 compute cloud servers.</p><p>Even though mechanisms that prevent these attacks have been implemented, the discussion still remains open in the community. Indeed, new side channel attacks (such as the one proposed in this work) compromising the VM isolation techniques may arise, consequently requiring new countermeasures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Contribution</head><p>In this work, we introduce a novel cross-core and cross-VM cache-based side-channel attack that exploits the shared L3 cache. The attack takes advantage of the additional physical address knowledge gained by the usage of huge size pages. Thus, the attack is not only applicable in non-virtualized environments but also in the cloud, since huge pages is enabled by default in all common hypervisors, i.e, Xen, VMware and KVM. Unlike the popular Flush+Reload attack <ref type="bibr" target="#b8">[9]</ref>, the new attack does not rely on deduplication features (no longer enabled by default in VMware and never enabled on Amazon AWS servers) and therefore, it can be applied with hypervisors not considered in <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[10]</ref> like Xen. Furthermore, the attack is nearly undetectable by the victim, since only a small number of sets are profiled in the L3 cache.</p><p>The viability of the new side channel attack is demonstrated on AES in both non-virtualized and virtualized cross VM scenarios. The attack is compared to previous attacks performed on AES in the cloud <ref type="bibr" target="#b11">[11]</ref>. The new attack is significantly more efficient than <ref type="bibr" target="#b11">[11]</ref> and achieves similar efficiency as <ref type="bibr" target="#b10">[10]</ref>. The attack requires very little time to succeed, i.e, the AES key is recovered in less than 3 minutes in fully virtualized Xen 4. -Shows that the attack can be applied in the cloud since most of the hypervisors allow the usage of huge size pages by the guest OSs. -Presents the viability of the new side channel technique by recovering AES keys when attacker and victim are located in different cores. -Demonstrates that the attack is also practical by recovering the AES key in less than 3 minutes in virtualized settings. We summarize existing cache-based side-channel attacks as well as virtual address translation and cache addressing in Section II. The new side channel attack is introduced in Section III. Results are presented in Section V. Before concluding in Section VII possible countermeasures are discussed in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head><p>In this section we give a brief overview of the background needed to understand the new attack presented in this work. After summarizing cache side channel attacks, their history and the improvements that have been developed over the last 15 years, a short explanation of Virtual Address Cache Mapping and the previous Prime+Probe technique are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Cache Side Channel Attacks</head><p>Cache side channel attacks take advantage of the information leakage stemming from microarchitectural time differences when data is retrieved from the cache rather than the memory. The cache is a small memory placed between the CPU and the RAM to avoid the big latency added by the retrieval of the data. Modern processors usually have more than one level of cache to improve the efficiency of memory accesses. Caches base their functionality on two different principles, i.e, temporal and spatial locality. The first one predicts that data accessed recently will be accessed soon, whereas the latter one predicts that data in nearby locations to the accessed data will also be accessed soon. Thus, when a value is fetched from memory by the CPU, a copy of that value will be placed in the cache, together with nearby memory values to reduce the latency of future accesses.</p><p>Obviously, data in cache can be accessed much faster than data only present in memory. This is also true for multilevel caches, where data accessed from the L1 cache will experience lower latencies than data accessed from subsequent cache levels. These time differences are used to decide whether a specific portion of the memory resides in the cache-implying that the data has been accessed recently. The resulting information leakage posses a risk especially for cryptographic algorithms, which might lead to compromise of secret keys. Although many spy processes have been introduced targeting the L1 cache, implying core co-location, lately cross-core spy processes have gained most of the attention. In the latter case, typically the Last Level Cache (LLC) acts as a covert channel, since it is usually shared by all the cores in most modern processors. Cross-core cache side channel attacks are particularly dangerous in cloud settings, where more than one user co-reside in the same hardware, and the chance of two users being co-located on different cores is high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Previous Cache Attacks</head><p>The cache was first considered to be a suitable covert channel for the unauthorized extraction of information in 1992 by Hu <ref type="bibr" target="#b12">[12]</ref>. Kesley et al. <ref type="bibr" target="#b13">[13]</ref> also mentioned the possibility of cache attacks based on the cache hit/miss ratio. Later, cache attack examples were studied theoretically by Page <ref type="bibr" target="#b14">[14]</ref> whereas Tsunoo et al. <ref type="bibr" target="#b15">[15]</ref> investigated timing leakage due to internal table look up collisions.</p><p>However it was not until 2004 when the first practical implementations of cache attacks were studied. For instance, Bernstein <ref type="bibr" target="#b0">[1]</ref> implemented a cache timing attack targeting AES based on the existing microarchitectural leakage when different memory position are loaded in the cache. He used this leakage to recover the full AES key in some implementations. At the same time Osvik et al. <ref type="bibr" target="#b1">[2]</ref> investigated the impact of two different trace driven attacks on AES: Evict + Time and Prime+Probe. They showed that both methods can be applied in spy processes to recover AES keys. One year later Bonneau and Mironov exploited the cache collisions due to internal table look ups in AES to obtain the secret key <ref type="bibr" target="#b16">[16]</ref>.</p><p>A similar collision timing attack was presented by Acıiçmez et al. <ref type="bibr" target="#b17">[17]</ref> targeting the first and second encryption rounds of AES while Neve and Seifert <ref type="bibr" target="#b18">[18]</ref> studied the impact of access driven cache attacks in the last round of AES. In 2007 Acıiçmez proved that AES and the data cache were not the only possible target of cache side channel attacks <ref type="bibr" target="#b19">[19]</ref>. He discovered leakages in the instruction cache during public key encryptions and applied cache side channel attacks to recover RSA keys.</p><p>However, most of the attacks mentioned above were implemented as spy processes in a native OS environment, reducing the practical impact of the attacks in realistic scenarios. It was not until 2009 when Ristenpart et al. managed to co-locate two virtual machines in a public cloud achieving the usage of the same CPU <ref type="bibr" target="#b5">[6]</ref> that cross VM attacks on the public cloud were considered practical. Their experiments in the Amazon EC2 public cloud <ref type="bibr" target="#b20">[20]</ref> achieved a co-residency success rate of up to 40% with the desired target by using different properties like IP range and instance type. The work also demonstrated that cache usage can be analyzed to deduce secret keystrokes used by a potential victim. Hence, the attack demonstrated for the first time that microarchitectural side channel attacks that require co-location are a potential threat in the cloud setting. Further co-residency detection methods such as traffic analysis later were studied, e.g. by Bates et al. <ref type="bibr" target="#b21">[21]</ref>.</p><p>The research made on detecting co-residency motivated many researchers to apply known side channel techniques in the cloud. For instance, Zhang et al. <ref type="bibr" target="#b22">[22]</ref> used the above mentioned Prime+Probe technique to detect whether any other tenant was co-located in the same hardware. Shortly later again Zhang et al. <ref type="bibr" target="#b6">[7]</ref> recovered El Gamal encryption keys by monitoring the L1 instruction cache in a virtualized setting, again with the Prime+Probe spy process. Their experiments were carried out in Xen VMs and they had to apply a hidden Markov model to reduce the noise present in their measurements. Bernstein's attack was also tried in virtualized environments, first by Weiss et al. <ref type="bibr" target="#b7">[8]</ref> in ARM processors and then by Irazoqui et al. in VMware or Xen <ref type="bibr" target="#b11">[11]</ref>.</p><p>At the same time new spy processes and improvements over previous techniques were investigated in non-virtualized scenarios. Chen et al. presented an improvement over Acıiçmez's technique to monitor the instruction cache and recover a RSA key <ref type="bibr" target="#b23">[23]</ref>, whereas Aly et al. <ref type="bibr" target="#b24">[24]</ref> studied an improvement on the detection method for the Bernstein's attack. Cache collision attacks on AES and instruction cache attacks on DSA were also further investigated by Spreitzer and Plos <ref type="bibr" target="#b25">[25]</ref> and Acıiçmez et al. <ref type="bibr" target="#b26">[26]</ref>, respectively. On the other hand, Gullasch et al. <ref type="bibr" target="#b4">[5]</ref> studied a new side channel technique that would later acquire the name of Flush+Reload and that is based on memory saving features like Kernel Samepage Merging (KSM). They were able to recover a full AES key by monitoring the data cache while getting control of the Control Fair Scheduler (CFS) <ref type="bibr">[27]</ref>. This new method proved that successful cache attacks can still be implemented in modern processors, contrary to the claim of <ref type="bibr" target="#b27">[28]</ref>.</p><p>More recently, Yarom et al. used the Flush+Reload technique to attack the RSA implementation of Libgcrypt <ref type="bibr" target="#b8">[9]</ref>. Furthermore, they showed that their attack is applicable in a cross-core and cross-VM setting. Hence, it could be applied in cloud environments, particularly in the VMMs implementing memory deduplication features like VMware or KVM. Shortly later, Benger et al. applied the same technique to recover ECDSA keys <ref type="bibr" target="#b28">[29]</ref>. Irazoqui et al. demonstrated that Flush+Reload can also be applied to recover AES keys without the need of controlling the CFS, and also proved the viability of their method across VMware VMs <ref type="bibr" target="#b10">[10]</ref>. Finally, Zang et al. <ref type="bibr" target="#b29">[30]</ref> showed that Flush+Reload can recover sensitive information from co-located processes in PaaS clouds.</p><p>In a concurrent work, Yarom et.al <ref type="bibr" target="#b30">[31]</ref> used the same technique described in this paper to recover a full RSA key in a sliding window implementation by recovering the positions where each ciphertext table entry is accessed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Virtual Address Translation and Cache Addressing</head><p>In this work we present an attack that takes advantage of some known information in the virtual to physical address mapping process. Thus, we give a brief overview about the procedure followed by modern processors to access and address data in the cache <ref type="bibr" target="#b31">[32]</ref>.</p><p>In modern computing, processes use virtual memory to access requested memory locations. Indeed processes do not have direct access to the physical memory, but use virtual addresses that are then mapped to physical addresses by the Memory Management Unit (MMU). This virtual address space is managed by the Operating System. The main benefits of virtual memory are security (processes are isolated from real memory) and use of more memory than physically available due to paging techniques.</p><p>The memory is divided into fixed length continuous blocks called memory pages. The virtual memory allows the usage  of these memory pages even when they are not allocated in the main memory. When a specific process needs a page not present in the main memory, a page fault occurs and the page has to be loaded from the auxiliary disk storage. Therefore, a translation stage is needed to map virtual addresses to physical addresses prior to the memory access. In fact, cloud systems have two translation processes, i.e, guest OS to VMM virtual address and VMM virtual address to physical address. The first translation is handled by shadow page tables while the second one is handled by the MMU. This adds an abstraction layer with the physical memory that is handled by the VMM.</p><p>During translation, the virtual address is split into two fields: the offset field and the page field. The length of both fields depends directly on the page size. Indeed, if the page size is 𝑝 bytes, the lower log 2 (𝑝) bits of the virtual address will be considered as the page offset, while the rest will be considered as the page number. Only the page number is processed by the MMU and needs to be translated from virtual to physical page number. The page offset remains untouched and will have the same value for both the physical and virtual address. Thus, the user still knows some bits of the physical address. Modern processors usually work with 4 KB pages and 48 bit virtual addresses, yielding a 12 bit offset and the remaining bits as virtual page number.</p><p>In order to avoid the latency of virtual to physical address translation, modern architectures include a Translation Lookaside Buffer (TLB) that holds the most recently translated addresses. The TLB acts like a small cache that is first checked prior to the MMU. One way to avoid TLB misses for large data processes is to increase the page size so that the memory is divided into fewer pages <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>. Since the possible virtual to physical translation tags have been significantly reduced, the CPU will observe less TLB misses than with 4 KB pages. This is the reason why most modern processors support the use of huge size pages, which typically have a size of at least 1 MB. This feature is particularly effective in virtualized settings, where virtual machines are typically rented to avoid the intensive use of hardware resources on private computers. In fact, most well known VMM providers support the use of huge size pages by guest OSs to improve the performance of those heavy load processes <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>.</p><p>Cache Addressing: Caches are physically tagged, i.e, the physical address is used to decide the position that the data will occupy in the cache. With 𝑏-byte size cache lines and 𝑚-way set associative caches (with 𝑛 number of sets), the lower log 2 (𝑏) bits of the physical address are used to index the byte in a cache line, while the following log 2 (𝑛) bits select the set that the memory line is mapped to in the cache. A graphical example of the procedure carried out to address the data in the cache can be seen in Figure <ref type="figure" target="#fig_2">1</ref>. Recall that if a page size of 4 KB is used, the offset field is 12 bits long. If log 2 (𝑛)+log 2 (𝑏) is not bigger than 12, the set that a cache line is going to occupy can be addressed by the offset. In this case we say that the cache is virtually addressed, since the position occupied by a cache line can be determined by the virtual address. In contrast, if more than 12 bits are needed to address the corresponding set, we say that the cache is physically addressed, since only the physical address can determine the location of a cache line. Note that when huge size pages are used, the offset field is longer, and therefore bigger caches can be virtually addressed. As we will see, this information can be used to mount a cross-VM attack in the L3 cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The Prime+Probe Technique</head><p>Our new attack is based on the methodology of the known Prime+Probe technique. Prime+Probe is a cache-based side channel attack technique used in <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b6">[7]</ref> that can be classified as an access driven cache attack. The spy process ascertains which of the sets have been accessed in the cache by a victim. The attack is carried out in 3 stages:</p><p>-Priming stage: In this stage, the attacker fills the monitored cache with his own cache lines. This is achieved simply by accessing his own made up data.</p><p>-Victim accessing stage: In this stage the attacker waits for the victim to access some positions in the cache, causing the eviction of some of the cache lines that were primed in the first stage. -Probing stage: In this stage the attacker accesses the priming data again. When the attacker reloads data from a set that has been used by the victim, some of the primed cache lines have been evicted, causing a higher probe time. However if the victim did not use any of the cache lines in a monitored set, all the primed cache lines will still reside in the cache causing a low probe time.</p><p>The Prime+Probe side channel attack has some limitations. First, it can only be applied in small caches (typically the L1 cache), since only a few bits of the virtual address are known. Second, the employment of such a spy process in small caches restricts its application to processes co-located on the same core. Finally, modern processors have very similar access times for L1 and L2 caches, only differing in a few cycles, which makes the detection method noisy and challenging. For instance, this challenge was also experienced in <ref type="bibr" target="#b6">[7]</ref>, where the authors had to apply a Hidden Markov Model in addition to the Prime+Probe technique to deal with noisy measurements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE S$A ATTACK</head><p>In this section we present the technical details of our S$A attack. Later we demonstrate the viability of the attack on the OpenSSL1.0.1.f's C-implementation of AES <ref type="bibr" target="#b38">[39]</ref> to achieve a full AES key recovery in a scenario where the attacker and the victim are co-located on the same machine but run on different cores. Our S$A attack has several advantages over the previous cache side channel attacks on AES:</p><p>-Our S$A attack is the first efficient cross-core cache attack that does not take advantage of deduplication features, yet succeeds in retrieving key information across VM boundaries. While some previous attacks, e.g. Flush+Reload rely on deduplication, other attacks such as Prime+Probe were also applied in the cloud but assumed to be co-located in the same core with the target process. In contrast, the new S$A attack detects accesses made to the last level cache by using huge size pages to allocate the attacker's data. Since the last level of cache is usually shared among all the cores in modern processors, our spy process can detect cache accesses even when the victim is co-located in a different core on the same machine; -We achieve almost the same efficiency as the Flush+Reload attack with the S$A spy process. Other attacks like Bernstein's attack require a much higher number of encryptions to get partial information of the AES key; -The S$A can be considered a non-intrusive cache attack.</p><p>In the case of AES only 4 sets from the last level cache need to be monitored to recover a full AES encryption key.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. S$A enabled by Huge Pages</head><p>The S$A attack proposed in this work, is enabled by making use of huge pages and thereby eliminating a major obstacle that normally restricts the Prime+Probe attack to target the L1 cache. A similar method was first discussed by Hund et al. <ref type="bibr" target="#b39">[40]</ref> to bypass the ASLR in a Windows OS. As explained in Section II, a user does not use the physical memory directly. Instead, the user is assigned a virtual memory so that a translation stage is performed from virtual to physical memory at the hardware level. The address translation step creates an additional challenge to the attacker since real addresses of the variables of the target process are unknown to him. However this translation is only performed in some of the higher order bits of the virtual address, while a lower portion, named the offset, remains untouched. Since caches are addressed by the physical address, if we have cache line size of 𝑏 bytes, the lower log 2 (𝑏) bits of the address will be used to resolve the corresponding byte in the cache line. Furthermore if the cache is set-associative and for instance divided into 𝑛 sets, then the next log 2 (𝑛) bits of the address will select the set that each memory data is going to occupy in the cache. The log 2 (𝑏)-bits that form the byte address within a cache line, are contained within the offset field. However, depending on the cache size the following field which contains the set address may exceed the offset boundary. The offsets allow addressing within a memory page. The OS's Memory Management Unit (MMU) keeps track of which page belongs to which process. The page size can be adjusted to better match the needs of the application. Smaller pages require more time for the MMU to resolve.</p><p>Here we focus on the default 4 KB page size and the larger page sizes provided under the common name of Huge pages. As we shall see, the choice of page size will make a significant difference in the attackers ability to carry out a successful attack on a particular cache level:</p><p>-4 KB pages: For 4 KB pages, the lower 12-bit offset of the virtual address is not translated while the remaining bits are forwarded to the Memory Management Unit. In modern processors the cache line size is usually set as 64 bytes. This leaves 6 bits untouched by the Memory Management Unit while translating regular pages. As shown in the top of Figure <ref type="figure" target="#fig_3">2</ref> the page offset is known to the attacker. Therefore, the attacker knows the 6-bit byte address plus 6 additional bits that can only resolve accesses to small size caches (64 sets at most). This is the main reason why techniques such as Prime+Probe have only targeted the L1 cache, since it is the only one permitting the attacker to have full control of the bits resolving the set. Therefore, the small page size indirectly prevents attacks targeting big size caches, i. remaining untouched during page translation. Observe the example presented in Figure <ref type="figure" target="#fig_3">2</ref>. For instance, assume that our computer has 3 levels of cache, with the last one shared, and the L1, L2 and L3 caches are divided into 64, 512 and 4096 sets, respectively. The first lowest 6-bits of the offset are used for addressing the 64 byte long cache lines. The following 6 bits are used to resolve the set addresses in the L1 cache. For the L2 and L3 caches this field is 9 and 12-bits wide, respectively. In this case, a huge page size of 256 KB (18 bit offset) or higher will give the attacker full control of the set occupied by his data in all three levels of cache, i.e. L1, L2 and L3 caches. A 256 KB or higher page size, will enable an attacker to target individual lines of the entire L3 cache. The significance of targeting the last level cache becomes apparent when one considers the access time gap between the last level cache and the memory, which is much more pronounced compared to the access time difference between the L1 and L2 caches. Therefore, using huge pages makes it possible to reach a higher resolution Prime+Probe style attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The S$A Attack</head><p>The S$A technique takes advantage of the control of the lower 𝑘 bits in the virtual address that we gain with the huge size pages. These are the main steps that our spy process will follow to detect accesses to the last level cache:</p><p>-Step 1 Allocation of huge size pages: The spy process is based on the control that the attacker gains on the virtual address when using huge size pages. Therefore the spy process has to have access to the available huge pages, which requires administrator rights. Recall that this is not a problem in the cloud scenario where the attacker has administrator privileges to his guest OS. -Step 2 Prime the desired set in the last level cache:</p><p>In this step the attacker creates data that will occupy one of the sets in the last level cache. By controlling the virtual address, the attacker knows the set that the created data will occupy in the last level cache. Once sufficiently many lines are created to occupy the set, the attacker primes it and ensures that the set is filled. Typically the last level caches are inclusive. Thus we will not only fill the shared last level cache set but also some sets in the upper level caches. -Step 3 Reprime to ensure that our data only resides in last level cache: Priming all cache levels can lead to misspredictions due to the different access times between the last level of cache and the upper levels. Since we clearly want to distinguish between accesses from the last level cache and memory, we reprime our upper level caches. The basic idea is to be sure to evict our data from the upper level caches, but not from the last level cache. Therefore we ensure that our reprime data goes to a different set in the last level cache, but to the same set in the upper level caches. -Step 4: Victim process runs: After the two priming stages, the victim runs the target process. Since one of the sets in the last level cache is already filled, if the targeted process uses the monitored set, one of the primed lines is going to be evicted. Remember we are priming the last level cache, so evictions will cause memory lines to reside in the memory. If the monitored set is not used, all the primed lines are going to reside in the last level cache after the victim's process execution. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Probability memory access</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L3 cache AccessTime histogram Memory AccessTime histogram</head><p>Fig. <ref type="figure">3</ref>. Histograms of 10,000 access times in the probe stage when all the lines are in the L3 cache and when all except one are in the cache (and the other one in the memory).</p><p>-Step 5: Probe and measure: Once the victim's process has finished, the spy process probes the primed memory lines and measures the time to probe them all. If one or more lines have been evicted by the targeted process, they will be loaded from the memory and we will see a higher probe time. However if all the lines still reside in the set, then we will see a shorter probe time. The last step can be made more concrete with the experiment results summarized in Figure <ref type="figure">3</ref>. The experiment was performed in native execution (no VM) on an intel i5-650 that has a 16-way associative last level cache. It can be seen that when all the lines reside in the last level cache we obtain very precise probe timings with average around 250 cycles and with very little variance. However when one of the lines is evicted from last level cache and resides in memory, both the access time and the variance are higher. We conclude that both types of accesses are clearly distinguishable.</p><p>For further clarification of the prime and reprime stages we present an example in Figure <ref type="figure" target="#fig_5">4</ref>. Assume that we want to monitor set 0 in the last level cache. The last level cache has 1024 sets, and the upper level caches have only 64 sets. Assume that the memory line size is 64 bytes and that the associativity for this cache is 8 and 4 for the last level cache and the upper level caches, respectively. In the example we also assume that all the caches are inclusive. We know that bits 0 -5 will select the corresponding byte in the memory line. We set our data so that the virtual address is 0 from bit 6 to bit 15, in order to ensure that we are filling set 0 in the last level cache. We have to take into account that not only the last level cache will be filled, but also the upper level caches. The reprime stage evicts the blue lines in the upper level caches and replaces them with the yellow lines, which will go to a different set in the last level cache. With this approach, we ensure that the lines we are working with only reside in set 0 of the last level cache.</p><p>Handling Cache Slices: Typically the last level of cache is divided into slices <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b32">[33]</ref>. This means that if the specifications say that we have a 4 MB last level cache, this might be divided into two (or more) slices of 2 MB each. Suppose now that the last level cache is a 𝑚-way set associative cache, and that it has 𝑛 sets. If the last level cache is divided into two slices, we would be addressing 𝑛/2 sets instead of 𝑛 sets. Depending on the slice selection method that the architecture implements, our data occupies slice 0 or slice 1. Recall that the last level of cache is usually shared among all the cores. This means that if the cache is not divided into slices, two cores will not be able to access data in the same set in the same clock cycle. However if the cache is divided in two slices, there is a 50% chance that two different cores are accessing different slices. Therefore, the cores can access data in the same set in the same clock cycle.</p><p>The division of the last level cache into slices makes it necessary to add another step to the S$A. Depending on the algorithm used to select the corresponding slice, the selection of the lines that fill one of the sets of one of the slices can be difficult. However we can always identify the lines that fill a specific set in a slice by measuring the reload time of those lines. If we are working with an 𝑚-way associative cache, we need 𝑚 lines to fill one of the sets in one of the slices. We can verify that we found those specific lines when priming and probing 𝑚 + 1 lines gives a significantly higher reload time, since the (𝑚+1) 𝑡ℎ line evicts one of the previous ones. Using this method, it is straightforward to try and identify such cache lines for each slice.</p><p>The Intel i5-650 processor used in our experiments has a two-sliced last level cache. The slice where the data is going to X X X X X X 0 0 0 0 . . . . . . . 0 0 0 X X X . . . . . . . . . X X X</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rest</head><p>Set Address Byte Address b16 b6 X X X X X X 0 0 0 0 . . . . . . . .0 0 0 X X X . . . . . be located is selected with the (𝑙 + 1) 𝑡ℎ bit, assuming we have 𝑙 bits to address the set and cache line byte. If the (𝑙 + 1) 𝑡ℎ bit is 0, the data will be stored in slice 0, whereas if the bit is a 1, the data will be stored in the slice 1.</p><formula xml:id="formula_0">. . . . X X X LINE 0 .. .. LINE 7 LAST LEVEL CACHE SET 0 X X X X X X 0 0 0 . . . . 0 0 0 X X X . . . . . . . . X X X Rest Set Address Byte Address b16 b6 X X X X X X X X X . . . . . . . . . X X X</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. S$A APPLIED TO AES</head><p>In this section we proceed to explain how the S$A spy process can be applied to attack AES. We use the C reference implementation of OpenSSL1.0.1f library which uses 4 different T-tables during the AES execution. The implementation of AES is based on the execution of three main operations, i.e., a table lookup operation, a MixColumns operation and a key addition operation. For AES-128 these operations are repeatedly executed for 9 rounds, whereas the last round only implements the table look up and key addition operations. OpenSSL uses 4 different 1 KB sized T-tables for the 10 rounds. Recovering one round key is sufficient for AES-128, as the key scheduling is invertible.</p><p>We use the last round as our targeted round for convenience. Since the 10 𝑡ℎ round does not implement the MixColumns operation, the ciphertext directly depends on the T-table position accessed and the last round key. Assume 𝑆 𝑖 to be the value of the 𝑖th byte prior to the last round T-table look up operation. Then the ciphertext byte 𝐶 𝑖 will be:</p><formula xml:id="formula_1">𝐶 𝑖 = 𝑇 𝑗 [𝑆 𝑖 ] ⊕ 𝐾 10 𝑖 (1)</formula><p>where 𝑇 𝑗 is the corresponding T-table applied to the 𝑖 𝑡ℎ byte and 𝐾 10  𝑖 . It can be observed that if the ciphertext and the Ttable positions are known, we can guess the key by a simple XOR operation. We assume the ciphertext to be always known by the attacker. Therefore the attacker will use the S$A spy process to guess the T-table position that has been used in the encryption and consequently, obtain the key.</p><p>Since S$A will decide which table look up position has been used by monitoring memory accesses, we need to know how the T-tables are handled in memory. With 64 byte memory lines, each T-table occupies 16 cache lines and each cache line holds 16 T-table positions for OpenSSL 1.0.1f. Furthermore the sets that each of these lines occupy in the cache increase sequentially, i.e, if 𝑇 [0 -15] occupies set 0, then 𝑇 <ref type="bibr">[16 -31]</ref> occupies set 1..etc. Since each encryption makes 40 accesses to each of the T-tables, the probability of not accessing one of the T-tables memory lines is:</p><formula xml:id="formula_2">Prob[no access𝑇 [𝑖]] = (1 -(15/16)) 40 ≈ 8%.<label>(2)</label></formula><p>Thus, if the attacker knows which set each of the T-table memory lines occupies, S$A will detect that the set is not accessed 8% of the time. We use the same procedure as in <ref type="bibr" target="#b10">[10]</ref> to determine the key used in the last round operation. Each ciphertext value is going to be assigned a counter that will depend on the usage of the monitored T-table line. Recall that the use of the monitored T-table memory line could have taken place in any of the 10 rounds of AES. However, since the accesses are profiled according to the corresponding ciphertext value, the attacker has two options:</p><p>-Assign an access counter: Assign an access counter to each possible ciphertext byte value 𝐶 𝑖 that increments each time the monitored T- Measuring microarchitectural timings implies dealing with noise that increases the measured time, e.g., TLB misses and context switches. Since in our attack scenario this noise is most of the time only biased in one direction (increasing access times), we decide to use the miss counter, since it is less susceptible to noise, hence ensuring very low false positives.   Thus, once enough measurements have been collected by S$A we will see that 16 ciphertext values have significantly higher access counters than the rest. The key is obtained by solving Equation (1), i.e, by XOR-ing each of the ciphertext values with each of the values in the monitored T-table memory line. This operation outputs sets of possible keys for each ciphertext value, while the correct key is present in all of them. : The previous description implicitly assumes that the attacker knows the location, i.e. the sets, that each T-table occupies in the shared level cache. A simple approach to gain this knowledge is to prime and probe every set in the cache, and analyze the timing behavior for a few random AES encryptions. The T-table based AES implementation leaves a distinctive fingerprint on the cache, as T-table size as well as the access frequency (92% per line per execution) are known. Once the T-tables are detected, the attack can be performed on a single line per table. Nevertheless, this locating process can take a significant amount of time when the number of sets is sufficiently high in the outermost shared cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Locating the Set of the T-Tables</head><p>An alternative, more efficient approach is to take advantage of the shared library page alignment that some OSs like Linux implement. Assuming that the victim is not using huge size pages for the encryption process, the shared library is aligned at a 4 KB page boundary. This gives us some information to narrow down the search space, since the lower 12 bits of the virtual address will not be translated. Thus, we know the offset 𝑓 𝑖 modulo 64 of each T-table memory line and the T-table location process has been reduced by a factor of 64. Furthermore, we only have to locate one T-table memory line per memory page, since the remaining table occupies the consecutive sets in the last level cache.</p><p>Attack stages: Putting all together, these are the main stages that the we follow to attack AES with S$A -Step 1: Last level cache profile stage: The first stage to perform the attack is to gain knowledge about the structure of the last level cache, the number of slices, and the lines that fill one of the sets in the last level cache. -Step 2: T-table set location stage: The attacker has to know which set in the last level cache each T-table occupies, since these are the sets that need to be primed to obtain the key. -Step 3: Measurement stage: The attacker primes and reprimes the desired sets, requests encryptions and probes again to check whether the monitored sets have been used or not. -Step 4: Key recovery stage: Finally, the attacker utilizes the measurements taken in Step 3 to derive the last round key used by the AES server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENT SETUP AND RESULTS</head><p>In this section we analyze our experiment setup and the results obtained in native machine, single VM and in the cross-VM scenarios. We also include a comparison with previous attacks that were performed in virtualized scenarios targeting AES. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Testbed Setup</head><p>The machine used for all our experiments is a dual core Nehalem Intel i5-650 <ref type="bibr" target="#b42">[43]</ref> clocked at 3.2 GHz. This machine works with 64 byte cache lines and has private 8-way associative L1 and L2 caches of size 2 15 and 2 18 bytes, respectively. In contrast, the 16-way associative L3 cache is shared among all the cores and has a size of 2 22 bytes, divided into two slices. Consequently, the L3 cache will have 2 12 sets in total. Therefore 6 bits are needed to address the byte address in a cache line and 12 more bits to specify the set in the L3 cache. The huge page size is set to 2 MB, which ensures a set field length of 21 bits that are untouched in the virtual to physical address translation stage. All the guest OSs use Ubuntu 12.04, while the VMMs used in our cloud experiments are Xen 4.1 fully virtualized and VMware ESXI 5.5. Both allow the usage of huge size pages by guest OSs <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b44">[45]</ref>.</p><p>The target process is going to use the C reference implementation of OpenSSL1.0.1f, which is the default if the library is configured with no-asm and no-hw options. We would like to remark that these are not the default OpenSSL installation options in most of the products.</p><p>The attack scenario is going to be the same one as in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b10">[10]</ref>, where one process/VM is handling encryption requests with an secret key. As in <ref type="bibr" target="#b10">[10]</ref>, the attacker's process/VM is co-located with the encryption server, but on different cores. We assume synchronization with the server, i.e, the attacker starts the S$A spy process and then sends random plaintexts to the encryption server. The communication between encryption server and attacker is carried out via socket connections. Upon the reception of the ciphertext, the attacker measures the L3 cache usage by the S$A spy process. All measurements are taken by the attackers process/VM with the rdtscp function, which not only reads the time stamp counters but also ensures that all previous processes have finished before its execution <ref type="bibr" target="#b45">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Cross-Core Cross-VM Attack</head><p>We perform the attack in three different scenarios: native machine, single VM and cross-VM. In the native and single VM scenarios, we assume that the huge size pages are set to be used by any non-root process running in the OS. Recall that in the cross-VM scenario, the attacker has administrator rights in his own OS.</p><p>The first step is to recognize the access pattern of the L3 cache in our Intel i5-650. Using S$A we detect that the L3 cache is divided in more than one slice, since generating 17 random lines that occupy the set 0 in the cache does not output higher probe timings. The spy process helps us to understand that the cache is divided into two slices, and that the slice selection method is based on the parity of the 17th bit, i.e, the first non set addressing bit. Thus we need 16 odd lines to fill a set in the odd slice, whereas we need 16 even lines to fill a specific set in the even slice.</p><p>The second step is to recognize the set that each T-table cache line occupies in the L3 cache. For that purpose we monitor each of the possible sets according to the offset obtained from the linux shared library alignment feature. Recall that if the offset modulo 64 𝑓 0 of one of the T-tables is known, we only need check the sets that are 64 positions apart, starting from 𝑓 0 . By sending random plaintexts the set holding a T-table cache line is used around 90% of the times, while around 10% of the times the set will remain unused. The difference between a set allocating a T-table cache line and a set not allocating a T-table cache line can be graphically seen in Figure <ref type="figure" target="#fig_7">5</ref>, where 500 random encryptions were monitored with S$A for both cases in a cross-VM scenario in Xen 4.1. It can be observed that monitoring an unused set results in more stable timings in the range of 200-300 cycles. However monitoring a set used by the T-tables outputs higher time </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spy-Process based Attacks:</head><p>Collision timing <ref type="bibr" target="#b16">[16]</ref> Pentium 4E Time measurement 0.9.8a 1 300.000 Prime+probe <ref type="bibr" target="#b1">[2]</ref> Pentium 4E L1 cache prime-probing 0.9.8a 16.000 Evict+time <ref type="bibr" target="#b1">[2]</ref> Athlon 64 L1 cache evicting 0.9.8a 500.000 Flush+Reload (CFS) 2 <ref type="bibr" target="#b4">[5]</ref> Pentium M Flush+reload w/CFS 0.9.8m 100 Flush+Reload <ref type="bibr" target="#b10">[10]</ref> i5-3320M L3 cache Flush+reload 0.9.8a 8.000 Bernstein <ref type="bibr" target="#b24">[24]</ref> Core2Duo Time measurement 1 OpenSSL 0.9.8a uses a less noisier implementation. 2 The attack is performed taking control of the CFS. 3 Huge Pages have to be configured to allow non-root processes to use them. 4 Only parts of the key were recovered, not the whole key. 5 The attack is only possible if deduplication is enabled by the VMM. Transparent Page Sharing is no longer enabled by default in VMware. Amazon never enabled deduplication on all their AWS servers.</p><p>values around 90% of the time, whereas we still see some lower time values below 300 around 10% of the times. Note that the key used by the AES server is irrelevant in this step, since the set used by the T-table cache lines is going to be independent of the key. The last step is to run S$A to recover the AES key used by the AES server. We consider as valid ciphertexts for the key recovery step those that are at least below half the average of the overall timings. This threshold is based on empirical results that can be seen in Figure <ref type="figure" target="#fig_8">6</ref>. The figure presents the miss counter value for all the possible ciphertext values of 𝐶 0 , when the last line in the corresponding T-table is monitored. The key in this case is 0𝑥𝑒1 and the measurements are taken in a cross-VM scenario in Xen 4.1. In this case only 8 values take low miss counter values because the T-table finishes in the middle of a cache line. These values are clearly distinguishable from the rest and appear in opposite sides of the empirical threshold.</p><p>Results for the three scenarios are presented in Figure <ref type="figure" target="#fig_9">7</ref>, where it can be observed that the noisier the scenario is, e.g. in the cross-VM scenario, the more monitored encryptions are needed to recover the key. The plot shows the number of correctly guessed key bytes vs. the number of encryptions needed. Recall that the maximum number of correctly guessed key bytes is 16 for AES-128. The attack only needs 150.000 encryptions to succeed on recovering the full AES key in the native OS scenario. Due to the higher noise in the cloud setting, the single VM recovers the full key with 250.000 encryptions. The cross-VM scenario was analyzed in two popular hypervisors, Xen and VMware, requiring 650.000 and 500.000 encryptions to recover the 16 key bytes respectively. We believe that Xen requires a higher number of encryptions due to the higher noise caused by the usage of a fully virtualized hypervisor. It is important to remark that the attack is completed in only 9 and 35 seconds, respectively, for the native and single VM scenarios. In the cross VM scenario, the attack succeeds in recovering the full key in 90 and 150 seconds in VMware and Xen, respectively. Recall that in the cross-VM scenario the external IP communication adds significant latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with previous attacks</head><p>We compare the efficiency of the attack presented in this work with previously proposed attacks that targeted the AES. The comparison is presented in Table <ref type="table" target="#tab_2">I</ref>. We make the following observations:</p><p>-Our attack is close to the efficiency achieved by the Flush+Reload attack in non-virtualized environments, and improves over previously proposed attacks. However, huge pages are required to be configured so that their usage by non-root processes is allowed. -Our new S$A attack is more efficient than Bernstein's attack in the cloud, which does not recover the entire key in the cloud even with a significantly higher number of encryptions. -In the cloud, S$A again requires more encryptions than Flush+Reload but not as much as to become impractical.</p><p>The attack can still be realized under 3 minutes in XEN and under 2 in VMware. However, it should be noted that S$A does not take advantage of memory deduplication process which is crucial for the cross-VM Flush+Reload attack. The deduplication feature (called Transparent Page Sharing in VMware) is now disabled by default in VMware <ref type="bibr" target="#b47">[48]</ref>. Moreover, we have also confirmed with Amazon that deduplication was never enabled on all of their AWS servers due to security concerns. Thus, the S$A attack turns VMMs that are not vulnerable to Flush+Reload due to the lack of memory deduplication into a valid target for cross-VM attacks. The only requirement is that guest OSs are allowed to use huge size pages. This feature is implemented at the OS level, and is not administered by the VMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. APPLICABILITY AND COUNTERMEASURES</head><p>In this section we shortly comment on the applicability of this attack beyond the scope of AES software implementations and discuss ways how this attack can be prevented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Applicability of S$A</head><p>As described earlier, the S$A attack is a cross-core cross-VM attack. S$A targets the shared level of cache (typically L3) in a SMP multiprocessor, hence can be used across cores. That is, the afttack works even if victim and spy are running on different cores in the same CPU. Unlike other cross-VM attacks, S$A does not require deduplication of the targeted data. Previous attacks use deduplication to solve two independent problems. The obvious one is the detection of cache accesses to extract secret information of the victim. However, deduplication also solves the location problem, i.e. automates the detection of where the leaking data of the target is stored in cache. In S$A, these two problems become independent. Hence, the attack is more challenging for the adversary, as the location problem needs to be solved before information can be extracted. However, since the extraction mechanism is the same, the S$A is applicable in all scenarios where Flush+Reload can be applied. We claim the S$A attack to be a substitute for the Flush+Reload attack whenever deduplication is not available. The added cost is the location step and a slightly decreased temporal resolution, since the (re-)priming needs to fill and check an entire set, not just a single line of cache. Hence, although this work demonstrates the applicability to AES only, the S$A attack is applicable in all cases where the Flush+Reload can be applied and has been applied. In other words, S$A can be applied to attack the public key cryptosystems targeted in <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b28">[29]</ref>. This also means that focusing on countermeasures for AES is not helpful, since those will not prevent attacks on other crypto schemes also vulnerable to this attack.</p><p>However, the S$A attack succeeds due to two main characteristics: the inclusiveness of the LLC in Intel processors and the usage of huge pages by client VMs. The first characteristic is not fulfilled in AMD processors, i.e, data located in L1 or L2 caches does not have to be present in the LLC. Therefore, when the victim accesses a particular memory block, it does not directly occupy a position in the LLC but resides in the upper level caches first. In this situation, our S$A attack does not detect the usage of the leaking memory block.</p><p>The second characteristic, to the best of our knowledge, is accomplished by most of the well known hypervisors. However, we did not succeed on implementing the attack in Citrix Xen 6.2. The main reason seems to be that neither the guest VMs nor the hypervisor are allowed to utilize huge size pages. This restriction makes our attack impossible to succeed in the LLC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. AES-specific Countermeasures</head><p>Cache-based side channels are not a new phenomenon, hence numerous countermeasures have been proposed. The most obvious one is the use of AES-NI or other AES hardware extensions, if available on the processor. A good discussion of that and several other countermeasures like data independent memory accesses and smaller T-tables can be found in <ref type="bibr" target="#b48">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. S$A-specific Countermeasures</head><p>Next, we discuss countermeasures that hinder the exploitability of the shared level cache and thereby prevent the S$A attack. Disable Huge Size Pages: In the particular case of the S$A cache side channel attack, if huge size pages are not allowed to be used by the guests the attack is no longer possible. The decision of using the huge size pages could still be done only by the VMM, depending on certain parameters based on the length or the memory resources needed by the code. Private L3 Cache Slices: One way to avoid the cache leakage that S$A uses is to make the cache slices private per VM, similar to the countermeasure suggested in <ref type="bibr" target="#b49">[50]</ref>. This means that a particular VM is not allowed to interfere with the cache slice that another co-located VM is using. In this scenario the attacker does not interfere with the victim's cache slice and therefore cannot decide whether a specific memory line was used with S$A. This however, requires modifications to the cache arbitration mechanism and has the adverse affect of reducing the size of the cache slices made available to a single VM. It also limits the number of Guest VMs to the number of slices. Hardware Masking of Addresses: Another possible solution is to apply a mask (implemented at the hardware level) to the offset field based on some of the non-set addressing bits in the physical address when huge size pages are used. Since the user no longer has control over the offset field, he cannot prime the specific set that he wants to target in the L3 cache and cannot decide whether the set was used or not by the victim. Shadow Page Tables as Masking Option: In this case the shadow page tables that VMMs use for a virtual to virtual translation would play a more important role. For instance, the shadow page tables could not only handle the translation from VM virtual memory to VMM virtual memory, but also apply a mask based on the non cache-addressing bits. Thereby, the guest user does not know the masking value applied by the VM, and he cannot control the set that his data will occupy in the L3 cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S$A: A new deduplication free L3 cache side channel technique:</head><p>We proposed a new side channel technique that is applied in the L3 cache and therefore can be applied in cross-core scenarios. The new side channel technique bases its methodology in the usage of huge size pages, which give extra information about the position that each memory location occupies in the L3 cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Targeting the cloud environment:</head><p>We demonstrated that the new side channel technique can also be implemented in the cloud, particularly in Xen 4.1 and VMware ESXI 5.5, where the usage of huge size pages by the guest OSs is allowed. Recall that the vast majority of the VMMs allow the usage of huge size pages, making S$A a suitable target for all of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Applying the attack on AES:</head><p>We demonstrated the viability of the new side channel technique by recovering AES keys monitoring only 4 sets in the L3 cache in both virtualized and non-virtualized scenarios. In the noisier scenario the attack succeeds to recover the full AES key in less than 3 minutes. Thus, we showed that the efficiency of S$A is close to the efficiency achieved by Flush+Reload (which uses memory deduplication techniques) and is significantly higher than Bernstein's attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. DISCLOSURE</head><p>We have disclosed our attack to the security teams of VMware, Amazon AWS and Citrix.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>1 and less than 2 minutes in VMware ESXI 5.5. In summary, this work -introduces a new side channel technique targeting the L3 cache enabled by the use of huge size memory pages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Cache accesses when it is physically addressed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Regular Page (4 KB, top) and Huge Page (256 KB, bottom) virtual to physical address mapping for an Intel x86 processor. For Huge pages the entire L3 cache sets become transparently accessible even with virtual addressing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Prime and reprime stages to ensure we monitor the last level cache.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Histograms of 500 access times monitored in the probe stage for a) a set used by a T-table memory line and b) a set not used by a T-able memory line. Measurements are taken in the Xen 4.1 cross-VM scenario.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Miss counter values for ciphertext 0 normalized to the maximum value. The key is e1 and we are monitoring the last 8 values of the T-table (since the table starts in the middle of a memory line).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Number of key bytes correctly recovered vs number of encryptions needed for native OS, single VM and cross-VM scenarios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF CROSS-VM CACHE SIDE-CHANNEL ATTACKS ON AES</figDesc><table><row><cell>Attack</cell><cell>Platform</cell><cell>Methodology</cell><cell>OpenSSL</cell><cell>Traces</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work is supported by the National Science Foundation, under grant CNS-1318919. We would like to thank the anonymous reviewers of S&amp;P 2015 for their helpful comments. We would also like to thank Craig Shue for his help on understanding huge page allocation procedures.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Cache-timing attacks on AES</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Bernstein</surname></persName>
		</author>
		<ptr target="http://cr.yp.to/papers.html#cachetiming" />
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cache Attacks and Countermeasures: The Case of AES</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Osvik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tromer</surname></persName>
		</author>
		<idno type="DOI">10.1007/11605805_1</idno>
		<ptr target="http://dx.doi.org/10.1007/11605805_1" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 The Cryptographers&apos; Track at the RSA Conference on Topics in Cryptology, ser. CT-RSA&apos;06</title>
		<meeting>the 2006 The Cryptographers&apos; Track at the RSA Conference on Topics in Cryptology, ser. CT-RSA&apos;06<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Increasing memory density by using KSM</title>
		<author>
			<persName><forename type="first">A</forename><surname>Arcangeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Eidus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the linux symposium</title>
		<meeting>the linux symposium</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="19" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Kernel Samepage Merging</title>
		<ptr target="http://kernelnewbies.org/Linux_2_6" />
		<imprint>
			<date type="published" when="2014-04">April 2014</date>
		</imprint>
	</monogr>
	<note>d3f32e41df508090810388a57efce73f52660ccb/</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cache Games -Bringing Access-Based Cache Attacks on AES to Practice</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gullasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bangerter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krenn</surname></persName>
		</author>
		<idno type="DOI">10.1109/SP.2011.22</idno>
		<ptr target="http://dx.doi.org/10.1109/SP.2011.22" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE Symposium on Security and Privacy, ser. SP &apos;11</title>
		<meeting>the 2011 IEEE Symposium on Security and Privacy, ser. SP &apos;11<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="490" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hey, you, get off of my cloud: Exploring information leakage in third-party compute clouds</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ristenpart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tromer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shacham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savage</surname></persName>
		</author>
		<idno type="DOI">10.1145/1653662.1653687</idno>
		<ptr target="http://doi.acm.org/10.1145/1653662.1653687" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM Conference on Computer and Communications Security, ser. CCS &apos;09</title>
		<meeting>the 16th ACM Conference on Computer and Communications Security, ser. CCS &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="199" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cross-VM side channels and their use to extract private keys</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Juels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ristenpart</surname></persName>
		</author>
		<idno type="DOI">10.1145/2382196.2382230</idno>
		<ptr target="http://doi.acm.org/10.1145/2382196.2382230" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 ACM Conference on Computer and Communications Security, ser. CCS &apos;12</title>
		<meeting>the 2012 ACM Conference on Computer and Communications Security, ser. CCS &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="305" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Cache Timing Attack on AES in Virtualization Environments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Heinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Stumpf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International Conference on Financial Cryptography and Data Security</title>
		<title level="s">ser. Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>Financial Crypto 2012</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">FLUSH+RELOAD: A High Resolution, Low Noise, L3 Cache Side-Channel Attack</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yarom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Falkner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd USENIX Security Symposium</title>
		<imprint>
			<publisher>USENIX Security</publisher>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<ptr target="https://www.usenix.org/conference/usenixsecurity14/technical-sessions/presentation/yarom" />
		<title level="m">USENIX Association</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08">Aug. 2014</date>
			<biblScope unit="page" from="719" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Wait a Minute! A fast, Cross-VM Attack on AES</title>
		<author>
			<persName><forename type="first">G</forename><surname>Irazoqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Inci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eisenbarth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sunar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="299" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fine grain cross-vm attacks on xen and vmware</title>
		<idno type="DOI">10.1109/BDCloud.2014.102</idno>
		<ptr target="http://dx.doi.org/10.1109/BDCloud.2014.102" />
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Fourth International Conference on Big Data and Cloud Computing</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-03">2014. December 3-5, 2014, 2014</date>
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lattice Scheduling and Covert Channels</title>
		<author>
			<persName><forename type="first">W.-M</forename><surname>Hu</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=882488.884165" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1992 IEEE Symposium on Security and Privacy, ser. SP &apos;92</title>
		<meeting>the 1992 IEEE Symposium on Security and Privacy, ser. SP &apos;92<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page">52</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Side Channel Cryptanalysis of Product Ciphers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kelsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schneier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hall</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=1297828.1297833" />
	</analytic>
	<monogr>
		<title level="j">J. Comput. Secur</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="141" to="158" />
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Theoretical Use of Cache Memory as a Cryptanalytic Side-Channel</title>
		<author>
			<persName><forename type="first">D</forename><surname>Page</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cryptanalysis of DES implemented on computers with cache</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsunoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Suzaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shigeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHES 2003, Springer LNCS</title>
		<meeting>of CHES 2003, Springer LNCS</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="62" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust Final-Round Cache-Trace Attacks Against AES</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bonneau</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/journals/iacr/iacr2006.html#Bonneau06" />
	</analytic>
	<monogr>
		<title level="m">IACR Cryptology ePrint Archive</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page">374</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cache Based Remote Timing Attack on the AES</title>
		<author>
			<persName><forename type="first">O</forename><surname>Aciiçmez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Çetin</forename><forename type="middle">K</forename><surname>Koç</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Topics in Cryptology CT-RSA 2007, The Cryptographers Track at the RSA Conference</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="271" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Advances on Access-Driven Cache Attacks on AES</title>
		<author>
			<persName><forename type="first">M</forename><surname>Neve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Seifert</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-74462-7_11</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-540-74462-7_11" />
	</analytic>
	<monogr>
		<title level="m">Selected Areas in Cryptography, ser</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Biham</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Youssef</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4356</biblScope>
			<biblScope unit="page" from="147" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Yet Another MicroArchitectural Attack:: Exploiting I-Cache</title>
		<author>
			<persName><forename type="first">O</forename><surname>Aciiçmez</surname></persName>
		</author>
		<idno type="DOI">10.1145/1314466.1314469</idno>
		<ptr target="http://doi.acm.org/10.1145/1314466.1314469" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 ACM Workshop on Computer Security Architecture, ser. CSAW &apos;07</title>
		<meeting>the 2007 ACM Workshop on Computer Security Architecture, ser. CSAW &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Amazon Web Services</title>
		<ptr target="http://aws.amazon.com/es/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detecting Co-residency with Active Traffic Analysis Techniques</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pletcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pruse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valafar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Butler</surname></persName>
		</author>
		<idno type="DOI">10.1145/2381913.2381915</idno>
		<ptr target="http://doi.acm.org/10.1145/2381913.2381915" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 ACM Workshop on Cloud Computing Security Workshop, ser. CCSW &apos;12</title>
		<meeting>the 2012 ACM Workshop on Cloud Computing Security Workshop, ser. CCSW &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">HomeAlone: Co-residency Detection in the Cloud via Side-Channel Analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Juels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Reiter</surname></persName>
		</author>
		<idno type="DOI">10.1109/SP.2011.31</idno>
		<ptr target="http://dx.doi.org/10.1109/SP.2011.31" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE Symposium on Security and Privacy, ser. SP &apos;11</title>
		<meeting>the 2011 IEEE Symposium on Security and Privacy, ser. SP &apos;11<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="313" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An Improved Trace Driven Instruction Cache Timing Attack on RSA</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">X</forename><surname>-C. Chen Cai-Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ping</surname></persName>
		</author>
		<ptr target="http://eprint.iacr.org/" />
	</analytic>
	<monogr>
		<title level="j">Cryptology ePrint Archive</title>
		<imprint>
			<date type="published" when="2011">2011/557, 2011</date>
		</imprint>
	</monogr>
	<note type="report_type">Report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attacking AES Using Bernstein&apos;s Attack on Modern Processors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elgayyar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AFRICACRYPT</title>
		<imprint>
			<biblScope unit="page" from="127" to="139" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the Applicability of Time-Driven Cache Attacks on Mobile Devices</title>
		<author>
			<persName><forename type="first">R</forename><surname>Spreitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Plos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Network and System Security -NSS 2013, 7th International Conference</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Javier Lopez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xinyi</forename><surname>Huang</surname></persName>
		</editor>
		<meeting><address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">June 3-4, 2013. 2013</date>
			<biblScope unit="volume">7873</biblScope>
			<biblScope unit="page" from="656" to="662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">New Results on Instruction Cache Attacks</title>
		<author>
			<persName><forename type="first">O</forename><surname>Aciiçmez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Brumley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grabher</surname></persName>
		</author>
		<ptr target="https://www.kernel.org/doc/Documentation/scheduler/sched-design-CFS.txt" />
	</analytic>
	<monogr>
		<title level="m">CHES, ser</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Mangard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F.-X</forename><surname>Standaert</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010-04">2010. April 2014</date>
			<biblScope unit="volume">6225</biblScope>
			<biblScope unit="page" from="110" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Are AES x86 Cache Timing Attacks Still Feasible?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mowery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Keelveedhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shacham</surname></persName>
		</author>
		<idno type="DOI">10.1145/2381913.2381917</idno>
		<ptr target="http://doi.acm.org/10.1145/2381913.2381917" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 ACM Workshop on Cloud Computing Security Workshop, ser. CCSW &apos;12</title>
		<meeting>the 2012 ACM Workshop on Cloud Computing Security Workshop, ser. CCSW &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Just a Little Bit&quot;: A Small Amount of Side Channel Can Go a Long Way</title>
		<author>
			<persName><forename type="first">N</forename><surname>Benger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van De Pol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Smart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yarom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ooh Aah</title>
		<imprint>
			<biblScope unit="page" from="75" to="92" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>CHES</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cross-tenant side-channel attacks in paas clouds</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Juels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ristenpart</surname></persName>
		</author>
		<idno type="DOI">10.1145/2660267.2660356</idno>
		<ptr target="http://doi.acm.org/10.1145/2660267.2660356" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications Security, ser. CCS &apos;14</title>
		<meeting>the 2014 ACM SIGSAC Conference on Computer and Communications Security, ser. CCS &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="990" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Last level cache side channel attacks are practical</title>
		<author>
			<persName><forename type="first">Fangfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Yarom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gernot</forename><surname>Heiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruby</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">36th IEEE Symposium on Security and Privacy (S&amp;P 2015)</title>
		<meeting><address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hennessy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Architecture</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
	<note>Fifth Edition: A Quantitative Approach, 5th ed</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Managing Distributed, Shared L2 Caches Through OS-Level Page Allocation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO.2006.31</idno>
		<ptr target="http://dx.doi.org/10.1109/MICRO.2006.31" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual IEEE/ACM International Symposium on Microarchitecture, ser. MICRO 39</title>
		<meeting>the 39th Annual IEEE/ACM International Symposium on Microarchitecture, ser. MICRO 39<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="455" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">How to Use Huge Pages to Improve Application Performance on Intelő Xeon Phi Ź Coprocessor</title>
		<ptr target="https://software.intel.com/sites/default/files/Large_pages_mic_0.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Using 4KB Page Size for Virtual Memory is Obsolete</title>
		<author>
			<persName><forename type="first">P</forename><surname>Weisberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wiseman</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=1689250.1689298" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th IEEE International Conference on Information Reuse &amp; Integration, ser. IRI&apos;09</title>
		<meeting>the 10th IEEE International Conference on Information Reuse &amp; Integration, ser. IRI&apos;09<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="262" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">VMware Large Page performance</title>
		<ptr target="http://www.vmware.com/files/pdf/large_pg_performance.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Huge Page Configuration in KVM</title>
		<ptr target="http://www-01.ibm.com/support/knowledgecenter/linuxonibm/liaat/liaattunconfighp.htm?lang=en" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">X-XEN : Huge Page Support in Xen</title>
		<ptr target="https://www.kernel.org/doc/ols/2011/ols2011-gadre.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Advanced Encryption Standard</title>
		<ptr target="http://csrc.nist.gov/publications/fips/fips197/fips-197.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Practical timing side channel attacks against kernel space aslr</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Holz</surname></persName>
		</author>
		<idno type="DOI">10.1109/SP.2013.23</idno>
		<ptr target="http://dx.doi.org/10.1109/SP.2013.23" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE Symposium on Security and Privacy, ser. SP &apos;13</title>
		<meeting>the 2013 IEEE Symposium on Security and Privacy, ser. SP &apos;13<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="191" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Towards Hybrid Last Level Caches for Chip-multiprocessors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Upton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Newell</surname></persName>
		</author>
		<idno type="DOI">10.1145/1399972.1399982</idno>
		<ptr target="http://doi.acm.org/10.1145/1399972.1399982" />
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="56" to="63" />
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<ptr target="http://blog.stuffedcow.net/2013/01/ivb-cache-replacement/" />
		<title level="m">Intel Ivy Bridge Cache Replacement Policy</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<ptr target="http://ark.intel.com/es/products/43546/Intel-Core-i5-650-Processor-4M-Cache-3_20-GHz" />
		<title level="m">Intelő Core Ź i5-650 Processor</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Xen and the art of virtualization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dragovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Neugebauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Warfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="164" to="177" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Xen 4.1 Release Notes</title>
		<ptr target="http://wiki.xen.org/wiki/Xen_4.1_Release_Notes" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">How to Benchmark Code Execution Times on Intelő IA-32 and IA-64 Instruction Set Architectures</title>
		<ptr target="http://www.intel.com/content/dam/www/public/us/en/documents/white-papers/ia-32-ia-64-benchmark-code-execution-paper.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fine grain crossvm attacks on xen and vmware are possible</title>
		<author>
			<persName><forename type="first">G</forename><surname>Irazoqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Inci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eisenbarth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sunar</surname></persName>
		</author>
		<ptr target="http://eprint.iacr.org/" />
	</analytic>
	<monogr>
		<title level="j">Cryptology ePrint Archive</title>
		<imprint>
			<date type="published" when="2014">2014/248, 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">Report</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Transparent Page Sharing: new default setting</title>
		<ptr target="http://blogs.vmware.com/security/2014/10/transparent-page-sharing-additional-management-capabilities-new-default-settings.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Efficient Cache Attacks on AES, and Countermeasures</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tromer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Osvik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00145-009-9049-y</idno>
		<ptr target="http://dx.doi.org/10.1007/s00145-009-9049-y" />
	</analytic>
	<monogr>
		<title level="j">Journal of Cryptology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="71" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">New Cache Designs for Thwarting Software Cache-based Side Channel Attacks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1145/1250662.1250723</idno>
		<ptr target="http://doi.acm.org/10.1145/1250662.1250723" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th Annual International Symposium on Computer Architecture, ser. ISCA &apos;07</title>
		<meeting>the 34th Annual International Symposium on Computer Architecture, ser. ISCA &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="494" to="505" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
