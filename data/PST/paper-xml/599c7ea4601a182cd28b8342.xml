<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Split Cache Hierarchy for Enabling Data-oriented Optimizations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Sembrant</surname></persName>
							<email>andreas.sembrant@it.uu.se</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Technology</orgName>
								<orgName type="institution">Uppsala University</orgName>
								<address>
									<postBox>P.O. Box 337</postBox>
									<postCode>SE-751 05</postCode>
									<settlement>Uppsala</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Erik</forename><surname>Hagersten</surname></persName>
							<email>erik.hagersten@it.uu.se</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Technology</orgName>
								<orgName type="institution">Uppsala University</orgName>
								<address>
									<postBox>P.O. Box 337</postBox>
									<postCode>SE-751 05</postCode>
									<settlement>Uppsala</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Black-Schaffer</surname></persName>
							<email>david.black-schaffer@it.uu.se</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Technology</orgName>
								<orgName type="institution">Uppsala University</orgName>
								<address>
									<postBox>P.O. Box 337</postBox>
									<postCode>SE-751 05</postCode>
									<settlement>Uppsala</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Split Cache Hierarchy for Enabling Data-oriented Optimizations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/HPCA.2017.25</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Today's caches tightly couple data with metadata (Address Tags) at the cache line granularity. The co-location of data and its identifying metadata means that they require multiple approaches to locate data (associative way searches and level-by-level searches), evict data (coherent writebacks buffers and associative level-by-level searches) and keep data coherent (directory indirections and associative level-by-level searches). This results in complex implementations with many corner cases, increased latency and energy, and limited flexibility for data optimizations.</p><p>We propose splitting the metadata and data into two separate structures: a metadata hierarchy and a data hierarchy. The metadata hierarchy tracks the location of the data in the data hierarchy. This allows us to easily apply many different optimizations to the data hierarchy, including smart data placement, dynamic coherence, and direct accesses.</p><p>The new split cache hierarchy, Direct-to-Master (D2M), provides a unified mechanism for cache searching, eviction, and coherence, that eliminates level-by-level data movement and searches, associative cache address tags comparisons and about 90% of the indirections through a central directory. Optimizations such as moving LLC slices to the near-side of the network and private/shared data classification can easily be built on top off D2M to further improve its efficiency. This approach delivers a 54% improvement in cache hierarchy EDP vs. a mobile processor and 40% vs. a server processor, reduces network traffic by an average of 70%, reduces the L1 miss latency by 30% and is especially effective for workloads with high cache pressure.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Today's cache hierarchies rely on many associative searches to locate data and ensure coherency: Within a node's private hierarchy outward searches look through tags and levels; At the shared level an associative search looks in the shared cache and directory; If coherence is required, an inward search proceeds down into the other nodes' hierarchies to find data and update coherence information. These search processes are a result of the explicit coupling of data and metadata (tags, sharing, state) in today's designs. This coupling forces the processor to look through layers of metadata to find the desired data, and incurs significant costs in energy and latency, while also limiting design flexibility.</p><p>In this work, we propose a cache hierarchy that splits data and metadata into two separate hierarchies. The metadata hierarchy tracks where data is located and how it is shared, and is implemented as an inclusive, coherent, and deterministic hierarchy with replacement policies tailored to the behavior of the metadata. This allows the data hierarchy to use a series of simple memory arrays, which need not enforce any particular inclusivity properties or even form an explicit hierarchy.</p><p>To separate the metadata and data hierarchies, we build upon the Direct-to-Data (D2D) <ref type="bibr" target="#b0">[1]</ref> cache hierarchy, which replaces TLB lookups with access to location tracking information to identify the location (level and way) of the data in the private cache hierarchy. This work goes beyond the D2D design by 1) extending it to support coherent multicore processors, and, 2) generalizing it to develop an explicit separation of metadata and data hierarchies.</p><p>Our separate metadata hierarchy simplifies optimizations that build upon, or modify, data properties, behavior, or placement, as these are now tracked and controlled by a simpler, purpose-built metadata hierarchy. This enables many previously proposed cache optimizations under one common framework. These include:</p><p>? Direct data access <ref type="bibr" target="#b0">[1]</ref> to lower latency by using the metadata information to skip level searches, way searches and directory indirections. (Guaranteeing determinism for metadata and extending it to support inter-node and shared level accesses for coherent multicore systems.) ? Data placement and replication to lower latency and reduce traffic by keeping data closer to where it is needed <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. (Leverage the metadata hierarchy's decoupling of cacheline addresses from placement, skipped directory indirections and providing an efficient mechanism to locate data.) ? Dynamic coherence to reduce coherence traffic through private/shared data classification <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>.</p><p>(Using classification from the sharing information in the metadata.) ? Cache bypassing to improve performance and energy by only installing a cacheline if it is likely to see reuse <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. (The metadata provides the functionality needed to bypassed some data while retaining the benefits of inclusion for other data.) ? Dynamic indexing to reduce conflict misses by changing the cache index functions <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b8">[9]</ref>. (Using the metadata to store the "scrambling" value for the data.)</p><p>We begin with an overview of existing support for direct access to data in private cache hierarchies (Section II) and how we extend and generalize it to support coherent shared caches and a general split metadata/data hierarchy (Section III). We then demonstrate how we can use the split cache hierarchy by exploring optimizations for dynamic coherence, data placement, cooperative caching, and dynamic indexing (Section IV), and evaluate the overall effectiveness of the design (Section V). An overview of the coherence protocol is provided as an appendix.</p><p>The key contributions of this work are:</p><p>? A new cache hierarchy that splits metadata and data.</p><p>? The removal of directory indirections for all coherent reads as well as writes to private data. ? Demonstration of how multiple data-oriented optimizations can be integrated with a split hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. A SPLIT CACHE HIERARCHY</head><p>The goal of this work is to show how splitting metadata and data provides an efficient framework for accessing data in a coherent multicore setting that supports a wide range of existing and future cache optimizations. As a first step, we extend the energy-efficient, but single-core only, Direct-to-Data (D2D) <ref type="bibr" target="#b0">[1]</ref> cache framework to support multiple cores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Private Hierarchies: Direct-to-Data (D2D)</head><p>The Direct-to-Data (D2D) design showed how private cache hierarchies could be made more efficient by replacing level-by-level searches with a single associative lookup that returns the cacheline's location. We generalize D2D as a hierarchy of metadata stores (MD1, MD2, etc.) which store cacheline Location Information (LI) separate from the data stores. With the Location Information from a metadata store, we can directly access the correct cache level and way for the cacheline, thereby avoiding the need for level-by-level searches, and improving efficiency.</p><p>To reduce overhead, the metadata information (MD) is organized into regions, each of which stores the Location Information (LI) for a number of adjacent cachelines, together with an address tag to identify the region. In D2D, each region contains 5 bits of Location Information per cacheline, which encodes the level and way (2-levels with 8-ways each, or memory). This encoding results in an implementation cost that is on par with the TLB and address tags it replaces, but still manages to deliver very effective tracking across the hierarchy: 99.7%, 87.2%, and 75.6% of L1, L2, and memory hits are tracked in the first level metadata (MD1), for a combined coverage of 98.8% of all memory accesses.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows a D2D system with two metadata stores (MD1 and MD2). MD1 is virtually tagged, and can therefore provide the location of cachelines ( A in L1, B in L2, C and D in memory) without the need for a separate TLB translation. MD2 is a larger backing store for MD1, and is physically tagged, thereby requiring a TLB2 lookup, which allows transparent handling of large pages. Across the MD levels only one entry can be active at a given time, to avoid having to update multiple LIs atomically. To identify the active MD1 entry, each MD2 entry has a tracking pointer (TP, E ) which points to the active MD1 entry if it is not itself active <ref type="foot" target="#foot_0">1</ref> .</p><p>Since there is no way to search through the cacheline data (cachelines no longer have tags and can only be found through the MD entries), D2D implements inclusion between MD2 and the lower-level structures MD1, L1 and L2 (and also with L1-I, not shown in Figure <ref type="figure" target="#fig_0">1</ref>). A consequence of this is that when an MD2 entry (region) is evicted, all its tracked cacheline residing in any of these lowerlevel structures must be evicted, or there would be no way to locate them again. As a result, the MD has precise information: if it claims that the cacheline is in a certain way in a given level, it will indeed be found there. We refer to this guarantee as deterministic information. To limit the effect of the forced eviction, the number of cachelines tracked in the MD2 is larger than the size of the L2 (typically by a factor four) and the replacement policy can favor choosing regions with few cachelines present.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Towards Shared Hierarchies</head><p>The goal of this work is to retain D2D's efficient direct access to data in a coherent/shared multicore setting. To do so, we rely on two key invariants:</p><p>1. Deterministic Location Information: the data location pointed to by the metadata is guaranteed to contain valid data when the node accesses it. This guarantee means that as long as a node can access the metadata it can directly access the data, thereby enabling efficient direct access from the nodes without having to consult a centralized directory.</p><p>2. Per-Region Private Classification: when a region's private bit is set in a node, it is guaranteed that other nodes can neither make direct reads to the region data nor have private cached copies of its data. The Private Bits inform nodes as to when they can not make direct write accesses.</p><p>Taken together, these properties enable direct access for all reads (both to private and non-private regions) and all writes to private regions, regardless of where data is located in the hierarchy. This only leaves writes to shared regions not covered by direct accesses. The challenge is to ensure that the Location Information remains deterministic by ensuring that the metadata is updated with care.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DIRECT-TO-MASTER (D2M)</head><p>A generic D2M configuration for up to eight nodes is shown in Figure <ref type="figure">2</ref>. Each node contains a core (not shown), separate 8-way instruction and data caches (only L1-D is shown<ref type="foot" target="#foot_1">2</ref> ), a unified 8-way L2, and 8-way first-level (MD1, virtually tagged) and second-level (MD2, physically tagged) metadata stores. All nodes are connected to each other and with the 32-way last-level cache and the metadata MD3 through the on-chip interconnect. D2M can also be applied to architectures with different numbers of levels and nodes.</p><p>As with traditional coherent cache designs, there is a single master location for each cacheline at any point in time, which is assigned to reply to coherent read requests (for example, cache lines in coherence states M, O, E or F). The master location may be a cache location, a remote node or memory. Valid cachelines that are not masters are referred to as replicated cachelines (e.g., in state S).</p><p>Another invariant maintained by D2M is Metadata Inclusion: as with D2D, we require that all data in the cache hierarchy be tracked in the metadata hierarchy, and, in particular, that data stored within a node must be tracked by metadata within that node. Each node in Figure <ref type="figure">2</ref> enforces inclusion between its MD2 and its MD1, L1, and L2. In order to not lose track of data, inclusion is also enforced between MD3 and the MD2s and LLC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Extending Metadata Entries for Coherency</head><p>In order to be able to use the MD information directly in a coherent setting, and thus avoid a costly directory lookup, D2M needs the following extensions over D2D:</p><p>1. Track data in the (globally shared) LLC 2. Track master data in remote nodes 3. Track the set of sharing nodes (for coherent writes) 4. Classify regions 5. Handle eviction of shared data The first two tasks are achieved by extending the location information encoding to cover four cases: 1) in a local cache level ("Level = 1 or 2"), 2) in the LLC ("Level = 3"), 3) in a specific remote Node ID ("NodeID"), or, 4) in memory ("MEM"). Compared with the 5 bit encoding for location information used in D2D, D2M needs one additional bit to add encodings for remote nodes. A possible encoding is shown in Table <ref type="table">I</ref>. The 6 location information bits per cacheline is substantially smaller than a typical address tag (? 30 bits).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>000NNN</head><p>In NodeID: NNN 001WWW</p><p>In L1, way=WWW 010WWW</p><p>In L2, way=WWW 011SSS Encoding of eight "symbols", where "MEM" is one of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1WWWWW</head><p>In LLC, way=WWWWW Note that D2M's location information identifies the exact location for data in its own L1 and L2 caches, and the LLC (level+way), but master cachelines in remote nodes are tracked only by their NodeID. This allows nodes to move their cachelines between their L1 and L2 without having to update metadata in other nodes. If a metadata lookup for a read access determines that a remote NodeID is the master location, a read request is sent directly to that node. The remote master node will perform a lookup in its MD2 (and possibly access its MD1 if it is active for this cacheline) in order to determine the location of the master cacheline.</p><p>The third task (tracking the set of sharing nodes) is handled by extending each region in MD3 with Presence Bits (PB bits), where each PB bit corresponds to one node. A PB bit is set if the corresponding node has a valid MD2 entry. This implies that the node may have a valid copy of any cacheline in this region. If the bit is not set, it is guaranteed (thanks to MD2 inclusion) that the node does not have any valid cachelines for this region.</p><p>The presence bits (PB bits) steer the multicast scope of messages. For example, invalidations need only be sent to the nodes whose PB bit is set. Each of these "PB nodes" will perform a lookup in their MDs and update the location information to point to the NodeID of the requesting node and also invalidate any local cached copy of the data. The presence bits allow us to trivially classify regions as private or shared (Table <ref type="table">II</ref>), and set the regions' private bits (P in Figure <ref type="figure">2</ref>) in MD1/MD2</p><p>Note that the PB bits are recorded per region, while the location information tracks cachelines individually. The PB bits therefore provide a course-grain representation of the set of sharing nodes. This may lead to invalidation messages being sent to nodes that have never cached the corresponding cacheline because they have cached other lines in the region, and, therefore, have an MD2 entry for that region. This can increase traffic and MD2 lookups, but the simple private/shared classification enabled by the PB bits allows us to implement several key optimizations, leading to a large overall reduction in network traffic. For tracking 16 cachelines in a region across 8 nodes, the number of bits needed (PB(8) + 16*LI( <ref type="formula">6</ref>)) is on-par with a traditional fully mapped directory <ref type="bibr">(16 * 9)</ref>.</p><p>While the D2M architecture discussed in this paper have tag-less caches at all cache levels, it is possible to design a D2M system that interfaces traditional caches at some levels, e.g., unmodified cores with traditional TLBs and L1 caches, and traditional coherence interfaces (e.g., ARM's ACE interface) while achieving most of the reported D2M advantages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Extending Replacement Metadata</head><p>One way to solve the fifth task (tracking evictions) is to add a Replacement Pointer (RP) to each cacheline. The RP identifies the victim location (a place in a higher-level cache, or in memory) that will become the master location for a cacheline when it is evicted. The victim location is determined prior to eviction and is identified by the RP (6 bits, using the same encoding as MD). All master cachelines must have a valid RP at all times, and by default that location is simply set to memory. Upon eviction of a master cacheline, the victim location identified by its RP automatically becomes the new master location for the cacheline, thereby significantly simplifying evictions.</p><p>To see how the Replacement Pointer simplifies evictions, consider the cacheline ( A 2 ) in L2 of Figure <ref type="figure">2</ref>. This is a clean cacheline tracked by a private region in MD1. Its Replacement Pointer (RP) points to a victim location in the LLC ( D ). To evict the cacheline, we need to update the location information in MD1 to point to this victim location. This requires first finding its active MD entry. To do so, we use the cacheline's tracking pointer (TP) to locate its MD2 entry ( B ) and then follow the MD2 entry's TP to the active MD1 entry ( C ). We can now update the Location Information in the MD1 entry to point to the victim location from the cacheline's Replacement Pointer ( D ). After this update, the L2 cacheline location can be reused. Note that in this process we never had to search for an eviction location or wait for other lines to be evicted before evicting our line.</p><p>If the cacheline in L2 had been dirty, the dirty data would have been copied to the victim location using the RP pointer before updating the Location Information (LI) in the active MD (case E in Appendix).</p><p>Replicated (i.e., non-master) cachelines in L2 can be replaced silently. Their RPs do not point to a victim location, but instead contain the location of the master cacheline, which may be in the LLC, memory, or another node. On replacement, the same steps are performed, however, the updated LI will point to the master location recorded in RP, and not to a victim location. L1 cachelines may have victim locations allocated for them in L2 (see the replication optimization in Section IV) and are handled similarly to the example above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Unified Data and Metadata Coherence Protocol</head><p>To support deterministic location information, and thereby enable cores to use their local metadata information to access data without a directory access, we need to keep global information in the metadata hierarchy coherent. This implies that changes to master cacheline locations (seen by all sharers) or to the PBs (which tell sharers who else may have a copy) may require coherent updates to metadata. Fortunately, several of the metadata coherence updates that are required can be handled as part of the "normal" data coherence protocol (e.g., per-cacheline state changes) or completely avoided by private region classifications. An overview of the coherence protocol is presented in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DATA-ORIENTED OPTIMIZATIONS</head><p>The metadata hierarchy's ability to specify arbitrary locations for data placement and efficiently locate data opens up some nearly trivial, but very important optimization: dynamic coherence by classifying regions as (private/shared), smart data placement by moving slices of the LLC "adjacent" to the node, and cooperative caching by choosing to move or replicate data on a per-cacheline basis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dynamic Coherence</head><p>By looking at the PB bits we can determine which other nodes may have cached copies of a cachelines in each region, which provides region-level data classification for free. We can classify regions into four types, as shown in Table <ref type="table">II</ref>.</p><p>Private (80% of data <ref type="bibr" target="#b7">[8]</ref>) and untracked regions allow for important optimizations. For private regions, we can completely avoid metadata and data coherence with respect to other nodes and allow for silent upgrades for writes. Any movements of a master cacheline (e.g., evictions) of a private region need only be recorded in the local MD1/MD2. When a region transitions from private to shared upon a first access from another node, the metadata information in the local MD2 is used to create an up-to-date MD3 entry. The region gets marked as not-private in MD1/MD2 (clear the P bit). These transitions are handled by the coherence protocol, as discussed in the Appendix.</p><p>Untracked regions can be evicted from LLC to memory without any metadata coherence updates, since they are only tracked in MD3. Depending on the MD2 and LLC sizes, most regions may become untracked before their cachelines are evicted from LLC.</p><p>The relatively large MD2 may limit the amount of data classified as private, since a region may remain in MD2 long after the node stops using a its data. A MD2 pruning heuristic may improve this situation by proactively evicting MD2 entries to make their region private or untracked. In the evaluation we use a very simplistic pruning heuristic that removes MD2 entries if a node receives an invalidate for a region for which its MD2 entry has no private L1/L2 copies and the TP pointer shows the MD1 entry is not active.</p><p>D2M implements data coherence, metadata determinism, region classification and dynamic coherence optimization (base on region classification) with one unified mechanism. In our evaluation, 68% of the accesses missing in the node's private caches are to private data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Placement</head><p>Typically a request from a core has to first traverse the on-chip interconnect to reach the LLC. Such a far-side LLC is shown in Figure <ref type="figure">2</ref>, while Figure <ref type="figure" target="#fig_1">3</ref> shows a near-side LLC (NS-LLC), where a slice of the LLC is co-located with each node. In this case each slice is a 4-way structure (for a total of 32-ways for all 8 nodes). The NS-LLC has the potential to reduce energy and latency if local data can be preferentially placed in its local slice and we can avoid associative searches across the full LLC. However, as with all NUCAlike arrangements, there are three challenges <ref type="bibr" target="#b4">[5]</ref>: decoupling data placement and addressing, efficiently locating data, and choosing a placement policy. In this work we demonstrate how D2M elegantly addresses the first two of these, but we only explore extremely simple policies <ref type="foot" target="#foot_2">3</ref> .</p><p>Handling the NUCA properties of a NS-LLC simply requires reinterpreting the LI encoding for LLC data (Table <ref type="table">I</ref>) as 1NNNWW, where NNN identifies the node where the NS-LLC resides and WW identifies its way information. With this change, the MD1 of Node-1 can track data in its own NS-LLC slice ( N ) and in remote slices (Node-2, F , in Figure <ref type="figure" target="#fig_1">3</ref>). Both can be accessed directly and efficiently using MD1 location information. Accesses to the remote NS-LLC require interconnect traffic, adding extra latency and energy equal to those of a regular access for a farside LLC. The latency and energy for accessing a local NS-LLC, however, is much lower. Note that there is no inclusion enforced between a node's MD2 and its near-side LLC.</p><p>If the NS-LLC placement policy can promote local NS-LLC accesses over remote NS-LLC accesses, we can reduce interconnect traffic, energy, and average latency compared to a traditional far-side LLC. In the evaluation section we use a very simplistic heuristic based on NS-LLC cache pressure. For this simple policy, the local cache pressure is simply the number of replacements in each NS-LLC every 10k cycles, and is periodically shared with the other NS-LLCs. If the cache pressure for the local NS-LLC is lower than the other NS-LLCs, then the node allocates data in its local NS-LLC. (There would be no point in using the other NS-LLCs, since the survival time for data would be lower due to the higher pressure.) However, if the local NS-LLC has a higher pressure, 80% of the allocations are made locally and 20% are made remotely. In spite of this simple heuristic, our evaluation (Section V) shows that 58% of NS-LLC data accesses are to the local NS-LLC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Cooperative Caching</head><p>In Figure <ref type="figure" target="#fig_1">3</ref>, the NS-LLC slices (with 1/N the LLC size) take the place of the L2 caches in Figure <ref type="figure">2</ref>. Yet our basic NS-LLC cannot replicate data (such as code) across each node's NS-LLC slice for lower latency, as a private L2</p><formula xml:id="formula_0">a) Base-2L b) Base-3L c) D2M-NS-R LLC LLC L2 L2 NS-LLC NS-LLC L1 L1 L1 L1 L1 L1 L1 L1 L1 L1 L1 L1 C 0 C 3 C 0 C 3 C 0 C 3 MD1 TLB TLB MD3 DIR DIR Figure 4.</formula><p>Processor configurations. Base-2L and D2M-NS-R have similar implementation costs while the cost of Base-3L is substantially higher due to its large L2 caches.</p><p>would. However, as already demonstrated, D2M is capable of handling replicated of data (e.g., the L2 cacheline in Figure <ref type="figure">2</ref> may also be replicated in some of the other nodes' L2s). The same technique can be used to support replicated data in the NS-LLCs, which gives us the potential to dynamically adjust whether the NS-LLC capacity is used as a private L2 (replicate) or a shared LLC. Figure <ref type="figure" target="#fig_1">3</ref> shows such a replicated cacheline ( R ) with its RP ( D ) pointing to the master location in the LLC of node 2. Inclusion is enforced between a node's MD2 and its replicated data in its near-side LLC.</p><p>This raises a classic question: when to replicate vs. preserve capacity. This questions has been addressed many time in many contexts (e.g., cooperative caching <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b21">[22]</ref>, victim replication <ref type="bibr" target="#b22">[23]</ref>, RNUCA <ref type="bibr" target="#b4">[5]</ref> and commercial NUMA/COMA designs <ref type="bibr" target="#b23">[24]</ref>). Replication reduces overall system shared cache capacity, but can improve local latency. In this work we demonstrate how D2M can provide the infrastructure for supporting this trade-off efficiently. To that end, we evaluate only the following simple heuristic: Instructions are always replicated and data read from the MRU (Most Recently Used) position of a remote NS-LLC are replicated. In spite of this simplistic heuristic, our evaluation shows that NS-LLC data accesses to the local NS-LLC increase to 76% when both the NS-LLC allocation policy above and the replication heuristics are applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Dynamic Indexing</head><p>Another example of the flexibility that the MD provides is in using the Location Information to assign random (scrambled) indices to each region, thereby eliminating conflict misses caused by regular address patterns. To do so, we simply store a few bits of random index value with each region's metadata when it is loaded into the MD3 and use them to index into the data hierarchy's caches. This results in a dramatic energy reduction for a few application's with malicious access patterns, such as LU <ref type="foot" target="#foot_3">4</ref> . This feature demonstrates the flexibility of our general metadata hierarchy for attaching properties to each region, which can be easily extended to record cache bypass policies, prefetch statistics, read-only predictions, etc. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATION A. Methodology</head><p>We use the gem5 ARM full-system simulator <ref type="bibr" target="#b25">[26]</ref> configured to be similar to a contemporary energy-efficient processor as shown in Table <ref type="table">III</ref>. We use CACTI <ref type="bibr" target="#b26">[27]</ref> and McPAT <ref type="bibr" target="#b27">[28]</ref> with 22nm technology, and published performance numbers to estimate latency and energy.</p><p>To evaluate D2M, we look at 5 categories of workloads, Parallel (Parsec <ref type="bibr" target="#b28">[29]</ref>), HPC (Splash2x <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>), Mobile (Chrome browser with Telemetry <ref type="bibr" target="#b31">[32]</ref>), Server (mixes of SPEC CPU-2006 <ref type="bibr" target="#b32">[33]</ref>) and Database (TPC-C <ref type="bibr" target="#b33">[34]</ref> with MySQL/InnoDB). For Parsec, Splash2x, TPC-C and Spec-Mix, we use Ubuntu 14.04. We use region-of-interest for Parsec and Splash2x, sampling for the Spec-Mix and TPC-C. For Chrome, we use Telemetry on ChromeOS/Veyron with 14 of the more commonly visited websites, and start simulating when the page begins to scroll. We use the Freon graphics stack to avoid X11 overhead and NoMali GPU <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref> emulator to avoid measuring software rendering for the mobile workloads.</p><p>We compare with baseline systems modeled on contemporary mobile and server processors (see Figure <ref type="figure">4</ref>). Base-2L is a 2-level baseline system with L1 caches and a shared LLC cache, similar to ARM's A57, but with an 8-way L1 with perfect way prediction (i.e., without the energy penalty for a parallel lookup). Our Base-3L system models a three-level cache hierarchy, with an additional 256kB L2 cache between the L1 and the LLC for each core.</p><p>For D2M we evaluate: L1 caches and a far-side LLC (D2M-FS), L1 caches and a near-side LLC with the simple policy for allocating near-side LLC space (D2M-NS), and D2M-NS with the heuristics for replicating data and instructions in near-side LLC and dynamic indexing (D2M-NS-R). All modeled systems have the same total LLC size, but the Base-3L systems have the highest implementation cost due to their large L2 caches.</p><p>To evaluate these systems we provide high-level metrics including coherency traffic, data traffic, locality in the NS-LLC, and average latency for memory accesses, as well as low-level (e.g., implementation-dependent) metrics for speedup (normalized to Base-2L) and energy.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Traffic</head><p>Figure <ref type="figure">5</ref> shows the interconnect network traffic as number of sent interconnect messages. D2M-NS-R reduces the amount of network traffic by 70% on average largely due to reduced far-side LLC requests and private regions requiring no coherence traffic. The average data-only traffic (number of bytes moved) is reduced by 65% (graphs omitted due to limited space). D2M-specific traffic (e.g., MD2 spill/fill) is shown in the lighter portion of the bars, while data coherence traffic is the darker portion. Canneal and streamcluster are outliers. Canneal is suffering from a exceptionally large number of MD2 misses while streamcluster is dominated by L1 misses going to memory where D2M can offer a latency advantage, but no traffic advantage. All other applications show a traffic advantage for D2M-NS-R.</p><p>Table <ref type="table" target="#tab_6">V</ref> shows number of received invalidations relative to Base-2L, and the percent of misses that are to private MD regions. For the Server workloads (SPEC mixes) we see that all misses are to private MD regions, since the programs do not share any data. Across all workloads, 68% of the misses are to private pages. D2M also puts a lower pressure on the different SRAM structures (not shown). D2M accesses to MD3 are 11% as frequent as directory accesses of Base-2L and 27% of Base-3L. MD2 is accessed 58% as often as the L2-tags in Base 3-L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Energy</head><p>Figure <ref type="figure" target="#fig_3">6</ref> shows EDP (static and dynamic) for the cache hierarchy normalized to Base-2L. D2M-NS generally performs better than D2M-FS. One exception is the cnn mobile benchmark, where EDP increases. This indicates that the simple heuristic for where to place cache lines in the NS-LLC made a non-optimal decision, but with replication (D2M-NS-R) this effect is reduced.</p><p>Across all benchmarks suites, D2M-NS-R increases performance and reduces network traffic and number of accesses to the far-side LLC. Combined, these improvements reduce EDP by 54% compared to Base-2L and by 40% compared to Base-3L. Most energy is spent searching levels and moving data over the interconnect and between cache levels. D2M eliminates cache searches and reduces data movement by placing data on the near-side of the interconnect. This results in a average EDP reduction of 54% (D2M-NS-R).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Performance</head><p>As seen in Section V-C, D2M-NS-R cuts the number of transactions and the data traffic both by more than a factor of two. Thus, simulating a bandwidth-constraint system could potentially result in a 2x speedup. To avoid mixing the performance effects of traffic reduction and latency reduction, we have simulated a system with infinite bandwidth.</p><p>D2M-NS-R reduces the average L1 miss latency by 30% compared with Base-2L. Since we are simulating a fairly aggressive OoO CPU, not all of this latency reduction will translated directly into performance improvement. Figure <ref type="figure" target="#fig_4">7</ref> shows speedups over Base-2L with the L1 miss ratios and NS-LLC (or L2 for Base-3L) hit ratios in Table <ref type="table" target="#tab_6">IV</ref>. D2M-FS improves performance through lower latency by an average of 5.7%. The near-side LLC (D2M-NS) reduces latency further by avoiding the performance cost of going over the interconnect on LLC hits for a 7% speedup. D2D-NS-R replicates shared data and instructions, and can therefore access the replicated data from the core's near-side LLC instead of going to the far-side cacheline copy. This increases the near-side hit ratio from 43% to 84% (instructions) and 58% to 76% (data) resulting in a 8.5% average speedup 5 .</p><p>The Mobile and Database workloads have by far the most instruction misses. As a result, we see larger performance gains for them as the out-of-order processor cannot hide instruction misses. This is especially visible in Database, which has an 8.8% L1-I miss ratio on Base-2L. While D2M- 5 We evaluated scaling the MD1, MD2, and MD3 entries from 1x (128, 4k, 16k) to 2x and 4x and found the average speedup went from 8.5% (1x) to 9.5% (2x) while the number of direct accesses to the NS-LLC (MD1 and NS-LLC hits) increased from 78% to 86%.</p><p>NS significantly improves performance by 21% over Base-2L, only 26% the LLC instructions access hit in the near-side LLC. With D2M-NS-R, we are able to service 97% of the L1-I misses from the NS-LLC with our simple replication heuristic. This gives a net speedup of 28% over Base-2L by automatically using the near side LLC slice as a private L2 cache for instructions.</p><p>Base-3L has an L2 cache that filters LLC access resulting in a 4% average speedup over Base-2L. However, Mobile and Database still experience significant L2 misses, with only 59% of L2 and LLC accesses hitting in the L2 for Database. Base-3L must still access the LLC 41% of the time. In such cases, the L2 cache may hurt performance due to the latency of searching it first. D2M-NS-R does not have a L2 cache, but it can use the core's slice of the NS-LLC for instructions (1MB NS-LLC vs. 256kB L2). This results in 6.8% (vs. 4% for Base-3L) and 28% (vs. 13% for Base-3L) speedups over Base-2L for Mobile and Database, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RELATED WORK</head><p>NUCA caches have been proposed to address the wire delays in LLCs <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. To optimize for varying access latencies, many policies have been suggested to migrate or replicate data closer to where it is needed. Our NS-LLC topology is reminiscent of NUCA caches in that each node has a slice of the LLC cache residing close to it. Our approach decouples the cacheline address from its placement, provides an efficient cacheline location mechanism, and can replicate cachelines. All these features fall out "for free" from our data tracking. These features simplify efficient NUCA implementations, while the placement and replication policies we investigated are simple, with much room for improvement. DDCache <ref type="bibr" target="#b6">[7]</ref> provides data replication, but adds a sharer list to each L1 cacheline whereas D2M has one sharer list per region. When replicating data in L2/NS-LLC DDCache increases the latency since the local slice need to be searched first before accesses the slave. In D2M, most accesses are directed to the slave node immediately after the MD1, which results in shorter latency since D2M does not have to search for the data. DDCache partially decouples data and metadata by keeping extra tags, but when the data is local they are still tightly coupled. D2M provides separate hierarchies for tracking metadata (MD) and data (cache), whereas DDCache uses tag over-provision.</p><p>Directory indirection avoidance has been proposed for pages classified as private <ref type="bibr" target="#b7">[8]</ref>. Our approach also avoids directory indirection for shared data by storing the necessary information in MD1 and enabling direct read accesses to LLC data classified as shared, without the involvement of other nodes. This is a key component that enables NS-LLC.</p><p>Coarse-grained tracking and indirect lookup is used by Zebchuk et al. <ref type="bibr" target="#b36">[37]</ref> in the L2 and last-level caches to reduce tag-area, eliminate snoops and improve prefetching. Seznec <ref type="bibr" target="#b37">[38]</ref> uses pointers to reduce tag storage by replacing the page number in the tags with a pointer to the page in a page number cache. This reduces the tag storage since the pointer is much smaller than the page number. However, both still use tag-based L1 caches and do not address multicore issues. Boettcher et al. <ref type="bibr" target="#b38">[39]</ref> and Sembrant et al. <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b0">[1]</ref> extend the TLB with cacheline way information to reduce L1 cache energy. Boettcher et al. <ref type="bibr" target="#b38">[39]</ref> keep the tag array and treat the TLB way information as hints. Sembrant et al. <ref type="bibr" target="#b39">[40]</ref> provide precise tracking and can therefore eliminate the tag array. Their work was later extended to provide direct accesses to data in multiple cache levels and memory (D2D), as discussed in Section II. Hallnor <ref type="bibr" target="#b40">[41]</ref> suggests an indirect index cache to locate data in a managed cache.</p><p>Private/shared classification has been proposed based on hardware detection mechanisms <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref> and OS support <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b43">[44]</ref>. The OS can classify data at page-granularity with minimal hardware support, but requires a complex shootdown and purging of cachelines when classification changes. Hardware classification can provide a finer granularity, but may have high storage requirements. Data classification has been used to steer NUCA placement <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> and for filtering coherence activities <ref type="bibr" target="#b36">[37]</ref> in broadcast protocols, among other things.</p><p>Our region classification solution is similar to earlier hardware mechanisms, even though the D2M detection supports region granularity and utilizes its existing coherence hardware to facilitate classification. D2M does not require shootdown or purging on re-classification.</p><p>Framework for optimization. While each of these related works addressed important problems, none of them provide a general framework that can enable the wide range of optimizations that D2M supports. D2M's split metadata and data hierarchies and efficient data location lookup enable us to implement and combine solutions to each of these problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>In this work we have introduced the Direct-to-Master (D2M) cache design that splits data and metadata into two separate cache hierarchies. To do so we provided deterministic metadata information across a coherent system. We accomplished this by extending D2D's data tracking infrastructure to be able to index remote nodes, track sharers, and store explicit replacement locations. This allows us to keep location information coherent, while providing for efficient (direct, coherence-free) access to private data, low-overhead (multicast vs. broadcast) coherence updates, and low-latency (no directory access with local sharing information) access to shared data.</p><p>The separation of the metadata and data hierarchies is not only efficient, but also provides a flexible framework for implementing many data-oriented optimizations. We illustrated this by combining optimizations for data placement, coherence, and indexing within the D2M framework. In particular, we demonstrated how a near-side LLC, wherein we moved the LLC slice to the core-side of the interconnect, can significantly lower latency and energy. This optimization comes for "free" as D2M allows us to arbitrarily direct and replicate a node's data to across the LLC slices. While the heuristics we used for this were decidedly simple, D2M's flexibility has addressed the underlying NUCA problems of decoupling placement and addressing and of efficiently accessing the data.</p><p>D2M was evaluated against server and mobile processor configurations and a wide range of server, database, mobile, and parallel workloads. Across these configurations, D2M provided significant gains in cache hierarchy EDP (54% vs. mobile and 40% vs. server) and reduced network traffic by an average of 70%. D2M is especially valuable for workloads with large instruction footprints (database and mobile) as the near-side LLC can automatically serve as a large private L2, thereby reducing instruction misses by up to 97%. ACKNOWLEDGEMENT This work was supported in part by the Swedish Foundation for Strategic Research (grant FFL12-0051), the Uppsala Programming for Multicore Architectures Research Center, and the Swedish National Infrastructure for Computing (SNIC) at NSC (Link?ping) and UPPMAX.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>We explain the unified coherence protocol for data and metadata at a high level through nine important coherence examples. Most examples are also shown graphically for clarity in Figure <ref type="figure">8</ref>.</p><p>The D2M coherence protocol relies in part on an atomic update mechanism, such as those used previously to implement deterministic directories, for example Sun Microsystem's WildFire, SunFire 10k and 15k servers <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b44">[45]</ref>. These server's coherence solution scaled to about 100 CPUs and relied on a blocking mechanism at the directory that only allowed for one outstanding coherence transaction for each cacheline. This ensured that the directory state always reflected the correct location for the "master cacheline" (deterministic). A similar mechanism is implemented at the MD3 level of D2M, guaranteeing that there can be at most one ongoing operation per region that could make changes to the region's metadata. The blocking mechanism can be implemented as a set of hashed lock bits that are explicitly blocked and unblocked by the coherence protocol. Simulation show that 1K lock bits result in a negligible collision rate.</p><p>D2M is optimized to handle the most important L1 miss event efficiently: read misses. To put the significance of each event in perspective, the events per kilo memory operation (PKMO) are provided in the text with each event. On average, there are 13.3 read misses and 2.4 write misses (PKMO) and an additional 19 and 21 late hits (PKMO), respectively <ref type="foot" target="#foot_4">6</ref> .</p><p>A: Read Miss, MD1/MD2 Hit (12.5): For read misses where the location information is present in the MD1 (9.2) or MD2 (3.3), the information is guaranteed to be deterministic with respect to a cacheline's master location. This allows D2M to satisfy a read miss with a read request directly to the master (hence the name: D2M), without the need to interact with MD3. A master location in LLC (8.9) or memory (2.7) can be accessed directly, similarly to D2D, while reading a master cacheline in a remote slave node (0.8), e.g. in a remote L1, requires an indirection through that node's active MD1 or MD2 to determine its location in that node. Finally, the cacheline's LI in the requesting node is updated to point to the new local location (e.g. in L1), while the global master location stays unchanged.</p><p>B: Write Miss, Private Region, MD1/MD2 hit (1.7): The private region is not tracked by any other node and its data is not cached in any other node. Thus, the requesting node can read data directly from the master location, similar to example A, and update the local MD1 to point to the new local location, which now becomes the new writable master location. This action makes the LI in MD3 invalid for private regions.</p><p>C: Write Miss, Shared Region, MD1/MD2 hit (0.72): The requesting node sends a blocking ReadEx request to MD3; Once the region has been blocked<ref type="foot" target="#foot_5">7</ref> , a DirectReadEx is sent to the master and Invs are sent to the slave nodes with PB bits set (excluding master node); The slaves send <ref type="foot" target="#foot_6">8</ref>Acks to the requesting node and set their LIs to point to the requesting node's NodeID (i.e., the new master location); The requesting node sends Done to MD3 when all Acks<ref type="foot" target="#foot_7">9</ref> have been received, which unblocks the region, and points MD3 entry with all LI set to "MEM"; Reply with metadata with the private bit set to the requesting node. 10 The sending can be done once it can be guaranteed that all subsequent reads to the victim location will return the copied data.</p><p>11 If MD3's LI does not point to the requesting node, it is no longer the master and the eviction is Nack-ed. 12 The sending can be done once it can be guaranteed that all subsequent reads to the victim location will return the copied data. 13 In the case of shared cachelines, their RP is instead updated. 14 NewMaster and Ack carry the number of Acks to expect. 15 Base-2L architecture in the Evaluation section, with the same implementation cost as D2M-NS-R.</p><p>For all of the above cases (D1-D4), MD3 also sets the PB bit for the requesting node and sends a direct read request to the master on behalf of the requesting node; When the requesting node has received the data and metadata, it send Done to MD3, which unblocks the region.</p><p>E: Node Eviction of dirty master, Private Region: Copy data to the victim location (e.g., in LLC or MEM) stored in RP; Change LI of the active MD1/MD2 in the requesting node to point to the victim location.</p><p>F: Node Eviction of dirty master, Shared Region: The requesting node copies data to the victim location stored in RP; The requesting node sends 10 EvictReq to MD3; Once the region has been blocked 12 , MD3 sends NewMaster messages to slave nodes with PB bits set; They send 8 Acks to the requesting node and point their LI 13 to the new master location; The requesting node sends Done to MD3 when all Acks 14 have been received, which unblocks the region.</p><p>We can observe that the most common cases, A and B (14.2 PKMO, or 90% of all misses) do not require any "directory" lookups in D2M, while the baseline 15 implementation does. The cost of the C and D cases are on-par with the baseline (assuming the MD3 lookup is as expensive as a traditional directory lookup), with the exception for case D2, which is fortunately quite rare (0.02 PKMO).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The basic D2D architecture with Metadata pointing to data in the L1, L2, and memory. MD1 is contains a Virtual Tag (VT) and the Physical Address (PA) for each region, as well as Location Information (LI) pointers for each cacheline in the region. MD2 has a Physical Tag (PT) and a Tracking Pointer (TP) that points to the active region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Near-side LLC (NS-LLC) with the LLC slices moved to the core side of the interconnect. Node 1 uses more data than Node 2, and has therefore more green cachelines in Node 2's NS-LLC slice.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. EDP of the cache hierarchy energy normalized to Base-2L. The darker bars show EDP contribution from standard structures and lighter bars show D2M-only structures (e.g., location trackers). Most energy is spent searching levels and moving data over the interconnect and between cache levels. D2M eliminates cache searches and reduces data movement by placing data on the near-side of the interconnect. This results in a average EDP reduction of 54% (D2M-NS-R).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Speedup in percent compared to Base-2L for systems with infinite bandwidth. From left to right: Base-2L (always zero), Base-3L, D2M-FS, D2M-NS, D2M-NS-R. On average, D2M-NS-R improves performance by 8.5% (max 28%) over Base-2L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Figure 2. The generic architecture for D2M. TLB-2 and instruction cache not shown. The Cacheline in L2 stores metadata used for eviction: TP = Tracker Pointer; RP = Replacement Pointer. The MD2 entry is passive if its TP pointer identifies the active MD1 entry. The Presence Bits (PB) indicate which nodes track the region. The Private bit (P) indicates to the node that the region is private.</figDesc><table><row><cell cols="2">Node 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Node 2</cell><cell>Node 8</cell></row><row><cell>MD1</cell><cell></cell><cell>...</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>L1</cell></row><row><cell></cell><cell cols="2">PA</cell><cell>P</cell><cell>LI</cell><cell></cell><cell>A 1</cell><cell>... ...</cell></row><row><cell></cell><cell></cell><cell cols="2">C</cell><cell>A 3</cell><cell>A 2</cell><cell>A 4</cell><cell>DATA</cell><cell>TP</cell><cell>RP</cell></row><row><cell cols="2">... MD2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>... ... ... L2</cell></row><row><cell>PT NoC</cell><cell>P</cell><cell>LI</cell><cell></cell><cell>TP</cell><cell cols="2">B</cell><cell>DATA</cell><cell>B</cell><cell>TP RP</cell></row><row><cell cols="2">L2 MD3</cell><cell></cell><cell></cell><cell></cell><cell cols="2">LLC</cell><cell></cell><cell>D</cell></row><row><cell>...</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>... ... ...</cell><cell></cell><cell>Memory</cell></row><row><cell>PT</cell><cell>PB</cell><cell></cell><cell>LI</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DATA</cell><cell>TP</cell></row></table><note><p>VT ...</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Figure 5. Network traffic in Messages per thousand instructions. The darker bars show basic network traffic and lighter bars show D2M-only traffic (e.g., new master updates). D2M reduces the amount of traffic by distributing the directory information (i.e., fewer network hops) and by placing the LLC on the near-side of the interconnect (i.e., no network traffic on near-side LLC hits).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Base-2L</cell><cell>Base-3L</cell><cell>D2M-FS</cell><cell>D2M-NS</cell><cell>D2M-NS-R</cell></row><row><cell>Network Traffic Network Traffic</cell><cell>Msg / 1000 Insts. Msg / 1000 Insts.</cell><cell></cell><cell cols="2">6 1 1 8 9</cell><cell></cell><cell>1 6 4 1 6 1 2 0 0 1 7 9 1 8 4</cell><cell>7 8 0 3 3 9 5 6 8 2 9 9 1 4 5 1 2 8</cell><cell>2 0 0 1 3 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>HPC (Splash2x)</cell></row><row><cell>Network Traffic Network Traffic</cell><cell>Msg / 1000 Insts. Msg / 1000 Insts.</cell><cell>0 20 40 60 80 100 120 0 20 40 60 80 100 120</cell><cell></cell><cell></cell><cell cols="2">Parallel (Parsec)</cell><cell>1 4 5 1 3 6 1 2 6 1 7 8 1 5 7 1 2 7 1 4 5 1 4 5 1 3 6 1 2 6 1 7 8 1 5 7 1 2 7 1 4 5</cell><cell>1 8 2 1 3 3 1 8 2 1 3 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>a m a z o n a n s w e r s . y a h o o b o o k i n g c n n a m a z o n a n s w e r s . y a h b o o k i n g c n n o o</cell><cell cols="3">e b a y e b a y Mobile (Chrome Browser) f a c e b o o k g o o g l e n e w s . y a h o o r e d d i t s p o r t s . y a h o o t e c h c r u n c h t w i t t e r w i k i p e d i a y o u t u b e g m e a n f a c e b o o k g o o g l e n e w s . y a h o o r e d d i t t e c h c r u n c h t w i t t e r w i k i p e d i a y o u t u b e g m e a n s p o r t s . y a h o o</cell><cell>m i x 1 m i x 1 Server (SPEC Mix) m i x 2 m i x 3 m i x 4 m i x 2 m i x 3 m i x 4</cell><cell>g m e a n g m e a n</cell><cell>t p c -c t p c -c Database</cell><cell>g m e a n g m e a n</cell></row><row><cell></cell><cell></cell><cell></cell><cell>L1 Cache (%)</cell><cell></cell><cell></cell><cell>Near-Side Hit Ratio (%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Misses Late Hits B-2L</cell><cell>B-3L</cell><cell>FS</cell><cell>NS</cell><cell>NS-R</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">L1I L1D L1I L1D I/D</cell><cell>I</cell><cell>D I/D I D I D</cell></row><row><cell cols="3">Parallel</cell><cell cols="2">0.2 1.9 0.1 2.9</cell><cell>-</cell><cell>67 57 -28 51 82 71</cell></row><row><cell cols="2">HPC</cell><cell></cell><cell cols="2">0.0 2.2 0.0 4.6</cell><cell>-</cell><cell>27 69 -17 54 44 79</cell></row><row><cell cols="3">Server</cell><cell cols="2">0.4 3.6 0.3 9.5</cell><cell>-</cell><cell>100 78 -82 83 95 83</cell></row><row><cell cols="3">Mobile</cell><cell cols="2">2.2 1.3 1.8 3.0</cell><cell>-</cell><cell>76 59 -56 66 96 73</cell></row><row><cell cols="3">Database</cell><cell cols="2">8.8 3.3 6.2 4.2</cell><cell>-</cell><cell>59 41 -26 34 97 72</cell></row><row><cell cols="3">Average</cell><cell cols="2">2.3 2.5 1.7 4.8</cell><cell>-</cell><cell>66 61 -42 57 83 76</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(L2 hits)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table V NUMBER</head><label>V</label><figDesc>OF RECEIVED INVALIDATIONS (INCLUDING FALSE INVALIDATIONS) NORMALIZED TO BASE-2L (IN %), AND PERCENT OF MISSES TO PRIVATE MD REGIONS. ON AVERAGE, 68% OF THE DATA MISSES ARE TO PRIVATE REGIONS, WHICH REQUIRE NO COHERENCE TRAFFIC.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Node Eviction of dirty master (L1 LLC), Private Region</head><label></label><figDesc></figDesc><table><row><cell cols="4">1. Req. Node (R)</cell><cell>2. MD3</cell><cell cols="3">3. Slave Nodes 4. Data Loc. (eg LLC)</cell><cell>1. Req. Node (R)</cell><cell>2. MD3</cell><cell>3. Slave Nodes 4. Data Loc. (eg LLC)</cell></row><row><cell>LI</cell><cell>L1</cell><cell></cell><cell></cell><cell cols="2">DirectRead</cell><cell></cell><cell>SRAM</cell><cell></cell><cell>DirectRead</cell><cell>SRAM</cell></row><row><cell cols="2">State</cell><cell>S</cell><cell></cell><cell></cell><cell>Data</cell><cell></cell><cell>Access</cell><cell></cell><cell>Data</cell><cell>Access</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">A: Read Miss, MD1/MD2 Hit</cell><cell></cell><cell cols="2">B: Write Miss, Private Region, MD1/MD2 Hit</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ReadEx</cell><cell>Block LI R</cell><cell>Inv Ack</cell><cell>State LI R DirectReadEx I</cell><cell>SRAM Access</cell><cell>Done ReadMM MD</cell><cell>Unblock Block PB[R] 1</cell><cell>DirectRead</cell><cell>SRAM Access</cell></row><row><cell cols="2">State LI L1</cell><cell>M</cell><cell>Done</cell><cell cols="2">Unblock Data</cell><cell></cell><cell></cell><cell cols="2">D1: Read Miss, MD1/MD2 Miss (#PB=0, Untracked Private)</cell></row><row><cell>LI</cell><cell>L1</cell><cell></cell><cell cols="4">C: Write Miss, Shared Region, MD1/MD2 Hit MD PB[R] 1 DirectRead GetMD MD P 0 ReadMM</cell><cell>SRAM Access</cell><cell cols="2">D3: Read Miss, MD1/MD2 Miss (#PB&gt;1, Shared Shared) Unblock Done Data Block ReadMM Access SRAM PB[R] 1 MD DirectRead</cell></row><row><cell>P</cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">State</cell><cell>S</cell><cell>Done</cell><cell cols="2">Unblock</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="6">D2: Read Miss, MD1/MD2 Miss (#PB=1, Private Shared)</cell><cell></cell></row><row><cell>LI</cell><cell>RP</cell><cell></cell><cell></cell><cell cols="2">DirectWriteData</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>F:</cell></row></table><note><p>E:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Node Eviction of dirty master (L1 LLC), Shared Region Block</head><label></label><figDesc>LI to the local location in L1. The location in L1 is now the new master location. D: Read Miss, MD1/MD2 Miss (0.82): This requires that we retrieve both the region metadata and the requested data. A blocking ReadMM request (Read Metadata Miss) is sent to MD3. Once the region has been blocked there are four possible outcomes depending on the classification of the region (i.e., the number of PB bits set): 1) #PB == 0: Untracked?Private (0.32): MD3 replies to the requesting node with metadata with the private bit set. 2) #PB == 1: Private?Shared (0.02): Note that MD3 LIs for private regions are invalid. MD3 sends an GetMD to the single slave node; It clears its private bit and sends 8 metadata to MD3; LIs pointing to local location in the slave node (e.g. in L1D) are converted to point to the slave NodeID and stored in MD3 metadata; MD3 Replies with metadata with the private bit cleared to the requesting node. 3) #PB &gt; 1: Shared?Shared (0.14): MD3 sends metadata with the private bit cleared to the requesting node. 4) MD3 miss: Uncached?Private (0.34): Create a new</figDesc><table><row><cell></cell><cell>LI</cell><cell>L1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">State</cell><cell>M</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>LI</cell><cell>L1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">P State 1</cell><cell>S</cell><cell></cell><cell>Data</cell><cell></cell></row><row><cell></cell><cell>LI</cell><cell>L1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Block</cell><cell cols="2">P State 0</cell><cell>S</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>WriteData</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>EvictReq</cell><cell></cell><cell>Ordered</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>LI</cell><cell>Victim Location</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NewMaster</cell><cell></cell></row><row><cell>Ordered</cell><cell>LI</cell><cell>RP</cell><cell>Done Ack</cell><cell cols="2">Unblock</cell><cell>LI</cell><cell>Victim Location</cell></row><row><cell></cell><cell cols="3">Figure 8. Coherence examples.</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>its</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>When an MD1 entry is evicted, its LI information is copied to MD2. When a cacheline is chosen for eviction, the corresponding active MD entry needs to be updated. This is handled by adding tracking pointers to each cacheline (e, f). More details on this can be found in<ref type="bibr" target="#b0">[1]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>There are separate MD1-I and L1-I structures, not shown in the example. An additional field in MD2 indicates whether the region's active location information is in the MD1-I or MD1-D.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Most proposed NUCA policies could readily be applied. D2M's contribution is in the mechanism, not the policy.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>We have not evaluated this scrambling solution compared to other alternatives<ref type="bibr" target="#b19">[20]</ref>,<ref type="bibr" target="#b24">[25]</ref>,<ref type="bibr" target="#b20">[21]</ref>,<ref type="bibr" target="#b8">[9]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>PKMO are average occurrences per 1,000 memory operations across all application categories for the basic D2M-FS architecture in the evaluation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>This guarantees that there are no other ongoing blocking operations for the region.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>The sending is delayed until the slave node has no outstanding direct read requests (examples A and B) for the region in order to guarantee that there are no outstanding request for the old master location.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>Inv and Ack carry the number of Acks to expect.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cache: Navigating the Cache Hierarchy with a Single Lookup</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sembrant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hagersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Black-Schaffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symposium on Computer Architecture (ISCA)</title>
		<meeting>Int. Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>The Direct-to-Data</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An adaptive, non-uniform cache structure for wire-delay dominated on-chip caches</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>Int. Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distance Associativity for High-Performance Energy-Efficient Non-Uniform Cache Architectures</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Vijaykumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symposium on Microarchitecture (MICRO)</title>
		<meeting>Int. Symposium on Microarchitecture (MICRO)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Managing Wire Delay in Large Chip-Multiprocessor Caches</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symposium on Microarchitecture (MI-CRO)</title>
		<meeting>Int. Symposium on Microarchitecture (MI-CRO)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reactive NUCA: Near-optimal Block Placement and Replication in Distributed Caches</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hardavellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symposium on Computer Architecture (ISCA)</title>
		<meeting>Int. Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Managing distributed, shared l2 caches through os-level page allocation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symposium on Microarchitecture (MICRO)</title>
		<meeting>Int. Symposium on Microarchitecture (MICRO)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DDCache: Decoupled and Delegable Cache Data and Metadata</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dwarkadas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conference on Parallel Architectures and Compilation Techniques</title>
		<meeting>Int. Conference on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Increasing the Effectiveness of Directory Caches by Deactivating Coherence for Private Memory Blocks</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Cuesta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>G?mez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Robles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Duato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symposium on Computer Architecture (ISCA)</title>
		<meeting>Int. Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ASCIB: Adaptive Selection of Cache Indexing Bits for Removing Conflict Misses</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xekalakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cintra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Acacio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Garc?a</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symposium on Low Power Electronics and Design</title>
		<meeting>Int. Symposium on Low Power Electronics and Design</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Temporal-Aware Mechanism to Detect Private Data in Chip Multiprocessors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cuesta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>G?mez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Robles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conference on Parallel Processing (ICPP)</title>
		<meeting>Int. Conference on Parallel essing (ICPP)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Effects of Granularity and Adaptivity on Private/Shared Classification for Coherence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Davari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hagersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Archit. Code Optim</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2015-08">Aug. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bypass and Insertion Algorithms for Exclusive Last-level Caches</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symposium on Computer Architecture (ISCA)</title>
		<meeting>Int. Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Optimal Bypass Monitor for High Performance Last-level Caches</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
		<meeting>Int. Conference on Parallel Architectures and Compilation Techniques (PACT)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Evicted-address Filter: A Unified Mechanism to Address Both Cache Pollution and Thrashing</title>
		<author>
			<persName><forename type="first">V</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kozuch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
		<meeting>Int. Conference on Parallel Architectures and Compilation Techniques (PACT)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving Cache Management Policies Using Dynamic Reuse Distances</title>
		<author>
			<persName><forename type="first">N</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cammarota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Veidenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symposium on Microarchitecture</title>
		<meeting>Int. Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fundamental Latency Trade-off in Architecting DRAM Caches: Outperforming Impractical SRAM-Tags with a Simple and Practical Design</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symposium on Microarchitecture</title>
		<meeting>Int. Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Introducing Hierarchy-awareness in Replacement and Bypass Algorithms for Last-level Caches</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bashyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nuzman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
		<meeting>Int. Conference on Parallel Architectures and Compilation Techniques (PACT)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adaptive Cache Bypassing for Inclusive Last Level Caches</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The Reuse Cache: Downsizing the Shared Last-level Cache</title>
		<author>
			<persName><forename type="first">J</forename><surname>Albericio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ib??ez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vi?als</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Llaber?a</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symposium on Microarchitecture (MICRO)</title>
		<meeting>Int. Symposium on Microarchitecture (MICRO)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Column-associative Caches: A Technique for Reducing the Miss Rate of Direct-mapped Caches</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Pudar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symposium on Computer Architecture (ISCA)</title>
		<meeting>Int. Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Balanced Cache: Reducing Conflict Misses of Direct-Mapped Caches</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symposium on Computer Architecture (ISCA)</title>
		<meeting>Int. Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cooperative Caching for Chip Multiprocessors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symposium on Computer Architecture (ISCA)</title>
		<meeting>Int. Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Victim Replication: Maximizing Capacity While Hiding Wire Delay in Tiled Chip Multiprocessors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symposium on Computer Architecture (ISCA)</title>
		<meeting>Int. Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">WildFire: A Scalable Path for SMPs</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hagersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symposium on High-Performance Computer Architecture (HPCA)</title>
		<meeting>Int. Symposium on High-Performance Computer Architecture (HPCA)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using Prime Numbers for Cache Indexing to Eliminate Conflict Misses</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kharbutli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symposium on High-Performance Computer Architecture (HPCA)</title>
		<meeting>Int. Symposium on High-Performance Computer Architecture (HPCA)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<author>
			<persName><forename type="first">N</forename><surname>Binkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sardashti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sewell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vaish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The gem5 Simulator</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">CACTI 6.0: A Tool to Model Large Caches</title>
		<author>
			<persName><forename type="first">N</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hewlett Packard Labs, Tech. Rep</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">McPAT: An Integrated Power, Area, and Timing Modeling Framework for Multicore and Manycore Architectures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Strong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symposium on Microarchitecture</title>
		<meeting>Int. Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The parsec benchmark suite: Characterization and architectural implications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bienia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
		<meeting>Int. Conference on Parallel Architectures and Compilation Techniques (PACT)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The SPLASH-2 Programs: Characterization and Methodological Considerations</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ohara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Torrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symposium on Computer Architecture (ISCA)</title>
		<meeting>Int. Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Parsec vs. splash-2: A quantitative comparison of two multithreaded benchmark suites on chip-multiprocessors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bienia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symposium on Workload Characterization (IISWC)</title>
		<meeting>Int. Symposium on Workload Characterization (IISWC)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><surname>Telemetry</surname></persName>
		</author>
		<ptr target="https://www.chromium.org/developers/telemetry" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SPEC CPU2006 Benchmark Descriptions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Henning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<ptr target="http://www.tpc.org/" />
		<title level="m">Transaction Processing Performance Council</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">NoMali: Understanding the Impact of Software Rendering Using a Stub GPU</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sandberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second gem5 User Workshop</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mobile Benchmarking Done Right: Understanding the System Impact of Mobile GPUs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sandberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symposium on Performance Analysis of Systems &amp; Software (ISPASS)</title>
		<meeting>Int. Symposium on Performance Analysis of Systems &amp; Software (ISPASS)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A Framework for Coarse-Grain Optimizations in the On-Chip Memory Hierarchy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zebchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Safi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symposium on Microarchitecture (MICRO)</title>
		<meeting>Int. Symposium on Microarchitecture (MICRO)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Don&apos;T Use the Page Number, but a Pointer to It</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symposium on Computer Architecture (ISCA)</title>
		<meeting>Int. Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">MALEC: A Multiple Access Low Energy Cache</title>
		<author>
			<persName><forename type="first">M</forename><surname>Boettcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gabrielli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Al-Hashimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kershaw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">TLC: A Tag-Less Cache for Reducing Dynamic First Level Cache Energy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sembrant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hagersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Black-Schaffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symposium on Microarchitecture (MICRO)</title>
		<meeting>Int. Symposium on Microarchitecture (MICRO)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A fully associative software-managed cache design</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Hallnor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symposium on Computer Architecture (ISCA)</title>
		<meeting>Int. Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">POPS: Coherence Protocol Optimization for Both Private and Shared Data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dwarkadas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
		<meeting>Int. Conference on Parallel Architectures and Compilation Techniques (PACT)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">SWEL: Hardware Cache Coherence Protocols to Map Shared Data onto Shared Caches</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Spjut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Nellans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
		<meeting>Int. Conference on Parallel Architectures and Compilation Techniques (PACT)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Subspace Snooping: Filtering Snoops with Operating System Support</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
		<meeting>Int. Conference on Parallel Architectures and Compilation Techniques (PACT)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The sun fireplane system interconnect</title>
		<author>
			<persName><forename type="first">A</forename><surname>Charlesworth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supercomputing, ACM/IEEE 2001 Conference</title>
		<imprint>
			<date type="published" when="2001-11">Nov 2001</date>
			<biblScope unit="page" from="2" to="2" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
