<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Analysis and Synthesis of Facial Image Sequences Using Physical and Anatomical Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Demetri</forename><surname>Terzopoulos</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Keith</forename><surname>Waters</surname></persName>
						</author>
						<title level="a" type="main">Analysis and Synthesis of Facial Image Sequences Using Physical and Anatomical Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2C3BC996A019AD1D081E2018EA0EFDD9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Tem-Computer graphics</term>
					<term>computer vision</term>
					<term>deformable models</term>
					<term>face modeling</term>
					<term>facial image analysis</term>
					<term>facial image synthesis</term>
					<term>nonrigid motion analysis</term>
					<term>physics-based modeling</term>
					<term>snakes</term>
					<term>tracking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a new approach to the analysis of dynamic facial images for the purposes of estimating and resynthesizing dynamic facial expressions. The approach exploits a sophisticated generative model of the human face originally developed for realistic facial animation. The face model, which may be simulated and rendered at interactive rates on a graphics workstation, incorporates a physics-based synthetic facial tissue and a set of anatomically motivated facial muscle actuators. We consider the estimation of dynamic facial muscle contractions from video sequences of expressive human faces. We develop an estimation technique that uses deformable contour models (snakes) to track the nonrigid motions of facial features in video images. The technique estimates muscle actuator controls with sufficient accuracy to permit the face model to resynthesize transient expressions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HE COMPLEXITY and expressiveness of the human</head><p>T face makes it a challenging subject for automated visual interpretation and recognition. Quick, robust facial image analysis is desirable for numerous applications. Among them is low-bandwidth teleconferencing, which may involve the realtime extraction of facial control parameters from live video at the transmission site and the reconstruction of a dynamic facsimile of the subject's face at a remote receiver. Teleconferencing and other applications require facial models that are not only computationally efficient but also realistic enough to accurately synthesize the various nuances of facial structure and motion. In this paper, we will show the following:</p><p>1) We present a 3-D dynamic model of the face that can be simulated in real time on graphics workstations. Our face model combines a physics-based model of facial tissue with an anatomically based facial muscle control process to synthesize realistic facial motions (Fig. <ref type="figure" target="#fig_1">1</ref>).</p><p>We enhance the apparent realism by employing geometric and photometric information acquired by scanning subjects with active sensors.  We develop a technique for analyzing video sequences of faces undergoing transient expressions. The goal is to estimate the dynamic muscle control parameters of the face model in order to reconstruct expressions. Our estimation technique employs interactive deformable contours (snakes) to track the nonrigid motions of extended facial features in video images.</p><p>Section I1 reviews prior research and motivates our approach. Section 111 presents the face model. The presentation includes a brief review of the histology and mechanical properties of facial tissue and the anatomical structure of facial muscles, a description of the synthetic tissue model and its real-time numerical simulation, a description of the muscle actuators embedded in our facial tissue model, and the facial action coding process that controls these muscles to produce recognizable expressions. Section IV presents techniques for enhancing the realism of the face model and personalizing it through the exploitation of geometric and photometric data acquired with active range sensors. Section V considers the analysis of video image sequences for the dynamic estimation of facial muscle parameters and demonstrates our approach 0162-8828/93$03.00 0 1993 IEEE using an example. Section VI discusses our work and suggests some future research directions. Section VI1 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">BACKGROUND AND MOTIVATION</head><p>The human face has attracted much attention in several disciplines, including psychology, computer vision, and computer graphics. Psychophysical investigations clearly indicate that faces are very special visual stimulii. Psychologists have studied various aspects of human face perception and recognition [SI, <ref type="bibr" target="#b2">[3]</ref>. They have also examined facial expression-the result of a confluence of voluntary muscle articulations that deform the neutral face into an expressive face. The facial pose space is immense. The face is capable of generating on the order of 55 000 distinguishable facial expressions with about 30 semantic distinctions. For example, Ekman and Friesen's facial action coding system (FACS) provides a quantification of facial expressions <ref type="bibr" target="#b7">[8]</ref>. Studies have identified six primary expressions that communicate anger, disgust, fear, happiness, sadness, and surprise in all cultures <ref type="bibr" target="#b6">[7]</ref>. The FACS quantifies facial expressions in terms of 44 action units (AU) involving one or more muscles and associated activation levels.</p><p>We employ a reduced version of Ekman and Friesen's FACS in a sophisticated computational model of the human face that we originally developed for the realistic animation of synthetic characters. Facial animation in computer graphics began with Parke's use of facial images as keyframes and his subsequent popularization of parameterized face models <ref type="bibr" target="#b22">[22]</ref>. State-of-theart parameterized models can produce impressive animation using parameters associated with facial muscle structures [33], <ref type="bibr" target="#b31">[30]</ref>. Graphics researchers have devoted significant effort to parametric facial modeling but little effort to the inverse problem of extracting parameters from facial images. There is some relevant work on lip synchronization during continuous speech animation, but the parameter extraction techniques proposed remain predominantly manual <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b31">[30]</ref>, [ 121. Reflective markers have been placed on the face in order to extract parameters for performance-driven facial animation P I .</p><p>Automatic facial recognition had an early start in image understanding, but work on the problem has been sporadic over the years, evidently due to the difficulty of extracting meaningful information from facial images. Facial classification systems based on measurements derived from interactively selected fiducial points (eye and mouth comers, nose, top of head, etc.) go back to the mid 1960's <ref type="bibr" target="#b1">[2]</ref>, [ E ] , <ref type="bibr">[ l l ]</ref> . Early attempts at recognition through automated facial feature identification include <ref type="bibr" target="#b26">[25]</ref> and <ref type="bibr" target="#b13">[13]</ref>. Part of the difficulty of facial image analysis is that the face is highly deformable, particularly around the forehead, eyes, and mouth, and these deformations convey a great deal of meaningful information. Techniques for dealing with the deformation of facial features include spring-loaded subtemplates <ref type="bibr" target="#b9">[9]</ref>, deformable contour models that are also known as snakes <ref type="bibr" target="#b14">[14]</ref>, and deformable templates [31], <ref type="bibr">[26]</ref>. In our approach, we apply a variant of snakes. Snakes are dynamic deformable contour models that require some routine image processing and, in our application, a modest amount of user input during initialization. Snakes have also been applied to the related problem of determining the location of the head in images <ref type="bibr">[32]</ref>.</p><p>We argue that the anatomy and physics of the human face, especially the arrangement and actions of the primary facial muscles, provide a good basis for facial image analysis <ref type="bibr">[29]</ref>. We use snakes to track the position of the head and the nonrigid motions of the eyebrows, nasal furrows, mouth, and jaw in the image plane. We are able to estimate dynamic facial muscle contractions directly from the snake state variables. These estimates make appropriate control parameters for resynthesizing facial expressions through our face model. The model resynthesizes facial images at real-time rates. Realtime synthesis is desirable for model-based analysis-synthesis coding of facial images (see e.g., <ref type="bibr">[ l ]</ref> and <ref type="bibr">[lo]</ref>). Our approach is philosophically similar to that described in [ 11 and in <ref type="bibr" target="#b3">[4]</ref> but differs in the details of the image analysis and resynthesis. In particular, we employ physical rather than geometric modeling methods.</p><p>The purely geometric nature of prior face models [33], [17], <ref type="bibr" target="#b31">[30]</ref> limits their ability to synthesize realistic facial animation because it ignores the fact that the human face is an elaborate biomechanical system. Our face model takes a more fundamental, physics-based approach to synthesizing the many subtleties of facial tissue deformation in response to facial muscle actions (such as the skin wrinkles and furrows shown in Fig. <ref type="figure" target="#fig_1">1</ref>). A wealth of biomedical literature on tissue mechanics [ 161 has provided motivation for finite element models of facial tissue that are suitable for surgical simulation [MI, <ref type="bibr" target="#b5">[6]</ref> (see also Pieper's deformable lattice model <ref type="bibr" target="#b23">[23]</ref>). The next section describes our realistic face model that incorporates anatomically based muscle actuators with a physics-based synthetic tissue model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">A REALISTIC FACE MODEL</head><p>We have developed a hierarchical model of the face that provides natural control parameters and is efficient enough to run at interactive rates. Conceptually, the model decomposes into six levels of abstraction. These representational levels encode specialized knowledge about the psychology of human facial expressions, the anatomy of facial muscle structures, the histology and biomechanics of facial tissues, and facial skeleton geometry and kinematics:</p><p>Expression: At the highest level of abstraction, the face model executes expression (or phoneme) commands. For instance, it can synthesize any of the six primary expressions within a specific time interval and with a specified degree of emphasis. The hierarchical structure of the model hides from the user most of the complexities of the underlying representations, relegating the details of their computation to automatic procedures. At the higher levels of abstraction, our face model offers the user a semantically rich set of control parameters that reflect the natural constraints of real faces.</p><p>With the above top-down overview in mind, we will now present some of the details of our model in a bottom-up fashion. We explain the structure and functionality of the synthetic tissue model and then describe the facial muscle models and how they interact with the tissue. Finally, we explain the assembly of the face model from these components as well as a specified surface geometry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Physics-Based Synthetic Tissue Model</head><p>Our synthetic facial tissue model is motivated by histology and tissue biomechanics. Human skin has a nonhomogeneous and nonisotropic layered structure consisting of the epidermis (a superficial layer of dead cells), which is about one tenth the thickness of the dermis that it protects <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr">[18]</ref>. The dermis is primarily responsible for the mechanical properties of skin.</p><p>Dermal tissue is composed of collagen (72%) and elastin (4%) fibers forming a densely convoluted network in a gelatinous ground substance (20%). Under low stress, dermal tissue offers little resistance to stretch as the collagen fibers begin to uncoil in the direction of the strain, but under greater stress, the fully uncoiled collagen fibers resist stretch much more markedly. This yields an approximately biphasic stress-strain curve (Fig. <ref type="figure" target="#fig_2">2</ref>). The incompressible ground substance retards the motion of the fibers and thereby gives rise to viscoelastic behavior. Finally, the elastin fibers act like elastic springs that return the collagen fibers to their coiled condition under zero load. A layer of subcutaneous fatty tissue that allows the skin to slide rather easily over fibrous fascia covering the underlying muscle layer is underneath the skin (see Section 1114).</p><p>The synthetic tissue is a deformable lattice, which is an assembly of point masses connected by springs, that is, a discrete deformable model <ref type="bibr">[28]</ref>. Let node i , where i = 1, . . . , N , where the small-strain stiffness Qk is smaller than the largestrain stiffness / &amp;. Like real dermal tissue, this biphasic spring is readily extensible at low strains but exerts rapidly increasing restoring stresses after reaching a threshold e' (Fig. <ref type="figure" target="#fig_2">2</ref>).</p><p>We assemble the tissue model by arranging biphasic springs into structurally stable tetrahedral and hexahedral elements.</p><p>Diagonal springs strut each face of the hexahedral elements so that they will resist shearing. Fig. <ref type="figure" target="#fig_3">3</ref> illustrates a small patch of the facial tissue model consisting of three layers of elements representing the cutaneous tissue, subcutaneous tissue, and muscle layer (the layers are not shown to scale). The biphasic springs (line segments) in each layer have different stiffnesses in accordance with the inhomogeneity of real facial tissue. The top-most surface represents the epidermis (which is a rather stiff layer of keratin and collagen), and we set the spring stiffnesses to make it moderately resistant to deformation. The biphasic springs underneath the epidermis represent the dermis. The springs in the second layer are highly deformable, reflecting the nature of subcutaneous fatty tissue. Nodes on the bottom-most surface of the second layer represent the fascia to which the muscle fibers in the third layer are attached. Nodes on the bottom surface of the third layer are fixed (in "bone").</p><p>To account for the incompressibility of the cutaneous ground substance and the subcutaneous fatty tissues, we include a constraint into each element that minimizes the deviation of the volume V, of a deformed element Ej from its natural volume yo at rest. The volumes of elements are readily computable using vector algebra. The tissue incompressibility constraint is given by Q = c,(V, -yo)2. Differentiation of the constraint yields a net volume restoration force q, for each node i :</p><formula xml:id="formula_0">q, = d Q / d x , .</formula><p>Note that the derivative at a given node involves nonzero terms only over elements that share the node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Numerical Simulation of Facial Tissue</head><p>other nodes j E Nt in the deformable lattice is</p><p>The total force on node i due to springs that connect it to</p><formula xml:id="formula_1">gz(t) = S k . (3) j € N ,</formula><p>The discrete Lagrange equations of motion for the dynamic nodelspring system is the system of second-order ordinary differential equations d2xa dx, m , ~+ : I I -+ g t + q , = f , : dt</p><formula xml:id="formula_2">L = 1 . . . . , 1v<label>(4)</label></formula><p>where ya is the coefficient of velocity-proportional damping dissipating kinetic energy in the lattice, g, is the net spring force (3), q, is the net volume restoration force, and f, is the net external force acting on node 2 . It is possible for facial muscle fibers to displace specific attachment nodes by applying driving forces f, to them. In our current face model, however, the f, are not used. Instead, inextensible muscle fibers displace attachment nodes by directly modifying their positions x,, as the following section will explain.</p><p>To simulate the dynamics of the deformable lattice, initial positions x : (as determined by the face assembly procedure; see below) and velocities vp = 0 are provided for each node L , and the equations of motion are numerically integrated forward though time. Each time step requires the evaluation of forces, accelerations, velocities, and positions for all of the nodes. The explicit Euler method is a simple and quick timeintegration method, but it has a limited range of stability <ref type="bibr" target="#b25">[24]</ref>. Unfortunately, the greater computational complexity per time step of inherently more stable numerical methods can compromise interactive performance. A satisfactory solution to the stability/complexity tradeoff is provided by a secondorder Runge-Kutta method, which requires two evaluations of the nodal forces per time step.</p><p>We choose m, and 7, such that the facial tissue exhibits a slightly overdamped behavior. The overdamped dynamics, the high flexibility of the biphasic springs in the small-strain region, and the use of muscle fiber displacements rather than driving forces all contribute to enhance the stability of the numerical simulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Facial Muscle Control Process</head><p>Muscles are bundles of muscle fibers working in unison. The shape of the fiber bundle determines the muscle type and its functionality. There are three main types of facial muscles: linear, sphincter, and sheet. Linear muscle, such as the zygomaticus major (which attaches to and raises the corner of the mouth), consists of a bundle of fibers that share a common emergence point in bone. Sheet muscle, such as the occipito frontalis (which attaches to and raises the eyebrow), is a broad, flat sheet of muscle fiber strands without a localized emergence point. Sphincter muscle consists of fibers that loop around facial orifices and can draw toward a virtual center; an example is the orbicularis oris, which circles the mouth and can pout the lips.</p><p>In the human face, more than 200 voluntary muscles can exert traction on the facial tissue to create expressions. When the muscles contract, they pull the facial soft tissue to which they attach toward the place where they emerge from the underlying bony framework of the skull. <ref type="bibr">Waters [33]</ref> and others have achieved a broad range of facial expressiveness by incorporating about 20 muscle actuators into their geometric face models.</p><p>In our physics-based face model, muscle actuators run through the third layer of the synthetic tissue (Fig. <ref type="figure" target="#fig_3">3</ref>). Muscles fibers emerge from some nodes fixed in "bone" at the bottom of the third layer and attach to mobile nodes on the upper surface of the layer (fascia).</p><p>Let m: denote the point where muscle i emerges from the "bone," and m: its point of attachment in the tissue. These two points specify a muscle vector mt = m: -mp. The displacement of node J in the fascia layer from xJ to xi due to muscle contraction is a weighted sum of m muscle activities acting on node j: where 0 5 c, 5 1 is a contraction factor, and b,, is a muscle blend function that specifies a radial zone of influence for the geometric model <ref type="bibr">[33]</ref>. The FACS AU's are grouped into those that affect the upper and lower faces, and they include vertical actions, horizontal actions, oblique actions, orbital actions, and miscellaneous actions such as nostril shape, jaw drop, and head and eye position. Through the FACS abstraction, it is possible to suppress the low-level details of coordinated muscle actuation and provide an interface to the model in terms of high-level expression commands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Assembling and Simulating the Model</head><p>The automatic face model assembly procedure starts with a nonuniform triangular facial mesh whose nodes and springs represent the epidermis. First, it projects normal vectors from the center of gravity of each triangle into the face to establish subcutaneous nodes and forms terahedral dermal elements by connecting them to epidermal nodes using dermal springs. Second, it forms hexahedral subcutaneous elements by attaching short weak springs from the subcutaneous nodes downwards to muscle layer nodes. Third, it adds the muscle layer of hexahedral elements, whose lower nodes are constrained, anchoring them in "bone." Finally, it inserts the muscle fibers through the muscle layer from their emergence in "bone" to their attachments at muscle layer nodes.</p><p>Fig. <ref type="figure" target="#fig_4">4</ref> shows the epidermal triangles and 14 muscle vectors (the dermal and subcutaneous layers are suppressed for clarity) after the automatic assembly starting from the facial mesh employed in <ref type="bibr">[33]</ref>. The synthetic tissue includes about 960 elements with approximately 6500 springs in total. The physics-based face model can be simulated and rendered at interactive rates on a single CPU of a Silicon Graphics Iris 4D-340VGX workstation. Fig. <ref type="figure" target="#fig_1">1</ref> shows several frames from a videotape recorded in real-time as the user interacted with the model through a menu-driven, mid-level interface enabling the contraction of individual muscles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I v . PERSONAL FACE MODELS FROM SCANNED DATA</head><p>It is possible to enhance the realism of the face model dramatically through texture mapping, which is a widely adopted technique in model-based facial image coding. We describe initial work in this direction in <ref type="bibr">[34]</ref> as well as recent work in <ref type="bibr">[19]</ref>.</p><p>More specifically, our polygonal face model is useful for capturing the 3-D geometry of faces from scanned data. For example, Fig. <ref type="figure" target="#fig_5">5</ref> shows a 360' head-to-shoulder scan of a woman (Heidi, which was acquired by Cybenvare, Inc.) using a Cybenvare Color 3-D Digitizer. The data set consists of a radial range map (Fig. <ref type="figure" target="#fig_6">5(a)</ref>) and a registered RGB photometric map (Fig. <ref type="figure" target="#fig_6">5(b)</ref>). The range and RGB maps are high-resolution 512x256 arrays in cylindrical coordinates, where the x axis is the latitudinal angle around the head, and the y axis is vertical distance. Fig. <ref type="figure" target="#fig_6">5(c</ref>) shows the epidermal mesh of Fig. <ref type="figure" target="#fig_4">4</ref> radially projected into the 2-D cylindrical domain and overlayed on the RGB map. The triangle edges in the mesh are stretchy springs, and the mesh has been conformed semi-interactively to the woman's face using both the range and RGB maps [34], <ref type="bibr">[19]</ref>. The nodes of the conformed mesh serve as sample points in the range map. Their cylindrical coordinates and the sampled range values are employed to compute 3-D Euclidean space coordinates for the polygon vertices. In addition, the nodal coordinates serve as polygon vertex texture map coordinates into the RGB map. Fig. <ref type="figure" target="#fig_5">5</ref>(d) shows the 3-D facial mesh with the texture-mapped photometric data.</p><p>The visual quality of the face model is comparable to a 3-D display of the original high resolution data, despite the significantly coarser mesh geometry. We can visualize the texture-mapped model from arbitrary viewpoints at interactive rates on the SGI workstation that implements texture mapping in hardware.</p><p>Once we have reduced the scanned data to the 3-D epidermal mesh of Fig. <ref type="figure" target="#fig_6">5(d)</ref>, we can assemble a physics-based face model of Heidi using the assembly procedure described in the previous section. Fig. <ref type="figure" target="#fig_5">5</ref>(e) and (f) demonstrates that we can animate the resulting face model by activating muscles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>v. ANALYSIS OF DYNAMIC FACIAL IMAGES</head><p>In this section, we consider the inverse problem to facial image synthesis, i.e., the analysis of images of expressive faces. Our specific goal is to infer dynamic muscle contraction parameters that may be used to drive the physics-based model. This problem is challenging because it requires the reliable estimation of quantitative information about extended facial features that are moving nonrigidly. We develop a method that enables us to capture dynamic facial expressions directly from video sequences.</p><p>Through straightforward image processing, we convert digitized image frames into 2-D potential functions whose ravines (extended local minima) correspond to salient facial features such as the eyebrows, mouth, and chin. We employ a variant calls for deformable contours that have some viscoelasticity and rigidity. We define a discrete deformable contour as a set of n nodes indexed by i = 1. . . . , n. We associate with these nodes time-varying positions xz(t) = [zcp(t), y Z ( t ) ] ' , along with "tension" forces a z ( t ) , "rigidity" forces @,(t), and external forces f,(t) that act in the image plane.'</p><p>We connect the nodes in series using nonlinear springs.</p><p>Following the formulation of (l), let I , be the given reference</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Discrete Deformable Contour Models</head><p>'Note that although the vectors x, and f, for the deformable contour are analogous to those found in the dynamics equations (4) of the synthetic facial tissue model, they are different, two-component vectors in the ensuing A contour can be thought Of as an energy minimizing spline in the z-y image plane. The present application discussion. length of the spring connecting node 1 to node I + 1, and let r t ( t ) = x,+1 -x, be the separation of the nodes. We want the spring to resist compression only when its actual length Ilr,il is less than 1,. Hence, given the deformation r l ( t ) = /lr211 -l , , we define the tension force <ref type="bibr" target="#b6">(7)</ref> where the a, 's are tension variables. A viscoelastic contour may be obtained by letting where vi is a coefficient of viscoelasticity. Introducing rigidity variables 6;, the rigidity force is The behavior of an interactive deformable contour is governed by the first-order dynamic system</p><formula xml:id="formula_3">dX lit Y-+a,+p, = f , : i = 1. . 71,</formula><p>where y is a velocity-dependent damping coefficient. Tension and rigidity are locally adjustable though the o, and 6, variables. In particular, by setting a, = 6, = 0, we are able to break a deformable contour to create several shorter contours on an image.</p><p>To simulate the deformable contour, we integrate the system of ordinary differential equations (10) forward through time using a semi-implicit Euler method <ref type="bibr" target="#b25">[24]</ref>. Applying the forward finite difference approximation dxl/dt z (xj+If -xj)/At to <ref type="bibr">(lo)</ref>, evaluating the linear terms in the x, (i.e., p,) at time t + At and the nonlinear terms at time t yields the pentadiagonal system of algebraic equations for the new node positions x l + l f in terms of the current positions x:. Since the system has a constant coefficient matrix, we factorize it only once at the beginning of the deformable contour simulation using a direct LDU factorization method and then efficiently resolve with different right-hand sides at each time step (see <ref type="bibr">[27]</ref> for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. External Forces and Imuge Processing</head><p>The deformable contour is responsive to an image force field that influences its shape and motion. It is convenient to express the force field as the gradient of a time-varying potential function P ( . i . y. t ) . A user may also interact with the deformable contour by applying forces fF(t) using a mouse (see <ref type="bibr" target="#b14">[14]</ref> for details about user forces). Combining the two types of forces, we have <ref type="bibr" target="#b12">(12)</ref> f, = y Y P ( <ref type="figure">x ,</ref><ref type="figure">) +,</ref><ref type="figure">,</ref><ref type="figure">,</ref><ref type="figure">I</ref> where 11 is the strength of the image forces and V = In the present application. we are concerned with the localization of extended image features such as the eyebrow and lip boundaries. Usually, these features correspond to highcontrast regions in the image intensity function I ( z . y. t ) . To make these regions deformable contour attractors, we use where G,* denotes convolution with a 2-D Gaussian smoothing filter of width IT, which broadens the ravines of P so that they attract the contours from a distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Tracking Nonrigid Facial Features</head><p>In a few simulation time steps, the deformable contours slide downhill in P ( x , y, t k ) (image frame IC), conforming to the shapes of its ravines as they come to equilibrium at their bottoms. Once they equilibrate, the contours accurately trace the facial features of interest. As soon as the contours have equilibrated in P ( x , y, t k ) , we replace it with P ( z , y, t k + l ) associated with the next video frame. Continuing from their previous equilibrium positions, the contours slide downhill to again equilibrate in the perturbed ravines, thus tracking their nonrigid motions. We repeat the process on successive frames.</p><p>This simple tracking scheme works if the motion of the facial features of interest is small enough to retain the contours on the slopes of the perturbed ravines along most of their lengths. Should part of a contour escape the attractive zone of a ravine, however, the rest of the contour will usually pull it back into place.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Estimating Facial Muscle Contractions</head><p>As the deformable contours evolve from frame to frame, their dynamic state variables X : and their time derivatives provide explicit information about the nonrigid shapes and motions of the facial features. The information is reduced to a head reference frame and 11 dynamic fiducial points.</p><p>In our muscle contraction estimation process, we have employed, to date, nine deformable contour sections. These localize and track the hairline, the left and right eyebrows, the left and right nasolabial furrows, the tip of the nose, the upper and lower lips, and the chin boss. Using the deformable contour state variables, an automatic procedure first calibrates the input image to the face model and then computes the following:</p><p>A head reference frame from the average position of the hairline contour contractions of the left and right inner, major, and outer occipitofrontalis from the positions of the inner-most, center, and outer-most points of the associated eyebrow contours, respectively contractions of the left and right zygomaticus major and depressor labii inferioris from the positions of the endpoints of the upper lip contour contraction of the left and right levator labii superioris alaeque nasi from the positions of the upper-most points of the associated nasolabial furrow contours jaw rotation from the average position of the chin boss contour. Fig. <ref type="figure" target="#fig_8">6</ref> illustrates the positions of the nine deformable contours in equilibrium at two different frames of an image sequence that will be described in the next section. The dots indicate the 11 fiducial points mentioned above, which are computed from the snakes. The positions of the points are computed relative to the head reference frame, whose origin is marked by the crosshair in the figure. Assuming a frontal view and relatively stable hairline, the head reference frame will track the head motion in the image.</p><p>The muscle contraction estimation scheme makes the simplifying assumption of orthographic projection. It estimates ci(t) in the 2-51 image plane using <ref type="bibr" target="#b4">(5)</ref> for each muscle independently while ignoring z coordinates. Starting with an image of the neutral face, the calibration procedure establishes the origin of the head reference frame and positions the muscle vector emergence points appropriately with respect to it. The snake fiducial points in Fig. <ref type="figure" target="#fig_8">6</ref>(a) serve to approximate the positions of the muscle attachments. Once the natural lengths of the primary facial muscles have been established, the muscle contractions that are responsible for a dynamic expression may be estimated immediately as the snake fiducial points move from frame to frame. The jaw rotation is also estimated as the chin snake fiducial point descends from its neutral position in the head reference frame.</p><p>The next section illustrates this dynamic estimation procedure with an example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. An Experiment in Facial Kdeo Analysis</head><p>We have applied our facial image analysis technique to a video sequence of one of the authors (DT) performing facial expressions in frontal view before a CCD camera.2 A surprise expression was digitized as a sequence of 2 5 6 ~2 5 6 x 8 -b images and analyzed using deformable contours. Fig. <ref type="figure" target="#fig_9">7</ref> illustrates the facial image analysis and the results of the muscle contraction estimation on three image frames. Fig. <ref type="figure" target="#fig_9">7(b)</ref> shows the (negative) potential functions computed from the frames in Fig. <ref type="figure" target="#fig_9">7(a</ref>). To compute the potential, we apply a discrete smoothing filter G ( i , j ) consisting of two four-neighbor local intensity averaging steps followed by the discrete gradient</p><formula xml:id="formula_4">operator V v ( i , j ) = [ ( ~( i + 1 , j ) -u ( i , j ) ) , ( u ( z , j + 1) - ~( i , j))]'.</formula><p>We bilinearly interpolate the result between pixels ( i , j ) to obtain the continuous potential function P ( x , y, t k ) .</p><p>We initialize the deformable contours on the first frame of the sequence using the mouse. The initialization procedure places their nodes roughly 1 pixel apart and sets the rest of the lengths Zi in (8) to the initial node separations. The parameter values of the deformable contour simulation are y / A t = 0.5, a, = 1.0 and bi = 0.5 (except at the jump discontinuities between the contours where a, = bi = 0.0), vi = 0.2, and From the first frame in the video sequence, which captures DT's face in a relaxed state, the analysis procedure first calibrates the face model to the frontal view of the subject and then estimates dynamic muscle contractions c , ( t ) , as we explained in the previous section. Fig. <ref type="figure" target="#fig_10">8</ref> shows a plot of the estimated contractions versus the frame number. They are input to the physics-based model as a time sequence. The model quickly attains dynamic equilibrium on each frame input, and the state variables are rendered in real time on the SGI workstation to synthesize an animated sequence of facial images, three of which are shown in Fig. <ref type="figure" target="#fig_9">7(c)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DISCUSSION</head><p>Our experiment has demonstrated the estimation of muscle contractions from video of a subject's face and their use in resynthesizing facial expression. Clearly, our technique is tolerant of the significant discrepancy between the 3-D geometry of the subject's face and the face model. It is difficult to assess quantitative accuracy, however, because ground-truth data are not readily available. Even though some contraction estimation errors may be quantitatively significant, we have noted that the expression resynthesis remains qualitatively robust.</p><p>The simple feed-forward analysis/synthesis scheme that we describe in this paper has some limitations. At present, the snakes require manual initialization, but we are confident that some heuristic facial feature detection procedure, like the one described in <ref type="bibr" target="#b13">[13]</ref>, can be modified to initialize them adequately. Although it is very efficient, the 2-D nature of the muscle-estimation scheme can cause problems in general, e.g., when the head turns significantly. Three-dimensional muscle contraction estimation assuming full, and not necessarily frontal, perspective projection is desirable. We can probably accomplish this in the future by exploiting multiple views of the face. It is evident that we can also improve the fidelity of the image-based facial expression analysis and graphical resynthesis loop with a more complete modeling of facial musculature. A limitation of the present facial model is the lack of an adequate model of the orbicularis oris, which is the highly articulate sphincter muscle that defines the lips. Once we incorporate a more sophisticated lip actuator, it will make sense to exploit more of the nodal variables available in the lip tracking snakes, rather than the two fiducial points that we currently employ. Another deficiency, at present, is the lack of eyelid and eye position estimation that results in discernible differences between the input expression and resynthesized expression. The eyelid is particularly difficult to track because of its speed. A more feasible solution would be to simply detect whether the eyelids are open or shut and input this information to the model.</p><p>Our work opens up many avenues for further research. For example, it seems possible to further automate the modeling approach that we have developed for working with scanned data and possibly extend it to the reconstruction of faces from grey-level images. Another interesting research direction would investigate the possibility of running the hierarchical model backward from the image level all the way up to the expression level, thereby addressing the problems of measuring, classifying, and recognizing dynamic human expressions from video sequences of the face. The FACS representation promises to be useful in addressing the expression recognition problem, as Mase [21] has argued recently.</p><p>Our demonstration that it is possible to analyze a particular face captured on video and reconstruct, with reasonable degree of accuracy, the expression in the different facial geometry of the model affirms the notion that muscle actions are the salient features of expression that are common across individual faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>A solid foundation for facial image analysis is the anatomy of the face, especially the arrangements and actions of the primary facial muscles. This paper has presented a new approach to facial image analysis using a realistic facial model. We have described a hierarchical model of the human face that incorporates a physics-based synthetic facial tissue and a set of anatomically motivated facial muscle actuators. Despite its sophistication, the model is efficient enough to produce facial animation at interactive rates on a graphics workstation. We use snakes to track the position of the head and the nonrigid motions of the eyebrows, nasal furrows, mouth, and jaw in the image plane. Reducing the snake measurements to fiducial points within a head reference frame, we are able to estimate the dynamic contractions of the primary facial muscles. These estimates make appropriate control parameters for resynthesizing facial expressions through our face model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Manuscript received October 10, 1991; revised December 1, 1992. This work was supported by the Natural Sciences and Engineering Research Council of Canada and the Information Technology Research Center of Ontario. Recommended for acceptance by T. Huang and P. Stucki. D. Terzopoulos is with the Department of Computer Science, University of Toronto, Toronto, Canada M5S 1A4. K. Waters is with Digital Equipment Corporation, Cambridge Research Laboratory, Cambridge, MA 02139. IEEE Log Number 9209281.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Images synthesized by the face model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Stress-strain curve of facial tissue and its biphasic approximation.The large-strain threshold e' occurs at the intersection of the two lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Trilayer facial tissue model: (a) Top view; (b) side view showing (right to left) epidermal surface, dermal layer (pentahedral elements), and subcutaneous and muscle layers (hexahedral elements).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Epidermal mesh and 16 muscle vectors (dark lines).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Facial modeling using scanned data: (a) Radial range map; (b) RGB photometric map; (c) RGB map with conformed epidermal mesh overlayed; (d) 3-D mesh and texture mapped triangles.of deformable contour models, or snakes, introduced in [14] and [27]. The deformable contours lock onto the ravines, tracking them from frame to frame. The deformable model's state variables provide quantitative information about the nonrigid shapes and motions of the evolving facial features. The automatic interpretation of this information leads to dynamic muscle parameters that allow the face model to reconstruct motions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Facial modeling using scanned data: (e). (f) Animate face model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>[ d / d . r . d/d!/]'. P(.r.,y.t) = -lITGu * I ( . r . y , t ) ~~ (13)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Snakes and fiducial points used for muscle contraction estimation: (a) Neutral face; (b) surprise expression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Dynamic facial image analysis and expression resynthesis. Sample video frames with superimposed deformable contours trackingfacial features; (a) intensity images with black snakes, (b) imagepotentials with white snakes. (c) Facial model resynthesizes surpriseexpression from estimated muscle contractions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Conuxuon"Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Estimated muscle contractions plotted as time series.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>2Using the available video camera and lighting in our lab environment, it was necessary to enhance DT's lips, eyebrows, and nasolabial furrows by subjecting him to a humiliating makeup job. Under more favorable imaging situations, makeup may not be necessary, depending on the individual.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>scientific visualization. From 1988 to 1991, he was a member of the technical staff at the Schlumberger Laboratory for Computer Science, Austin, TX, where he worked on 3-D visualization of seismic and borehole data. His current research interests include medical facial applications, physics-based modeling, computer-based facial synthesis, and volume visualization. He has published papers in computer graphics, computer vision, and biomedical visualization.Dr. Waters serves on the editorial board for the Journal of Ksualization and Computer Animation and is a member of the ACM Siggraph.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We thank Y. Lee for his contributions to our facial modeling research. He wrote the algorithms that generated the images for Fig. <ref type="figure">5</ref> and constructed the physics-based face models of us shown in the biography photos. Scanned data were provided courtesy of Cyberware, Inc., Monterey, CA. We also thank R. Smith and I. Chakravarty for their support at Schlumberger LCS and T. Crossley for his assistance with image digitization.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Model-based analysis synthesis image coding (MBASIC) system for a person&apos;s face</title>
		<author>
			<persName><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Harashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Commun</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="139" to="152" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Man-machine facial recognition</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Bledsoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Panoramic Res. Inc</title>
		<imprint>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="1966-08">Aug. 1966</date>
			<pubPlace>Palo Alto, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Recognizing Faces</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bruce</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Analysis and synthesis of facial expressions in knowledge-based coding of facial image sequences</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Harashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Takebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf Acoustics Speech Signal Processing</title>
		<meeting>Int. Conf Acoustics Speech Signal essing</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="2737" to="2740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Shepherd</surname></persName>
		</author>
		<title level="m">Perceiving and Remembering Faces</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A finite element analysis of surgery of the human facial tissues</title>
		<author>
			<persName><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graduate Sch. Arts Sci</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Columbia Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Unmasking the Human Face</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Manual for the Facial Action Coding System</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Lawrence Erlbaum</publisher>
			<pubPlace>Hillsdale</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m">Consulting Psychologists</title>
		<meeting><address><addrLine>New York; Palo Alto</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="1977">1981. 1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The representation and matching of pictorial structures</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Elschlager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="67" to="92" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image coding-From waveforms to animation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Forchheimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kronander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoustics Speech Signal Processing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2008" to="2023" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Man-machine interaction in human face identification</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Harmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Lesk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell System Tech. J</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">399427</biblScope>
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Animating speech: A automated approach using speech synthesised by rules</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wyvill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Kwal Comput</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Picture processing system by computer complex and recognition of human faces</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. of Inform. Sci., Kyoto Univ</title>
		<imprint>
			<date type="published" when="1973-11">Nov. 1973</date>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Snakes: Active contour models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="321" to="331" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A basic study on human face recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers of Pattern Recognition</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1972">1972</date>
			<biblScope unit="page">265</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tissue mechanics</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Kenedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Barbenel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="699" to="717" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human skin model capable of natural shape variation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Komatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Comput</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="265" to="271" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A finite element model of skin deformation I. Biomechanics of skin and soft tissue: A review</title>
		<author>
			<persName><forename type="first">W</forename><surname>Larrabee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">111. The finite element model</title>
		<imprint>
			<date type="published" when="1986">1986. 1986. 1986</date>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page">413419</biblScope>
		</imprint>
	</monogr>
	<note>Laryngoscope</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Constructing physics based facial models of individuals</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Graphics Interface &apos;93</title>
		<meeting>Graphics Interface &apos;93<address><addrLine>Toronto</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-05">May 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automated lipsynch and speech synthesis for character animation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">I</forename><surname>Parke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Human Factors Comput. Syst. Graphics Interface &apos;87</title>
		<meeting>Human Factors Comput. Syst. Graphics Interface &apos;87<address><addrLine>Toronto</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="14S" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recognition of facial expression from optical flow</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Trans</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3474" to="3483" />
			<date type="published" when="1991-10">Oct. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Parameterized models for facial animation</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">1</forename><surname>Parke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graphics Applications</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="61" to="68" />
			<date type="published" when="1982-11">Nov. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">More than skin deep: Physical modeling of facial tissue</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pieper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. of Media A r t s Sci., Mass. Inst. of Techno</title>
		<imprint>
			<biblScope unit="page" from="277" to="287" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
	<note type="report_type">M.Sc. thesis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Cambridge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989-01">Jan. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Flanney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Teukolsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Verttering</surname></persName>
		</author>
		<title level="m">Numerical Recipes: The Art of Scientific Computing</title>
		<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Line extraction and pattern detection in a photograph</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nagao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jujibayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Putt. Recogn</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">233</biblScope>
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Classification of facial features for recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Shakleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Welsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Vision Putt. Recogn. Conj (CVPR&apos;91</title>
		<meeting>Comput. Vision Putt. Recogn. Conj (CVPR&apos;91<address><addrLine>Lahaina, HI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="573" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On matching deformable models to images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Topical Meeting Machine Vision</title>
		<title level="s">Tech. Digest Series</title>
		<meeting>Topical Meeting Machine Vision<address><addrLine>Washington, DC; Palo Alto, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986">1987. Nov. 1986</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="16G" to="163" />
		</imprint>
	</monogr>
	<note>Opt. Soc. Amer.</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deformable models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fleischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Comput</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="306" to="331" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Analysis of dynamic facial images using physical and anatomical models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Third Int. Conf Comput. Vision (ICCV&apos;90</title>
		<meeting>Third Int. Conf Comput. Vision (ICCV&apos;90<address><addrLine>Osaka</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="727" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Abstract muscle action procedures for face animation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Magnenat-Thalmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Primeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Comput</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="29" to="297" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Feature extraction from faces using deformable templates</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Hallinan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. &amp;ion Putt. Recogn. Conj</title>
		<meeting>Comput. &amp;ion Putt. Recogn. Conj<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989-06">June 1989</date>
			<biblScope unit="page" from="104" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Head boundary location using snakes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Waite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Welsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brit. Telecom Tech. J</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="127" to="136" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A muscle model for animating three-dimensional facial expression</title>
		<author>
			<persName><forename type="first">K</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="17" to="24" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Modeling and animating faces using scanned data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Waters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J . Visualization Comvut. Animation</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="123" to="128" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">xmance-driven facial animation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="235" to="242" />
		</imprint>
	</monogr>
	<note>Perf</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">From 1985-1992, he was affiliated with Schlumberger, Inc., serving as Program Leader at the Laboratory for Computer Science, Austin, TX, and at the former Palo Alto Research Laboratory. During 19861985, he was a Research Scientist at the MIT Artificial Intelligence Laboratory, Cambridge, MA. Previously, he held summer positions at the National Research Council of Canada</title>
		<author>
			<persName><forename type="first">Demetri</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Terzopoulos has received several scholarships and awards, including an AAAI-87 Conference Best Paper Award from the American Association for Artificial Intelligence. He serves on the editorial boards of CVGIP: Graphical Models and Image Processing and the Journal of Visualization and Computer Animation</title>
		<title level="s">He received the B.Eng. degree with distinction in honours electrical engineering and the M.Eng. degree in electrical engineering from McGill University</title>
		<editor>
			<persName><forename type="first">Greece</forename><surname>Krestena</surname></persName>
		</editor>
		<meeting><address><addrLine>Montreal, Canada; Cambridge, MA; Ottawa, Canada; Northem Research, Montreal, Canada; London</addrLine></address></meeting>
		<imprint>
			<publisher>Academy of Sciences and Sigma Xi</publisher>
			<date type="published" when="1956">1956. 1978. 1980. 1984. 1977-1978. 1980</date>
		</imprint>
	</monogr>
	<note>respectively, and the Ph.D. degree in artificial intelligence from the Massachusetts Institute of Technology. Keith Waters (M&apos;92) was born in Kent, England, in 1962. He received the B.A. honours degree in graphic design in 1984 and the Ph.D. degree in computer graphics in 1988 from Middlesex Polytechnic</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">he has been a member of the research staff at Digital Equipment Corporation&apos;s Cambridge Research Lab</title>
		<imprint>
			<date type="published" when="1991-08">August 1991</date>
		</imprint>
	</monogr>
	<note>where he is involved in</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
