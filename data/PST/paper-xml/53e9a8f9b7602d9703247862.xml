<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dimension-wise integration of high-dimensional functions with applications to finance</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2010-07-01">1 July 2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Michael</forename><surname>Griebel</surname></persName>
							<email>griebel@ins.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Numerical Simulation</orgName>
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<addrLine>Wegelerstr. 6</addrLine>
									<postCode>53115</postCode>
									<settlement>Bonn</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Markus</forename><surname>Holtz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Numerical Simulation</orgName>
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<addrLine>Wegelerstr. 6</addrLine>
									<postCode>53115</postCode>
									<settlement>Bonn</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dimension-wise integration of high-dimensional functions with applications to finance</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2010-07-01">1 July 2010</date>
						</imprint>
					</monogr>
					<idno type="MD5">C95255B64D020AFF8922A5A4CE7D3BB4</idno>
					<idno type="DOI">10.1016/j.jco.2010.06.001</idno>
					<note type="submission">Received 24 February 2009 Accepted 22 June 2010</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>ANOVA decomposition Numerical integration Sparse grids Effective dimension</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a new general class of methods for the computation of high-dimensional integrals. The quadrature schemes result by truncation and discretization of the anchored-ANOVA decomposition. They are designed to exploit low effective dimensions and include sparse grid methods as special case. To derive bounds for the resulting modelling and discretization errors, we introduce effective dimensions for the anchored-ANOVA decomposition. We show that the new methods can be applied in locally adaptive and dimension-adaptive ways and demonstrate their efficiency by numerical experiments with high-dimensional integrals from finance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>High-dimensional integrals appear in various mathematical models from physics, chemistry and finance. Their large number of dimensions arises, e.g., from small time steps in time discretizations and/or a large number of state variables. Examples from finance are option pricing, bond valuation or the pricing of collateral mortgage backed securities. In most cases, the arising integrals cannot be calculated analytically and numerical methods must be applied. Here, one of the key prerequisites is that the curse of dimension can be avoided at least to some extent. The curse of dimension states that the cost to compute an approximation with a prescribed accuracy ε depends exponentially on the dimension d of the problem. It is one of the main obstacles for a conventional numerical treatment of high-dimensional problems; see, e.g., <ref type="bibr" target="#b10">[11]</ref>. Classical quadrature methods for the computation of multivariate integrals (e.g. based on product rules) which use n function evaluations achieve an accuracy of ε(n) = O(n -r/d ) for functions with bounded derivatives up to order r; see, e.g., <ref type="bibr" target="#b4">[5]</ref>. For fixed r, their convergence rates thus decrease exponentially with increasing dimension. On the positive side, the case r = d indicates that the problem of a high dimension can sometimes be compensated by, e.g., a high degree of smoothness. Also other aspects such as the concentration of measure phenomenon 1 or the superposition theorem of Kolmogorov 2 show that there is some chance to treat high-dimensional problems despite the curse of dimension.</p><p>Furthermore, the curse of dimension can be approached from the point of numerical complexity theory. There it is shown that for some integration problems even the best algorithm of a specific algorithm class cannot avoid the curse of dimension. Such problems are therefore called intractable, see, e.g. <ref type="bibr" target="#b34">[35]</ref>. However, application problems are often in different or smaller problem classes and thus may be tractable. In addition, there may exist algorithms from other settings which are able to break the curse of dimension. Randomised algorithms (e.g. Monte Carlo methods) form one such class of algorithms. For square integrable functions, the expected mean square error of the Monte Carlo method with n samples is</p><formula xml:id="formula_0">ε(n) = O(n -1/2 )</formula><p>and is thus independent of the dimension. 3 Nevertheless, the convergence rate is quite low and a high accuracy is only achievable with tremendously many function evaluations. Deterministic numerical integration schemes, such as quasi-Monte Carlo methods (see, e.g., <ref type="bibr" target="#b9">[10]</ref>) and sparse grids (see, e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31]</ref>) are alternatives to the Monte Carlo method, which can attain faster rates of convergence. The error of quasi-Monte Carlo methods can be shown to be</p><formula xml:id="formula_1">ε(n) = O(n -1 (log n) d )</formula><p>for integrands of bounded variation and thus decreases with the number of function evaluations n asymptotically faster than the expected mean square error of the Monte Carlo method. Sparse grids as introduced in <ref type="bibr" target="#b30">[31]</ref> achieve</p><formula xml:id="formula_2">ε(n) = O(n -r (log n) (d-1)(r+1) )</formula><p>for integrands which have bounded mixed partial derivatives of order r and can thus also make use of higher smoothness of the integrand. The convergence rates of quasi-Monte Carlo and sparse grid methods still exhibit a logarithmic dependence on the dimension, however. Furthermore, the implicit constants depend on d and often increase exponentially with the dimension. For problems with higher dimensions the asymptotic advantages of the deterministic numerical methods over the Monte Carlo method might thus not pay off for practical sample sizes n. <ref type="foot" target="#foot_0">4</ref>However, for many problems of finance, numerical experiments show that quasi-Monte Carlo methods converge nearly independent of the dimension and are faster and more accurate than Monte Carlo, see, e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25]</ref>. For sufficiently smooth integrands, similar results have been observed for sparse grid methods, see <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. One explanation of this success is based on the analysis of variance (ANOVA) decomposition of the integrands. There it turns out that in many finance applications the importance of each dimension is naturally weighted by certain hidden weights where with the increase of dimension the lower-order terms continue to play a significant role and the higher-order 1 The concentration of measure phenomenon says that every Lipschitz function on a sufficiently high-dimensional domain is well approximated by a constant function.</p><p>2 The theorem of Kolmogorov shows that every continuous function of several variables can be represented by the superposition of continuous functions with only one variable. 3 The probabilistic error of a Monte Carlo estimate with n samples is σ 2 (f )/ √ n. Here, the term n -1/2 describes the convergence rate and the variance σ 2 (f ) can be considered as the constant of the Monte Carlo method.</p><p>terms tend to be negligible, see, e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b38">39]</ref>. Moreover, often coordinate transformations, such as the Brownian bridge, see <ref type="bibr" target="#b19">[20]</ref>, the PCA construction, see <ref type="bibr" target="#b0">[1]</ref>, or the LT construction, see <ref type="bibr" target="#b14">[15]</ref>, can be used to enforce the importance of the first few dimensions. The integrands are thus of so called low effective dimension and can be well approximated by a sum of low-dimensional functions. In <ref type="bibr" target="#b3">[4]</ref>, two notions of effective dimension have been introduced, the truncation and the superposition dimension. Quasi-Monte Carlo methods profit from a low superposition dimension by their well-distributed low-dimensional projections and from a low truncation dimension by the fact that their points are usually more uniformly distributed in smaller dimensions than in higher ones. Sparse grid methods can exploit low truncation and low superposition dimension in a very general way by a dimensionadaptive grid refinement. If the weights decay sufficiently fast with increasing dimension and if the integrands are of bounded mixed regularity then tractability of the deterministic algorithms can be proved, see <ref type="bibr" target="#b28">[29]</ref>. A further argument given in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19]</ref> states that the lower-order terms in the ANOVA decomposition are in certain cases smoother than the original function. This may explain the high convergence rates of the deterministic methods despite the fact that application problems do usually not fulfill the smoothness assumptions on bounded mixed regularity. While the classical ANOVA decomposition is very useful to analyse the importance of different dimensions and their interactions of a high-dimensional function it cannot be used as a tool for the design of new integration schemes since already the first term in the decomposition requires to integrate the function. In this article, we present a new general class of quadrature methods for the computation of high-dimensional integrals, which is based on the anchored-ANOVA decomposition. The anchored-ANOVA decomposition has the advantage that its sub-terms are much cheaper to compute than the terms of the classical ANOVA decomposition since instead of integrals only point evaluations are required. The new quadrature methods, which we refer to as dimension-wise quadrature methods, are defined in two steps: First, the anchored-ANOVA decomposition is truncated either a priori based on function space weights or in an dimension-adaptive fashion, which automatically detects the important terms of the decomposition. This truncation introduces a modelling error which can be shown to be small, however, if the integrand is of low effective dimension. Then, the integrals of the kept terms are computed using appropriate low-dimensional quadrature rules which may be different from term to term and may refine the approximation in a locally adaptive way. This introduces a discretization error which, however, only depends on the superposition dimension of the integrand and not on the nominal dimension d, if we correctly balance the costs of the quadrature methods with the importance of the corresponding anchored-ANOVA terms. We further show that the new method includes the class of generalised sparse grid methods, see <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b40">41]</ref>, as special case which results if we use particular tensor product methods for the integration of the sub-terms. This allows us to intertwine the truncation of the anchored-ANOVA series and the subsequent discretization and to balance modelling and discretization error in an optimal way. We demonstrate this approach in more detail for integrands from weighted tensor product Sobolev spaces.</p><p>We also present dimension-wise quadrature methods which are not of sparse grid form. Here we use the CUHRE method <ref type="bibr" target="#b1">[2]</ref> for the integration of the low-order anchored-ANOVA terms and quasi-Monte Carlo methods for the higher-order terms. This way, we obtain mixed CUHRE/QMC methods which are to our knowledge the first numerical quadrature methods which can profit from low effective dimension by dimension adaptivity and can at the same time resolve low regularity by local adaptivity to some extent. Numerical experiments with a test function from finance with discontinuous first derivatives demonstrates the efficiency of this new approach. We provide several more numerical experiments based on several application problems from finance with up to 512 dimension. The results show that our dimension-wise quadrature methods can profit significantly more than quasi-Monte Carlo methods from coordinate transformation, which reduce the effective dimension, such as the LT construction. Dimension-adaptive sparse grid methods based on univariate Gauss-Hermite formulae can in addition optimally profit from smoothness by addressing the arising integrals directly on R d . By exploiting both low effective dimension and smoothness, these methods can outperform quasi-Monte Carlo methods by several orders of magnitude even in hundreds of dimensions as our numerical results demonstrate.</p><p>The remainder of this article is organised as follows: In Section 2, we recall two different dimension-wise decompositions of multivariate functions, the classical ANOVA and the anchored-ANOVA decomposition, and give corresponding notions of effective dimensions. In Section 3, we then use the anchored ANOVA to define a new general class of methods for multivariate integration. In Section 4, we discuss its relation to sparse grid methods. In Section 5, we then specify the components of our general approach to balance error and costs for integrands of particular weighted tensor product Sobolev spaces. In Section 6, we show that the methods can be applied in a dimension-adaptive way. In Section 7, we present numerical results with applications from finance. The article finally closes in Section 8 with concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Dimension-wise decompositions</head><p>In this section, we introduce the classical ANOVA and the anchored ANOVA of a multivariate function f . Based on these decomposition, we then define different notions of effective dimensions of f . To this end, let Ω ⊆ R be a domain and let</p><formula xml:id="formula_3">dµ(x) = d j=1 dµ j (x j ) (1)</formula><p>denote a d-dimensional product measure defined on Borel subsets of Ω d . Here, x = (x 1 , . . . , x d ) and µ j , j = 1, . . . , d, are probability measures on Borel subsets of Ω. Let V (d) denote the Hilbert space of all functions f :</p><formula xml:id="formula_4">Ω d → R with the inner product (f , g) := Ω d f (x)g(x) dµ(x).</formula><p>For a given set u ⊆ D, where D := {1, . . . , d} denotes the set of coordinate indices, the measure µ induces projections P u :</p><formula xml:id="formula_5">V (d) → V (|u|) by P u f (x u ) := Ω d-|u| f (x)dµ D\u (x).<label>(2)</label></formula><p>Here, x u denotes the |u|-dimensional vector containing those components of x whose indices belong to the set u and dµ D\u (x) := j ∈u dµ j (x j ). The projections define a decomposition of f ∈ V (d) into a finite sum according to</p><formula xml:id="formula_6">f (x 1 , . . . , x d ) = f 0 + d i=1 f i (x i ) + d i,j=1 i&lt;j f i,j (x i , x j ) + • • • + f 1,...,d (x 1 , . . . , x d )</formula><p>which is often written in the more compact notation</p><formula xml:id="formula_7">f (x) = u⊆D f u (x u ).<label>(3)</label></formula><p>The 2 d terms f u describe the dependence of the function f on the dimensions j ∈ u with respect to the measure µ. They are recursively defined by</p><formula xml:id="formula_8">f u (x u ) := P u f (x u ) - v⊂u f v (x v )<label>(4)</label></formula><p>and can also be given explicitly by</p><formula xml:id="formula_9">f u (x u ) = v⊆u (-1) |u|-|v| P v f (x v ),<label>(5)</label></formula><p>see <ref type="bibr" target="#b15">[16]</ref>. The resulting decomposition (3) is unique for a fixed measure µ and orthogonal in the sense that</p><formula xml:id="formula_10">(f u , f v ) L 2 = 0<label>(6)</label></formula><p>for u = v, see, e.g., <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Classical ANOVA decomposition</head><p>For Ω = [0, 1] and the example of the Lebesgue measure dµ(x) = dx in (1), the space V (d) is the space of square integrable functions and the projections are given by</p><formula xml:id="formula_11">P u f (x u ) = [0,1] d-|u| f (x)dx D\u .</formula><p>The decomposition (3) then corresponds to the well-known analysis of variance (ANOVA) decomposition which is used in statistics to identify important variables and important interactions between variables in high-dimensional models. Recently, it has extensively been used for the analysis of QMC methods, see, e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33]</ref> and the references cited therein.</p><p>Here, the orthogonality <ref type="bibr" target="#b5">(6)</ref> implies that the variance of the function f can be written as</p><formula xml:id="formula_12">σ 2 (f ) = u⊆D u =∅ σ 2 (f u ),<label>(7)</label></formula><p>where σ 2 (f u ) denotes the variance of the term f u . The values σ 2 (f u )/σ 2 (f ), called global sensitivity indices in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>, can then be used to measure the relative importance of the term f u with respect to the function f . Based on the ANOVA decomposition, different notions of effective dimensions have been introduced in <ref type="bibr" target="#b3">[4]</ref>. For the proportion α ∈ (0, 1], the effective dimension in the truncation sense (the truncation dimension) of the function f is defined as the smallest integer d t , such that</p><formula xml:id="formula_13">u⊆{1,...,d t } u =∅ σ 2 (f u ) ≥ α σ 2 (f ).<label>(8)</label></formula><p>Here, often the proportion α = 0.99 is used. The effective dimension in the superposition sense (the superposition dimension) is defined as the smallest integer d s , such that</p><formula xml:id="formula_14">|u|≤ds u =∅ σ 2 (f u ) ≥ α σ 2 (f ).<label>(9)</label></formula><p>If the variables are ordered according to their importance, the truncation dimension d t roughly describes the number of important variables of the function f . The superposition dimension d s roughly describes the highest order of important interactions between variables.</p><p>The following two lemmas relate the effective dimensions to approximation errors. The second lemma is taken from <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 1. Let d t denote the truncation dimension of the function f with proportion α and let f d</head><formula xml:id="formula_15">t (x) := u⊆{1,...,d t } f u (x u ). Then f -f d t 2 L 2 ≤ (1 -α)σ 2 (f ). Proof. Note that σ 2 (f u ) = f u 2 L 2 for u = ∅ since [0,1] |u| f u (x u ) dx u = 0 for u = ∅. From (3), one obtains f -f d t 2 L 2 = u ⊆{1,...,d t } f u 2 L 2 = u ⊆{1,...,d t } f u 2 L 2 = u⊆D σ 2 (f u ) - u⊆{1,...,d t } σ 2 (f u ) ≤ (1 -α)σ 2 (f ),</formula><p>where the second equality holds by orthogonality and where the inequality follows from ( <ref type="formula" target="#formula_12">7</ref>) and (8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 2. Let d s denote the superposition dimension of the function f with proportion α and let f d</head><formula xml:id="formula_16">s (x) := |u|≤d s f u (x u ). Then f -f d s 2 L 2 ≤ (1 -α) σ 2 (f ).</formula><p>Proof. Similar to Lemma 1 we compute</p><formula xml:id="formula_17">f -f d s 2 L 2 = |u|&gt;d s f u 2 L 2 = |u|&gt;d s f u 2 L 2 = |u|&gt;d s σ 2 (f u ) ≤ (1 -α) σ 2 (f )</formula><p>using orthogonality, (3), ( <ref type="formula" target="#formula_12">7</ref>) and <ref type="bibr" target="#b8">(9)</ref>.</p><p>For integration, we immediately obtain as corollary the error bound Quasi-random points are usually more uniformly distributed in smaller dimensions than in higher ones such that we can expect that If d t is well approximated for small d t . Moreover, quasi-random points usually have very well-distributed low-dimensional projections such that we can expect that If d s is efficiently computed for small d s . Hence, the bound <ref type="bibr" target="#b9">(10)</ref> partly explains the success of quasi-Monte Carlo methods for high-dimensional integrals with functions of low truncation dimension or low superposition dimension. The bound (10) also partly explains the success of sparse grid methods for high-dimensional integrals with functions of low effective dimension since these methods can compute If tr very efficiently for small d s or small d t with the help of a dimension-adaptive grid refinement.</p><formula xml:id="formula_18">|If -If tr | ≤ f -f tr L 1 ≤ f -f tr L 2 ≤ √ 1 -α σ (f )<label>(10</label></formula><p>Remark 1. We can also choose Ω = R and the Gaussian measure dµ(x) = ϕ d (x)dx in (1) where ϕ d (x) := e -x T x/2 /(2π ) d/2 <ref type="bibr" target="#b10">(11)</ref> denotes the standard Gaussian density in d dimensions. This induces projections</p><formula xml:id="formula_19">P u f (x u ) = R d-|u| f (x)ϕ d-|u| (x u )dx D\u .</formula><p>Then, by (3), a corresponding decomposition of the function f on R d results, which we refer to as ANOVA decomposition with Gaussian weight. Based on this decomposition, effective dimensions for the ANOVA decomposition with Gaussian weight can be defined as in <ref type="bibr" target="#b7">(8)</ref> and (9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Anchored-ANOVA decomposition</head><p>For Ω = [0, 1] and the example of the Dirac measure located at a fixed anchor point a ∈ [0, 1] d , i.e. dµ(x) = δ(xa)dx, we obtain from (2) the projections</p><formula xml:id="formula_20">P u f (x u ) = f (x)| x=a\x u</formula><p>where we use the notation f (x)| x=a\x i = f (a 1 , . . . , a i-1 , x i , a i+1 , . . . , a d ) with its obvious generalisation to a \ x u . The terms of the anchored-ANOVA decomposition are thus related to the terms of the classical ANOVA decomposition in the sense that all integrals are replaced by point evaluations at a fixed anchor point a ∈ [0, 1] d . This approach is considered in <ref type="bibr" target="#b27">[28]</ref> under the name CUT-HDMR. The decomposition expresses f as superposition of its values on lines, faces, hyperplanes, etc., which intersect the anchor point a and are parallel to the coordinate axes. It is closely related to the multivariate Taylor expansion and to anchored Sobolev spaces, see <ref type="bibr" target="#b10">[11]</ref> and the references cited therein.</p><p>While the classical ANOVA decomposition is very useful to analyse the importance of different dimensions and of their interactions it cannot be used as a tool for the design of new integration schemes since already the constant term in the classical ANOVA decomposition requires to compute the integral. The anchored-ANOVA decomposition has the advantage that its sub-terms are much cheaper to compute since instead of integrals only point evaluations at the anchor point a ∈ [0, 1] d are required. We will use this property in Section 3 to design new quadrature methods for highdimensional functions.</p><p>We next define a new notion of effective dimension which is based on the anchored-ANOVA decomposition. While the effective dimensions in the classical case are based on the L 2 -norm, we now introduce effective dimensions for the anchored case, which are based on the operator |I(•)| and, since |I(f</p><formula xml:id="formula_21">)| = | [0,1] d f (x) dx| ≤ f L 1</formula><p>, which are thus related to the L 1 -norm. While the effective dimensions in the classical case directly lead to error bounds for approximation (see Lemmas 1 and 2), we will use the effective dimensions in the anchored case to derive error bounds for integration (see Lemmas 3 and 4). <ref type="foot" target="#foot_1">5</ref> To this end, let</p><formula xml:id="formula_22">σ (f ) := u⊆D u =∅ |If u | ≤ u⊆D u =∅ f u L 1 (<label>12</label></formula><formula xml:id="formula_23">)</formula><p>be the sum of the absolute values of the integrals of all anchored-ANOVA terms. Then, analogous to (8),</p><p>for the proportion α ∈ (0, 1], the truncation dimension in the anchored case is defined as the smallest</p><formula xml:id="formula_24">integer d t , such that u⊆{1,...,d t } u =∅ |If u | ≥ α σ (f ),<label>(13)</label></formula><p>whereas, analogous to (9), the superposition dimension in the anchored case is defined as the smallest integer d s , such that</p><formula xml:id="formula_25">|u|≤ds u =∅ |If u | ≥ α σ (f ).<label>(14)</label></formula><p>As in the classical case, these notions describe roughly the number of important dimensions and the order of important interactions, respectively. Compared to the classical case, the effective dimensions in the anchored case have the following advantages: They are directly related to integration errors as we show below and they can easily be determined by dimension-wise integration methods as we will explain in Section 3 in more detail. We also have a direct relation of the effective dimensions in the anchored case to sparse grid methods as we will show in Section 4. As |I(•)| is not a norm, it may happen, however, that the effective dimensions in the anchored case fail to detect some important dimensions and interactions. <ref type="foot" target="#foot_2">6</ref> This may be the case if f is a function of varying sign. For instance, let a = (1/2, 1/2) and consider the function</p><formula xml:id="formula_26">f (x 1 , x 2 ) = e x 1 -e 0.5 +x 2 -1 2 . Then we obtain f 2 (x 2 ) = x 2 -1 2 such that σ 2 (f 2 ) &gt; 0 but |If 2 | = 0 which misleadingly indicates independence of x 2 .</formula><p>This effect which we have not yet observed in practical applications from finance, though, is closely related to the early determination problem of dimension-adaptive sparse grid methods which is discussed in <ref type="bibr" target="#b8">[9]</ref>.</p><p>To compute the effective dimensions in the anchored case, we use multivariate quadrature methods to compute approximations q u ≈ If u . By summation of the computed values q u , u ⊆ D, we estimate σ (f ) and the effective dimensions d t and d s according to ( <ref type="formula" target="#formula_22">12</ref>)- <ref type="bibr" target="#b13">(14)</ref>.</p><p>The following two estimates relate effective dimensions in the anchored case and integration errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 3. Let d t denote the truncation dimension of the function f in the anchored case with proportion</head><formula xml:id="formula_27">α and let f d t (x) := u⊆{1,...,d t } f u (x u ). Then |If -If d t | ≤ (1 -α) σ (f ). Proof. We obtain |If -If d t | = | u ⊆{1,...,d t } If u | ≤ u ⊆{1,...,d t } |If u | = u⊆D |If u | - u⊆{1,...,d t } |If u | ≤ (1 -α) σ (f )</formula><p>where the first equality results from (3) and from the definition of the function f d t . The last inequality follows from <ref type="bibr" target="#b11">(12)</ref> and <ref type="bibr" target="#b12">(13)</ref>. </p><formula xml:id="formula_28">= |u|≤d s f u (x u ). Then |If -If d s | ≤ (1 -α) σ (f ).</formula><p>Proof. Similar to Lemma 3 we obtain</p><formula xml:id="formula_29">|If -If d s | = | |u|&gt;d s If u | ≤ |u|&gt;d s |If u | ≤ (1 -α) σ (f )</formula><p>using ( <ref type="formula" target="#formula_22">12</ref>) and ( <ref type="formula" target="#formula_25">14</ref>) for the last inequality.</p><p>Remark 2. We can also choose Ω = R and the measure dµ(x) = δ(x-a)ϕ d (x)dx with a fixed anchor point a ∈ R d , where ϕ d is the Gaussian density <ref type="bibr" target="#b10">(11)</ref>. This example induces projections</p><formula xml:id="formula_30">P u f (x u ) = f (x)ϕ d-|u| (x u ) | x=a\x u</formula><p>and, by (3), a corresponding decomposition of functions f : R d → R which we refer to as anchored-ANOVA decomposition with Gaussian weight. Based on this decomposition, effective dimensions for the anchored-ANOVA decomposition with Gaussian weight can be defined as in <ref type="bibr" target="#b12">(13)</ref> and <ref type="bibr" target="#b13">(14)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dimension-wise quadrature methods</head><p>Next, we use the anchored-ANOVA decomposition to define a new class of methods for the computation of high-dimensional integrals</p><formula xml:id="formula_31">If := [0,1] d f (x)dx (15)</formula><p>on the unit cube and for integrals</p><formula xml:id="formula_32">I ϕ f := R d f (z) ϕ d (z)dz (16)</formula><p>on R d with the Gaussian weight function ϕ d from <ref type="bibr" target="#b10">(11)</ref>. Note that these two domains typically appear in high-dimensional applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Truncation and discretization</head><p>In the following, we develop our new class of quadrature methods. We start with Ω = [0, 1] and take µ as the Dirac measure with anchor point a ∈ [0, 1] d . Then, ( <ref type="formula" target="#formula_5">2</ref>) and (4) imply</p><formula xml:id="formula_33">f u (x u ) = P u f (x u ) - v⊂u f v (x v ) where P u f (x u ) = f (x)| x=a\x u . (<label>17</label></formula><formula xml:id="formula_34">)</formula><p>Applying the integral operator to the anchored-ANOVA decomposition (3), the d-dimensional integral is decomposed, by linearity, into the finite sum</p><formula xml:id="formula_35">If = u⊆D If u = f (a) + d i=1 If i + d i,j=1 j&lt;j If i,j + • • • + If 1,...,d<label>(18)</label></formula><p>which contains d j many j-dimensional integrals for j = 0, . . . , d. Starting with the decomposition <ref type="bibr" target="#b17">(18)</ref> we now define a general class of quadrature methods for the approximation of If . We proceed as follows:</p><p>1. Truncation: We take only a subset S of all indices u ⊆ D and thus truncate the sum in <ref type="bibr" target="#b17">(18)</ref>. Here, we assume that the set S satisfies the admissibility condition<ref type="foot" target="#foot_3">7</ref> </p><formula xml:id="formula_36">u ∈ S and v ⊂ u ⇒ v ∈ S.<label>(19)</label></formula><p>For example, the set S d s := {u ⊆ D : |u| ≤ d s } or the set S d t := {u ⊆ {1, . . . , d t }} could be used to take into account the superposition and the truncation dimension of the function f , respectively. Alternatively, dimension-wise adaptive methods can be applied to build up an appropriate index set S. This will be later discussed in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Discretization:</head><p>For each u ∈ S, we compute approximations to If u . To this end, we choose |u|dimensional quadrature rules Q u . Starting with q ∅ = f (a) we recursively compute</p><formula xml:id="formula_37">q u := Q u (P u f ) - v⊂u q v .<label>(20)</label></formula><p>Then, q u is an approximation to If u due to the recursive representation ( <ref type="formula" target="#formula_33">17</ref>) of f u . Observe that we avoid to compute and integrate the functions f u explicitly. Instead we numerically integrate P u f and correct the resulting value by the (previously computed) values q v using (17). The admissibility condition ensures that we can run over the set S by starting with u = ∅ and proceeding with indices u for which the values q v , v ⊂ u, have already be computed in previous steps. Note that we allow for arbitrary quadrature methods Q u in <ref type="bibr" target="#b19">(20)</ref> which can be different for each u. Specific choices for Q u will be discussed later.</p><p>Altogether, this defines a quadrature formula A S f for the approximation of If which is given by</p><formula xml:id="formula_38">A S f := u∈S q u<label>(21)</label></formula><p>and which we refer to as dimension-wise quadrature method in the following. Note that the method</p><formula xml:id="formula_39">A S f requires n = u∈S n u</formula><p>evaluations of the function f , where n u denotes the number of function evaluations of Q u .</p><p>Remark 3. Dimension-wise quadrature methods for integrals on R d with Gaussian weight can be constructed analogously to <ref type="bibr" target="#b20">(21)</ref>. To this end, we set Ω = R and use the measure dµ(x) = δ(xa)ϕ d (x)dx such that the anchored-ANOVA decomposition with Gaussian weighted results.</p><p>Then, we select as above a suitable index set S and appropriate quadrature rules Q u to integrate the resulting functions P u f . Since now f : R d → R, either quadrature rules for unbounded domains, e.g. Gauss-Hermite rules, or transformations of the resulting integrals to the unit cube must be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Error and costs</head><p>We first consider the case of arbitrary quadrature methods Q u . By construction, we then have the error bound</p><formula xml:id="formula_40">|If -A S f | = | u⊆D If u - u∈S q u | ≤ u∈S |If u -q u | + u ∈S |If u |. (<label>22</label></formula><formula xml:id="formula_41">)</formula><p>This shows how the error of the method (21) depends on the quadrature rules Q u (which determine q u ) and on the choice of the index set S. Here, the second term describes the modelling error which is introduced by the truncation of the anchored-ANOVA series whereas the first term describes the discretization error which results from the subsequent discretization of the remaining subspaces.</p><p>In the following, we aim to balance costs and accuracies by relating the cost of the quadrature method Q u to the importance of the anchored-ANOVA term f u . We first relate the accuracy of the methods Q u to the accuracy of the method A S f .</p><p>To this end, we fix α ∈ (0, 1] and assume that d s and d t , the corresponding superposition and truncation dimensions in the anchored case, are known. With help of these effective dimensions we define the index set</p><formula xml:id="formula_42">S d t ,d s := {u ⊆ {1, . . . , d t } : |u| ≤ d s }. (<label>23</label></formula><formula xml:id="formula_43">)</formula><p>We now have the following lemma.</p><formula xml:id="formula_44">Lemma 5. Let S = S d t ,d s . For ε &gt; 0, let furthermore Q u be such that |I(P u f ) -Q u (P u f )| ≤ ε(|u|) with ε(j) := ε/(e d d s t d t j ) for all u ∈ S. Then, it holds |If -A S f | ≤ ε + 2(1 -α) σ (f ). Proof. Starting with |If -A S f | ≤ |If -If d t ,d s | + |If d t ,d s -A S f | with the function f d t ,d s := u∈S d t ,ds f u ,</formula><p>we observe that the modelling error is bounded by</p><formula xml:id="formula_45">|If -If d t ,d s | = | u ∈S d t ,ds If u | ≤ |u|&gt;d s |If u | + u ⊂{1,...,d t } |If u | ≤ 2(1 -α) σ (f ),</formula><p>see the proofs of Lemmas 3 and 4. From ( <ref type="formula" target="#formula_9">5</ref>) and ( <ref type="formula" target="#formula_37">20</ref>), we have the explicit representation</p><formula xml:id="formula_46">If u -q u = v⊆u (-1) |u|-|v| (I(P v f ) -Q v (P v f )) .<label>(24)</label></formula><p>Since</p><formula xml:id="formula_47">|I(P v f ) -Q v (P v f )| ≤ ε(|v|) for all v ⊆ u, we obtain for all u with |u| ≤ d t that |If u -q u | ≤ v⊆u ε(|v|) = |u| j=1 |u| j ε(j) ≤ |u| j=1 d t j ε(j),</formula><p>where we used that there are </p><formula xml:id="formula_48">|If d t ,d s -A S d t ,ds f | ≤ u∈S d t ,ds |If u -q u | ≤ d s k=1 d t k k j=1 d t j ε(j) ≤ d s k=1 d t k k j=1 ε e d d s t = ε e d d s t d s k=1 d t k k ≤ ε e d d s t d s k=1 d d s t k! k = ε e d s k=1 1 (k -1)! ≤ ε,</formula><p>which concludes the proof.</p><p>We next relate the error |If -A S f | to the cost n = u∈S n u of the method A S f . Furthermore, we aim to balance the cost n u of the methods Q u with their accuracy. Here, we restrict ourselves to the case that all employed methods Q u are based on a univariate quadrature formula U m with m points, which converges for f ∈ C r ([0, 1]) with rate m -r . An examples for such a univariate formula with r = 1 is the trapezoidal rule. For arbitrary r, Gauss rules can be used. </p><formula xml:id="formula_49">:= n 1/d s then |If -A S f | ≤ c(d t , d s ) n -r/d s + 2(1 -α) σ (f ) for all f ∈ C r ([0, 1] d ).</formula><p>Here, the constant c(d t , d s ) depends on the effective dimensions d t and d s in the anchored case, but not on the nominal dimension d.</p><p>Proof. We have the same modelling error as in Lemma 5, i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|If -</head><formula xml:id="formula_50">A S f | ≤ |If d t ,d s -A S f | + 2(1 -α) σ (f ) with the function f d t ,d s := u∈S d t ,ds f u . Since f ∈ C r ([0, 1] d ) also f u ∈ C r ([0, 1] |u| ) for all u ⊆ D. Consequently, Q u converge with rate r/|u|. By definition, Q u requires n u = n |u|/d s function evaluations such that |I(P u f ) -Q u (P u f )| ≤ c(|u|) n -r/|u| u ≤ c(|u|) n -r/d s</formula><p>for a constant c(|u|) &gt; 0 which depends on the order |u|. <ref type="foot" target="#foot_4">8</ref> With the help of ( <ref type="formula" target="#formula_46">24</ref>) we estimate</p><formula xml:id="formula_51">|If d t ,d s -A S d t ,ds f | ≤ u∈S d t ,ds |If u -q u | ≤ u∈S d t ,ds v⊆u |I(P v f ) -Q v (P v f )| ≤ u∈S d t ,ds v⊆u c(|v|) n -r/d s = d s k=1 d t k k j=1 k j c(j) n -r/d s = c(d t , d s ) n -r/d s with the constant c(d t , d s ) := d s k=1 d t k k j=1 k j c(j) ≤ c(d s ) d s k=1 d t k 2 k ≤ c(d s ) d s k=1 d t d s k! 2 k ≤ c(d s )(e 2 -1) d t d s ,</formula><p>where c(d s ) := max j=1,...,d s c(j). This completes the proof.</p><p>Note that the first term in the error bound describes the discretization error which depends on n whereas the second term corresponds to the modelling error which depends on the proportion α. Note furthermore that the cost to obtain a prescribed discretization error does not exponentially depend on the nominal dimension d, but only on the superposition dimension d s in the anchored case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">A priori construction using function space weights</head><p>In applications, the effective dimensions of f are usually unknown. These dimensions can also not be computed since this would be at least as expensive as the integration of f . In general it is thus difficult to determine the set S d t ,d s in <ref type="bibr" target="#b22">(23)</ref>.</p><p>To overcome this obstacle, we here assume that the integrand is contained in some function class that is defined by certain function space weights γ u ≥ 0, which describe the importance of the term f u of the anchored-ANOVA decomposition. Using this a priori information, we then determine the index set S by including those indices u which correspond to the largest weights γ u . In the following, we use the set S γ := {u ⊆ D : γ u &gt; ε} which includes all indices u for which γ u is larger than some threshold ε. It is known (see, e.g. <ref type="bibr" target="#b38">[39]</ref>) that many functions f in practice are of low effective dimension either in the truncation or in the superposition sense. For these two classes of functions we can hope to determine the index set S γ of the most important terms by the following approaches to define the weights: order-dependent weights for functions f with low superposition dimension and product weights for functions f with low truncation dimension.</p><p>• Order-dependent weights: We define the order-dependent weights γ u = 1/|u|. Then, the indices are added according to their order |u|. Note that by construction, the admissibility condition <ref type="bibr" target="#b18">(19)</ref> is always satisfied. The weights are the larger the lower the order of the anchored-ANOVA term. If the function f has a small superposition dimension then we can hope that the resulting index set S γ includes the most important terms.</p><p>• Product weights: As in <ref type="bibr" target="#b28">[29]</ref>, we assume that the dimensions are ordered according to their importance which is modulated by a sequence of weights</p><formula xml:id="formula_52">γ 1 ≥ γ 2 ≥ • • • ≥ γ d ≥ 0.</formula><p>Using the weights γ i , we then assign the product weight</p><formula xml:id="formula_53">γ u := j∈u γ j (<label>25</label></formula><formula xml:id="formula_54">)</formula><p>to the index u ⊆ D. The weights γ i can here either be input parameters of the algorithm similar to the CBC construction of lattice rules <ref type="bibr" target="#b29">[30]</ref> or they can be derived from the first-order terms f j of the anchored-ANOVA decomposition. In the latter case they are defined by <ref type="bibr" target="#b19">(20)</ref>. The weights γ u are the larger the lower the dimensions that are associated with their index set u. In this way we can hope that the resulting index set S γ includes the most important terms if the function f has a small truncation dimension.</p><formula xml:id="formula_55">γ j := |q j | |q ∅ | = |Q j (P j f ) -f (a)| |f (a)| for j = 1, . . . , d, see</formula><p>We will use these weights and the resulting index sets S γ in our numerical experiments in Section 7. Note that also more general weights can be used in our construction as long as the admissibility condition ( <ref type="formula" target="#formula_36">19</ref>) is satisfied. Nevertheless, we here only shifted the problem of the choice of S to the problem of determining the weights γ u . In Section 6, we will consider a different approach.</p><p>There, we will determine the index set S a posteriori in a dimension-adaptive way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Sparse grid quadrature</head><p>In this section, we use tensor product methods Q u for the approximation of the integrals If u in <ref type="bibr" target="#b17">(18)</ref>. This allows us to intertwine the truncation of the anchored-ANOVA series and the subsequent discretization and allows to balance modelling and discretization error in an optimal way. We will demonstrate this in Section 5 for integrands from weighted tensor product Sobolev spaces in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Generalised sparse grids</head><p>For a univariate function f : [0, 1] → R and a sequence of non-decreasing integers m k , k ∈ N, let</p><formula xml:id="formula_56">U m k f := m k i=1 w i,k f (x i,k ) (26)</formula><p>denote a sequence of univariate quadrature rules with m k points x i,k and weights w i,k , which converges pointwise to If for k → ∞. We assume m 1 = 1 and U 1 f = f (1/2) and define the difference quadrature formulae </p><formula xml:id="formula_57">∆ k := U m k -U m k-1 with U m 0 := 0 (27) for k ≥ 1. Now let f : [0, 1] d → R</formula><formula xml:id="formula_58">k f := ∆ k 1 ⊗ • • • ⊗ ∆ k d f .<label>(29)</label></formula><p>A specific class of quadrature methods for the approximation of If is then obtained by a truncation of the sum (28) using an appropriate index set I ⊂ N d , which can be regarded as a refinement of the index set S ⊆ D from Section 3.1. To ensure the validity of the telescoping sum expansion the index set I has to satisfy the admissibility condition</p><formula xml:id="formula_59">k ∈ I and l ≤ k ⇒ l ∈ I,<label>(30)</label></formula><p>where l ≤ k is defined by l j ≤ k j for j = 1, . . . , d. In this way, the generalised sparse grid method</p><formula xml:id="formula_60">SG I f := k∈I k f (31)</formula><p>is obtained, see, e.g., <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b40">41]</ref>. Different ways to truncate the sum then correspond to different quadrature methods. Examples are the classical sparse grid construction from <ref type="bibr" target="#b30">[31]</ref>, often denoted as Smolyak method, which is, on level ∈ N, recovered with the index set</p><formula xml:id="formula_61">I = k ∈ N d : |k| 1 ≤ + d -1<label>(32)</label></formula><p>where |k| 1 := d j=1 k j , or product methods, which correspond on level to index sets of the form</p><formula xml:id="formula_62">I = k ∈ N d : |k| ∞ ≤<label>(33)</label></formula><p>where |k| ∞ := max{k j : j = 1, . . . , d}.</p><p>Remark 4. Sparse grid methods can directly be applied to the numerical computation of integrals on R d with Gaussian weight. To this end, only the sequence of univariate quadrature rules U m k must be replaced by quadrature formulae for functions f : R → R on unbounded domains, such as Gauss-Hermite rules, see, e.g., <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Relation to dimension-wise quadrature methods</head><p>There is a close relation of the sparse grid approach and the anchored-ANOVA decomposition. The sparse grid approach (31) can indeed be interpreted as a refinement of the anchored-ANOVA decomposition by first expanding each term of the decomposition into an infinite basis and then truncating this expansion appropriately. <ref type="foot" target="#foot_5">9</ref> It can thus be regarded as special case of the method <ref type="bibr" target="#b20">(21)</ref> where the set S and the rules Q u are chosen in a systematic way to exploit smoothness of the integrand.</p><p>We now show this in more detail. To this end, we always use the anchor a = (1/2, . . . , 1/2). We start with the following lemma. Lemma 7. Let f u and P u f be as in <ref type="bibr" target="#b16">(17)</ref> and</p><formula xml:id="formula_63">N u := k ∈ N d : k j &gt; 1 if and only if j ∈ u . (<label>34</label></formula><formula xml:id="formula_64">)</formula><formula xml:id="formula_65">Then, k f = k (P u f ) if k ∈ N v with v ⊆ u. Moreover, k f = k f u if k ∈ N u .</formula><p>Proof. The proof follows from the fact that the projection P u and the operator ∆ 1 are defined in a way such that f is evaluated at the same anchor point. Indeed, if k ∈ N v and v ⊆ u then k j = 1 for all j ∈ u and thus</p><formula xml:id="formula_66">k f = k (P u f ) since k = ∆ k 1 ⊗ • • • ⊗ ∆ k d and ∆ 1 f = P ∅ f = f (1/</formula><p>2) for all univariate functions f . To show the second assertion, let k ∈ N u . Then, we obtain from (4) that</p><formula xml:id="formula_67">k (P u f ) = k f u + v⊂u k f v . Since f v (x v )| x j =1/2 = 0 for all j ∈ v, which is a direct consequence of the orthogonality (6), we conclude k f v = 0 for all v ⊂ u and k ∈ N u . This proves k f = k (P u f ) = k f u for all k ∈ N u .</formula><p>By Lemma 7 and ( <ref type="formula">28</ref>) we obtain</p><formula xml:id="formula_68">If = u⊆D k∈N u k f</formula><p>since N d is the disjoint union of the sets N u , u ⊆ D. By <ref type="bibr" target="#b17">(18)</ref>, we also have If = u⊆D If u , which yield a decomposition</p><formula xml:id="formula_69">If u = k∈N u k f</formula><p>of the integrals of the anchored-ANOVA terms into an infinite sum. Next, we truncate this sum. To this end, we select index sets I u ⊂ N u for all u ⊆ D, which satisfy the condition <ref type="bibr" target="#b29">(30)</ref>, and use</p><formula xml:id="formula_70">q u := k∈I u k f (<label>35</label></formula><formula xml:id="formula_71">)</formula><p>as approximation to If u . The corresponding method <ref type="bibr" target="#b20">(21)</ref> with S = D can then be represented as</p><formula xml:id="formula_72">A S f = u⊆D q u = k∈I k f</formula><p>with the index set I = u⊆D I u . We see that in this way both, the modelling and the discretization error is expressed 10 in terms of the values k f . We further see that the resulting method A S f coincides with the generalised sparse grid approach <ref type="bibr" target="#b30">(31)</ref>. To this end, we define</p><formula xml:id="formula_73">I u := k ∈ I : k j &gt; 1 if and only if j ∈ u = I ∩ N u . (<label>36</label></formula><formula xml:id="formula_74">)</formula><p>Theorem 8. The dimension-wise quadrature method <ref type="bibr" target="#b20">(21)</ref> with anchor a = ( 1 2 , . . . , 1 2 ), the index set S = D and the quadrature methods</p><formula xml:id="formula_75">Q u f := v⊆u k∈I v k f (<label>37</label></formula><formula xml:id="formula_76">)</formula><p>coincides with the generalised sparse grid method <ref type="bibr" target="#b30">(31)</ref>.</p><p>Proof. We have to show that (20) holds with q u as in <ref type="bibr" target="#b34">(35)</ref> and Q u as in <ref type="bibr" target="#b36">(37)</ref>. In fact, 10 Modelling errors are here represented by the case I u = ∅ for any u ⊆ D.</p><formula xml:id="formula_77">Q u (P u f ) - v⊂u q v = v⊆u k∈I v k (P u f ) - v⊂u k∈I v k f = k∈I u k (P u f ) + v⊂u k∈I v ( k (P u f ) -k f ) = k∈I u k f = q u</formula><p>where we twice used Lemma 7.</p><p>Remark 5. Similar to Theorem 8, we see that generalised sparse grid methods for integrals on R d with Gaussian weight (e.g. sparse grids based on Gauss-Hermite rules) are special cases of the dimensionwise quadrature method for integrals with Gaussian weight, see Remark 3. Both methods result from a discretization of the terms of the anchored-ANOVA decomposition with Gaussian weights using the anchor a = (0, . . . , 0), see Remark 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Optimal sparse grids in weighted spaces</head><p>In Section 4, we introduced the index set I ⊂ N d as a refinement of the index set S ⊆ D and specified the quadrature rules Q u such that the general approach (21) corresponds to the class of generalised sparse grid methods. Now we determine the index set I which balances the resulting modelling and discretization errors in an optimal way for integrands from weighted tensor product Sobolev spaces, see, e.g., <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b40">41]</ref>. To this end, we partly proceed as in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>For reasons of simplicity, we restrict ourselves to the case that the univariate quadrature rules U m k in <ref type="bibr" target="#b25">(26)</ref> are given by the trapezoidal rule with</p><formula xml:id="formula_78">m 1 = 1, U 1 f = f (0) and m i = 1 + 2 i-2 points for i ≥ 2.</formula><p>Our analysis is based on the univariate function space</p><formula xml:id="formula_79">H 1 γ ([0, 1]) := {f : [0, 1] → R : f 1,γ &lt; ∞} with the norm f 2 1,γ := f (0) 2 + γ -1 f 2 L 2 ,<label>(38)</label></formula><p>where γ ∈ (0, 1] denotes a weight. In the multivariate case we consider a given sequence of weights</p><formula xml:id="formula_80">1 = γ 1 ≥ γ 2 ≥ • • • ≥ γ d ≥ 0</formula><p>and assign to each set u ⊆ D the product weight γ u from <ref type="bibr" target="#b24">(25)</ref>. We then define the tensor product space 11</p><formula xml:id="formula_81">H 1, mix γ ([0, 1] d ) := d j=1 H 1 γ j ([0, 1])</formula><p>with the norm</p><formula xml:id="formula_82">f 2 1,γ := u∈D γ -1 u f u 2 1, mix with f u 2 1, mix := [0,1] |u| ∂ |u| ∂x u f (x u , 0) 2 dx u ,</formula><p>where f u denote the sub-terms of the anchored-ANOVA decomposition anchored at the origin. 12   11 Note that H 1, mix γ ([0, 1] d ) is the reproducing kernel Hilbert space to the product kernel K (x, y) = d j=1 k(x j , y j ), where k(x, y) := 1 + γ min{x, y} is the reproducing kernel of the space H 1 γ ([0, 1]).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Cost-benefit ratio</head><p>For the space H mix γ ([0, 1] d ) we next determine the index set I of the generalised sparse grid method SG I , which has the best possible cost-benefit ratio. To this end, we first associate each index k ∈ N d with a local cost value, namely the number of function evaluations n k required by k f . Since the methods U m i are nested and since m i ≤ 2 i-1 , we have</p><formula xml:id="formula_83">n k = d j=1 m k j ≤ 2 |k-1| 1 =: c k .</formula><p>For the global costs of (31) we thus have the bound</p><formula xml:id="formula_84">k∈I n k ≤ k∈I c k =: n I . (<label>39</label></formula><formula xml:id="formula_85">)</formula><p>We now consider the error of the method SG I . To this end, note that</p><formula xml:id="formula_86">|If -SG I f | = | k∈N d k f - k∈I k f | ≤ k∈N d \I | k f |. (<label>40</label></formula><formula xml:id="formula_87">)</formula><p>To derive bounds for k f , we associate to each index k ∈ N d the product weight</p><formula xml:id="formula_88">γ k := j=1,...,d k j &gt;1 γ j ,</formula><p>where the product is taken over all j for which k j &gt; 1 holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 9. It holds</head><formula xml:id="formula_89">| k f | ≤ b k f 1,γ<label>(41)</label></formula><p>where</p><formula xml:id="formula_90">b k := 2 -|k-1| 1 γ 1/2 k . (<label>42</label></formula><formula xml:id="formula_91">)</formula><p>Proof. We first consider the univariate case and show that</p><formula xml:id="formula_92">|∆ i f | ≤ γ 1/2 2 -i+1 f 1,γ<label>(43)</label></formula><p>for i ≥ 2. In fact, by <ref type="bibr" target="#b37">(38)</ref> we have</p><formula xml:id="formula_93">f 2 L 2 = γ ( f 2 1,γ -f (0) 2 ) ≤ γ 1/2 f 1,γ .</formula><p>Therefore,</p><formula xml:id="formula_94">|∆ i f | = |U m i f -U m i -1 f | ≤ 2 -i+1 f 2 L 2 ≤ γ 1/2 2 -i+1 f 1,γ for i ≥ 2,</formula><p>where a proof of the first inequality can be found in <ref type="bibr" target="#b40">[41]</ref>. For i = 1, we have </p><formula xml:id="formula_95">|∆ i f | = |U m 1 f | = |f (0)| ≤ f 1,</formula><formula xml:id="formula_96">cbr k := b k /c k = 2 -2|k-1| 1 γ 1/2 k (45)</formula><p>associated with the index k according to their size. The optimal index set I then contains all indices whose local cost-benefit ratios are larger than or equal to some constant value. Here, we use the value</p><formula xml:id="formula_97">cbrk := 2 -2( -1)<label>(46)</label></formula><p>as threshold, which is associated with the index k = ( , 1, . . . , 1).</p><p>Theorem 10 (Optimal Sparse Grids in the Weighted Case). The optimal index set in the weighted case is given by</p><formula xml:id="formula_98">I ,γ := {k ∈ N d : |k| 1 + σ k ≤ + d -1}<label>(47)</label></formula><p>where</p><formula xml:id="formula_99">σ k := j=1,...,d k j &gt;1</formula><p>σ j with σ j := -log 2 (γ j )/4.</p><formula xml:id="formula_100">Proof. Using γ 1/2 k = 2 j∈D k log 2 (γ j )/2 = 2 -2σ k with D k := {j ∈ D : k j &gt; 1}</formula><p>we obtain from (45) that</p><formula xml:id="formula_101">cbr k = 2 -2(|k-1| 1 +σ k ) .</formula><p>The comparison with (46) shows that cbr k ≥ cbrk if and only if -2(|k -1| 1 + σ k ) ≥ -2( -1), i.e., if |k| -d + σ k ≤ -1, which proves the assertion.</p><p>The resulting sparse grid method with the index set I ,γ is then given by</p><formula xml:id="formula_102">SG ,γ f := k∈I ,γ k f .<label>(48)</label></formula><p>Note that the method SG ,γ is the classical sparse grid approach (32) in the unweighted case γ j = 1, j = 1, . . . , d.</p><p>Example 1. For illustration, the resulting optimal index sets I ,γ on level = 7 are shown in Fig. <ref type="figure" target="#fig_5">1</ref> for d = 2 and different choices of the weights γ = (γ 1 , γ 2 ). There, the local cost-benefit ratios of the</p><formula xml:id="formula_103">indices k = (k 1 , k 2 ), k i ∈ {1, . . . , 8}, i = 1, 2</formula><p>, are color coded. In addition the indices k which belong to I ,γ with = 7 are marked with a dot. One can see that the index sets I ,γ can be represented by I ,γ = u⊂{1,2} I u , i.e., the disjoint union of the four subsets</p><formula xml:id="formula_104">I ∅ = {(1, 1)}, I 1 = {(k 1 , 1) : k 1 &gt; 1 and k 1 &lt; b 1 }, I 2 = {(1, k 2 ) : k 2 &gt; 1 and k 2 &lt; b 2 }, I 12 = {(k 1 , k 2 ) : k 1 , k 2 &gt; 1 and |k| 1 &lt; b 12 },<label>(49)</label></formula><p>with b 1 , b 2 , b 12 ∈ N which depend on the weights γ 1 and γ 2 , the dimension d and on the level . Note that all four subsets correspond to index sets of classical sparse grid methods. In general 2 d subsets are required; one for each ANOVA subterm. We will use this decomposition of the index set in the next two sections to derive cost and error bounds for the generalised sparse grid method SG ,γ from the known cost and error bounds in the unweighted case.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Cost analysis</head><p>In the following, we use n(d, , γ) to denote the number of function evaluations of the method SG ,γ . To analyse these costs we first recall the well-known cost bound for classical sparse grids, see, e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b39">40]</ref>. In this case, we omit the index γ and write n(d, ) := n(d, , 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 11 (Costs of Classical Sparse Grids). Let m</head><formula xml:id="formula_105">i ≤ 2 i-1 . Then n(d, ) ≤ 2 + d -2 d -1 .</formula><p>In the following, we will also consider sparse grid methods that start with m 1 = 2 points on their lowest level = 1 instead of m 1 = 1 as in the case of Lemma 11. The index set of such a sparse grid method with level and dimension d can be written in the form</p><formula xml:id="formula_106">Ī = {k ∈ (N \ {1}) d : |k| 1 ≤ + 2d -1}. (<label>50</label></formula><formula xml:id="formula_107">)</formula><p>As a corollary of Lemma 11, we see that if m i ≤ 2 i then the number of points in such a sparse grid satisfies</p><formula xml:id="formula_108">n(d, ) ≤ 2 d+ + d -2 d -1 .<label>(51)</label></formula><p>We now present a generalised cost bound for the weighted case. The cost bound results from the insight that the sparse grid method SG ,γ can be represented by the combination of 2 d many classical sparse grids (one for each anchored-ANOVA term). Here and in the following, we define that </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 12 (Costs of Weighted Sparse Grids). Let m</head><formula xml:id="formula_109">i ≤ 2 i-1 . Then n(d, , γ) ≤ 2 u⊆D γ u 1/4 + log 2 (γ u )/4 -2 |u| -1 .</formula><p>Proof. We start with the remark that the index set I ,γ from (47) can be represented by I ,γ = u⊆D I u as the disjoint union of the sets </p><formula xml:id="formula_110">I u = {k ∈ N d : |k| 1 + σ k ≤ + d -</formula><formula xml:id="formula_111">n k = u⊆D k∈I u n k .<label>(52)</label></formula><p>Let l ∈ (N \ {1}) |u| denote the vector that collects all components of k ∈ I u that are larger than one.</p><p>We then can write</p><formula xml:id="formula_112">I u = {l ∈ (N \ {1}) |u| : |l| 1 + d -|u| + σ u ≤ + d -1} = {l ∈ (N \ {1}) |u| : |l| 1 ≤ -σ u -|u| + 2|u| -1}</formula><p>, where σ u = j∈u σ j . By comparison with (50), we see that I u is the index set of a |u|-dimensional classical sparse grid starting with m 1 = 2 points and with levelσ u -|u|. Hence by (51),</p><formula xml:id="formula_113">k∈I u n k = n(|u|, -σ u -|u|) ≤ 2 -σ u -σ u -2 |u| -1 . (<label>53</label></formula><formula xml:id="formula_114">)</formula><p>Using σ u = -log 2 (γ u )/4 and 2 -σ u = γ 1/4 u we obtain the assertion by combining ( <ref type="formula" target="#formula_111">52</ref>) and (53).</p><p>Note that Theorem 12 recovers Lemma 11 in the unweighted case γ j = 1, j = 1, . . . , d. This can be seen as follows: We have</p><formula xml:id="formula_115">n(d, , 1) ≤ 2 u⊆D -2 |u| -1 = 2 d j=1 d j -2 j -1 and use d j=1 d j -2 j -1 = d j=1 d d -j -2 j -1 = d-1 j=0 d d -1 -j -2 j = d + -2 d -1</formula><p>where we applied Vandermonde's identity for the last equality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Error analysis</head><p>We now consider the error of the method SG ,γ . To this end, we start with an error bound for classical sparse grids, see, e.g., <ref type="bibr" target="#b2">[3]</ref>. In the unweighted case, we again omit the index γ and write</p><formula xml:id="formula_116">f 1 := f 1,1 .</formula><p>Lemma 13 (Error of Classical Sparse Grids). Let SG denote the classical sparse grid method with the index set <ref type="bibr" target="#b31">(32)</ref> </p><formula xml:id="formula_117">and let ≥ d -1. Then |If -SG f | ≤ k∈N d \I 2 -|k-1| 1 f 1 ≤ 2 -A(d, ) f 1 where A(d, ) := 2d + d -1 d -1 .</formula><p>If SG denotes the classical sparse grid method with the index set Ī from (50) that starts with m 1 = 2 points and if ≥ d -1, then we obtain as a corollary of Lemma 13 that</p><formula xml:id="formula_118">|If -SG f | ≤ k∈(N\{1}) d \ Ī 2 -|k-1| 1 f 1 ≤ 2 -d-A(d, ) f 1 .<label>(54)</label></formula><p>We now present an error bound for the weighted case. To derive this bound, we again use the fact that the error of the sparse grid method SG ,γ can be bounded by the sum of the errors of 2 d many classical sparse grids (one for each anchored-ANOVA term).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 14 (Error of Weighted Sparse Grids</head><formula xml:id="formula_119">). Let ≥ d -log 2 (γ {1,...,d} )/4 -1. Then |If -SG ,γ f | ≤ 2 - u⊆D γ u 1/4 2|u| + log 2 (γ u )/4 -1 |u| -1 f 1,γ . Proof. We start with |If -SG ,γ f | ≤ k∈N d \I ,γ b k f 1,γ<label>(55)</label></formula><p>which follows from ( <ref type="formula" target="#formula_86">40</ref>) and <ref type="bibr" target="#b40">(41)</ref>. Note that</p><formula xml:id="formula_120">N d \ I ,γ = u⊆D (N u \ I u )</formula><p>with N u as in <ref type="bibr" target="#b33">(34)</ref> and I u as in the proof of Theorem 12. By (42), we thus have</p><formula xml:id="formula_121">k∈N d \I ,γ b k f 1,γ = u⊆D γ 1/2 u k∈N u \I u 2 -|k-1| 1 f 1,γ<label>(56)</label></formula><p>with γ u = j∈u γ j = 2 -4σ u . As in the proof of Theorem 12, we see that I u corresponds to the index set of a |u|-dimensional classical sparse grid starting with m 1 = 2 points and with levelσ u -|u|.</p><p>By (54) we then obtain</p><formula xml:id="formula_122">k∈N u \I u 2 -|k-1| 1 ≤ 2 -+σ u A(|u|, -σ u -|u|).<label>(57)</label></formula><p>If ≥ dlog 2 (γ {1,...,d} )/4 -1, it holdsσ u ≥ |u| -1 for all u ⊆ D and thus</p><formula xml:id="formula_123">A(|u|, -σ u -|u|) = 2|u| -σ u -1 |u| -1<label>(58)</label></formula><p>by the definition of A(d, ), see Lemma 13. Using σ u = -log 2 (γ u )/4 and γ</p><formula xml:id="formula_124">1/2 u 2 σ u = γ 1/4</formula><p>u , we finally obtain the assertion by combining (55)-(58).</p><p>Note that Theorem 14 recovers Lemma 13 in the unweighted case γ j = 1, j = 1, . . . , d. We see</p><formula xml:id="formula_125">this from |If -SG ,1 f | ≤ 2 - u⊆D 2|u| -1 |u| -1 f 1,1 = 2 -2d d j=1 d j -1 j -1 f 1,1 = 2 -2d + d -1 d -1 f 1,1 = 2 -A(d, ) f 1 .</formula><p>Here, the second equality follows from Vandermonde's identity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Analysis of error versus cost</head><p>Using the results of Sections 5.2 and 5.3, we now represent the error of the method SG ,γ as a function of its costs n = n(d, , γ). We again start with the classical case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 15 (Error Versus Cost of Classical Sparse Grids). For</head><formula xml:id="formula_126">f ∈ H mix 1 ([0, 1] d ) and ≥ d -1 it holds |If -SG f | = O(n -1 (log 2 n) 2(d-1) )</formula><p>where n denotes the number of points used by the method SG .</p><p>Proof. Note that A(d, ) = O( d-1 ) in Lemma 13. By Lemma 13 we can thus estimate</p><formula xml:id="formula_127">|If -SG f | = O 2 -d-1 = O 2(d-1) 2 d-1 = O (log 2 n) 2(d-1)</formula><p>n .</p><p>Here, we used ≤ log 2 (n) and n = O(2 d-1 ) where the latter bound can be derived with the help of Lemma 11.</p><p>We now generalise this result such that also the weighted case is covered. Proof. We first show that</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 16 (Error Versus Cost of Weighted Sparse Grids</head><formula xml:id="formula_128">). Let ≥ d -log 2 (γ {1,...,d} )/4 -1. Then |If -SG ,γ f | ≤ n -1 2d B(d, , γ) 2 f 1,γ<label>(59</label></formula><formula xml:id="formula_129">n(d, , γ) ≤ 2 B(d, , γ). (<label>60</label></formula><formula xml:id="formula_130">)</formula><p>To this end, note that γ {1,...,j} = j i=1 γ i ≥ γ u for all u with |u| = j since the weights are ordered according to their size. Thus, by <ref type="bibr">Theorem</ref>  </p><formula xml:id="formula_131">|If -SG ,γ f | ≤ 2 -2d B(d, , γ) f 1,γ<label>(61)</label></formula><p>from Theorem 14. From (60) and (61) with n = n(d, , γ) we conclude that</p><formula xml:id="formula_132">|If -SG ,γ f | ≤ 2d B(d, , γ) 2 2 B(d, , γ) f 1,γ ≤ 2d B(d, , γ) 2 n -1 f 1,γ</formula><p>which proves the theorem.</p><p>We now comment on Theorem 16:</p><formula xml:id="formula_133">• In the unweighted case γ j = 1, j = 1, . . . , d, we obtain B(d, , γ) = A(d, ) = O((log 2 n) d-1</formula><p>).</p><p>Theorem 16 is thus a generalisation of the classical case in Lemma 15.</p><p>• Theorem 16 shows that the method SG ,γ converges with rate n -1 which is independent of the dimension. The error bound still depends on the value B(d, , γ), however. In general, we see that • If the weights decay sufficiently fast such that sup</p><formula xml:id="formula_134">B(d, , γ) = O( d-1</formula><formula xml:id="formula_135">d d j=1 γ 1/2 j &lt; ∞ (63)</formula><p>then the general results of Wasilkowski and Woźniakowski <ref type="bibr" target="#b40">[41]</ref> indicate that B(d, , γ) and hence also the method SG ,γ depends only polynomially on the dimension. • Note that the error bound (59) also depends on the norm f 1,γ of the integrand. This norm may grow exponentially fast for increasing d which can cause problems in higher dimensions. Note that this effect is not included in the notion of tractability in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b40">41]</ref> since only functions with norm f 1,γ ≤ 1 are addressed there.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 2.</head><p>As in <ref type="bibr" target="#b40">[41]</ref>, we consider the family of weights</p><formula xml:id="formula_136">γ j = j -α , with α ≥ 0.</formula><p>This example with α = 2 is motivated by the fact that in many application problems from finance the high nominal dimension arises from the discretization of an underlying continuous time process. The corresponding integrals can thus be written as an approximation to some infinite-dimensional integrals with respect to the Wiener measure. 13 In these cases, the integrands are contained in some weighted function spaces whose weights are related to the eigenvalues of the covariance operator of the Wiener measure. These eigenvalues, sorted by their magnitude, are decaying proportionally to j -2 , where j is the number of the eigenvalue.</p><p>For γ j = j -α , we obtain γ {1,...,j} = (j!) -α and * = d + α/4 log 2 (d!). We thus can compute the level * in (62) for different exponents α and different dimensions d. The results are shown in Table <ref type="table" target="#tab_5">1</ref>. For instance, let d = 360 and α = 2. Then, one can see that the asymptotic log-factor d-1 in the error bound from Theorem 16 does not appear as long as &lt; * = 1633. In the unweighted case, the level * = 361 is significantly smaller. If α &gt; 2 holds, then the condition ( <ref type="formula">63</ref>) is satisfied and we can use the general results of Wasilkowski and Woźniakowski <ref type="bibr" target="#b40">[41]</ref> to see that the ε-cost of the method SG ,γ is independent of the dimension. In this case the number of function evaluations n(ε) to obtain an accuracy of ε can be bounded by</p><formula xml:id="formula_137">n(ε) ≤ c ε -max{1, 2 α-1 }</formula><p>for integrands from the unit ball f γ ≤ 1, where the constant c is independent of d and ε. It is known, see <ref type="bibr" target="#b26">[27]</ref>, that the ε-exponent in this bound cannot be improved using generalised sparse grid methods. It is optimal for α ≥ 3 but far from optimal for α ≈ 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Dimension adaptivity</head><p>In Section 3.3, we shifted the problem of the choice of the index set S to the problem of determining the weights γ u . In Section 5.1, we then determined optimal index sets for sparse grid methods in weighted tensor product Sobolev spaces. In practice, however, the weights are usually unknown and can also not be computed as this would be more expensive than computing the integral.</p><p>In these cases adaptive algorithms are required which can estimate the weights a posteriori during the actual calculation of the integral. This way appropriate index sets can be constructed automatically for a given function f without any a priori information on the dimension structure of the integrand being required.</p><p>In the following we determine the index set S in a dimension-adaptive fashion. To estimate the importance of the term I(f u ) we define γ u := |q u | ≈ I(f u ) with q u from <ref type="bibr" target="#b19">(20)</ref>. Furthermore, we denote Algorithm 6.1: Dimension-adaptive constructions of the index set S.</p><p>Initialise: Let S = {∅}, q ∅ := f (a) and s = f (a). repeat 1) Compute the values q u from ( <ref type="formula" target="#formula_37">20</ref>) for all u ∈ A for which q u has not yet been computed and set s = s + q u . 2) Move the index u ∈ A with the largest weight</p><formula xml:id="formula_138">γ u = |q u | from A to S. until γ u ≤ ε ; Set A S f = s.</formula><p>by A the subset of all indices u ∈ D \ S which satisfy the admissibility condition <ref type="bibr" target="#b18">(19)</ref> with respect to the set S. With help of the weights γ u , we then heuristically build up the index set S in a general and automatic way by the following Greedy approach: We start with the initial set S = {∅} and add step by step the index u ∈ A with the largest weight γ u to S until the largest weight is below some threshold ε, see Algorithm 6.1.</p><p>Note that in the first step of Algorithm 6.1 we still have the flexibility to choose the quadrature rules Q u for the computation of q u . They can be different for each u and can be tailored to the dimension and smoothness of the terms f u , e.g., by the use of local adaptivity. Furthermore, note that in the first step of Algorithm 6.1 the values q u have to be computed for all u ∈ A for which q u has not yet been computed in previous steps. In high dimensions d, this may result in a certain overhead since not all of these values significantly contribute to the integral value.</p><p>Similarly as in Section 4, we now restrict ourselves to tensor product methods Q u for the approximation of the integrals If u in <ref type="bibr" target="#b17">(18)</ref>. Then, the truncation and discretization of the anchored-ANOVA series can be intertwined if we use the refined index set I instead of the finite set S. In this case, we see, as a corollary of Theorem 8, that our dimension-adaptive approach corresponds to the dimensionadaptive sparse grid method as introduced in \ I which satisfy the admissibility condition <ref type="bibr" target="#b29">(30)</ref> with respect to the set I. Altogether, the algorithm allows for an adaptive detection of the important dimensions and heuristically constructs optimal index sets I in the sense of Bungartz and Griebel <ref type="bibr" target="#b2">[3]</ref>. Note that this is closely related to best N-term approximation <ref type="bibr" target="#b5">[6]</ref>.</p><p>Remember that the ε-cost analysis in Section 5 was also based on the values k f , which are used here for the error estimation. We can thus expect that dimension-adaptive sparse grid methods correctly identify the optimal index sets I ,γ from (47) provided no early determination problems occur. In this case, the results of Section 5 can also be used to show that dimension-adaptive sparse grid methods can avoid the curse of dimension in weighted function spaces whose weights decay sufficiently fast. For comparison recall that Algorithm 6.1 can be based on two separate types of adaptivity. The important anchored-ANOVA terms If u are found in a dimension-adaptive fashion with help of the weights γ u and are approximated by q u using possibly locally adaptive methods. In Algorithm 6.2 the calculation of the contributions ∆ k is more restrictive since the telescoping sum expansion has to hold. The algorithm is already completely determined by the choice of the univariate quadrature rule U m k . While Algorithm 6.1 has the advantage that low regularity of low-order anchored-ANOVA terms can be resolved by local adaptivity, Algorithm 6.2 has the advantage that modelling and discretization errors are simultaneously taken into account and can thus be balanced in an optimal way. </p><formula xml:id="formula_139">= s + ∆ k f . 2) Move the index k ∈ A with the largest weight |∆ k f | from A to I. until |∆ k f | ≤ ε ; Set SG I f = s.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Numerical results</head><p>In this section, we use multivariate integrals from finance (corresponding to Asian options, zero coupon bonds and collateralized mortgage obligations) to investigate the performance of sparse grids and other dimension-wise quadrature methods of the form <ref type="bibr" target="#b20">(21)</ref>. We first describe the setting of our numerical experiments. Then we compare the convergence behaviour of different numerical methods and relate the results to the effective dimensions of our model problems in the classical and in the anchored case. 15   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Setting</head><p>The model problems considered in our numerical experiments all lead to high-dimensional integrals of the form</p><formula xml:id="formula_140">I d := 1 (2π ) d/2 √ det(C) R d f (W) e -1 2 W T C -1 W dW,<label>(64)</label></formula><p>where </p><formula xml:id="formula_141">W ∈ R d , C ∈ R d×d</formula><formula xml:id="formula_142">I d := R d f (Az) ϕ d (z)dz<label>(65)</label></formula><p>with standard Gaussian weight ϕ d . It is well known that the matrix A can be chosen in many different ways. Here we consider the following methods, which are usually interpreted as different generating methods of the path of a Brownian motion <ref type="bibr" target="#b14">[15]</ref>.</p><p>• In the random walk (RW) construction the path of the Brownian motion is generated sequentially in time. Here A is the Cholesky matrix of C.</p><p>• In the Brownian bridge (BB) construction, see <ref type="bibr" target="#b19">[20]</ref>, the path is constructed in a hierarchical way.</p><p>This has the effect that for many payoff functions more importance is placed on the leading variables. The corresponding matrix A is given in <ref type="bibr" target="#b35">[36]</ref> explicitly. <ref type="bibr" target="#b14">15</ref> The effective dimensions in the anchored case can directly be derived from the q u values, see Section 2.2. In the worst case 2 d many integrals are needed to compute σ (f ) with sufficient precision. In practice, however, often many of the terms |If u | are very small or zero, in particular if |u| is large. Then Algorithm 6.1 can be used to compute σ (f ) even in high dimensions, which we demonstrate in Sections 7.3 and 7.4 using examples with d = 256 and d = 512. In the numerical experiments in Section 7 we apply Algorithm 6.1 with the threshold ε = 10 -6 f (a). For the derivation of the truncation dimensions in the classical case, we use the algorithm from <ref type="bibr" target="#b37">[38]</ref>. It requires the computation of several integrals with up to 2d -1 dimensions.</p><p>For their approximation we used 2 16 Sobol quasi-Monte Carlo points. To compute the superposition dimension the recursive method described in <ref type="bibr" target="#b38">[39]</ref> can be used. Because of cancellation problems and costs which are exponential in the superposition dimension, the computation of the superposition dimension is only feasible for moderately high-dimensional function.</p><p>• In the principal component (PCA) construction <ref type="bibr" target="#b0">[1]</ref> the matrix A results from the eigenvalue decomposition of C. If we disregard the payoff function this construction maximizes the concentration of the total variance in the leading dimensions. The corresponding matrix A can be found in <ref type="bibr" target="#b9">[10]</ref>.</p><p>• The linear transformation (LT) construction, see <ref type="bibr" target="#b14">[15]</ref>, aims to identify the matrix A which minimizes the effective truncation dimension of the integrand in the classical ANOVA sense. While BB and PCA do not respect the particular structure of the function f , the LT construction in addition takes the gradient of f at a certain anchor point into account. The corresponding matrix A is constructed column by column from a Gram-Schmidt like local minimization, which involves f . For details see <ref type="bibr" target="#b14">[15]</ref>, where this approach is referred to by LT-II.</p><p>Below we will study the efficiency of different sparse grid methods to compute the integral (65). To this end, remember that the univariate quadrature rule U m k in the sparse grid construction of Section 4 was left open. If a quadrature rule is used which is defined on R, then the integral (65) can be treated with a sparse grid method directly on R d . In our tests, we will use the dimension-adaptive sparse grid method based on the Gauss-Hermite rule and refer to this specific method as SGH.</p><p>To apply a univariate quadrature rule on [0, 1], it is necessary to transform the integral (65) over R d into an integral over the unit cube [0, 1] d . To this end, we use the standard component-wise substitution z = Φ -1 (x), where Φ denotes the cumulative normal distribution function. <ref type="foot" target="#foot_8">16</ref> This yields</p><formula xml:id="formula_143">I d := [0,1] d g(x)dx (66)</formula><p>with the integrand g(x) := f (AΦ -1 (x)).</p><p>This way we can apply the dimension-adaptive sparse grid method based on the Gauss-Patterson rule as univariate rule U m k , which we refer to as SGP. This method was first presented in <ref type="bibr" target="#b8">[9]</ref>. It is a special case of the method (21), compare Theorem 8.</p><p>In our tests, we will also consider different variants of the dimension-wise quadrature method (21), which are not of sparse grid form, but which can resolve low regularity in the low-order anchored-ANOVA terms using local adaptivity. To this end, recall that we still have to specify the quadrature rules Q u and the index set S to finalize the construction of the method (21) from Section 3. Here we choose the Q u 's as follows: If |u| &lt; 4, we use the locally adaptive product method CUHRE, see <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13]</ref>, to address low regularity of the terms P u f . If |u| ≥ 4, we use a randomised quasi-Monte Carlo method based on Sobol point sets to lift the dependence on the dimension. We always use the anchor point (1/2, . . . , 1/2). For the construction of the index set S (i.e. for finding the most important terms of the anchored-ANOVA decomposition) we employ the a priori constructions from Section 3.3 and the a posteriori construction from Section 6. This defines the following three new quadrature methods:</p><p>• mixed CUHRE/QMC method with order-dependent weights (COW),</p><p>• mixed CUHRE/QMC method with product weights (CPW),</p><p>• mixed CUHRE/QMC method with dimension adaptivity (CAD).</p><p>We refer to COW, CPW and CAD as mixed CUHRE/QMC methods. To our knowledge these methods are the first numerical quadrature methods which can profit from low effective dimension (by the selection of appropriate function space weights or by dimension adaptivity) and which can at the same time resolve low regularity to some extent by local adaptivity.</p><p>For comparison we also consider Monte Carlo integration (MC) and quasi-Monte Carlo integration based on Sobol point sets (QMC). These two methods are most commonly used for the computation of high-dimensional integrals. In preliminary numerical experiments, Sobol point sets turned out to be the most efficient representative of several quasi-Monte Carlo variants. 17</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Asian options</head><p>We first consider the commonly used test problem to determine the fair value of an Asian option with geometric average. The arising integrands are explicitly given in <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>. Here, we distinguish the two cases K = 0 and K = 100 (where K denotes the strike price of the option) which we refer to as Asian0 and Asian100, respectively. While we obtain a smooth integrand in the first case, the integrand has discontinuous first derivatives in the latter case. We use the same parameters as in <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> and consider the case d = 16.</p><p>We first show in Table <ref type="table" target="#tab_6">2</ref> the truncation dimensions of the arising integral in the classical and in the anchored case for different proportions α ∈ [0.9, 0.9999]. One can see that the truncation dimensions in the classical case almost coincide with the truncation dimensions in the anchored case. <ref type="bibr" target="#b17">18</ref> For instance, for the case K = 0 with α = 0.999 we obtain d t = 15, 14, 3, 1 using RW, BB, PCA and LT, respectively, for the anchored case as well as for the classical one. The LT construction achieves the optimal result d t = 1 in both cases. One can show that this holds even for the extreme case α = 1, i.e. this problem can be reduced by LT to one with only one nominal dimension.</p><p>Further numerical calculations, see Table <ref type="table" target="#tab_7">3</ref>(a), show that the Asian0 problem is of very low superposition dimension d s ≤ 2 independent of the employed path construction. For the Asian100 problem, d s increases if we switch from LT to PCA, to BB and RW as can be seen in Table <ref type="table" target="#tab_7">3</ref>(b).</p><p>Next we compute the integral values using different numerical approaches (MC, QMC, COW, CPW, CAD, SGP and SGH) and different path constructions (RW, BB, PCA, LT) as introduced in Section 7.1. We display the convergence behaviour of these methods in Figs. <ref type="figure" target="#fig_14">3</ref> and<ref type="figure" target="#fig_0">4</ref>. There, we show the number of function evaluations which is required by each of the different numerical methods to obtain a fixed accuracy.</p><p>One can see that the convergence rate of the MC method is always about 0.5 as predicted by the law of large numbers. The rate is not affected by the path construction since the total variance stays <ref type="bibr" target="#b16">17</ref> We compared Halton, Faure, Sobol low discrepancy point sets and three different lattice rules based on the CBC construction from <ref type="bibr" target="#b29">[30]</ref>. The lattice rules yield in many cases equal or even more precise results as Sobol points if good function space weights are used for their construction. But the selection of good weights is a priori not always clear. It would be interesting to see if our anchored-ANOVA weights γ i in <ref type="bibr" target="#b24">(25)</ref> can successfully be used in the CBC construction. 18 Note that our numerical computations for the anchored case with K = 100 and RW or BB might be inaccurate. For these particular problems accurate results are difficult to obtain since the truncation dimension is high (hence many terms have to be integrated) and the integrals are not smooth (hence their computation is expensive). unchanged. The convergence rate of the QMC method increases if BB, PCA or LT is used since these path constructions concentrate the total variance in the first few dimensions. This way QMC outperforms MC and achieves higher convergence rates of almost one, smaller relative errors and a less oscillatory convergence behaviour. From Figs. <ref type="figure" target="#fig_14">3</ref> and<ref type="figure" target="#fig_0">4</ref> we also observe that the impact of the path construction is considerably bigger in case of the dimension-wise quadrature methods COW, CPW, CAD, SGP and SGH. This is explained by the fact that these methods are tailored to the effective dimension of the problem by the choice of the respective function space weights from Section 3.3, or by the dimension-adaptive grid refinement. The convergence of these methods is thus significantly accelerated by path constructions which reduce the effective dimension of the associated integrand. For instance, in the case K = 0, compare Fig. <ref type="figure" target="#fig_14">3</ref>, one can see that the performance of dimension-wise quadrature methods significantly improves if we switch from RW to BB, to PCA and then to LT. While COW, CPW, CAD and SGP provide results which are similar to or even worse than (Q)MC in case of RW, they outperform (Q)MC slightly, clearly and drastically in case of BB, PCA and LT, respectively. Note here that two different regimes have to be distinguished to describe the convergence behaviour of these methods, compare, e.g., Fig. <ref type="figure" target="#fig_14">3(d</ref>). In the preasymptotic regime, the methods COW, CPW, CAD, SGP and SGH first search for the important dimensions and interactions, whereas, in the asymptotic regime, the important dimensions are identified and the grid is then refined only in these directions.</p><p>Since the LT construction reduces the problem to only one dimension, its combination with dimension-adaptive methods is particularly efficient. We see from Fig. <ref type="figure" target="#fig_14">3(d</ref>) that the sparse grid and the dimension-adaptive methods correctly identify the important dimension and then only refine in this respect, which leads to an extremely rapid convergence in the asymptotic regime.</p><p>A comparison of the convergence rates of the COW, CPW and CAD method shows that the a priori constructions (with order-dependent weights or product weights) and the dimension-adaptive construction of the index set S lead to very similar results. The results of COW and CPW even coincide in most cases.</p><p>For the Asian0 problem, SGH is by far the most efficient method independent of the employed path construction. It exploits the low effective dimension by its dimension-adaptive grid refinement and can profit from the smoothness of the integrand much better than all other approaches since it avoids the transformation to the unit cube. This way we obtain relative errors smaller than 10 -12 with only about 10 5 , 10 4 , 10 3 and 10 2 function evaluations in case of RW, BB, PCA and LT, respectively, which is 7-10 orders of magnitude more precise than the results of QMC.</p><p>Comparing the two cases K = 0 and K = 100, we furthermore see that the convergence rates of the QMC method are only slightly affected by the kink in the integrand, whereas the SG methods clearly suffer from the low degree of regularity. This drawback is to some extent overcome by the COW, the CPW and the CAD method, which are in combination with PCA or LT the most efficient approaches for the Asian100 problem. These methods profit from the low effective dimension and can in addition deal with the low regularity of the integrand by local adaptivity in the low-order anchored-ANOVA terms due to the CUHRE approach. With LT, these methods obtain relative errors smaller than 10 -8 with only about 1000 function evaluations, see Fig. <ref type="figure" target="#fig_16">4(d)</ref>, which is about 100,000 times more accurate than the results of QMC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Zero coupon bonds</head><p>We now consider the problem to price zero coupon bonds using the Vasicek model. This model problem is taken from <ref type="bibr" target="#b20">[21]</ref>. It is also studied in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b38">39]</ref>. We use the same parameters as in <ref type="bibr" target="#b20">[21]</ref> and consider the case d = 512.</p><p>First we again study the effective dimensions of this problem. In Table <ref type="table" target="#tab_8">4</ref>(a) one can see that the effective dimensions in the classical case almost coincide with the effective dimensions in the anchored case. For instance, for α = 0.99 we obtain d t = 420, 7, 1, 1 using RW, BB, PCA and LT, respectively, for the anchored case and d t = 419, 7, 1, 1 for the classical one. We conjecture that a more precise numerical computation would yield even exact equal results and we believe that this equality holds for a wider class of functions. Note however that this does not hold in general. We will give a counterexample in Section 7.4.</p><p>Observe that the path construction has a significant impact on the truncation dimensions. For α close to one the dimensions d t are almost as large as the nominal dimension d = 512 if we employ the RW approach. The dimensions d t are significantly smaller if BB, PCA or LT is used instead. The LT construction even obtains the optimal result d t = 1 for this problem. While it is not surprising that such an optimal transformation exists, 19 it is nevertheless interesting that it is correctly identified by the LT construction, which takes only the gradient of the integrand at a certain anchor point into account. Further computations show, see Table <ref type="table" target="#tab_9">5</ref>(a), that the integral is of very low superposition dimension d s ≤ 2 in the anchored case and that d s is almost independent of the path construction.</p><p>We display the convergence behaviour of the different numerical methods using the different path generating methods for the Vasicek problem in Fig. <ref type="figure" target="#fig_18">5</ref>. We observe similar results as for the Asian0 problem despite the fact that the nominal dimension d = 512 is significantly higher here.</p><p>With RW and BB, the methods COW, CPW and CAD provide similar results as QMC. They partly seem to stop to converge, which happens if important contributions to the integral value are contained in anchored-ANOVA terms of higher order that are not yet identified. For instance, in Fig. <ref type="figure" target="#fig_18">5(b</ref>) one can see that 10 6 function evaluations are not sufficient to find those contributions which would reduce the relative error to be below 10 -6 . The dimension-wise integration methods again clearly   profit from path constructions which lead to low effective dimension. In combination with LT, see Fig. <ref type="figure" target="#fig_18">5</ref>(d), all these methods again correctly identify the important dimension of the problem. This way the dependence on the dimension is completely avoided in the asymptotic regime. There, the convergence of the methods is as fast as it is known for univariate problems despite the high nominal dimension d = 512. SGH is again by far the most efficient method. It outperforms (Q)MC by several orders of magnitude independent of the employed path construction. By exploiting the low effective dimension and smoothness of the integrand, SGH achieves in combination with PCA or LT almost machine accuracy with only about 1000 function evaluations.</p><formula xml:id="formula_144">= 512) 1 -α RW BB PCA LT 1e-1 1 1 1 1 1e-2 1 1 1 1 1e-3 1 1 1 1 1e-4 2 2 1 1 (b) CMO problem (d = 256) 1 -α RW BB PCA LT 1e-1 1 1 1 1 1e-2 2 2 1 2 1e-3 2 2 2 2 1e-4 2 2<label>2</label></formula><p>To better understand the fast performance of the SGH method we look at the index set I that is build up by Algorithm 6.2 in a dimension-adaptive way. For the visualization we consider the twodimensional slices through the index set I that correspond to the set of 512-dimensional indices k of the form (k 1 , k 2 , 1, . . . , 1) and (k 1 , 1, . . . , 1, k 2 ) for k 1 , k 2 ≥ 1, respectively. The resulting index sets are shown in Fig. <ref type="figure" target="#fig_11">2</ref> for the example of the Brownian bridge construction. In Fig. <ref type="figure" target="#fig_11">2</ref> all indices k are marked with a dot that are included in the index set I which is build up by Algorithm 6.2 if the threshold ε = 10 -3 is used, compare also Fig.   to capture all indices k that correspond to significant contributions to the integral value. This explains why high precisions can be achieved with only little costs by the dimension-adaptive SGH method. <ref type="foot" target="#foot_9">20</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Collateralized mortgage obligations</head><p>We finally deal with the problem to price a collateralized mortgage obligation (CMO). This model problem is described in detail in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25]</ref>. There and in several further references, it is used to study the performance of quasi-Monte Carlo methods. It is also considered in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> to demonstrate the efficiency of SG methods. We use the same parameters as in <ref type="bibr" target="#b7">[8]</ref> and consider the case d = 256.</p><p>We first computed the superposition dimension in the anchored case for the CMO problem.</p><p>We obtained again d s ≤ 2 for all α ∈ [0.9, 0.9999] and all path constructions, see Table <ref type="table" target="#tab_9">5</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b).</head><p>The truncation dimensions d t of this problem are shown in Table <ref type="table" target="#tab_8">4</ref>(b). It is striking that the path construction has only a small impact on the truncation dimension in the anchored case, i.e., the advantage of BB, PCA and LT compared to RW is not so clear for the CMO problem. For α = 0.9 we have d t = 123 in case of RW and LT. This truncation dimension is reduced to d t = 18 and d t = 13 if BB and PCA is used, respectively. For higher accuracy requirements, however, i.e. for α ≥ 0.99, significantly  less or even no reduction at all is achieved with these constructions. Note that for the CMO problem the effective dimensions in the classical case clearly differ from the truncation dimensions in the anchored case. There BB, PCA and LT lead to significant dimension reductions. LT even reduces the problem to the truncation dimension one in the classical case.</p><p>We next study the convergence behaviour of the different numerical methods for this problem. The respective numerical results are illustrated in Fig. <ref type="figure" target="#fig_21">6</ref>. One can see that the QMC method converges faster, less oscillatory and superior to MC if we switch from RW to BB, PCA or LT. SGP performs similar to QMC in case of BB and PCA and slightly worse in case of RW and LT. The mixed CUHRE/QMC methods COW, CPW and CAD attain the best results in combination with PCA. In this case they outperform (Q)MC and SGP. One can finally see that SGH combined with BB or PCA is by far the most efficient method for the CMO problem. It achieves the highest convergence rate and the most precise results. With 10 4 function evaluations SGH obtains a relative error which is about 100 times smaller than the relative error of the QMC method.</p><p>We next discuss the relation of the convergence behaviour of the numerical methods to the effective dimension of the CMO problem. We already showed that the path construction affects both the performance of the numerical methods (except for MC) and the truncation dimension of the integral. Since the truncation dimension in the classical case differs from the truncation dimension in the anchored case for this problem, it is interesting to see which of these two notions better predicts the convergence behaviour of the numerical methods. Remember that LT does not lead to an improved convergence of the dimension-wise quadrature methods compared to RW. This observation cannot be  explained by the effective dimension in the classical case since LT obtains the optimal result d t = 1 for the CMO problem. The observation is, however, in clear correspondence with the fact that LT provides no reduction of the truncation dimension in the anchored case. This indicates that the performance of the dimension-wise quadrature methods depends on the truncation dimension in the anchored case, but not on the truncation dimension in the classical case. Note that the QMC method converges faster and less oscillatory with LT than with RW. This indicates that the convergence behaviour of the QMC method is rather related to the effective dimension d t in the classical case than to the anchored one.</p><p>The different effective dimensions in the anchored and in the classical case are related to the fact that in the anchored-ANOVA decomposition the contributions If u are of different sign for different u in the CMO problem. Summing the contributions thus leads to cancellation effects which are not seen in the anchored case since the absolute values |If u | are taken into account there. Nevertheless, also the error indicator of dimension-adaptive sparse grid methods and of other dimension-wise integration methods is based on the absolute values |∆ k | and |q u |, respectively, see Algorithms 6.2 and 6.1. These methods can thus also not profit from such cancellation effects and their convergence behaviour therefore rather depends on the effective dimensions in the anchored case than on the effective dimension in the classical case.</p><p>Note finally that the truncation dimension d t in the anchored case explains the impact of the path construction but not the high performance of the SGH methods since d t is high for this problem. The   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Concluding remarks</head><p>In this article, we introduced a new general class of methods for the computation of highdimensional integrals, which we referred to as dimension-wise quadrature methods. Our starting point was the anchored-ANOVA decomposition, which has (compared to the classical ANOVA decomposition) the advantage that only a finite number of function values is required for its computation. Our new methods then resulted from truncation of the anchored-ANOVA decomposition and from integration of the remaining terms using appropriate low-dimensional quadrature rules.</p><p>We discussed a priori (using function space weights) and a posteriori (using dimension adaptivity) approaches for the truncation and derived bounds for the resulting modelling error. To this end, we introduced a new notion of effective dimensions in the anchored case. We showed that the presented analysis also applies to sparse grid methods as they can be regarded as special cases of our general approach. We explained that sparse grid methods intertwine the truncation of the anchored-ANOVA series and the subsequent discretization which allows to balance modelling and discretization error in an optimal way.</p><p>We also presented dimension-wise quadrature methods which are not of sparse grid form but use the CUHRE method for the integration of the low-order anchored-ANOVA terms and quasi-Monte Carlo for the higher-order ones. This way, we obtained mixed CUHRE/QMC methods which are to our knowledge the first numerical quadrature methods which can profit from low effective dimension by dimension adaptivity and can at the same time deal with low regularity by local adaptivity. A correct balancing of modelling and discretization errors is then more difficult. Numerical experiments for the Asian option as a test function from finance with discontinuous first derivatives demonstrate, however, that this disadvantage is more than compensated by the benefits of the local adaptivity. The numerical results showed the superiority of our new method to (quasi-)Monte Carlo methods and sparse grid methods for this model problem.</p><p>We considered further application problems from finance which lead to the integration of smooth functions with up to 512 dimensions. For these model problems the dimension-adaptive sparse grid method based on the Gauss-Hermite rule turned out to be most efficient. This method profits from the low effective dimension of the integral by its dimension-adaptive grid refinement and optimally exploits the smoothness of the integrand since it avoids the singular transformation to the unit cube. This way (quasi-)Monte Carlo methods were outperformed by several orders of magnitude even in hundreds of dimensions. We finally analysed the effective dimensions of our application problems in the classical and in the anchored case and showed that the results mainly explain the behaviour of our numerical quadrature methods.</p><p>Of course, our results could be extended into various directions. For instance, it would be interesting to identify the function classes for which the effective dimension in the anchored case coincides with the effective dimension in the classical case. We indicated that the pricing problems of Asian options or zero coupon bonds belong to such a function class, but not the CMO problem. Note further that we always used the center of the integration domain as anchor point. Of course also other choices are possible. It would be of interest to analyse the impact of the anchor point in more detail and to understand how the anchor may be chosen best, see also <ref type="bibr" target="#b33">[34]</ref> or <ref type="bibr" target="#b36">[37]</ref>. Other possible areas for future research include improvements of our dimension-wise quadrature methods that are not of sparse grid form by, e.g., a more sophisticated balancing of modelling and discretization errors. For applications from finance, our mixed CUHRE/QMC methods can be further improved if, instead of the CUHRE method, a different locally adaptive method is employed which treats the integrals directly on R d such that the singular transformation to the unit cube can be avoided. The local error estimator of such a method could for example be based on Genz-Keister points, see <ref type="bibr" target="#b6">[7]</ref>.</p><p>Note furthermore that our dimension-wise approach cannot only be used for integration but also for the representation and approximation of high-dimensional functions in the sense of Rabitz and Alis <ref type="bibr" target="#b27">[28]</ref>. In this context, it would be interesting to study the efficient computation of further quantities, which cannot be formulated as expected values or integrals, but which are also important for financial institutions, such as quantiles. Quasi-Monte Carlo methods can compute also quantiles very efficiently as shown by Papageorgiou and Paskov <ref type="bibr" target="#b23">[24]</ref> using the example of Value at Risk calculations. To our knowledge, it is not yet known if similar or even better results can be obtained with dimension-wise quadrature methods based on sparse grids. We finally remark that most of our methods and results are not restricted to applications from finance, but can also be used in other application areas such as chemistry or physics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Lemma 4 .</head><label>4</label><figDesc>Let d s denote the superposition dimension of the function f in the anchored case with proportion α and let f d s (x) :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>v ⊆ u which satisfy |v| = j. Using the definition of ε(j), we can bound the discretization error by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>be a multivariate function. Then, the d-dimensional integral If can be represented by the infinite telescoping sum If = k∈N d k f (28) which collects the products of each possible combination of the univariate difference formulae. Here, k ∈ N d denotes a multi-index with k j &gt; 0 and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>γ . Using the tensor product structure k = d i=1 ∆ k i we obtain the assertion. Motivated by (41), we refer to b k in the following as the local benefit associated with the index k ∈ N d . The global benefit of the method (31) is then given by B I := k∈I b k .(44) This leads to the restricted optimization problem max n I =w B I , w ∈ N to maximize the global benefit B I for fixed global costs n I . Using the argument from [3], this global optimization problem can be reduced to the problem of ordering the local cost-benefit ratios</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) γ = (1, 1). (b) γ = (1, 2 -8 ). (c) γ = (2 -8 , 2 -8 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Optimal index sets I ,γ on the level = 7.</figDesc><graphic coords="18,284.65,63.38,85.32,85.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>n d := 0 for n &lt; d and that x d := x d for x ∈ R.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>+</head><label></label><figDesc>log 2 (γ {1,...,j} )/4 -1 j -1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc><ref type="bibr" target="#b8">[9]</ref>. This method finds the index set I in a dimension-adaptive way with the help of the error indicators | k f |. Starting with the smallest index set I = {(1, . . . , 1)}, those admissible indices k are added step by step to I which are expected to provide the largest error reduction. The resulting dimension-adaptive construction of the index set 14 is shown in Algorithm 6.2. There, A denotes the subset of all indices k ∈ N d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Algorithm 6 . 2 : 1 )</head><label>621</label><figDesc>Dimension-adaptive sparse grid construction of the index set I. Initialise: Let I = {(1, . . . , 1)} and s = (1,...,1) f . repeat Compute the values ∆ k f from (29) for all k ∈ A for which ∆ k f has not yet been computed and set s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>and f : R d → R. In our examples, the integral value I d describes the price of a financial derivative. The vector W relates to the path of an underlying stochastic process, C to the covariance matrix of the process and f to the payoff function of the financial contract. Using the factorization C = AA T with A ∈ R d×d and the substitution W = Az the integral is transformed into the integral</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>2 (</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Two-dimensional slices through the index set I corresponding to the threshold ε = 10 -3 for the Vasicek problem with the Brownian bridge construction (d = 512).</figDesc><graphic coords="29,108.30,238.60,123.84,104.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Convergence behaviour of the different methods for the Asian0 problem (d = 16).</figDesc><graphic coords="30,59.57,225.59,166.32,131.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>(a) Random walk. (b) Brownian bridge. (c) Principal components. (d) Linear transformation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Convergence behaviour of the different methods for the Asian100 problem (d = 16).</figDesc><graphic coords="31,65.51,227.88,167.40,133.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>(a) Random walk. (b) Brownian bridge. (c) Principal components. (d) Linear transformation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Convergence behaviour of the different methods for the Vasicek problem (d = 512).</figDesc><graphic coords="32,51.52,227.15,174.60,134.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>fast convergence is explained by the low superposition dimension d s ≤ 2 and by the smoothness of the integrand.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>(a) Random walk. (b) Brownian bridge. (c) Principal components. (d) Linear transformation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Convergence behaviour of the different methods for the CMO problem (d = 256).</figDesc><graphic coords="33,52.47,228.93,179.64,138.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>) either if the function f tr := f d t as in Lemma 1 or if f tr := f d s as in Lemma 2 and if α is the proportion corresponding to d t and d s , respectively. One can see that quadrature methods produce small errors if α is close to one and if the methods can compute If tr efficiently.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Theorem 6 (Error Versus Cost). If we choose S = S d t ,d s and Q u to be the |u|-dimensional tensor product of the rule U m with m</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1</head><label>1</label><figDesc>The values * := d + α/4 log 2 (d!) for α ∈ {0, 1, 2, 3} and different values d.</figDesc><table><row><cell>α \ d</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>10</cell><cell>50</cell><cell>100</cell><cell>360</cell><cell>1024</cell></row><row><cell>0</cell><cell>1</cell><cell>4</cell><cell>6</cell><cell>11</cell><cell>51</cell><cell>101</cell><cell>361</cell><cell>1025</cell></row><row><cell>1</cell><cell>1</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>105</cell><cell>232</cell><cell>997</cell><cell>3217</cell></row><row><cell>2</cell><cell>1</cell><cell>5</cell><cell>9</cell><cell>22</cell><cell>158</cell><cell>363</cell><cell>1633</cell><cell>5409</cell></row><row><cell>3</cell><cell>1</cell><cell>6</cell><cell>11</cell><cell>27</cell><cell>212</cell><cell>495</cell><cell>2268</cell><cell>7601</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2</head><label>2</label><figDesc>Truncation dimensions of the Asian option pricing problems (see footnote 18).</figDesc><table><row><cell cols="2">(a) Asian0 (d = 16)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1 -α</cell><cell cols="2">Anchored ANOVA</cell><cell></cell><cell></cell><cell cols="2">Classical ANOVA</cell><cell></cell><cell></cell></row><row><cell></cell><cell>RW</cell><cell>BB</cell><cell>PCA</cell><cell>LT</cell><cell>RW</cell><cell>BB</cell><cell>PCA</cell><cell>LT</cell></row><row><cell>1e-1</cell><cell>9</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>9</cell><cell>2</cell><cell>1</cell><cell>1</cell></row><row><cell>1e-2</cell><cell>13</cell><cell>6</cell><cell>2</cell><cell>1</cell><cell>13</cell><cell>6</cell><cell>2</cell><cell>1</cell></row><row><cell>1e-3</cell><cell>15</cell><cell>14</cell><cell>3</cell><cell>1</cell><cell>15</cell><cell>14</cell><cell>3</cell><cell>1</cell></row><row><cell>1e-4</cell><cell>16</cell><cell>16</cell><cell>6</cell><cell>1</cell><cell>16</cell><cell>16</cell><cell>6</cell><cell>1</cell></row><row><cell cols="2">(b) Asian100 (d = 16)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Anchored ANOVA</cell><cell></cell><cell></cell><cell cols="2">Classical ANOVA</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RW</cell><cell>BB</cell><cell>PCA</cell><cell>LT</cell><cell>RW</cell><cell>BB</cell><cell cols="2">PCA</cell><cell>LT</cell></row><row><cell>10</cell><cell>3</cell><cell>1</cell><cell>1</cell><cell>10</cell><cell>2</cell><cell>1</cell><cell></cell><cell>1</cell></row><row><cell>10</cell><cell>4</cell><cell>2</cell><cell>1</cell><cell>14</cell><cell>7</cell><cell>2</cell><cell></cell><cell>1</cell></row><row><cell>10</cell><cell>7</cell><cell>3</cell><cell>1</cell><cell>15</cell><cell>14</cell><cell>3</cell><cell></cell><cell>1</cell></row><row><cell>13</cell><cell>15</cell><cell>5</cell><cell>1</cell><cell>16</cell><cell>16</cell><cell>6</cell><cell></cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3</head><label>3</label><figDesc>Superposition dimensions in the anchored case.</figDesc><table><row><cell>(a) Asian0 (d = 16)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1 -α</cell><cell>RW</cell><cell>BB</cell><cell>PCA</cell><cell>LT</cell></row><row><cell>1e-1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>1e-2</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>1e-3</cell><cell>2</cell><cell>2</cell><cell>1</cell><cell>1</cell></row><row><cell>1e-4</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>1</cell></row><row><cell>(b) Asian100 (d = 16)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1 -α</cell><cell>RW</cell><cell>BB</cell><cell>PCA</cell><cell>LT</cell></row><row><cell>1e-1</cell><cell>7</cell><cell>2</cell><cell>1</cell><cell>1</cell></row><row><cell>1e-2</cell><cell>8</cell><cell>3</cell><cell>1</cell><cell>1</cell></row><row><cell>1e-3</cell><cell>8</cell><cell>4</cell><cell>2</cell><cell>1</cell></row><row><cell>1e-4</cell><cell>8</cell><cell>4</cell><cell>3</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc>Truncation dimensions of the Vasicek and CMO problem.</figDesc><table><row><cell cols="2">(a) Vasicek problem (d = 512)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1 -α</cell><cell cols="2">Anchored ANOVA</cell><cell></cell><cell></cell><cell cols="2">Classical ANOVA</cell><cell></cell><cell></cell></row><row><cell></cell><cell>RW</cell><cell>BB</cell><cell>PCA</cell><cell>LT</cell><cell>RW</cell><cell>BB</cell><cell>PCA</cell><cell>LT</cell></row><row><cell>1e-1</cell><cell>302</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>305</cell><cell>2</cell><cell>1</cell><cell>1</cell></row><row><cell>1e-2</cell><cell>420</cell><cell>7</cell><cell>1</cell><cell>1</cell><cell>419</cell><cell>7</cell><cell>1</cell><cell>1</cell></row><row><cell>1e-3</cell><cell>471</cell><cell>16</cell><cell>2</cell><cell>1</cell><cell>471</cell><cell>16</cell><cell>2</cell><cell>1</cell></row><row><cell>1e-4</cell><cell>494</cell><cell>59</cell><cell>5</cell><cell>1</cell><cell>494</cell><cell>52</cell><cell>5</cell><cell>1</cell></row><row><cell cols="2">(b) CMO problem (d = 256)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Anchored ANOVA</cell><cell></cell><cell></cell><cell cols="2">Classical ANOVA</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RW</cell><cell>BB</cell><cell>PCA</cell><cell>LT</cell><cell>RW</cell><cell>BB</cell><cell></cell><cell>PCA</cell><cell>LT</cell></row><row><cell>191</cell><cell>134</cell><cell>108</cell><cell>191</cell><cell>110</cell><cell>10</cell><cell></cell><cell>5</cell><cell>1</cell></row><row><cell>225</cell><cell>192</cell><cell>235</cell><cell>225</cell><cell>158</cell><cell>36</cell><cell></cell><cell>11</cell><cell>1</cell></row><row><cell>242</cell><cell>229</cell><cell>254</cell><cell>242</cell><cell>181</cell><cell>80</cell><cell></cell><cell>23</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5</head><label>5</label><figDesc>Superposition dimensions in the anchored case.</figDesc><table><row><cell>(a) Vasicek problem (d</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>1. For instance, one can see that the index (3, 1, . . . , 1) is included in I but not the index (1, . . . , 1, 3). Moreover, the values | k f | are shown color coded from 10 0 (dark red) to 10 -16 (dark blue) for 1 ≤ k 1 , k 2 ≤ 7. One can see that these values are decaying rapidly for increasing k 1 , k 2 . They are already below 10 -10 if k 1 &gt; 4 or k 2 &gt; 4. With respect to the dimension 512 we see in Fig. 2(b) that |</figDesc><table /><note><p>k f | &lt; 10 -10 already if k 2 &gt; 1, which reflects the low truncation dimension of the Vasicek problem with the Brownian bridge construction. The results shown in Fig. 2 indicate for this particular example that already a rather small index set I is sufficient</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>Note however that the constants of the Monte Carlo method depend on the variance of the integrand, which can exponentially grow with the dimension. In this case also Monte Carlo integration is intractable.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>The approximation error of the truncated anchored-ANOVA decomposition is studied in<ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b36">37]</ref>. There also the impact of the choice of the anchor point is investigated.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>It is also possible to base the definition on the L 1 -norm. Then this drawback disappears. Nevertheless we here stick to the operator |I(•)| to exploit a more direct relation to dimension-adaptive sparse grid methods.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3"><p>Note that this condition is closely related to the admissibility condition<ref type="bibr" target="#b29">(30)</ref> for sparse grid indices in Section 4.1.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4"><p>Note that the constant c(|u|) depends on the norm of P u f and thus also on the smoothness parameter r.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5"><p>Note the close relation to<ref type="bibr" target="#b16">[17]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_6"><p>γ ([0, 1]) at the point 1/2.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_7"><p>Note that in<ref type="bibr" target="#b8">[9]</ref> more sophisticated stopping criteria are used than | k f | ≤ ε.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_8"><p>The resulting transformed integrand is unbounded on the boundary of the unit cube, which is undesirable from a numerical as well as from a theoretical point of view. Nevertheless, in combination with (quasi-)Monte Carlo methods this transformation turns out to be very effective because it cancels the Gaussian weight. But this singular transformation deteriorates the efficiency of quadrature methods which take advantage of higher smoothness, such as sparse grids. Here, it is often better to avoid the transformation and the corresponding loss of regularity and to address the integral directly on R d .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20" xml:id="foot_9"><p>Note that the results shown in Fig.2also indicate that Algorithm 6.2 indeed correctly identifies the most important contributions to the integral value.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank Ian Sloan from the University of New South Wales and two unknown referees for valuable comments and hints.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A comparison of some Monte Carlo and quasi-Monte Carlo methods for option pricing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Acworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Broadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Glasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Monte Carlo and Quasi-Monte Carlo Methods in Scientific Computing</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Hellekalek</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Niederreiter</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Algorithm 698: DCUHRE -an adaptive multidimensional integration routine for a vector of integrals</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bernsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Espelid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Genz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Softw</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="452" to="456" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Bungartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Griebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sparse grids</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Valuation of mortgage backed securities using Brownian bridges to reduce effective dimension</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caflisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Morokoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Owen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Finance</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="46" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Methods of Numerical Integration</title>
		<author>
			<persName><forename type="first">P</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rabinowitz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Nonlinear approximation, Acta Numer</title>
		<author>
			<persName><forename type="first">R</forename><surname>Devore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="51" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fully symmetric interpolatory rules for multiple integrals over infinite regions with Gaussian weight</title>
		<author>
			<persName><forename type="first">A</forename><surname>Genz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Keister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="299" to="309" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Numerical integration using sparse grids</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gerstner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Griebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Algorithms</title>
		<imprint>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="209" to="232" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dimension-adaptive tensor-product quadrature</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gerstner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Griebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="87" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Glasserman</surname></persName>
		</author>
		<title level="m">Monte Carlo Methods in Financial Engineering</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sparse grids and related approximation schemes for higher dimensional problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Griebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Computational Mathematics (FoCM05)</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Pardo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Pinkus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Suli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Todd</surname></persName>
		</editor>
		<meeting><address><addrLine>Santander</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="106" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The smoothing effect of the ANOVA decomposition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Griebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sloan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>University of New South Wales</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cuba -a library for multidimensional numerical integration</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Phys. Comm</title>
		<imprint>
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="page" from="78" to="95" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive sparse grids</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hegland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ANZIAM J</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="335" to="C353" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A general dimension reduction technique for derivative pricing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Imai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Finance</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="155" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On decompositions of multivariate functions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sloan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wasilkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Woźniakowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>University of New South Wales</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Quasi-regression and the relative importance of the ANOVA components of a function</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lemieux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Owen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Monte Carlo and Quasi-Monte Carlo Methods</title>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Fang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Hickernell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Niederreiter</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="331" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The path integral approach to financial modeling and options pricing</title>
		<author>
			<persName><forename type="first">V</forename><surname>Linetsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computat. Econ</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="129" to="163" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Estimating mean dimensionality of analysis of variance decompositions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Owen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="712" to="721" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Smoothness and dimension reduction in quasi-Monte Carlo methods</title>
		<author>
			<persName><forename type="first">B</forename><surname>Moskowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caflisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Comput. Modeling</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="37" to="54" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Toward real-time pricing of complex financial derivatives</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ninomiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tezuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Math. Finance</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">High dimensional integration of smooth functions over cubes</title>
		<author>
			<persName><forename type="first">E</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Math</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="79" to="97" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A multiscale method for the evaluation of Wiener integrals</title>
		<author>
			<persName><forename type="first">E</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Steinbauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Approximation Theory IX</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Chui</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Schumaker</surname></persName>
		</editor>
		<imprint>
			<publisher>Vanderbilt Univ. Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="251" to="258" />
		</imprint>
	</monogr>
	<note>Computational Aspects</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deterministic simulation for risk management</title>
		<author>
			<persName><forename type="first">A</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Paskov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Portfolio Manag</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="122" to="127" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Faster valuation of financial derivatives</title>
		<author>
			<persName><forename type="first">S</forename><surname>Paskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Traub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Portfolio Manag</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="113" to="120" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The exponent of discrepancy of sparse grids is at least 2. 1933</title>
		<author>
			<persName><forename type="first">L</forename><surname>Plaskota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Comput. Mat</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="3" to="24" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The exact exponent of sparse grid quadratures in the weighted case</title>
		<author>
			<persName><forename type="first">L</forename><surname>Plaskota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Wasilkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Complexity</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="840" to="848" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">General foundations of high-dimensional model representations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rabitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Alis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Chem</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="197" to="233" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">When are quasi-Monte Carlo algorithms efficient for high-dimensional integrals?</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sloan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Woźniakowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Complexity</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Constructing randomly shifted lattice rules in weighted Sobolev spaces</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Sloan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Y</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1650" to="1665" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Quadrature and interpolation formulas for tensor products of certain classes of functions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Smolyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dokl. Akad. Nauk SSSR</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="240" to="243" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sensitivity estimates for nonlinear mathematical models</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sobol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Mode. and Comput. Exp</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="407" to="414" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Global sensitivity indices for nonlinear mathematical models and their Monte Carlo estimates</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sobol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Comput. Simulation</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="271" to="280" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Theorems and examples on high dimensional model representation</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sobol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reliab. Eng. and Syst. Saf</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="187" to="193" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Traub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Woźniakowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wasilkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information-Based Complexity</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Academic Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On the effects of dimension reduction techniques on some high-dimensional problems in finance</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oper. Res</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1063" to="1078" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On the approximation error in high dimensional model representation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 2008 Winter Simulation Conference</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Mason</surname></persName>
		</editor>
		<meeting>eeding of the 2008 Winter Simulation Conference</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="453" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The effective dimension and quasi-Monte Carlo integration</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-T</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Complexity</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="101" to="124" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Why are high-dimensional finance problems often of low effective dimension?</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sloan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="159" to="183" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Explicit cost bounds of algorithms for multivariate tensor product problems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wasilkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Woźniakowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Complexity</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="56" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Weighted tensor product algorithms for linear multivariate problems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wasilkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Woźniakowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Complexity</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="402" to="447" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
