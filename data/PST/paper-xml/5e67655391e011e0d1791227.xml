<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Causal Interpretability for Machine Learning -Problems, Methods and Evaluation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-03-19">19 Mar 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Raha</forename><surname>Moraffah</surname></persName>
							<email>rmoraffa@asu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<settlement>Tempe</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mansooreh</forename><surname>Karami</surname></persName>
							<email>mkarami@asu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<settlement>Tempe</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruocheng</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<settlement>Tempe</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adrienne</forename><surname>Raglin</surname></persName>
							<email>adrienne.raglin2.civ@mail.mil</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Army Research Lab</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
							<email>huanliu@asu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<settlement>Tempe</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Causal Interpretability for Machine Learning -Problems, Methods and Evaluation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-03-19">19 Mar 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2003.03934v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Interpratablity</term>
					<term>explainability</term>
					<term>causal inference</term>
					<term>counterfactuals</term>
					<term>machine learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine learning models have had discernible achievements in a myriad of applications. However, most of these models are black-boxes, and it is obscure how the decisions are made by them. This makes the models unreliable and untrustworthy. To provide insights into the decision making processes of these models, a variety of traditional interpretable models have been proposed. Moreover, to generate more humanfriendly explanations, recent work on interpretability tries to answer questions related to causality such as "Why does this model makes such decisions?" or "Was it a specific feature that caused the decision made by the model?". In this work, models that aim to answer causal questions are referred to as causal interpretable models. The existing surveys have covered concepts and methodologies of traditional interpretability. In this work, we present a comprehensive survey on causal interpretable models from the aspects of the problems and methods. In addition, this survey provides in-depth insights into the existing evaluation metrics for measuring interpretability, which can help practitioners understand for what scenarios each evaluation metric is suitable.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>With the surge of machine learning in critical areas such as healthcare, law-making and autonomous cars, decisions that had been previously made by humans are now made automatically using these algorithms. In order to ensure the reliability of such decisions, humans need to understand how these decisions are made. However, machine learning models are usually inherently black-boxes and do not provide explanations for how and why they make such decisions. This has become especially problematic when recent work shows that the decisions made by machine learning models are sometimes biased and enforce inequality <ref type="bibr" target="#b68">[69]</ref>. For instance, Angwin et al. <ref type="bibr" target="#b3">[4]</ref> demonstrates that predictions made by Correctional Offender Management Profiling for Alternative Sanctions (COMPAS), which is a widely used criminal risk assessment tool, shows racial biases. With recent regulations such as European Unions "Right to Ex-planation" <ref type="bibr" target="#b31">[32]</ref> and AI call for diversity and inclusion <ref type="bibr" target="#b8">[9]</ref>, interpretable models which are capable of explaining the decisions they made are necessary. Moreover, recent research shows that machine learning models, especially deep neural networks, can be easily fooled into predicting a specific class label for an image when its pixel values are under minimal perturbations <ref type="bibr">[30; 74; 80]</ref>. Such results imply that machine learning models suffer from the risk of making unexpected decisions. Understanding decisions of machine learning models and the process leading to decision making can help us understand the rules the models use to make their decisions and therefore, prevent potential unexpected situations from happening. More specifically, through interpretable machine learning models, we aim to guarantee that (a) decisions made by machine learning models comply with the rules toward social good; (b) the classifier does not pick up the biases in the data and the decisions made are compatible with human understandings. Previously, various frameworks have been proposed to generate explanations for machine learning algorithms. These algorithms can be mainly divided into two categories, <ref type="bibr" target="#b0">(1)</ref> algorithms that are inherently interpretable, which includes the models that generate explanations at training time <ref type="bibr" target="#b105">[106]</ref>;</p><p>(2) post-hoc interpretations that refer to the model that generate explanations for already made decisions <ref type="bibr">[75; 85; 47]</ref>. Henceforth, these models are referred to as traditional interpretable models. In this work, we focus on causal interpretable models that can explain their decisions through what decisions would have been made if they had been under alternative situations (e.g., being trained with different inputs, model components or hyperparameters). Note that traditional interpretable models are unable to answer such questions about decision making under alternative situations, although they can explain how and why a decision is made by an existing model on an observed instance. For instance, in the case of credit applications, to impose fairness on the decision making process, we may need to answer questions such as Did the protected features (e.g., race and gender etc.) cause the system to reject the application of the i-th applicant?" and "If the i-th applicant had different protected features, would the system still make the same decision?" In other words, in order to make the explanations more understandable and useful for humans, we need to ask questions such as "Why did the classifier make this decision instead of another?", "What would have happened to this decision of a classifier had we had a different input to it?", or "Was it feature X that caused decision Y ?". Traditional interpretability frameworks which only consider correlations are not capable of generating such explanations. This is due to the fact that these frameworks cannot estimate how altering a feature or a component of a model would change the predictions made by the rest of the model or the predicted labels on the data samples. Therefore, in order to answer such questions about both data samples and models, counterfactual analysis needs to be leveraged. Counterfactual analysis is a concept from the causal inference literature <ref type="bibr" target="#b24">[25]</ref>. In counterfactual analysis, we aim to infer the output of a model in imaginary scenarios that we have not observed or cannot observe. Recently, counterfactual analysis and causal inference have gained a lot of attention from the interpretable machine learning field. Research in this area has mainly focused on generating counterfactual explanations from both the data perspective <ref type="bibr">[34; 76]</ref> as well as the components of a model <ref type="bibr">[77; 38]</ref>. Existing surveys on interpretable machine learning focus on the traditional methods and do not discuss the existing methods from a causal perspective. In this survey, we present commonly used definitions for interpretability, discuss interpretable models from a causal perspective and provide guidelines for evaluating these methods. More specifically, in Section 2, we first provide different definitions for interpretability. We then briefly introduce the existing methods on traditional interpretablity and present different types of interpretable models in this category (Section 2.2). Section 3 discusses concepts from causal inference, which are used in this survey. In section 4, we provide an overview of existing works on causal interpretability. We also compare the proposed models for both traditional and causal models from different perspectives to provide insights on advantages and disadvantages of each type of interpretability. Section 5 provides detailed guidelines on the experimental settings such as commonly used datasets and evaluation metrics for both traditional and causal approaches. We then discuss evaluation metrics specifically used for causal methods in more detail and provide different scenarios for which these metrics can be used. Since the evaluation of causal interpretable models is a challenging task, these guidelines can be helpful for future research in this area and can be used to evaluate approaches with similar characteristics. In addition, they can also be used to create new evaluation metrics for the approaches with different functionalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interpretability algorithms</head><p>Traditional interpretability (Section 2.2) Causal interpretability (Section 4)</p><p>Figure <ref type="figure">1</ref>: Main categories for Interpretable frameworks</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">AN OVERVIEW OF INTERPRETABIL-ITY</head><p>In this section, we present an overview of existing definitions for interpretability. Miller et al. <ref type="bibr" target="#b69">[70]</ref> suggest that interpretability is the degree to which a human can understand the cause of a decision. Kim et al. <ref type="bibr" target="#b47">[48]</ref> propose that interpretability is the degree to which a human can consis-tently predict the model's decisions. Doshi-Velez et al. <ref type="bibr" target="#b16">[17]</ref> define interpretability as the ability to explain in intelligible ways to a human. Gilpin et al. <ref type="bibr" target="#b26">[27]</ref> take a step further and define interpretability as a part of explainability. They state that explainable models are those that summarize the reasons for neural network behaviors, gain the trust of the users, or generate insights into the causes of their decisions while interpretable models may not be able to describe the operation of a system in an accurate way<ref type="foot" target="#foot_0">1</ref> . Pearl <ref type="bibr" target="#b83">[84]</ref> claims that tasks such as explainability require a causal model of the environment and cannot be handled at the level of association.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Interpetability in Machine Learning</head><p>Interpretable machine learning has been widely explored and discussed in previous literature. However, to the best of our knowledge, there is no comprehensive review on causal interpretability models. For instance, Lipton <ref type="bibr" target="#b58">[59]</ref> discusses the motivation behind creating interpretable models and categorizes interpretable models into two main categories: transparent models and post-hocs. Doshi-velez et al. <ref type="bibr" target="#b16">[17]</ref> provide a definition of model interpretability and evaluation criteria. However, this review only proposes definitions and evaluations that are used for traditional interpretability of models and does not cover causal and counterfactual questions. Gilpin et al. <ref type="bibr" target="#b26">[27]</ref>, explain fundamental concepts of explainability and use them to classify the literature on interpretable models. Zhang and Zhu <ref type="bibr" target="#b109">[111]</ref> review the existing interpretable models proposed for deep models used in visual domains. Du et al. <ref type="bibr" target="#b17">[18]</ref> provide a comprehensive survey of existing interpretable methods and discuss issues that should be considered in future work. It is worth mentioning that none of the existing work discussed interpretable models from a causal perspective. In this work, we first introduce the state-of-the-art research in traditional interpretability (Sec. 2.2) and then give a detailed survey on causal interpretable models (Sec. 4). Figure <ref type="figure">1</ref> shows an overview of intepretable models and their classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Traditional Interpretablity</head><p>Before proceeding with the detailed review of the methodologies in causal interpretable models, we provide an overview of existing state-of-the-art methods in traditional machine learning. We categorize traditional models into two main categories:</p><p>• Inherently interpretable models: Models that generate explanations in the process of decision making or while being trained.</p><p>• Post-hoc interpretability: Generating explanations for an already existing model using an auxiliary model.</p><p>Example-based interpretablity also falls into this category. In example-based interpretablity, we are looking for examples from the dataset which explain the model's behavior the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Interpretable Models</head><p>A machine learning model can be designed to include explanations embedded as part of their architecture or output interpretable decisions as part of their training process. Most of these models are created in application of the deep neural network. In this section, we present common interpretable models in the literature. Decision Trees. These methods make use of a tree-structured framework in which each internal node checks whether a condition on a feature is satisfied or not while the leaf nodes show the final predictions (class labels). A decision infers the label of an instance by starting from the root and tracing a path till a leaf node is reached, which can be interpreted as an if..then.. rule. An example is illustrated in Figure <ref type="figure" target="#fig_0">2</ref>. Rule-Based Models. Rule-based classifiers also create explanations that are interpretable for humans. These classifiers use a collection of if..then.. rules to infer the class labels. In a sense, rule-based classifiers are the text representation of the decision trees. However, there are some key differences. Rule-based models can have rules that are not mutually exclusive (i.e., two or more rules might trigger by the same record), not exhausted (i.e., a record may not trigger any rules) and ordered (i.e., the rule set is ordered based on their priority) <ref type="bibr" target="#b94">[95]</ref>. Linear Regression. Another common method known to be interpretable is Linear Regression. Linear Regression models the linear relation between a dependent variable and a set of explanatory variables (features). The weight of each feature represents the mean change in the prediction given a one unit increase of the feature. Accordingly, it is reasonable to think that the features with larger weights has more effect on the final result. However, different types of variables (e.g., categorical data vs numerical features) have different scales. This makes it difficult to interpret the effect of each feature. Fortunately, there are several methods that can be used to find the importance of a feature in a linear regression such as t-statistics and chi-square score <ref type="bibr" target="#b56">[57]</ref>.</p><formula xml:id="formula_0">Att1 +1 Att2 Att3 -1 -1 +1 Yes No Yes No Yes No</formula><p>The aforementioned methods are restricted by users' limitations (i.e., human understanding). With the increase in the number of features, these models become more and more complex; for example, decision trees become much deeper, and the number of the rules increase in the rule sets. This makes comprehending the prediction of these models difficult for humans <ref type="bibr" target="#b88">[89]</ref>. Below, we discuss recent inherently interpretable models which are designed for more sophisticated scenarios. Attention Networks. Attention networks have been successful in various highly-impactful tasks such as graph em-bedding <ref type="bibr" target="#b98">[99]</ref> and machine translation <ref type="bibr">[98; 6]</ref>. These models are widely known not only for their improved performance over previous methods but also for their capability to show which input features or learned representations are more important for making a specific prediction. Yang et al. <ref type="bibr" target="#b105">[106]</ref> use a hierarchical attention network in document classification to capture the informative words as well as the sentences that have a significant role in the decision. This is because the same word or sentence may be differentially important in different contexts. Attention networks also proved to be a useful tool in visual question answering applications, which require a joint image-text understanding to answer a question about the image <ref type="bibr">[103; 64; 102; 65]</ref>. Yang et al. <ref type="bibr" target="#b104">[105]</ref> propose a Stacked Attention Network (SAN) that uses two attention layers to infer the answer progressively. While the first attention layer focuses on all referred concepts in the question, the higher-level layer provides a sharper attention distribution to highlight regions that are more relevant to the answer. Disentangled Representation Learning. One goal of representation learning is to break down the features into the independent latent variables that are highly correlated with meaningful patterns <ref type="bibr" target="#b28">[29]</ref>. In traditional machine learning, approaches such as PCA <ref type="bibr" target="#b43">[44]</ref>, ICA <ref type="bibr" target="#b41">[42]</ref> and spectrum analysis <ref type="bibr" target="#b99">[100]</ref> are proposed to discover disentangled components of data. Recently, deep latent-variable models such as VAE <ref type="bibr" target="#b49">[50]</ref>, InfoGAN <ref type="bibr" target="#b12">[13]</ref> and β-VAE <ref type="bibr" target="#b39">[40]</ref> were developed to learn disentangled latent variables through variational inference. For example, in empirical studies, it is shown that β-VAE and InfoGAN can learn interpretable factorized latent variables of human face images such as azimuth, hairstyle and emotion <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Post-hoc Interpretability</head><p>Post-hoc interpretable methods aim to explain the decisionmaking process of the black-box models after they are trained. These methods map an abstract concept used in a trained machine learning model into a domain that is understandable by humans such as a block of pixels or a sequence of words. Following are the widely known post-hoc methods.</p><p>Local Explanations. Local Interpretable Model-Agnostic Explanations (LIME) <ref type="bibr" target="#b88">[89]</ref> is a representative and pioneer framework that generates local explanations of black-box models. LIME approximates the prediction of any blackbox via local surrogate interpretable models. LIME selects an instance to explain by perturbing it around its neighborhood (i.e., eliminating patches of pixels or zeroing out the values of some features). These samples are then fed to the complex model for labeling and then it will be weighted based on their proximity to the original data. Finally, LIME learns an interpretable model on the weighted perturbed data and their associated labels to create the explanations.</p><p>It is worth noting that LIME is a fast approximation of a broader approach named SHAP <ref type="bibr" target="#b65">[66]</ref> that measures feature importance. Saliency Maps. Originally introduced by Simonyan et al. <ref type="bibr" target="#b92">[93]</ref> as "image-specific class saliency maps", saliency maps highlight pixels of a given input image that are mostly involved in deciding a particular class label for the image.</p><p>To extract those pixels, the derivative of the weight vector is found by a single backpropagation pass (deconvolution). The magnitude of the derivative shows the importance of each pixel for the class score. Similar concepts were used by other researchers to deconvolve the prediction and show the locations of the input image that strongly impacts the activation of the neurons <ref type="bibr">[108; 94; 91]</ref>. While these methods belong to a popular class of tools for interpretability, Adebayo et al. <ref type="bibr" target="#b1">[2]</ref> and Ghorbani et al. <ref type="bibr" target="#b25">[26]</ref> suggest that relying on visual assessment is not adequate and can be misleading.</p><p>Example-Based Explanations. As proved in education <ref type="bibr" target="#b86">[87]</ref> and psychology domains <ref type="bibr" target="#b0">[1]</ref>, learning from experiences and examples are promising tools to explain complex concepts. In these methods, a certain example is selected from the dataset to represent the model's prediction (e.g., knearest neighbor) or the distribution of the data. It is worth mentioning that example-based explanations should not be confused with those explanations that perturb features in the dataset <ref type="bibr" target="#b84">[85]</ref>. Although using prototypes as the representation of data has shown to be effective in the human learning process <ref type="bibr" target="#b0">[1]</ref>, Kim et al. <ref type="bibr" target="#b46">[47]</ref> use a method called Maximum Mean Discrepancy (MMD) to capture a more complex distribution of the data. This method uses some instances as criticisms to explain which prototypes are not captured by the model to improve the interpretability of the black-boxes. Gurumoorthy et al. <ref type="bibr" target="#b36">[37]</ref> extend this method and designed a fast prototype selection algorithm called ProtoDash to not only select the prototypes and criticism instances, but also output non-negative weights indicating their importance. Influence Functions. To track the impact of a training sample on the prediction of a machine learning model, one can simply modify an example or delete it (leave-one-out), retrain the model, and observe the effect. However, this approach can be extremely expensive. To alleviate the issue, influence functions, a classic method from the robust statistics literature, can be used. Koh and Liang <ref type="bibr" target="#b51">[52]</ref> proposed a second-order optimization technique to approximate these influence functions. They verified their technique with different assumptions on the empirical risk ranging from being strictly convex and twice-differentiable to non-convex and non-differentiable losses. Suppose ŷ(xt, θ) is the model's prediction for the sample xt with an optimal parameter θ. Lets ŷ(xt, θ−z) be the prediction on the sample xt when the training sample z was removed while the model's optimal parameter is θ−z. The influence function tries to approximate the difference between the two predictions, ŷ(xt, θ) − ŷ(xt, θ−z), without retraining the model with the following equation,</p><formula xml:id="formula_1">ŷ(xt, θ) − ŷ(xt, θ−z) = − 1 n ∇ θ ŷ(xt, θ) T H −1 θ ∇ θ L(z, θ) (1)</formula><p>where L(z, θ) is the loss function and <ref type="figure">, θ</ref>) is the Hessian matrix. The same authors <ref type="bibr" target="#b50">[51]</ref> also investigate the effect of removing large groups of training points in large datasets on the accuracy of influence functions. They find out that the approximation computed by the influence functions are correlated with the actual effect. Inspired by this work, Cheng et al. <ref type="bibr" target="#b13">[14]</ref> propose an explanation method, Fast Influence Analysis, that employs influence functions on Latent Factor Models to resolve the lack of interpretability of the collaborative filtering approaches for recommender systems. Feature Visualization. Another way of describing what the model has learned is feature visualization. Most methods in this category deal with image inputs. Erhan et al. <ref type="bibr" target="#b19">[20]</ref> present an optimization technique called activation maximization to visualize what a neuron computes in an arbi-trary layer of deep neural network. Let θ be the learned fixed parameters after training and hij( θ, x) be the activation of neuron i in layer j, the learned image for that neuron can be calculated by solving the following optimization problem,</p><formula xml:id="formula_2">H θ = 1 n ∇ 2 θ L(zi</formula><formula xml:id="formula_3">x * = arg max x hij ( θ, x), subject to ||x||2 = 1 (2)</formula><p>Despite this method being used as a tool in providing explanations for higher-layer features <ref type="bibr">[55; 79; 75]</ref>, it has been reported that due to the complexity of the input distribution, some returned images might contain optical illusions <ref type="bibr">[78; 20]</ref>.</p><p>Explaining by Base Interpretable Models. In section 2.2.1 we discussed base models such as decision tree, rule-based and linear regression, that are known to be interepretable. Following, we will introduce some works that utilize these algorithms to explain a more sophisticated framework. Craven and Shavlik <ref type="bibr" target="#b15">[16]</ref> are one of the first to use treestructured representations to approximate neural networks.</p><p>Since their model is independent of the network architecture and training algorithm, it can be generalized to a wide variety of models. Their method, TREPAN, is similar to CART and C4.5 and uses a gain ratio criterion to evaluate the potential splits, but expands the tree based on a node that increases the fidelity of the extracted tree to the network. Inspired by TREPAN, Boz <ref type="bibr" target="#b9">[10]</ref> propose a method called DECTEXT to extract a decision tree that mimics the behavior of a trained Neural Network. In their method, they propose a new splitting technique, a new discretization method, and a novel pruning procedure. With these modifications, the proposed method can handle continuous features, optimize fidelity and minimize the tree size. A technique called distillation <ref type="bibr" target="#b40">[41]</ref> can also be used to fully understand why a specific answer is returned for a particular example. Frosst and Hinton <ref type="bibr" target="#b23">[24]</ref> answer this question by creating a model in the form of soft decision tree and examine all the learned filters from the root of the tree to the classification's leaf node. Zhang et al. <ref type="bibr" target="#b108">[110]</ref> adopt the same concept but explained the network knowledge at a humaninterpretable semantic level and also showed how much each filter contributes to the prediction. The MofN algorithm <ref type="bibr" target="#b95">[96]</ref> is one of the well-known methods that is used to extracts symbolic rules from trained neural networks. This method clusters the links based on the weights and eliminates those groups that unlikely to have any impact on the consequent. It then forms rules that are the sum of the weighted antecedents with regard to the bias. Authors also report experiments on the fidelity of the model and the comprehensibility of the set rules and the individual rules. Lou et al. <ref type="bibr" target="#b61">[62]</ref> use a generalized version of linear regression called generalized additive models (GAM) in the form of g(y) = fi(xi) = f1(x1) + ... + fn(xn) to interpret the contribution of each predictor for different classifiers or regression models. g(.) is a link function that controls whether we want to describe the model as an additive model (regression by setting g(y) = y) or generalized additive model (classification by setting it to a logistic function). f (.) is a shape function that quantifies the impact of each individual feature. This gives the ability to interpret spline models and tree-based shape functions such as single trees, bagged trees, boosted trees and boosted-bagged trees. Due to the model not considering the interactions between the features, there is a significant gap in terms of accuracy between these models and complex models. To fill this gap, the same authors propose a method named Generalized Additive Models plus Interactions (GA 2 Ms) in the form of g(y) = fi(xi) + fij (xi, xj) which takes into account the twodimensional interactions that still can be interpretable as heat maps <ref type="bibr" target="#b62">[63]</ref>. Two case studies are conducted on real healthcare problems on predicting pneumonia risks by using GA 2 Ms. These studies uncover new patterns that are ignored by state-of-the-art complex models while still hitting their accuracy <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CAUSAL INFERENCE</head><p>In this section, we briefly review the concepts from causal inference used in this paper for causal interpretable models. In their paper, Guo et al. <ref type="bibr" target="#b35">[36]</ref> provide a comprehensive review of existing causal inference methods and definitions.</p><p>Definition 1 (Structural Causal Models ). A 4tuple variable M (X, U, f, Pu) where X is a finite set of endogenous variables, usually the observable variables, U denotes a finite set of exogenous variables which usually account for unobserved or noise variables, f is a set of function {f1, f2, ..., fn} where each function represents a causal mechanism such that ∀xi ∈ X, xi = fi(P a(xi), ui) and P a(xi) is a subset of (X \ {xi}) ∪ U and Pu is a probability distribution over U is called An Structural Causal Model (SCM) or Structural Equation Model (SEM) <ref type="bibr" target="#b81">[82]</ref>.</p><p>Definition 2 (Causal Bayesian Network). To represent an SCM M (X, U, f, Pu), a directed graphical model G(V, E) is used. V is the set of endogenous variables X and E denotes the causal mechanisms. This indicates for each causal mechanism xi = fi(P a(xi), ui), there exists a directed edge from each node in the parent set P a(xi) to xi. The entire graph representing this SCM is called a Causal Bayesian Network (CBN).</p><p>Definition 3 (Average Causal Effect). The Average Causal Effect (ACE) of a binary random variable x (treatment) on another random variable (outcome) is defined as:</p><formula xml:id="formula_4">ACE = E[y|do(x = 1)] − E[y|do(x = 0)],<label>(3)</label></formula><p>Where do(.) operator denotes the corresponding interventional distribution defined by the SCM or CBN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CAUSAL INTERPRETABLITY</head><p>In this section, we discuss the state-of-the-art frameworks on causal interpretability. These frameworks are particularly needed since objective functions of machine learning models only capture correlations and not real causes. Therefore, these models might cause problems in real-world decision making, such as making policies related to smoking and cancer. Moreover, training data used to train these models might not perfectly represent the environment; and the train and the test sets might also have different distributions. A causal interpretable model can help us understand the real causes of decisions made by machine learning algorithms, improve their performance, and prevent them from failing in unexpected circumstances.</p><p>Pearl <ref type="bibr" target="#b82">[83]</ref> introduces different levels of said interpretability and argues that generating counterfactual explanations is the way to achieve the highest level of interpretability. Below are those levels of interpretability and their definitions:</p><p>• Statistical (associational) interpretability: Aims to uncover statistical associations by asking questions such as "How would seeing x change my belief in y?"</p><p>• Causal interventional interpretability: Is designed to answer "What if" questions.</p><p>• Counterfactual interpretability: Is the highest level of interpretability, which aims to answer "Why" questions.</p><p>Traditional interpretability mainly focuses on the statistical interpretability, whereas causal interpretability aims to answer questions associated with the causal interventional interpretability and counterfactual interpretability. In the following, we provide an extensive review of existing work on causal interpretability. We classify the existing works in this field into four main categories:</p><p>1. Causal interpretablity for model-based interpretations:</p><p>In this category, methods explain the causal effect of a model component on the final decision.</p><p>2. Counterfactual explanation generators: Methods in this category aim to generate counterfactual explanations for alternate situations and scenarios.</p><p>3. Causal interpretability and fairness: Lipton <ref type="bibr" target="#b58">[59]</ref> explains that interpretable models are often indispensable to guarantee fairness. Motivated by this, we provide an overview of the state-of-the-art methods on causal fairness.</p><p>4. Causal interpretability and its role in verifying the causal relationships discovered from data: In this category, we review methods which leverage interpretability as a tool to verify causal assumptions and relationships. We also discuss the scenarios, where causal inference can be used to guarantee the interpretability of a machine learning model.</p><p>In the following, we discuss each category in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Causal Inference and Model-based Interpretation</head><p>Recently, causality has gained increasing attention in explaining machine learning models <ref type="bibr">[12; 38]</ref>. These approaches are usually designed to explain the role and importance of each component of a machine learning model on its decisions with concepts from the causality. For instance, one way to explain the role of a neuron on the decision of a neural network is to estimate the ACE of the neuron on the output <ref type="bibr">[12; 81]</ref>. Traditional interpretable models cannot answer vital questions for understanding machine learning models. For instance, traditional machine interpretability frameworks are not capable to answer causal questions such as "What is the impact of the n-th filter of the m-th layer of a deep neural network on the predictions of the model?" which are helpful and required for understanding a neural network model. Furthermore, despite being simple and intuitive, performing ablation testing (i.e., removing a component of the model and retraining it to measure the performance for a fixed dataset) is computationally expensive and impractical. To address these problems, causal interpretability frameworks have been proposed. These frameworks are mainly designed to explain the importance of each component of a deep neural network on its predictions by answering counterfactual questions such as "What would have happened to the output of the model had we had a different component in the model?". These types of questions are answered by borrowing some concepts from the causal inference literature. The main idea is to model the structure of the DNN as a SCM and estimate the causal effect of each component of the model on the output by performing causal reasoning. Narendra et al. <ref type="bibr" target="#b76">[77]</ref> consider the DNN as an SCM, apply a function on each filter of the model to obtain the targeted value such as variance or expected value of each filter and reason on the obtained SCM. </p><formula xml:id="formula_5">ACE y do(x i =α) = E[y|do(xi = α)] − baselinex i ,<label>(4)</label></formula><p>where xi is i-th neuron of the network, y is the output of the model and α is an arbitrary value the neuron is set to. They also propose to calculate the baselinex i as</p><formula xml:id="formula_6">Ex i [Ey[y|do(xi = α)]]</formula><p>In another research direction, Zhao and Hastie <ref type="bibr" target="#b110">[112]</ref> state that to extract the causal interpretations from black-box models, one needs a model with good predictive performance, domain knowledge in the form of a causal graph, and an appropriate visualization tool. They further explore partial dependence plot (PDP) <ref type="bibr" target="#b22">[23]</ref> and Individual Conditional Expectation (ICE) <ref type="bibr" target="#b27">[28]</ref> to extract causal interpretations from black-box models. Alvarez-Melis and Jaakkola Causal interpretation has also gained a lot of attention in Generative Adversarial Networks (GANs) interpretability. Bau et al. <ref type="bibr" target="#b6">[7]</ref> propose a causal framework to understand "How" and "Why" images are generated by Deep Convolutional GANs (DCGANs). This is achieved by a two-step framework which finds units, objects or scenes that cause specific classes in the data samples. In the first step, dissection is performed, where classes with explicit representations in the units are obtained by measuring the spatial agreement between individual units of the region we are examining and classes using a dictionary of object classes. In the second step, intervention is performed to estimate the causal effect of a set of units on the class. This framework is then used to find the units with the highest causal effect on the class. Following equation shows the objective of this framework,</p><formula xml:id="formula_7">α * = arg min α (−δα→c + λ||α||2),<label>(5)</label></formula><p>where α indicates the units that have causal effect on the outcome, δα→c measures the causal effect of units on the class by intervening on α and set it to the constant c and λ||α||2 is a regularization term. Besserve et al. <ref type="bibr" target="#b7">[8]</ref> propose to better understand the internal functionality of generative models such as GANs or Variational Autoencoders (VAE) and answer questions like "For a face generator, is there an internal encoding of the eyes, independent of the remaining facial features?", by manipulating the internal variables using counterfactual inference. Madumal et al. <ref type="bibr" target="#b67">[68]</ref> leverage causal inference to explain the behavior of reinforcement learning agents by learning an SCM during reinforcement learning and generate counterfactual examples using the learned SCM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Causal Inference and Example-based Interpretation</head><p>As mentioned in Section 2.2, in example based explanations, we are looking for data instances that are capable of explaining the model or the underlying distribution of the data. In this subsection, we explain counterfactual explanations, a type of example-based explanations, which are one of the widely used explanations for interpreting a model's decisions. Counterfactual explanations aim to answer "Why" questions such as "Why the model's decision is Y?" or "Was it input X that caused the model to predict Y?". Generally speaking, counterfactuals are designed to answer hypothetical questions such as "What would have happened to Y, had I not done X?". They are designed based on a new type of conditional probability P (yx|x ′ , y ′ ). This probability indicates how likely the outcome (label) of an observed instance, i.e., y ′ , would change to yx if x ′ is set to x. These kinds of questions can be answered using SCMs <ref type="bibr" target="#b24">[25]</ref>.</p><p>Counterfactual explanations are defined as examples that are obtained by performing minimal changes in the original instance's features and have a predefined output. For example, what minimal changes can be made in a credit card applicant's features such that their application gets accepted. These explanations are human friendly because they are usually focused on a few number of features and therefore are more understandable. However, they suffer from the Roshomon effect <ref type="bibr" target="#b70">[71]</ref> which means there could be multiple true versions of explanations for a predefined outcome. To alleviate this problem, we could report all possible explanations, or find a way to evaluate all explanations and report the best one. Recently, several works have been proposed to generate counterfactual explanations. In order to generate counterfacutal examples, Wachter et al. <ref type="bibr" target="#b100">[101]</ref> propose to minimize the mean squared error between the model's predictions and counterfactual outcomes as well as the distance between the original instances and their corresponding counterfactuals in the feature space. Eq. ( <ref type="formula" target="#formula_8">6</ref>) shows the objective function to achieve this goal, arg min</p><formula xml:id="formula_8">x cf max λ L(x, x cf , y, y cf ) L(x, x cf , y, y cf ) = λ • ( f (x cf ) − y cf ) 2 + d(x, x cf ),<label>(6)</label></formula><p>where the first term indicates the distance between the model's prediction for the counterfactual input x cf and the desired counterfactual output, while the second term indicates the distance between the actual instance features x and the counterfactual features x cf . Liu et al. <ref type="bibr" target="#b59">[60]</ref> propose a generative model to generate counterfactual explanations for explaining a model's decisions using Eq.( <ref type="formula" target="#formula_8">6</ref>). Garth et al. <ref type="bibr" target="#b34">[35]</ref> propose a method to generate counterfactual examples in a high dimensional setting. The method is proposed for credit application prediction via off-the-shelf interchangeable black-box classifiers. In the case of high dimensional feature space, the generated explanation might not be interpretable due to the existence of too many features. To alleviate the problem, the authors propose to reweigh the distance between the features of an instance and its corresponding counterfactual with the inverse median absolute deviation (Eq.( <ref type="formula">7</ref>)). This metric is robust to outliers and results in more sparse, and therefore, more explainable solutions.</p><p>M ADj = median i∈{1,2,...,n} (|xi,j −median l∈{1,2,...,n} (x l,j )|) (7) Goyal et al. <ref type="bibr" target="#b33">[34]</ref> propose to generate counterfactual visual explanations for a query image I by using a distractor image I ′ which belongs to the class c ′ (a different class from the actual output of the classifier). To generate counterfactual explanations, the authors propose to detect spatial regions in I and I ′ such that replacing those regions in I with regions in I ′ results in system classifying the generated image as c ′ . In order to avoid trivial solutions such as replacing the entire image I with I ′ , authors propose to minimize the number of edits to transform I to I ′ . The proposed framework is shown in the following equation, min</p><formula xml:id="formula_9">P,a ||a||1 s.t. c ′ = argmax g((1 − a) • f (I) + a • P f (I ′ ))</formula><p>ai ∈ {0, 1} ∀i and P ∈ P, <ref type="bibr" target="#b7">(8)</ref> where a ∈ R hw (h and w represent height and width of an image, respectively) is a binary vector which indicates whether the feature in I needs to be changed with the feature in I ′ (value 1) or not (value 0). P ∈ R hw×hw is a permutation matrix used to align spatial cells of f (I ′ ) with f (I), f (I) and f (I ′ ) correspond to spatial feature maps of I and I ′ , respectively. Function g(.) represents the classifier and P is a set of all hw × hw permutation matrices. Goyal et al. <ref type="bibr" target="#b32">[33]</ref> propose to explain classifiers' decisions by measuring the Causal Concept Effect (CACE). CACE is defined as the causal effect of a concept (such as the brightness or an object in the image) on the prediction. In order to generate counterfactuals, authors leverage a VAE-based architecture. Hendricks et al. <ref type="bibr" target="#b44">[45]</ref> propose a method to generate counterfactual explanations using multimodal information for video classification tasks. The proposed method in this work generates visual-linguistic explanations in two steps. First, it trains a classification model for which we would like to generate explanations. Then, in the second step, it trains a post-hoc explanation model by leveraging the output and mid-level features of the trained model in first the step. The explanation model predicts the counterfactuality score for all the negative classes (classes that the instance does not belong to according to the prediction model trained in the first step). The explanation model then generates explanations by maximizing the counterfactuality score between positive and negative classes. Moore et al. <ref type="bibr" target="#b72">[73]</ref> propose to leverage adversarial examples to generate counterfactual explanations. In order to generate plausible explanations, the number of changed features should be small. Moreover, some features such as age cannot be changed arbitrarily. For example, we cannot ask loan applicants to reduce their age. Therefore, to constrain the number of changed features and the direction of gradients in the generated adversarial examples, authors propose to mask the unwanted features and gradients in a way that only desired features change in the generated explanations. Kommiya et al. <ref type="bibr" target="#b75">[76]</ref> propose to explain the decision of a machine learning framework by generating counterfactual examples which satisfy the following two criteria, ( <ref type="formula">1</ref>) generated examples must be feasible given users conditions and context such as range for the features or features to be changed; <ref type="bibr" target="#b1">(2)</ref> counterfactual examples generated for explanations should be as diverse as possible. In order to impose the diversity criterion, authors propose to either maximize the point-wise distance between examples in feature-space or leverage the concept from Determinantal point processes to select a subset of samples with the diversity constraint. <ref type="bibr">Van Looveren and Klaise [61]</ref> propose to leverage class prototypes to generate counterfactual explanations. They also claim that using class prototypes for counterfactual example generation accelerates the process. This work suggests that the generated examples by traditional counterfactual generation frameworks <ref type="bibr">[101; 35]</ref> do not satisfy two main criteria: (1) they do not consider the training data manifold which may result in out-of-distribution examples, and (2) the hyperparameters in the framework should be carefully tuned in an appropriate range which could be time consuming. To solve the mentioned problems, the authors propose to add a reconstruction loss term (defined as L2 reocnstruction error between counterfactuals and an autoencoder trained on the training samples) as well as a prototype loss term, which is defined as L2 loss between the class prototype and the counterfactual samples, to the original objective function of counterfactual generation (Eq (6)). Rathi <ref type="bibr" target="#b85">[86]</ref> generates counterfactual explanations using shapely additive explanations (SHAP).</p><p>Hendricks et al. <ref type="bibr" target="#b38">[39]</ref> defined a method to generate natural language counterfactual explanations. The framework checks for evidences of a counterfactual class in the text explanation generated for the original input. It then checks if those factors exist in the counterfactual image and returns the existing ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Causal Inference and Fairness</head><p>Nowadays, politicians, journalists and researchers are concerned regarding the interpretability of model's decisions and whether they comply with ethical standards <ref type="bibr" target="#b30">[31]</ref>. Algorithmic decision making has been widely utilized to perform different tasks such as approving credit lines, filtering job applicants and predicting the risk of recidivism <ref type="bibr" target="#b14">[15]</ref>. Prediction of recidivism is used to determine whether to detain or free a person and therefore, it needs to be guaranteed that it does not discriminate against a group of people. Since conventional evaluation metrics such as accuracy does not take these into account, it is usually required to come up with interpretable models in order to satisfy fairness criteria. Recently, huge attention has been paid to incorporating fairness into decision making methods and its connection with causal inference. Kusner et al. <ref type="bibr" target="#b52">[53]</ref> propose a new metric for measuring how fair decisions are based on counterfactuals. According to this paper, a decision is fair for an individual if the outcome is the same both in the actual world and a counterfactual world in which the individual belonged to a different demographic group. Kilbertus et al. <ref type="bibr" target="#b45">[46]</ref> address the problem from a data generation perspective by going beyond observational data. The authors propose to utilize causal reasoning to address the fairness problem by asking the question "What do we need to assume about the causal data generating process?" instead of "What should be the fairness criterion?". Madras et al. <ref type="bibr" target="#b66">[67]</ref> propose a causal inference model in which the sensitive attribute confounds both the treatment and the outcome. It then leverages deep learning techniques to learn the parameters of the model. Zhang and Bareinboim <ref type="bibr" target="#b107">[109]</ref> propose a metric (i.e., causal explanations) to quantitatively measure the fairness of an algorithm. This measure is based on three measures of transmission from cause to effect namely counterfactual direct (Ctf-DE), indirect (Ctf-IE), and spurious (Ctf-SE) effects as defined below. Given an SCM M , the counterfactual indirect effect of intervention X = x1 on Y = y (relative to baseline X = x0) conditioned on X = x with mediator W = Wx 1 is defined as,</p><formula xml:id="formula_10">IEx 0 ,x 1 (y|x) = P (yx 0 ,Wx 1 |x) − P (yx 0 |x)<label>(9)</label></formula><p>the counterfactual direct effect of intervention X = x1 on Y (with baseline x0) conditioned on X = x is defined as,</p><formula xml:id="formula_11">DEx 0 ,x 1 (y|x) = P (yx 1 ,W x0 |x) − P (yx 0 |x)<label>(10)</label></formula><p>And finally, the spurious effect of event X = x1 on Y = y (relative to baseline x0) is defined as,</p><formula xml:id="formula_12">SEx 0 ,x 1 (y|x) = P (yx 0 |x1) − P (y|x0) (11)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Causal Inference as Guarantee for Interpretability</head><p>Machine learning has had great achievements in medical, legal and economic decision making. Frameworks for these applications must satisfy the following two criteria: 1) they must be causal 2) they must be interpretable. For example, in order to find the efficacy of a drug on patient's health, one needs to estimate the causal effect of the drug on patient's health status. Moreover, in order for the results to be reliable for doctors and experts, an explanation of how the decision has been made is necessary. Despite recent achievements in these two fields separately, not so many works have been done to cover both requirements simultaneously. Moreover, the state-of-the-art approaches in each field are incompatible and therefore can not be combined and used together. <ref type="bibr">Kim and Bastani [49]</ref> propose a framework to bridge the gap between causal and interpretable models by transforming any algorithm into an interpretable individual treatment effect estimation framework. To be more specific, this work leverages the algorithm proposed in <ref type="bibr" target="#b91">[92]</ref> to learn an oracle function f which estimates the causal effect of a treatment for any observed instance and then learn an interpretable function f ′ to estimate f . They further provide a bound for the error produced by their framework.</p><p>In another line of research, causal interpretability has been used to verify the causal relationships in the data. Caruana et al. <ref type="bibr" target="#b10">[11]</ref> perform two case studies to discover the rules which show cases where generalized additive models with pairwise interactions (GA 2 M s) learn rules based on only correlations in the data and invade causal rules. They then propose to fix the learned rules based on domain experts knowledge.</p><p>Bastani et al. <ref type="bibr" target="#b87">[88]</ref> propose a decision tree based explanation method to generate global explanations for a blackbox model. Their proposed framework provides powerful insights into the data such as causal issues confirmed by the physicians previously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">PERFORMANCE EVALUATION</head><p>In this section we provide a detailed review of evaluation methods and common datasets used to assess the interpretability of models for causal interpretablity. Evaluation of interpretability is a challenging task due to the lack of consensus definition of interpretability and understanding of humans from the concept. Evaluation of causal interpretability is even more challenging due to the lack of groundtruth data for causal explanations and verification of causal relationships. Therefore, it is important to have a unified guideline on how to evaluate the proposed models. Traditional interpretability of a model is usually measured with quantifiable proxies such as if a model is approximated using sparse linear models it can be considered interpretable. To evaluate the causal interpretability, researchers also came up with some proxy metrics such as size and diversity of the counterfactual explanation. In this section, we discuss all criteria defined for the "goodness" of both causal and traditional interpretations and proxy metrics to measure how good the proposed framework can generate these explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>In this section, we briefly introduce benchmark datasets commonly used to evaluate interpretable models. Depending on the the type of the data (i.e., text, image or tabular) different datasets are used to assess the interpretability . Some commonly used datasets for image are "Ima-geNet (ILSVRC)" <ref type="bibr" target="#b89">[90]</ref>, "MNIST" <ref type="bibr" target="#b55">[56]</ref> and "PASCAL VOC dataset" <ref type="bibr" target="#b20">[21]</ref>. While for text they experimented on "20 Newsgroup Dataset" <ref type="bibr" target="#b53">[54]</ref>, "Yelp" [107], "IMDB" <ref type="bibr" target="#b42">[43]</ref> and "Amazon" <ref type="bibr" target="#b4">[5]</ref> reviews. "UCI repository" <ref type="bibr" target="#b96">[97]</ref> consists of some tabular datasets that were used by the litreture such as "Spambase", "Insurance", "Magic", "Letter", and "Adult" datasets. In order to explain the outcome of the test sample, the explanations are provided by the model. For instance, in the case of image data, those patches of the images that are mostly responsible for the class label were selected. For the text data, words involved in the final decision are made bold with different shades of color, which represent the degree of their involvement. In addition to the mentioned datasets, there are some datasets commonly used to evaluate the causal interpretable frameworks. In the following, we list common datasets used for the evaluation of causal interpretability.</p><p>• German loan dataset <ref type="bibr" target="#b18">[19]</ref>. This dataset contains 1000 observations of loan applicants which contains, numeric, categorical and ordinal attributes.</p><p>• LendingClub. This dataset 2 contains 5 years of loan records <ref type="bibr">(2007)</ref><ref type="bibr">(2008)</ref><ref type="bibr">(2009)</ref><ref type="bibr">(2010)</ref><ref type="bibr">(2011)</ref> given by LendingClub company. After preprocessing, it contains 8 features, namely, employment years, annual income, number of open credit accounts, credit history, loan grade as decided by LendingClub, home ownership, purpose, and the state of residence in the United States. .</p><p>• COMPAS. Collected by ProPublica <ref type="bibr" target="#b21">[22]</ref> for analysis purposes on recidivism decisions in the United States, after preprocessing, this data contains 5 features, namely, bail applicant's age, gender, race, prior count of offenses, and degree of criminal charge.</p><p>Unfortunately, datasets used for this purpose are not specifically designed for causal interpretability and do not contain the groundtruth that captures the causal aspect of the model such as counterfactual explanations or the ACE of different components of the model on the final decision. On the other hand, there are existing benchmark datasets specifically designed for evaluating tasks in causal inference. Cheng et al. <ref type="bibr" target="#b57">[58]</ref> provide a comprehensive survey on benchmark datasets for different causal tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Metrics</head><p>In order to assess the performance of a causal interpretable framework, authors are required to evaluate the interpretability of generated explanations from two aspects, (1) the quality of the generated explanations, i.e., are generated explanations interpretable to humans?; and (2) are the generated explanations causal? In the following two subsections, we provide comprehensive guidelines and metrics on how to answer these questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Interpretability Evaluation Metrics</head><p>Evaluating the interpretability of a machine learning model is usually a challenging task. Interpretable frameworks often evaluate their methods via two main perspectives, (1) how well the generated explanations by the method match the human expectation from different aspects; (2) how well the generated explanations are without using any human subjects. Thus, we will categorize different assessment methods based on the aforementioned perspectives and provide some examples of experiments conducted by the researchers.</p><p>2 https://www.lendingclub.com/info/download-data.action</p><p>Human Subject-Based Evaluation Metrics. Part of the research in interpretability aims to let humans understand the reasons behind the outcome of a product. Accordingly, experiments carried out by the researchers usually answer the following questions:</p><p>• By providing two different models, can the explanations help users choose the better classifier in terms of generalizability? This will help us to investigate whether the explanations can be used to decide which model is better. Ribeiro et al. <ref type="bibr" target="#b88">[89]</ref> used human subjects from "Amazon Mechanical Turk" (AMT) to choose between two models, one that generalizes better than the other while its accuracy was lower on cross validation. With the provided explanations, the subjects were able to choose the more generalized model 89% of the time.</p><p>• With explanations provided by the interpretable methods for a particular sample, can a user correctly predict the outcome of that sample? This is also called "Forward Simulation/Prediction" by Doshi-Velez and Kim <ref type="bibr" target="#b16">[17]</ref>. We can verify the explanations actually defines the output we are looking for.</p><p>• Based on the explanations, do users trust the classifier to be used in real-world applications? Selvaraju et al. <ref type="bibr" target="#b90">[91]</ref> evaluated the trust by asking 54 AMT workers to rate the reliability of the models via a 5-point scale questionnaire. A sample along with its explanations were demonstrated to subjects for two different models, AlexNet and VGG-16 (VGG-16 is known to be more reliable than AlexNet). Moreover, only those instances that provided the same prediction and were aligned with the ground truth label were considered.</p><p>The results of the evaluation shows that with the proposed explanation the subjects trust the model that generalizes better (VGG-16).</p><p>• Do the resulted explanations match human intuition?</p><p>The model is described to human subjects in detail and they were asked to provide insights about the outcome of the model (human-produced explanations). The test assumes that the explanations provided by the human should be aligned with one that the model provides <ref type="bibr" target="#b65">[66]</ref>. Moreover, experts in a specific field (e.g., doctors) can also be used to provide the explanations (e.g., important factors/symptoms) on the task (e.g., recognizing the disease).</p><p>• Given two different explanations from different algorithms, which one provides a better quality explanation? This is also known as "Binary Forced Choice" evaluation metric <ref type="bibr" target="#b16">[17]</ref>. This test can be used to compare the different explanations from different interpretable models. 3 Proximity Counterfactual explanations should be as similar as possible to the original instance <ref type="bibr" target="#b75">[76]</ref> 4 Speed Generating counterfactuals should be fast enough to be deployable in real-world applications Measure the time and number of gradient updates <ref type="bibr" target="#b60">[61]</ref> 5 Diversity Counterfatual explanations generated for a data instance should be different from each other</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-human</head><formula xml:id="formula_13">P roximity = − 1 k k i=1 dist(x cf i , x)</formula><formula xml:id="formula_14">Diversity = 1 |C k | 2 k−1 i=1 k j=i+1 dist(x cf i , x cf j ) [76]<label>6</label></formula><p>Visual-Linguistic Counterfactuals Visual explanation is the region which retains high positiveness or negativeness (i.e., on the model prediction for specific positive or negative classes).</p><p>Measure how the output of the target classifier changes corresponding to the negative class when a specific region is removed from the input using accuracy <ref type="bibr" target="#b44">[45]</ref>.</p><p>Linguistic explanation is compatible to the visual counterpart.</p><p>Measure how the output of the target classifier changes corresponding to the negative class when a specific region is removed from the input using accuracy <ref type="bibr" target="#b44">[45]</ref>.</p><p>Table <ref type="table">1</ref>: A summary of evaluation metrics for counterfactual explanations known beforehand. We should verify that the model will pick up the important features of the data. One simply can use any base method introduced in section 2.2.1 as a proxy model to extract the important features. The fraction of these important features recovered by the interpretable method can be used as an evaluation score <ref type="bibr" target="#b88">[89]</ref>.</p><p>• How locally faithful the proposed method is compared to the original model (fidelity)? Lack of fidelity will result in a limited insight to the original model <ref type="bibr" target="#b103">[104]</ref>. In convolutional neural network, one common approach is the image occlusion. The pixels that the interpretable method defines as important will be masked to see whether it reflects on the classification score or not <ref type="bibr">[91; 108]</ref>.</p><p>• How consistent the explanations are for the similar instances with the same class label? The explanations should not be significantly different for samples with the same label with a slightly different features. This instability could be the result of a high variance as well as the non-deterministic components of the explanation method <ref type="bibr" target="#b71">[72]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Causal Evaluation Metrics</head><p>Due to the lack of groundtruth for causal explanations, to verify the causal aspect of the proposed framework, we need to quantify the desired characteristics of the model and measure the "goodness" of them via some predefined proxy metrics. In the following, we go over the existing metrics to evaluate the proposed causal interpretable frameworks for different categories of causal interpretability.</p><p>Counterfactual Explanations Evaluation Metrics. Existing approaches for causal interpretability are mostly based on generating counterfactual explanations. For such approaches, the causal interpretability is often measured through the goodness of generated counterfactual explanation. As mentioned in section 4, a counterfactual explanation is the highest level of explanation and therefore, we can claim that if an explanation is a counterfactual explanation and is generated by considering causal relationships, it is indeed explainable. However, due to the lack of groundtruth for counterfactuals, we are unable to measure if the generated explanations are generated based on causal relationships. Therefore, to measure the "goodness" of counterfactual explanations, we suggest to conduct experiments to (1) measure the interpretability of the explanations using the metrics designed for interpretability; and (2) evaluate the conterfactuals themselves by measuring different characteristics of them. An interpretable Counterfatual explanation should have the following characteristics:</p><p>• The model prediction on the counterfactual sample (x cf ) needs to be close to the predefined output for counterfactual explanation.</p><p>• The perturbation δ changing the original instance x into x cf = x + δ should be sparse. In other words, size of counterfactual (i.e., number of features) should be small.</p><p>• A counterfactual explanation x cf is considered interpretable if it lies close to the models training data distribution.</p><p>• The counterfactual instance x cf needs to be found fast enough to ensure it can be used in a real life setting.</p><p>• Counterfatual explanations generated for a data instance should be different from each other. In other words, counterfactual explanations should be diverse.</p><p>• Visual-linguistic counterfactual explanations must satisfy the following two criteria, (1) Visual explanation is the region which keeps high positiveness/negativeness on the model prediction for specific positive/negative classes;</p><p>(2) Linguistic explanation should be compatible to the visual counterpart in the generated visual explanations.</p><p>Below, we briefly discuss these evaluation metrics designed to assess aformentioned characteristics of a counterfactual explanation:</p><p>To evaluate the sparsity of the generated counterfactual examples, Mc Grath et al. <ref type="bibr" target="#b34">[35]</ref> measures the size of a generated example by counting the number of features each example consists of. Van Looveren and Klaise <ref type="bibr" target="#b60">[61]</ref> use elastic net loss term EN (δ) = β||δ||1 + ||δ|| 2 2 where δ is the distance between the original instance and its generated counterfactual example and β is the hyperparameter. In order for counterfactual explanations to be interpretable, they need to be close to the data manifold. Looveren and Klaise improves this criterion by suggesting that the counterfactuals are interpretable if they are close to the data manifold of the counterfactual class <ref type="bibr" target="#b60">[61]</ref>. To measure the interpretability defined above, Looveren and Klaise propose to measure the ratio of the reconstruction errors when the model used for generating counterfactuals is trained only on the counterfactual class vs when it is trained on the original class <ref type="bibr" target="#b60">[61]</ref>. The proposed metric is shown in the following equation,</p><formula xml:id="formula_15">IM 1(AEi, AEt 0 , x cf ) = ||x0 + δ − AEi(x0 + δ)|| 2 2 ||x0 + δ − AEt 0 (x0 + δ)|| 2 2 + ǫ<label>(12)</label></formula><p>Where AEi and AEt 0 represent the autoencoders used to generate the counterfacutals trained on the class i (counterfactual class) and class t0 (the original class), respectively. We let x cf and x0 be the counterfactual explanation and the original sample. In addition, δ denotes the distance between the original and counterfactual samples. A lower value of IM 1 shows that counterfactual examples can be better reconstructed from the autoencoder trained on the counterfactual class in comparison to the autoencoder trained on the original class. This implies that the generated counterfactuals are closer to the counterfactual class data manifold. Another metric proposed by <ref type="bibr" target="#b60">[61]</ref> measures how similar the generated counterfactuals are when generated using the autoencoder trained on only counterfactuals vs the autoencoder trained on all classes. The metric is shown in the following equation,</p><formula xml:id="formula_16">IM 2(AEi, AEt 0 , x cf ) = ||AEi(x0 + δ) − AE(x0 + δ)|| 2 2 ||x0 + δ||1 + ǫ<label>(13)</label></formula><p>A lower value of IM 2 shows that counterfactuals generated by both autoencoders trained on all classes and counterfactuals are more similar. This implies that the generated coun-terfactual distribution is as good as the distribution over all classes. Generated counterfactual explanations can be used to measure users' understanding of a machine learning model's local decision boundary. <ref type="bibr">Mothilal et al. [76]</ref> propose to mimic users' understanding of a model's local decision boundaries by, (a) constructing an auxiliary classifier on both original inputs and counterfactual examples; and (b) measuring how well it mimics the actual decision boundaries. More specifically, they train a 1-nearest neighbor (1-NN) classifier on both the original and the counterfactual samples to predict the class of new inputs. The accuracy of this model is then compared with the accuracy of the original model. The definition of counterfactual explanations implies that generated explanations should be as similar as possible to the original instance. In order to evaluate the proximity between original samples and counterfactual explanations, Mothilal et al. <ref type="bibr" target="#b75">[76]</ref> defines proximity as Eq. ( <ref type="formula" target="#formula_17">14</ref>),</p><formula xml:id="formula_17">P roximity = − 1 k k i=1 dist(x cf i , x)<label>(14)</label></formula><p>In order to be able to calculate the proximity for both categorical and continuous features, the authors further propose two metrics to calculate the proximity for categorical and continuous features. For continuous features, the proximity is defined as the mean of feature-wise L1 distances between the original sample and counterfactuals divided by the median absolute deviation (MAD) of the features values in the training set. For categorical features, disctance function is calculated such that for each categorical feature it assigns 1 if the feature differs from the original feature and otherwise it assigns 0. In order to gauge the speed of generating counterfactual explanations, Looveren and Klaise <ref type="bibr" target="#b60">[61]</ref> measure the time and the number of gradient updates until the desired counterfactual explanation is generated. Diversity of generated counterfactuals is measured via measuring feature-wise distances between each pair of counterfactual examples and calculating diversity as the mean of the distances between each pair of examples <ref type="bibr" target="#b75">[76]</ref>. Eq. ( <ref type="formula" target="#formula_18">15</ref>) illustrates the measure used for diversity.</p><formula xml:id="formula_18">Diversity = 1 |C k | 2 k−1 i=1 k j=i+1 d(x cf i , x cf j )<label>(15)</label></formula><p>Where C k represents a set of k counterfactuals generated for the original input, x cf i and x cf j are the i-th and j-th counterfactuals in the set C k . Kanehira et al. <ref type="bibr" target="#b44">[45]</ref> propose metrics to evaluate visuallinguistic counterfactual explanations to ensure, (a) visual explanations keep possession of high positiveness/negativeness on the model predictions for positive/negative classes; (b) linguistic explanations are compatible with their corresponding visual explanations. To measure if the generated examples meet these criteria, authors in <ref type="bibr" target="#b44">[45]</ref> propose two metrics based on the accuracy. More specifically, to check for the first condition, they investigate how the output of the target classifier changes towards the negative class when a specific region is removed from the input. To measure the second criterion, for each output pair (s, R) they examine how the region R makes the concept s distinguishable by humans. To measure this quantitatively, they compute the accuracy by Overview of interpretable models and their categories</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Traditional Interpretability</head><p>Interpretable Models: <ref type="bibr" target="#b105">[106]</ref>, <ref type="bibr" target="#b102">[103]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b101">[102]</ref>, <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b104">[105]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b39">[40]</ref> Causal Interpretability</p><p>Model-based: <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b110">[112]</ref>, <ref type="bibr" target="#b80">[81]</ref>, <ref type="bibr" target="#b6">[7]</ref>, Example-based: <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b100">[101]</ref>, <ref type="bibr" target="#b75">[76]</ref>, <ref type="bibr" target="#b85">[86]</ref>, <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> Post-hoc: <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b88">[89]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b92">[93]</ref>, <ref type="bibr" target="#b90">[91]</ref>, <ref type="bibr" target="#b106">[108]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b95">[96]</ref>, <ref type="bibr" target="#b61">[62]</ref> Fairness: <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b107">[109]</ref> Guarantee: <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b87">[88]</ref> Table <ref type="table">2</ref>: A summary of the state-of-the-art frameworks for each type of interpretability utilizing bounding boxes for each attribute in the test set. More specifically, IoU (intersection over union) between a given R and all bounding boxes R0 corresponding to attribute s0 is calculated. Then the accuracy is measured by selecting the the attribute s0 with the largest IoU score and checking its consistency with s a counterpart of R. Table <ref type="table">1</ref> summarizes evaluation metrics for counterfactulas explanations based on the properties of the generated examples. Model-based Evaluation Metrics. Due to the lack of evaluation groundtruth for representing the actual effect of each component of the model on its final decisions, evaluation for this type of models is still an open problem. One common way of evaluating such models is to report the most important components of a model by measuring their causal effects on the outcome of the model <ref type="bibr">[38; 77]</ref>. Chattopadhyay et al. also used the causal attribution of each neuron on the output to visualize the local decisions of the model by saliency map. Moreover, to further investigate how well the model estimates the ACE, they proposed to run the model on datasets for causal effect estimations <ref type="bibr" target="#b11">[12]</ref>. Causal Fairness Evaluation. Evaluation of causal fairness models is a challenging task. Papers in this field usually assess the performance of the model for detecting discrimination. Zhang et al. leverage direct, indirect and spurious effect measures (defined in section 4.3) to detect and explain discrimination <ref type="bibr" target="#b107">[109]</ref>. However, to the best of our knowledge, no quantitative measure of causality of a fairness algorithm existis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>In this survey, we introduce the problem of interpretability in machine learning. We view the problem from two perspectives, (1) Traditional interpretability algorithms; (2) causal interpretability algorithms. However, the primary focus of the survey is on causal frameworks. We first provide different definitions of interpretability, then review the state-of-the-art methods in both categories and point out the differences between them. Each type of interpretable models is further subdivided into other sub categories to provide readers with better overview of existing directions and approaches in the field. More conceretely, for traditional methods, we divide existing work into inherently interpretable models and post-hoc intrerpretability. For causal models, we divide the existing works into the following four categories: counterfactual examples, model-based interpretability, causal models in fairness and interpretability for verifying causal relationships. We also address the challenging problem of evaluating interpretable models , explain existing metrics in detail and categorize them based on the scenarios they are designed for. Table <ref type="table">2</ref> summarizes state-of-the-art methods which belong to each category of interpretability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example of a decision tree with positive and negative class (binary) and three attributes. The red path has a decision rule, if ¬Att1 ∧ Att2 ∧ ¬Att3 ⇒ +1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>[ 3 ]</head><label>3</label><figDesc>generated causal explanations for structured input structured output black-box models by (a) generating perturbed samples using a variational auroencoder; (b) generating a weighted bipartite graph G = (Vx ∪ Vy, E), where Vx and Vy are elements in x and y and Eij represents the causal influence of xi and yj; and (c) generating explanation components using graph partitioning algorithms.Parafita and Vitria<ref type="bibr" target="#b80">[81]</ref> introduce a causal attribution framework to explain decisions of a classifier based on the latent factors. The framework consists of three steps, (a) constructing Distributional Causal Graph which allows us to sample and compute likelihoods of the samples; (b) generating a counterfactual image which is as similar as possible to the original image; and (c) estimating the effect of the modified factor by estimating the causal effect.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Harradon et  al.<ref type="bibr" target="#b37">[38]</ref> further suggest that in order to have an effective interpretability, having a human-understandable causal model of DNN, which allows different kinds of causal interventions, is necessary. Based on this hypothesis, the authors propose an interpretability framework, which extracts humanunderstandable concepts such as eyes and ears of a cat from deep neural networks, learns the causal structure between the input, output and these concepts in an SCM and performs causal reasoning on it to gain more insights into the model. Chattopadhyay et al.<ref type="bibr" target="#b11">[12]</ref> propose an attribution method based on the first principle of causality, particularly SCMs and do(•) calculus. More concretely, similar to other proposed methods in this category, the proposed framework models the structure of the machine learning algorithm as an SCM. It then proposes a scalable causal inference approach to the estimate individual treatment effect of a desired component on the decision made by the algorithm.</figDesc><table /><note>Chattopadhyay et al. suggest  to simplify the SCM defined on a multi-layer network M ([l1, l2, l3...., ln], U, f, PU ) to another network as SCM M ′ ([l1, ln], U, f ′ , PU ) where l1 and ln represent neurons in the input and output layers, li represents neurons in the i-th layer of the network, U denotes the set of unknown variables, f and f ′ correspond to the SCM functions and PU defines distributions of the unknown variables. They then propose to calculate the ACE of any neurons of the model on the output by performing causal reasoning on M as follows,</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">In this survey we use the words interpretable and explainable interchangeably.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We would like to thank Andre Harrison for helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Case-based reasoning: Foundational issues, methodological variations, and system approaches</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aamodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Plaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI communications</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="59" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sanity checks for saliency maps</title>
		<author>
			<persName><forename type="first">J</forename><surname>Adebayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Muelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9505" to="9515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A causal framework for explaining the predictions of black-box sequenceto-sequence models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Alvarez-Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09">Sept. 2017</date>
			<biblScope unit="page" from="412" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Angwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mattu</surname></persName>
		</author>
		<ptr target="https://www.propublica.org/article/machine-bias-risk-asses" />
		<title level="m">Machine bias</title>
				<imprint>
			<date type="published" when="2019-03">Mar 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<ptr target="https://s3.amazonaws.com/amazon-reviews-pds/readme.html" />
	</analytic>
	<monogr>
		<title level="j">AWS. Amazon customer reviews dataset</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">GAN dissection: Visualizing and understanding generative adversarial networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno>CoRR, abs/1811.10597</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Counterfactuals uncover the modular structure of deep generative models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Besserve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno>CoRR, abs/1812.03253</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Critical questions for big data: Provocations for a cultural, technological, and scholarly phenomenon. Information, communication &amp; society</title>
		<author>
			<persName><forename type="first">D</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Crawford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="662" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Extracting decision trees from trained neural networks</title>
		<author>
			<persName><forename type="first">O</forename><surname>Boz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="456" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;15</title>
				<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1721" to="1730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Neural network attributions: A causal perspective</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Manupriya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
		<idno>CoRR, abs/1902.02302</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Incorporating interpretability into latent factor models via fast influence analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="885" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fair prediction with disparate impact: A study of bias in recidivism prediction instruments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chouldechova</surname></persName>
		</author>
		<idno>CoRR, abs/1703.00056</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Extracting treestructured representations of trained networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Shavlik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="24" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Towards a rigorous science of interpretable machine learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08608</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.00033</idno>
		<title level="m">Techniques for interpretable machine learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Graff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Visualizing higher-layer features of a deep network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">1341</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Montreal</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010-06">June 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">False positives, false negatives, and false analyses: A rejoinder to machine bias: Theres software used across the country to predict future criminals. and its biased against blacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Flores</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bechtel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lowenkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Federal probation</title>
		<imprint>
			<biblScope unit="page">80</biblScope>
			<date type="published" when="2016-09">09 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Greedy function approximation: a gradient boosting machine</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of statistics</title>
		<imprint>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Distilling a neural network into a soft decision tree</title>
		<author>
			<persName><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09784</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Causality: models, reasoning, and inference, by judea pearl</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Gerson</forename><surname>Neuberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometric Theory</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="675" to="685" />
			<date type="published" when="2000">2000. 2003</date>
			<publisher>cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Interpretation of neural networks is fragile</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3681" to="3688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Explaining explanations: An overview of interpretability of machine learning</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Gilpin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Specter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kagal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="80" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapelner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bleich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pitkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="65" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Eu regulations on algorithmic decision-making and a &quot;right to explanation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Flaxman</surname></persName>
		</author>
		<idno>arxiv:1606.08813Comment</idno>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Human Interpretability in Machine Learning (WHI 2016)</title>
				<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">European union regulations on algorithmic decision-making and a right to explanation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Flaxman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="50" to="57" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Explaining classifiers with causal concept effect (cace)</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<idno>CoRR, abs/1907.07165</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Counterfactual visual explanations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<idno>CoRR, abs/1904.07451</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Interpretable credit application predictions with counterfactual explanations</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Grath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Costabello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kamiab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lécué</surname></persName>
		</author>
		<idno>CoRR, abs/1811.05245</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.09337</idno>
		<title level="m">A survey of learning causality with data: Problems and methods</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Efficient data representation by selecting prototypes with importance weights</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Gurumoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dhurandhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cecchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Causal learning and explanation of deep neural networks via autoencoded activations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Harradon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Druce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Ruttenberg</surname></persName>
		</author>
		<idno>CoRR, abs/1802.00541</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Generating counterfactual explanations with natural language</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<idno>CoRR, abs/1806.09809</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">betavae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Independent component analysis: algorithms and applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks</title>
				<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="411" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title/>
		<ptr target="https://www.imdb.com/interfaces/" />
	</analytic>
	<monogr>
		<title level="j">IMDb. Imdb datasets</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Principal component analysis</title>
		<author>
			<persName><forename type="first">I</forename><surname>Jolliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Multimodal explanations by predicting counterfactuality in videos</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kanehira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Takemoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Inayoshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<idno>CoRR, abs/1812.01263</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Avoiding discrimination through causal reasoning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kilbertus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rojas Carulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Parascandolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="656" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Examples are not enough, learn to criticize! criticism for interpretability</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">O</forename><surname>Koyejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2280" to="2288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Examples are not enough, learn to criticize! criticism for interpretability</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Koyejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Khanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05">2016. December 5-10, 2016. 2016</date>
			<biblScope unit="page" from="2280" to="2288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Learning interpretable models with causal guarantees</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bastani</surname></persName>
		</author>
		<idno>CoRR, abs/1901.08576</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-S</forename><surname>Ang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13289</idno>
		<title level="m">On the accuracy of influence functions for measuring group effects</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Understanding black-box predictions via influence functions</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1885" to="1894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Counterfactual fairness</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Loftus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silva ; Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Von</surname></persName>
		</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09">2017, 4-9 December 2017. 2017</date>
			<biblScope unit="page" from="4069" to="4079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">K</forename><surname>Lang</surname></persName>
		</author>
		<ptr target="http://qwone.com/~jason/20Newsgroups/" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>20 newsgroups</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE international conference on acoustics, speech and signal processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="8595" to="8598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<title level="m">The mnist database</title>
				<imprint>
			<date type="published" when="2020-01">Jan 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Feature selection: A data perspective</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Morstatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Trevino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">94</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Causal learning in question quality improvement</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Bench-Council International Symposium on Benchmarking, Measuring and Optimizing (Bench19)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">The mythos of model interpretability</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<idno>CoRR, abs/1606.03490</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Generative counterfactual introspection for explainable deep learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kailkhura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Loveland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<idno>CoRR, abs/1907.03077</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Interpretable counterfactual explanations guided by prototypes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Looveren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Klaise</surname></persName>
		</author>
		<idno>CoRR, abs/1907.02584</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Intelligible models for classification and regression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="150" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Accurate intelligible models with pairwise interactions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hooker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;13</title>
				<meeting>the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="623" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Knowing when to look: Adaptive attention via a visual sentinel for image captioning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="375" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4765" to="4774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Fairness through causal awareness: Learning latent-variable models for biased data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Madras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Creager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno>CoRR, abs/1809.02519</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Explainable reinforcement learning through a causal lens</title>
		<author>
			<persName><forename type="first">P</forename><surname>Madumal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sonenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vetere</surname></persName>
		</author>
		<idno>CoRR, abs/1905.10958</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">A survey on bias and fairness in machine learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mehrabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Morstatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09635</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Explanation in artificial intelligence: Insights from the social sciences</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<idno>CoRR, abs/1706.07269</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Molnar</surname></persName>
		</author>
		<ptr target="https://christophm.github.io/interpretable-ml-book/" />
		<title level="m">Interpretable Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Interpretable machine learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Molnar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">Lulu. com, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Explaining deep learning models with constrained adversarial examples</title>
		<author>
			<persName><forename type="first">J</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hammerla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Watkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906">1906.10671, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Deepfool: a simple and accurate method to fool deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<idno>CoRR, abs/1511.04599</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Inceptionism: Going deeper into neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tyka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Explaining machine learning classifiers through diverse counterfactual explanations</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Mothilal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<idno>CoRR, abs/1905.07697</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Explaining deep learning models using causal inference</title>
		<author>
			<persName><forename type="first">T</forename><surname>Narendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vijaykeerthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mani</surname></persName>
		</author>
		<idno>CoRR, abs/1811.04376</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Feature visualization. Distill</title>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schubert</surname></persName>
		</author>
		<ptr target="https://distill.pub/2017/feature-visualization" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">The building blocks of interpretability</title>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Satyanarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
		<ptr target="https://distill.pub/2018/building-blocks" />
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">The limitations of deep learning in adversarial settings</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
		<idno>CoRR, abs/1511.07528</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Explaining visual models by causal attribution</title>
		<author>
			<persName><forename type="first">Á</forename><surname>Parafita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vitrià</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08891</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><surname>Causality</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Theoretical impediments to machine learning with seven sparks from the causal revolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<idno>CoRR, abs/1801.04016</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">The seven tools of causal inference, with reflections on machine learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="54" to="60" />
			<date type="published" when="2019-02">Feb. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Model agnostic supervised local explanations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Plumb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molitor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2515" to="2524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Generating counterfactual and contrastive explanations using SHAP. CoRR, abs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rathi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906">1906.09293, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Toward an instructionally oriented theory of example-based learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Renkl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05386</idno>
		<title level="m">Modelagnostic interpretability of machine learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Why should i trust you?: Explaining the predictions of any classifier</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Im-ageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Estimating individual treatment effect: generalization bounds and algorithms</title>
		<author>
			<persName><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3076" to="3085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<title level="m">Striving for simplicity: The all convolutional net</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Introduction to data mining</title>
		<author>
			<persName><forename type="first">P.-N</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Pearson Education India</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Extracting refined rules from knowledge-based neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Towell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Shavlik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="101" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Uci machine learning repository</title>
		<ptr target="https://archive.ics.uci.edu/ml/index.php" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
		<respStmt>
			<orgName>UCI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName><forename type="first">U</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Von</forename><surname>Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Counterfactual explanations without opening the black box: Automated decisions and the GDPR</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wachter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Mittelstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<idno>CoRR, abs/1711.00399</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Ask, attend and answer: Exploring question-guided spatial attention for visual question answering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="451" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Evaluating explanation without ground truth in interpretable machine learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies</title>
				<meeting>the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">Fairness in decisionmaking -the causal explanation formula</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bareinboim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">02</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Interpreting cnns via decision trees</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6261" to="6270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Visual interpretability for deep learning: a survey</title>
		<author>
			<persName><forename type="first">Q.-S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Information Technology &amp; Electronic Engineering</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="39" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Causal interpretations of black-box models</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Business &amp; Economic Statistics</title>
		<imprint>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>just-accepted</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
