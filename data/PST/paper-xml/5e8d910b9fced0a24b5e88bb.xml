<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RhythmNet: End-to-end Heart Rate Estimation from Face via Spatial-temporal Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Student Member, IEEE, Shiguang</roleName><forename type="first">Xuesong</forename><surname>Niu</surname></persName>
							<email>xuesong.niu@vipl.ict.ac.cn</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Hu</forename><surname>Han</surname></persName>
							<email>hanhu@ict.ac.cn</email>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Xilin</forename><surname>Chen</surname></persName>
							<email>xlchen@ict.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Shan</forename><surname>Shiguang</surname></persName>
						</author>
						<author>
							<persName><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="middle">H</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName><forename type="middle">S</forename><surname>Shan</surname></persName>
							<email>sgshan@ict.ac.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Chinese Academy of Sci-ences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Acade-my of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shen-zhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RhythmNet: End-to-end Heart Rate Estimation from Face via Spatial-temporal Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A0A60EA2B0D5776A96D4917140FB8082</idno>
					<idno type="DOI">10.1109/TIP.2019.2947204</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2019.2947204, IEEE Transactions on Image Processing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2019.2947204, IEEE Transactions on Image Processing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2019.2947204, IEEE Transactions on Image Processing Shiguang Shan (M&apos;04-SM&apos;15) received Ph.D. de-</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Remote heart rate estimation</term>
					<term>rPPG</term>
					<term>spatialtemporal representation</term>
					<term>end-to-end learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Heart rate (HR) is an important physiological signal that reflects the physical and emotional status of a person. Traditional HR measurements usually rely on contact monitors, which may cause inconvenience and discomfort. Recently, some methods have been proposed for remote HR estimation from face videos; however, most of them focus on well-controlled scenarios, their generalization ability into less-constrained scenarios (e.g., with head movement, and bad illumination) are not known. At the same time, lacking large-scale HR databases has limited the use of deep models for remote HR estimation. In this paper, we propose an end-to-end RhythmNet for remote HR estimation from the face. In RyhthmNet, we use a spatial-temporal representation encoding the HR signals from multiple ROI volumes as its input. Then the spatial-temporal representations are fed into a convolutional network for HR estimation. We also take into account the relationship of adjacent HR measurements from a video sequence via Gated Recurrent Unit (GRU) and achieves efficient HR measurement. In addition, we build a large-scale multi-modal HR database (named as VIPL-HR 1 ), which contains 2,378 visible light videos (VIS) and 752 near-infrared (NIR) videos of 107 subjects. Our VIPL-HR database contains various variations such as head movements, illumination variations, and acquisition device changes, replicating a less-constrained scenario for HR estimation. The proposed approach outperforms the stateof-the-art methods on both the public-domain and our VIPL-HR databases.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Heart rate (HR) is an important physiological signal that reflects the physical and emotional status of a person; therefore, HR measurement is useful in many applications, such as training aid, health monitoring, and nursing care. Traditional Fig. <ref type="figure">1</ref>: Comparisons between (a) several public domain HR databases (MAHNOB-HCI <ref type="bibr" target="#b7">[8]</ref>, MMSE-HR <ref type="bibr" target="#b5">[6]</ref>, PFF <ref type="bibr" target="#b8">[9]</ref>, PURE <ref type="bibr" target="#b9">[10]</ref>) and (b) our VIPL-HR database in terms of illumination condition, head movement, and acquisition device.</p><p>HR measurement usually relies on contact monitors, such as electrocardiograph (ECG) and contact photoplethysmography (cPPG) based sensors, which are inconvenient for the users and thus limit the application scenarios. In recent years, a growing number of studies have been reported on remote HR estimation from videos <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, which allows HR estimation from the skin, i.e., the face area, without contact with a person.</p><p>The existing video-based HR estimation methods can be grouped into two main categories: remote photoplethysmography (rPPG) based approaches <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> and the ballistocardiographic (BCG) <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b10">[11]</ref> based approaches. While rPPG-based approaches aim to extract HR signals from the color changes caused by variations in volume and oxygen saturation of the blood in the vessels due to heart beats, BCG-based approaches aim to extract HR signals from head movements arising from the periodic ejections of blood into the great vessels along with each heartbeat. Most of the existing non-contact HR measurement methods in literature are rPPG-based approaches because it is easier to capture the skin color changes than to get the small head movement in lessconstrained scenarios, especially when the subjects perform large-scale head movements. However, head movements and environment conditions, e.g., light conditions, may have great impact on rPPG-based approaches since the amplitude of the HR signal extracted from the color changes of facial skin is too small to be perceived by human eyes.</p><p>In order to get stable HR signal, many rPPG-based methods have been proposed. Color space transformation and signal decomposition are commonly used tools for HR signal generation, e.g., using independent component analysis (ICA) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> and CHROM <ref type="bibr" target="#b3">[4]</ref>. These methods reported good results under constrained situations, but may not perform well in the presence of large head motions and lighting variations. Some latter approaches introduced more sophisticated color space transformation and signal decomposition algorithms and obtained improved heart rate measurement accuracy <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> under less-constrained situations. However, these methods usually made some certain assumptions w.r.t. the skin reflection (e.g., linear combination assumption <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>), and the influence by head movement <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Such assumptions may not hold in the presence of large head motions and lighting variations. In addition, many prior methods usually perform evaluations on private databases, making it difficult to provide comparisons among different methods. While a few public-domain HR databases are available <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, the sizes of these databases are quite limited (usually less than 50 subjects) compared with the large scale databases in the other computer vision tasks. Moreover, these databases were usually built in well-controlled scenarios, i,e., with minor illumination and motion variations (see Fig. <ref type="figure">1(a)</ref>), which has a big gap with the practical application scenarios.</p><p>In this paper, we aim to perform remote HR estimation from face under less-constrained scenarios, where there are large variations in pose and illumination. In one aspect, we aim to build an end-to-end trainable HR estimator leveraging the strong modelling capacity of deep neural networks (DNN) to address the challenges of head movement, illumination variation, and acquisition device difference. In the other aspect, we also hope to provide an effective approach for modelling the relationship between adjacent HR measurement. To replicate practical scenarios, we also build a large-scale multimodal HR database (named as VIPL-HR), which consist of 2,378 visible face videos and 752 NIR face videos from 107 subjects. Three different recording devices (RGB-D camera, smart phone, and web-camera) are used for video recording under 9 different situations covering variations in face pose, scale, and illumination (see Fig. <ref type="figure">1</ref>). We provide evaluations on two widely used public-domain datasets (MAHNOB-HCI <ref type="bibr" target="#b7">[8]</ref> and MMSE-HR <ref type="bibr" target="#b5">[6]</ref>) and our VIPL-HR datasets covering both intra-database and cross-database testing protocols.</p><p>This work is an extension of our early work <ref type="bibr" target="#b12">[13]</ref>. Compared with our previous work, the major extensions in this work are three-fold: i) a detailed review of the published methods for remote HR estimation, and in-depth analysis of the remaining challenges in remote HR estimation from face; ii) we take into account the correlation between adjacent HR measurements and model it via Gated Recurrent Unit (GRU) to achieve robust HR estimation; and iii) extensive evaluations are provided on multiple databases covering a number of aspects of intradatabase testing, inter-database testing, visible light imaging, near-infrared imaging, video compression, etc. The experimental results show that the proposed approach outperforms the state-of-the-art algorithms by a large margin.</p><p>The rest of this paper is organized as follows. We review the related work of remote HR estimation in Section II. The large-scale VIPL-HR database we built is introduced in</p><formula xml:id="formula_0">t f 𝑡𝑡 f 𝑡𝑡+∆𝑡𝑡 f 𝑡𝑡+2∆𝑡𝑡 f 𝑡𝑡+3∆𝑡𝑡 f 𝑡𝑡+4∆𝑡𝑡 f 𝑡𝑡+5∆𝑡𝑡</formula><p>Fig. <ref type="figure">2</ref>: Magnified skin color changes due to heartbeat using the Eulerian Video Magnification method <ref type="bibr" target="#b29">[30]</ref>. Best viewed in color.</p><p>Section III. The proposed end-to-end approach for remote HR estimation is detailed in Section IV. Experimental evaluations and analysis are provided in Section V. Finally, we conclude this work in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>A. Remote Heart Rate Estimation from Face using rPPG</p><p>The principle of the rPPG-based HR estimation is based on the fact that optical absorption of skin varies periodically with the blood volume pulse (BVP) <ref type="bibr" target="#b13">[14]</ref>. Human skin can be regarded as a three-layer model, subcutis, dermis and epidermis. The light is usually absorbed by the chromophores in the skin. The major chromophores of human skin are hemoglobin in the microvasculars of dermis and subcutis layers and melanin in the epidermis layer. The changes of hemoglobin content during a cardiac cycle would cause tiny color changes of the skin (see Fig. <ref type="figure">2</ref>) <ref type="foot" target="#foot_0">2</ref> . Although the color changes are invisible to human eyes, they can be captured by commodity RGB sensors, and used for computing the HR.</p><p>An early study of remote PPG (rPPG) based HR measurement was reported in <ref type="bibr" target="#b13">[14]</ref>, in which a commodity webcam was used to record face videos. Although it is not a fully automated approach, it shows the possibility of remote HR estimation from face videos. After that, a number of approaches have been reported on this topic.</p><p>Although it is possible to perform rPPG based HR measurement, it is still a challenge task since the amplitude of the light absorption variation w.r.t. BVP is minor. As illustrated in <ref type="bibr" target="#b19">[20]</ref>, the peak ac/dc ratio is only about 2% for the rPPG signals. In addition, these signals can be easily affected by head movements, illumination conditions and noise introduced by the recording devices, making it hard to build a robust HR measurement system under less-constrained situations. A database considering the influences of these factors are also lacking to the community.</p><p>Blind signal separation (BSS) methods were introduced for remote HR estimation in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, in which independent component analysis (ICA) was applied to the temporal RGB color signal sequences to find the dominant component related to HR. In a later work of <ref type="bibr" target="#b14">[15]</ref>, ICA was used for color signals captured by a five-band camera instead of an RGB camera, and better performance was reported. Different from these holistic  <ref type="bibr" target="#b15">[16]</ref> and reported the state-of-the-art on the public domain database MAHNOB-HCI <ref type="bibr" target="#b7">[8]</ref>. Besides ICA, principal components analysis (PCA) <ref type="bibr" target="#b16">[17]</ref> and constrained ICA (cICA) <ref type="bibr" target="#b17">[18]</ref> were also used to extract the purified HR signal from the input RGB video sequence. The BSS-based HR estimation approaches usually make certain assumptions about the skin reflection and head movement. When these assumptions hold, BSS-based methods can effectively improve the signal-to-noise rate (SNR) of the HR signals, and their computational cost is relatively low. However, the assumptions may not hold under less-constrained scenarios, e.g., with large head movement, poor illumination, diverse sensors, etc. As a result, the HR estimation accuracy of BSS-based methods could degrade severely.</p><p>Since the robustness of rPPG-based methods is highly related to the skin, it is important to leverage the prior knowledge of skin to improve the robustness. An optical skin model considering head movement was proposed in <ref type="bibr" target="#b3">[4]</ref>, and used for calculating a chrominance feature representation from an input RGB video sequence to reduce the influence of head movement to HR estimation. In a later work of <ref type="bibr" target="#b18">[19]</ref>, pixelwise chrominance features were computed and used for HR estimation. Instead of modelling the influence of head movement w.r.t. different color channels, Feng et al. <ref type="bibr" target="#b19">[20]</ref> treated the human face as a Lambertian radiator and provided a radiance analysis of different motion types. Besides modelling the optical status of skin under head motion, Haan and Leest <ref type="bibr" target="#b20">[21]</ref> took the camera optical properties into consideration and proposed a linear decomposition of RGB signals to perform HR estimation. A detailed discussion of different skin optical models for rPPG-based HR estimation was presented in <ref type="bibr" target="#b6">[7]</ref>, in which a new projection method was also proposed for extracting pulse signals from an input RGB video. The various skin models utilized by individual HR estimation methods lead to improved robustness. However, when it comes to some complicated scenarios, the assumptions used by these models may not hold, and the HR estimation accuracy could drop significantly. Models that can adaptively deal with various variations in practical application scenarios are required to build a robust HR estimation system. Thus, data-driven based feature learning methods may have the ability to obtain such a representation.</p><p>After obtaining the signals related to HR, a common operation is to apply Fast Fourious Transform (FFT) to the signals to get the spectral distribution. The peak of the spectral distribution is considered as the HR frequency (in Hz). Directly applying FFT to the HR signals may get a spectral distribution consisting of many spectral noises. Therefore, a few approaches have been proposed to reduce such noises. In <ref type="bibr" target="#b21">[22]</ref>, Kumar et al. used the frequency characteristics as the weights to combine the HR signals from different ROIs in order to get a better HR signal. Instead of enhancing the HR signals, Wang et al. <ref type="bibr" target="#b22">[23]</ref> directly adjusted the spectral distribution according to the prior knowledge of the HR signal. Similarly, Niu et al. <ref type="bibr" target="#b23">[24]</ref> took the continuous estimation situations into consideration and used preceding estimations to learn an HR distribution and use it to modulate the spectral distribution. While the frequency domain representation is helpful for remote HR estimation, temporal domain information can also be helpful.</p><p>All the methods mentioned above mainly focus on improving the hand-crafted pipelines from periodical temporal signal extraction to signal analysis. Their generalization ability can be poor under unconstrained scenarios. At the same time, datadriven based methods are believed to have strong ability to learn more relevant features to a specific task, particularly when there are given enough data. Some recent methods tried to obtain better temporal signals of the HR from a face video. Tulyakov et al. <ref type="bibr" target="#b5">[6]</ref> divided the face into multiple ROI regions to get a matrix of temporal representation and utilized matrix completion to purify the signals related to HR. Wang et al. <ref type="bibr" target="#b24">[25]</ref> proposed a subject-dependent approach to compute the HR signal using spatial subspace rotation (SSR) given a complete continuous sequence of a subject's face. Another kind of datadriven methods for remote HR estimation is to learn a HR estimator using the frequency domain representations as input. Hsu et al. <ref type="bibr" target="#b25">[26]</ref> combined the frequency domain features from RGB channels and the ICA components to get a HR signal representation and used support vector regression (SVR) to estimate the HR. Hsu et al. <ref type="bibr" target="#b8">[9]</ref> generated time-frequency maps from the pre-processed green channel signals and used them as input of a VGG-16 model to estimate the HR. Chen et al. <ref type="bibr" target="#b26">[27]</ref> fed the original RGB face video into a convolutional network with attention mechanism and output the related HR signals.</p><p>Although the existing data-driven approaches attempted to make use of statistical learning, as opposed to physical model based signal analysis, they failed to build an end-to-end HR estimator. In addition, the feature representations used by these methods remain hand-crafted, which may not be optimum for the HR estimation task.</p><p>Besides rPPG-based HR measurement, another kind of remote HR estimation methods is based on the ballistocardiographic (BCG) signals, which is the subtle head motions caused by cardiovascular circulation. Inspired by the Eulerian magnification method <ref type="bibr" target="#b29">[30]</ref>, Balakrishnan et al. performed facial key points tracking and used PCA to extract the pulse signal from the trajectories of key points <ref type="bibr" target="#b2">[3]</ref>. Instead of using the PCA component of key points' trajectories to represent HR signal, Irani et al. <ref type="bibr" target="#b10">[11]</ref> performed a stochastic search method using discrete cosine transform (DCT) to find the PCA component with the most significant periodicity. Since this kind of methods is based on subtle motion analysis, the voluntary movements by the subjects will significantly influence the HR estimation accuracy, leading to very limited use in practical applications.</p><p>Camera sensors can also have impact on the HR estimation accuracy. Besides the widely used commodity RGB sensors, several approaches also studied the possibility of using non-RGB, e.g., near-infra-read (NIR) sensors. Inspired by the PVB pulse-extraction method in <ref type="bibr" target="#b20">[21]</ref>, Gastel et al. investigated the feasibility of rPPG-based HR estimation in the NIR spectrum and achieved a robust HR estimation result with both NIR and color sensors <ref type="bibr" target="#b27">[28]</ref>. Chen et al. <ref type="bibr" target="#b28">[29]</ref> directly used the NIR im-ages captured by RealSense 3 and demonstrated the possibility of using NIR images for HR measurement. Instead of using non-color sensors to perform rPPG-based HR estimation, a BCG-based method using depth images was studied in <ref type="bibr" target="#b30">[31]</ref>. Similar to the color sensor based methods, all the remote HR estimation using non-color sensors are still hand-crafted, and make certain assumptions.</p><p>The existing remote HR measurement methods are summarized in Table <ref type="table" target="#tab_0">I</ref>. Despite tremendous progress has been made in remote HR estimation, there are still limitations. First, the existing approaches usually made certain assumptions when building the model, which may limit the application scenarios. Second, most of the existing approaches are designed in a stepby-step hand-crafted way requiring sophisticated knowledge and experiences. Therefore, end-to-end trainable approaches are required to bridge the gap between the capability of remote HR estimation methods and the real application requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Public Databases for Remote HR Estimation</head><p>Many existing methods reported their performance using private databases, leading to difficulties in performance comparison across individual approaches. Public-domain database for evaluating remote HR measurement methods are quite limited. MAHNOB-HCI database <ref type="bibr" target="#b7">[8]</ref> was firstly used for remote HR estimation in <ref type="bibr" target="#b4">[5]</ref>, which consists of 527 videos of 27 subjects, recorded with small head movement and facial expression variation under laboratory illumination. MMSE-HR <ref type="bibr" target="#b5">[6]</ref> was introduced by Tulyakov et al., which consists of 102 videos of 40 subjects involving various subjects' facial expressions and head movements. These two databases are originally designed for emotion analysis, and the subjects' variousness is mainly limited to facial expression changes. At the same time, all the videos of MAHNOB-HCI and MMSE-HR are compressed, which may cause some damages to the HR signals used for further calculation.</p><p>There are also a few public-available databases especially designed for the task of remote HR estimation. Stricker et al. <ref type="bibr" target="#b9">[10]</ref> released the PURE database consisting of 60 videos from 10 subjects, in which all the subjects are asked to perform 6 kinds of movements such as talking or head rotation. Hus et al. <ref type="bibr" target="#b8">[9]</ref> released the PFF database, consisting of 104 videos of 10 subjects, in which only small head movements and illumination variations are involved. These two databases are limited by the number of subjects and recording scenarios, making them not suitable for building a real-world HR estimator. In 2018, Xiaobai et al. <ref type="bibr" target="#b11">[12]</ref> proposed the OBF database specifically designed for heart rate variability (HRV) analysis, and all the scenarios in this database are well-controlled, making it very easy to measure HR.</p><p>We summarize the public-domain databases for remote HR estimation in Table <ref type="table" target="#tab_1">II</ref>. We can see that all the existing databases are limited in either the number of subjects or the recording scenarios, particularly limited to constrained scenarios. A large-scale database recorded under less-constrained environment is required for studying remote HR estimation methods in practice. 3 https://www.intel.com/content/www/us/en/homepage.html</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. VIPL-HR DATABASE</head><p>Considering the constrained data acquisition scenarios used by most of the existing databases for remote HR estimation, we built a new dataset, named as VIPL-HR to promote the research of remote HR estimation under less-constrained scenarios, i.e., with head movement, illumination variation, and acquisition device diversity. VIPL-HR contains 3,130 face videos from 107 subjects, which are recorded with three different sensors (webcam, RealSense, and smartphone) under varying illumination and pose variations. In this section, we provide the details in building our VIPL-HR database covering: i) device setup and data collection, ii) face video compression, and iii) database statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Device Setup and Data Collection</head><p>We design our data collection procedure with the objective in mind that face video recording conditions should cover diversities of environmental illumination, subject pose, acquisition sensor, and HR distribution in order to replicate daily application scenarios.</p><p>The recording environmental setup is illustrated in Figure <ref type="figure" target="#fig_1">3</ref>. In order to cover diverse illumination conditions, we utilize a filament lamp placed in front of the subject as well as the ceiling lamp. Three different illumination scenarios are conducted, i.e., both lamps are turned on, only the ceiling lamp is turned on, and both lamps are turned off. In order to cover diverse pose variation of the subject, we first ask the subjects to sit casually in front of the cameras, then encourage them to perform daily activities such as talking and looking around. In order to cover a wider range of HR in our VIPL-HR database, we ask each subject to do some exercises and then came back for database recording. At the same time, different distances between the subject and the cameras are also considered.</p><p>Sensor diversity is also considered when building the database. We choose three different cameras, i.e., Logitech C310 webcam, Intel Realsense F200 RGB-D camera, and Huawei P9 smartphone (with its frontal camera). Logitech C310 and RealSense F200 are used for visible face video recording with different video resolution, and RealSense F200 is also used for NIR face videos recording to investigate the possibility of remote HR estimation under dark lighting conditions. Since smartphones have become an indispensable part of our daily lives, we also record the face videos when the smartphone is placed in front of the subjects. At the same time, the subjects are also asked to hold the smartphone by themselves to record videos like a video chat scenario. All the related physiological signals, including HR, SpO2, and BVP signals, are synchronously recorded with a CONTEC CMS60C BVP sensor. The details of these recording scenarios can be found in Table <ref type="table" target="#tab_3">III</ref> and the details of the device specifications can be found in Table IV</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Face Video Compression</head><p>The raw data of our VIPL-HR is about 1.05 TB in total, making it inconvenient for distribution. In order to facilitate access to our dataset by the other researchers, we investigate    to make a compressed and resized version of our database. As studied in <ref type="bibr" target="#b31">[32]</ref>, video compression may have big influence on video-based heart rate estimation. Therefore, we carefully compare the individual video compress methods, and frame resizing methods. We considered five different video codecs, i.e., 'MJPG', 'FMP4', 'DIVX', 'PIM1' and 'X264' <ref type="foot" target="#foot_1">4</ref> , which are commonly used in the video coding community. The resizing scales we considered are 1/2, 2/3, and 3/4 for each dimension of the original frame. All the videos are compressed using OpenCV<ref type="foot" target="#foot_2">5</ref>   with FFmpeg interface <ref type="foot" target="#foot_3">6</ref> . We choose a widely used remote HR estimation method Haan2013 <ref type="bibr" target="#b3">[4]</ref> as a baseline HR estimation method to verify how data compression influence its HR estimation accuracy.</p><p>The HR estimation accuracies in terms of root mean square error (RMSE) by the baseline HR estimator <ref type="bibr" target="#b3">[4]</ref> on various compressed videos are reported in Fig. <ref type="figure">4</ref>. From the results, we can see that the 'MJPG' video codec works better in maintaining the HR signal in the videos while it is able to reduce the size of the database significantly. Resizing the frames to 1057-7149 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Database Statistics</head><p>The VIPL-HR dataset contains a total of 2,378 color videos and 752 NIR videos from 107 participants with 79 males and 28 females, and ages distributed between 22 and 41. Each video is recorded with a length of about 30s, and the frame rate is about 30fps (see Table <ref type="table" target="#tab_1">II</ref>). Some example video frames of one subject captured by different devices are shown in Fig. <ref type="figure">5</ref>.</p><p>To obtain additional statics about the pose variations in our VIPL-HR database, we estimate head poses (yaw, pitch, and roll angles) for each video using the OpenFace head pose estimator 7 (see Situation 2 in Table <ref type="table" target="#tab_3">III</ref>). Histograms for maximum amplitudes of yaw, pitch, and roll rotations for all the videos can be found in Fig. <ref type="figure">6</ref>. From the histograms, we can see that the maximum rotation amplitudes of the subjects vary in a large range, i.e., the maximum rotation amplitudes are 92 • in roll, 105 • in pitch and 104 • in yaw. This is reasonable because every subject is allowed to look around during the video recording process.</p><p>At the same time, in order to quantitatively demonstrate the illumination changes in VIPL-HR database, we have calculated the mean gray-scale intensity of face area for Situation 1, Situation 4, and Situation 5 in Table <ref type="table" target="#tab_3">III</ref>. The results are shown 7 https://github.com/TadasBaltrusaitis/OpenFace Fig. <ref type="figure">6</ref>: Histograms of the maximum amplitudes (in degree) of the yaw, pitch, and roll rotations for all the videos with head movement in VIPL-HR. in Fig. <ref type="figure" target="#fig_4">7</ref>. We can see that the mean gray-scale intensity varies from 60 to 212, covering complicated illumination variations.</p><p>A histogram of ground-truth HRs is also shown in Fig. <ref type="figure" target="#fig_5">8</ref>. We can see that the ground-truth HRs in VIPL-HR vary from 47 bpm to 146bpm, which covers the typical HR range <ref type="foot" target="#foot_4">8</ref> . The wide HR distribution in VIPL-HR mitigates the gap between the HR range covered in dataset collection and the HR distribution that can appear in daily-life scenes. The relatively large size of our VIPL-HR also makes it possible to leverage deep learning methods to build more robust data-driven HR estimation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PROPOSED APPROACH</head><p>Prior work performed remote HR estimation mainly based on hand-crafted features, which are built based on certain assumptions of the HR signal and environment. These methods may fail in realistic situations when the assumptions do not hold. Given the large-scale VIPL-HR database with diversities in illumination, pose, and sensors, we are able to build a datadriven HR estimation model using deep learning methods. In this section, we are going to introduce the RhythmNet for remote HR estimation from face video. Fig. <ref type="figure">9</ref> gives an overview of our RhythmNet. We first perform detection and facial landmark detection to locate region of interest (ROI) facial areas, from which a spatial-temporal map is computed as a novel low-level representation of the HR signal. Finally, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Face Detection, Landmark Detection and Segmentation</head><p>We use an open source face detector SeetaFace 9 to detect the face and localize 81 facial landmarks (see Fig. <ref type="figure">10</ref>). Since face and facial landmarks detection is able to run at a frame rate of more than 30 fps, we perform face detection and landmarks detection on every frame in order to get accurate and consistent ROI facial areas in a video sequence. A moving average filter is also applied to the 81 facial landmarks across frames to get more stable landmark locations.</p><p>In order to make full use of all the informative parts containing the color changes due to heart rhythms, we choose to use the whole face area for further processing. Face alignment is firstly performed using the eye centre points, and then a face bounding box is defined with a width of w (where w is the horizontal distance between the outer cheek border points) and height 1.2 * h (where h is the vertical distance between chin location and eye eyebrow centre). Skin segmentation is then applied to the defined ROI to remove the non-face area such as eye region and background area. Since resizing may introduce noise to the HR signals, we choose to use the original face region cropped based on facial landmarks for further processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Spatial-temporal Map for Representing HR Signals</head><p>The only useful information for rPPG-based HR estimation in face video is the skin color variations caused by variations in volume and oxygen saturation of the blood in the vessels due to heart beats, which are very minor in amplitude and can be affected by head movements, illumination variations and sensor noises. In order to suppress noises and improve the SNR of HR signals, Most rPPG-based HR estimation methods use the average pixel values of RGB channels of the whole face as the HR signal representation <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Such an average pooling representation provides better robustness than each single pixel. This can be regarded as the empirical rule, and can work smoothly for rPPG-based HR estimation under various conditions. Unlike the existing methods which only use the average pooling of ROI blocks to compute the HR signal, we have proposed a spatial-temporal 9 https://github.com/seetaface/SeetaFaceEngine map (see Fig. <ref type="figure">10</ref>) to highlight the heart rhythm signals while suppressing the other information irrelevant to heart rhythm. Such a spatial-temporal map can enforce the input to the succeeding network to be as specific as to the heart rhythm signal, and make it possible for us to leverage CNN to learn informative representation for the final HR estimation task.</p><p>Specifically, for a video clip with T frames and c color space dimensions, we first get the face areas for all the frames as stated in IV-A. Then, the face area in each frame is divided into n ROI blocks R 1 , R 2 , • • • , R n . As stated in <ref type="bibr" target="#b13">[14]</ref>, the amplitude of these skin color variations can be very weak, and average pooling has been widely used to remove noises and improve the SNR of HR signals in rPPG-based HR estimation methods. Let C(x, y, t) denote the value at location (x, y) of the t th frame from different dimensions of the color space, and the average pooling of the i th ROI block of the t th frame for each channel can be stated as</p><formula xml:id="formula_1">C i (t) =</formula><p>x,y∈ROIi C(x, y, t)</p><formula xml:id="formula_2">|ROI i |<label>(1)</label></formula><p>where |ROI i | denotes the area of a block ROI (the number of pixels). So, for each video clip we can obtain 3 × n temporal sequences with the length of T for different dimensions of the color space, e.g., Color space selection is very important for representing the HR signal. As stated in <ref type="bibr" target="#b32">[33]</ref>, alternative color spaces derived from RGB video are beneficial for getting a better HR signal representation. Therefore, we take two kinds of color spaces into consideration: i) color spaces with dimension directly related to color, such as HSV color space, and ii) color spaces which are commonly used for skin segmentation, i.e., YUV and YCrCb color spaces. Testing on different color spaces can be found in Section V-D. We imperially choose to use the YUV color space (see our evaluations in Section V-D). The color space transformation can be formulized as</p><formula xml:id="formula_3">C i = {C i (1), C i (2), • • • , C i (T )},</formula><formula xml:id="formula_4">   Y U V    =    0.299 0.587 0.114 -0.169 -0.331 0.5 0.5 -0.419 -0.081       R G B    +    0 128 128   <label>(2)</label></formula><p>Another situation we need to consider is face detection failures for a few frames, which may happen when the subject's head moves or rotates too fast. This will lead to missing data of HR signals and the spatial-temporal representation. In order to handle this issue, we randomly mask a small part of the spatial-temporal maps along the time dimension to simulate the missing data cases, and use the partially masked spatialtemporal maps as augmented data to enhance the robustness of our RhythmNet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Temporal Modeling for HR measurement</head><p>For each face video, we divide it into individual video clips, i.e., v 1 , v 2 , v t , using a fixed sliding window containing w frames moved with a step of 0.5 seconds. The video clips are then used for computing the spatial-temporal maps of each clip, which are used as the input to our succeeding network. We choose ResNet-18 <ref type="bibr" target="#b33">[34]</ref> as the backbone convolutional layers, which includes four blocks made up of convolutional layers and residual link, one convolutional layer, and one fully connected layer for the final regression. L 1 loss is used for measuring the difference between the predicted HR and ground truth HR. The output of the network is a single HR value regressed by a fully connected layer. All the estimated HRs from individual video clips are normalized based on the frame rate of the face video.</p><p>We further take the relationship between adjacent measurements of two video clips into consideration and utilize a Gated Recurrent Unit (GRU) <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref> consisting of a cell, a reset gate and an update gate to model the temporal relationship between succeeding measurement. To be specific, the features extracted from the backbone CNN are fed to a one-layer GRU structure. The output of GRU is fed into a fully connected layer to regress the HR values for individual video clips. For each face video, the average of all the predicted HRs for individual video clips are computed as the final HR result.</p><p>Since the variance of the subjects' HRs is small during a very small period of time <ref type="bibr" target="#b23">[24]</ref>, we introduce a smooth loss function to constrain the smoothness of adjacent HR measurements. We take T continuous measurements hr 1 , hr 2 , • • • , hr T into consideration, which are estimated from the GRU for a small period of time, i.e., 3 seconds. The mean HR within the T continuous measurements is then computed as hr mean = 1 T T i=1 hr t and the smooth loss L smooth is defined as</p><formula xml:id="formula_5">L smooth = 1 T T i=1 hr t -hr mean<label>(3)</label></formula><p>During the backpropagation, the partial derivative of the smooth loss L smooth with respect to the input hr t can be computed as</p><formula xml:id="formula_6">∂L smooth ∂hr t =( 1 T -1)sgn(hr mean -hr t )+ T i=1,i =t 1 T sgn(hr mean -hr t )<label>(4)</label></formula><p>where sgn(x) is a sign function. The final loss function can be written as L = L l1 + λL smooth <ref type="bibr" target="#b4">(5)</ref> where L l1 denotes the L 1 loss function and λ is a parameter for balancing the two terms.</p><p>While RNN is widely used for modeling the temporal continuity in object tracking, action recognition, etc., its effectiveness in modeling the temporal relationships of physiological signals (such as HR), is not known. Therefore, our work is the first known approach uses RNN to improve the temporal estimation stability of HR.</p><p>V. EXPERIMENTS In this section, we provide evaluations of the proposed RhythmNet covering three aspects: i) intra-database testing, ii) cross-database testing, and iii) key components analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Database and Experimental Setting</head><p>We provide evaluations on two widely used public-domain databases, i.e., MAHNOB-HCI <ref type="bibr" target="#b7">[8]</ref> and MMSE-HR <ref type="bibr" target="#b5">[6]</ref> as well as the VIPL-HR database we collected. The details about these databases can be found in Table <ref type="table" target="#tab_1">II</ref>. The ground-truth HRs of MAHNOB-HCI and MMSE-HR database are computed from the electrocardiography (ECG) signals provided in the databases using the OSET ECG Toolbox <ref type="foot" target="#foot_5">10</ref> .</p><p>Different metrics have been used in the literature for evaluating the HR estimation performance, such as the mean and standard deviation (Mean and Std) of the HR error, the mean absolute HR error (MAE), the root mean squared HR error (RMSE), the mean of error rate percentage (MER), and Pearson's correlation coefficients r <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. We also use these evaluation metrics in our experiments below.</p><p>For the MMSE-HR and VIPL-HR datasets, we use a temporal sliding window with 300 frames to compute the spatialtemporal maps. For the MAHNOB-HCI database, since its original frame rate is 61 fps, we first downsample the videos to 30.5 fps before using the same temporal window size. The number of the ROI blocks used for spatial-temporal map generation for all the database is 25 (5 × 5 grids). During the training phase, half of the generated spatial-temporal maps were randomly masked, and the mask length varies from 10 frames to 30 frames. For the temporal relationship modelling, adjacent estimated HRs are used to compute the smooth . The balance parameter λ in Equ. 5 is set to 100. Since the face sizes in the NIR videos are small for the face detector, only 497 NIR videos with detected faces are involved in the experiments. Our RhythmNet is implemented using PyTorch <ref type="foot" target="#foot_6">11</ref> . We use an Adam solver <ref type="bibr" target="#b36">[37]</ref> with an initial learning rate of 0.001, and set the maximum epoch number to 50. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Intra-database Testing</head><p>We first conduct intra-database experiments on each of the three databases. For the intra-database testing, in order to prove the effectiveness of the proposed HR estimator, we use a five-fold subject-exclusive cross-validation for the VIPL-HR database, which means that the subjects in the training set will not appear in the testing set. Since the MAHNOB-HCI and MMSE-HR databases have very limited face videos, we use a three-fold subject-independent cross-validation protocol.</p><p>1) Experiments on Color Face Videos: We first perform intra-database evaluations using RGB face videos in our large VIPL-HR database, and compare the proposed approach with a number of state-of-the-art traditional methods (Haan2013 <ref type="bibr" target="#b3">[4]</ref>, Tulyakov2016 <ref type="bibr" target="#b5">[6]</ref>, POS <ref type="bibr" target="#b6">[7]</ref>). Two deep learning based methods, i.e., I3D <ref type="bibr" target="#b37">[38]</ref> and DeepPhy <ref type="bibr" target="#b26">[27]</ref>, are also used for comparison. I3D <ref type="bibr" target="#b36">[37]</ref> is widely used for video analysis, and DeepPhy <ref type="bibr" target="#b26">[27]</ref> is the state-of-the-art deep learning based HR estimation method. For I3D, we directly use the code provided by DeepMind <ref type="foot" target="#foot_7">12</ref> , and trained it for HR estimation task. For DeepPhy, we contacted the authors, but the code is not publicly available; so we have re-implemented DeepPhy based on the descriptions in <ref type="bibr" target="#b26">[27]</ref>. We use the same five-fold subjectexclusive protocol for all the deep learning based methods. For our RhythmNet, we report both results using and without using GRU for comparisons with the traditional state-of-theart methods. The results of individual HR estimation methods are given in Table <ref type="table" target="#tab_4">V</ref>.</p><p>From Table <ref type="table" target="#tab_4">V</ref>, we can see that the proposed approach achieves a promising result with a RMSE of 8.94 bpm when only using the CNN part of RhythmNet, which is a much lower error than the best of the baseline methods <ref type="bibr" target="#b26">[27]</ref> (RMSE = 13.8 bpm). The HR estimation error by the complete RhythmNet is further reduced to 8.14 bpm. In Fig. <ref type="figure">11</ref>(a), we also draw a Bland-Altman plot of the ground-truth HR and the estimated HR by our RhythmNet to analyze the estimation consistencies by individual approaches. The Bland-Altman plot for Deep-Phy <ref type="bibr" target="#b26">[27]</ref> is also given for comparison in Fig. <ref type="figure">11(b</ref>). Again, it can be seen that our method achieves a better consistency than DeepPhy <ref type="bibr" target="#b26">[27]</ref> on the VIPL-HR database.</p><p>We further check the HR estimation error distributions of the proposed method and <ref type="bibr" target="#b26">[27]</ref>. As shown in Fig. <ref type="figure">12</ref>, for most 1057-7149 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. of the samples (71%), the proposed approach achieves a lower HR estimation error than 5bpm, while the percentage for <ref type="bibr" target="#b26">[27]</ref> is only 41.5%. We also plot the estimated HR comparing against the ground-truth HR in Fig. <ref type="figure" target="#fig_9">13</ref> to see the correlations between the estimated HR and the ground-truth HR. We can see that overall the predicted HRs are well correlated with the ground truth in a wide range of HR from 47 bpm to 147 bpm.</p><p>The results show the effectiveness of the proposed end-to-end learning-based HR estimation method in handling challenges due to pose, illumination, and sensor variations. We can also notice from Table V that deep learning based methods (our RhythmNet, I3D <ref type="bibr" target="#b37">[38]</ref>, and Deep-Phy <ref type="bibr" target="#b26">[27]</ref>) performs better than the traditional methods (e.g.,  Tulyakov2016 <ref type="bibr" target="#b5">[6]</ref>, POS <ref type="bibr" target="#b6">[7]</ref>, Haan2013 <ref type="bibr" target="#b3">[4]</ref>) on the challenging VIPL-HR dataset. These results indicate that given a largescale database (such as VIPL-HR), deep learning based methods are able to learn more informative representation for HR estimation.</p><p>We also perform intra-database testing on the MAHNOB-HCI and MMSE-HR databases using a three-fold subjectexclusive protocol as stated in Section V-A. The results can be found in Table <ref type="table" target="#tab_6">VII</ref> and Table <ref type="table" target="#tab_7">VIII</ref>, respectively. From the results, we can see that the proposed RhythmNet again achieves the best results on both databases. The results indicate that our method has a good generalization ability under different image acquisition conditions, and is fairly robust when the training dataset become small.</p><p>2) Experiment on NIR Face Video: The experiments on NIR face videos are also conducted using the five-fold subjectexclusive protocol. Since the NIR face video only has one channel, we do not use color space transformation and compute one-channel spatial-temporal map for our RhythmNet. The baseline methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b26">[27]</ref> for RGB based HR estimation do not work on NIR videos. Thus, we only report the results by our RhythmNet without GRU and the complete RhythmNet in Table . VI. The Bland-Altman plots for NIR face videos is also given in Fig. <ref type="figure">14</ref>.</p><p>From Fig. <ref type="figure">11</ref>(a) and Fig. <ref type="figure">14</ref>, we can see that the results using NIR face videos are not as good as those using RGB face videos. This is understandable because a single NIR channel may not convey as sufficient HR rhythm information as the three-channel RGB videos. In addition, we notice the SeetaFace face detector is trained using RGB images, and may fail to detect some faces in the NIR face images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Cross-database Testing</head><p>In order to validate the generalization ability of our Rhythm-Net, we also perform cross-database evaluations using our  and we directly use the results of these methods provided in <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b5">[6]</ref>.</p><p>From the results, we can see that the proposed method could achieve promising results even when we directly test our VIPL-HR pre-trained model on these two databases, i.e., a RMSE of bpm on the MAHNOB-HCI database, and a RMSE of 7.33 bpm on the MMSE-HR database. The error rates are further reduced to 3.99 bpm and 5.49 bpm on MAHNOB-HCI and MMSE-HR, respectively when we finetune the pre-trained model on MAHNOB-HCI and MMSE-HR databases. Both results by the proposed approach are much better than previous methods. These results indicate that the variations of illumination, movement, and acquisition sensors covered in the VIPL-HR database are helpful for learning a deep HR estimator which has good generalization ability to unseen scenarios. In addition, the proposed RhythmNet is able to leverage the diverse image acquisition conditions in VIPL-HR to learn a robust HR estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Key Components Analysis</head><p>In this section, we further analyze the effectiveness of our approach from multiple aspects, covering spatial-temporal map, color space selection, temporal modeling via GRU and computational cost. All the experiments are conducted on the VIPL-HR database under a subject-exclusive five-fold crossvalidation protocol.</p><p>1) Spatial-temporal Map: From the results in Table <ref type="table" target="#tab_4">V</ref>, we can see that under the same experimental settings, I3D <ref type="bibr" target="#b37">[38]</ref> trained from raw RGB face videos achieves a RMSE of 15.9 bpm, which is much worse than our RhythmNet with spatial-temporal maps as representations (RMSE = 8.14 bpm). These results indicate that it is very difficult for deep neural networks to directly learn informative features from the raw RGB videos. We further visualize the feature maps of I3D for the input video clips using the visualization method proposed in <ref type="bibr" target="#b38">[39]</ref>. The visualization map is calculated based on the average of all the feature maps of all video clips, i.e., 16427 video clips from 476 videos in one fold of test. The map is shown in Fig. <ref type="figure" target="#fig_10">15(a)</ref>. The visualization map of our RhythmNet is given in Fig. <ref type="figure" target="#fig_10">15(b)</ref>. Since our RhythmNet takes the spatialtemporal maps as input, we calculated the mean responses of each rows (spatial dimension) and then reshaped the responses to a visualization map. From the visualization maps, we can see that our RhythmNet focuses on the informative face regions, and thus gives more accurate estimation results.</p><p>2) Color Space Selection: As discussed in <ref type="bibr" target="#b39">[40]</ref>, head movement effects much more on the intensity than the chromaticity of the image because the chromaticity reflects the intrinsic optical properties of hemoglobin in blood. Therefore, choosing a color space separating chromaticity from intensity is helpful to reduce the movement-induced artifacts and thus benefit the training process. We considered three commonly used color space transformations, i.e., HSV, YCrCb, and YUV, as well as the RGB color space. For each experiment, the color space used for testing is the same as the color space used for training. The results by RhythmNet using these color spaces are given in Fig. <ref type="figure" target="#fig_11">16</ref>.</p><p>From the results, we can see that different color spaces lead to very different HR estimation accuracies, and using YUV color space achieves the best result with a RMSE of 8.8 bpm.</p><p>The results indicate that the YUV color space is more suitable in representing the HR signals, and helpful for capturing more information about heart rhythm. This is the reason that we choose to use the YUV color space in all the experiments.</p><p>3) Temporal Modeling: For HR measurement, we use GRU to model the relationship between adjacent measurements from succeeding video clips. We study the importance of this part by removing it from our RhythmNet and direct compute the average HR of estimations for all the clips. The results by RhythmNet with and without using the GRU are given in Table <ref type="table" target="#tab_4">V</ref>. From the results, we see that removing the GRU module leads to increased RMSE from 8.11bmp to 8.94 bpm, which suggests the usefulness of the temporal modelling module in our RhythemNet.</p><p>4) Computational Cost: The model size of our HR estimator is about 42 MB, and the inference time is about 8ms on a Titan 1080Ti GPU when using a sliding window with 300 frames. These results indicate that the proposed approach is efficient, enabling real-time HR measurement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Further Analysis</head><p>While the proposed RhythmNet achieves promising results on multiple databases, robust remote HR estimation under less-constrained scenarios faces a number of challenges. We provide additional analysis in terms of the challenging factors, i.e., video compression, illumination, head movement.</p><p>1) Video Compression: As shown in Section III-B, when using MJPG to compress the video data, the state-of-the-art method <ref type="bibr" target="#b3">[4]</ref> shows minor performance degradation. We are also interested to see how such a compression can influence the proposed RyhthmNet. As shown in Table <ref type="table" target="#tab_0">IX</ref>, the HR estimation errors by our RhythmNet before and after compression are also the same. This suggests that the proposed approach is also very robust to the video compression by MJPG. From the result, we can see that the estimated results based on compressed data are very close to the results based on uncompressed data, which indicates that the compressed version VIPL-HR database is able to maintain the HR signals.</p><p>All the experiments in this paper are based on the compressed version of VIPL-HR database.</p><p>2) Illumination: We study the influence of illumination variations on our RhythemNet by using the RGB face videos of Scenario 1, 4, and 5 in Table III from the VIPL-HR dataset. The RMSE are reported in Fig. <ref type="figure" target="#fig_4">17(a)</ref>. From the results, we can see that under a bright lighting condition, i.e., with the IX: The HR estimation results by our RyhthmNet using compressed and uncompressed RGB face videos of VIPL-HR.  indicate that NIR face videos can be a good choice for HR measurement in dark environment.</p><p>3) Head Movement: In order to analyse the influence of head movements, we compute the HR estimation errors for Situation 1, 2, and 3 in Table III from the VIPL-HR dataset. The results are shown in Fig. <ref type="figure" target="#fig_4">17(b</ref>). From the results, we can see that all kinds of head movements could significantly influence the estimation accuracy. The RMSE raises to 8.28 bpm when the subject is talking, and 9.40 bpm when the subject performs large scale head movements. These results indicate that HR estimation under different head movements is still a challenging task, and should be considered when building a robust HR estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND FUTURE WORK</head><p>While remote HR estimation from a face video has wide applications, it is a challenging problem in less-constrained scenarios due to variations of head movement, illumination, and sensor diversity. In this paper, we propose an end-toend learning approach (RhythmNet) for robust heart rate estimation from face videos. Our RhythmNet uses a novel spatial-temporal map represent the HR signals in the videos, which allows efficient modelling via CNN. We also take into account the temporal relationship of adjacent HR estimates to perform continuous HR measurement. At the same time, we also build a multi-modality database (VIPL-HR) for studying HR estimation under less-constrained scenarios covering variations of illumination, head movement, and sensor diversity. The proposed approach achieves promising HR estimation accuracies in both within-database and cross-database testing scenarios on the MAHNOB-HCI, MMSE-HR, and VIPL-HR databases. In our future work, we would like to investigate the effectiveness of the proposed approach for the other rPPGbased physiological status measurement tasks, such as the breath rate and blood pressure measurement from videos. We would also like to investigate robust HR estimation approaches leveraging distribution learning <ref type="bibr" target="#b40">[41]</ref> or multi-task learning <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>L</head><label></label><figDesc>= Lab Environment, D = Dim Environment, B = Bright Environment, E = Expression, S = Stable, SM = Slight Movement, LM = Large Movement, T = Talking, C = Color Camera, N = NIR Camera, P = Smart Phone Frontal Camera</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Devices and setup used in collecting VIPL-HR.</figDesc><graphic coords="6,137.48,474.94,87.58,61.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, LM = Large Movement, T = Talking, L = Lab Environment, D = Dim Environment, B = Bright Environment</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :Fig. 5 :</head><label>45</label><figDesc>Fig. 4: Evaluations on VIPL-HR using (a) different video compression codecs and (b) different image resolutions for data compression. All tests used the same HR estimator of Haan2013 [4].</figDesc><graphic coords="7,119.06,216.66,50.20,55.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: The histogram of the average image intensity (grayscale) for the videos recorded under the illumination-specific situations in VIPL-HR.</figDesc><graphic coords="7,349.64,272.17,175.74,80.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: The histogram of the ground-truth HR distribution in VIPL-HR.</figDesc><graphic coords="8,86.62,56.07,175.74,98.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>where C donates one of the c color space dimensions and i donates the index of the ROI. In order to make the best use of the HR signals, a min-max normalization is applied to each temporal signal, and the values of the temporal series are scaled into [0, 255]. Then, we place the n temporal sequences into rows and obtain a spatial-temporal map representation for the HR signal in a video clip. Eventually, we get a spatial-temporal representation from the raw face video clip with the size of T × n × c as the input of our succeeding deep HR estimation network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :Fig. 10 :</head><label>910</label><figDesc>Fig. 9: Overview of our RhythmNet. Given an input video sequence, we first divide it into multiple short video clips (v 1 , v 2 , • • • , v t ), and perform face and landmark detection on each frame for face alignment. Then spatial-temporal maps are generated from the aligned face images per video clip to represent the HR signals, and a deep network consisting of convolutional layers and recurrent layers is trained to predict the HR from the spatial-temporal maps. Finally, the HR estimated for the input video sequence is computed as the average of all the estimated HRs from individual video clips.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 :Fig. 12 :</head><label>1112</label><figDesc>Fig.11:The Bland-Altman plots demonstrating the agreement of the HR est and HR gt for (a) RhythmNet and (b) Deep-Phy<ref type="bibr" target="#b26">[27]</ref> in the VIPL-HR database. The lines represent the mean and 95% limits of agreement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 :</head><label>13</label><figDesc>Fig.13:The scatter plot comparing the ground truth HR gt and the estimated HR est by our RhythmNet for RGB face videos on the VIPL-HR dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 15 :</head><label>15</label><figDesc>Fig.15:The mean visualization maps of all the video clips in the test set of one fold for (a) I3D model<ref type="bibr" target="#b37">[38]</ref> and (b) our RhythmNet.</figDesc><graphic coords="12,363.24,61.05,62.76,62.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 16 :</head><label>16</label><figDesc>Fig.16:The HR estimation errors (in RMSE) by RhythmNet using spatial-temporal maps generated from different color spaces (RGB, HSV, YCrCb, and YUV).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 17 :Fig. 18 :</head><label>1718</label><figDesc>Fig. 17: The HR estimation errors (in RMSE) by RhythmNet under (a) different illumination conditions and (b) different head movement conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>A brief summary of the existing rPPG based remote HR estimation methods.</figDesc><table><row><cell cols="4">approaches, in which ICA was applied to the whole region of</cell><cell></cell><cell></cell></row><row><cell cols="4">interest (ROI), Lam and Kuno applied ICA in a patch level</cell><cell></cell><cell></cell></row><row><cell>for HR estimation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Publication</cell><cell>Camera</cell><cell>Input singal</cell><cell>HR temporal signal extraction</cell><cell>HR estimation</cell><cell>Results (RMSE in bpm)</cell></row><row><cell>Verkruysse et al. [14]</cell><cell>Webcam</cell><cell>G</cell><cell>bandpass filter</cell><cell>FFT</cell><cell>-</cell></row><row><cell>Poh et al. [1]</cell><cell>Webcam</cell><cell>R,G,B</cell><cell>ICA + bandpass filter</cell><cell>FFT</cell><cell>MAHNOB-HCI 25.9 bpm</cell></row><row><cell>Poh et al. [2]</cell><cell>Webcam</cell><cell>R,G,B</cell><cell>ICA + bandpass &amp; detrending filter</cell><cell>Peak detection</cell><cell>MAHNOB-HCI 13.6 bpm</cell></row><row><cell>McDuff et al. [15]</cell><cell>Five-band camera</cell><cell>R,G,B,C,O</cell><cell>ICA + bandpass&amp; detrending filter</cell><cell>Peak detection</cell><cell>-</cell></row><row><cell>Lam et al. [16]</cell><cell>Webcam</cell><cell>R,G,B</cell><cell>Patch level ICA + bandpass&amp; detrending filter</cell><cell>FFT + majority vote</cell><cell>MAHNOB-HCI 8.9 bpm</cell></row><row><cell>Lewandowska et al. [17]</cell><cell>Webcam</cell><cell>R,G,B</cell><cell>PCA + bandpass filter</cell><cell>FFT</cell><cell>-</cell></row><row><cell>Tsouri et al. [18]</cell><cell>Webcam</cell><cell>R,G,B</cell><cell>constrained ICA + bandpass filter</cell><cell>FFT</cell><cell>-</cell></row><row><cell>Haan and Jeanne [4]</cell><cell>Webcam</cell><cell>R,G,B</cell><cell>Color space transformation based on skin model (CHROM feature)</cell><cell>FFT</cell><cell>MAHNOB-HCI / MMSE-HR / VIPL-HR 6.23 bpm / 11.37 bpm / 16.9 bpm</cell></row><row><cell>Wang et al. [19]</cell><cell>Webcam</cell><cell>R,G,B</cell><cell>Pixel level CHROM feature + PCA</cell><cell>FFT</cell><cell>-</cell></row><row><cell>Feng et al. [20]</cell><cell>Webcam</cell><cell>R,G</cell><cell>Adaptive Green/Red difference</cell><cell>FFT</cell><cell>-</cell></row><row><cell>Haan and Leest [21]</cell><cell>Webcam</cell><cell>R,G,B</cell><cell>Color space transformation based on skin-tone and camera properties</cell><cell>FFT</cell><cell>-</cell></row><row><cell>Wang et al. [7]</cell><cell>Webcam</cell><cell>R,G,B</cell><cell>Color space transformation based on skin model</cell><cell>FFT</cell><cell>VIPL-HR 17.2 bpm</cell></row><row><cell>Kumar et al. [22]</cell><cell>Webcam</cell><cell>G</cell><cell>Patch level signals combination using weighted average</cell><cell>FFT</cell><cell>-</cell></row><row><cell>Wang et al. [23]</cell><cell>Webcam</cell><cell>R,G,B</cell><cell>Using the prior knowledge of heart rate to remove noise</cell><cell>FFT</cell><cell>-</cell></row><row><cell>Niu et al. [24]</cell><cell>Webcam</cell><cell>R,G,B</cell><cell>Patch level CHROM feature</cell><cell>FFT + distribution learning</cell><cell>MAHNOB-HCI 8.72 bpm</cell></row><row><cell>Tulyakov et al. [6]</cell><cell>Webcam</cell><cell>R,G,B</cell><cell>CHROM features + self-adaptive matrix completion</cell><cell>FFT</cell><cell>MAHNOB-HCI / MMSE-HR / VIPL-HR 6.23 bpm / 11.37 bpm / 21.0 bpm</cell></row><row><cell>Wang et al. [25]</cell><cell>Webcam</cell><cell>R,G,B</cell><cell>Color space transformation based on spatial subspace of skin-pixels</cell><cell>FFT</cell><cell>-</cell></row><row><cell>Hsu et al. [26]</cell><cell>Webcam</cell><cell>R,G,B</cell><cell>ICA</cell><cell>FFT+SVR</cell><cell>-</cell></row><row><cell>Hsu et al. [9]</cell><cell>Webcam</cell><cell>R,G,B</cell><cell>CHROM feature + detrending filter + 2nd order difference</cell><cell>STFT + VGG model</cell><cell>MAHNOB-HCI 4.27 bpm</cell></row><row><cell>Chen et al. [27]</cell><cell>Webcam</cell><cell>R,G,B</cell><cell>Convolutional Network with attention module</cell><cell>FFT</cell><cell>MAHNOB-HCI (MAE) 4.57 bpm</cell></row><row><cell>Li et al. [5]</cell><cell>Webcam</cell><cell>G</cell><cell>Background noise removing + bandpass &amp; detrending filter</cell><cell>FFT</cell><cell>MAHNOB-HCI / MMSE-HR 7.62 bpm / 19.95 bpm</cell></row><row><cell>Van et al. [28]</cell><cell>NIR Sensor</cell><cell>NIR</cell><cell>Signal modified using prior of skin reflectance</cell><cell>FFT</cell><cell>-</cell></row><row><cell>Chen et al. [29]</cell><cell>RealSense</cell><cell>NIR</cell><cell>Global self-similarity filter</cell><cell>FFT</cell><cell>-</cell></row><row><cell>Proposed</cell><cell cols="2">Webcam/Phone R,G,B/NIR RealSense</cell><cell>Spatial-temporal maps</cell><cell>Deep regression model with GRU</cell><cell>MAHNOB-HCI / MMSE-HR / VIPL-HR 4.00 bpm / 5.03 bpm/ 8.14 bpm</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>A summary of the public-domain databases and our VIPL-HR database for remote HR estimation.</figDesc><table><row><cell></cell><cell># Subjects</cell><cell>Illumination variation</cell><cell cols="2">Head movement diversity Sensor</cell><cell>#Videos</cell></row><row><cell>MAHNOB-HCI [8]</cell><cell>27</cell><cell>L</cell><cell>E</cell><cell>C</cell><cell>527</cell></row><row><cell>MMSE-HR [6]</cell><cell>40</cell><cell>L</cell><cell>E</cell><cell>C</cell><cell>102</cell></row><row><cell>PURE [10]</cell><cell>10</cell><cell>L</cell><cell>S/SM/T</cell><cell>C</cell><cell>60</cell></row><row><cell>PFF [9]</cell><cell>13</cell><cell>L/D</cell><cell>S/SM</cell><cell>C</cell><cell>104</cell></row><row><cell>OBF [12]</cell><cell>106</cell><cell>L</cell><cell>S</cell><cell>C/N</cell><cell>2,120</cell></row><row><cell>VIPL-HR</cell><cell>107</cell><cell>L/D/B</cell><cell>S/LM/T</cell><cell>C/N/P</cell><cell>3,130</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV :</head><label>IV</label><figDesc>Specifications of individual recording devices used in our VIPL-HR database.</figDesc><table><row><cell>Device</cell><cell>Specification</cell><cell>Setting</cell><cell>Output</cell></row><row><cell>Computer</cell><cell>Lenovo ThinkCentre</cell><cell>Windows 10 OS</cell><cell>N/A</cell></row><row><cell>Color camera</cell><cell>Logitech C310</cell><cell>25fps  *  960×720 color camera</cell><cell>Color videos</cell></row><row><cell>RGB-D camera</cell><cell>RealSense F200</cell><cell>30fps  *  , 640×480 NIR camera 1920×1080 color camera,</cell><cell>Color videos NIR videos</cell></row><row><cell>Smart phone</cell><cell>HUAWEI P9 frontal camera</cell><cell>30fps, 1920×1080 color camera</cell><cell>Color videos</cell></row><row><cell>BVP recoder</cell><cell>CONTEC CMS60C</cell><cell>N/A</cell><cell>HR, SpO2, and BVP signals</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Details of the nine recording situations in the VIPL-HR database.</figDesc><table><row><cell>Scenario</cell><cell>Head movement</cell><cell cols="3">Illumination Distance Exercise</cell><cell>Phone recording method</cell></row><row><cell>1</cell><cell>S</cell><cell>L</cell><cell>1m</cell><cell>No</cell><cell>Fixed</cell></row><row><cell>2</cell><cell>LM</cell><cell>L</cell><cell>1m</cell><cell>No</cell><cell>Fixed</cell></row><row><cell>3</cell><cell>T</cell><cell>L</cell><cell>1m</cell><cell>No</cell><cell>Fixed</cell></row><row><cell>4</cell><cell>S</cell><cell>B</cell><cell>1m</cell><cell>No</cell><cell>Fixed</cell></row><row><cell>5</cell><cell>S</cell><cell>D</cell><cell>1m</cell><cell>No</cell><cell>Fixed</cell></row><row><cell>6</cell><cell>S</cell><cell>L</cell><cell>1.5m</cell><cell>No</cell><cell>Fixed</cell></row><row><cell>7</cell><cell>S</cell><cell>L</cell><cell>1m</cell><cell>Yes</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V :</head><label>V</label><figDesc>The HR estimation results by the proposed approach and several state-of-the-art methods on color face videos of the VIPL-HR database.</figDesc><table><row><cell>Method</cell><cell>Mean Std MAE RMSE MER r (bpm) (bpm) (bpm) (bpm)</cell></row><row><cell>Tulyakov2016 [6]</cell><cell>10.8 18.0 15.9 21.0 26.7% 0.11</cell></row><row><cell>POS [7]</cell><cell>7.87 15.3 11.5 17.2 18.5% 0.30</cell></row><row><cell>Haan2013 [4]</cell><cell>7.63 15.1 11.4 16.9 17.8% 0.28</cell></row><row><cell>I3D [38]</cell><cell>1.37 15.9 12.0 15.9 15.6% 0.07</cell></row><row><cell>DeepPhy [27]</cell><cell>-2.60 13.6 11.0 13.8 13.6% 0.11</cell></row><row><cell cols="2">RhythmNet w/o GRU 1.02 8.88 5.79 8.94 7.38% 0.73</cell></row><row><cell>RhythmNet</cell><cell>0.73 8.11 5.30 8.14 6.71% 0.76</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI :</head><label>VI</label><figDesc>The HR estimation results on NIR face videos of our VIPL-HR database. The Bland-Altman plots demonstrating the agreement of the estimated HR est by RhythmNet and the ground-truth HR gt for NIR face videos in VIPL-HR. The lines represent the mean and 95% limits of agreement.</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell cols="5">Mean Std MAE RMSE MER r (bpm) (bpm) (bpm) (bpm)</cell></row><row><cell cols="8">RhythmNet w/o GRU 1.26 13.8 9.11 13.9 11.6% 0.62</cell></row><row><cell cols="2">RhythmNet</cell><cell></cell><cell cols="5">1.74 12.4 8.45 12.5 10.8% 0.71</cell></row><row><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>mean+1.96</cell></row><row><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>est gt -HR</cell><cell>0 10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>mean</cell></row><row><cell>HR</cell><cell>-20 -10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>mean-1.96</cell></row><row><cell></cell><cell>-30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>40</cell><cell>60</cell><cell>80</cell><cell>100</cell><cell>120</cell><cell>140</cell><cell>160</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(HR gt + HR est )/2</cell><cell></cell><cell></cell></row><row><cell>Fig. 14:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII :</head><label>VII</label><figDesc>The HR estimation results by the proposed approach and several state-of-the-art methods on the MAHNOB-HCI database.</figDesc><table><row><cell>Method</cell><cell cols="4">Mean (bpm) (bpm) (bpm) Std RMSE MER</cell><cell>r</cell></row><row><cell>Poh2010 [1]</cell><cell>-8.95</cell><cell>24.3</cell><cell>25.9</cell><cell cols="2">25.0% 0.08</cell></row><row><cell>Poh2011 [2]</cell><cell>2.04</cell><cell>13.5</cell><cell>13.6</cell><cell cols="2">13.2% 0.36</cell></row><row><cell>Balakrishnan2013 [3]</cell><cell>-14.4</cell><cell>15.2</cell><cell>21.0</cell><cell cols="2">20.7% 0.11</cell></row><row><cell>Li2014 [5]</cell><cell>-3.30</cell><cell>6.88</cell><cell>7.62</cell><cell cols="2">6.87% 0.81</cell></row><row><cell>Haan2013 [4]</cell><cell cols="2">-2.89 13.67</cell><cell>10.7</cell><cell cols="2">12.9% 0.82</cell></row><row><cell>Tulyakov2016 [6]</cell><cell>3.19</cell><cell>5.81</cell><cell>6.23</cell><cell cols="2">5.93% 0.83</cell></row><row><cell>RhythmNet(WithinDB)</cell><cell>0.41</cell><cell>3.98</cell><cell>4.00</cell><cell cols="2">4.18% 0.87</cell></row><row><cell>RhythmNet(CrossDB)</cell><cell>-5.66</cell><cell>6.06</cell><cell>8.28</cell><cell cols="2">8.00% 0.64</cell></row><row><cell cols="2">RhythmNet(Fine-tuned) 0.43</cell><cell>3.97</cell><cell cols="3">3.99 4.06% 0.87</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VIII :</head><label>VIII</label><figDesc>The HR estimation results by the proposed approach and several state-of-the-art methods on the MMSE-HR database.</figDesc><table><row><cell>Method</cell><cell cols="4">Mean (bpm) (bpm) (bpm) Std RMSE MER</cell><cell>r</cell></row><row><cell>Li2014 [5]</cell><cell cols="4">11.56 20.02 19.95 14.64% 0.38</cell></row><row><cell>Haan2013 [4]</cell><cell cols="4">9.41 14.08 13.97 12.22% 0.55</cell></row><row><cell>Tulyakov2016 [6]</cell><cell cols="4">7.61 12.24 11.37 10.84% 0.71</cell></row><row><cell cols="2">RhythmNet(WithinDB) -0.85</cell><cell>4.99</cell><cell>5.03</cell><cell>3.67% 0.86</cell></row><row><cell>RhythmNet(CrossDB)</cell><cell cols="2">-.2.33 6.98</cell><cell>7.33</cell><cell>3.62% 0.78</cell></row><row><cell cols="2">RhythmNet(Fine-tuned) -0.88</cell><cell>5.45</cell><cell>5.49</cell><cell>3.58% 0.84</cell></row><row><cell cols="5">VIPL-HR database and two widely used HR estimation</cell></row><row><cell cols="5">databases, i.e., MAHNOB-HCI, and MMSE-HR databases.</cell></row><row><cell cols="5">Specifically, we train our RhythmNet on the color videos of</cell></row><row><cell cols="5">VIPL-HR database and directly test it on the MAHNOB-HCI</cell></row><row><cell cols="5">and MMSE-HR databases. We also fine-tune the VIPL-HR</cell></row><row><cell cols="5">pre-trained model on MAHNOB-HCI and MMSE-HR follow-</cell></row><row><cell cols="5">ing a three-fold subject-independent cross-validation testing</cell></row><row><cell cols="5">protocol to see whether per-database fine-tuning could improve</cell></row><row><cell cols="5">the HR estimation performance or not. All the results are pro-</cell></row><row><cell cols="5">vided in Table VII and Table VIII. The baseline methods we</cell></row><row><cell cols="5">use for comparisons are Poh2010 [1], Poh2011 [2], Balakrish-</cell></row><row><cell cols="5">nan2013 [3], Li2014 [5], Haan2013 [4] and Tulyakov2016 [6],</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>We provide a video (MagnificatedSkinColorChange.avi) of such magnified skin color changes in https://drive.google.com/file/d/1 sjPOaXP8lFLQkGotJIPsDuU0mRSxegv/view?usp=sharing</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>http://www.fourcc.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>https://opencv.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>http://ffmpeg.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4"><p>https://en.wikipedia.org/wiki/Heart rate</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_5"><p>http://www.oset.ir</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_6"><p>https://pytorch.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_7"><p>https://github.com/deepmind/kinetics-i3d</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1057-7149 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2019.2947204, IEEE</p><p>Transactions on Image Processing</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the editors and anonymous reviewers for their constructive comments and suggestions.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This research was supported in part by the National Key R&amp;D Program of China (grant 2017YFA0700800), Natural Science Foundation of China (grants 61672496 and 61702486), External Cooperation Program of Chinese Academy of Sciences (CAS) (grant GJHZ1843).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Non-contact, automated cardiac pulse measurements using video imaging and blind source separation</title>
		<author>
			<persName><forename type="first">M.-Z</forename><surname>Poh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Express</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="10" to="762" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Advancements in noncontact, multiparameter physiological measurements using a webcam</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="11" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detecting pulse from head motions in video</title>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3430" to="3437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust pulse rate from chrominance-based rppg</title>
		<author>
			<persName><forename type="first">G</forename><surname>De Haan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jeanne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2878" to="2886" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Remote heart rate measurement from face videos under realistic situations</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="4264" to="4271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Self-adaptive matrix completion for heart rate estimation from face videos under realistic conditions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2396" to="2404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Algorithmic principles of remote ppg</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Brinker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stuijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>De Haan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1479" to="1491" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A multimodal database for affect recognition and implicit tagging</title>
		<author>
			<persName><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lichtenauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="55" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning with time-frequency representation for pulse estimation</title>
		<author>
			<persName><forename type="first">M.-S</forename><forename type="middle">C</forename><surname>Gee-Sern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arulmurugan</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><surname>Ambikapathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE IJCB</title>
		<meeting>IEEE IJCB</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="642" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Non-contact video-based pulse rate measurement on a mobile service robot</title>
		<author>
			<persName><forename type="first">R</forename><surname>Stricker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE RO-MAN</title>
		<meeting>IEEE RO-MAN</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1056" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improved pulse detection from head motions using dct</title>
		<author>
			<persName><forename type="first">R</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nasrollahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VISAPP</title>
		<meeting>VISAPP</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="118" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The obf database: A large face video database for remote physiological signal measurement and atrial fibrillation detection</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-T</forename><forename type="middle">S J J K M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-V</forename><forename type="middle">M T</forename><surname>Xiaobai Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iman</forename><surname>Alikhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE FG</title>
		<meeting>IEEE FG</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">VIPL-HR: A multi-modal database for pulse estimation from less-constrained face video</title>
		<author>
			<persName><forename type="first">X</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACCV</title>
		<meeting>ACCV</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="562" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Remote plethysmographic imaging using ambient light</title>
		<author>
			<persName><forename type="first">W</forename><surname>Verkruysse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Svaasand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Nelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Express</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">26</biblScope>
			<biblScope unit="page" from="21" to="434" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improvements in remote cardiopulmonary measurement using a five band digital camera</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gontarek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2593" to="2601" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust heart rate measurement from video using select random patches</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kuno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3640" to="3648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Measuring pulse rate with a webcam -a non-contact method for evaluating cardiac activity</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lewandowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ruminski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kocejko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ComSIS</title>
		<meeting>ComSIS</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="405" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Constrained independent component analysis approach to nonobtrusive pulse rate measurements</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Tsouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Dianat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Mestha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Optics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">77011</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploiting spatial redundancy of image sensor for motion robust rppg</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stuijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>De Haan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="415" to="425" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Motion-resistant remote imaging photoplethysmography based on the optical properties of skin</title>
		<author>
			<persName><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-M</forename><surname>Po</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="879" to="891" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved motion robustness of remote-ppg by using the blood volume pulse signature</title>
		<author>
			<persName><forename type="first">G</forename><surname>De Haan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Leest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physiological Measurement</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1913</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distanceppg: Robust non-contact vital signs monitoring using a camera</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Opt. Express</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1565" to="1588" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Amplitudeselective filtering for remote-ppg</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Brinker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stuijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>De Haan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Opt. Express</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1965" to="1980" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Continuous heart rate measurement from face: A robust rppg approach with distribution learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE IJCB</title>
		<meeting>IEEE IJCB</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="642" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A novel algorithm for remote photoplethysmography: Spatial subspace rotation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stuijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>De Haan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1974" to="1984" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning-based heart rate detection from remote photoplethysmography features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="4433" to="4437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deepphys: Video-based physiological measurement using convolutional attention networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcduff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="356" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Motion robust remote-ppg in infrared</title>
		<author>
			<persName><forename type="first">M</forename><surname>Van Gastel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stuijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>De Haan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1425" to="1433" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">RealSense = real heart rate: Illumination invariant heart rate estimation from videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE IPTA</title>
		<meeting>IEEE IPTA</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Eulerian video magnification for revealing subtle changes in the world</title>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Estimating heart rate and rhythm via 3D motion tracking in depth video</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stankovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1625" to="1636" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The impact of video compression on remote cardiac pulse measurement using imaging photoplethysmography</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Blackford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Estepp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE FG</title>
		<meeting>IEEE FG</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="63" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the benefits of alternative color spaces for noncontact heart rate measurements using standard red-green-blue cameras</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Tsouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Optics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">48002</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic engagement prediction with gap feature</title>
		<author>
			<persName><forename type="first">X</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM ICMI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="599" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">A</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Remote photoplethysmograph signal measurement from facial videos using spatio-temporal networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Motion robust remote photoplethysmography in cielab color space</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tsow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomed. Opt</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mean-variance loss for deep age estimation from a face</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5285" to="5294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep multi-task learning for joint prediction of heterogeneous face attributes</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="173" to="179" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improving 2d face recognition via discriminative face depth estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICB</title>
		<imprint>
			<biblScope unit="page" from="140" to="147" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m">Xuesong Niu received the B.E. degree from Nankai University and is pursing the Ph.D. degree from Institute of Computing Technology (ICT), Chinese Academy of Sciences (CAS)</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>His research interests include computer vision, machine learning and affective computing</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
