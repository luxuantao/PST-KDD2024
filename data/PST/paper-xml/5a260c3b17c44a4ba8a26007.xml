<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Load Value Prediction via Path-based Address Prediction: Avoiding Mispredictions due to Conflicting Stores</title>
				<funder>
					<orgName type="full">Qualcomm Technologies, Inc.</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rami</forename><surname>Sheikh</surname></persName>
							<email>ralsheik@qti.qualcomm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Qualcomm Technologies, Inc. ? Qualcomm Datacenter Technologies, Inc</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Qualcomm Technologies, Inc. ? Qualcomm Datacenter Technologies, Inc. {ralsheik</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Qualcomm Technologies, Inc. ? Qualcomm Datacenter Technologies, Inc</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Qualcomm Technologies, Inc. ? Qualcomm Datacenter Technologies, Inc. {ralsheik</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Harold</forename><forename type="middle">W</forename><surname>Cain</surname></persName>
							<email>tcain@qti.qualcomm.com</email>
						</author>
						<author>
							<persName><forename type="first">Raguram</forename><surname>Damodaran</surname></persName>
							<email>raguramd@qti.qualcomm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Qualcomm Technologies, Inc. ? Qualcomm Datacenter Technologies, Inc</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Qualcomm Technologies, Inc. ? Qualcomm Datacenter Technologies, Inc. {ralsheik</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Qualcomm Technologies, Inc. ? Qualcomm Datacenter Technologies, Inc</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Qualcomm Technologies, Inc. ? Qualcomm Datacenter Technologies, Inc. {ralsheik</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Load Value Prediction via Path-based Address Prediction: Avoiding Mispredictions due to Conflicting Stores</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3123939.3123951</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Microarchitecture</term>
					<term>Value Prediction</term>
					<term>Address Prediction</term>
					<term>Path-based Predictor</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current flagship processors excel at extracting instruction-levelparallelism (ILP) by forming large instruction windows. Even then, extracting ILP is inherently limited by true data dependencies. Value prediction was proposed to address this limitation. Many challenges face value prediction, in this work we focus on two of them. Challenge #1: store instructions change the values in memory, rendering the values in the value predictor stale, and resulting in value mispredictions and a retraining penalty. Challenge #2: value mispredictions trigger costly pipeline flushes. To minimize the number of pipeline flushes, value predictors employ stringent, yet necessary, high confidence requirements to guarantee high prediction accuracy. Such requirements can negatively impact training time and coverage.</p><p>In this work, we propose Decoupled Load Value Prediction (DLVP), a technique that targets the value prediction challenges for load instructions. DLVP mitigates the stale state caused by stores by replacing value prediction with memory address prediction. Then, it opportunistically probes the data cache to retrieve the value(s) corresponding to the predicted address(es) early enough so value prediction can take place. Since the values captured in the data cache mirror the current program data (except for in-flight stores), this addresses the first challenge. Regarding the second challenge, DLVP reduces pipeline flushes by using a new context-based address prediction scheme that leverages load-path history to deliver high address prediction accuracy (over 99%) with relaxed confidence requirements. We call this address prediction scheme Path-based Address Prediction (PAP). With a modest 8KB prediction table, DLVP improves performance by up to 71%, and 4.8% on average, without increasing the core energy consumption.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Single-thread (ST) performance is critical for both single-threaded and multi-threaded applications <ref type="bibr" target="#b14">[16]</ref>. Current flagship processors excel at extracting instruction-level-parallelism (ILP) by forming large instruction windows. Unfortunately, extracting ILP is inherently limited by true data dependencies. Value prediction was proposed to address this limitation <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b18">20]</ref>. By predicting the value(s) produced by an instruction (producer), instructions that consume the value(s) (consumers) can speculatively execute before the producer has executed. The prediction is later confirmed when the producer is executed. If the predicted value did not match the computed value, recovery actions take place. In this work we assume a flushbased recovery microarchitecture, similar to the work of Perais and Seznec <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b27">29]</ref>.</p><p>The recently proposed state-of-art value predictor VTAGE <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b27">29]</ref> addresses some of the key practical challenges facing value prediction. In this work, we focus on two of the remaining challenges:</p><p>(1) A value predictor's history tables attempt to capture the current program state. Store instructions can change the program state by modifying the values in memory. This change can render the values in the value predictor stale, resulting in value mispredictions and then retraining the predictor. To demonstrate the severity of this problem, we profiled the loadstore sequences in our workloads (discussed in Section 4.1).</p><p>Figure <ref type="figure">1</ref> shows the fraction of dynamic load instructions that exhibit the following sequences: (a) Load ! Store ! Load: two dynamic instances of the same static load read the same memory location with an interleaving store that modifies the value in that memory location.</p><p>Figure <ref type="figure">1</ref>: Fraction of dynamic loads that consume a value that is produced by a store that occurs since the prior dynamic instance of that load. X-axis shows different workloads.</p><p>(b) Load ! "In-flight" Store ! Load: similar to the earlier sequence except that by the time the latter load is fetched, the conflicting store is not committed yet. For the loads that belong to the first sequence, a conventional value predictor (e.g., Last-Value-Predictor, LVP <ref type="bibr" target="#b18">[20]</ref>) might mispredict the second load's value because the value has been changed by the interleaving store. The shaded region represents value mispredictions that can be avoided if the value prediction table can be pro-actively updated when stores commit. Unfortunately, doing so requires a complicated machinery for identifying which predictor entries are impacted by the committing stores. We refer the readers to the EXACT branch predictor <ref type="bibr" target="#b0">[1]</ref> in which a similar machinery is employed to pro-actively update the branch predictor on stores.</p><p>(2) Due to the high performance cost of value mispredictions (they trigger pipeline flushes), value predictors employ stringent, yet necessary, high confidence requirements to guarantee very high prediction accuracy. For example, VTAGE <ref type="bibr" target="#b26">[28]</ref> establishes confidence after encountering the value 64 or 128 times. The high confidence requirements negatively impact training time and coverage.</p><p>In this work, we target the value prediction challenges for one class of critical instructions: load instructions.</p><p>Given that data cache contents mirror the current program state, except for in-flight stores, we argue that by replacing value prediction with memory address prediction, we can potentially eliminate most of the negative interactions between stores and value prediction. As shown in Figure <ref type="figure">1</ref>, 67% of the load-store conflicts are between loads and previously committed stores. This can effectively mitigate Challenge #1.</p><p>Similar to early work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b13">13]</ref>, we observed that load memory addresses exhibit similar temporal locality properties as loaded values. This observation is illustrated in Figure <ref type="figure" target="#fig_0">2</ref>, which shows the breakdown of dynamic load instructions averaged across all of our workloads. The x-axis shows how often an address or value repeats, and the y-axis shows the fraction of load instructions. On average, not shown in the figure, loaded values are encountered 4% more than memory addresses. I.e., a particular load encounters the same address less often than it encounters the same value. This introduces an interesting challenge for address prediction. To deliver coverage equivalent to that of value prediction, address prediction needs to establish confidence sooner. To put it another way, an address predictor needs to establish confidence after encountering fewer address occurrences, compared to the number of value occurrences needed to establish confidence in a value predictor. Regarding Challenge #2, ensuring high prediction accuracy, we propose a new context-based address prediction scheme that leverages a unique context information that we refer to as load-path history, to enable high address prediction accuracy with less stringent confidence requirements. In contrast to branch-path history <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b35">37]</ref>, load-path history is less compact but allows the predictor to distinguish among multiple loads in the same basic block. We call the proposed address prediction scheme Path-based Address Prediction (PAP). Experimentally, we found that an address needs to be observed only 8 times to establish high confidence in PAP, as opposed to observing a value 64 or 128 times in VTAGE <ref type="bibr" target="#b26">[28]</ref>. Interestingly, by inspecting the corresponding points on Figure <ref type="figure" target="#fig_0">2</ref> (i.e., points 8 and 64 on the x-axis), we see that the fraction of loads with addresses that repeat 8 or more times is 91%, while the fraction of loads with values that repeat 64 or more times is 80%. This 11% difference suggests that PAP can potentially deliver higher coverage of load instructions. Our results, presented in Section 5, confirm this observation.</p><p>In this paper, we propose Decoupled Load Value Prediction (DLVP), a novel approach to value prediction through address prediction. DLVP works as follows: using PAP, we predict the load memory addresses early in the pipeline (in our studies we assume the first stage of fetch.) Then, we communicate the predicted memory addresses to the out-of-order engine (OoO), where they are used to speculatively probe the data cache. If the data is present in the cache, we retrieve it and use it to perform value prediction. If the data is not present in the cache, we can generate a prefetch request. We discuss the details of DLVP in Section 3.2.2.</p><p>This paper makes the following contributions:</p><p>? We identify ISA-specific efficiency and accuracy challenges with conventional value predictors, and propose a simple workaround to mitigate these challenges. ? We present comprehensive analysis of our solution and contrast it against prior art in value prediction (e.g., VTAGE <ref type="bibr" target="#b26">[28]</ref>) and address prediction (e.g., CAP <ref type="bibr" target="#b2">[3]</ref>).</p><p>The paper is organized as follows. In Section 2, we discuss related work in address prediction and value prediction. In Section 3, we present our address prediction scheme (PAP) and the microarchitecture that uses it for value prediction (DLVP). In Section 4, we discuss our methodology and describe our evaluation framework and baseline. In Section 5, we present a thorough evaluation of our solution. We conclude the paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Value Prediction</head><p>Since the introduction of value prediction <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b18">20]</ref>, there has been a plethora of work on this subject. In general, value predictors can be classified into two broad categories.</p><p>Computation-based Predictors. In this class of predictors, predicted values are generated by applying a function to the value(s) produced by previous instance(s) of the instruction. Stride predictors <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b12">12]</ref> are good examples of this class: the prediction is generated by adding a constant (stride) to the previous value.</p><p>Context-based Predictors. This class of predictors rely on identifying patterns in the history of a given static instruction to predict the value. Finite Context Method predictors (FCM) <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b31">33]</ref> are good examples of this class. Typically, such predictors use two structures: one captures the history for the instruction. This history is used to index the second structure, which captures the values.</p><p>Most, if not all, value predictors employ confidence mechanisms to ensure high prediction accuracy.</p><p>The recently proposed state-of-art value predictors VTAGE <ref type="bibr" target="#b26">[28]</ref> and D-VTAGE <ref type="bibr" target="#b27">[29]</ref> are context-based predictors. VTAGE uses several tagged prediction tables that are indexed using a hash of instruction PC and different number of bits from the global branch history (context). These tables are backed up by a PC indexed, tagless last-value predictor (LVP). D-VTAGE augments VTAGE with a lastvalue-table (LVT) that is located before the first VTAGE table (VT0). LVT stores the last value (per instruction), while the VTAGE tables store the strides/deltas. D-VTAGE introduces additional complexity as it requires an addition on the prediction critical path, moreover, it requires maintaining a speculative window to track in-flight last values.</p><p>In our evaluation, we use VTAGE as a representative of contextbased value predictors. We use the best performing VTAGE configuration we identified through extensive design space exploration: we did a full sweep of the following predictor dimensions: number of tables, table parameters (tag width, history length, and associativity), hash functions, and forward probabilistic counter (FPC) vector. Interestingly, we found that using tags with the LVP table is crucial for delivering the expected high prediction accuracy. A quantitative comparison against VTAGE is presented in Section 5.2.3. Arguably, VTAGE might indirectly workaround the conflicting store problem by using long branch history. If that is the case, then it is already included in our evaluation. We show that directly addressing the conflicting store problem through address prediction is a more optimal solution.</p><p>Our proposed value prediction solution differs from conventional value prediction in many ways:</p><p>Functionality. First, we do not directly predict the value, instead we predict the memory address. Second, predicted values are acquired by probing the data cache using the predicted memory address. In a way, the data cache acts as the data-store in our solution. This can potentially eliminate most of the negative interactions between stores and value prediction. Finally, by virtue of using address prediction, our solution is capable of generating highly accurate prefetches.</p><p>Storage efficiency. By virtue of predicting memory addresses (32-bit for ARMv7 or 49-bit for ARMv8) as opposed to 64-bit values, DLVP is more storage efficient in that respect. Moreover, some ISAs (e.g., ARM) support load instructions that have two or more destination registers. E.g., load-pair (LDP) has two destination registers, and load-multiple (LDM) has up to sixteen destination registers. With DLVP (i.e., address prediction), only one address needs to be predicted (the base address), all destination registers are loaded from relative or consecutive memory locations starting at the predicted address. Therefore, only one address predictor entry is required per instruction. Meanwhile, with value prediction, the value of each destination register needs to be predicted, requiring up to sixteen predictor entries (i.e., one value predictor entry per destination register). Overall, DLVP is more storage efficient.</p><p>Coverage. DLVP can predict loads only, while conventional value predictors can potentially predict all instructions (including loads). Therefore, conventional predictors can provide higher coverage. But, given that it is very common for non-load instructions to directly or indirectly depend on loads, this may indirectly expose them to the load-store vulnerability discussed in Section 1. Therefore, it is possible that the higher coverage achieved by predicting all instructions can translate into higher vulnerability. Additionally, our evaluation (in Section 5.2.2) confirms that given a modest predictor budget, conventional value predictors get most benefit when they target load instructions only <ref type="bibr" target="#b5">[5]</ref>.</p><p>Complexity. DLVP requires probing the data cache to retrieve the predicted values. This is a necessary evil to work around the negative interactions between stores and value prediction (discussed in Section 1). To retrieve the predicted values, conventional value predictors do not require accessing the data cache, but this makes them more vulnerable to the negative interaction between stores and value prediction. In our baseline, the L1 prefetcher generates prefetch requests that check the L1 cache before they are propagated down the memory hierarchy. Our work (DLVP) leverages the same path to probe the data cache. To help mitigate the energy cost incurred by speculatively probing the data cache, we augment DLVP with way prediction. Therefore, only one cache way is checked instead of the entire cache set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Address Prediction</head><p>Address predictors can also be categorized into: Computation-based Predictors <ref type="bibr" target="#b10">[10]</ref>, and Context-based Predictors <ref type="bibr" target="#b2">[3]</ref>. Address prediction has been mostly used for hiding the multi-cycle access latency of the memory hierarchy <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b32">34]</ref>, and to prefetch the data to be accessed by the next invocation in strided memory accesses <ref type="bibr" target="#b13">[13]</ref>. Similar to other prediction schemes, address predictors employ confidence mechanisms to ensure high prediction accuracy.</p><p>Correlated Address Predictor (CAP) <ref type="bibr" target="#b2">[3]</ref> is a context-based predictor that uses previous load memory addresses as context. A dedicated structure, called the Load Buffer table, is used to capture the memory address history per static load. Load's PC is used to probe the load buffer table, and the memory address history read is used to probe a second structure, called the Link table, which stores the predicted memory addresses. CAP is capable of capturing stride and non-stride memory addresses. Therefore, we believe it is a good representative of address predictors in the literature.</p><p>Like CAP, our proposed path-based address prediction scheme (PAP) is context-based. Unlike CAP, PAP uses global program context (load-path history) as opposed to history per-static load. Therefore, it does not require a dedicated table for storing context information. PAP captures context using a single history register, which simplifies the management of the predictor's speculative state: speculative history update and restoration on a misprediction. For instance, with PAP, we take a snapshot of the history register after each speculative history update. Upon recovery from a misprediction, we simply restore the snapshot associated with the value mispredicted load. Meanwhile, for other prediction schemes that use per static instruction history (e.g., CAP), the management of the predictor's speculatively updated history is more complicated: we take a snapshot of the per static instruction history after each speculative history update. Upon recovery from a misprediction, we walk the structure that stores the snapshots in reverse program order, and we restore the snapshots corresponding to squashed instructions. This is a serial process. In Section 5.1, we present a quantitative comparison of PAP against CAP predictor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Memory Dependence Prediction</head><p>Data dependences between instructions can be explicit (via registers) or implicit (via memory). Memory Dependence Predictors (MDPs) <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b22">24]</ref> predict dependencies between memory operations (e.g.,loads and stores). One common application of MDP is to prevent memory ordering violations: by predicting store-load dependencies, and then delaying the dependent load until it is safe to execute.</p><p>By speculatively reading the data cache using the predicted load memory address (well before the actual load can execute), DLVP can incur a value misprediction if an in-flight, conflicting store exists in the pipeline. This is a form of memory ordering violation. Our baseline MDP <ref type="bibr" target="#b16">[18]</ref> (described in section 4.2) can be potentially leveraged to detect and prevent such violations. Unfortunately, since the MDP is tightly coupled with the processor's back-end, it is not feasible to use it for this purpose. To workaround this issue, DLVP introduces a small (4-entry) table, called LSCD, shown in Figure <ref type="figure">3</ref> and described in section 3.2.2. LSCD acts as a simple, special-purpose MDP that prevents such violations.</p><p>Memory Renaming (MR) <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b40">41]</ref> is another application that leverages MDP to predict loads and stores that are dependent, and speculatively remove them from the DEF-store-load-USE chains containing them. I.e., it speculatively forwards the data from the store data producer (DEF) to the load value consumer (USE). Effectively, transforming these chains into DEF-USE chains.</p><p>MR can partially address the first challenge (described in Section 1), namely, the negative interaction between stores and value prediction. In particular, it can effectively detect the load-store conflict and predict the loaded value given that both instructions are in the pipeline. Per this definition, MR is profitable when the instructions of the DEF-store-load-USE chain co-exist in the pipeline. Meanwhile, DLVP targets the scenario in which the conflicting store has left the pipeline. As a matter of fact, LSCD is introduced to prevent predicting loads that depend on in-flight stores. Therefore, DLVP and MR are orthogonal and can complement one another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DLVP: DECOUPLED LOAD VALUE PREDICTION</head><p>In this section, we discuss our proposed address prediction scheme (PAP), and then we present DLVP, a microarchitecture that leverages PAP to perform value prediction. Figure <ref type="figure">3</ref> shows our processor pipeline. Throughout this section, we will reference this figure and highlight the changes required to support value prediction, in general, and DLVP, in particular.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PAP: Path-based Address Prediction</head><p>We propose a context-based address prediction scheme. We investigated using various context information (e.g., local or global branch history <ref type="bibr" target="#b35">[37]</ref>, branch-path history <ref type="bibr" target="#b24">[26]</ref> ...etc.) Our investigation revealed a new, unique context that showed strong correlation with load memory addresses. We call this context load-path history, and we call our proposed prediction scheme path-based address prediction (PAP).</p><p>Load-path history is constructed by shifting the least significant, non-zero bit from each load PC (i.e., bit-2, the third bit, because most instructions are 4 bytes) into a new load-path history register. This load-path history forms a global context of the path by which a current load was reached. By virtue of using global program context rather than per static instruction context, the management of the predictor's speculatively updated history register is simple (discussed in Section 2.2).</p><p>Next, we discuss prediction and training PAP predictor.</p><p>3.1.1 Prediction. We introduce a partially tagged, direct-mapped structure called the Address Prediction Table (APT). It resides in the processor front-end, inside the address prediction block in Figure <ref type="figure">3</ref>.</p><p>Table <ref type="table" target="#tab_2">1</ref> shows the fields of an APT entry:</p><p>? Tag field is 14 bits, and it is computed using an XOR of the low order bits of load PC and load-path history. ? Memory Address field is 32-bit or 49-bit, and it stores a predicted memory address. ? Confidence is a 2-bit FPC <ref type="bibr" target="#b29">[31]</ref>. An FPC is different than a conventional counter in that each forward transition is only triggered with a certain probability. We use the following probability vector in our design {1, 1 /2, 1 /4}. ? Size field is 2-bit, and it encodes the number of bytes to be read (e.g., 0 means 4 bytes, 1 means 8 bytes ..etc.) ? Cache Way is an optional field, it stores the cache way at which the cache block is expected to be present. We index the APT in the first stage of fetch using the fetch group address (FGA). Experimentally, we found that the FGA is a good proxy for the load PC. Therefore, in our design, FGA is used. For simplicity, we will refer to this proxy PC as load PC in the text.</p><p>APT is indexed and tagged using an XOR of the low order bits from the load PC and folded load-path history. On a tag mismatch (APT miss), no prediction happens. On a tag match (APT hit), we read the contents of the hitting entry. If the confidence counter is saturated, signifying the predictor is confident, we predict using the memory address field. Otherwise, the predictor is still training and no prediction is made.</p><p>Predicting Multiple Load Instructions. In our workloads, over 98% of the fetch groups contain at most two load instructions. In our design, we use load PC and load PC plus one (aka, fetch group PC and fetch group PC plus one) to predict up to two load addresses in any given cycle. For fetch groups that contain more than two load instructions (less than 2% of the fetch groups), only the first two load instructions are predicted. Training on an APT Miss. We considered two allocation policies. Policy-1, a new entry is always allocated and replaces the probed entry. Policy-2, a new entry is allocated if the confidence of the probed entry is zero. If the confidence of the probed entry is greater than zero, we decrement it.</p><p>Our experiments (not included due to limited space) show that Policy-2 is superior. This is expected since entries with high confidence can survive eviction. Over time, confident but less frequently accessed entries will get reallocated. We adopt Policy-2 in our design.</p><p>On allocation, the confidence is initialized to zero, and the remaining entry fields (tag, memory address, and cache way) are initialized with the executed load information.</p><p>Training on an APT Hit. If the predicted address matches the computed address, we increment the confidence of the hitting entry. Otherwise, we reset the confidence and reallocate the entry. Similarly, the allocated entry is initialized with the executed load information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Value Prediction</head><p>First, we discuss the hardware support needed for communicating predicted values from producers to consumers. Such hardware support is common to all value prediction schemes. Then, we discuss the proposed DLVP microarchitecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Value Prediction Engine.</head><p>In a conventional design, values are communicated between instructions through the physical register file (PRF). Producer instructions write computed values to the PRF (using write ports), after execution (Execute stage). Consumer instructions read their source values from the PRF (using read ports) after rename, at Register File Access stage. Our baseline, shown in Table <ref type="table" target="#tab_6">4</ref>, has a 4-wide in-order front-end and an 8-wide out-of-order execution engine. Therefore, the PRF has 8 read ports (2 read ports per instruction: one per source register), and 8 write ports (one per execution lane.)</p><p>With value prediction, a mechanism is needed to communicate the predicted values from the value-predicted producers to their consumers. Timely predicted values are usually available before Register File Access stage. Their consumers however can be any instruction(s) younger than the value-predicted producer. Therefore, predicted values need to be captured in anticipation of future consumption.</p><p>In order to leverage the existing value communication machinery (through the PRF) to communicate predicted values to their consumers, we envision two possible designs:</p><p>(1) Design #1: arbitrating on PRF write ports between executed and value-predicted instructions. This design requires minimal changes to the PRF and has little or no impact on its area and energy-per-access. The main disadvantage of this design is: if all execution lanes are heavily utilized, PRF write ports can become a bottleneck, disrupting the seamless flow of instructions and degrading performance. Such design may not be compelling for high performance cores. (2) Design #2: increasing number of PRF write ports to accommodate writing the predicted values. Since the PRF is a large, multi-ported structure, increasing the number of write ports can have significant implications on the area and energyper-access for the PRF, yielding this option unattractive as well <ref type="bibr" target="#b41">[42]</ref>.</p><p>Our profiling shows that, at any given point in time, a relatively small subset of the registers are value predicted. This is especially true since we only target load instructions in this work. We advocate for using a new, dedicated structure that captures the predicted values, called the Predicted Values Table (PVT). By virtue of using a dedicated structure, we no longer require the PRF write port arbitration (needed for design #1), and we do not increase the number of PRF write ports (needed for design #2).</p><p>PVT is simply a small cache for predicted values (only 32 entries): it uses the destination register identifier (i.e., physical register number) of the predicted instruction as a tag, and stores the predicted value in the payload. A PVT entry is allocated for each value-predicted register. If the PVT is full, a value prediction is treated as no prediction (i.e., no entry is allocated). In our evaluation, this scenario is almost never encountered. PVT entries are deallocated when the value predicted instructions execute and validate the predictions (now, the actual values can be retrieved from the PRF instead.)</p><p>In this work, we assume that up to two value predictions can be made every cycle. Therefore, we design the PVT with two write ports. To reduce the PVT read port requirements, we augment each rename-map-table (RMT) entry with one bit, called the predicted bit, that signals whether the corresponding register is value predicted or not: predicted values are read from PVT, and non-predicted values are read from PRF. Consequently, PVT will only be probed for predicted values. For our workloads, using a PVT with two read ports is sufficient. When the value predicted instruction executes, it writes the computed value to the PRF, it clears the corresponding predicted bit in the RMT (if the register mapping has not changed), and it deallocates the corresponding PVT entry. On a pipeline flush, we deallocate the PVT entries corresponding to squashed instructions, we restore the RMT from a checkpoint, and we reset all predicted bits. We call the machinery that manages the PVT, the Value Prediction Engine (VPE), shown in Figure <ref type="figure">3</ref>. A PVT-based design requires a MUX to allow PVT values to get selected instead of PRF values, which can add some delay to the critical path. We refer to this approach as: Design #3.</p><p>Using the area and energy model described in Section 4.2, we estimate the area, read energy, and write energy for the three designs. Table <ref type="table" target="#tab_3">2</ref> shows the normalized area and energy (assuming that 30% of the register values read/written are predicted.) Since the PVT is small, design #3 incurs a modest area increase compared to design #1. Additionally, PVT has fewer ports than PRF, and its read energy is smaller. This results in design #3 having: a lower read energy (by replacing some of the PRF reads with PVT reads), and a higher write energy (because predicted values are written to PVT). Each one of the three designs has its own merits, it is up to the designers to decide amongst the three design options. We adopt design #3.</p><p>In Figure <ref type="figure">3</ref>, we show Register File Access stage immediately after Rename stage. An alternative core design might have the Register File Access after Issue stage <ref type="bibr" target="#b19">[21]</ref>. The VPE operations described earlier work similarly in both designs, except that in the latter design, values read from PVT need to be captured with the instructions in Issue stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">DLVP Microarchitecture.</head><p>DLVP is a technique that performs value prediction through address prediction. It works as follows (refer to Figure <ref type="figure">3</ref>):</p><p>? Leverage the PAP address predictor to generate high accuracy predictions early in the pipeline (?). In our studies we assume that address prediction takes place in the first stage of fetch. ? Communicate the predicted memory addresses to the out-oforder engine (?), where they get deposited in a newly introduced FIFO queue, called Predicted Address Queue (PAQ). ? On load-store execution lane bubbles, opportunistically probe the data cache using the predicted memory addresses from PAQ (?, first cache access).</p><p>-On a cache hit (?), retrieve the predicted values from the cache data array and communicate them to the Value Prediction Engine, at rename stage. -On a cache miss, a prefetch request can be generated (?).</p><p>? Update the address predictor and confirm the value prediction, when the load instruction executes (?, second cache access).</p><p>-We always update the address predictor as described in Section 3.1.2. -If the predicted value does not match the loaded value, a value misprediction happened, we flush the pipeline. We assume a 1-cycle penalty for checking and confirming the correctness of the predicted value. This penalty is exposed only on a misprediction.</p><p>Next, we discuss important aspects specific to DLVP. Generating Timely Value Predictions.</p><p>In conventional value prediction, the predicted values are read from the prediction tables, and delivered to VPE, in a timely manner. With DLVP, the predicted values are read from the data cache. For a timely value prediction, the predicted value(s) must be delivered to the VPE by the time the predicted load reaches rename stage. This introduces an interesting challenge to DLVP: the timely delivery of predicted values to the VPE.</p><p>We propose to drop an PAQ entry after a fixed, predefined number of cycles (N) from its allocation. N corresponds to the guaranteed minimum number of cycles available for retrieving the values from the data cache before the load reaches Rename. Therefore, it's value is influenced by the pipeline depth, wiring delays, and cache access latency. In our model, the value of N is determined with the following latencies in mind: 1-cycle for load address prediction, 1-cycle for sending the request or data from the front-end to the back-end and vice versa, and 1-cycle for reading the data cache (facilitated by way prediction.) In a pipeline similar to Cortex-A72 <ref type="bibr" target="#b11">[11]</ref>, the Fetch and Decode stages take 5 and 3 cycles, respectively. Therefore, N equals 4. Pipeline stalls can allow for a larger N, but, for design simplicity we adopt this definition. Our evaluation shows less than 0.1% of the PAQ entries are dropped. Note that in our design, a request can bypass the PAQ if empty.</p><p>Avoiding Value Mispredictions Due to Conflicts with In-flight Stores.</p><p>By virtue of predicting memory addresses, not values, DLVP eliminates most of the negative interactions between stores and value prediction. As shown in Figure <ref type="figure">1</ref>, two thirds of the loadstore conflicts are eliminated in our workloads. The only exception is the older in-flight stores that modify the memory content read speculatively by DLVP. Under these circumstances, even a correctly predicted memory address can result in an incorrect value prediction.</p><p>To prevent such stores from hurting DLVP, we augment our address predictor with a 4-entry filter called the Load-Store Conflict Detector (LSCD). LSCD is used to capture the PCs of address predicted loads that suffered conflicts with older, in-flight stores. A load PC is inserted into the filter when its address is predicted correctly but the predicted value was incorrect. This signifies that an in-flight store updated the content of the memory location after the value prediction was made. LSCD prevents future instances of the captured loads from getting predicted or updating the prediction table. The entries allocated for these loads will be naturally evicted from the APT as new entries get allocated. This optimization eliminates the value mispredictions that are caused by in-flight, conflicting stores. An alternative to LSCD is to leverage the existing memory dependence predictor (MDP) <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b16">18]</ref> to detect and prevent the conflicts. In our baseline, the MDP is tightly coupled with the processor's back-end, therefore, it is not feasible to use it for this purpose. LSCD functions as a simple, special-purpose MDP.</p><p>Power Optimization.</p><p>Due to the non-negligible energy cost of probing the data cache speculatively (using predicted memory addresses), we propose augmenting our address predictor with cache way prediction, shown in Table <ref type="table" target="#tab_2">1</ref>. The width of the cache way field is log2(cache-associativity) bits. This optimization can help mitigate the aforementioned energy cost. A way misprediction is possible, and it can happen if the cache block is evicted and then reinserted into the cache, but at a different cache way. Our evaluation shows way mispredictions almost never happen.</p><p>Memory Consistency. ARM's relaxed consistency model allows for reordering most memory operations with one exception: dependent loads are not allowed to be reordered <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b25">27]</ref>. Value prediction can violate this rule. To avoid violating the memory consistency model, we employ a technique similar to the work of Martin et al. <ref type="bibr" target="#b20">[22]</ref>. Also, address prediction is not used with memory ordering instructions, atomic and exclusive memory accesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY AND EVALUATION ENVIRONMENT 4.1 Methodology</head><p>We cast a wide net to expose as many load address and value occurrence patterns as possible. We use benchmarks from the following benchmark suites: SPEC2K <ref type="bibr" target="#b36">[38]</ref>, SPEC2K6 <ref type="bibr" target="#b37">[39]</ref>, and EEMBC <ref type="bibr" target="#b28">[30]</ref>. Moreover, we enriched our benchmark pool with other popular applications: Linpack <ref type="bibr" target="#b17">[19]</ref>, media player <ref type="bibr" target="#b21">[23]</ref>, browser benchmark <ref type="bibr" target="#b3">[4]</ref>, and various Javascript benchmarks <ref type="bibr" target="#b8">[8,</ref><ref type="bibr">14,</ref><ref type="bibr">15,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b38">40]</ref>.</p><p>Table <ref type="table" target="#tab_5">3</ref> shows a list of our benchmarks. We use 100-million instruction simpoints, except for short-running benchmarks (i.e., EEMBC), we simulate the first 100 million instructions, or until the benchmark completes.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Environment</head><p>The microarchitecture presented in Section 3 is faithfully modeled in our internally-developed, cycle-accurate, RTL-validated, industry simulator. The simulator runs ARM ISA binaries (v7 and v8). It is used by our CPU research and development organization. Because the simulator is specific to our proprietary custom ARM CPU design, we don't release it, and there is nothing publicly available that we can cite. Similarly, we use an in-house, RTL-PTPX validated area and energy model. We assume 28nm technology. Section 4.1 lists the benchmark suites used. All benchmarks are compiled to the ARM ISA using gcc with -O3 level optimization: SPEC2K and SPEC2K6 are compiled for ARMv8 (aarch64), and the remaining benchmarks are compiled for ARMv7.</p><p>The parameters of our baseline core are configured as close as possible to those of Intel's Skylake core <ref type="bibr" target="#b19">[21]</ref>. The baseline core uses state-of-art TAGE and ITTAGE branch predictors <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b35">37]</ref>, and an MDP similar to Alpha 21264 <ref type="bibr" target="#b16">[18]</ref>. We use a fetch-to-execute latency of 13 cycles. Table <ref type="table" target="#tab_6">4</ref> shows the baseline core configuration.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS AND ANALYSIS</head><p>In this section, we start by demonstrating the strengths of our PAP address predictor (i.e., its superior coverage and accuracy), by comparing it to related prior work, the CAP address predictor <ref type="bibr" target="#b2">[3]</ref>. Then, we transition into value prediction and present a detailed evaluation of our proposed technique, DLVP. In an effort to compare DLVP to prior art in value prediction, we implemented and evaluated state-of-art value predictor VTAGE <ref type="bibr" target="#b26">[28]</ref>. Our evaluation uncovered ISA-specific efficiency and accuracy challenges with conventional value predictors (including VTAGE) that prevent them from delivering their full potential. We propose a simple workaround to mitigate these challenges. After that, we compare DLVP to two value predictors: VTAGE, and CAP (this one is simply, DLVP but uses CAP address predictor instead of PAP). Finally, we evaluate the performance impact of using replay instead of pipeline flush as a misprediction recovery mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Address Prediction</head><p>In this section, we quantitatively compare PAP and CAP <ref type="bibr" target="#b2">[3]</ref> address predictors. A qualitative comparison between the two predictors is presented in Section 2.2. It is worth noting that as the accuracy and coverage of address prediction improve, the benefits of DLVP improve accordingly.</p><p>Through design space exploration, we identified that a confidence of 8 suffices for PAP to deliver high prediction accuracy (greater than 99%.) In <ref type="bibr" target="#b2">[3]</ref>, CAP used a confidence of 3. Figure <ref type="figure" target="#fig_3">4</ref> shows the coverage and accuracy of PAP and CAP predictors when used as standalone address predictors: PAP is evaluated with a confidence of 8, and CAP is evaluated with multiple confidence levels: 3 through 64.</p><p>When the same confidence requirements are used (i.e., Confidence = 8), data shows a clear advantage for PAP over CAP, in terms of both: coverage (37% vs. 29.5%) and accuracy (99.1% vs. 97.7%) 1 . 1 Coverage is defined as the number of predicted dynamic loads divided by the number of dynamic loads, and accuracy is defined as the number of correctly predicted dynamic loads divided by the number of predicted dynamic loads.</p><p>CAP accuracy improves as we increase the confidence level, but that comes with a price: a non-negligible reduction in coverage. Interestingly, for CAP to deliver an accuracy on par with PAP, it needs to use a confidence of 64. At that design point, CAP's coverage is reduced to 24%. This clearly demonstrates the high accuracy and low confidence requirements of PAP predictor.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Value Prediction</head><p>In this section, we present: a detailed evaluation of DLVP, an analysis of state-of-art VTAGE predictor, a comparison between VTAGE, CAP and DLVP, and the integration of DLVP and VTAGE. Finally, we evaluate the benefits of DLVP, CAP and VTAGE when replay is used as the misprediction recovery mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">DLVP.</head><p>Figure <ref type="figure" target="#fig_5">6a</ref> shows the speedup of DLVP (amongst other schemes that we discuss in subsequent sections) on our workloads. DLVP improves 45 out of the 78 workloads by more than 1%, with maximum speedup of 71% on perlbmk, and an average speedup of 4.8%.</p><p>By virtue of predicting memory addresses, unlike conventional value predictors, DLVP is capable of generating high accuracy prefetch requests. We evaluate the impact of enabling or disabling this feature of DLVP in Figure <ref type="figure" target="#fig_4">5</ref>. Bars show speedup on a subset of the benchmarks along with the average across all benchmarks. The curve shows the fraction of loads for which DLVP generated a prefetch. Observe that the fraction of loads prefetched by DLVP is quite low (less than 1.1% for h264ref, and 0.3% on average), which explains the small average performance improvement when this feature is enabled (only 0.1%.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">VTAGE Implementation.</head><p>We evaluated state-of-art, contextbased value predictor VTAGE, on our workloads. Our initial evaluation showed speedups and accuracies far lower than what is reported by Perais and Seznec <ref type="bibr" target="#b26">[28]</ref>. We conducted a thorough investigation to uncover the reasons behind this. Our investigation revealed the following interesting interaction between conventional value predictors (including VTAGE) and the ISA. Recall that our benchmarks are compiled for the ARM ISA, while in <ref type="bibr" target="#b26">[28]</ref> x86 binaries are used.</p><p>The ARM ISA supports many load instructions that have multiple destination registers: for example, load-pair instructions (LDP) have two destination registers, and load-multiple instructions (LDM) can load any subset of the 16 general-purpose registers. With conventional value prediction, the value of each destination register needs to be predicted, requiring up to sixteen predictor entries in the case of LDM (i.e., one predictor entry per destination register). Moreover, vector-load instructions (VLD) load a 128-bit value, which typically gets stored into two 64-bit chunks in the value predictor (consuming two entries.) This exposes a storage inefficiency issue with conventional value predictors.</p><p>Our experiments with VTAGE predictor show suboptimal accuracy and coverage for the aforementioned types of load instructions, namely: LDP, LDM, and VLD. We adjusted VTAGE so it can handle such loads gracefully, by concatenating the number of destination registers to the PC (which is then hashed with history). E.g., if a load has 2 destination registers, the load PC is concatenated with a 0 (to predict the first destination register) and a 1 (to predict the second destination register). Unfortunately, that did not eliminate the destructive aliasing problem introduced by the significant increase in prediction table pressure. The bottom line is: mispredicting any of the many predicted values will trigger a pipeline flush.</p><p>We evaluate three flavors of VTAGE:</p><p>(1) Vanilla VTAGE: unmodified VTAGE, as described in <ref type="bibr" target="#b26">[28]</ref>.</p><p>(2) VTAGE augmented with a dynamic opcode filter: vanilla VTAGE augmented with a table that tracks the prediction accuracy for the different instructions types. Instruction types that exhibit low prediction accuracy (less than 95%) are prevented from getting predicted or updating the prediction tables.</p><p>(3) VTAGE augmented with a static opcode filter: vanilla VTAGE augmented with a static filter that is preloaded with the instruction types that exhibit low prediction accuracy, namely: LDP, LDM, and VLD. No filter training is required in this case. We considered using VTAGE to predict load instructions only, or all instructions. Figure <ref type="figure" target="#fig_6">7</ref> shows the speedup, coverage and accuracy of the three VTAGE designs. The results clearly show two things. First, the performance of Vanilla VTAGE improves significantly when a dynamic/static filter is used. Interestingly, using a static filter beats using a dynamic filter. This is due to the cost of training the dynamic filter as it takes time (and mispredictions) to detect the load types that exhibit low prediction accuracy. Second, predicting only load instructions is more rewarding than predicting all instructions, especially when the predictor budget is modest (e.g., 8KB in our evaluation.) The criticality of the predicted instructions comes into play. Load instructions are widely known to be very critical. Therefore, it is not a surprise that under these circumstances (i.e., modest budget), predicting only load instructions is superior to predicting all instructions.</p><p>In all of our subsequent evaluations, we use VTAGE with a static filter to predict load instructions only. <ref type="figure" target="#fig_5">6a</ref> and<ref type="figure" target="#fig_5">6b</ref> show the speedup and coverage of the three value predictors: VTAGE, DLVP (uses our proposed PAP predictor), and CAP (just like DLVP except CAP address predictor is used). We swept the confidence level parameter for CAP, found that a confidence of 24 delivers the best average speedup. Therefore, in our evaluation, we use CAP with confidence of 24. For some workloads, CAP delivers a relatively low prediction accuracy (in the low 90s) resulting in some performance degradation. Despite these degradations, CAP delivers an average speedup of 2.3%. Compared to VTAGE, DLVP delivers a higher coverage (31.1% vs. 29.6%), and a significantly higher speedup (4.8% vs. 2.1%). Interestingly, the figure shows benchmarks that favor VTAGE and others that favor DLVP. E.g., aifirf favors DLVP, while nat favors VTAGE. Observe that DLVP's coverage (31.1%) is lower than PAP's coverage (37%, shown in Figure <ref type="figure" target="#fig_3">4</ref>) due to LSCD filtering out loads the did conflict with older, in-flight stores. The same observation applies to CAP, where coverage went from 27% to 23.8%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Comparing value predictors. Figures</head><p>Perlbmk presents an interesting case in which the criticality of the predicted values influence the observed speedup. Despite DLVP's lower coverage compared to VTAGE, the speedup of DLVP is much higher due to the positive interaction between value prediction and branch prediction. In DLVP, the value predicted loads facilitate the early resolution of mispredicted branches, thus lowering the branch misprediction cost and magnifying the benefits of value prediction.</p><p>Since DLVP predicted loads probe the L1 data cache twice, it's energy consumption increases. The speedups achieved by DLVP allow the core to complete execution sooner, and can reduce the total core energy. Figure <ref type="figure" target="#fig_5">6c</ref> shows the total core energy (includes L1 cache and prediction tables) normalized to our baseline (with no value prediction). The data shows that DLVP's speedup more than offset the energy increase due to additional core activity. Interestingly, the average core energy is on par with that of VTAGE, in which no extra cache activity is needed. Figure <ref type="figure" target="#fig_5">6d</ref> shows the area, read and write energy of the three predictors, normalized to PAP predictor. Figure <ref type="figure" target="#fig_8">9</ref> shows the speedup and coverage of VTAGE and DLVP for a subset of the benchmarks (namely: bzip2, pdfjs, gcc, soplex, and avmshell). These benchmarks show an interesting property: the speedup does not seem to correlate with coverage. Upon a closer look, we uncovered the following:</p><p>In bzip, DLVP suffers a higher TLB miss rate. This is caused by a second order effect induced by probing the data cache twice for address predicted loads (one time using the predicted address and another time using the computed address). The opposite scenario happens in avmshell, where VTAGE suffers a higher TLB miss rate. In this case, probing the data cache two times (in DLVP) results in better TLB behavior. In pdfjs, the accuracy of VTAGE (100%) is higher than DLVP (99.7%). The opposite happens in gcc and soplex, DLVP accuracy (99.9%) is higher than VTAGE (99.3%).</p><p>Combining VTAGE and DLVP.</p><p>We evaluate integrating DLVP and VTAGE as tournament predictors: both predictors run concurrently, and a chooser table decides which predictor makes the final prediction. The chooser is PC indexed, and uses 2-bit counters to track which predictor performs better. Figure <ref type="figure" target="#fig_7">8a</ref> shows the average speedup and coverage when each predictor is used alone and when combined (tournament predictor). The small increase in coverage, when combining the predictors, clearly show that there is a significant overlap between the loads captured by both schemes. Perhaps a more intelligent chooser can reduce this overlap by partitioning the loads amongst the two predictors. This is an interesting observation and warrants further investigation. Figure <ref type="figure" target="#fig_7">8b</ref> shows the breakdown of predicted loads, with respect to the predictor that made the final prediction. On average, DLVP delivers more predictions (18.2%) than VTAGE (16.1%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Value Misprediction</head><p>Recovery. Due to the complexity of replay-based value misprediction recovery schemes, we assumed a flush-based recovery microarchitecture, similar to the work of Perais and Seznec <ref type="bibr" target="#b26">[28]</ref>.</p><p>In this section, we entertain the thought of using replay as the recovery mechanism. Instead of throwing away all instructions following a value mispredicted load, one can replay only the load dependents. This change has implications on some design aspects. E.g., consumers of predicted values can no longer leave the instruction queue because they have the potential to replay. This could take away from the benefits of value prediction.</p><p>We approximate an oracle replay mechanism in our model, by simply treating mispredictions as no-predictions. i.e., we use oracle information to treat value mispredictions as if the load was never predicted in the first place. Figure <ref type="figure" target="#fig_9">10</ref> shows the speedup of CAP, DLVP and VTAGE with flush and oracle replay as the recovery mechanism. Observe that CAP predictor's performance improves significantly, from 2.3% to 4.2%, with oracle replay. This is expected, because as we stated in Section 5.2.3, CAP delivered relatively low prediction accuracy for some workloads: replacing excessive flushes with less costly replays helps. Meanwhile, the speedup of VTAGE and DLVP predictors improve by 0.7% and 0.8%, respectively. This is expected as well, because both predictors have very high prediction accuracy across all workloads (greater than 99%), and therefore, rarely flush or replay. Note that if we lessen the accuracy requirements for VTAGE and DLVP, their coverage is likely to increase, and their performance benefits, in a replay-based design, can improve. To truly harvest the benefits of replay as a recovery mechanism, one can trade accuracy for higher coverage, and then, identify the sweet spot at which maximum performance can be achieved. We leave this exercise as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we address some of the challenges facing value prediction, namely: the negative interaction between stores and value prediction, and the high cost of value mispredictions which forces value predictors to employ stringent, yet necessary, high confidence requirements.</p><p>We proposed Decoupled Load Value Prediction (DLVP), a microarchitecture that targets the challenges of value prediction for load instructions. The basic idea is to replace value prediction with memory address prediction then, rely on the data cache to deliver the predicted values early enough so value prediction can take place. Since the values captured in the data cache mirror the current program state (except for in-flight stores), this can eliminate most of the negative interactions between stores and value prediction. Moreover, we proposed Path-based Address Prediction (PAP), a new context-based address prediction scheme that leverages load-path history to enable high address prediction accuracy with less stringent confidence requirements.</p><p>We demonstrate the benefits of DLVP using a wide range of workloads, and show how it compares favorably to other address and value prediction schemes. DLVP delivers an average speedup of 4.8%, which is more than twice the speedup of state-of-art value predictor VTAGE, without increasing the core energy consumption.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Breakdown of dynamic load instructions according to the repeatability of observed addresses or values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>3.1.2 Training. APT is trained when a load executes. Training takes place for both scenarios: APT hit, and APT miss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>-of-art 32KB TAGE predictor and 32KB ITTAGE predictor RAS: 16 entries Memory Hierarchy Block size: 64B (L1), 128B (L2 and L3) L1: split, 64KB each, 4-way set-associative, 1-cycle/2-cycle (I/D) access latency L2: unified, private, 512KB, 8-way set-associative, 16-cycle access latency L3: unified, shared, 8MB, 16-way set-associative, 32-cycle access latency Memory: 200-cycle access latency Stride-based prefetchers TLB 512-entry, 8-way set-associative Fetch through Rename Width 4 instr./cycle Issue through Commit Width 8 instr./cycle (8 execution lanes: 2 support load-store operations, and 6 generic) ROB/IQ/LDQ/STQ 224/97/72/56 (modeled after Intel Skylake) Fetch-to-Execute Latency 13-cycle Physical RF 348 DLVP 1k-entry, direct-mapped, use 16-bit load-path history. Total budget = 1k x (50 or 67) = 50k bits (ARMv7) or 67k bits (ARMv8) 32-entry predicted address queue (PAQ) CAP 2 tables, 1k-entry each, direct-mapped. Each load buffer entry uses 14-bit tag, 2bit confidence, 8-bit offset, and 16-bit history. Each link entry uses 14-bit tag and 24-bit or 41-bit link. Total budget = 78k bits (ARMv7) or 95k bits (ARMv8) 32-entry predicted address queue (PAQ) VTAGE 3 tables, 256-entry each, direct-mapped, use global branch histories of {0, 5, 13}. Each entry uses 16-bit tag, 64-bit value and 3-bit confidence. Total budget = 3 x 256 x 83 = 62.3k bits</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Address prediction accuracy and coverage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Benefits of DLVP generated prefetches.</figDesc><graphic url="image-41.png" coords="8,355.88,350.90,166.50,85.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparison of the three prediction schemes: CAP, VTAGE, and DLVP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: VTAGE evaluation: y-axis shows average speedup across our workloads, x-axis shows the evaluated configurations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Combining DLVP and VTAGE.</figDesc><graphic url="image-64.png" coords="11,207.47,108.94,328.10,91.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Speedup and coverage of DLVP and VTAGE on selected benchmarks that show no correlation between the two metrics.</figDesc><graphic url="image-71.png" coords="11,92.86,366.35,168.00,102.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Average speedup across our workloads for two misprediction recovery schemes: flush and oracle replay.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>?</head><label></label><figDesc>We propose PAP, a new address prediction scheme that is capable of delivering high accuracy predictions with relaxed confidence requirements.</figDesc><table /><note><p>? We propose DLVP, a microarchitecture that leverages PAP to perform data value prediction and prefetching. With a modest 8KB prediction table, DLVP improves performance by up to 71%, and 4.8% on average, without increasing the core energy consumption.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table</head><label></label><figDesc>Figure 3: Pipeline with support for value prediction and DLVP.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Flush on Value</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Misprediction</cell></row><row><cell>Load-Store</cell><cell cols="2">Fetch (multi-cycle)</cell><cell></cell><cell>Decode (multi-cycle)</cell><cell>Rename</cell><cell>Register File Access</cell><cell cols="2">Allocate</cell><cell>Issue</cell><cell>Execute Data</cell><cell>Commit</cell></row><row><cell cols="7">Address Prediction Address Prediction LSCD Values Tag Value Prediction Engine (VPE) Value Prediction Conflict Detector Engine (VPE) Fetch Group Address (a proxy for load PC) If hit and confident pred ? ? ? Predicted PVT Memory Address Confidence Size Cache Way (Optional)</cell><cell>?</cell><cell cols="2">Cache On pipe On a hit, return predicted value Predicted Address Queue (PAQ) bubbles ?</cell><cell>(Optional) On a miss, generate a prefetch Components needed ? by any value predictor Components specific to DLVP</cell></row><row><cell cols="2">14 bits 32 bits (for ARMv7), or</cell><cell>2 bits</cell><cell>2 bits</cell><cell>2 bits</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>49 bits (for ARMv8)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Fields of our address predictor entry.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Area and energy normalized to design #1.</figDesc><table><row><cell></cell><cell>PVT</cell><cell>Design #1</cell><cell>Design #2</cell><cell>Design #3</cell></row><row><cell></cell><cell cols="4">(2rd/2wr ports) (PRF with 8rd/8wr ports) (PRF with 8rd/10wr ports) (Design #1 plus PVT)</cell></row><row><cell>Area</cell><cell>0.06</cell><cell>1.00</cell><cell>1.16</cell><cell>1.06</cell></row><row><cell>Read energy</cell><cell>0.10</cell><cell>1.00</cell><cell>1.10</cell><cell>0.80</cell></row><row><cell>Write energy</cell><cell>0.07</cell><cell>1.00</cell><cell>1.51</cell><cell>1.07</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Applications used in our evaluation.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Baseline core configuration.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">ACKNOWLEDGEMENTS</head><p>We thank <rs type="person">Arthur Perais</rs>, <rs type="person">Mikko Lipasti</rs>, <rs type="person">Derek Hower</rs>, <rs type="person">Eric Rotenberg</rs>, <rs type="person">Anil Krishna</rs>, <rs type="person">Vignyan Reddy</rs>, <rs type="person">David Palframan</rs>, and the anonymous reviewers for comments that greatly improved the manuscript. This research was supported by <rs type="funder">Qualcomm Technologies, Inc.</rs> Any opinions, findings, and conclusions or recommendations expressed herein are those of the authors and do not necessarily reflect the views of Qualcomm Technologies, Inc.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">EXACT: Explicit Dynamic-branch Prediction with Active Updates</title>
		<author>
			<persName><forename type="first">Muawya</forename><surname>Al-Otoom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elliott</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Rotenberg</surname></persName>
		</author>
		<idno type="DOI">10.1145/1787275.1787321</idno>
		<ptr target="https://doi.org/10.1145/1787275.1787321" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM International Conference on Computing Frontiers (CF &apos;10)</title>
		<meeting>the 7th ACM International Conference on Computing Frontiers (CF &apos;10)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="165" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Zero-cycle loads: microarchitecture support for reducing load latency</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO.1995.476815</idno>
		<ptr target="https://doi.org/10.1109/MICRO.1995.476815" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual International Symposium on</title>
		<meeting>the 28th Annual International Symposium on</meeting>
		<imprint>
			<date type="published" when="1995">1995. 1995</date>
			<biblScope unit="page" from="82" to="92" />
		</imprint>
	</monogr>
	<note>Microarchitecture</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Correlated load-address predictors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bekerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jourdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ronen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kirshenboim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rappoport</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yoaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Weiser</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA.1999.765939</idno>
		<ptr target="https://doi.org/10.1109/ISCA.1999.765939" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Symposium on</title>
		<meeting>the 26th International Symposium on</meeting>
		<imprint>
			<date type="published" when="1999">1999. 1999</date>
			<biblScope unit="page" from="54" to="63" />
		</imprint>
	</monogr>
	<note>Computer Architecture</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Browser benchmark</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><surname>Browsermark</surname></persName>
		</author>
		<ptr target="http://web.basemark.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Selective Value Prediction</title>
		<author>
			<persName><forename type="first">Brad</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glenn</forename><surname>Reinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Symposium on Computer Architecture (ISCA &apos;99)</title>
		<meeting>the 26th Annual International Symposium on Computer Architecture (ISCA &apos;99)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="64" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reasoning About the ARM Weakly Consistent Memory Model</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samin</forename><surname>Ishtiaq</surname></persName>
		</author>
		<idno type="DOI">10.1145/1353522.1353528</idno>
		<ptr target="https://doi.org/10.1145/1353522.1353528" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGPLAN Workshop on Memory Systems Performance and Correctness: Held in Conjunction with the Thirteenth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS &apos;08) (MSPC &apos;08)</title>
		<meeting>the 2008 ACM SIGPLAN Workshop on Memory Systems Performance and Correctness: Held in Conjunction with the Thirteenth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS &apos;08) (MSPC &apos;08)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="16" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Memory dependence prediction using store sets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Z</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA.1998.694770</idno>
		<ptr target="https://doi.org/10.1109/ISCA.1998.694770" />
	</analytic>
	<monogr>
		<title level="m">Proceedings. 25th Annual International Symposium on Computer Architecture</title>
		<meeting>25th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="142" to="153" />
		</imprint>
	</monogr>
	<note>Cat. No.98CB36235</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Javascript Performance Testing</title>
		<author>
			<persName><surname>Dromaeo</surname></persName>
		</author>
		<ptr target="http://dromaeo.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A load-instruction unit for pipelined processors</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Eickemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vassiliadis</surname></persName>
		</author>
		<idno type="DOI">10.1147/rd.374.0547</idno>
		<ptr target="https://doi.org/10.1147/rd.374.0547" />
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="547" to="564" />
			<date type="published" when="1993-07">1993. July 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A load-instruction unit for pipelined processors</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Eickemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vassiliadis</surname></persName>
		</author>
		<idno type="DOI">10.1147/rd.374.0547</idno>
		<ptr target="https://doi.org/10.1147/rd.374.0547" />
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="547" to="564" />
			<date type="published" when="1993-07">1993. July 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Ian</forename><surname>Forsyth</surname></persName>
		</author>
		<title level="m">The ARM Cortex-A72 processor: Delivering high efficiency for Server, Networking and HPC</title>
		<meeting><address><addrLine>Londom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Presented at ARM TechDay</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Speculative Execution based on Value Prediction</title>
		<author>
			<persName><forename type="first">Freddy</forename><surname>Gabbay</surname></persName>
		</author>
		<idno>. EE Department TR 1080</idno>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Technion -Israel Institue of Technology</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Speculative Execution via Address Prediction and Data Prefetching</title>
		<author>
			<persName><forename type="first">Jos?</forename><surname>Gonz?lez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Gonz?lez</surname></persName>
		</author>
		<idno type="DOI">10.1145/263580.263631</idno>
		<ptr target="https://doi.org/10.1145/263580.263631" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Supercomputing (ICS &apos;97)</title>
		<meeting>the 11th International Conference on Supercomputing (ICS &apos;97)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="196" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Amdahl&apos;s Law in the Multicore Era</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Marty</surname></persName>
		</author>
		<idno type="DOI">10.1109/MC.2008.209</idno>
		<ptr target="https://doi.org/10.1109/MC.2008.209" />
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="33" to="38" />
			<date type="published" when="2008-07">2008. July 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><surname>Benchmark</surname></persName>
		</author>
		<ptr target="http://ibench.sourceforge.net" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Alpha 21264 microprocessor</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kessler</surname></persName>
		</author>
		<idno type="DOI">10.1109/40.755465</idno>
		<ptr target="https://doi.org/10.1109/40.755465" />
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="24" to="36" />
			<date type="published" when="1999-03">1999. Mar 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Linpack</forename><surname>Benchmark</surname></persName>
		</author>
		<ptr target="http://www.netlib.org/benchmark/hpl" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Value Locality and Load Value Prediction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mikko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">B</forename><surname>Lipasti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">Paul</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1145/248209.237173</idno>
		<ptr target="https://doi.org/10.1145/248209.237173" />
	</analytic>
	<monogr>
		<title level="j">SIGPLAN Not</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="138" to="147" />
			<date type="published" when="1996-09">1996. Sept. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Technology Insight: Intel&apos;s Next Generation Microarchitecture Code Name Skylake. Presented at Intel Developer Forum</title>
		<author>
			<persName><forename type="first">Julius</forename><surname>Mandelblat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<pubPlace>San Francisco</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Correctly implementing value prediction in microprocessors that support multithreading or multiprocessing</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M K</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sorin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Cain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Lipasti</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO.2001.991130</idno>
		<ptr target="https://doi.org/10.1109/MICRO.2001.991130" />
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="328" to="337" />
		</imprint>
	</monogr>
	<note>In Microarchitecture, 2001. MICRO-34. Proceedings. 34th ACM</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<ptr target="http://mplayerhq.hu/design7/dload.html" />
		<title level="m">Media Player Benchmark</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic Speculation and Synchronization of Data Dependences</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Moshovos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Breach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Vijaykumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gurindar</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
		<idno type="DOI">10.1145/264107.264189</idno>
		<ptr target="https://doi.org/10.1145/264107.264189" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual International Symposium on Computer Architecture (ISCA &apos;97)</title>
		<meeting>the 24th Annual International Symposium on Computer Architecture (ISCA &apos;97)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="181" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Streamlining inter-operation memory communication via data dependence prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO.1997.645814</idno>
		<ptr target="https://doi.org/10.1109/MICRO.1997.645814" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 30th Annual International Symposium on Microarchitecture</title>
		<meeting>30th Annual International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="235" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic Path-based Branch Correlation</title>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Nair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual International Symposium on Microarchitecture (MICRO 28)</title>
		<meeting>the 28th Annual International Symposium on Microarchitecture (MICRO 28)<address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="15" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Memory Barriers: a Hardware View for Software Hackers</title>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">E</forename><surname>Mckenney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linux Technology Center</title>
		<imprint>
			<publisher>IBM Beaverton</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Practical data value speculation for future high-end processors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Perais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCA.2014.6835952</idno>
		<ptr target="https://doi.org/10.1109/HPCA.2014.6835952" />
	</analytic>
	<monogr>
		<title level="m">High Performance Computer Architecture (HPCA), 2014 IEEE 20th International Symposium on</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="428" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">BeBoP: A cost effective predictor infrastructure for superscalar value prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Perais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCA.2015.7056018</idno>
		<ptr target="https://doi.org/10.1109/HPCA.2015.7056018" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="13" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Benchmark Characterization of the EEMBC Benchmark Suite</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Poovey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Conte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gal-On</surname></persName>
		</author>
		<idno type="DOI">10.1109/MM.2009.74</idno>
		<ptr target="https://doi.org/10.1109/MM.2009.74" />
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="18" to="29" />
			<date type="published" when="2009-09">2009. Sept 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Probabilistic counter updates for predictor hysteresis and stratification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zilles</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCA.2006.1598118</idno>
		<ptr target="https://doi.org/10.1109/HPCA.2006.1598118" />
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Symposium on. 110-120</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
	<note>High-Performance Computer Architecture</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Implementations of Context-Based Value Predictors</title>
		<author>
			<persName><forename type="first">Yiannakis</forename><surname>Sazeides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The predictability of data values</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sazeides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO.1997.645815</idno>
		<ptr target="https://doi.org/10.1109/MICRO.1997.645815" />
	</analytic>
	<monogr>
		<title level="m">Microarchitecture, 1997. Proceedings., Thirtieth Annual IEEE/ACM International Symposium on</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="248" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The Performance Potential of Data Dependence &amp; Collapsing</title>
		<author>
			<persName><forename type="first">Yiannakis</forename><surname>Sazeides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stamatis</forename><surname>Vassiliadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 29th Annual ACM/IEEE International Symposium on Microarchitecture<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Java benchmark for scientific and numerical computing</title>
		<author>
			<persName><surname>Scimark</surname></persName>
		</author>
		<ptr target="http://math.nist.gov/scimark2/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A 64-Kbytes ITTAGE indirect branch predictor</title>
		<author>
			<persName><forename type="first">Andr?</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third Championship Branch Prediction</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A New Case for the TAGE Branch Predictor</title>
		<author>
			<persName><forename type="first">Andr?</forename><surname>Seznec</surname></persName>
		</author>
		<idno type="DOI">10.1145/2155620.2155635</idno>
		<ptr target="https://doi.org/10.1145/2155620.2155635" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO-44)</title>
		<meeting>the 44th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO-44)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="117" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The SPEC CPU 2000 Benchmark Suite</title>
		<ptr target="http://www.spec.org" />
		<imprint/>
		<respStmt>
			<orgName>Standard Performance Evaluation Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The SPEC CPU 2006 Benchmark Suite</title>
		<ptr target="http://www.spec.org" />
		<imprint/>
		<respStmt>
			<orgName>Standard Performance Evaluation Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m">Sunspider Javascript Benchmark</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName><surname>Sunspider</surname></persName>
		</author>
		<ptr target="http://www.webkit.org/perf/sunspider/sunspider.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Memory Renaming: Fast, Early and Accurate Processing of Memory Communication</title>
		<author>
			<persName><forename type="first">Gary</forename><forename type="middle">S</forename><surname>Tyson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><forename type="middle">M</forename><surname>Austin</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1018734923512</idno>
		<ptr target="https://doi.org/10.1023/A:1018734923512" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Parallel Program</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="357" to="380" />
			<date type="published" when="1999-10">1999. Oct. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The energy complexity of register files</title>
		<author>
			<persName><forename type="first">V</forename><surname>Zyuban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kogge</surname></persName>
		</author>
		<idno type="DOI">10.1145/280756.280943</idno>
		<ptr target="https://doi.org/10.1145/280756.280943" />
	</analytic>
	<monogr>
		<title level="m">Proceedings. 1998 International Symposium on Low Power Electronics and Design</title>
		<meeting>1998 International Symposium on Low Power Electronics and Design</meeting>
		<imprint>
			<publisher>IEEE Cat</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="305" to="310" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
