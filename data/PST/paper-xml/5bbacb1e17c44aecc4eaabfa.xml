<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tumor-Aware, Adversarial Domain Adaptation from CT to MRI for Lung Cancer Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jue</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Medical Physics</orgName>
								<orgName type="institution">Memorial Sloan Kettering Cancer Center</orgName>
								<address>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu-Chi</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Medical Physics</orgName>
								<orgName type="institution">Memorial Sloan Kettering Cancer Center</orgName>
								<address>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Neelam</forename><surname>Tyagi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Medical Physics</orgName>
								<orgName type="institution">Memorial Sloan Kettering Cancer Center</orgName>
								<address>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengpeng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Medical Physics</orgName>
								<orgName type="institution">Memorial Sloan Kettering Cancer Center</orgName>
								<address>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Rimner</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Radiation Oncology</orgName>
								<orgName type="institution">Memorial Sloan Kettering Cancer Center</orgName>
								<address>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gig</forename><forename type="middle">S</forename><surname>Mageras</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Medical Physics</orgName>
								<orgName type="institution">Memorial Sloan Kettering Cancer Center</orgName>
								<address>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joseph</forename><forename type="middle">O</forename><surname>Deasy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Medical Physics</orgName>
								<orgName type="institution">Memorial Sloan Kettering Cancer Center</orgName>
								<address>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Harini</forename><surname>Veeraraghavan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Medical Physics</orgName>
								<orgName type="institution">Memorial Sloan Kettering Cancer Center</orgName>
								<address>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Tumor-Aware, Adversarial Domain Adaptation from CT to MRI for Lung Cancer Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">12C0BD973F1BA8C4B93E7F6233FC66D9</idno>
					<idno type="DOI">10.1007/978-3-030-00934-2_86</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an adversarial domain adaptation based deep learning approach for automatic tumor segmentation from T2-weighted MRI. Our approach is composed of two steps: (i) a tumor-aware unsupervised cross-domain adaptation (CT to MRI), followed by (ii) semisupervised tumor segmentation using Unet trained with synthesized and limited number of original MRIs. We introduced a novel target specific loss, called tumor-aware loss, for unsupervised cross-domain adaptation that helps to preserve tumors on synthesized MRIs produced from CT images. In comparison, state-of-the art adversarial networks trained without our tumor-aware loss produced MRIs with ill-preserved or missing tumors. All networks were trained using labeled CT images from 377 patients with non-small cell lung cancer obtained from the Cancer Imaging Archive and unlabeled T2w MRIs from a completely unrelated cohort of 6 patients with pre-treatment and 36 on-treatment scans. Next, we combined 6 labeled pre-treatment MRI scans with the synthesized MRIs to boost tumor segmentation accuracy through semi-supervised learning. Semi-supervised training of cycle-GAN produced a segmentation accuracy of 0.66 computed using Dice Score Coefficient (DSC). Our method trained with only synthesized MRIs produced an accuracy of 0.74 while the same method trained in semi-supervised setting produced the best accuracy of 0.80 on test. Our results show that tumor-aware adversarial domain adaptation helps to achieve reasonably accurate cancer segmentation from limited MRI data by leveraging large CT datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>MRI-guided radiotherapy is an emerging technology for improving treatment accuracy over conventional CT-based radiotherapy due to better soft-tissue con-H. Veeraraghavan-Equal contributing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Electronic supplementary material</head><p>The online version of this chapter (https:// doi.org/10.1007/978-3-030-00934-2 86) contains supplementary material, which is available to authorized users.</p><p>trast in MR compared to CT images. Real-time and accurate tumor segmentation on MRI can help to deliver high dose to tumors while reducing normal tissue dose. However, as MRI-guided radiotherapy is not used in standard-of-care, only very few MRIs are available for training. Therefore, we developed an adversarial domain adaptation from large CT datasets for tumor segmentation on MRI.</p><p>Although deep neural networks excel in learning from large amounts of (labeled) data, their accuracy is reduced when applied to novel datasets or domains <ref type="bibr" target="#b0">[1]</ref>. Differences between source and target domain distribution is called domain shift. Typically used fine-tuning methods require prohibitively large labeled data in the target domain. As an alternative, domain adaptation methods attempt to minimize domain shift either by feature sharing <ref type="bibr" target="#b1">[2]</ref> or by learning to reconstruct the target from source domain <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. In essence, domain adaptation methods learn the marginal distributions <ref type="bibr" target="#b4">[5]</ref> to transform source to target domain.</p><p>The problems of domain shift are exacerbated in medical images, where imaging modalities capture physical properties of the underlying anatomy differently (eg. CT vs. MRI). For example, whereas bones appear hyper-dense on CT and dark on MRI, tumors appear with similar contrast as normal soft-tissue on CT but have a distinct appearance on MRI (Fig. <ref type="figure" target="#fig_0">1</ref>(a) and (b)). Consequently, learning the marginal distributions of the domains alone may not be sufficient. Cross-domain adaptation of highly different modalities, has been applied in medical image analysis for image synthesis using paired images <ref type="bibr" target="#b5">[6]</ref> and unpaired images <ref type="bibr" target="#b6">[7]</ref>, as well as for segmentation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. However, all aforementioned approaches aim to only synthesize images that match the marginal but not the structure-specific conditional distribution such as tumors. Therefore, segmentation/classification using such synthetic images will lead to lower accuracy.</p><p>Therefore, we introduced a novel target specific loss, called tumor-aware loss, for unsupervised cross-domain adaptation that helps to preserve tumors on synthesized MRIs produced from CT images (Fig. <ref type="figure" target="#fig_0">1(d)</ref>), which cannot be captured with just the cycle-loss (Fig. <ref type="figure" target="#fig_0">1(c)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Our objective is to solve the problem of learning to segment tumors from MR images through domain adaptation from CT to MRI, where we have access to a reasonably sized labeled data in the source domain (X CT , y CT ) but are provided with very limited number of target samples X MRI X CT and fewer labels y MR . Our solution first employs tumor-aware unsupervised cross-domain adaptation to synthesize a reasonably large number of MRI from CT through adversarial training. Second, we combine the synthesized MRI with a fraction of real MRI with corresponding labels and train a Unet <ref type="bibr" target="#b9">[10]</ref> for generating tumor segmentation as outlined in Fig. <ref type="figure" target="#fig_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Step 1: MRI Synthesis Using Tumor-Aware Unsupervised Cross Domain Adaptation</head><p>The first step is to learn a mapping G CT →MRI that synthesizes MRI from the CT images to fool a discriminator D MRI using adversarial training <ref type="bibr" target="#b10">[11]</ref>. Additionally, we compute an adversarial loss L CT adv for synthesizing CT from MRI by simultaneously training a network that learns a mapping G MRI→CT . The adversarial loss, L MRI adv , for synthesizing MRI from CT, and L CT adv , for synthesizing CT from MRI, are computed as:</p><formula xml:id="formula_0">L MRI adv (G CT →MRI , D MRI , X MRI , X CT ) = E xm∼XMRI [log(D MRI (x m ))] +E xc∼XCT [log(1 -(D MRI (G CT →MRI (x c ))] L CT adv (G MRI→CT , D CT , X CT , X MRI ) = E xc∼XCT [log(D CT (x c ))] +E xm∼XMRI [log(1 -(D CT (G MRI→CT (x m ))] (1)</formula><p>where x c and x m are real images sampled from the CT (X CT ) and MRI (X MRI ) domains, respectively. The total adversarial loss (Fig. <ref type="figure" target="#fig_1">2</ref> (purple ellipse)) is then computed as the summation of the two losses as L adv = L MRI adv + L CT adv . We also compute a cycle consistency loss <ref type="bibr" target="#b4">[5]</ref> to regularize the images synthesized through independent training of the two networks. By letting the synthesized images be x m = G CT →MRI (x c ) and x c = G MRI→CT (x m ), the cycle consistency loss L cyc is calculated as:</p><formula xml:id="formula_1">Lcyc(G CT →MRI , G MRI→CT , X CT , X MRI ) = E xc∼X CT G MRI→CT (x m ) -xc 1 + E xm∼X MRI G CT →MRI (x c ) -xm 1 .</formula><p>(</p><formula xml:id="formula_2">)<label>2</label></formula><p>The cycle consistency and adversarial loss only constrain the model to learn a global mapping that matches the marginal distribution but not the conditional distribution pertaining to individual structures such as the tumors. Therefore, a model trained using these losses does not need to preserve tumors, which can lead to either deterioration or total loss of tumors in the synthesized MRIs (Fig. <ref type="figure" target="#fig_0">1(c)</ref>). Therefore, we introduced a tumor-aware loss that forces the network to preserve the tumors. To be specific, the tumor-aware loss is composed of a tumor loss (Fig. <ref type="figure" target="#fig_1">2</ref> (red ellipse)) and a feature loss (Fig. <ref type="figure" target="#fig_1">2</ref> (orange ellipse)).</p><p>We compute the tumor loss by training two parallel tumor detection networks using simplified models of the Unet <ref type="bibr" target="#b9">[10]</ref> for CT (U CT ) and the synthesized MRI (U MRI ). The tumor loss constrains the CT and synthetic MRI-based Unets to produce similar tumor segmentations, thereby, preserving the tumors and is computed as:</p><formula xml:id="formula_3">L tumor = E xc∼XCT ,yc∼yCT [logP (y c |G CT →MRI (x c ))] + E xc∼XCT ,yc∼yCT [logP (y c |X CT )].<label>(3)</label></formula><p>On the other hand, the tumor feature loss L feat forces the high-level features of X CT and X MRI CT to be shared by using a constraint inspired by <ref type="bibr" target="#b11">[12]</ref> as:</p><formula xml:id="formula_4">L feat (x c ∼ X CT ) = 1 C × H × W φ CT (x c ) -φ MRI (G CT →MRI (x c )) 2 . (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where φ CT and φ MRI are the high-level features extracted from the U CT and U MRI , respectively; C, H and W indicate the size of the feature. The total loss is then expressed as:</p><formula xml:id="formula_6">L total = L adv + λ cyc L cyc + λ tumor L tumor + λ feat L feat , (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>where λ cyc , λ tumor and λ feat are the weighting coefficients for each loss. During training, we alternatively update the domain transfer or generator network G, the discriminator D, and the tumor constraint network U with the following gradients, -Δ θG (L adv + λ cyc L cyc + λ tumor L tumor + λ feat L feat ), -Δ θD (L adv ) and -Δ θU (L tumor + λ feat L feat ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Step 2: Semi-supervised Tumor Segmentation from MRI</head><p>The synthesized MRI from the first step were combined with a small set of real MRI with labels ( XMRI and ỹMRI in Fig. <ref type="figure" target="#fig_1">2</ref>) to train a U-net <ref type="bibr" target="#b9">[10]</ref> using Dice loss <ref type="bibr" target="#b12">[13]</ref> (Fig. <ref type="figure" target="#fig_1">2</ref> (blue ellipse)) to generate tumor segmentation. Adversarial network optimization for MRI synthesis was frozen prior to semi-supervised tumor segmentation training to prevent leakage of MRI label information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Network Structure and Implementation</head><p>The generators G and discriminators D for CT and MRI synthesis networks were implemented similar to that in <ref type="bibr" target="#b4">[5]</ref>. We tied the penultimate layer in U MRI and U CT . The details of all networks are shown in the supplementary documents.</p><p>Pytorch library <ref type="bibr" target="#b13">[14]</ref> was used for implementing the proposed networks, which were trained on Nvidia GTX 1080Ti of 12 GB memory with a batch size of 1 during image transfer and batch size of 10 during semi-supervised segmentation. The ADAM algorithm <ref type="bibr" target="#b14">[15]</ref> with an initial learning rate of 1e-4 was used during training. We set λ cyc = 10, λ tumor = 5 and λ feat = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Ablation Tests</head><p>We tested the impact of adding tumor-aware loss to the cycle loss (proposed vs. cycle-GAN <ref type="bibr" target="#b4">[5]</ref> vs. masked-cycle-GAN <ref type="bibr" target="#b7">[8]</ref>). Images synthesized using aforementioned networks were trained to segment using semi-supervised learning by combining with a limited number of real MRI. We call adversarial synthesis <ref type="bibr" target="#b7">[8]</ref> that combined tumor labels as an additional channel with the original images as masked-cycle-GAN. We also evaluated the effect of adding a limited number of original MRI to the synthesized MRI on segmentation accuracy (tumor-aware with semi-supervised vs. tumor-aware with unsupervised training). We benchmarked the lowest achievable segmentation accuracy by training a network with only the pre-treatment (or week one) MRI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Datasets</head><p>The image synthesis networks were trained using contrast-enhanced CT images with expert delineated tumors from 377 patients with non-small cell lung cancer (NSCLC) <ref type="bibr" target="#b15">[16]</ref> available from The Cancer Imaging Archive (TCIA) <ref type="bibr" target="#b16">[17]</ref>, and an unrelated cohort of 6 patients scanned with T2w MRI at our clinic before and during treatment every week (n = 7) with radiation therapy. Masked cycle-GANs used both tumor labels and the images as additional channels even for image synthesis training. Image regions enclosing the tumors were extracted and rescaled to 256 × 256 to produce 32000 CT image slices and 9696 T2w MR image slices. Only 1536 MR images from pre-treatment MRI were used for semi-supervised segmentation training of all networks. Segmentation validation was performed on the subsequent on-treatment MRIs (n = 36) from the same 6 patients. Test was performed using 28 MRIs consisting of longitudinal scans (7, 7, 6) from 3 patients and pre-treatment scans from 8 patients not used in training. Tumor segmentation accuracy was evaluated by comparing to expert delineations using the Dice Score Coefficient (DSC), and the Hausdorff Distance 95% (HD95).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">MR Image Synthesis Results</head><p>Figure <ref type="figure" target="#fig_2">3</ref> shows the representative qualitative results of synthesized MRI produced using only the cycle-GAN (Fig. <ref type="figure" target="#fig_2">3(b</ref>)), masked cycle-GAN (Fig. <ref type="figure" target="#fig_2">3(c)</ref>) and using our method (Fig. <ref type="figure" target="#fig_2">3(d)</ref>). As seen, our method best preserves the anatomical details between CT and MRI. Quantitative evaluation using the Kullback -Leibler (KL) divergence computed from tumor regions between synthesized and original MRI, used for training, confirmed that our method resulted in the best match of tumor distribution with the lowest KL divergence of 0.069 compared with those obtained using the cycle-GAN (1.69) and masked cycle-GAN (0.32). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Segmentation Results</head><p>Figure <ref type="figure" target="#fig_3">4</ref> shows the segmentations generated using the various methods (yellow contours) for three representative cases from the test and validation sets, together with the expert delineations (red contours). As shown in Table <ref type="table" target="#tab_0">1</ref>, our approach outperformed cycle GAN irrespective of training without (unsupervised) or with (semisupervised) labeled target data. Semi-supervised segmentation outperformed all methods in both test and validation datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>In this work, we introduced a novel target-specific, tumor-aware loss for synthesizing MR images from unpaired CT datasets using unsupervised cross-domain adaptation. The tumor-aware loss forces the network to retain tumors that are typically lost when using only the cycle-loss and leads to accurate tumor segmentation. Although applied to lung tumors, our method is applicable to other structures and organs. Segmentation accuracy of our approach trained with only synthesized MRIs exceeded other methods trained in a semi-supervised manner. Adding small set of labeled target domain data further boosts accuracy. The validation set produced lower but not significantly different (p = 0.1) DSC accuracy than the test set due to significantly smaller (p = 0.0004) tumor volumes in validation (mean 37.66cc) when compared with the test set (mean 68.2cc). Our results showed that masked-cycle-GAN produced lower test performance compared to basic cycle-GAN, possibly due to poor modeling from highly unbalanced CT and MR datasets. As a limitation, our approach only forces the synthesized MRIs to preserve tumors but not the MR intensity distribution within tumors. Such modeling would require learning the mapping for individual scan manufacturers, magnet strengths and coil placements which was outside the scope of this work. Additionally, synthesized images irrespective of the chosen method do not produce a one-to-one pixel mapping from CT to MRI similar to <ref type="bibr" target="#b7">[8]</ref>. There is also room for improving the segmentation accuracy by exploring more advanced segmentation models, e.g. boundary-aware fully convolutional networks (FCN) <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we proposed a tumor-aware, adversarial domain adaptation method using unpaired CT and MR images for generating segmentations from MRI.</p><p>Our approach preserved tumors on synthesized MRI and generated the best segmentation performance compared with state-of-the-art adversarial cross-domain adaptation. Our results suggest feasibility for lung tumor segmentation from MRI trained using MRI synthesized from CT.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. MRI synthesized from a representative (a) CT image using (c) cycle-GAN [5] and (d) proposed method. The corresponding MRI scan for (a) is shown in (b). As shown, the proposed method (d) using tumor-aware loss helps to fully preserve tumor in the synthesized MRI compared with (c).</figDesc><graphic coords="2,56.79,316.28,310.12,84.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Approach overview. XCT and XMRI are the real CT and MRI; X MRI CT and X CT MRI are the synthesized MR and CT images; yCT is the CT image label; GCT →MRI and GMRI→CT are the CT and MRI transfer networks; XMRI and ỹMRI are a small sample set from the real MRI, used to train semi-supervised segmentation.</figDesc><graphic coords="3,105.96,208.67,240.88,171.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. MRI synthesized from CT using different deep learning methods. The red contour indicates the manually delineated tumor region in the NSCLC datasets [16]. (a) CT image; (b) Cycle-GAN [5]; (c) Masked cycle-GAN [8]; (d) Proposed.</figDesc><graphic coords="6,108.81,366.59,207.04,110.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Segmentation results on the representative examples from the validation and test set of different methods. The red contour stands for the expert delineations and the yellow contour stands for the segmentation results. (a) segmentation with only week 1 MRI; (b) segmentation using MRI synthesized by cycle-GAN [5]; (c) segmentation using MRI synthesized by masked cycle-GAN [8]; (d) tumor-aware unsupervised learning; (e) tumor-aware semi-supervised learning</figDesc><graphic coords="7,63.96,374.63,324.76,164.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Segmentation accuracy</figDesc><table><row><cell>Method</cell><cell>Validation</cell><cell></cell><cell>Test</cell></row><row><cell></cell><cell>DSC</cell><cell>HD95 mm</cell><cell>DSC</cell><cell>HD95 mm</cell></row><row><cell>Week one only</cell><cell>0.63 ± 0.27</cell><cell cols="3">7.22 ± 7.19 0.55 ± 0.25 13.23 ± 0.75</cell></row><row><cell>Cycle-GAN [5]</cell><cell cols="4">0.57 ± 0.24 11.41 ± 5.57 0.66 ± 0.16 11.91 ± 4.44</cell></row><row><cell>Masked cycle-GAN [8]</cell><cell>0.67 ± 0.21</cell><cell cols="3">7.78 ± 4.40 0.63 ± 0.24 11.65 ± 6.53</cell></row><row><cell>Tumor aware unsupervised</cell><cell>0.62 ± 0.26</cell><cell cols="2">7.47 ± 4.66 0.74 ± 0.15</cell><cell>8.88 ± 4.83</cell></row><row><cell cols="5">Tumor aware semi-supervised 0.70 ± 0.19 5.88 ± 2.88 0.80 ± 0.08 7.16 ± 4.52</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This work was funded in part through the NIH/NCI Cancer Center Support Grant P30 CA008748.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07">2017. July 2017. 2017</date>
			<biblScope unit="page" from="2962" to="2971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2107" to="2116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pixel-level domain transfer</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Paek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46484-8_31</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46484-831" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9912</biblScope>
			<biblScope unit="page" from="517" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Medical image synthesis with context-aware generative adversarial networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-66179-7_48</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-66179-748" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2017</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Duchesne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10435</biblScope>
			<biblScope unit="page" from="417" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep MR to CT synthesis using unpaired data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolterink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dinkla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H F</forename><surname>Savenije</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Seevinck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A T</forename><surname>Van Den Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Išgum</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-68127-6_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-68127-62" />
	</analytic>
	<monogr>
		<title level="m">SASHIMI 2017</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Gooya</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Prince</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10557</biblScope>
			<biblScope unit="page" from="14" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adversarial image synthesis for unpaired multi-modal cardiac data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chartsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joyce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dharmakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-68127-6_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-68127-61" />
	</analytic>
	<monogr>
		<title level="m">SASHIMI 2017</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Gooya</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Prince</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10557</biblScope>
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adversarial synthesis learning enables segmentation without target modality ground truth</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Assad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Abramson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-A</forename><surname>Landman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Symposium on Biomedical Imaging</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46475-6_43</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46475-6" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9906</biblScope>
			<biblScope unit="page">43</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">V-net: fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<title level="m">Automatic differentiation in Py Torch</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations (ICLR</title>
		<meeting>the 3rd International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Data from NSCLC-radiomics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Aerts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Cancer Imaging Archive</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The cancer imaging archive (TCIA): maintaining and operating a public information repository</title>
		<author>
			<persName><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Digital Imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1045" to="1057" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Boundary-aware fully convolutional network for brain tumor segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Mckenna</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-66185-8_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-66185-8" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2017</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Descoteaux</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Franz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Jannin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Collins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Duchesne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10434</biblScope>
			<biblScope unit="page">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
