<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PointDAN: A Multi-Scale 3D Domain Adaption Network for Point Cloud Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Can</forename><surname>Qin</surname></persName>
							<email>qin.ca@husky.neu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical &amp; Computer Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haoxuan</forename><surname>You</surname></persName>
							<email>haoxuan.you@columbia.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical &amp; Computer Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lichen</forename><surname>Wang</surname></persName>
							<email>wanglichenxj@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">C.-C</forename><forename type="middle">Jay</forename><surname>Kuo</surname></persName>
							<email>cckuo@sipi.usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical &amp; Computer Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Khoury College of Computer Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
							<email>yunfu@ece.neu.edu</email>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">33rd Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2019)</postCode>
									<settlement>Vancouver</settlement>
									<region>NeurIPS</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PointDAN: A Multi-Scale 3D Domain Adaption Network for Point Cloud Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">55B805293A34DDDFBC12C7A60D4F1788</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Domain Adaptation (DA) approaches achieved significant improvements in a wide range of machine learning and computer vision tasks (i.e., classification, detection, and segmentation). However, as far as we are aware, there are few methods yet to achieve domain adaptation directly on 3D point cloud data. The unique challenge of point cloud data lies in its abundant spatial geometric information, and the semantics of the whole object is contributed by including regional geometric structures. Specifically, most general-purpose DA methods that struggle for global feature alignment and ignore local geometric information are not suitable for 3D domain alignment. In this paper, we propose a novel 3D Domain Adaptation Network for point cloud data (PointDAN). PointDAN jointly aligns the global and local features in multi-level. For local alignment, we propose Self-Adaptive (SA) node module with an adjusted receptive field to model the discriminative local structures for aligning domains. To represent hierarchically scaled features, node-attention module is further introduced to weight the relationship of SA nodes across objects and domains. For global alignment, an adversarial-training strategy is employed to learn and align global features across domains. Since there is no common evaluation benchmark for 3D point cloud DA scenario, we build a general benchmark (i.e., PointDA-10) extracted from three popular 3D object/scene datasets (i.e., ModelNet, ShapeNet and ScanNet) for cross-domain 3D objects classification fashion. Extensive experiments on PointDA-10 illustrate the superiority of our model over the state-of-the-art general-purpose DA methods. 1 1 The PointDA-10 data and official code are uploaded on https://github.com/canqin001/PointDAN * Equal Contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D vision has achieved promising outcomes in wide-ranging real-world applications (i.e., autonomous cars, robots, and surveillance system). Enormous amounts of 3D point cloud data is captured by depth cameras or LiDAR sensors nowadays. Sophisticated 3D vision and machine learning algorithms are required to analyze its content for further exploitation. Recently, the advent of Deep Neural Network (DNN) has greatly boosted the performance of 3D vision understanding including tasks of classification, detection, and segmentation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41]</ref>. Despite its impressive success, DNN requires massive amounts of labeled data for training which is time-consuming and expensive to collect. This issue significantly limits its promotion in the real world. Domain adaptation (DA) solves this problem by building a model utilizing the knowledge of label-rich dataset, i.e., source domain, which generalizes well on the label-scarce dataset, i.e., target domain. However, due to the shifts of distribution across different domains/datasets, a model trained on one domain usually performs poorly on other domains. Most DA methods address this problem by either mapping original features into a shared subspace or minimizing instance-level distances, such as MMD, CORAL etc., to mix cross-domain features <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31]</ref>. Currently, inspired by Generative Adversarial Network (GAN) <ref type="bibr" target="#b11">[12]</ref>, adversarial-training DA methods, like DANN, ADDA, MCD etc., have achieved promising performance in DA and drawn increasing attentions <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b25">26]</ref>. They deploy a zero-sum game between a discriminator and a generator to learn domain-invariant representations. However, most of the existing DA approaches mainly target on 2D vision tasks, which globally align the distribution shifts between different domains. While for 3D point cloud data, the geometric structures in 3D space can be detailedly described, and different local structures also have clear semantic meaning, such as legs for chairs, which in return combine to form the global semantics for a whole object. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, two 3D objects might be weak to align in global, but would have similar 3D local structures, which are easier to be aligned. So it is urgently desired for a domain adaptation framework to focus on local geometric structures in 3D DA scenario.</p><p>To this end, this paper introduces a novel point-based Unsupervised Domain Adaptation Network (PointDAN) to achieve unsupervised domain adaptation (UDA) for 3D point cloud data. The key to our approach is to jointly align the multi-scale, i.e., global and local, features of point cloud data in an end-to-end manner. Specifically, the Self-Adaptive (SA) nodes associated with an adjusted receptive field are proposed to dynamically gather and align local features across domains. Moreover, a node attention module is further designed to explore and interpret the relationships between nodes and their contributions in alignment. Meanwhile, an adversarial-training strategy is deployed to globally align the global features. Since there are few benchmarks for DA on 3D data ( i.e., point cloud) before, we build a new benchmark named PointDA-10 dataset for 3D vision DA. It is generated by selecting the samples in 10 overlapped categories among three popular datasets (i.e., ModelNet <ref type="bibr" target="#b34">[35]</ref>, ShapeNet <ref type="bibr" target="#b2">[3]</ref> and ScanNet <ref type="bibr" target="#b4">[5]</ref>). In all, the contributions of our paper could be summarized in three folds:</p><p>‚Ä¢ We introduce a novel 3D-point-based unsupervised domain adaptation method by locally and globally align the 3D objects' distributions across different domains. ‚Ä¢ For local feature alignment, we propose the Self-Adaptive (SA) nodes with a node attention to utilize local geometric information and dynamically gather regional structures for aligning local distribution across different domains.  <ref type="bibr" target="#b16">[17]</ref> proposes œá-Conv to aggregate features in local pitches and applies a bottom-up network structure like typical CNNs. In 3D object detection tasks, <ref type="bibr" target="#b40">[41]</ref> proposes to divide a large scene into many voxels, where features of inside points are extracted respectively and a 3D Region Proposal Network (RPN) structure is followed to obtain detection prediction.</p><p>In spite of the broad usage, point cloud data has significant drawbacks in labeling efficiency. During labeling, people need to rotate several times and look through different angles to identify an object. In real-world environment where point cloud data are scanned from LiDAR, it also happens that some parts are lost or occluded (e.g.tables lose legs), which makes efficient labeling more difficult. Under this circumstance, a specific 3D point-based unsupervised domain adaptation method designed to mitigate the domain gap of source labeled data and target unlabeled data is extremely desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Unsupervised Domain Adaptation (UDA)</head><p>The main challenge of UDA is that distribution shift (i.e., domain gap) exists between the target and source domain. It violates the basic assumption of conventional machine learning algorithms that training samples and test samples sharing the same distribution. To bridge the domain gap, UDA approaches match either the marginal distributions <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b32">33]</ref> or the conditional distributions <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b3">4]</ref> between domains via feature alignment. It addresses this problem by learning a mapping function f which projects the raw image features into a shared feature space across domains. Most of them attempt to maximizing the inter-class discrepancy, while minimize the intra-class distance in a subspace simultaneously. Various methods, such as Correlation Alignment (CORAL) <ref type="bibr" target="#b30">[31]</ref>, Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18]</ref>, or Geodesic distance <ref type="bibr" target="#b12">[13]</ref> have been proposed.</p><p>Apart from the methods aforementioned, many DNN-based domain adaptation methods have been proposed due to their great capacity in representation learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b15">16]</ref>. The key to these methods is to apply DNN to learn domain-invariant features through an end-to-end training scenario. Another kind of approach utilizes adversarial training strategy to obtain the domain invariant representations <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b23">24]</ref>. It includes a discriminator and a generator where the generator aims to fool the discriminator until the discriminator is unable to distinguish the generated features between the two domains. Such approaches include Adversarial Discriminative Domain Adaptation (ADDA) <ref type="bibr" target="#b31">[32]</ref>, Domain Adversarial Neural Network (DANN) <ref type="bibr" target="#b9">[10]</ref>, Maximum Classifier Discrepancy (MCD) <ref type="bibr" target="#b25">[26]</ref> etc.</p><p>Most of UDA methods are designed for 2D vision tasks and focus on the alignment of global image features across different domains. While in 3D data analytical tasks, regional and local geometry information is crucial for achieving good learning performance. Zhou et al. <ref type="bibr" target="#b39">[40]</ref> firstly introduced UDA on the task of 3D keypoint estimation relying on the regularization of multi-view consistency term. However, this method cannot be extended to more generalized tasks, i.e., classification. In <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b33">34]</ref>, point cloud data are first projected into 2D images (bird-eye view or front view), and 2D DA methods are applied, which would lose essential 3D geometric information. To this end, we propose a generalized 3D point-based UDA framework. It well preserves the local structures and explores the global correlations of all local features. Adversarial training strategies are further employed to locally and globally align the distribution shifts across the source and target domains.</p><p>3 Proposed Model</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition and Notation</head><p>In 3D point-based UDA, we have the access to labeled source domain S = {(x s i , y s i )} ns i=1 where y s i ‚àà Y = {1, ..., Y } with n s annotated pairs and target domain T = {x t j } nt j=1 of n t unlabeled data points. The inputs are point cloud data usually represented by 3-dimensional coordinates (x, y, z) where</p><formula xml:id="formula_0">x s i , x t j ‚àà X ‚äÇ R T √ó3</formula><p>, where T is the number of sampling points of one 3D object, with the same label space Y s = Y t . It is further assumed that two domains are sampled from the distributions P s (x s i , y s i ) and P t (x t i , y t i ) respectively while the i.i.d. assumption is violated due to the distribution shift P s = P t . The key to UDA is to learn a mapping function Œ¶ : X ‚Üí R d that projects raw inputs into a shared feature space H spreadable for cross-domain samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Local Feature Alignment</head><p>The local geometric information plays an important role in describing point cloud objects as well as domain alignment. As illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, given the same "table" class, the one from ScanNet misses parts of legs due to the obstacles through LiDAR scanning. The key to align these two "tables" is to extract and match the features of similar structures, i.e., plains, while ignoring the different parts.</p><p>To utilize the local geometric information, we propose to adaptively select and update key nodes for better fitting the local alignment.</p><p>Self-Adaptive Node Construction: Here we give the definition of node in point cloud. For each point cloud, we represent its n local geometric structures as n point sets</p><formula xml:id="formula_1">{S c |S c = {x c , x c1 , ..., x ck }, x ‚äÜ R 3 } n c=1</formula><p>, where the c-th region S c contains a node xc and its surrounding k nearest neighbor points {x c1 , ..., x ck }. The location of a node decides where the local region is and what points are included.</p><p>To achieve local features, directly employing the farthest point sampling or random sampling to get the center node is commonly used in previous work <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b16">17]</ref>. These methods guarantee full coverage over the whole point cloud. However, for domain alignment, it is essential to make sure that these nodes cover the structures of common characteristics in 3D geometric space and drop the parts unique to certain objects. In this way, the local regions sharing similar structures are more proper to be aligned, while the uncommon parts would bring a negative transfer influence.</p><p>Inspired by deformable convolution in 2D vision <ref type="bibr" target="#b5">[6]</ref>, we propose a novel geometric-guided shift learning module, which makes the input nodes self-adaptive in receptive field for network. Different from Deformable Convolution where semantic features are used for predicting offset, we utilize the local edge vector as a guidance during learning. As show in Fig. <ref type="figure">2</ref>, our module transforms semantic information of each edge into its weight and then we aggregate the weighted edge vectors together to obtain our predicted offset direction. Intuitively, the prediction shift is decided by the voting of surrounding edges with different significance. We first initialize the location of node by the farthest point sampling over the point cloud to get n nodes, and their k nearest neighbor points are collected together to form n regions. For the c-th node, its offset is computed as:</p><formula xml:id="formula_2">‚àÜx c = 1 k k j=1 (R T (v cj -vc ) ‚Ä¢ (x cj -xc )),<label>(1)</label></formula><p>where x and x cj denote location of node and its neighbor point, so x cj -xc means the edge direction. v cj and vc are their mid-level point feature extracted from the encoder v = E(x|Œò E ) and R T is the weight from one convolution layer for transforming feature. We apply the bottom 3 feature extraction layers of PointNet as the encoder E. ‚àÜx c is the predicted location offset of the c-th node.</p><p>After obtaining learned shift ‚àÜx c , we achieve the self-adaptive update of nodes and their regions by adding shift back to node xc and finding their k nearest neighbor points:</p><formula xml:id="formula_3">xc = xc + ‚àÜx c ,<label>(2)</label></formula><p>{x c1 , ..., x ck } = kN N (x c |x j , j = 0, ..., M -1).</p><p>(</p><p>Then the final node features vc is computed by gathering all the point features inside their regions:</p><formula xml:id="formula_5">vc = max j=1,..,k R G (v cj ).<label>(4)</label></formula><p>where R G is the weight of one convolution layer for gathering point features in which R G R T = R, and the output node features are employed for local alignment. For better engaging SA node features, we also interpolate them back into each point following the interpolation strategy in <ref type="bibr" target="#b22">[23]</ref> and fuse them with the original point features from a skip connection. The fused feature is input into next-stage generator for higher-level processing.</p><p>SA Node Attention: Even achieving SA nodes, it is unreasonable to assume that every SA node contributes equally to the goal of domain alignment. The attention module, which is designed to model the relationship between nodes, is necessary for weighting the contributions of different SA nodes for domain alignment and capturing the features in larger spatial scales. Inspired by the channel attention <ref type="bibr" target="#b38">[39]</ref>, we apply a node attention network to model the contribution of each SA nodes for alignment by introducing a bottleneck network with a residual structure <ref type="bibr" target="#b13">[14]</ref>:</p><formula xml:id="formula_6">h c = œï(W U Œ¥(W D z c )) ‚Ä¢ vc + vc ,<label>(5)</label></formula><p>where z c = E(v c (k)) indicates the mean of the c-th node feature. Œ¥(‚Ä¢) and œï(‚Ä¢) represent the ReLU function <ref type="bibr" target="#b19">[20]</ref> and Sigmoid function respectively. W D is the weight set of a convolutional layer with 1 √ó 1 kernels, which reduces the number of channels with the ratio r. The channel-upscaling layer W U , where W U W D = W, increases the channels to its original number with the ratio r.</p><p>SA Node Feature Alignment: The optimization of both offsets and network parameters for local alignment are sensitive to the disturbance of gradients, which makes GAN-based methods perform unstable. Therefore, we minimize the MMD <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18]</ref> loss to align cross-domain SA node features as:</p><formula xml:id="formula_7">L mmd = 1 n s n s ns i,j=1 Œ∫(h s i , h s j ) + 1 n s n t ns,nt i,j=1 Œ∫(h s i , h t j ) + 1 n t n t nt i,j=1 Œ∫(h t i , h t j ),<label>(6)</label></formula><p>where Œ∫ is a kernel function and we apply the Radial Basis Function (RBF) kernel in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Global Feature Alignment</head><p>After having the features f i ‚àà R d corresponding to the i-th sample by a generator network, the global feature alignment attempts to minimize the distance between features across different domains. In difference of local feature alignment, global feature alignment process is more stable due to the invariance of receptive field of inputs, which provides more options for choosing GAN-based methods.</p><p>In this paper, we apply Maximum Classifier Discrepancy (MCD) <ref type="bibr" target="#b25">[26]</ref> for global feature alignment due to its outstanding performance in general-purpose domain alignment.</p><p>The encoder E designed for SA node feature extraction is also applied for extracting raw point cloud features: hi = E (x i |Œò E ) over the whole object. And the point features are concatenated with interpolated SA-node features as ƒ•i = [h i , hi ] to capture the geometry information in multi-scale. Then, we feed the ƒ•i to the generator network G which is the final convolution layer (i.e., conv4) of PointNet attached with a global max-pooling to achieve high-level global feature</p><formula xml:id="formula_8">f i = max -pooling(G( ƒ•i |Œò G ))</formula><p>, where f i ‚àà R d represents the global feature of the i-th sample. And d is usually assigned as 1,024. The global alignment module attempts to align domains with two classifier networks F 1 and F 2 to keep the discriminative features given the support of source domain decision boundaries. The two classifiers F 1 and F 2 take the features f i and classify them into K classes as p j (y i |x i ) = F j f i |Œò j F , j = 1, 2, where p j (y i |x i ) is the K-dimensional probabilistic softmax results of classifiers.</p><p>To train the model, the total loss is composed of two parts: the task loss and discrepancy loss. Similar as most UDA methods, the object of task loss is to minimize the empirical risk on source domain {X s , Y s }, which is formulated as follows:</p><formula xml:id="formula_9">L cls (X s , Y s ) = -E (xs,ys)‚àº(Xs,Ys) K k=1 1 [k=ys] log(p((y = y s )|G(E(x s |Œò E )|Œò G ))). (7)</formula><p>The discrepancy loss is calculated as the l 1 distance between the softmax scores of two classifiers:</p><formula xml:id="formula_10">L dis (x t ) = E xt‚àºXt [|p 1 (y|x t ) -p 2 (y|x t )|].<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Procedure</head><p>We apply the Back-Propagation <ref type="bibr" target="#b24">[25]</ref> to optimize the whole framework under the end-to-end training scenario. The training process is composed of two steps in total:</p><p>Step1. Firstly, it is required to train two classifiers F 1 and F 2 with the discrepancy loss L dis in Eq. ( <ref type="formula" target="#formula_10">8</ref>) and classification loss L cls obtained in Eq. ( <ref type="formula">7</ref>). The discrepancy loss, which requires to be maximized, helps gather target features given the support of the source domain. The classification loss is applied to minimize the empirical risk on source domain. The objective function is as follows:</p><formula xml:id="formula_11">min F1,F2 L cls -ŒªL dis .<label>(9)</label></formula><p>Step2. In this step, we train the generator G, encoder E, the node attention network W and transform network R by minimizing the discrepancy loss, classification loss and MMD loss to achieve discriminative and domain-invariant features. The objective function in this step is formulated as: min</p><formula xml:id="formula_12">G,E,W,R L cls + ŒªL dis + Œ≤L mmd ,<label>(10)</label></formula><p>where both Œª and Œ≤ are hyper-parameters which manually assigned as 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Theoretical Analysis</head><p>In this section, we analyze our method in terms of the H‚àÜHdistance theory <ref type="bibr" target="#b0">[1]</ref>. The H‚àÜH-distance is defined as</p><formula xml:id="formula_13">d H‚àÜH (S, T ) = 2 sup h1,h2‚ààH |P x‚àºS [h 1 (x) = h 2 (x)] -P x‚àºT [h 1 (x = h 2 (x))]| ,<label>(11)</label></formula><p>which represents the discrepancy between the target and source distributions, T and S, with regard to the hypothesis class H. According to <ref type="bibr" target="#b0">[1]</ref>, the error of classifier h on the target domain T (h) can be bounded by the sum of the source domain error S (h), the H‚àÜHdistance and a constant C which is independent of h, i.e.,</p><formula xml:id="formula_14">T (h) ‚â§ S (h) + 1 2 d H‚àÜH (S, T ) + C. (<label>12</label></formula><formula xml:id="formula_15">)</formula><p>The relationship between our method and the H‚àÜHdistance will be discussed in the following. The H‚àÜHdistance can also be denoted as below:</p><formula xml:id="formula_16">d H‚àÜH (S, T ) = 2 sup h1,h2‚ààH E x‚àºS 1 [h1(x) =h2(x)] -E x‚àºT 1 [h1(x) =h2(x)] .<label>(13)</label></formula><p>As the term E x‚àºS 1 [h1(x) =h2(x)] would be very small if h 1 and h 2 can classify samples over S correctly. In our case, p 1 and p 2 correspond to h 1 and h 2 respectively, which agree on their predictions on source samples S. As a result, d H‚àÜH (S, T ) can be approximately calculated by </p><formula xml:id="formula_17">sup h1,h2‚ààH E x‚àºT 1 [h1(x) =h2(x)] ,</formula><formula xml:id="formula_18">E x‚àºT 1 [h1(x) =h2(x)] = sup F1,F2 E x‚àºT 1 [F1‚Ä¢G(x) =F2‚Ä¢G(x)] .<label>(14)</label></formula><p>Further, we replace sup with max, and attempt to minimize <ref type="bibr" target="#b13">(14)</ref> with respect to G:</p><formula xml:id="formula_19">min G max F1,F2 E x‚àºT 1 [F1‚Ä¢G(x) =F2‚Ä¢G(x)] .<label>(15)</label></formula><p>Problem ( <ref type="formula" target="#formula_19">15</ref>) is similar to the problem (9,10) in our method. Consider the discrepancy loss L dis , we first train classifiers F 1 , F 2 to maximize L dis on the target domain and next train generator G to minimize L dis , which matches with problem <ref type="bibr" target="#b14">(15)</ref>. Although we also need consider the source loss L cls and MMD loss L mmd , we can see from <ref type="bibr" target="#b0">[1]</ref> that our method still has a close connection to the H‚àÜHdistance. Thus, by iteratively train F 1 , F 2 and G, we can effectively reduce d H‚àÜH (S, T ), and further lead to the better approximate T (h) by S (h).  As there is no 3D point cloud benchmark designed for domain adaptation, we propose three datasets with different characteristics, i.e., ModelNet-10, ShapeNet-10, ScanNet-10, for the evaluation of point cloud DA methods.</p><p>To build them, we extract the samples in 10 shared classes from Mod-elNet40 <ref type="bibr" target="#b34">[35]</ref>, ShapeNet <ref type="bibr" target="#b2">[3]</ref> and Scan-Net <ref type="bibr" target="#b4">[5]</ref> respectively. The statistic and visualization are shown in Table <ref type="table" target="#tab_4">1</ref> and Fig. ModelNet-10 (M): ModelNet40 contains clean 3D CAD models of 40 categories. To extract overlapped classes, we regard 'nightstand' class in ModelNet40 as 'cabinet' class in ModelNet-10, because these two objects almost share the same structure. After getting the CAD model, we sample points on the surface as <ref type="bibr" target="#b22">[23]</ref> to fully cover the object. ShapeNet-10 (S): ShapeNetCore contains 3D CAD models of 55 categories gathered from online repositories. ShapeNet contains more samples and its objects have larger variance in structure compared with ModelNet. We apply uniform sampling to collect the points of ShapeNet on surface, which, compared with ModelNet, may lose some marginal points.</p><p>ScanNet-10 (S*): ScanNet contains scanned and reconstructed real-world indoor scenes. We isolate 10 classes instances contained in annotated bounding boxes for classification. The objects often lose some parts and get occluded by surroundings. ScanNet is a challenging but realistic domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiments Setup</head><p>In this section, we evaluate the proposed method under the standard protocol <ref type="bibr" target="#b10">[11]</ref> of unsupervised domain adaptation on the task of point cloud data classification.</p><p>Implementation Details: We choose the PointNet <ref type="bibr" target="#b21">[22]</ref> as the backbone of Encoder E and Generator G and apply a two-layer multilayer perceptron (MLP) as F 1 and F 2 . The proposed approach is implemented on PyTorch with Adam <ref type="bibr" target="#b14">[15]</ref> as the optimizer and a NVIDIA TITAN GPU for training. The learning rate is assigned as 0.0001 under the weight decay 0.0005. All models have been trained for 200 epochs of batch size 64. We extract the SA node features from the third convolution layer (i.e., conv3) for local-level alignment and the number of SA node is assigned as 64.</p><p>Baselines: We compare the proposed method with a serial of general-purpose UDA methods including: Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b17">[18]</ref>, Adversarial Discriminative Domain Adaptation (ADDA) <ref type="bibr" target="#b31">[32]</ref>, Domain Adversarial Neural Network (DANN) <ref type="bibr" target="#b9">[10]</ref>, and Maximum Classifier Discrep- ancy (MCD) <ref type="bibr" target="#b25">[26]</ref>. During these experiments, we take the same loss and the same training policy. w/o Adapt refers to the model trained only by source samples and Supervised means fully supervised method.</p><p>Ablation Study Setup: To analyze the effects of each module, we introduce the ablation study which is composed of four components: global feature alignment,i.e., G, local feature alignment, i.e., L, SA node attention ,i.e., A, and the self-training <ref type="bibr" target="#b41">[42]</ref>, i.e., P, to finetune the model with 10% pseudo target labels generated from the target samples with the highest softmax scores.</p><p>Evaluation: Given the labeled samples in source domain and unlabeled samples from target domain for training, all the models would be evaluated on the test set of target domain. All the experiments have been repeated three times and we then report the average top-1 classification accuracy in all tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Classification Results on PointDA-10 Dataset</head><p>The quantitative results and comparison on PointDA-10 dataset are summarized in Table <ref type="table" target="#tab_6">2</ref>. The proposed methods outperform all the general-purpose baseline methods on all adaptation scenarios. Although the largest domain gap appears on M ‚Üí S* and S ‚Üí S*, ours exhibit the large improvement which demonstrates its superiority in aligning different domains. In comparison to the baseline methods, MMD, although defeated by GAN-based methods in 2D vision tasks, is only slightly inferior and even outperforms them in some domain pairs. The phenomenon could be explained as global features limit the upper bound due to its weakness in representing diversified geometry information. In addition, there still exists a great margin between supervised method and DA methods.</p><p>The Table <ref type="table" target="#tab_7">3</ref> represents the class-wise classification results on the domain pair M ‚Üí S. Local alignment helps boost the performance on most of the classes, especially for Monitor and Chair. However, some of the objects, i.e., sofa and bed, are quite challenging for recognition under the UDA scenario where the negative transfer happens as the performance could drop on these classes. Moreover, we observed that the imbalanced training samples do affect the performance of our model and other domain adaptation (DA) models, which makes  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Quantitative Analysis</head><p>Ablation Study: We further analyze the effect of four components proposed in our model (i.e., G, L, S, A). From the Table <ref type="table" target="#tab_6">2</ref>, we find that together with SA node, adding local alignment will bring significant improvement, but only local alignment with fixed node wouldn't improve a lot. Above results substantially validate the effectiveness of our SA nodes that attributes to its self-adapt in region receptive field and significant weight. And an interesting phenomenon in Table <ref type="table" target="#tab_7">3</ref> is that the full version method is defeated by G+L+A in class-wise accuracy. It means that inference of pseudo labels is easily influenced by imbalance distribution of samples in different classes where certain classes would dominate the process of self-training and cause errors accumulation.</p><p>Convergence: We evaluate the convergence of proposed methods as well as baseline methods on ModelNet-to-ShapeNet in Fig. <ref type="figure" target="#fig_3">4</ref>(d). Compared with baselines methods, local alignment helps accelerate the convergence and makes them more stable since being convergent.</p><p>SA Node Feature Extraction Layer: The influence of different layers for mid-level feature extraction is analyzed in Fig. <ref type="figure" target="#fig_3">4</ref>(c) on M ‚Üí S and S* ‚Üí M. Compared with conv1 and conv2 whose features are less semantical, conv3 contains the best mid-level feature for local alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results Visualization</head><p>We visualize the top contributed SA nodes for local alignment of two cross-domain objects to interpret the effectiveness of local feature alignment in Fig. <ref type="figure" target="#fig_3">4</ref>(a)-4(b). The matched nodes are selected from the elements with the highest values from the matrix M = h s i √ó h t j ‚àà R 64√ó64 obtained from Eq. 5. It is easily observed that the SA nodes representing similar geometry structure, i.e., legs, plains, contribute most to local alignment no matter they are between same objects or different objects across domains. It significantly demonstrates the common knowledge learned by SA nodes for local alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a novel 3D Unsupervised Domain Adaptation Network on Point Cloud Data (PointDAN). PointDAN is a specifically designed framework based on multi-scale feature alignment. For local feature alignment, we introduce Self-Adaptive (SA) nodes to represent common geometry structure across domains and apply a GAN-based method to align features globally. To evaluate the proposed model, we build a new 3D domain adaptation benchmark. In the experiments, we have demonstrated the superiority of our approach over the state-of-the-art domain adaptation methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison between 2D-based and 3D-based DA approaches.</figDesc><graphic coords="2,288.06,68.90,92.30,133.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Samples of PointDA-10 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3 .</head><label>3</label><figDesc>Given the access to the three subdatasets, we organize six types of adaptation scenarios which are M ‚Üí S, M ‚Üí S*, S ‚Üí M, S ‚Üí S*, S* ‚Üí M and S* ‚Üí S respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a)-(b) Matched SA nodes for aligning cross-domain objects. (c) Analysis of different feature extraction layers for local feature alignment, and (d) convergence analysis.</figDesc><graphic coords="9,57.00,37.69,193.02,162.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>‚Ä¢</head><label></label><figDesc>We collect a new 3D point cloud DA benchmark, named PointDA-10 dataset, for fair evaluation of 3D DA methods. Extensive experiments on PointDA-10 demonstrate the superiority of our model over the state-of-the-art general-purpose DA methods.</figDesc><table><row><cell>Lichen-Figure2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Global-level Alignment</cell></row><row><cell>Source Target</cell><cell>ùëÅ √ó 3</cell><cell>Encoder ‚Ä¶</cell><cell>Mid-level ‚Ä¶ Farthest Point Sampling</cell><cell>Identity Connection</cell><cell>Feature Origin Feature Interpolated</cell><cell>‚Ä¶ ‚Ä¶</cell><cell>Interpolation</cell><cell>Generator ‚Ä¶</cell><cell>High-level ‚Ä¶</cell><cell>Max Pooling</cell><cell>‚Ä¶</cell><cell>ùë≠ ùüè ùë≠ ùüê Training Adversarial</cell><cell>ùêø ùëêùëôùë†1 ùêø ùëêùëôùë†2 ‚Ä¶ ‚Ä¶ ùêø ùëëùëñùë†</cell></row><row><cell cols="2">2 Related Works</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">2.1 3D Vision Understanding</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="14">Different from 2D vision, 3D vision has various data representation modalities: multi-view, voxel</cell></row><row><cell cols="14">grid, 3D mesh and point cloud data. Deep networks have been employed to deal with the above</cell></row><row><cell cols="14">different formats of 3D data [29, 19, 36, 8]. Among the above modalities, point cloud, represented by</cell></row></table><note><p><p><p>a set of points with 3D coordinates {x, y, z}, is the most straightforward representation to preserve 3D spatial information. Point cloud can be directly obtained by LiDAR sensors, which brings a lot of 3D environment understanding applications from scene segmentation to automatic driving. PointNet</p><ref type="bibr" target="#b21">[22]</ref> </p>is the first deep neural networks to directly deal with point clouds, which proposes a symmetry function and a spatial transform network to obtain the invariance to point permutation. However,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Concat Alignment ‚Ä¶ Source SA Node Feature ‚Ä¶ Target SA Node Feature</head><label></label><figDesc></figDesc><table><row><cell>ùë•ùëê2</cell><cell></cell></row><row><cell>ùë•ùëê3</cell><cell>ùë•ùëê5</cell></row><row><cell>‡∑ú ùë•ùëê</cell><cell>ùë•ùëê4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>SA Node Initialization Geometric-Guided Shift Learning SA Node Update Node Attention Local-level Alignment</head><label></label><figDesc>Illustration of PointDAN which mainly consists of local-level and global-level alignment. local geometric information is vital for describing object in 3D space, which is ignored by PointNet. So recent work mainly focuses on how to effectively utilize local feature. For instance, in PointNet++ [23], a series of PointNet structures are applied to local point sets with varied sizes and local features are gathered in a hierarchical way. PointCNN</figDesc><table><row><cell>‚àÜ ‡∑ú ùë•ùëê</cell></row><row><cell>ùëÅ √ó ùê∑</cell></row><row><cell>Figure 2:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>which is the supremum of L dis in our problem. If decomposing the hypothesis h 1 into G and F 1 , and h 2 into G and F 2 , and fix G, we can get</figDesc><table><row><cell>sup</cell></row><row><cell>h1,h2‚ààH</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Number of samples in proposed datasets.</figDesc><table><row><cell cols="2">Dataset</cell><cell cols="8">Bathtub Bed Bookshelf Cabinet Chair Lamp Monitor Plant</cell><cell>Sofa</cell><cell>Table</cell><cell>Total</cell></row><row><cell>M</cell><cell>Train Test</cell><cell>106 50</cell><cell>515 100</cell><cell>572 100</cell><cell>200 86</cell><cell>889 100</cell><cell>124 20</cell><cell>465 100</cell><cell>240 100</cell><cell>680 100</cell><cell>392 100</cell><cell>4, 183 856</cell></row><row><cell>S</cell><cell>Train Test</cell><cell>599 85</cell><cell>167 23</cell><cell>310 50</cell><cell>1, 076 126</cell><cell cols="2">4, 612 1, 620 662 232</cell><cell>762 112</cell><cell cols="4">158 2, 198 5, 876 17, 378 30 330 842 2, 492</cell></row><row><cell>S*</cell><cell>Train Test</cell><cell>98 26</cell><cell>329 85</cell><cell>464 146</cell><cell>650 149</cell><cell>2, 578 801</cell><cell>161 41</cell><cell>210 61</cell><cell>88 25</cell><cell>495 134</cell><cell cols="2">1, 037 6, 110 301 1, 769</cell></row><row><cell cols="4">4 PointDA-10 Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Bathtub Bed Bookshelf Cabinet Chair Plant Sofa Table ModelNet-10 ShapeNet-10 ScanNet-10 ModelNet-10 ShapeNet-10 ScanNet-10 Lamp Monitor</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Quantitative classification results (%) on PointDA-10 Dataset.</figDesc><table><row><cell></cell><cell cols="8">G L A P M‚ÜíS M‚ÜíS* S‚ÜíM S‚ÜíS* S*‚ÜíM S*‚ÜíS Avg</cell></row><row><cell>w/o Adapt MMD [18] DANN [10] ADDA [32] MCD [26] Ours</cell><cell>‚àö ‚àö ‚àö ‚àö ‚àö ‚àö ‚àö ‚àö ‚àö ‚àö ‚àö ‚àö ‚àö</cell><cell>42.5 57.5 58.7 61.0 62.0 62.5 63.7 64.2</cell><cell>22.3 27.9 29.4 30.5 31.0 31.2 32.1 33.0</cell><cell>39.9 40.7 42.3 40.4 41.4 41.5 44.5 47.6</cell><cell>23.5 26.7 30.5 29.3 31.3 31.5 33.7 33.9</cell><cell>34.2 47.3 48.1 48.9 46.8 46.9 48.2 49.1</cell><cell>46.9 54.8 56.7 51.1 59.3 59.3 63.0 64.1</cell><cell>34.9 42.5 44.2 43.5 45.3 45.5 47.5 48.7</cell></row><row><cell>Supervised</cell><cell></cell><cell>90.5</cell><cell>53.2</cell><cell>86.2</cell><cell>53.2</cell><cell>86.2</cell><cell>90.5</cell><cell>76.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Class-wise classification results (%) on ModelNet to ShapeNet.</figDesc><table><row><cell></cell><cell cols="11">G L A P Bathtub Bed Bookshelf Cabinet Chair Lamp Monitor Plant Sofa Table Avg</cell></row><row><cell>w/o Adapt MMD [18] DANN [10] ADDA [32] MCD [26] Ours</cell><cell>‚àö ‚àö ‚àö ‚àö ‚àö ‚àö ‚àö ‚àö ‚àö ‚àö ‚àö ‚àö ‚àö</cell><cell>59.4 77.1 82.6 84.5 84.8 84.6 85.7 84.7</cell><cell>1.0 0.7 0.4 1.0 4.4 0.8 2.4 1.6</cell><cell>18.4 20.0 20.1 22.9 18.4 19.2 20.4 19.0</cell><cell>7.4 1.6 1.5 2.4 7.7 1.6 1.0 1.3</cell><cell>55.7 63.6 72.1 66.7 74.9 75.6 79.0 81.9</cell><cell>43.5 58.4 52.6 62.8 62.0 61.2 64.2 63.3</cell><cell>84.8 88.8 90.2 83.6 85.6 92.7 90.1 90.5</cell><cell>60.0 83.4 86.7 70.1 80.0 86.3 83.3 82.3</cell><cell>3.4 0.5 1.0 1.8 1.6 0.9 3.6 2.2</cell><cell>39.7 37.3 87.6 48.2 80.2 48.6 86.8 48.3 82.2 50.2 83.4 50.6 83.0 51.3 82.9 51.0</cell></row><row><cell>Supervised</cell><cell></cell><cell>88.9</cell><cell>88.6</cell><cell>47.8</cell><cell>88.0</cell><cell>96.6</cell><cell>90.9</cell><cell>93.7</cell><cell cols="3">57.1 92.7 91.1 83.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3</head><label>3</label><figDesc></figDesc><table /><note><p>slightly noisy. Chair, Table, and Sofa (easily confusing with Bed) cover more than 60% samples in M-to-S scenario which causes the drop of certain classes (e.g., Bed and Sofa).</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Qianqian Ma from Boston University for her helpful theoretical insights and comments for our work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Integrating structured biological data by kernel maximum mean discrepancy</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch√∂lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="49" to="e57" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3D model repository</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Joint distribution optimal transportation for domain adaptation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Courty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rakotomamonjy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3730" to="3739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ScanNet: Richlyannotated 3D reconstructions of indoor scenes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nie√üner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5828" to="5839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantic-transferable weakly-supervised endoscopic lesions segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">MeshNet: mesh neural network for 3D shape representation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8279" to="8286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">GVCNN: Group-view convolutional neural networks for 3D shape recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="264" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.7495</idno>
		<title level="m">Unsupervised domain adaptation by backpropagation</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Connecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="222" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain adaptation for object recognition: An unsupervised approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="999" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">PointCNN: Convolution on X-transformed points</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="820" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transfer feature learning with joint distribution adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">VoxNet: A 3D convolutional neural network for real-time object recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Intelligent Robots and Systems</title>
		<meeting>the IEEE International Conference on Intelligent Robots and Systems</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">PointNet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">PointNet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generatively inferential co-training for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019-10">Oct 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page">533</biblScope>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3723" to="3732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Domain adaptation for vehicle detection from bird&apos;s eye view LiDAR point cloud data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abobakr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Attia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Iskander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nahavandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hossny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08955</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-view convolutional neural networks for 3D shape recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Direct importance estimation with model selection and its application to covariate shift adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Buenau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kawanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1433" to="1440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Low-rank transfer human motion segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1023" to="1034" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SqueezeSegV2: Improved model structure and unsupervised domain adaptation for road-object segmentation from a lidar point cloud</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Robotics and Automation</title>
		<meeting>the International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4376" to="4382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3D shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">PVNet: A joint convolutional network of point cloud and multi-view for 3d shape recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Multimedia Conference on Multimedia Conference</title>
		<meeting>the ACM Multimedia Conference on Multimedia Conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">PVRNet: Point-view relation neural network for 3D shape recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9119" to="9126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Domain adaptation under target and conditional shift</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch√∂lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="819" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for 3d keypoint estimation via view consistency</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="137" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">VoxelNet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vijaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
