<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ABM-SpConv: A Novel Approach to FPGA-Based Acceleration of Convolutional Neural Network Inference</title>
				<funder ref="#_xyuSWGh">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_q3sV8P7 #_maX6CtN #_8EweAAj">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_tHqnZuu">
					<orgName type="full">NNSF of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
							<email>wangdong@bjtu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Ke</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qun</forename><surname>Jia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Soheil</forename><surname>Ghiasi</surname></persName>
							<email>ghiasi@ucdavis.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Davis Davis</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">DAC &apos;19</orgName>
								<address>
									<addrLine>June 2-6, Las Vegas</addrLine>
									<postCode>2019</postCode>
									<region>NV</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ABM-SpConv: A Novel Approach to FPGA-Based Acceleration of Convolutional Neural Network Inference</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3316781.3317753</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hardware accelerators for convolutional neural network (CNN) inference have been extensively studied in recent years. The reported designs tend to utilize a similar underlying architecture based on multiplier-accumulator (MAC) arrays, which has the practical consequence of limiting the FPGA-based accelerator performance by the number of available on-chip DSP blocks, while leaving other resource under-utilized. To address this problem, we consider a transformation to the convolution computation, which leads to transformation of the accelerator design space and relaxes the pressure on the required DSP resources. We demonstrate that our approach enables us to strike a judicious balance between utilization of the on-chip memory, logic, and DSP resources, due to which, our accelerator considerably outperforms state of the art. We report the effectiveness of our approach on a Stratix-V GXA7 FPGA, which shows 55% throughput improvement, while using 6.25% less DSP blocks, compared to the best reported CNN accelerator on the same device.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Convolutional Neural Network (CNN) has become the dominate approach in many artificial intelligence (AI) applications, such as computer vision, speech recognition and robotics. Many studies <ref type="bibr" target="#b10">[11]</ref> have been carried out to design various CNN hardware accelerators for real-time processing. Being able to provide massive computational resources with flexible data precision, lower power dissipation and shorter deployment cycle, FPGA-based CNN inference accelerator has received great attention in various application fields from large-scale data-centers to energy-constrained IoT devices.</p><p>One of the key challenges in designing FPGA-based CNN accelerator is to take full advantage of the on-chip computing resource to speedup multiply-and-accumulate (MAC) operations in the convolution (CONV) and fully-connected (FC) layers, which account for over 99% <ref type="bibr" target="#b8">[9]</ref> of the total operations for most CNNs. According to the way in which convolution computation is implemented, existing designs can be divided into three major categories. The first category of designs, such as <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>  of the convolution computation in Spatial Domain (referred to as SDConv) by using large number of DSP blocks performing massive number of MAC operations in every cycle. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, the design space of SDConv-based accelerator has a computational roof of 2 ? N mac ? Freq, where N mac and Freq denote the number of on-chip MAC units and the operating frequency, respectively. For instance, on a Stratix-V-A7 FPGA which has 256 DSPs, the maximum attainable inference throughput is 204.8 GOP/s under frequency of 200 MHz. (each DSP can perform two 16/8-bit fixedpoint MACs).</p><p>The second category of designs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref> perform convolution in the Frequency Domain (referred to as FDConv). By reducing the number of MAC operations required for convolution, FDConv-based accelerator raises the computational roof over SDConv-based design by a factor of R mac , where R mac is the reduction rate in MAC operation. For instance, up to 69.2% of the MAC operation is saved in <ref type="bibr" target="#b2">[3]</ref>, resulting in a theoretical speedup of 3.3? in peak performance. The throughput achieved by <ref type="bibr" target="#b2">[3]</ref> on a Intel HARP platform is 669.1 GOP/s, which is very close to this roof.</p><p>The third type of designs reduce the number of MAC operations by directly pruning the CNN model <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b7">8]</ref>. Unimportant weights are forced to zero during the training (or fine-tuning) stage so that they will not contribute to computational workload and memory bandwidth in inference. Convolution performed on a pruned CNN model is referred to as Sparse Convolution (SpConv). The design space of SpConv-based accelerators share a similar computational roof of 2 ? R mac ? N mac ? Freq with FDConv-based ones. The performance of the reported SpConv-based FPGA accelerators have not exceed that of <ref type="bibr" target="#b2">[3]</ref>.</p><p>From an architecture view, existing FPGA accelerators tend to utilize a similar underlying architecture based on MAC arrays. The practical consequence is that the on-chip DSPs are exhausted, while To this end, we first develop a new approach which performs the multiplication and accumulation operations of convolution in separated stages. Then, we design a heterogeneous hardware architecture comprised of a "big" accumulator array and a "small" multiplier array on which the two stages of convolution can be efficiently mapped. Our scheme transforms the design space of FPGAbased CNN accelerators to one with a raised computational roof of 2 ? N acc ? Freq, where N acc is the number of accumulators used and N acc is much larger than N mac as shown in Figure <ref type="figure" target="#fig_0">1</ref> In summary, the contributions of this work are:</p><p>? We propose ABM-SpConv, a new sparse convolution scheme which achieves a higher arithmetic intensity for accumulation than multiplication so that the computational roof of the accelerator design space is transformed as accumulatorbound. ? We propose an FPGA accelerator architecture which consists of heterogeneous arrays of accumulators and multipliers to match the distinct computation flow of ABM-SpConv. Several optimization schemes, including semi-synchronous parallel processing and index-based weight encoding, are developed to ensure highly efficient data-path utilization and low external memory bandwidth requirement. ? A complete flow for design space exploration is introduced, and key design steps for finding the optimal hardware parameters are developed. The implemented accelerator achieves 1.55? speedup in inference throughput compared to the stateof-the-art design on a Stratix-V GXA7 FPGA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>CNNs are composed of multiple functional layers <ref type="bibr" target="#b10">[11]</ref>, each of which performs certain type of arithmetic operations on the input image or feature map. As the most compute-intensive layer, convolution computation is comprised of iterative 3-dimensional (3-D) MAC operations as follow:</p><formula xml:id="formula_0">FO m,r ? ,c ? = N -1 n=0 K -1 k=0 K -1 k ? =0 F I n,r ? ?S +k,c ? ?S +k ? ? W m,n,k,k ? (1)</formula><p>where S denotes the convolution stride. F I n,r ,c , FO m,r ? ,c ? andW m,n,k,k ? denote the input and output feature maps and the weight, which are of the size N ?R?C, M?R ? ?C ? , and M?N ?K ?K, respectively. A convolution kernel is defined as the 3-D MAC operation that generates one feature map pixel, while channels refer to the feature matrices of size R ? C or R ? ? C ? . For FC computation, Equation ( <ref type="formula">1</ref>) can be reused by setting R = 1, C = 1 and K = 1, which becomes a 1-D inner-product operation. Table <ref type="table" target="#tab_1">1</ref> gives an example of the dimensional parameters and the number of operations (#OP) for the VGG16 <ref type="bibr" target="#b4">[5]</ref> model that has been widely used as a performance benchmark in the literature. Due to limited space, only the numbers for a few selected layers and the entire CNN are shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE ABM-SPARSE CONVOLUTION</head><p>In this section, we introduce the Accumulate-Before-Multiply Sparse Convolution (ABM-SpConv) scheme. The key idea is to perform accumulate and multiply operations in separated steps so that the arithmetic intensity of multiplication can be reduced by removing redundant operations, which consequently relaxes the demand for DSP units when implemented on FPGA.</p><p>Assuming that weight W is quantized and kept in fixed-point format with q-bit precision, there exists at most Q = 2 q different values for W . By denoting these fixed-point value as Wp , where p = 0, ? ? ? , Q -1, we perform factorization on Equation 1 as follow:</p><formula xml:id="formula_1">FO m,r ? ,c ? = w F I 0 (w) ? W0 + ? ? ? + w F I Q -1 (w) ? WQ-1 = Q -1 p=0 Wp ? w F I p (w)<label>(2)</label></formula><p>where F I p (w) represents the input feature pixels that are multiplied by the same weight Wp in a convolution kernel. Based on this new equation, we propose to conduct the convolution computation in a two-stage flow as follow:</p><p>(1) For each non-zero Wp , find and Accumulate all the feature points F I p (w) in the convolution kernel, producing Q -1 partial products in total;</p><p>(2) Multiply each partial product with corresponding Wp and do a final accumulation to obtain the output value for the current convolution kernel.</p><p>Iteratively repeating step (1) and ( <ref type="formula" target="#formula_1">2</ref>) for all convolution channels will generate all M ? R ? ? C ? output feature map pixels.</p><p>Studies <ref type="bibr" target="#b5">[6]</ref> have shown that the weight can be quantized with 8-bit (or less) precision with less than 1% decrease in inference accuracy. Thus, with a 8-bit quantized weight, our scheme requires at most 256 rather than N ? K ? K multiplications for each convolution kernel (For pruned weight, the actual number is much smaller than 256). On the other hand, accumulation of F I p (w) for Wp = 0 can be easily skipped in the computation flow, which means that hardware implementation can also exploit sparsity to reduce computation complexity and memory footprint for weight storage. In Table <ref type="table" target="#tab_1">1</ref>, we report the number of addition and multiplication required when performing ABM-SpConv on a pruned VGG16 model <ref type="bibr" target="#b6">[7]</ref>. 83.6% of the total operations (accumulate and multiply) is saved compared to SDConv, while the reduction over FDConv <ref type="bibr" target="#b2">[3]</ref> and SpConv <ref type="bibr" target="#b6">[7]</ref> are 47.1% and 50%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">HARDWARE ARCHITECTURE 4.1 Design Challenges</head><p>The new convolution scheme introduces the following novel design challenges: (i) The irregular sparsity pattern among different convolution kernels introduces imbalanced workload, which prevent us from achieving full parallelism of convolution on an accelerator architecture consisting of tightly-synchronized executed processing elements, such as MAC array. (ii) The accumulate and multiply operations have different arithmetic intensity and are carried out in two distinct computation stages. Mapping both operations on a homogeneous hardware architecture causes low utilization of computational resources. (iii) The algorithm also requires random memory access to the feature map data in memory accordingly to the irregular locations of the weights, which degrades the efficiency of external memory bandwidth and requires complicated data flow control in data path design. In the following section, we discuss our proposed hardware architecture, which addresses the aforementioned design challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Architecture Design</head><p>Figure <ref type="figure" target="#fig_1">2-(a)</ref> shows the overall architecture of the proposed ABM-SpConv accelerator, including a task scheduling unit, a fetch/store unit and multiple convolution units (CUs). The task scheduler detects the status of each CU and, whenever there is an idle CU, it launches a new computation task on that CU. As depicted in Figure <ref type="figure">3</ref>, a computation task is defined as a group of convolution operations that are performed on a prefetch window of the input feature map. Each CU has its own loop counter so that it can independently execute tasks with varying workload. Synchronization of the CUs is infrequently conducted, only when feature map buffers are updated with new data. Measured CU utilization data (see Section 6) show that this semi-synchronous CU architecture successfully solves the first design challenge.</p><p>Convolution Unit. To address the second design issue, we propose a heterogeneous CU architecture of two independent arrays  of accumulators and multipliers as shown in Figure <ref type="figure" target="#fig_1">2</ref>-(b). The accumulator array accumulates the input feature map F I p pixels which share the same weight Wp and stores the partial results into following FIFOs. The multipliers then read the partial result, multiply it by the fixed-point Wp and send the product to the Sum/Round logic for final processing. We further separate the accumulators into groups such that every N accumulators share one multiplier in the data-path. During convolution, the multiplier dynamically chooses the outputs of upstream FIFOs as its input operand in a round-robin manner. With a proper setting of FIFO depth, the twostage convolution computation can be efficiently pipelined. This hardware structure enables the accelerator to use N times more accumulators than DSPs to speedup CNN inference computation.</p><p>In the final design, 16-bit accumulator and 16b-by-16b multiplier are adopted to ensure full-precision fixed-point computation and no information loss during convolution, which guarantees the validity of Equation ( <ref type="formula" target="#formula_1">2</ref>). Rounding is performed only once before writing feature map data back to main memory. On-Chip Buffer. Figure <ref type="figure">4</ref> illustrates how the pruned CNN model is encoded and stored in a pair of local buffers. To alleviate the overhead of data-path control, we propose to encode the indexes (n, k, k ? ) of the non-zero weights according to the order of corresponding Wp in the weight buffer (WT-Buffer). Another small buffer (Q-Table ) is designed to store extra information, including the fixed-point value (VAL) of Wp , the corresponding number (NUM) of occurrence of indexes and the total number of occurrence of the encoded weights, which are used by the loop counter and multipliers. A dedicated Address Generator is designed to decode the weight on-the-fly, map the indexes onto the feature map domain   </p><formula xml:id="formula_2">C mem = C 5 + (C 6 ? S ec + C 7 ? N knl ) ? N cu (<label>10</label></formula><formula xml:id="formula_3">)</formula><p>where C 0 to C 7 are platform-dependent constants which can be determined by characterizing the target FPGA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Exploration Flow</head><p>The proposed design space exploration flow is illustrated in Figure <ref type="figure">5</ref>. The flow first analyzes the network structure of the target CNN, encodes the pruned model layer-by-layer according to the proposed weight encoding scheme and determines the buffer sizes of D w and D q . At the same, the ratio of the arithmetic intensity between accumulate and multiply operations is analyzed and N is determined to fit the minimum ratio (see last column in Table <ref type="table" target="#tab_1">1</ref>). The Performance Model is then used to estimate the inference throughput for different values of design parameter N knl . During this stage, preset values are assumed for parameters S ec and N cu . The optimal N knl is selected in a way that normalized performance boost is maximized as shown in Figure <ref type="figure" target="#fig_2">6</ref>.</p><p>In the following stage, several rounds of fast compilation of the design code (OpenCL kernels) are carried out for the target FPGA device, and hardware resource utilization information, including logic, DSP and on-chip memory, are collected. Design constants which characterize the hardware cost of the accelerator are then solved based on the resource information and the Resource Requirement Model.</p><p>In the final stage, the attainable performance is explored in a S ec -N cu design space by using the Performance Model as depicted in Figure <ref type="figure" target="#fig_4">7</ref>. Constraints of full utilization of the DSP and memory resources are applied during exploration. However, a strict budget on logic resource (such as 70%) may leads to failure in FPGA compilation or large degradation in operating frequency. Therefore, several design candidates with close logic utilization ratio are selected for final implementation. Moreover, because pruned CNN models are adopted in the design flow, the size of the encoded weight is much smaller than original model (see Table <ref type="table" target="#tab_4">3</ref>). The external memory bandwidth spent on weight transmission is significantly reduced compared to previous works. By using the Bandwidth Model, we have verified that our design is compute-bound for most FPGA devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>We use the DE5-Net platform for performance evaluation. The onboard FPGA is an Intel Stratix-V GXA7 device, which has 234,720 ALMs, 256 DSP blocks and 2560 M20K memory resources. DDR3 SDRAM is attached to the FPGA providing 12.8 GB/s external memory bandwidth.</p><p>A high-level-synthesis (HLS)-based FPGA design methodology was adopted in hardware implementation. The proposed architecture was modeled in OpenCL kernels and compiled by using the Intel FPGA OpenCL SDK v17.1. FPGA executes all convolution and FC layers, while the remaining layers, such as pooling, LRN and softmax, are executed by the host program on CPU. By adopting pipelined processing, the execution time of CPU were hidden by FPGA. The proposed accelerator is evaluated by running inference computation of two CNNs (AlexNet and VGG16) and the design parameters configured are summarized in Table <ref type="table" target="#tab_4">3</ref>. Both models were pruned by the scheme proposed by Han et al. <ref type="bibr" target="#b6">[7]</ref> and quantized with 8-bit precision <ref type="bibr" target="#b5">[6]</ref> with less than 1% accuracy drop compared to the original model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Comparison with State-of-the-Art</head><p>Table <ref type="table" target="#tab_3">2</ref> summarizes the comparison with state-of-the-art FPGA accelerators. As in other studies, throughput is calculated as the total #OP for spatial convolution of the original model divided by the average inference time. Moreover, in order to make a fair comparison, we only use the number achieved by FPGA accelerator rather than the whole system in the following discussion. Designs of <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> are based on spatial convolution, while the works of <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref> use frequency domain convolution.</p><p>The latest work of <ref type="bibr" target="#b2">[3]</ref> uses a frequency domain convolution scheme which gains 3.3? reduction in MAC operations for both CNN models. For VGG16, the model pruning scheme adopted in our design maintains a similar reduction rate of 3.06?. The implemented accelerator achieves 1.55? speedup in throughput compared to <ref type="bibr" target="#b2">[3]</ref> as a result of being able to utilize 1.6? accumulators to accelerate the convolution computation. Note that, although our scheme quantizes the CNN model in 8-bit, the precision of the datapath is of the same (16-bit) as <ref type="bibr" target="#b2">[3]</ref>. For AlexNet, the pruning scheme adopted by us only reduces the total MAC operations by 2.3? (30%  lower than that of <ref type="bibr" target="#b2">[3]</ref>), but our scheme still improves the inference throughput by 5.4%. When compared with design that implements spatial convolution <ref type="bibr" target="#b12">[13]</ref> on the same device, we achieve considerably 3.8? improvement in throughput when normalized by frequency. To compare with <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b9">[10]</ref> that are implemented on a different type of FPGA device, we further normalize the performance by the number of DSPs used. It is clear that our design shows over 3? advantage in performance density than all three designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>Study of <ref type="bibr" target="#b0">[1]</ref> presented an energy-efficient accelerator that deployed sparse convolutional neural networks on a Artix-7 FPGA. Techniques presented in this work were resource utilization and power optimization orientated. The CNN model evaluated was of low complexity (0.44 GOP) and the performance achieved was only 31.79 GOP/s. In <ref type="bibr" target="#b7">[8]</ref>, the authors reported an design framework which mapped sparse CNNs onto FPGA accelerator. The reported performance was 271.6 GOP/s on a Zynq VC706 FPGA without further information disclosed on resource utilization and working frequency. The work of <ref type="bibr" target="#b1">[2]</ref> presented an algorithm-hardware codesign scheme to improve the efficiency of sparse convolutional layers executed in hardware. A structurally pruned AlexNet model was accelerated on a Virtex VC707 board and only the execution efficiency (64.5%) was disclosed, which was also lower than ours (87% for VGG16 and 81% for AlexNet). For the first time, we present a sparse-convolution-based FPGA accelerator for high-throughput CNN inference that surpasses the performance of state-of-the-art design on the same device.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSIONS</head><p>This work presented the first high-throughput FPGA accelerator design which targeted efficient implementation of sparse convolutional neural network. A new sparse convolution scheme along with an efficient hardware architecture were developed. By transforming the design space of FPGA-based accelerator from MACbound to accumulator-bound, our design successfully achieved 1.55? throughput improvement over the best frequency-domain-convolution-based accelerator on the same FPGA device.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparing the design space of traditional MACbased accelerator with the proposed architecture in a roofline model for Stratix-V-A7 FPGA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The proposed hardware architecture. (a) high-level architecture. (b) architecture of the convolution unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Exploration for the optimal value of N knl . An operating frequency of 200 MHz is assumed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Exploration for the attainable throughput. The example shown is for VGG16 with N knl = 14, N = 4 and Freq = 200 MHz. The constraint for logic utilization is 75%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, directly exploit parallelism</figDesc><table><row><cell>CNN inference throughput</cell><cell>(GOP/s)</cell><cell>SDConv B a n d w &amp; SpConv FDConv New Design Space id t h R o o f DSP Limit</cell><cell>ABM-SpConv</cell><cell>2 Nmac Freq (204.8 GOP/s) 2 Nacc Freq (1046 GOP/s) 2 Rmac Nmac Freq (675 GOP/s) Computational Roof</cell><cell>3.3x speedup achieved by [12] 1.55x speedup achieved by this work</cell></row><row><cell></cell><cell></cell><cell cols="3">Throughput to data communication ratio</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">(GOP/s/byte)</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of the #OP required by different convolution approaches for selected layers and the entire VGG16 model.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Layer Parameters</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>#OP (MOP)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Pruning</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ABM-SpConv</cell></row><row><cell>Layer</cell><cell>C</cell><cell>R</cell><cell cols="2">N K?K</cell><cell>M</cell><cell cols="7">Ratio SDConv FDConv[3] SpConv[7] Acc. Mult. Acc./Mult.</cell></row><row><cell>CONV1_1</cell><cell cols="2">224 224</cell><cell>3</cell><cell>3?3</cell><cell>64</cell><cell>42%</cell><cell>173</cell><cell>52.5</cell><cell cols="2">100 50.3</cell><cell>12.1</cell><cell>4.1</cell></row><row><cell>CONV1_2</cell><cell cols="2">224 224</cell><cell>64</cell><cell>3?3</cell><cell>64</cell><cell>78%</cell><cell>3,699</cell><cell>1,119</cell><cell>814</cell><cell>407</cell><cell>119</cell><cell>3.4</cell></row><row><cell>CONV4_1</cell><cell>28</cell><cell>28</cell><cell>256</cell><cell>3?3</cell><cell>512</cell><cell>68%</cell><cell>1,849</cell><cell>559</cell><cell>592</cell><cell>296</cell><cell>9.23</cell><cell>32.0</cell></row><row><cell>CONV4_2</cell><cell>28</cell><cell>28</cell><cell>512</cell><cell>3?3</cell><cell>512</cell><cell>73%</cell><cell>3,699</cell><cell>1,119</cell><cell>998</cell><cell>499</cell><cell>7.95</cell><cell>62.7</cell></row><row><cell>FC6</cell><cell>1</cell><cell cols="2">1 25088</cell><cell cols="2">1?1 25088</cell><cell>96%</cell><cell>205</cell><cell>205</cell><cell cols="2">8.23 4.11</cell><cell>0.037</cell><cell>111</cell></row><row><cell>FC7</cell><cell>1</cell><cell>1</cell><cell>4096</cell><cell>1?1</cell><cell>4096</cell><cell>96%</cell><cell>33.6</cell><cell>33.6</cell><cell cols="2">1.34 0.67</cell><cell>0.021</cell><cell>31.9</cell></row><row><cell>Entire CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30,941</cell><cell>9,531</cell><cell>10,082</cell><cell cols="2">5,040</cell><cell></cell></row><row><cell>#OP Saved</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0%</cell><cell>69.2%</cell><cell>67.4%</cell><cell cols="2">83.6%</cell><cell></cell></row><row><cell cols="7">leaving other resource under-utilized. In this paper, we improve</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">the performance of FPGA-based CNN inference accelerator beyond</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">MAC-based designs by transforming the accelerator design space</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">in which the computational roof is bound by accumulator resource</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">rather than MAC. The insight enabling this idea is that quantized</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">CNN models only have a fixed number of possible values (For in-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">stance, 16 values for a 4-bit fixed-point number), so many multi-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">plications performed in convolution can be avoided by factorization.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison with state-of-the-art FPGA CNN accelerators.</figDesc><table><row><cell></cell><cell>[13]</cell><cell>[12]</cell><cell>[4]</cell><cell>[10]</cell><cell>[3]</cell><cell>[3]</cell><cell cols="2">Proposed</cell></row><row><cell></cell><cell>SDConv</cell><cell>SDConv</cell><cell>SDConv</cell><cell>FDConv</cell><cell>FDConv</cell><cell>FDConv</cell><cell cols="2">ABM-SpConv</cell></row><row><cell>CNN Model</cell><cell>AlexNet</cell><cell>VGG16</cell><cell>VGG16</cell><cell>AlexNet</cell><cell>AlexNet</cell><cell>VGG16</cell><cell>AlexNet</cell><cell>VGG16</cell></row><row><cell>FPGA</cell><cell>Stratix-V</cell><cell>Arria-10</cell><cell>Arria-10</cell><cell>Arria-10</cell><cell>Stratix-V</cell><cell>Stratix-V</cell><cell>Stratix-V</cell><cell>Stratix-V</cell></row><row><cell></cell><cell>GXA7</cell><cell>GT1150</cell><cell>GX1150</cell><cell>GX1150</cell><cell>GXA7</cell><cell>GXA7</cell><cell>GXA7</cell><cell>GXA7</cell></row><row><cell>Freq. (MHz)</cell><cell>100</cell><cell>231</cell><cell>385</cell><cell>303</cell><cell>200</cell><cell>200</cell><cell>202</cell><cell>204</cell></row><row><cell cols="3">Model Precision (bit) 8-16 (fixed) 8-16 (fixed)</cell><cell>16 (fixed)</cell><cell>16 (float)</cell><cell>16 (fixed)</cell><cell>16 (fixed)</cell><cell>8 (fixed)</cell><cell>8 (fixed)</cell></row><row><cell cols="3">Logic Usage (ALM) 121K (52%) 313K (73%)</cell><cell>-</cell><cell>246K (58%)</cell><cell>107K (46%)</cell><cell cols="3">107K (46%) 170K (73%) 160K (68%)</cell></row><row><cell cols="2">DSP Usage 256 (100%)</cell><cell>1500 (98%)</cell><cell>1378 (91%)</cell><cell>1476 (97%)</cell><cell>256 (100%)</cell><cell>256 (100%)</cell><cell>243 (95%)</cell><cell>240 (94%)</cell></row><row><cell cols="2">On-chip Memory (M20K) 1552 (61%)</cell><cell>1668 (61%)</cell><cell>1450 (53%)</cell><cell>2487 (92%)</cell><cell>1377 (73%)</cell><cell>1377 (73%)</cell><cell cols="2">2460 (96%) 2435 (95%)</cell></row><row><cell>Design Methodology</cell><cell>RTL</cell><cell>RTL</cell><cell>RTL+OpenCL</cell><cell>OpenCL</cell><cell>RTL</cell><cell>RTL</cell><cell>OpenCL</cell><cell>OpenCL</cell></row><row><cell>Throughput (GOP/s)</cell><cell>134.1</cell><cell>1171</cell><cell>1790</cell><cell>1382</cell><cell cols="2">663.5 1 (780.6 2 ) 662.3(669.1)</cell><cell>699</cell><cell>1029</cell></row><row><cell>Perf. Density (GOP/s/DSP)</cell><cell>0.52</cell><cell>0.78</cell><cell>1.29</cell><cell>0.94</cell><cell>2.59</cell><cell>2.58</cell><cell>2.87</cell><cell>4.29</cell></row><row><cell cols="2">1 Performance achieved by FPGA accelerator.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p>2 </p>Overall system performance.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Design parameters and size of encoded weights.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Design Parameter</cell><cell></cell><cell></cell><cell cols="2">Weight Size (MB)</cell></row><row><cell></cell><cell>N k nl</cell><cell>Ncu</cell><cell>N</cell><cell>S e c</cell><cell>D f</cell><cell>Dw</cell><cell>Dq</cell><cell>Original</cell><cell>Encoded</cell></row><row><cell>AlexNet</cell><cell>14</cell><cell>3</cell><cell>4</cell><cell>20</cell><cell>1152</cell><cell>1024</cell><cell>128</cell><cell>61</cell><cell>11.9</cell></row><row><cell>VGG16</cell><cell>14</cell><cell>3</cell><cell>4</cell><cell>20</cell><cell>1568</cell><cell>2048</cell><cell>128</cell><cell>138</cell><cell>26.4</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work was partially supported by <rs type="funder">NNSF of China</rs> Grants <rs type="grantNumber">NO.61-574013</rs>, <rs type="grantNumber">61532005</rs>, <rs type="grantNumber">61702286</rs>, <rs type="grantNumber">61503300</rs> and the <rs type="funder">NSF</rs> of Tianjin NO.<rs type="grantNumber">18-JCYBJC15600</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_tHqnZuu">
					<idno type="grant-number">NO.61-574013</idno>
				</org>
				<org type="funding" xml:id="_q3sV8P7">
					<idno type="grant-number">61532005</idno>
				</org>
				<org type="funding" xml:id="_maX6CtN">
					<idno type="grant-number">61702286</idno>
				</org>
				<org type="funding" xml:id="_xyuSWGh">
					<idno type="grant-number">61503300</idno>
				</org>
				<org type="funding" xml:id="_8EweAAj">
					<idno type="grant-number">18-JCYBJC15600</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>2 0 0</p><p>The proposed sparse weight encoding scheme. A simplified case for M = 1, N = 2, K = 3. Weighs are quantized in 3-bit, i.e., 1 sign bit and 2 integer bits.</p><p>and load continuous data stream from the feature map buffer (FT-Buffer). As illustrated in Figure <ref type="figure">3</ref>, batched feature maps are cached in terms of prefetch window and feature matrices are vectorized so that data-level parallelism is exploited to further increase the throughput of the accelerator. The width of the local buffers FT-Buffer, WT-Buffer and Q-Table <ref type="table">are 8</ref> ? S ec , 16 and 16 bits, respectively.</p><p>Design Parameters. The proposed architecture can be configured by the following design parameters to achieve flexible performance and hardware cost:</p><p>(1) N cu -the number of parallel CUs; N knl -the maximum number of convolution operations that can be executed in parallel on one CU; N -the number of accumulators that share the same multiplier. (2) S ec -the width of the vectorized input data.</p><p>(3) D f , D w and D q -the depth of the local feature, weight and Q-Table buffers, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DESIGN SPACE EXPLORATION</head><p>In this section, we define the mathematical models for design space exploration and then, present the complete flow and show how the models are applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Performance and Resource Estimation Models</head><p>The Performance Model, Bandwidth Model and Resource Requirement Model used in the proposed design space exploration flow are defined as follow:  Performance Model. The theoretical execution time for the l-th convolution layer is calculated by:</p><p>where #OP l is the number of accumulations performed in layer l.</p><p>Then, the average performance (image/s) can be estimated by</p><p>Bandwidth Model. As show in Figure <ref type="figure">3</ref>, the whole input feature map for the l-th layer is processed after G r l ? G c l times of prefeching, where G c l and G r l equal</p><p>Consequently, the total amount (Byte) of feature map data that are transmitted from external memory per image is</p><p>The corresponding amount of encoded weight that is fetched per image (assuming a minimum batch size of S ec ) is</p><p>where P l denotes the pruning rate of the l-th layer. In total, the average external memory bandwidth can be estimated by</p><p>Resource Requirement Model. The following equations are used to estimate the required hardware resources, including logic, DSP and on-chip memory, respectively.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SPARCNet: A Hardware Accelerator for Efficient Deployment of Sparse Convolutional Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Emerg. Technol. Comput. Syst</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Running sparse and low-precision neural network: When algorithm meets hardware</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASP-DAC</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Framework for Generating High Throughput CNN Implementations on FPGAs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FPGA</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving the Performance of OpenCL-based FPGA Accelerator for Convolutional Neural Network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FPGA</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ristretto: A Framework for Empirical Study of Resource-Efficient Inference in Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gysel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5784" to="5789" />
			<date type="published" when="2018-11">2018. Nov 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<idno>CoRR abs/1510.00149</idno>
		<ptr target="http://arxiv.org/abs/1510.00149" />
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An FPGA Design Framework for CNN Sparsification and Acceleration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FCCM</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Designing Energy-Efficient Convolutional Neural Networks Using Energy-Aware Pruning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<author>
			<persName><forename type="first">U</forename><surname>Aydonat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FPGA</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient Processing of Deep Neural Networks: A Tutorial and Survey</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Automated Systolic Array Architecture Synthesis for High Throughput CNN Inference on FPGAs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Scalable and modularized RTL compilation of Convolutional Neural Networks onto FPGA</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
