<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Corpus for Understanding and Generating Moral Stories</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jian</forename><surname>Guan</surname></persName>
							<email>guan19@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">The CoAI group</orgName>
								<orgName type="department" key="dep2">DCST; Institute for Artificial Intelligence; State Key Lab of Intelligent Technology and Systems</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziqi</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">The CoAI group</orgName>
								<orgName type="department" key="dep2">DCST; Institute for Artificial Intelligence; State Key Lab of Intelligent Technology and Systems</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
							<email>aihuang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">The CoAI group</orgName>
								<orgName type="department" key="dep2">DCST; Institute for Artificial Intelligence; State Key Lab of Intelligent Technology and Systems</orgName>
								<orgName type="department" key="dep3">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Corpus for Understanding and Generating Moral Stories</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Teaching morals is one of the most important purposes of storytelling. An essential ability for understanding and writing moral stories is bridging story plots and implied morals. Its challenges mainly lie in: (1) grasping knowledge about abstract concepts in morals, (2) capturing inter-event discourse relations in stories, and (3) aligning value preferences of stories and morals concerning good or bad behavior. In this paper, we propose two understanding tasks and two generation tasks to assess these abilities of machines. We present STORAL, a new dataset of Chinese and English human-written moral stories. We show the difficulty of the proposed tasks by testing various models with automatic and manual evaluation on STORAL. Furthermore, we present a retrieval-augmented algorithm that effectively exploits related concepts or events in training sets as additional guidance to improve performance on these tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Stories play an essential role in one's moral development <ref type="bibr">(Vitz, 1990)</ref>. For example, individuals usually learn morals from life experiences or literature such as fables and tell their morals by representing their lived experience in a narrative form <ref type="bibr" target="#b46">(Tappan and Brown, 1989)</ref>. Accordingly, it is a crucial ability for humans to bridge abstract morals and concrete events in stories. However, this ability has not yet been investigated for machines.</p><p>There have been many tasks proposed for evaluating story understanding and generation, including story ending selection <ref type="bibr" target="#b33">(Mostafazadeh et al., 2016)</ref> and story generation from short prompts <ref type="bibr" target="#b7">(Fan et al., 2018)</ref>. Unlike these tasks, which focus on reasoning plots from context, we emphasize the ability to associate plots with implied morals. As exemplified in Table <ref type="table">1</ref>, the challenges mainly lie in (1) Table <ref type="table">1</ref>: An example in STORAL grasping knowledge about abstract concepts (e.g., "unity," "strength") and relations among them (e.g., "is") in morals; (2) capturing inter-event discourse relations in stories (e.g., the contrast between endings of the "cows" when they are "united" and "divided"); and (3) aligning value preferences <ref type="bibr" target="#b22">(Jiang et al., 2021)</ref> of stories and morals (e.g., the story implies support for "unity", not opposition, which agrees with "is strength" in the moral). To test these abilities of machines, we propose two understanding tasks and two generation tasks. Both understanding tasks require selecting the correct moral from several candidates given a story. And they have respective candidate sets for testing machines in two aspects, including concept understanding (MOCPT for short) and preference alignment (MOPREF for short). The generation tasks require concluding the moral of a story (ST2MO for short), and conversely generating a coherent story to convey a moral (MO2ST for short).</p><p>Furthermore, we collected a new dataset named STORAL composed of 4k Chinese and 2k English human-written stories paired with morals through human annotation to address the above challenges. We call the Chinese dataset STORAL-ZH and the English dataset STORAL-EN, respectively. And we construct datasets for the proposed tasks based on STORAL. Our focus of morals is on the social set of standards for good or bad behavior and character, or the quality of being right, honest or accept-arXiv:2204.09438v1 [cs.CL] 20 Apr 2022 able <ref type="bibr" target="#b21">(Ianinska and Garcia-Zamor, 2006)</ref>. We conduct extensive experiments on the proposed tasks. Furthermore, we present a retrieval-augmented algorithm to improve model performance by retrieving related concepts or events from training sets as additional guidance. However, the experiment results demonstrate that existing models still fall short of understanding and generating moral stories, which requires a better modeling of discourse and commonsense relations among concrete events and abstract concepts 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Story Datasets ROCStories <ref type="bibr" target="#b33">(Mostafazadeh et al., 2016)</ref> and WritingPrompts <ref type="bibr" target="#b7">(Fan et al., 2018)</ref> are two frequently used story datasets in related studies. The former consists of artificial five-sentence stories regarding everyday events, while the latter contains fictional stories of 1k words paired with short prompts. Besides, some recent works collected extra-long stories such as roleplayerguild <ref type="bibr" target="#b31">(Louis and Sutton, 2018)</ref>, PG-19 <ref type="bibr" target="#b38">(Rae et al., 2020)</ref>, and STORIUM <ref type="bibr" target="#b0">(Akoury et al., 2020)</ref>. <ref type="bibr" target="#b13">Guan et al. (2022)</ref> proposed a collection of Chinese stories. These stories usually aim to narrate a coherent event sequence but not convince readers of any morals.</p><p>Story Understanding and Generation There have been many tasks proposed for evaluating story understanding and generation. Firstly, some works tested the machinery commonsense reasoning ability regarding inter-event causal and temporal relations through story ending selection <ref type="bibr" target="#b33">(Mostafazadeh et al., 2016)</ref>, story ending generation <ref type="bibr" target="#b16">(Guan et al., 2019)</ref> and story completion <ref type="bibr">(Wang and Wan, 2019)</ref>. Secondly, a series of studies focused on the coherence of story generation <ref type="bibr" target="#b7">(Fan et al., 2018;</ref><ref type="bibr">Yao et al., 2019;</ref><ref type="bibr">Guan et al., 2020)</ref>. Another line of works concentrated on controllability to impose specified attributes into story generation. These attributes involved outlines <ref type="bibr" target="#b40">(Rashkin et al., 2020)</ref>, emotional trajectories <ref type="bibr" target="#b2">(Brahman and Chaturvedi, 2020)</ref> and story styles <ref type="bibr" target="#b24">(Kong et al., 2021)</ref>. Our tasks investigate not only the above aspects but also the ability to understand abstract concepts and reason value preferences of stories.</p><p>A task similar to ST2MO is text summarization (Finlayson, 2012) since both tasks require generating a short text to condense crucial information 1 All data and evaluation scripts are available at https: //github.com/thu-coai/MoralStory. of a long text. But summarization requires reorganizing a few words of the original text instead of concluding a character-independent moral. For example, a plausible summary of the story in Table <ref type="table">1</ref> is "Four cows were killed by two tigers and a lion" (generated by BART Large <ref type="bibr" target="#b25">(Lewis et al., 2020)</ref> finetuned on a summarization dataset XSUM <ref type="bibr" target="#b34">(Narayan et al., 2018)</ref>), which includes specific characters and events of the original story. Moreover, MO2ST is similar to persuasive essay generation <ref type="bibr" target="#b45">(Stab and Gurevych, 2017)</ref>, which also requires conveying a viewpoint in generated texts. However, persuasive essays usually convince readers by directly presenting arguments but not narrating a story.</p><p>Morals <ref type="bibr" target="#b18">Haidt and Joseph (2004)</ref> provided a theoretical framework named Moral Foundations Theory (MFT) to summarize five basic moral foundations such as "Care/Harm," "Fairness/Cheating," etc. Based on the theory, recent studies have explored to classify the moral foundations of partisan news <ref type="bibr" target="#b10">(Fulgoni et al., 2016)</ref>, tweets <ref type="bibr" target="#b23">(Johnson and Goldwasser, 2018;</ref><ref type="bibr" target="#b20">Hoover et al., 2020)</ref>, and crowd-sourced texts <ref type="bibr" target="#b36">(Pavan et al., 2020</ref><ref type="bibr">). And Volkova et al. (2017)</ref> proposed identifying suspicious news based on the features of moral foundations. However, we focus on morals which are free-form texts far beyond the scope of the five categories in MFT. In addition, recent studies proposed multiple datasets for machine ethics research such as SBIC <ref type="bibr" target="#b43">(Sap et al., 2020)</ref>, Social Chemistry <ref type="bibr" target="#b9">(Forbes et al., 2020)</ref>, Moral Stories <ref type="bibr" target="#b6">(Emelin et al., 2020)</ref>, ETHICS <ref type="bibr" target="#b19">(Hendrycks et al., 2021)</ref> and Scruples <ref type="bibr" target="#b32">(Lourie et al., 2021)</ref>. But these datasets focus more on how machines behave ethically in some scenario, while STORAL emphasizes the ability to conclude the moral implied by a story. Moreover, most cases in these datasets consist of short texts of descriptive ethical behavior, typically in the form of one sentence. In contrast, STORAL provided longer and more context-specific stories for moral understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">STORAL Dataset</head><p>We collected STORAL from multiple web pages of moral stories. All stories are allowed to use and redistribute for research and have been reviewed by the website editors as stated on the pages. We show the full list of links to these pages in Section A.1. After de-duplication, we collected 19,197 Chinese and 2,598 English raw texts. Then we adopted human annotation for decoupling the story and moral in each raw text. Due to resource limitations, we only constructed 4,209 Chinese and 1,779 English story-moral pairs. We will first show the details of human annotation, then present the topic analysis and statistics of STORAL, and finally describe the details of dataset construction for the proposed tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Human Annotation</head><p>To narrow down our focus, we define a story as a series of coherent events involving several interrelated characters, and implies support or opposition of some behavior. Such a definition constrains the story to exhibit a moral without any explicit arguments. And we define a moral as a judgment to describe what the story implies concerning good or bad behavior. Note that we do not require morals in STORAL to be always reflective of normatively virtuous behavior. We emphasize that the morals should align with the story. Then, a key issue is how to extract the story and moral from a raw text. We observe that there are no markers such as "The story tells us" to separate the story and moral in most cases. The moral may be tightly weaved into the plot (e.g., included in a dialogue). Therefore, we adopted human annotation for this extraction task. We hired a commercial team to annotate STORAL-ZH. All annotators are native Chinese speakers and well trained for our task. For STORAL-EN, we hired three graduates with good English language proficiency. We did not use AMT since it is inconvenient to train online annotators. Figure <ref type="figure" target="#fig_0">1</ref> shows the annotation pipeline.</p><p>We first ask annotators to judge whether the raw text contains a story and moral and whether they meet our constraints shown in Figure <ref type="figure" target="#fig_0">1</ref>. We show the examples given to the annotators to inform them of our requirements for stories and morals in Section A.2. If the constraints are not met, we then ask annotators to refine the story and moral.</p><p>In the refinement stage, annotators have to clean up the data with following heuristics: (1) refusing examples which may violate general ethical principles (e.g., discrimination); (2) deleting noisy words (e.g., links, codes); (3) refining the stories and morals to be coherent and formal. And to ensure the quality of collected data, annotators may refuse to refine the example if it requires much creative writing. Finally, we review the annotation results and provide detailed feedback to the annotators before approving their submissions. We show an annotation example in Table <ref type="table">2</ref>.</p><p>Raw Text: A man whowWw.xxx.c0Mlived a long time ago believed that he could read the future in the stars. He called himself an Astrologer, and spent his time at night gazing at the sky. One evening he was walking along the open road outside the village. His eyes were fixed on the stars. He thought he saw there that the end of the world was at hand, when all at once, down he went into a hole full of mud and water. There he stood up to his ears, in the muddy water, and madly clawing at the slippery sides of the hole in his effort to climb out. His cries for help soon brought the villagers running. As they pulled him out of the mud, one of them said:"You pretend to read the future in the stars, and yet you fail to see what is at your feet! This may teach you to pay more attention to what is right in front of you, and let the future take care of itself.""what use is it? " said another, " to read the stars, when you can't see what's right here on the earth?" Story: A man who lived a long time ago believed that he could read ? ? ? As they pulled him out of the mud, one of them said: "You pretend to read the future in the stars, and yet you fail to see what is at your feet!" Moral: Pay more attention to what is right in front of you, and let the future take care of itself. Table <ref type="table">2</ref>: An example for extracting the story and moral from a raw text. We highlight the words which should be revised in the raw text in italic. And the moral in the raw text is bold. To save space, we replace some events with "? ? ? " in the story.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Topic Analysis</head><p>To provide insight into the taxonomy of morals within STORAL, we adopt LDA <ref type="bibr" target="#b1">(Blei et al., 2003)</ref> for topic modeling of morals. Let B denote the number of topics and V denote the vocabulary size. Based on the variational parameter for topic word distribution ? ? R B?V , we determine B as the minimum value that makes the following formula</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic Words</head><p>Examples ?? (understand), ?? (also), ?? (know), ?? (method), ? ? (gain), ?? (protect), ?? (brain), ?? (able), ?? (pay), ?? (progress) ?????????????????????????????(When making mistakes, we must understand the overall situation. And we are able to have a definition of things only when knowing the overall situation.) ? ? (not), ? ? ? (must), ? ? (danger), ? ? (when), ?? (treat), ?? (safety), ? ? (any way), ? ? (well), ? ? (learn), ?? (encounter) ???????"??"?????"??"?????????????? ???????? (Keep in mind the word "safety" in your life, and do not take any chances to treat safety as a joke.) ?? (thing), ? ? (able), ?? (do well), ? ? (excellent), ?? (should), ? ? (achieve), ? ? (self-confident), ? ? (somewhat), ?? (unable), ?? (may) ?????????????????(Do what you should do and be your own master.) ? ? (when), ? ? (actually), ? ? (many), ?? (discover), ? ? (wish), ? ? (happen), ? ? (life), ? ? (already), ? ? (hurt), ?? (may) ?????????????????????????(We should be good at discovering ourselves instead of waiting for others to do.)  holds true for any b ? {1, 2, ? ? ? , B}:</p><formula xml:id="formula_0">?? (encounter), ? ? (question), ?? (difficulty), ? ? (solve) ? ? (think), ?? (help), ? ? (when), ? ? (should), ?? (give), ?? (brain) ?????????????????????????????????? ??????????(</formula><formula xml:id="formula_1">s b = v?V (k) b ? bv V v=1 ? bv ? h, V (k) b = argmax V * (k) b v?V * (k) b ? bv ,</formula><p>where ? bv is the element at the b-th row and v-th</p><formula xml:id="formula_2">column of ?, k ? {1, 2, ? ? ? , V } is the size of the top-k vocabulary V (k)</formula><p>b , and h ? [0, 1] is a predefined threshold. s b is used to measure the specificity of the b-th topic. Intuitively, the larger s b , the more specific the topic. We set k to 20 and h to 0.5. Finally, we derive 40/24 topics for STORAL-ZH/STORAL-EN, respectively. And the minimum proportion of examples of one topic is 1.6%/3.2% for STORAL-ZH/STORAL-EN, respectively.</p><p>Table <ref type="table" target="#tab_1">3</ref> shows the topic words in V (10) of each topic and two morals assigned to each topic with the highest probabilities for the five topics with the largest specificity scores. The topics cover diverse situations ranging from facing others ("honesty," "help"), parents ("love"), ourselves ("self-help," "self-discovery") to facing difficulties ("think") and danger ("safety"). And examples of the same topic present related semantics to some extent, such as "being honest" and "not believing liars" for the first topic in STORAL-EN. We also show the analysis of high-frequency words of stories and morals in Section A.3 and discussion about the commonsense and discourse relations in stories in Section A.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dataset Statistics of STORAL</head><p>Table <ref type="table" target="#tab_3">4</ref> shows the statistics of STORAL. We regard the unlabeled data which contain entangled stories and morals as an in-domain resource for research on unsupervised or semi-supervised learning for the proposed tasks. And the data are also suitable for learning to generate morals stories where the morals are weaved naturally into the story plots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Task-Specific Dataset Construction</head><p>Based on STORAL, we build task-specific datasets for our understanding tasks (MOCPT and MOPREF) and generation tasks (ST2MO and MO2ST). We randomly split the labeled data in STORAL-ZH and STORAL-EN for training/validation/testing by 8:1:1 and 3:1:1, respectively.  descriptions and data sizes.</p><p>MOCPT It requires selecting the correct moral from five candidates given a story. We constructed the dataset by taking the original moral as the correct candidate and four negatively sampled morals as incorrect candidates for each example. To avoid more than one plausible candidate, we ensured that the negative morals are assigned to different topics from the original one by the LDA model (Section 3.2). In this way, MOCPT can effectively test the ability to distinguish different concepts.</p><p>MOPREF It requires selecting the correct moral from two candidates. Its difference from MOCPT is that we created the incorrect candidate by substituting one random token in the original moral to its antonym. For example, the moral "unity is strength" can be transformed to "unity is weakness". We perform the transformation using a rulebased method <ref type="bibr" target="#b41">(Ribeiro et al., 2020)</ref>. Because there exist examples where no words have antonyms, the number of examples for MOPREF are a little fewer than MOCPT. MOPREF will serve for testing the ability to capture the value preference of stories.</p><p>ST2MO It requires generating the moral of a given story. We regard the original story as input and the original moral as target output.</p><p>MO2ST It requires generating a story to convey a given moral. Unfortunately, automatic evaluation for open-ended story generation is still highly challenging due to the notorious one-to-many issue (Zhao et al., 2017): There may be multiple plausible stories with the same moral. For example, the moral in Table <ref type="table">1</ref> can also be conveyed by another story: "bees unite to build their beehives." Such openness makes automatic metrics unreliable for quality evaluation <ref type="bibr">(Guan and Huang, 2020)</ref>.</p><p>To alleviate this issue, we extract the first sentence and an outline from a target story, and pair them with the moral as input for generating the story. We follow <ref type="bibr" target="#b40">Rashkin et al. (2020)</ref> to extract a set of at most eight phrases from a story through RAKE <ref type="bibr" target="#b42">(Rose et al., 2010)</ref> as the outline. We set the maximum number of words in each phrase to eight. We also filtered those phrases that are substrings of others. For example, the outline for the story in Table <ref type="table">1</ref> is {"lions," "friends fought," "good friends," "grazed," "perfect opportunity"}. Finally, for STORAL-ZH/STORAL-EN, the average number of phrases for each example is 7.5/6.8 and the average number of words in each phrase is 2.87/2.44, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Retrieval Augmentation</head><p>A critical challenge for tackling the proposed tasks is the sparsity of morals and events makes it difficult to learn relations between them. Prior studies have shown that retrieval improves performance towards infrequent data points across various tasks such as open-domain question answering <ref type="bibr" target="#b3">(Chen et al., 2017</ref>) and text classification <ref type="bibr" target="#b28">(Lin et al., 2021)</ref>. We present a retrieval-augmented algorithm that exploits the moral-event relations in training sets. We illustrate our model for the MOPREF task in Fig-  ure 2. Our models for other tasks are similar.</p><p>For both MOCPT and MOPREF, we encode the story and candidates using an input encoder, and then predict a probability distribution over the candidates by normalizing the dot-product scores between the representations of the story and each candidate. We optimize the model by minimizing the cross-entropy loss. We insert special tokens [S] and [C] before the story and each candidate, respectively, and take the corresponding hidden states as their representations. Furthermore, we propose to retrieve related concepts from the training set using the input story. We encode the story using a query encoder, then take the output as the query to retrieve m most related stories based on a story index, i.e., a set of dense vectors as the representations of stories in the training set. We adopt BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> followed by a mean-pooling layer to build the query encoder and story index, which are frozen in the training stage. Finally, we extract the nouns, verbs, adjectives and adverbs from the morals of the top-m stories and lemmatize them as the retrieved concepts. We feed the concepts together with the original input to the input encoder. For example, the retrieved concepts for the story in Table <ref type="table">1</ref> include "support" and "strength", which may serve as additional guidance for models' prediction.</p><p>The retrieval-augmented algorithm can easily adapt to the generation tasks. For ST2MO, we take the input story paired with the retrieved concepts into the encoder and then generate the output using the decoder. And for MO2ST, we use the input moral as the query to retrieve top-m stories, and regard their outlines as the retrieved additional information to guide the subsequent story generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluated Models</head><p>We evaluated the following baselines for the understanding tasks: BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>, RoBERTa <ref type="bibr" target="#b29">(Liu et al., 2019)</ref> and T5 <ref type="bibr" target="#b39">(Raffel et al., 2020)</ref>. When evaluating T5, we feed the input to both the encoder and decoder of T5 and optimize the model using the cross-entropy loss. To investigate potential biases of the proposed datasets, we added a baseline called BERT w/o story, which is fine-tuned to make prediction without taking the story as input. For the generation tasks, we evaluated ConvS2S <ref type="bibr" target="#b11">(Gehring et al., 2017)</ref>, Fusion <ref type="bibr" target="#b7">(Fan et al., 2018)</ref>, GPT2 <ref type="bibr" target="#b37">(Radford et al., 2019)</ref> and T5, which are trained or fine-tuned with the standard language modeling objective. Moreover, we also evaluate a task-specific model PlotMachines (PM for short) <ref type="bibr" target="#b40">(Rashkin et al., 2020)</ref>, which is proposed for tackling outline-conditioned generation by tracking the dynamic plot states. We use GPT2 as the backbone model of PM.</p><p>We also design models to test the adaption of the unlabeled data of STORAL to the proposed tasks. Specifically, we first post-train RoBERTa and T5 on the unlabeled data with their original pretraining objectives, respectively (i.e., masked language model and text infilling) and then fine-tune them on the labeled data for the downstream tasks <ref type="bibr" target="#b17">(Gururangan et al., 2020)</ref>. We call the baselines RoBERTa-Post and T5-Post. We perform our retrieval-augmented algorithm based on the post-trained models, called RA-RoBERTa and RA-T5, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiment Settings</head><p>We implement the pretrained models based on the codes and pretrained checkpoints of Hugging-Face's Transformers <ref type="bibr">(Wolf et al., 2020)</ref>. We use LongLM base <ref type="bibr" target="#b13">(Guan et al., 2022)</ref> as the T5 model for experiments on STORAL-ZH, and set all pretrained models to the base version due to limited computational resources. As for the hyperparameters, we set the batch size to 16, the maximum sequence length to 1,024, the learning rate to 3e-5, m to 10 for our retrieval-augmented model. We generate outputs using top-k sampling <ref type="bibr" target="#b7">(Fan et al., 2018)</ref> with k = 40 and a softmax temperature of 0.7 <ref type="bibr" target="#b12">(Goodfellow et al., 2016)</ref>. We show more details in Section B.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Automatic Evaluation</head><p>Evaluation Metrics We adopt accuracy to evaluate the understanding tasks. For generation tasks, we do not use perplexity since perplexity scores are not comparable among models with different vocabularies. We adopt the following metrics for automatic evaluation: (1) BLEU (B-n): It is used to measure n-gram overlaps (n = 1, 2) between generated and ground-truth texts <ref type="bibr" target="#b35">(Papineni et al., 2002)</ref>. (2) BERTScore-F1 (BS): It is used to measure the semantic similarity between generated and ground-truth texts <ref type="bibr">(Zhang et al., 2019)</ref>. (3) Repetition (R-n): It calculates the ratio of texts that repeat at least one n-gram in all generated texts <ref type="bibr" target="#b44">(Shao et al., 2019)</ref>. (4) Distinct (D-n): It measure the diversity using the percentage of distinct n-grams to all n-grams in generated texts <ref type="bibr" target="#b26">(Li et al., 2016)</ref>. For both R-n and D-n, we set n = 2 for ST2MO and n = 4 for MO2ST considering the much shorter length of morals than stories. Besides, we also report the average number of generated words (Len).</p><p>We also adopt the following metrics for automatic evaluation of MO2ST: (1) Coverage (Cov): It computes Rouge-L recall <ref type="bibr" target="#b27">(Lin, 2004)</ref> between generated stories and phrases in the corresponding outlines. A higher score means the generated stories cover more phrases in the given outlines. (2) Order (Ord): It measures the disparity between the positional orders of given phrases in the ground truth and generated story using the percentage of inversions in the generated story <ref type="bibr" target="#b13">(Guan et al., 2022</ref>). An inversion is a position pair of two phrases that is out of the ground-truth order. Higher order scores mean that the stories arrange the outline more reasonably. In Section B.2, we also construct a learnable automatic metric to measure the faithfulness between morals and stories.</p><p>Results Table <ref type="table">6</ref> and 7 show the results on the understanding and generation tasks, respectively. To get the human performance on MOCPT and MO-PREF, we sampled 100 examples from the test set and recruited three annotators with good Chinese or English language proficiency to complete these tasks. We made final decisions among the annotators through major voting. The annotation results show an almost perfect agreement with Fleiss's ? &gt; 0.85 <ref type="bibr">(Fleiss and Joseph, 1971)</ref>.</p><p>We summarize the results on the understanding tasks as follows: (1) The MOPREF datasets suffer from innate biases as indicated by the high accuracy of BERT w/o story. Such biases may result from the noise introduced by the automatic construction technique, i.e., antonym substitution. And models may learn patterns of good behavior (e.g., "unity" is good and "disunity" is bad in general) and make predictions easily without depending on stories. However, MOPREF is still meaningful as an evaluation task since BERT can achieve much better accuracy when taking stories as input. And we experiment using manually constructed examples for evaluating preference alignment in the appendix.</p><p>(2) T5 performs better than RoBERTa on MOCPT but worse on MOPREF, indicating T5 may not be good at capturing value preferences.</p><p>(3) Post-training on the unlabeled data (i.e., RoBERTa-Post and T5-Post) does not always bring improvement on both tasks, suggesting that it is necessary to develop a better way to exploit these data in future work. (4) Retrieving additional concepts improves models' performance effectively, particularly for the MOCPT task on STORAL-EN. However, there is still a big gap between our models and human performance.</p><p>As for the generation tasks, we draw the following conclusions: (1) Almost all pretrained models achieve better lexical and semantic similarity with ground-truth texts than non-pretrained models, as indicated by higher BLEU and BERTScore values. (2) Non-pretrained models have less repetition than pretrained ones, and repeat even less than the ground-truth texts when generating morals. It may be because non-pretrained models generate shorter sequences than pretrained models despite the same decoding algorithm, which also accounts for the higher distinct scores of the non-pretrained models on the MO2ST task. (3) When generating stories, T5-Post can cover more input phrases and arrange them in a correct order than other baselines, as indicated by higher coverage and order scores. (4) Retrieval augmentation can improve the generation similarity with the ground-truth texts on both tasks and improve the coverage and order scores on ST2MO significantly compared with T5-Post.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Manual Evaluation</head><p>On the generation tasks, we conducted a Likertscale based manual evaluation to measure the gap between existing models and humans. For STOAL-ZH, we hired three graduate students (native Chinese speakers) as annotators. We conducted evaluation on STORAL-EN using Amazon Mechanical Turk (AMT). For both tasks, we randomly sampled 100 examples from the test set, and obtained 300 generated texts from Fusion, T5 and RA-T5. For each text we require three annotators to rate its qual-ity along with the input using a binary score in three following aspects: (1) linguistic fluency: correctness in grammaticality;</p><p>(2) coherence: reasonable relations between sentences regarding relatedness, causality and temporal orders; and (3) moral faithfulness: exhibition of a faithful moral to the input. Three aspects are independently evaluated. We decided the final score of a text through majority voting. The annotation instruction is shown in Section B.3. Table <ref type="table" target="#tab_8">8</ref> shows the manual evaluation results. We show p-values of the results in Section B.4. For ST2MO, T5 achieves a substantial improvement compared with Fusion (p &lt; 0.01), and our model further outperforms T5. The superiority becomes less significant for MO2ST. However, the big gap between these models and humans, particularly in terms of faithfulness, proves both tasks challenging for existing models. Furthermore, we evaluate whether machines can capture the value preference of a story using manually constructed examples. And we show error analysis and case study for the proposed tasks in Section C. We believe that explicit modeling of the relations among events and abstract concepts will further promote progress on these tasks, which we regard as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We present STORAL, a collection of Chinese and English moral stories. To test the ability to bridge concrete events and abstract morals, we propose new understanding and generation tasks based on STORAL, including selecting the correct moral from several candidates with different topics or opposite value preferences, concluding the moral of a story and generating a story to convey a moral. Extensive experiments prove these tasks still to be challenging for existing models. We propose a retrieval-augmented algorithm to improve performance by retrieving related concepts or events from training sets. Although it is possible to further increase the dataset size, we expect to make meaningful progress by developing better representations of commonsense and discourse relations among events and abstract concepts in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgement</head><p>This work was supported by the National Science Foundation for Distinguished Young Scholars (with No. 62125604) and the NSFC projects (Key project with No. 61936010 and regular project with No. 61876096). This work was also supported by the Guoqiang Institute of Tsinghua University, with Grant No. 2019GQG1 and 2020GQG0005. This work was also sponsored by Tsinghua-Toyota Joint Research Fund. We would also like to thank the anonymous reviewers for their invaluable suggestions and feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Ethics Statements and Broader Impact</head><p>We collected STORAL from public web resources. All stories are under licenses that allow use and redistribution for research purposes. We asked commercial annotation teams to extract stories and morals from the crawled raw texts. We required the annotators to refuse the examples which violate general ethical principles (e.g., showing discrimination for someone, containing disrespectful content, or encouraging to disturb public order, etc.). Totally, we payed more than $7 (CNY 45) per hour on average for annotating each example in STORAL, which was far beyond the minimum hourly wage in China (CNY 21). Furthermore, we resorted to AMT for manual evaluation of generated and human-written texts for two proposed generation tasks. We hired three annotators and payed each annotator $0.2 on average for annotating each example.</p><p>In this paper, we emphasize the ability to model relations between concrete events and abstract morals, which is also helpful for various scenar-ios such as reading comprehension (e.g., drawing authors' viewpoints from narratives) and essay writing (e.g., writing essays to convince readers of some arguments by presenting examples or anecdotes). STORAL provides a good start point for exploring these directions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A STORAL Construction</head><p>A.1 Data Source</p><p>We show the full list of web pages used for constructing STORAL in Table <ref type="table" target="#tab_12">11</ref>. We initially collect 52,017 Chinese and 2,630 English raw texts from the web pages. Then we de-duplicate the texts by removing those texts which overlap with others more than twenty words. After de-duplication, we finally collected 19,197 Chinese and 2,598 English texts. And we construct STORAL based on these texts.  The sentences causing the above issues are in italic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Data Annotation</head><p>Example 1: If you saw a thief in a crowded bus, would you bravely stop him? Please reflect on yourself instead of just complaining that our world is becoming worse. Without the foothold for dirt, the flower of civilization is bound to be fragrant.</p><p>Example 2: The story tells us: we should remember that we should become a polite person and communicate with others carefully.</p><p>Example 3: As long as you keep your sanity and make right judgments, all the barriers will not become an obstacle, just like the beautiful girl in the story.</p><p>Table <ref type="table">10</ref>: Examples of morals provided for the annotators. Each of the examples does not satisfy one of the following constraints in order: (1) describing only the main standpoint and not stating any sub-arguments or proofs, and (2) not stating anything irrelevant to the moral, and (3) not involving any specific characters in the story. We highlight the sentences leading to the above issues in italic.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Analysis of High-Frequency Words</head><p>To investigate the topic features of STORAL, we count the top 50 most frequent nouns in STORAL (excluding stop words) as shown in Figure <ref type="figure">3</ref>. We roughly categorize these words into four types: (1) Animals: animals are popular as protagonists in moral stories since they usually have various but clear characteristics (e.g., "sly foxes"), which embody rich commonsense knowledge;</p><p>(2) Relationships: such nouns are used to describe the inter-character relationships in a story (e.g., "friend"), which are useful for modeling characters' motivation and behavior;</p><p>(3) Concrete nouns: they refer to physical entities that can be observed, such as "water"; and (4) Abstract nouns: they re-fer to abstract concepts, such as "difficulty". We manually check the proportional distribution of the four types for stories and morals, respectively. The results in Figure <ref type="figure">3</ref> demonstrate that morals contain significantly less concrete nouns and more abstract nouns than stories. And morals contain little animal words but almost as many relationship words as stories, indicating that morals may be independent of specific characters but relate to general interpersonal relations. The result shows that morals are more abstract than stories.</p><p>Furthermore, Table <ref type="table" target="#tab_14">12</ref> shows the most frequent 4-grams in STORAL, further indicating that morals are more abstract than stories. Each of the 4-grams in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Discussion about STORAL</head><p>The high-quality examples in STORAL are full of commonsense and discourse relations. As exemplified in Table <ref type="table">1</ref> in the main paper, the common sense is mainly regarding the characters' reaction and intention (e.g., "the cows dispersed" and then the "tiger" and "lion" intend to kill them), as well as the nature of physical objects and abstract concepts (e.g. "cows" may be the food of "lions" and "tigers", and "unity" refers to "keeping together for a common goal"). Additionally, the stories usually have a specific discourse structure, i.e., the premise to introduce the story settings (e.g., the characters "four cows" and the location "a meadow"), the right or wrong behavior ("stay together or not") and the endings ("living well or being killed"). We believe it is an essential topic of future work to develop a better approach to model such commonsense and discourse relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Implementation</head><p>We implement the pretrained models used in our experiment mainly based on the register models of HuggingFace <ref type="bibr">(Wolf et al., 2020)</ref>. Table <ref type="table" target="#tab_1">13</ref> shows the names of the used register models. Note that we use LongLM base <ref type="bibr" target="#b13">(Guan et al., 2022)</ref> as the T5 model for experiments on STORAL-ZH, which has not been registered on HuggingFace.</p><p>All results in the main paper and the appendix are based on one NVIDIA Tesla v100 (16G memory). All reported results are based on one single running. The CPU is Intel Xeon Gold 5218. It cost less than 5 hours for fine-tuning each model on STORAL. We set the hyper-parameters following the default parameters of HuggingFace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Automatic Evaluation for Moral Faithfulness</head><p>We follow <ref type="bibr">Guan and Huang (2020)</ref> to train a learnable metric to evaluate moral faithfulness. Specifically, we fine-tune RoBERTa BASE as a classifier to distinguish whether a story matches a moral. We regard ground-truth examples as positive where the story and moral are matched, and construct negative examples by replacing the story or moral with a randomly sampled one. Finally, the classifier achieves an accuracy of 77.32/79.21% on the data constructed based on the test set of STORAL-ZH/STORAL-EN respectively. Then we calculate the faithfulness score as the average classifier score of all generated texts for the inputs. Table <ref type="table" target="#tab_15">14</ref> presents the evaluation results. We can see that pretrained models achieve better faithfulness than the non-pretrained models as shown by the much higher faithfulness scores. However, we also observe that the faithfulness score of the ground-truth texts is lower than some models (e.g., T5) when generating morals. Therefore, it is still necessary to manually evaluate faithfulness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Validation Sets</head><p>We show the performance of several baselines and RA-T5 on the validation sets of the understanding tasks and the generation tasks in Table <ref type="table" target="#tab_16">15</ref> and<ref type="table" target="#tab_17">Table 16</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Manual Evaluation Instruction</head><p>We show the manual annotation interface in <ref type="bibr">Figure 4.</ref> To ensure that the annotators guarantee a consistent standard in the annotation process, we asked annotators to rate four examples with the same input at the same HIT (human intelligence task). In these four examples, one is written by humans and three are generated by models (i.e., Fusion, T5 and RA-T5). We payed each annotator $0.2 on average for annotating each example. LongLM <ref type="bibr" target="#b13">(Guan et al., 2022)</ref> t5-base <ref type="bibr" target="#b39">(Raffel et al., 2020)</ref> Table <ref type="table" target="#tab_1">13</ref>: Names of register models used in our experiment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Significance of Manual Evaluation Results</head><p>Table <ref type="table" target="#tab_7">17</ref> shows the p-values (sign test) when comparing the manual evaluation results (Table <ref type="table" target="#tab_8">8</ref> in the main paper) between each pair of the ground truth, Fusion, T5 and RA-T5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Evaluating Value Preference Alignment</head><p>Although we have used MOPREF to evaluate whether machines can capture the value preference </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instruction</head><p>Moral: Nothing can be gained without effort. First Sentence: There was a farmer who had three sons.  of a story, the automatically constructed dataset may bias machines to focus on distinguishing general standards of good behaviour without considering story plots. Therefore, in this section, we construct examples manually to test this ability beyond the token level. Specifically, we randomly sampled 50 examples from the test sets of STORAL-ZH and STORAL-EN respectively. For each example, we manually rewrote the moral to convey a synonymous or antonymous value preference. For example, a synonymous moral with "unity is strength" in Table <ref type="table">1</ref> can be "we are powerful as long as we unite with each other" and an antonymous one can be "everyone can also be powerful enough." Then we expect a model to be able to accept the synonymous moral but reject the antonymous one. We use three typical models, including BERT w/o Story, RA-RoBERTa and RA-T5, to compute the winning</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The pipeline of human annotation for constructing STORAL (Left) and our constraints (Right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Model overview for the MOPREF task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 3: Top 50 most frequent nouns for stories and morals in STORAL-ZH and STORAL-EN. The numbers in the legend show the percentages of the total frequency of the nouns of the same type among the 50 nouns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>-chinese<ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> bert-base-uncased(Devlin et al.,  2019) RoBERTa hfl/chinese-roberta-wwm-ext (Cui et al., 2020) roberta-base (Liu et al., 2019) GPT2 uer/gpt2-chinese-cluecorpussmall (Zhao et al., 2019) gpt2 (Radford et al., 2019) T5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Story 1 :</head><label>1</label><figDesc>All of his sons were very lazy ? Story 2: The farmer loved his sons very much ... ... Story 5: The farmer said: "It's a good job is that ...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A simplified version of the manual annotation interface for MO2ST. The interface for ST2MO is similar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note><p>Topic words and examples for STORAL-ZH (top) and STORAL-EN (bottom). We underline the topic words that occurs in the examples.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table 5 shows the task</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Labeled Data</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Unlabeled Data</cell><cell></cell><cell></cell></row><row><cell>Datasets</cell><cell># Examples</cell><cell># Word</cell><cell>Stories # Sent</cell><cell>Vocab</cell><cell># Word</cell><cell>Morals # Sent</cell><cell>Vocab</cell><cell># Examples</cell><cell># Word</cell><cell># Sent</cell><cell>Vocab</cell></row><row><cell>STORAL-ZH</cell><cell>4,209</cell><cell>321.75</cell><cell>17.62</cell><cell>63,493</cell><cell>25.09</cell><cell>1.35</cell><cell>10,522</cell><cell>14,988</cell><cell>487.00</cell><cell>26.12</cell><cell>147,805</cell></row><row><cell>STORAL-EN</cell><cell>1,779</cell><cell>302.33</cell><cell>17.71</cell><cell>15,873</cell><cell>19.77</cell><cell>1.45</cell><cell>3,384</cell><cell>819</cell><cell>614.55</cell><cell>38.05</cell><cell>20,853</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>STORAL statistics. We use Jieba 2 /NLTK<ref type="bibr" target="#b30">(Loper and Bird, 2002)</ref> for word tokenization of STORAL-ZH/ STORAL-EN. # Word / # Sent is the average number of words/sentences. Vocab is the vocabulary size.</figDesc><table><row><cell>Tasks</cell><cell>Abilities</cell><cell>Inputs &amp; Outputs</cell><cell>STORAL-ZH |Train| |Val| |Test|</cell><cell>STORAL-EN |Train| |Val| |Test|</cell></row><row><cell>MOCPT</cell><cell>Concept Understanding</cell><cell>Given a story and five candidate morals, choosing the correct moral.</cell><cell>3,368 / 420 / 421</cell><cell>1,068 / 355 / 356</cell></row><row><cell>MOPREF</cell><cell>Preference Alignment</cell><cell>Given a story and two candidate morals, choosing the correct moral.</cell><cell>3,276 / 410 / 411</cell><cell>988 / 344 / 339</cell></row><row><cell>ST2MO</cell><cell>Moral Generation</cell><cell>Given a story, generating a moral which is character-independent</cell><cell>3,368 / 420 / 421</cell><cell>1,068 / 355 / 356</cell></row><row><cell></cell><cell></cell><cell>and generally applicable.</cell><cell></cell><cell></cell></row><row><cell>MO2ST</cell><cell>Story Generation</cell><cell>Given a moral and a story beginning and outline, generating a story</cell><cell>3,368 / 420 / 421</cell><cell>1,068 / 355 / 356</cell></row><row><cell></cell><cell></cell><cell>which has a coherent plot and convinces readers of the moral.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Description of the proposed tasks about the abilities they investigate, inputs and outputs, and the data sizes.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Automatic evaluation results for ST2MO (Top) and MO2ST (Bottom). ? / ? means the lower/higher the better. All scores except Len are multiplied by 100. The best result is in bold and the second best is underlined. The scores marked with * and * * of RA-T5 mean it outperforms the best baseline significantly with p-value&lt;0.1 and p-value&lt;0.05 (sign test), respectively.</figDesc><table><row><cell cols="2">Models</cell><cell># P</cell><cell cols="2">B-1?</cell><cell>B-2?</cell><cell cols="3">Dataset: STORAL-ZH BS? R-2?</cell><cell>D-2?</cell><cell>Len</cell><cell>B-1?</cell><cell>B-2?</cell><cell cols="3">Dataset: STORAL-EN BS? R-2?</cell><cell>D-2?</cell><cell>Len</cell></row><row><cell cols="2">ConvS2S</cell><cell>50M</cell><cell cols="2">14.31</cell><cell>1.86</cell><cell>56.71</cell><cell></cell><cell>26.60</cell><cell>43.67</cell><cell>19.31</cell><cell>9.69</cell><cell>0.93</cell><cell>82.57</cell><cell>6.46</cell><cell></cell><cell>47.35</cell><cell>11.75</cell></row><row><cell cols="2">Fusion</cell><cell>100M</cell><cell cols="2">14.78</cell><cell>2.23</cell><cell>56.90</cell><cell></cell><cell>27.55</cell><cell>41.21</cell><cell>21.96</cell><cell>9.87</cell><cell>0.82</cell><cell>82.68</cell><cell>6.18</cell><cell></cell><cell>43.59</cell><cell>13.15</cell></row><row><cell cols="2">GPT2</cell><cell>124M</cell><cell cols="2">14.54</cell><cell>2.16</cell><cell>60.75</cell><cell></cell><cell>35.39</cell><cell>48.22</cell><cell>20.72</cell><cell>10.98</cell><cell>1.24</cell><cell>79.39</cell><cell cols="2">20.22</cell><cell>60.36</cell><cell>16.19</cell></row><row><cell>T5</cell><cell></cell><cell>220M</cell><cell cols="2">18.19</cell><cell>3.60</cell><cell>61.61</cell><cell></cell><cell>76.48</cell><cell>44.84</cell><cell>29.06</cell><cell>13.31</cell><cell>2.26</cell><cell>85.89</cell><cell cols="2">33.15</cell><cell>58.73</cell><cell>19.39</cell></row><row><cell cols="2">T5-Post</cell><cell>220M</cell><cell cols="2">17.98</cell><cell>3.91</cell><cell>61.52</cell><cell></cell><cell>69.12</cell><cell>51.97</cell><cell>29.14</cell><cell>13.83</cell><cell>2.11</cell><cell>85.85</cell><cell cols="2">34.83</cell><cell>57.12</cell><cell>18.49</cell></row><row><cell cols="2">RA-T5</cell><cell>220M</cell><cell cols="2">18.32</cell><cell>3.64</cell><cell cols="2">61.93  *  *</cell><cell>70.78</cell><cell>48.14</cell><cell>29.44</cell><cell>14.59</cell><cell>2.61</cell><cell>86.16  *  *</cell><cell cols="2">31.46</cell><cell>60.61</cell><cell>18.54</cell></row><row><cell cols="2">Truth Morals</cell><cell>N/A</cell><cell>N/A</cell><cell></cell><cell>N/A</cell><cell>N/A</cell><cell></cell><cell>29.22</cell><cell>73.70</cell><cell>25.09</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell cols="2">16.85</cell><cell>73.95</cell><cell>20.41</cell></row><row><cell>Models</cell><cell>B-1?</cell><cell>B-2?</cell><cell>BS?</cell><cell cols="3">Dataset: STORAL-ZH R-4? D-4?</cell><cell>Cov?</cell><cell>Ord?</cell><cell>Len</cell><cell>B-1?</cell><cell>B-2?</cell><cell>BS?</cell><cell cols="2">Dataset: STORAL-EN R-4? D-4?</cell><cell cols="2">Cov?</cell><cell>Ord?</cell><cell>Len</cell></row><row><cell>ConvS2S</cell><cell>15.57</cell><cell>6.43</cell><cell>60.00</cell><cell cols="2">75.30</cell><cell>78.41</cell><cell>21.61</cell><cell>33.03</cell><cell>150</cell><cell>16.25</cell><cell>6.38</cell><cell>79.27</cell><cell>61.85</cell><cell>80.29</cell><cell cols="2">6.46</cell><cell>41.88</cell><cell>122</cell></row><row><cell>Fusion</cell><cell>15.53</cell><cell>6.45</cell><cell>60.06</cell><cell cols="2">74.11</cell><cell>80.51</cell><cell>22.86</cell><cell>33.33</cell><cell>148</cell><cell>17.17</cell><cell>6.82</cell><cell>79.52</cell><cell>61.24</cell><cell>75.79</cell><cell cols="2">7.27</cell><cell>43.07</cell><cell>137</cell></row><row><cell>GPT2</cell><cell>14.91</cell><cell>6.48</cell><cell>63.32</cell><cell cols="2">91.45</cell><cell>58.67</cell><cell>48.57</cell><cell>51.58</cell><cell>282</cell><cell>25.83</cell><cell>12.91</cell><cell>83.25</cell><cell>84.27</cell><cell>74.63</cell><cell cols="2">45.18</cell><cell>59.95</cell><cell>247</cell></row><row><cell>PM</cell><cell>15.82</cell><cell>7.04</cell><cell>63.58</cell><cell cols="2">90.97</cell><cell>57.33</cell><cell>50.51</cell><cell>52.35</cell><cell>280</cell><cell>26.34</cell><cell>13.92</cell><cell>81.63</cell><cell>80.90</cell><cell>72.64</cell><cell cols="2">47.07</cell><cell>60.31</cell><cell>264</cell></row><row><cell>T5</cell><cell>17.74</cell><cell>9.44</cell><cell>65.89</cell><cell cols="2">91.69</cell><cell>61.76</cell><cell>58.18</cell><cell>56.11</cell><cell>166</cell><cell>30.56</cell><cell>16.75</cell><cell>79.89</cell><cell>90.17</cell><cell>77.53</cell><cell cols="2">74.21</cell><cell>63.45</cell><cell>283</cell></row><row><cell>T5-Post</cell><cell>18.42</cell><cell>9.77</cell><cell>65.63</cell><cell cols="2">94.54</cell><cell>58.13</cell><cell>60.11</cell><cell>56.96</cell><cell>176</cell><cell>32.36</cell><cell>18.04</cell><cell>83.80</cell><cell>94.10</cell><cell>77.27</cell><cell cols="2">76.09</cell><cell>64.33</cell><cell>281</cell></row><row><cell>RA-T5</cell><cell>23.36 **</cell><cell>12.98 **</cell><cell>67.37 **</cell><cell cols="2">95.72</cell><cell>59.49</cell><cell>69.24 **</cell><cell>60.44 **</cell><cell>241</cell><cell>32.46</cell><cell>18.31 *</cell><cell>84.07 **</cell><cell>92.42</cell><cell>76.74</cell><cell cols="2">80.21 **</cell><cell>66.10 **</cell><cell>253</cell></row><row><cell>Truth</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell cols="2">55.34</cell><cell>96.06</cell><cell>100.00</cell><cell>100.00</cell><cell>324</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>58.71</cell><cell>95.09</cell><cell cols="2">100.00</cell><cell>100.00</cell><cell>281</cell></row><row><cell>Data</cell><cell>Task</cell><cell>Model</cell><cell cols="2">Flu (?)</cell><cell cols="2">Cohe (?)</cell><cell cols="2">Faith (?)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>STORAL-ZH</cell><cell>ST2MO MO2ST</cell><cell>Fusion T5 RA-T5 Truth Fusion T5 RA-T5 Truth</cell><cell cols="2">0.24 (0.31) 0.75 (0.40) 0.85 (0.65) 1.00 (1.00) 0.25 (0.39) 0.38 (0.40) 0.45 (0.29) 0.98 (0.93)</cell><cell cols="2">0.22 (0.37) 0.61 (0.38) 0.63 (0.26) 0.99 (0.96) 0.11 (0.61) 0.24 (0.37) 0.34 (0.25) 1.00 (1.00)</cell><cell cols="2">0.08 (0.72) 0.31 (0.32) 0.36 (0.27) 0.86 (0.57) 0.02 (0.93) 0.05 (0.81) 0.11 (0.72) 0.96 (0.84)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>STORAL-EN</cell><cell>ST2MO MO2ST</cell><cell>Fusion T5 RA-T5 Truth Fusion T5 RA-T5 Truth</cell><cell cols="2">0.32 (0.39) 0.76 (0.35) 0.81 (0.51) 0.94 (0.80) 0.47 (0.43) 0.56 (0.35) 0.58 (0.28) 0.95 (0.69)</cell><cell cols="2">0.26 (0.35) 0.74 (0.27) 0.79 (0.40) 0.94 (0.77) 0.40 (0.47) 0.48 (0.37) 0.51 (0.31) 0.98 (0.69)</cell><cell cols="2">0.24 (0.41) 0.55 (0.33) 0.67 (0.37) 0.88 (0.56) 0.37 (0.45) 0.49 (0.39) 0.57 (0.31) 0.93 (0.53)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Manual evaluation results for ST2MO and MO2ST. Flu, Cohe and Faith mean fluency, coherence and moral faithfulness, respectively. The best results are highlighted in bold. All results show fair or moderate inter-annotator agreement measured byFleiss'  ? (Fleiss and Joseph, 1971).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Paul C Vitz. 1990. The use of stories in moral development: New psychological reasons for an old education method. American psychologist, 45(6):709.</figDesc><table><row><cell>Svitlana Volkova, Kyle Shaffer, Jin Yea Jang, and</cell></row><row><cell>Nathan Hodas. 2017. Separating facts from fiction:</cell></row><row><cell>Linguistic models to classify suspicious and trusted</cell></row><row><cell>news posts on twitter. In Proceedings of the 55th An-</cell></row><row><cell>nual Meeting of the Association for Computational</cell></row><row><cell>Linguistics (Volume 2: Short Papers), pages 647-</cell></row><row><cell>653.</cell></row><row><cell>Tianming Wang and Xiaojun Wan. 2019. T-CVAE:</cell></row><row><cell>transformer-based conditioned variational autoen-</cell></row><row><cell>coder for story completion. In Proceedings of the</cell></row><row><cell>Twenty-Eighth International Joint Conference on Ar-</cell></row><row><cell>tificial Intelligence, IJCAI 2019, Macao, China, Au-</cell></row><row><cell>gust 10-16, 2019, pages 5233-5239. ijcai.org.</cell></row><row><cell>Thomas Wolf, Julien Chaumond, Lysandre Debut, Vic-</cell></row><row><cell>tor Sanh, Clement Delangue, Anthony Moi, Pier-</cell></row><row><cell>ric Cistac, Morgan Funtowicz, Joe Davison, Sam</cell></row><row><cell>Shleifer, et al. 2020. Transformers: State-of-the-</cell></row><row><cell>art natural language processing. In Proceedings of</cell></row><row><cell>the 2020 Conference on Empirical Methods in Nat-</cell></row><row><cell>ural Language Processing: System Demonstrations,</cell></row><row><cell>pages 38-45.</cell></row><row><cell>Lili Yao, Nanyun Peng, Ralph Weischedel, Kevin</cell></row><row><cell>Knight, Dongyan Zhao, and Rui Yan. 2019. Plan-</cell></row><row><cell>and-write: Towards better automatic storytelling. In</cell></row><row><cell>Proceedings of the AAAI Conference on Artificial In-</cell></row><row><cell>telligence, volume 33, pages 7378-7385.</cell></row><row><cell>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q</cell></row><row><cell>Weinberger, and Yoav Artzi. 2019. Bertscore: Eval-</cell></row><row><cell>uating text generation with bert. In International</cell></row><row><cell>Conference on Learning Representations.</cell></row><row><cell>Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi.</cell></row><row><cell>2017. Learning discourse-level diversity for neural</cell></row><row><cell>dialog models using conditional variational autoen-</cell></row><row><cell>coders. In Proceedings of the 55th Annual Meet-</cell></row><row><cell>ing of the Association for Computational Linguistics</cell></row><row><cell>(Volume 1: Long Papers), pages 654-664.</cell></row><row><cell>Zhe Zhao, Hui Chen, Jinbin Zhang, Xin Zhao, Tao Liu,</cell></row><row><cell>Wei Lu, Xi Chen, Haotang Deng, Qi Ju, and Xiaoy-</cell></row><row><cell>ong Du. 2019. Uer: An open-source toolkit for pre-</cell></row><row><cell>training models. EMNLP-IJCNLP 2019, page 241.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 /</head><label>9</label><figDesc>10  shows the examples given to the annotators to inform them of the requirements for stories/morals, respectively. If the constraints are not met, we ask annotators to refine the story and moral. All workers were paid more than $7 per hour on average. I have a well-off friend. When she first entered college, she had many good wishes and thought she could achieve her goals. ? ? ? Now she felt very painful under the strong mental pressure. I can understand her feelings. ? ? ? If magnifying your own pain, you will get trapped in the mire of your pain, and even feel that life is too unfair to you. Raul sat at his door, frowning. ? ? ? His father told Raul a true story: A wild wolf escaped into a cave after being wounded by a hunter's arrow. ? ? ? After hearing the story, Raul cheered up immediately. ? ? ?</figDesc><table><row><cell>Example 1: Come on Bear! What a beautiful day! Go for</cell></row><row><cell>a walk with your father! Take a deep breath and smell the</cell></row><row><cell>flowers. But don't pick the flowers. Listen to the birds sing.</cell></row><row><cell>But don't scare them. How beautiful the world is. Isn't it,</cell></row><row><cell>dear Bear?</cell></row><row><cell>Example 2: When I was a child, I heard a story that felt</cell></row><row><cell>very regrettable. I felt sorry for the protagonist of the story.</cell></row><row><cell>Long ago, there lived ? ? ? Such trees are now found all over</cell></row><row><cell>Uganda.</cell></row><row><cell>Example 3: Example 4:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Examples of stories provided for the annota-</figDesc><table><row><cell>tors. Each example does not meet one of the follow-</cell></row><row><cell>ing requirements in order: (1) having a clear beginning</cell></row><row><cell>and ending; (2) not stating anything irrelevant to the</cell></row><row><cell>main plot; (3) not stating any explicit arguments for the</cell></row><row><cell>moral; and (4) not telling the story in a nested form.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>List of source web pages used for constructing STORAL-ZH (Top) and STORAL-EN (Bottom). Numbers in the right column means the number of raw texts initially collected from the corresponding web page.</figDesc><table><row><cell>Frequency</cell><cell>500 1000 1500 2000</cell><cell>14.67% Animals 12.70% Relationships 46.84% Concrete Nouns 26.00% Abstract Nouns Frequency</cell><cell>50 100 150 200 250</cell><cell>00.00% Animals 15.28% Relationships 23.67% Concrete Nouns 62.65% Abstract Nouns</cell></row><row><cell></cell><cell>0</cell><cell>ma n day tim e boy king frie nd way son life fath er peo ple wat er thin g lion tree fox hou se dog mo the r one plac e foo d yea r han d nigh t mo ney villa ge chil d farm er fore st bird anim al ma ster cat wol f mo use wife god hea d fish girl eye hom e mo nke y wor ld wor k per son side face rive r</cell><cell>0</cell><cell>life thin g oth er tim e peo ple way god frie nd one per son wor k hap pine ss wor d pro blem situ atio n wor ld ma n chil d love stre ngt h stor y par ent suc ces s hea rt min d self help mis take mo ney acti on nat ure goo d fea r pow er trut h tod ay mo me nt rea son resp ect nee d solu tion care frie nds hip fam ily dee d valu e diff icul ty dan ger day ang er</cell></row><row><cell></cell><cell></cell><cell>(c) Stories in STORAL-EN</cell><cell></cell><cell></cell></row></table><note><p>(a) Stories in STORAL-ZH (b) Morals in STORAL-ZH</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Table 12 comprises less than 0.01% of all 4-</figDesc><table><row><cell>Stories</cell><cell>Morals</cell></row><row><cell>Dataset: STORAL-ZH</cell><cell></cell></row><row><cell>as one is walking</cell><cell>we should be a</cell></row><row><cell>say to him that you</cell><cell>everyone has</cell></row><row><cell>say after thinking</cell><cell>everyone has his own</cell></row><row><cell>the most in the world</cell><cell>has own</cell></row><row><cell>all the animals</cell><cell>each of us</cell></row><row><cell>all the persons</cell><cell>we should know to</cell></row><row><cell>a place far away</cell><cell>for anything, we should</cell></row><row><cell>the dad of the pink pig</cell><cell>be one who knows to</cell></row><row><cell>this is my</cell><cell>for anything, be</cell></row><row><cell cols="2">in the forest there lived a is a true</cell></row><row><cell>Dataset: STORAL-EN</cell><cell></cell></row><row><cell>once upon a time</cell><cell>we should try to</cell></row><row><cell>upon a time there</cell><cell>the best way to</cell></row><row><cell>a time there was</cell><cell>it is better to</cell></row><row><cell>time there was a</cell><cell>it is easy to</cell></row><row><cell>there was once a</cell><cell>we should learn to</cell></row><row><cell>once there was a</cell><cell>those who help themselves</cell></row><row><cell>was not able to</cell><cell>with what we have</cell></row><row><cell>as soon as he</cell><cell>be happy with what</cell></row><row><cell>and asked him to</cell><cell>we should not judge</cell></row><row><cell>did n't want to</cell><cell>look before you leap</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>Top 10 most frequent 4-grams in STORAL-ZH and STORAL-EN respectively. The Chinese 4-grams in STROAL-ZH are translated into English.</figDesc><table /><note><p>grams in the corresponding dataset, showing the diversity of STORAL.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 14 :</head><label>14</label><figDesc>Automatic moral faithfulness scores. The score of PM for the ST2MO task is N/A since we do not experiment with PM for this task.</figDesc><table><row><cell>Models</cell><cell cols="2">ST2MO ZH EN</cell><cell cols="2">MO2ST ZH EN</cell><cell></cell></row><row><cell cols="6">ConvS2S 31.92 28.68 33.44 35.59</cell></row><row><cell>Fusion</cell><cell cols="5">33.85 25.23 38.81 35.16</cell></row><row><cell>GPT2</cell><cell cols="5">68.52 73.73 50.49 64.90</cell></row><row><cell>PM</cell><cell>N/A</cell><cell>N/A</cell><cell cols="3">52.41 62.53</cell></row><row><cell>T5</cell><cell cols="5">89.20 90.57 56.11 63.45</cell></row><row><cell>T5-Post</cell><cell cols="5">90.98 91.87 58.58 75.67</cell></row><row><cell>RA-T5</cell><cell cols="5">86.50 88.69 59.50 74.92</cell></row><row><cell>Truth</cell><cell cols="5">77.49 80.03 77.49 80.03</cell></row><row><cell>Models</cell><cell># P</cell><cell cols="2">MOCPT ZH EN</cell><cell cols="2">MOPREF ZH EN</cell></row><row><cell>BERT w/o Story</cell><cell>110M</cell><cell>20.71</cell><cell>21.69</cell><cell>72.64</cell><cell>77.62</cell></row><row><cell>BERT</cell><cell>110M</cell><cell>65.24</cell><cell>54.08</cell><cell>85.37</cell><cell>79.36</cell></row><row><cell>RoBERTa</cell><cell>110M</cell><cell>66.90</cell><cell>61.69</cell><cell>90.49</cell><cell>80.52</cell></row><row><cell>RoBERTa-Post</cell><cell>110M</cell><cell>67.14</cell><cell>55.77</cell><cell>89.27</cell><cell>84.01</cell></row><row><cell>T5</cell><cell>220M</cell><cell>74.52</cell><cell>62.25</cell><cell>78.05</cell><cell>77.91</cell></row><row><cell>T5-Post</cell><cell>220M</cell><cell>74.05</cell><cell>67.61</cell><cell>81.22</cell><cell>81.10</cell></row><row><cell>RA-RoBERTa</cell><cell>110M</cell><cell>66.43</cell><cell>63.94</cell><cell>88.54</cell><cell>86.63</cell></row><row><cell>RA-T5</cell><cell>220M</cell><cell>74.05</cell><cell>67.61</cell><cell>80.73</cell><cell>80.23</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 15 :</head><label>15</label><figDesc>Accuracy (%) for MOCPT and MOPREF on the validation set.</figDesc><table><row><cell>Models</cell><cell>ZH</cell><cell cols="2">ST2MO</cell><cell>EN</cell><cell>ZH</cell><cell cols="2">MO2ST</cell><cell>EN</cell></row><row><cell>Fusion</cell><cell cols="2">14.44/1.80</cell><cell cols="2">10.78/0.92</cell><cell cols="2">16.06/6.44</cell><cell>16.74/6.72</cell></row><row><cell>T5</cell><cell cols="2">18.54/4.08</cell><cell cols="2">13.17/2.05</cell><cell cols="2">18.98/10.17</cell><cell>28.87/15.48</cell></row><row><cell>RA-T5</cell><cell cols="2">18.68/3.64</cell><cell cols="2">14.49/4.47</cell><cell cols="2">23.98/13.17</cell><cell>31.72/17.97</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 16 :</head><label>16</label><figDesc>BLEU-1/BLEU-2 for ST2MO and MO2ST on the validation set.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Q1: Is the Story Fluent? Yes No Q2: Is the Story Coherent? Yes No Repetition Unrelatedness Conflicting logic Chaotic Scenes Others Q3: Is the Story faithful to Moral? Yes No Not a moral story Unrelated concepts Conflicting value preference Others 1</head><label></label><figDesc>. Read the moral, first sentence and four stories. 2. Comparing the stories with one another in terms of fluency, coherence and faithfulness to the input. 3. Answer the question for each story. Please choose the reasons if your answer is "no" when evaluating coherence and faithfulness.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/fxsjy/jieba</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Table <ref type="table">18</ref>: Winning rates of pair-wise comparisons which require selecting a correct moral from two candidates. Each candidate is a ground-truth (True), synonymous (Syn), or antonymous (Ant) moral. The number in the parenthesis is the corresponding p-value (sign test).</p><p>Table <ref type="table">18</ref> shows the evaluation results. We observe that BERT can not distinguish different types of morals without input stories. RA-RoBERTa fails to accept the synonymous morals on STORAL-EN (winning rate of only 36% w.r.t the ground truth, p &lt; 0.1), and can not distinguish synonymous and antonymous morals on both STORAL-ZH and STORAL-EN (winning rate near 50% with p &gt; 0.1). Additionally, it prefers antonymous morals to the ground truth significantly on both datasets (winning rate less than 50% and p &lt; 0.1 ). The results indicate that existing models still struggle to capture the value preference of moral stories.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Error Analysis and Case Study</head><p>In this section, we conducted a case study and investigated the errors of existing models on the proposed tasks to provide insight into future work. We show several typical error cases in Table <ref type="table">20</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Understanding Tasks</head><p>The example in Table <ref type="table">20</ref> for MOCPT shows that the model may not grasp abstract concepts such as "good will" and "good acts" and align them to the story plots. It makes predictions possibly based on only token-level features such as relations between "ask after" and "attention". On the other hand, the example for MOPREF indicates that the model can not capture the value preference of the story in terms of "whether it is intelligent to regard others are illiterate". The results demonstrate the necessity of introducing concept knowledge and modeling high-level semantic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Generation Tasks</head><p>Table <ref type="table">21</ref> shows cases generated by several baselines and our model for the generation tasks. We can see that retrieval can provide effective guidance for both moral and story generation. Baseline models including GPT2 and T5 tend to generate unrelated concepts or non-moral texts. However, as shown by the manual evaluation results, there is still a big gap between RA-T5 and humans. To provide quantitative error analysis, in the process of manual evaluation on STORAL-EN, we required annotators to annotate the error type of a text when it exhibit an unfaithful moral. We summarize three main error types as follows:</p><p>(1) Not a moral text (NAM): not stating or imply-Understanding Task: MOCPT Input Story: A stag had fallen sick. He had just strength enough to gather some food and find a quiet clearing in the woods, where he lay down to wait until his strength should return. The animals heard about the stag's illness and came to ask after his health. Of course, they were all hungry, and helped themselves freely to the stag's food; and as you would expect, the stag soon starved to death.</p><p>Candidate Moral 1: Good will is worth nothing unless it is accompanied by good acts. Candidate Moral 2: Every man in need is your neighbor. Candidate Moral 3: Your everyday good deeds never go in vain as they will return to you when you least expect them. Candidate Moral 4: Don't trust strangers. Candidate Moral 5: Everyone person is significant and deserve your attention and respect.</p><p>True Answer: Moral 1 Model Prediction: Moral 5</p><p>Understanding Task: MOPREF Input Story: Once upon a time there lived a cat that loved to read. At night, when everybody was asleep, she would put on the spectacles and read the big book for cats. One day, she read in the book: if you want a mouse for dinner, repeat the following rhyme: in this house there is a mouse, where is the mouse, where is the mouse? The cat looked up from the book and found that there was a mouse on the top of the table. The cat repeated the rhyme and soon found the same mouse on the bed. Then she jumped upon the bed to catch the mouse and the mouse was gone! The mouse was very clever. Suddenly he squeaked, "Oh, dear cat, run, run fast! there is dog after you!" The cat left the mouse and was ready to jump out of the window. The mouse sat near his hole and said, "Ha-ha-ha! dear cat that was the trick I learnt from the bio book for mice!" And the mouse ran into his hole! Candidate Moral 1: An intelligent person should not think that others are illiterate. Candidate Moral 2: An intelligent person should not forget that others are illiterate.</p><p>True Answer: Moral 1 Model Prediction: Moral 2 Generation Task: ST2MO Input Story: In the forest, there was a deer and an owl. The deer loved to scare this fearful owl all the time. One day, the deer scared the owl so hard, the owl past out. The deer ran away, and the owl woke up and came to her senses. She had had enough of being scared. She went to the deer and said, "I'm not scared of you." She said this with such a force that the deer backed down Generated Case 4: There was a grocery shop in a town. There was a big fat cat in there. It was very big. It was a giant cat. It was big, and it was very big and powerful. It was big and powerful. It was very powerful. The mouse could move freely and had a nice time hunting. The cat was very big and powerful. The mouse wanted to live in a very large land ? ? ? (NAM) Generated Case 5: There was a grocery shop in a town. Just the mouse and cat lived together. ? ? ? The mouse was very active in the shop and in the street. The mouse was very happy. But the mouse was too busy to move freely. He was the only mouse in town. He was very busy with work. ? ? ? As soon as the mouse had left, he moved slowly. ? ? ? So the mouse was so happy and satisfied with everything that it was so happy for him. ? ? ? (UNREL) Generated Case 6: There was a grocery shop in a town. There were two stores. ? ? ? The third was an empty solution. Each time the mouse went on a few days. Then, the third was a little while later the next day the mice were gone. They were gone. They were happy at last ? ? ? (CONF) True Story: There was a grocery shop in a town. Plenty of mice lived in that grocery shop. Food was in plenty for them. They ate everything and spoiled all the bags. They also wasted the bread, biscuits and fruits of the shop. The grocer got really worried. So, he thought "I should buy a cat and let it stay at the grocery. only then I can save my things." He bought a nice, big fat cat and let him stay there. The cat had a nice time hunting the mice and killing them. The mice could not move freely now. They were afraid that anytime the cat would eat them up. The mice wanted to do something. They held a meeting and all of them tweeted "We must get rid of the cat. can someone give a suggestion"? All the mice sat and brooded. A smart looking mouse stood up and said, "The cat moves softly. that is the problem. if we can tie a bell around her neck, then things will be fine. we can know the movements of the cat". "Yes, that is answer," Stated all the mice. An old mouse slowly stood up and asked, "Who would tie the bell?" After some moments there was no one there to answer this question.</p><p>Table 20: Typical error cases predicted by RA-T5 (for the understanding tasks) or sampled from RA-T5 (for the generation tasks). For the generation tasks, the error types in terms of moral faithfulness include "not a moral text" (NAM), "unrelated concepts" (UNREL) and "conflicting value preference" (CONF). The underlined words are improper concepts/events which leads to corresponding errors. Bold words for MO2ST are the given first sentence and the outline of multiple phrases.</p><p>ing what is right or what is wrong; (2) Unrelated concepts (UNREL): containing unrelated concepts with the input; and (3) Conflicting value preference (CONF): conveying a value preference conflicting with the input despite related concepts. In addition, we also provide annotators with another option Others. The annotators are allowed to annotate a text with multiple errors. When at least two of three annotators annotate the text with some error, we decide it has the error. We show the distribution of the error types in Table <ref type="table">19</ref>, suggesting that existing models still struggle to generate meaningful morals and stories, and align the concepts and value preferences between them. Furthermore, as exemplified in Table <ref type="table">20</ref>, when generating morals, we can see from Case 1 that the Generation Task: ST2MO Input Story: Once upon a time there was a spring who lived happily and safely inside a pen. Although he heard many noises coming from outside, he lived believing that outside his world inside the pen, there was nothing good. Even just to think about leaving his pen made him so scared that he was quite content to spend his life compacting and stretching himself again and again inside that tiny space. However, one day, the ink ran out, and when the pen's owner was busy changing it, there was an accident. The spring was flung through the air and landed in the toilet drain, well out of sight. Terrified, and cursing his bad luck, the spring was flushed through pipe after pipe, each time thinking it might be his end. During the journey, he did not dare open his eyes out of pure fear. Nor did he every stop crying. Swept away by the water, he travelled on and on and on, until he ended up in a river. When the river current lost its force, and the spring could see that things had calmed down a bit, he stopped crying and listened all around him. Hearing birdsong and wind in the trees, he felt encouraged to finally open his eyes. What the spring saw was the pure, crystal waters of the river, the rich green rocks of the riverbed, and all kinds of fish of many colours, whose skin seemed to dance under the sunlight. Now he understood that the world was much greater than the space inside the pen, and that there had always been many things outside, waiting to be enjoyed. After spending a while playing with the fish, he went over to the riverbank, and then moved on to a field of flowers. There he heard weeping. He followed the sound, which took him to a lovely flower that had been flattened by a rabbit, and could no longer stand up straight. The spring realised that he could help the flower, so he offered to be his support. The flower accepted, and slipped through the middle of the spring. There they lived happily together. And they would always laugh when remembering how the spring used to think that all there was to life was being a sad and fearful spring.</p><p>GPT2: Always think of what you can do to make things better.</p><p>T5: Fear and frustration are one thing, but they can be turned into happiness.</p><p>RA-T5: Life should not be afraid of the unknown. There should be lots of positive things out there.</p><p>Top-5 Retrieved Morals by RA-T5:</p><p>(1) Everything in life has its unpleasant or uncomfortable side. These aspects cannot be removed from life.</p><p>(2) It's always better to have friends and be happy -even though it may bring discomfort -than isolate yourself from people in order to avoid problems.</p><p>(3) Most fear is without foundation, and plays on what is unknown or different. (4) Friends are for cheering you up in your worst moments.</p><p>(5) Someone who harm's no one is still very far from being good.</p><p>True Moral: Changes are not a bad thing, it is up to ourselves to open the eyes and make something positive of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generation Task: ST2MO</head><p>Input Moral: Never give up.</p><p>GPT2: One day a dragon and a dino met in the late jurassic period. They were friends, and on the way they met a huge tree. Dragon was huge and his big claws were full and he was going to kill him. Dino was going to die. Dino was going to die. When the dragon made half of the tree, he was about to die. Dragon was going to die, and all the beasts and beasts were dead. Dino had no idea what to do. He was the strongest dino in the whole tree. Dragon was a total surprise. Dino was going to die. He was about to die. He was going to die. The dragon was astonished. Dino was not able to die. Dino was so angry, he started to die. Dino was so angry, and the dragon threw his big claws at the tree and ran to the forest. This was the best match. Then the dino was nearly killed.</p><p>T5: One day a dragon and a dino met in the late jurassic period. The dragon was standing next to a huge tree. It was full of beasts, lions and zebras that were strong enough to carry out their duties. The dragon was so strong that he could move himself without losing his foot. The dino was so strong that he could carry anything, even if that tree had much more branches. But the dino was very weak, so he could not carry anything. The dragon came to the tree and said, "I will do everything I can to get my foot into the tree." Then the dino started running through the tree. The dragon made half way through the tree and put on his big claws. Then he took off his big claws in his claws. Just then a huge tree started to go through a storm. Then the monster took out his fiery breath. The dino could not contain his fiery breath and flew away.</p><p>RA-T5: One day a dragon and a dino met in the late jurassic period. The dragon was very proud of the dino because of his big claws and fiery breath. The dino was going to jump on a huge tree and climb it up. But the dragon didn't know how to jump. The dragon made half way up the tree and started to roll down. The dino was completely surprised. The dragon said, "I can't jump," The dragon started to jump. Then the tree started to roll. The dino started to roll over the top of the tree and jumped up. The tree started to thrash off. Suddenly, the dino saw that the dragon had a fiery breath. The dragon said, "That's my strength." The dino was all alone. Then he stood, looking at the dragon. Then the dragon stopped, shouted at him, "Hello dino! what a total surprise."</p><p>The dino started running and jumping towards the tree, and the dragon got close to him. Then he hit the tree. Now he found a boulder on the tree started to crash and he jumped on the tree. Then the strongest dino ever was surprised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Top-5 Retrieved Story Outines:</head><p>(1) {baldwin flew, baldwin scratched rattler, team beat baldwin, baldwin dodged rattler, football game, goal post, baldwin threw rattler, baldwin started} (2) {eagle resting, tree top, tortoise rested, eagle answered, deep sleep, tortoise sleeping, tortoise smiled, hunter suddenly]} (3) {loud thump, man happened, cry intruded, ugly wolf, wolf named pete walked, long neck, man walked, thin air} (4) {cat suddenly fell, bird flew, started climbing, cat thanked} (5) {lion won, race started, croc won} True Story: One day a dragon and a dino met in the late jurassic period. The dragon said, "I'm stronger than you!" The dino said, "I'm the strongest dino ever!" The next day the dino and the dragon met in the forest. The dragon made half of a tree fall down with its big claws and fiery breath. The dragon said, "You can't beat that!" The dino started running toward a huge tree. The dino rammed the huge tree with its head. Nothing happened. The dragon laughed. Then the tree started to fall. The dragon just stared in total surprise.</p><p>Table <ref type="table">21</ref>: Cases generated by different models for the generation tasks. The underlined words are improper concepts/events which leads to incoherence or unfaithfulness. Bold words for MO2ST are the given first sentence and the outline of multiple phrases. The red moral for ST2MO is related to the generated moral of RA-T5 in semantics. Note that we only take concepts in these retrieved morals as input for RA-T5. And red words in the retrieved outlines for MO2ST indicate that they also show up in the generated story of RA-T5. models still often state events involved with specific characters (e.g., "owls") but do not tell what is right and what is wrong. And Case 2 shows that they struggle to conclude related concepts from the story (e.g., "greedy" is not embodied in the story at all). Furthermore, in Case 3, the models conclude a conflicting value preference with the story despite correct concepts (e.g., the story shows that "it is bad to be scared" but not "good"). On the other hand, models also are shown to suffer from similar issues when generating stories. In Case 4, the model only describes some scenes (e.g., "it was very big" and "it was very powerful") but does not aims to convince readers of anything. And Case 5 seems to tell a story centered on some concepts such as "active" and "busy", but the concepts do not relate to the input. Case 6 implies "empty solutions may be useful," which is conflicting with the input. These cases indicate the necessity of modeling the relations between events and abstract concepts for understanding and generating moral stories.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">STO-RIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation</title>
		<author>
			<persName><forename type="first">Nader</forename><surname>Akoury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shufan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Whiting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Hood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.525</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6470" to="6484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>David M Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling protagonist emotions for emotion-aware storytelling</title>
		<author>
			<persName><forename type="first">Faeze</forename><surname>Brahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.426</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5277" to="5294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reading Wikipedia to answer opendomain questions</title>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1171</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Revisiting pretrained models for Chinese natural language processing</title>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="657" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Denis</forename><surname>Emelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jena</forename><forename type="middle">D</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15738</idno>
		<title level="m">Moral stories: Situated reasoning about norms, intents, actions, and their consequences</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical neural story generation</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning narrative structure from annotated folktales</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">L</forename><surname>Joseph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="378" to="382" />
			<date type="published" when="1971">2012. 1971</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology. Fleiss and</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
	<note>Measuring nominal scale agreement among many raters</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Social chemistry 101: Learning to reason about social and moral norms</title>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jena</forename><forename type="middle">D</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.48</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="653" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An empirical exploration of moral foundations theory in partisan news sources</title>
		<author>
			<persName><forename type="first">Dean</forename><surname>Fulgoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lyle</forename><surname>Ungar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Preo?iuc-Pietro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3730" to="3736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">LOT: A Story-Centric Benchmark for Evaluating Chinese Long Text Understanding and Generation</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoer</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yamei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruilin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changjie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00469</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="434" to="451" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A knowledge-enhanced pretraining model for commonsense story generation</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="93" to="108" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">UNION: an unreferenced metric for evaluating open-ended story generation</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.736</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-16">2020. November 16-20, 2020</date>
			<biblScope unit="page" from="9157" to="9166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Story ending generation with incremental encoding and commonsense knowledge</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yansen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6473" to="6480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Marasovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8342" to="8360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Intuitive ethics: How innately prepared intuitions generate culturally variable virtues</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Haidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Joseph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Daedalus</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="55" to="66" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Aligning {ai} with shared human values</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Critch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Moral foundations twitter corpus: A collection of 35k tweets annotated for moral sentiment</title>
		<author>
			<persName><forename type="first">Joe</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gwenyth</forename><surname>Portillo-Wightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leigh</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shreya</forename><surname>Havaldar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aida</forename><forename type="middle">Mostafazadeh</forename><surname>Davani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Atari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zahra</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madelyn</forename><surname>Mendlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriela</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyee</forename><forename type="middle">E</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenna</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yen</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arineh</forename><surname>Mirinjian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morteza</forename><surname>Dehghani</surname></persName>
		</author>
		<idno type="DOI">10.1177/1948550619876629</idno>
	</analytic>
	<monogr>
		<title level="j">Social Psychological and Personality Science</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1057" to="1071" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Morals, ethics, and integrity: How codes of conduct contribute to ethical adult education practice</title>
		<author>
			<persName><forename type="first">Silvana</forename><surname>Ianinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Claude</forename><surname>Garcia-Zamor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Public Organization Review</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="20" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jena</forename><forename type="middle">D</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Borchardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07574</idno>
		<title level="m">Towards machine ethics and norms</title>
		<meeting><address><addrLine>Delphi</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Classification of moral foundations in microblog political discourse</title>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Goldwasser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="720" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stylized story generation with style-guided planning</title>
		<author>
			<persName><forename type="first">Xiangzhe</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziquan</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.215</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2430" to="2436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">BART: denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05">Marjan Ghazvininejad,. 2020. July 5-10, 2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05727</idno>
		<title level="m">Bertgcn: Transductive text classification by combining gcn and bert</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Nltk: The natural language toolkit</title>
		<author>
			<persName><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics</title>
		<meeting>the ACL-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="63" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep dungeons and dragons: Learning character-action interactions from role-playing game transcripts</title>
		<author>
			<persName><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Short Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="708" to="713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scruples: A corpus of community ethical judgments on 32, 000 real-life anecdotes</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Le Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="13470" to="13479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A corpus and cloze evaluation for deeper understanding of commonsense stories</title>
		<author>
			<persName><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="839" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Don&apos;t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</title>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1797" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Morality classification in natural language text</title>
		<author>
			<persName><surname>Matheus C Pavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G Dos</forename><surname>Vitor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex Gj</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caio</forename><surname>Wesley R Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><forename type="middle">B</forename><surname>Deutsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><forename type="middle">C</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivandre</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><surname>Paraboni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Plotmachines: Outlineconditioned generation with dynamic plot state tracking</title>
		<author>
			<persName><forename type="first">Asli</forename><surname>Hannah Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4274" to="4295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Beyond accuracy: Behavioral testing of NLP models with CheckList</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongshuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.442</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4902" to="4912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Automatic keyword extraction from individual documents</title>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Cramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendy</forename><surname>Cowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text mining: applications and theory</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Social bias frames: Reasoning about social and power implications of language</title>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saadia</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianhui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.486</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5477" to="5490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Long and diverse text generation with planning-based hierarchical variational model</title>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangtao</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1321</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3257" to="3268" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Parsing argumentation structures in persuasive essays</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Stab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="619" to="659" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Stories told and lessons learned: Toward a narrative approach to moral development and moral education</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Tappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lyn</forename><forename type="middle">Mikel</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Harvard Educational Review</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="182" to="206" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<ptr target="https://www.yuyangushi.com/lz/xgsddl1,886http://www.gushi88.cn/ErTong/ZhongGuoYuYan_1" />
		<title level="m">Links Number</title>
		<imprint>
			<biblScope unit="volume">474</biblScope>
			<biblScope unit="page">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">Grand Total</orgName>
		</author>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<ptr target="https://www.studentuk.com/category/fable101http://www.english-for-students.com/Moral-Stories.html97https://www.advance-africa.com/English-Moral-Stories.html65https://www.gutenberg.org/files/25512/25512-h/25512-h" />
		<title level="m">Links Number</title>
		<imprint>
			<date>193</date>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page">630</biblScope>
		</imprint>
	</monogr>
	<note>htm 52 Others 518 Grand Total 2</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
