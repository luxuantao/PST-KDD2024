<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Optimal Bypass Monitor for High Performance Last-level Caches</title>
				<funder ref="#_Cy2KMtA">
					<orgName type="full">National Science and Technology Major Project of the Ministry of Science and Technology of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lingda</forename><surname>Li</surname></persName>
							<email>lilingda@mprc.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Microprocessor Research and Development Center</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dong</forename><surname>Tong</surname></persName>
							<email>tongdong@mprc.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Microprocessor Research and Development Center</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zichao</forename><surname>Xie</surname></persName>
							<email>xiezichao@mprc.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Microprocessor Research and Development Center</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junlin</forename><surname>Lu</surname></persName>
							<email>lujunlin@mprc.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Microprocessor Research and Development Center</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xu</forename><surname>Cheng</surname></persName>
							<email>chengxu@mprc.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Microprocessor Research and Development Center</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Optimal Bypass Monitor for High Performance Last-level Caches</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>B.3.2 [Memory Structures]: Design Styles-cache memories Design</term>
					<term>Performance Optimal Bypass</term>
					<term>Replacement</term>
					<term>Last-level Cache valid bit. RP: replacement policy bits. P: prefetch bit. SI: IB signature. IT: IB tag. VT: VB tag. RHT: Replacement History Table. BDC: Bypass Decision Counter BDCT: Bypass Decision Counter Table</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the last-level cache, large amounts of blocks have reuse distances greater than the available cache capacity. Cache performance and efficiency can be improved if some subset of these distant reuse blocks can reside in the cache longer. The bypass technique is an effective and attractive solution that prevents the insertion of harmful blocks.</p><p>Our analysis shows that bypass can contribute significant performance improvement, and the optimal bypass can achieve similar performance compared to OPT+B, which is the theoretical optimal replacement policy. Thus, we propose a bypass technique called Optimal Bypass Monitor (OBM), which makes bypass decisions by learning and predicting the behavior of the optimal bypass. OBM keeps a short global track of the incoming-victim block pairs. By detecting the first reuse block in each pair, the behavior of the optimal bypass on the track can be asserted to guide the bypass choice.</p><p>Any existing replacement policy can be extended with OBM while requiring negligible design modification. Our experimental results show that using less than 1.5KB extra memory, OBM with the NRU replacement policy outperforms LRU by 9.7% and 8.9% for single-thread and multiprogrammed workloads respectively. Compared with other state-of-the-art proposals such as DRRIP and SDBP, it achieves superior performance with less storage overhead.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Energy efficiency is treated as a critical metric in modern processor design, and future processors tend to integrate larger last-level caches (LLCs) due to their high energy efficiency <ref type="bibr" target="#b3">[3]</ref>. However, the cache management policy is critical to the processor performance and efficiency. The commonlyused Least Recently Used (LRU) policy and its approximations perform poorly for LLCs because most of the temporal locality is filtered by inner-level caches. This paper focuses on improving the LLC performance and efficiency with very low hardware cost <ref type="foot" target="#foot_0">1</ref> .</p><p>As shown in Figure <ref type="figure">1</ref>, the reuse distances of numerous LLC blocks (cache lines) are greater than the cache size, which leads to the poor performance of LRU policy. Such kinds of cache blocks are called distant reuse blocks in this paper. A good LLC management policy should retain cache blocks with high temporal locality in the LLC first, and then avoid thrashing caused by distant reuse blocks. In order to achieve such goals, various replacement policies <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b38">38]</ref> attempt to insert distant reuse blocks into the LRU position. Dead block prediction techniques <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b22">22]</ref> try to identify distant reuse blocks to evict them earlier. Several adaptive methods <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b32">32]</ref> dynamically change between different replacement policies to accommodate to the changing access patterns. However, such proposals either cannot achieve significant performance improvement, or require significant hardware overhead and large modification to the cache structure. There is still large potential space for the exploration of the LLC management policies.</p><p>Bypass is an effective and attractive technique. Figure <ref type="figure">2</ref> shows that on average 81.2% of blocks are not reused before eviction in a 2MB LLC. Among them, 25.6% of blocks are never accessed again, and they should be bypassed rather than being inserted into the LLC. For the remaining 55.6% of blocks, Belady's OPT <ref type="bibr" target="#b2">[2]</ref> with bypass (OPT+B), which is the theoretical optimal replacement policy, inserts the blocks with minimal reuse distances to fill up the cache at first, and then bypasses the others to avoid thrashing. Besides, our experiments show that the optimal bypass, which bypasses the incoming block if its reuse distance is larger than or equal to that of the victim block selected by the baseline replacement policy, can achieve similar performance improvement compared to OPT+B. Thus, bypass can make significant contribution to the LLC performance. Moreover, bypass can save energy consumption due to the reduction of replacement and writebacks. In this paper, we propose a bypass technique called Optimal Bypass Monitor (OBM), which learns and predicts the behavior of the optimal bypass to make bypass decisions. OBM uses a small Replacement History Table (RHT) to keep track of the recent incoming-victim block pairs. On each cache access, the current incoming block and victim candidate will compare with the RHT content to assert the behavior of the optimal bypass on a recorded pair. A PC indexed Bypass Decision Counter Table (BDCT) of saturating counters is used to learn which operation, bypass or replacement, is dominated under the optimal bypass recently and determine whether bypass should be used.</p><p>OBM can be applied to the current LLC design with any replacement policy, and it requires negligible modification to the existing cache design. Furthermore, OBM is both thread-aware and prefetch-aware.</p><p>We evaluate OBM with NRU, LRU, and SRRIP <ref type="bibr" target="#b12">[12]</ref>. Our evaluation shows that they all improve cache performance significantly while requiring only less than 1.5KB extra storage. Among them, OBM with NRU performs well enough and requires minimal hardware cost. On average it outperforms LRU by 9.7% and 8.9% for single-thread and multi-programmed workloads respectively in the absence of prefetching, and it can also improve performance significantly in the presence of prefetching. Compared to other stateof-the-art proposals including DRRIP <ref type="bibr" target="#b12">[12]</ref>, SDBP <ref type="bibr" target="#b19">[19]</ref>, and DSB <ref type="bibr" target="#b6">[6]</ref>, OBM with NRU has superior performance while requiring less storage overhead.</p><p>The rest of this paper is organized as follows. Section 2 demonstrates the motivation of OBM. Section 3 describes the design and implementation of OBM. We show the experimental methodology and analyze the results in Section 4 and 5. Section 6 discusses some related work. Finally, the paper is concluded in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MOTIVATION</head><p>Bypass is a promising technique for LLCs due to its high performance and efficiency, and thus we aim to design an LLC bypass technique in this paper. In order to understand how bypass should work, we first study OPT <ref type="bibr" target="#b2">[2]</ref> with bypass (OPT+B), which is the theoretical optimal replacement policy for minimal misses. On a miss, among all victim candidates, OPT selects the block with the largest reuse distance for replacement. After enhanced with bypass, OPT+B does not replace any block if the reuse distance of the incoming block is larger than or equal to that of any victim candidate.</p><p>Figure <ref type="figure">3</ref> illustrates the behavior of OPT+B for four representative cache access patterns <ref type="bibr" target="#b12">[12]</ref>. Let ai donate a cache block. (a1, a2, a3, . . . , an) denotes an access sequence from block a1 to block an, and (a1, a2, a3, . . . , an) I denotes that an access sequence repeats I times.</p><p>For cache friendly access patterns, OPT+B behaves as normal replacement policies to insert all incoming blocks because of their good locality. For streaming access patterns, since there is no temporal locality for any block, OPT+B bypasses all incoming blocks. Although these bypasses cannot improve performance, the power used for replacement can be saved to improve efficiency.</p><p>While the performance of LRU and its approximations is close to that of OPT+B for cache friendly and streaming patterns, they perform poorly for the remaining two access patterns. For thrashing access patterns, OPT+B first places a subset of the working set into the cache. Then, the rest of blocks are bypassed to avoid thrashing. In mixed access patterns, blocks with different locality are mixed together. The accesses to blocks with poor locality are called scans because their insertion evicts good blocks in the cache. For the example in Figure <ref type="figure">3</ref>, OPT+B retains d1 and d2 which have the best locality and a subset of ei, while bypasses the rest. Our analysis of OPT+B shows that bypass plays an important role on improving the LLC performance and efficiency.</p><p>OPT+B uses both the optimal replacement and optimal bypass to improve cache performance. However, the optimal bypass itself can have similar performance compared to OP-T+B. The optimal bypass compares the reuse distances of the incoming block and the victim block, which is selected by the baseline replacement policy. If the incoming block has a larger or equal reuse distance, it will be bypassed by the optimal bypass. Otherwise it replaces the victim. Figure <ref type="figure">4</ref> shows that for a 16-way 2MB LLC, the optimal bypass with LRU (LRU+OB) reduces average misses by 23.5% compared to LRU, and it bridges roughly four-fifths of the gap between LRU and OPT+B. Compared to other recent proposals like DRRIP <ref type="bibr" target="#b12">[12]</ref> and SDBP <ref type="bibr" target="#b19">[19]</ref>, it outperforms them dramatically.  Although it is still impractical to implement the optimal bypass due to the need of future information, we can learn the past behavior of the optimal bypass to predict its future behavior. Therefore, to achieve high performance, the proposed bypass technique should behave similarly to the optimal bypass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">OPTIMAL BYPASS MONITOR</head><p>Our goal is to design a new bypass technique, which can make proper bypass decisions by dynamically learning and predicting the behavior of the optimal bypass. Thus, we propose Optimal Bypass Monitor (OBM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>On a cache miss, the optimal bypass determines whether or not to use bypass based on the reuse distances of the incoming block (IB) and its corresponding victim block (VB), which is selected by the baseline replacement policy. Therefore, to learn the behavior of the optimal bypass, we propose Replacement History Table (RHT) to keep track of IB-VB pairs on cache misses. Then according to the relative reuse order on a recorded pair, the behavior of the optimal bypass on that pair can be asserted:</p><p>? If IB is accessed earlier, the reuse distance of IB is smaller than that of VB. For that IB-VB pair, the optimal bypass should replace VB with IB;</p><p>? If VB is accessed earlier, the reuse distance of IB is larger than that of VB. For that IB-VB pair, the optimal bypass should bypass IB;</p><p>? If neither IB nor VB is accessed in the future, the reuse distances of IB and VB are both infinite, and the optimal bypass should also bypass IB.</p><p>These three conditions are called Optimal Bypass Assertions, because we can assert the behavior of the optimal bypass by detecting the occurrence of one certain condition. Table <ref type="table" target="#tab_1">1</ref> shows three Optimal Bypass Assertions. Since Assertion 3 is impossible to detect, we consider that Assertion 3 is satisfied when IB and VB both have reuse distances larger than the cache size. This conversion is reasonable because if both blocks are not reused before eviction, IB should be bypassed to improve efficiency. Our experiments show that the probability of each assertion occurrence is 12.9%, 78.7%, and 8.4% respectively under LRU. Therefore, all assertions are essential to learn the behavior of the optimal bypass accurately.</p><p>Using Optimal Bypass Assertions, we can learn the behavior of the optimal bypass on the RHT content, and then saturating counters called Bypass Decision Counters (BD-Cs) are employed to record the learning results. All BDCs are kept in the Bypass Decision Counter Table (BDCT), and they are initialized to -1. When the behavior of the optimal bypass on a recorded pair is detected, the signature of IB in that pair is used to index the BDCT to update its corresponding BDC. If Assertion 1 is satisfied, the BDC is decreased by 1; if Assertion 2 or 3 is satisfied, it is increased by 1.</p><p>On a miss, the incoming block consults the BDCT to find out whether it should be bypassed. If the BDC indexed by its signature is greater than or equal to 0, it indicates that the optimal bypass uses more bypasses for blocks with that signature recently, and thus OBM predicts that the incoming block should be bypassed. Otherwise, it indicates that replacement is the dominant behavior of the optimal bypass for blocks with that signature recently, and OBM predicts that the incoming block should be placed in the cache.  Figure <ref type="figure">5</ref> and 6 illustrate the structure and algorithm of OBM respectively. OBM requires only a little necessary information from the baseline LLC, and it uses a signal to inform the LLC whether to bypass the current miss or not. Therefore, the existing LLC design does not need to be changed.</p><p>OBM can be used with any deterministic replacement policy. It can also potentially cooperate with non-deterministic replacement policies like the random policy. However, because the random policy chooses victim blocks randomly, which makes the behavior of the optimal bypass less predictable, OBM with the random policy does not work as well as OBM with other policies, although it still outperforms L-RU. Thus, we only evaluate the performance of OBM with some deterministic replacement policies, including the Not Recently Used policy (NRU), LRU, and SRRIP <ref type="bibr" target="#b12">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>The RHT can be organized as fully-associate, set-associate, or direct-associate. In practice we use a 16-way set-associate RHT. Each RHT entry contains 6 fields: a valid bit indicates whether the entry is valid, and an RHT entry is invalidated when the behavior of the optimal bypass on it is detected; the RP bits are used to implement the replacement policy of RHT, which is similar to LRU; the P bit introduces prefetch awareness; the SI bits keep the signature of IB; IT and VT store the tag of IB and VB respectively. We also use partial tags to reduce the RHT storage overhead, and IT and VT store the lower 21 bits of tags instead of the whole tags.</p><p>To reduce the storage overhead, it is not necessary to record all misses in the RHT. A miss should be recorded only if there are invalid entries in the corresponding set of RHT or a low probability is satisfied. Our experiments show that, for a 16-way 128-entry RHT and a 16-way 2MB LLC, OBM can perform well when 1/512 of misses are recorded.</p><p>Various kinds of block signatures can be used to index the BDCT, such as memory address, instruction program counter (PC), or many others <ref type="bibr" target="#b35">[35]</ref>. Previous studies have shown that methods using PC can be more effective than other methods. Therefore, we use the instruction PC which causes the miss as the signature to update and consult the BDCT. For a 1024-entry BDCT, the lower 10 bits of PC are used to index the BDCT. Like all PC based methods <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b37">37]</ref>, the shortened PC is delivered along with the request through the cache hierarchies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Thread-awareness</head><p>OBM is naturally thread-aware because of its PC based design. Since OBM makes bypass decisions based on the behavior of the optimal bypass, it can implicitly partition the shared LLC to minimize the total misses. The program with poor locality will have more blocks bypassed, and thus its cache space is released to improve the performance of programs with good locality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Prefetch-awareness</head><p>Prefetch is an important feature in modern high performance processors, and a simple extension is proposed to make OBM prefetch-aware. Demand and prefetch accesses usually have different behavior. For instance, in a thrash-  ing access pattern, demand accesses have large reuse distances and should be bypassed, while prefetch accesses are expected to be used soon and should be inserted. Therefore, it is reasonable to make bypass decisions for demand and prefetch accesses separately. In the prefetch-aware OBM, we assign an additional BDC for each core, which is dedicated for prefetch accesses. The prefetch bit (P in Figure <ref type="figure">5</ref>) in the RHT entry indicates whether the entry records a prefetch access, and the signature field is used to store the core number of the incoming block instead of the shortened PC when recording a prefetch access.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Simulator</head><p>The simulator we used is CMP$im, a Pin based tracedriven x86 simulator. We use a modified version which is provided for the 1st JWAC Cache Replacement Championship. It models an out-of-order, 4-wide, and 8-stage pipeline with a 128-entry instruction window. The microarchitecture parameters of memory hierarchies are shown in Table <ref type="table" target="#tab_2">2</ref>, which are similar to Intel Core i7 <ref type="bibr" target="#b11">[11]</ref>. The L1 and L2 caches are private to each core. In single-core configuration, the LLC (L3 cache) is 16-way 2MB for single-thread workloads. In 4-core configuration, the LLC is 16-way 8MB for multi-programmed workloads. It also models a stream hardware prefetcher in each L2 cache, and the prefetched blocks are inserted into both the L2 and L3 caches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Benchmarks</head><p>We use SPEC CPU2006 benchmarks <ref type="bibr">[9]</ref> with the first reference inputs to do evaluation. These benchmarks are compiled using GCC 4.5.2 with -O2 optimizations. We use PinPoints <ref type="bibr" target="#b26">[26]</ref> to obtain a single representative one billion instructions for each benchmark. Among these benchmarks, gamess, namd, povray, sjeng, tonto, and specrand are not evaluated because their working sets are so small that their misses are mostly compulsory misses, and the performance improvement is less than 1% when the cache size increases from 2MB to 32MB under LRU. The rest of 23 benchmarks are used in our experiments <ref type="foot" target="#foot_1">2</ref> .</p><p>For multi-programmed workloads, we choose four benchmarks out of our memory-intensive set of SPEC CPU2006 at random to combine into a mix workload. Totally, we create 15 mix workloads. Simulations run until all benchmarks have executed one billion instructions. If one benchmark finishes its one billion instructions early, it restarts from the beginning to continue modeling the contention of four cores. Our experimental methodology is similar to other recent work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS AND ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">OBM with NRU</head><p>At first we evaluate the performance of Optimal Bypass Monitor in the absence of hardware prefetching. Figure <ref type="figure" target="#fig_4">7</ref> shows misses per thousand instructions (MPKI) normalized to LRU for OBM with NRU (NRU+OBM) and other techniques. Besides the baseline NRU and NRU+OBM, we also investigate the recently proposed techniques including DIP <ref type="bibr" target="#b27">[27]</ref>, DRRIP<ref type="foot" target="#foot_2">3</ref>  <ref type="bibr" target="#b12">[12]</ref>, sampling dead block prediction (SDBP) <ref type="bibr" target="#b19">[19]</ref>, and dueling segmented LRU with adaptive bypassing (DSB)<ref type="foot" target="#foot_3">4</ref>  <ref type="bibr" target="#b6">[6]</ref>. While the MPKI of NRU is similar to that of LRU, NRU+OBM reduces MPKI by 15.5% compared to L-RU on average, and the best reduction is 65.1% for sphinx3. NRU+OBM also outperforms other proposals: DIP reduces 8.2% of MPKI on average, while the reduction is 9.7% for DRRIP, 10.4% for SDBP, and 13.0% for DSB.</p><p>Figure <ref type="figure">8</ref> shows the speedup over LRU. The speedup is computed by dividing the IPC of various proposals by the IPC of LRU. The geometric mean speedup of NRU+OBM is 9.7%, while it is 4.9% for DIP, 6.0% for DRRIP, 6.3% for SDBP, and 7.5% for DSB. Moreover, the performance degradation is less than 0.5% compared to the baseline NRU for all benchmarks. Our results show that NRU+OBM can deliver significantly superior performance compared to other recent work.  Figure <ref type="figure">9</ref> shows the fraction of bypassed cache blocks for NRU+OBM. It shows that on average 75.2% of incoming blocks are bypassed. For benchmarks which have a large fraction of blocks not reused in Figure <ref type="figure">2</ref> such as cactusAD-M, libquantum, and sphinx3, most incoming blocks are bypassed and OBM performs well for them. Even for benchmarks which LRU performs well enough like zeusmp, dealII, and astar, roughly half of the misses are bypassed. Although these bypasses do not help to improve the performance, they reduce the writeback number of dirty victim blocks. As a result, the contention on shared system buses is reduced, and the power used to write back dirty victim blocks is saved, too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">OBM with Other Replacement Policies</head><p>Besides NRU, we also investigate the performance of OB-M with LRU and SRRIP. The results are similar to those of NRU+OBM. LRU+OBM outperforms LRU by 9.5%, and the reduction in MPKI is 14.6% on average. SRRIP+OBM outperforms LRU by 9.4% and reduces MPKI by 15.5%. Compared to SRRIP, SRRIP+OBM outperforms by 7.7%. These results show that OBM can cooperate with various replacement policies well. Among these replacement policies, NRU needs the least storage and is the simplest, while the performance of NRU+OBM is comparable to that of L-RU+OBM and SRRIP+OBM. Therefore, we mainly focus on the study of NRU+OBM in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Sensitivity to the Size of RHT and BDCT</head><p>Figure <ref type="figure" target="#fig_8">10</ref> studies the sensitivity of NRU+OBM performance to different size of RHT and BDC using a 1024-entry BDCT. The number of RHT entries is changed from 16 to 1024 while the associativity is fixed at 16, and the probability of recording misses in the RHT is adjusted accordingly. The number of BDC bits is changed from 2 to 5. Figure <ref type="figure" target="#fig_8">10</ref> shows that a 128-entry RHT with 4-bit BDCs can perform sufficiently well. A small RHT with large BDCs performs poorly because the learning process is slower. On the other hand, a large RHT with small BDCs also degrades performance because BDCs are easily affected by accidental events and become more sensitive.</p><p>Figure <ref type="figure">11</ref> studies the performance sensitivity to the BD-CT size when using a 128-entry RHT and 4-bit BDCs. The results show that a 1024-entry BDCT is enough and more entries are not necessary. We also notice that even a 16-entry RHT with a 128-entry BDCT and 3-bit BDCs can achieve a significant speedup of 8.2% compared to LRU, which requires only 0.17KB extra storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Sensitivity to the Cache Size</head><p>Figure <ref type="figure" target="#fig_0">12</ref> shows the speedup of NRU+OBM for different cache sizes. We vary the size of LLC from 512KB to 8MB and the associativity is fixed at 16. The speedup is normalized to the speedup of LRU for the specified cache size. We show the speedup for five representative benchmarks and the geometric mean speedup for all 23 benchmarks. The figure shows that the performance gain of OBM for small caches is limited because there is less wasted space to place distant reuse blocks such as soplex and libquantum. While for large caches, the performance gain decreases because there are fewer distant reuse blocks and the working set is more likely to fit into the cache such as perlbench and hmmer. However, NRU+OBM can still achieve a geometric mean speedup of    5.1% for an 8MB LLC. Therefore, we conclude that OBM is scalable to different cache sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">LRU Insertion Instead of Bypass</head><p>It is straightforward to apply OBM in non-inclusive and exclusive LLCs, but to satisfy inclusion, bypass cannot be used. In order to apply OBM in inclusive LLCs, a reasonable choice is to insert the incoming block into the LRU position instead of bypassing it <ref type="bibr" target="#b27">[27]</ref>. The reason is that when bypass is not allowed, if the reuse distance of the incoming block is larger than or equal to that of any victim candidate, OPT will insert it into the LRU position so that it can be evicted on the next miss. Therefore, we modify OBM to Optimal LRU-insertion Monitor (OLM). OLM uses the penultimate victim block instead of the real victim block to train the RHT. Our experiments show that with the same configuration, the speedup of NRU+OLM is 8.9%. Although it underperforms NRU+OBM slightly, it still outperforms other recent work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Results in the presence of prefetching</head><p>Next, we evaluate the performance of OBM when the hardware prefetcher is enabled. Figure <ref type="figure">13</ref> shows that N-RU+OBM reduces average MPKI by 16.2% and achieves a performance gain of 5.3% compared to LRU in the presence of prefetching. It doubles the performance gain of other recent proposals due to its ability to make bypass decisions for demand and prefetch accesses separately. Compared to LRU without prefetching, it can outperform by 66.1%. The experiments of SRRIP+OBM and LRU+OBM show similar results in the presence of prefetching. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Results for Multi-threaded Workloads</head><formula xml:id="formula_0">IPC i SingleIPC i</formula><p>, where SingleIPC i is got when program i runs alone on an 8MB LRU-managed LLC. NRU+OBM can achieve a normalized weighted speedup of 8.9% on geometric mean, while it is 6.8% for SDBP which performs best among various state-of-the-art techniques. N-RU+OBM improves 14 out of 15 mix workloads more than 1%, and does not degrade performance for any workload. In the presence of prefetching, the performance gain of N-RU+OBM is 6.2% compared to that of LRU with prefetching, which is twice the performance gain of other proposals in which SDBP performs best and outperforms LRU by 3.1%.</p><p>Figure <ref type="figure" target="#fig_11">14</ref>(b) summarizes the results for OBM with other replacement policies. For LRU+OBM, the weighted speedup is 9.3% and 6.1% without and with prefetching respectively. For SRRIP+OBM it is 9.6% and 6.3%. SRRIP+OBM performs best for multi-programmed workloads because SR-RIP can reduce the conflicts between the accesses of different cores by evicting no reuse blocks earlier. These results show that OBM is also an effective technique in multi-core environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Storage, Latency, and Power</head><p>Table <ref type="table" target="#tab_3">3</ref> compares the storage overhead of various techniques for the 2MB LLC used in single-thread experiments. Each RHT entry consists of 1 valid bit, 4 bits for implementing the replacement of RHT, 1 prefetch bit, 10-bit signature, 21-bit IT and 21-bit VT, and each BDC needs 4 bits. It to-    We use CACTI 6.5 <ref type="bibr" target="#b25">[25]</ref> to simulate the latency and power under 32nm process technology. We model the RHT as the tag array of a 32-way associative cache with 256 blocks. The access time of the 2MB LLC is 1.30ns, while the access time of RHT and BDCT is 0.14ns and 0.19ns respectively. It is fast enough to fit well in the access time of LLC. Consequently, OBM does not affect the access latency of LLC.</p><p>The dynamic access energy of LLC is 0.72nJ, and the leakage power is 678.4mW. While OBM has a dynamic access energy of 0.003nJ, and a leakage power of 3.5mW. Assuming the CPU frequency is 3GHz, according to our experiment, the LLC is accessed every 60.5ns on average. Consequently, OBM only consumes 0.51% of the power budget of LLC. What's more, since OBM can reduce LLC misses and thus the execution time of programs, actually it reduces the energy consumption of the memory system and processors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RELATED WORK</head><p>Extensive research has been done to improve the LLC performance, and we will discuss some of the primary studies, which are most relevant to our work.</p><p>Replacement: Lots of LLC replacement policies try to retain some fraction of the working set in caches to avoid thrashing. DIP <ref type="bibr" target="#b27">[27]</ref> attempts to insert most distant reuse blocks into the LRU position when the working set is larger than the cache size. Pseudo-LIFO <ref type="bibr" target="#b4">[4]</ref> prioritizes to evict blocks on the top of fill stack to keep blocks in the bottom longer. A recent proposal guides replacement by explicitly predicting the reuse distance with a PC based predictor <ref type="bibr" target="#b17">[17]</ref>.</p><p>RRIP <ref type="bibr" target="#b12">[12]</ref> can further identify frequently accessed blocks and retain them longer in the cache. Using signatures based on PC or instruction sequences, SHiP <ref type="bibr" target="#b35">[35]</ref> proposes a re-reference interval predictor to improve performance. PACMan <ref type="bibr" target="#b36">[36]</ref> extends RRIP to be prefetch-aware. Shepherd cache <ref type="bibr" target="#b29">[29]</ref> uses two separate caches to record the relative access order of blocks, and then emulates the replacement and bypass decisions of OPT+B.</p><p>Dead Block Prediction: Dead block prediction tries to identify distant reuse blocks, also known as dead blocks in their papers. By evicting or bypassing dead blocks preferentially, the remaining blocks could reside longer in caches. Dead block prediction can be classified into three categories based on how to identify dead blocks: trace based <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b21">21]</ref>, time based <ref type="bibr" target="#b10">[10]</ref>, and counter based <ref type="bibr" target="#b20">[20]</ref>. Cache burst predictor <ref type="bibr" target="#b22">[22]</ref> makes prediction for continuous access sequences rather than individual accesses to improve prediction accuracy. SDBP <ref type="bibr" target="#b19">[19]</ref> samples a part of sets to reduce conflicts in the predictor for high accuracy.</p><p>Bypass: A few researchers have proposed bypass techniques for cache management. Based on how to predict distant reuse blocks, these studies can be classified into PC based <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b33">33]</ref> and address based <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31]</ref>. LRF <ref type="bibr" target="#b37">[37]</ref> combines PC based and address based methods to improve performance. Annex caches <ref type="bibr" target="#b14">[14]</ref> and PCC <ref type="bibr" target="#b34">[34]</ref> filter no reuse blocks before they are brought into the main cache. Dead block prediction and bypass techniques mentioned above are all based on the assumption that all distant reuse blocks are useless. However, as we have shown, many blocks among them are actually useful and should be retained in caches.</p><p>DSB <ref type="bibr" target="#b6">[6]</ref> records the incoming block and the victim block for each set on a miss. Then it adjusts the bypass probability based on which one is accessed first. As stated above, OBM does not use bypass based on a probability, and three Optimal Bypass Assertions are used to learn the behavior of the optimal bypass. Since we collect replacement pairs globally, not for each set, our storage overhead is much lower. Moreover, OBM makes prediction based on PC to improve performance. A bypass and insertion algorithm for exclusive LLCs was presented recently <ref type="bibr" target="#b7">[7]</ref>. It classifies blocks based on their access number in the L2 cache and the hit number in the LLC. NUcache <ref type="bibr" target="#b23">[23]</ref> dedicates a part of LLC to keep distant reuse blocks. Only blocks accessed by selected PCs are inserted into the dedicated part, and the others are bypassed.</p><p>Some researchers have proposed similar policies using some kinds of incoming-victim block relationship. They study the characteristic of accesses using metrics which are either similar to Assertion 1 and 2 <ref type="bibr" target="#b6">[6]</ref>, or similar to Assertion 1 and 3 <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b37">37]</ref>. However, since such proposals do not assert the behavior of the optimal bypass, and they only use a fraction of the three Optimal Bypass Assertions, they are not as accurate as OBM.</p><p>Others: Victim caches <ref type="bibr" target="#b16">[16]</ref> use a small fully-associative buffer to improve direct-associative cache performance. However, as shown in Figure <ref type="figure">1</ref>, the LLC blocks usually have rather large reuse distances, which makes victim caches ineffective for LLCs <ref type="bibr" target="#b18">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>Since the reuse distances of numerous blocks are larger than the cache capacity, the commonly-used LRU and its approximations perform poorly for LLCs. Many previous proposals attempt to address this problem, which either have limited performance or require significant hardware overhead and large modification to the existing cache design. Based on the analysis of the optimal bypass, this paper proposes Optimal Bypass Monitor (OBM). By keeping track of a short replacement history, OBM accurately learns the recent behavior of the optimal bypass to make bypass decisions. Our experiments show that OBM can cooperate well with various replacement policies including NRU, LRU, and SRRIP. Especially, OBM with NRU achieves a significant speedup for both single-thread and multi-programmed workloads, in both cases whether prefetching is enable or not. It also outperforms other state-of-the-art proposals including DIP, DRRIP, SDBP, and DSB. OBM only consumes less than 1.5KB extra storage and does not need to change the original design of LLC.</p><p>To the best of our knowledge, the idea that learns the behavior of the optimal bypass to guide the LLC management is firstly proposed. We believe that OBM can be widely used in other relative research areas, such as memory and storage management <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b24">24]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Normalized hit distribution in the stack of a 64-way 8MB LRU LLC. This figure shows lots of blocks have reuse distances larger than 2MB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: The behavior of OPT+B for representative cache access patterns using a 4-entry cache.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The algorithm of OBM.</figDesc><graphic url="image-2.png" coords="4,327.44,143.82,215.39,212.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Reduction in MPKI normalized to LRU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Figure 8: Speedup for various policies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Sensitivity to the size of RHT and BDC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :Figure 12 Figure 13 :</head><label>111213</label><figDesc>Figure 11: Sensitivity to the size of BDCT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 14 (</head><label>14</label><figDesc>Figure 14(a) shows the weighted speedup normalized to LRU for various techniques on 15 multi-programmed workloads in the absence of prefetching. The weighted speedup is computed as the formula ? 4 i=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>NRU+OBM and other proposals without prefetching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Normalized weighted speedup for multi-programmed workloads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 : Optimal Bypass Assertions to detect the behavior of the optimal bypass. Action</head><label>1</label><figDesc></figDesc><table><row><cell>V</cell><cell>RP</cell><cell>SI</cell><cell>IT</cell><cell>VT</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Demand/prefetch access</cell></row><row><cell></cell><cell></cell><cell>RHT</cell><cell></cell><cell>Miss?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Existing</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>LLC design</cell></row><row><cell></cell><cell></cell><cell>BDCT</cell><cell></cell><cell>Bypass?</cell></row></table><note><p>on IB-VB pair Reuse distance relationship optimal bypass behavior Assertion 1 Current incoming block hits IB. IB's reuse distance &lt; VB's reuse distance Replacement Assertion 2 Current incoming block hits VB. IB's reuse distance &gt; VB's reuse distance Bypass Assertion 3 Current victim block hits IB. IB's reuse distance &gt; cache size and VB's reuse distance &gt; cache size Bypass</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 : Parameters of memory hierarchies</head><label>2</label><figDesc></figDesc><table><row><cell>Parameter</cell><cell>Configuration</cell></row><row><cell>L1 ICache</cell><cell>64B blocks, 32KB, 4-way, 1 cycle, LRU</cell></row><row><cell>L1 DCache</cell><cell>64B blocks, 32KB, 8-way, 1 cycle, LRU</cell></row><row><cell>L2 Cache</cell><cell>64B blocks, 256KB, 8-way, 10 cycles, LRU</cell></row><row><cell>Last-level Cache</cell><cell>64B blocks, 2MB per core, 16-way, 30 cycles</cell></row><row><cell>Memory Latency</cell><cell>200 cycles</cell></row><row><cell>RHT</cell><cell>128-entry, 16-way</cell></row><row><cell>BDCT</cell><cell>4-bit BDC, 1-entry per core+1024-entry</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 : Storage overhead of various techniques for a 16-way 2MB LLC.</head><label>3</label><figDesc>41KB of extra storage to implement OBM for singlecore configuration, which is less than 0.1% of the total storage of a 2MB LLC. For 4-core configuration, only 3 extra BDCs are needed. Compared to other recent proposals, N-RU+OBM requires the lowest storage overhead.</figDesc><table><row><cell></cell><cell cols="4">Speedup 1 Speedup with prefetching 1 Storage per block Extra storage</cell><cell>Total</cell></row><row><cell>LRU</cell><cell>1</cell><cell>1.577</cell><cell>4 bits</cell><cell>0</cell><cell>16KB</cell></row><row><cell>NRU</cell><cell>1.001</cell><cell>1.575</cell><cell>1 bit</cell><cell>0</cell><cell>4KB</cell></row><row><cell>DIP</cell><cell>1.049</cell><cell>1.593</cell><cell>4 bits</cell><cell>10 bits</cell><cell>16KB</cell></row><row><cell>DRRIP</cell><cell>1.060</cell><cell>1.601</cell><cell>2 bits</cell><cell>10 bits</cell><cell>8KB</cell></row><row><cell>SDBP</cell><cell>1.063</cell><cell>1.582</cell><cell>5 bits</cell><cell>9.75KB</cell><cell>29.75KB</cell></row><row><cell>DSB</cell><cell>1.075</cell><cell>1.618</cell><cell>5 bits</cell><cell>11.35KB</cell><cell>31.35KB</cell></row><row><cell>NRU+OBM</cell><cell>1.097</cell><cell>1.661</cell><cell>1 bit</cell><cell>1.41KB</cell><cell>5.41KB</cell></row><row><cell cols="3">1 The speedup is normalized to LRU without prefetching.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">tally consumes ((1+4+1+10+21+21)?128+4?(1024+1))</cell><cell></cell><cell></cell><cell></cell></row><row><cell>bits = 1.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Unless stated otherwise, cache refers to the last-level cache in this paper.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Our infrastructure cannot address gobmk, and previous work has reported that gobmk is not memory-intensive.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We use 2-bit SRRIP and DRRIP in this paper because it performs slightly better in our experiments.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>We use its second configuration which is reported to perform best in their paper.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8.">ACKNOWLEDGMENTS</head><p>We would like to thank the anonymous reviewers for their helpful comments. This work is supported by the <rs type="funder">National Science and Technology Major Project of the Ministry of Science and Technology of China</rs> under grant <rs type="grantNumber">2009ZX01029-001-002-2</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Cy2KMtA">
					<idno type="grant-number">2009ZX01029-001-002-2</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Car: Clock with adaptive replacement</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Modha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST-3</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A study of replacement algorithms for a virtual-storage computer</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Belady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Systems Journal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="78" to="101" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The future of microprocessors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Borkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="67" to="77" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Pseudo-lifo: the foundation of a new family of replacement policies for last-level caches</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chaudhuri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving cache performance by selective cache bypass</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dietz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HICSS-22</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A dueling segmented lru replacement algorithm with adaptive bypassing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JWAC-1</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bypass and insertion algorithms for exclusive last-level caches</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA-38</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A data cache with multiple caching strategies tuned to different types of locality</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gonz?lez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Aliagas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS-9</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spec cpu2006 benchmark descriptions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Henning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Timekeeping in the memory system: predicting and optimizing memory behavior</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA-29</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Intel core i7 processor</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<ptr target="http://www.intel.com/products/processor/corei7/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">High performance cache replacement using re-reference interval prediction (rrip)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Steely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jr</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>In ISCA-37</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A novel approach to cache block reuse predictions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jalminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPP &apos;03</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Design and performance evaluation of a cache assist to implement selective caching</title>
		<author>
			<persName><forename type="first">L</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Subramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCD &apos;97</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Run-time cache bypassing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Connors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Merten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-M</forename><surname>Hwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1338" to="1354" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>Computers</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA-17</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Cache replacement based on reuse-distance prediction</title>
		<author>
			<persName><forename type="first">G</forename><surname>Keramidas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Petoumenos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>In ICCD-25</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Using dead blocks as a virtual victim cache</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACT-19</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sampling dead block prediction for last-level caches</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jimenez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO-43</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Counter-based cache replacement and bypassing algorithms. Computers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kharbutli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="433" to="447" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dead-block prediction &amp; dead-block correlating prefetchers</title>
		<author>
			<persName><forename type="first">A.-C</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA-28</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Cache bursts: A new approach for eliminating dead blocks and increasing cache efficiency</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<idno>MICRO-41</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Nucache: An efficient multicore cache organization based on next-use distance</title>
		<author>
			<persName><forename type="first">R</forename><surname>Manikantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Govindarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA-17</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Arc: A self-tuning, low overhead replacement cache</title>
		<author>
			<persName><forename type="first">N</forename><surname>Megiddo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Modha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>In FAST-2</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cacti 6.0: A tool to understand large caches</title>
		<author>
			<persName><forename type="first">N</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HP Research Report</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pinpointing representative portions of large intel itanium programs with dynamic instrumentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Charney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karunanidhi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>In MICRO-37</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adaptive insertion policies for high performance caching</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Steely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<idno>ISCA-34</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A case for mlp-aware cache replacement</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<idno>ISCA-33</idno>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Emulating optimal replacement with a shepherd cache</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ramaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO-40</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reducing conflicts in direct-mapped caches with a temporality-based design</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rivers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPP &apos;96</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Utilizing reuse information in data cache management</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Rivers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Tyson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farrens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS-12</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Adaptive caches: Effective shaping of cache behavior to workloads</title>
		<author>
			<persName><forename type="first">R</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Smaragdakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
		<idno>MICRO-39</idno>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A modified approach to data cache management</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tyson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farrens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Pleszkun</surname></persName>
		</author>
		<idno>MICRO-28</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pollution control caching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Board</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCD &apos;95</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Ship: Signature-based hit predictor for high performance caching</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hasenplaugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Steely</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>In MICRO-44</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Pacman: Prefetch-aware cache management for high performance caching</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Steely</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>In MICRO-44</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Less reused filter: improving l2 cache performance via filtering less reused lines</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS-23</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pipp: Promotion/insertion pseudo-partitioning of multi-core shared caches</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA-36</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
