<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Overcoming Catastrophic Forgetting by Incremental Moment Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-01-30">30 Jan 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
							<email>slee@bi.snu.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="department">Clova AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
							<email>jhkim@bi.snu.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="department">Clova AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jaehyun</forename><surname>Jun</surname></persName>
							<email>jhjun@bi.snu.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="department">Clova AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
							<email>jungwoo.ha@navercorp.com</email>
						</author>
						<author>
							<persName><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
							<email>btzhang@bi.snu.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="department">Clova AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Naver</forename><surname>Corp</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Surromind</forename><surname>Robotics</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">31st Conference on Neural Information Processing Systems (NIPS 2017)</orgName>
								<address>
									<settlement>Long Beach</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Overcoming Catastrophic Forgetting by Incremental Moment Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-01-30">30 Jan 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">C016770A5440C2BA6B90FA041D674880</idno>
					<idno type="arXiv">arXiv:1703.08475v3[cs.LG]</idno>
					<note type="submission">Mean µ 1 1 1 1 1 1:2 1 2 1 1 2 2 1:2 1 2</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Catastrophic forgetting is a problem of neural networks that loses the information of the first task after training the second task. Here, we propose a method, i.e. incremental moment matching (IMM), to resolve this problem. IMM incrementally matches the moment of the posterior distribution of the neural network which is trained on the first and the second task, respectively. To make the search space of posterior parameter smooth, the IMM procedure is complemented by various transfer learning techniques including weight transfer, L2-norm of the old and the new parameter, and a variant of dropout with the old parameter. We analyze our approach on a variety of datasets including the MNIST, CIFAR-10, Caltech-UCSD-Birds, and Lifelog datasets. The experimental results show that IMM achieves state-of-the-art performance by balancing the information between an old and a new network.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Catastrophic forgetting is a fundamental challenge for artificial general intelligence based on neural networks. The models that use stochastic gradient descent often forget the information of previous tasks after being trained on a new task <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Online multi-task learning that handles such problems is described as continual learning. This classic problem has resurfaced with the renaissance of deep learning research <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Recently, the concept of applying a regularization function to a network trained by the old task to learning a new task has received much attention. This approach can be interpreted as an approximation of sequential Bayesian <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. Representative examples of this regularization approach include learning without forgetting <ref type="bibr" target="#b6">[7]</ref> and elastic weight consolidation <ref type="bibr" target="#b7">[8]</ref>. These algorithms succeeded in some experiments where their own assumption of the regularization function fits the problem.</p><p>Here, we propose incremental moment matching (IMM) to resolve the catastrophic forgetting problem. IMM uses the framework of Bayesian neural networks, which implies that uncertainty is introduced on the parameters in neural networks, and that the posterior distribution is calculated <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. The dimension of the random variable in the posterior distribution is the number of the parameters in the neural networks. IMM approximates the mixture of Gaussian posterior with each component representing parameters for a single task to one Gaussian distribution for a combined task. To merge the posteriors, we introduce two novel methods of moment matching. One is mean-IMM, which simply averages the parameters of two networks for old and new tasks as the minimization of the average of KL-divergence between one approximated posterior distribution for the combined task Mean-IMM simply averages the parameters of two neural networks, whereas mode-IMM tries to find a maximum of the mixture of Gaussian posteriors. To make IMM be reasonable, the search space of the loss function between the posterior means µ 1 and µ 2 should be reasonably smooth and convex-like. To find a µ 2 which satisfies this condition of a smooth and convex-like path from µ 1 , we propose applying various transfer techniques for the IMM procedure.</p><p>and each Gaussian posterior for the single task <ref type="bibr" target="#b10">[11]</ref>. The other is mode-IMM, which merges the parameters of two networks using a Laplacian approximation <ref type="bibr" target="#b8">[9]</ref> to approximate a mode of the mixture of two Gaussian posteriors, which represent the parameters of the two networks.</p><p>In general, it is too naïve to assume that the final posterior distribution for the whole task is Gaussian.</p><p>To make our IMM work, the search space of the loss function between the posterior means needs to be smooth and convex-like. In other words, there should not be high cost barriers between the means of the two networks for an old and a new task. To make our assumption of Gaussian distribution for neural network reasonable, we applied three main transfer learning techniques on the IMM procedure: weight transfer, L2-norm of the old and the new parameters, and our newly proposed variant of dropout using the old parameters. The whole procedure of IMM is illustrated in Figure <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous Works on Catastrophic Forgetting</head><p>One of the major approaches preventing catastrophic forgetting is to use an ensemble of neural networks. When a new task arrives, the algorithm makes a new network, and shares the representation between the tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. However, this approach has a complexity issue, especially in inference, because the number of networks increases as the number of new tasks that need to be learned increases.</p><p>Another approach studies the methods using implicit distributed storage of information, in typical stochastic gradient descent (SGD) learning. These methods use the idea of dropout, maxout, or neural module to distributively store the information for each task by making use of the large capacity of the neural network <ref type="bibr" target="#b3">[4]</ref>. Unfortunately, most studies following this approach had limited success and failed to preserve performance on the old task when an extreme change to the environment occurred <ref type="bibr" target="#b2">[3]</ref>. Alternatively, Fernando et al. <ref type="bibr" target="#b13">[14]</ref> proposed PathNet, which extends the idea of the ensemble approach for parameter reuse <ref type="bibr" target="#b12">[13]</ref> within a single network. In PathNet, a neural network has ten or twenty modules in each layer, and three or four modules are picked for one task in each layer by an evolutionary approach. This method alleviates the complexity issue of the ensemble approach to continual learning in a plausible way.</p><p>The approach with a regularization term also has received attention. Learning without forgetting (LwF) is one example of this approach, which uses the pseudo-training data from the old task <ref type="bibr" target="#b6">[7]</ref>. Before learning the new task, LwF puts the training data of the new task into the old network, and uses the output as pseudo-labels of the pseudo-training data. By optimizing both the pseudotraining data of the old task and the real data of the new task, LwF attempts to prevent catastrophic forgetting. This framework is promising where the properties of the pseudo training set is similar to the ideal training set. Elastic weight consolidation (EWC), another example of this approach, uses sequential Bayesian estimation to update neural networks for continual learning <ref type="bibr" target="#b7">[8]</ref>. In EWC, the posterior distribution trained by the previous task is used to update the new prior distribution. This new prior is used for learning the new posterior distribution of the new task in a Bayesian manner.</p><p>EWC assumes that the covariance matrix of the posterior is diagonal and there are no correlations between the nodes. Though this assumption is fragile, EWC performs well in some domains.</p><p>EWC is a monumental recent work that uses sequential Bayesian for continual learning of neural networks. However, updating the parameter of complex hierarchical models by sequential Bayesian estimation is not new <ref type="bibr" target="#b4">[5]</ref>. Sequential Bayes was used to learn topic models from stream data by Broderick et al. <ref type="bibr" target="#b5">[6]</ref>. Huang et al. applied sequential Bayesian to adapt a deep neural network to the specific user in the speech recognition domain <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. They assigned the layer for the user adaptation and applied MAP estimation to this single layer. Similar to our IMM method, Bayesian moment matching is used for sum-product networks, a kind of deep hierarchical probabilistic model <ref type="bibr" target="#b16">[17]</ref>. Though sum-product networks are usually not scalable to large datasets, their online learning method is useful, and it achieves similar performance to the batch learner. Our method using moment matching focuses on continual learning and deals with significantly different statistics between tasks, unlike the previous method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Incremental Moment Matching</head><p>In incremental moment matching (IMM), the moments of posterior distributions are matched in an incremental way. In our work, we use a Gaussian distribution to approximate the posterior distribution of parameters. Given K sequential tasks, we want to find the optimal parameter µ * 1:K and Σ * 1:K of the Gaussian approximation function q 1:K from the posterior parameter for each kth task, (µ k , Σ k ).</p><formula xml:id="formula_0">p 1:K ≡ p(θ|X 1 , • • • , X K , y 1 , • • • , y K ) ≈ q 1:K ≡ q(θ|µ 1:K , Σ 1:K ) (1) p k ≡ p(θ|X k , y k ) ≈ q k ≡ q(θ|µ k , Σ k )<label>(2)</label></formula><p>q 1:K denotes an approximation of the true posterior distribution p 1:K for the whole task, and q k denotes an approximation of the true posterior distribution p k over the training dataset (X k , y k ) for the kth task. θ denotes the vectorized parameter of the neural network. The dimension of µ k and µ 1:k is D, and the dimension of Σ k and Σ 1:k is D × D, respectively, where D is the dimension of θ.</p><p>For example, a multi-layer perceptrons (MLP) with [784-800-800-800-10] has the number of nodes, D = 1917610 including bias terms.</p><p>Next, we explain two proposed moment matching algorithms for the continual learning of modern deep neural networks. The two algorithms generate two different moments of Gaussian with different objective functions for the same dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Mean-based Incremental Moment Matching (mean-IMM)</head><p>Mean-IMM averages the parameters of two networks in each layer, using mixing ratios α k with</p><formula xml:id="formula_1">K k α k = 1.</formula><p>The objective function of mean-IMM is to minimize the following local KL-distance or the weighted sum of KL-divergence between each q k and q 1:K <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref>:</p><formula xml:id="formula_2">µ * 1:K , Σ * 1:K = argmin µ 1:K ,Σ 1:K K k α k KL(q k ||q 1:K )<label>(3)</label></formula><formula xml:id="formula_3">µ * 1:K = K k α k µ k (4) Σ * 1:K = K k α k (Σ k + (µ k -µ * 1:K )(µ k -µ * 1:K ) T )<label>(5)</label></formula><p>µ * 1:K and Σ * 1:K are the optimal solution of the local KL-distance. Notice that covariance information is not needed for mean-IMM, since calculating µ * 1:K does not require any Σ k . A series of µ k is sufficient to perform the task. The idea of mean-IMM is commonly used in shallow networks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. However, the contribution of this paper is to discover when and how mean-IMM can be applied in modern deep neural networks and to show it can performs better with other transfer techniques.</p><p>Future works may include other measures to merge the networks, including the KL-divergence between q 1:K and the mixture of each q k (i.e. KL(q 1:K || K k α k q k )) <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Mode-based Incremental Moment Matching (mode-IMM)</head><p>Mode-IMM is a variant of mean-IMM which uses the covariance information of the posterior of Gaussian distribution. In general, a weighted average of two mean vectors of Gaussian distributions is not a mode of MoG. In discriminative learning, the maximum of the distribution is of primary interest. According to Ray and Lindsay <ref type="bibr" target="#b20">[21]</ref>, all the modes of MoG with K clusters lie on (K -1)-</p><formula xml:id="formula_4">dimensional hypersurface {θ|θ = ( K k a k Σ -1 k ) -1 ( K k a k Σ -1 k µ k ), 0 &lt; a k &lt; 1 and k a k = 1}. See Appendix A for more details.</formula><p>Motivated by the above description, a mode-IMM approximate MoG with Laplacian approximation, in which the logarithm of the function is expressed by the Taylor expansion <ref type="bibr" target="#b8">[9]</ref>. Using Laplacian approximation, the MoG is approximated as follows:</p><formula xml:id="formula_5">log q 1:K ≈ K k α k log q k + C = - 1 2 θ T ( K k α k Σ -1 k )θ + ( K k α k Σ -1 k µ k )θ + C (6) µ * 1:K = Σ * 1:K • ( K k α k Σ -1 k µ k )<label>(7)</label></formula><formula xml:id="formula_6">Σ * 1:K = ( K k α k Σ -1 k ) -1<label>(8)</label></formula><p>For Equation <ref type="formula" target="#formula_6">8</ref>, we add I to the term to be inverted in practice, with an identity matrix I and a small constant .</p><p>Here, we assume diagonal covariance matrices, which means that there is no correlation among parameters. This diagonal assumption is useful, since it decreases the number of parameters for each covariance matrix from O(D 2 ) to O(D) for the dimension of the parameters D.</p><p>For covariance, we use the inverse of a Fisher information matrix, following <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref>. The main idea of this approximation is that the square of gradients for parameters is a good indicator of their precision, which is the inverse of the variance. The Fisher information matrix for the kth task, F k is defined as:</p><formula xml:id="formula_7">F k = E ∂ ∂µ k ln p(ỹ|x, µ k ) • ∂ ∂µ k ln p(ỹ|x, µ k ) T ,<label>(9)</label></formula><p>where the probability of the expectation follows x ∼ π k and ỹ ∼ p(y|x, µ k ), where π k denotes an empirical distribution of X k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Transfer Techniques for Incremental Moment Matching</head><p>In general, the loss function of neural networks is not convex. Consider that shuffling nodes and their weights in a neural network preserves the original performance. If the parameters of two neural networks initialized independently are averaged, it might perform poorly because of the high cost barriers between the parameters of the two neural networks <ref type="bibr" target="#b22">[23]</ref>. However, we will show that various transfer learning techniques can be used to ease this problem, and make the assumption of Gaussian distribution for neural networks reasonable. In this section, we introduce three practical techniques for IMM, including weight-transfer, L2-transfer, and drop-transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Weight-Transfer</head><p>Weight-transfer initialize the parameters for the new task µ k with the parameters of the previous task µ k-1 <ref type="bibr" target="#b23">[24]</ref>. In our experiments, the use of weight-transfer was critical to the continual learning performance. For this reason, the experiments on IMM in this paper use the weight-transfer technique by default.</p><p>The weight-transfer technique is motivated by the geometrical property of neural networks discovered in the previous work <ref type="bibr" target="#b22">[23]</ref>. They found that there is a straight path from the initial point to the solution without any high cost barrier, in various types of neural networks and datasets. This discovery suggests that the weight-transfer from the previous task to the new task makes a smooth loss This figure shows that there are better solutions between the three locally optimized parameters.</p><p>surface between two solutions for the tasks, so that the optimal solution for both tasks lies on the interpolated point of the two solutions.</p><p>To empirically validate the concept of weight-transfer, we use the linear path analysis proposed by Goodfellow et al. <ref type="bibr" target="#b22">[23]</ref> (Figure <ref type="figure" target="#fig_1">2</ref>). We randomly chose 18,000 instances from the training dataset of CIFAR-10, and divided them into three subsets of 6,000 instances each. These three subsets are used for sequential training by CNN models, parameterized by θ 1 , θ 2 , and θ 3 , respectively. Here, θ 2 is initialized from θ 1 , and then θ 3 is initialized from θ 2 , in the same way as weight-transfer. In this analysis, each loss and accuracy is evaluated at a series of points</p><formula xml:id="formula_8">θ = θ 1 + α(θ 2 -θ 1 ) + β(θ 3 - θ 2 )</formula><p>, varying α and β. In Figure <ref type="figure" target="#fig_1">2</ref>, the loss surface of the model on each online subset is nearly convex. The figure shows that the parameter at 1 3 (θ 1 + θ 2 + θ 3 ), which is the same as the solution of mean-IMM, performs better than any other reference points θ 1 , θ 2 , or θ 3 . However, when θ 2 is not initialized by θ 1 , the convex-like shape disappears, since there is a high cost barrier between the loss function of θ 1 and θ 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">L2-transfer</head><p>L2-transfer is a variant of L2-regularization. L2-transfer can be interpreted as a special case of EWC where the prior distribution is Gaussian with λI as a covariance matrix. In L2-transfer, a regularization term of the distance between µ k-1 and µ k is added to the following objective function for finding µ k , where λ is a hyperparameter:</p><formula xml:id="formula_9">log p(y k |X k , µ k ) -λ • ||µ k -µ k-1 || 2 2 (10)</formula><p>The concept of L2-transfer is commonly used in transfer learning <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> and continual learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> with large λ. Unlike the previous usage of large λ, we use small λ for the IMM procedure. In other words, µ k is first trained by Equation 10 with small λ, and then merged to µ 1:k in our IMM. Since we want to make the loss surface between µ k-1 and µ k smooth, and not to minimize the distance between µ k-1 and µ k . In convex optimization, the L2-regularizer makes the convex function strictly convex. Similarly, we hope L2-transfer with small λ help to find a µ k with a convexlike loss space between µ k-1 and µ k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Drop-transfer</head><p>Drop-transfer is a novel method devised in this paper. Drop-transfer is a variant of dropout where µ k-1 is the zero point of the dropout procedure. In the training phase, the following μk,i is used for the weight vector corresponding to the ith node µ k,i :</p><formula xml:id="formula_10">μk,i = µ k-1,i , if ith node is turned off 1 1-p • µ k,i -p 1-p • µ k-1,i , otherwise<label>(11)</label></formula><p>where p is the dropout ratio. Notice that the expectation of μk,i is µ k,i .</p><p>Table <ref type="table">1</ref>: The averaged accuracies on the disjoint MNIST for two sequential tasks (Top) and the shuffled MNIST for three sequential tasks (Bottom). The untuned setting refers to the most natural hyperparameter in the equation of each algorithm, whereas the tuned setting refers to using heuristic hand-tuned hyperparameters. Hyperparam denotes the main hyperparameter of each algorithm. For IMM with transfer, only α is tuned. The numbers in the parentheses refer to standard deviation. Every IMM uses weight-transfer. There are studies <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b19">20]</ref> that have interpreted dropout as an exponential ensemble of weak learners. By this perspective, since the marginalization of output distribution over the whole weak learner is intractable, the parameters multiplied by the inverse of the dropout rate are used for the test phase in the procedure. In other words, the parameters of the weak learners are, in effect, simply averaged oversampled learners by dropout. At the process of drop-transfer in our continual learning setting, we hypothesize that the dropout process makes the averaged point of two arbitrary sampled points using Equation 11 a good estimator.</p><p>We investigated the search space of the loss function of the MLP trained from the MNIST handwritten digit recognition dataset for with and without dropout regularization, to supplement the evidence of the described hypothesis. Dropout regularization makes the accuracy of a sampled point from dropout distribution and an average point of two sampled parameters, from 0.450 (± 0.084) to 0.950 (± 0.009) and 0.757 (± 0.065) to 0.974 (± 0.003), respectively. For the case of both with and without dropout, the space between two arbitrary samples is empirically convex, and fits to the second-order equation. Based on this experiment, we expect not only that the search space of the loss function between modern neural networks can be easily nearly convex <ref type="bibr" target="#b22">[23]</ref>, but also that regularizers, such as dropout, make the search space smooth and the point in the search space have a good accuracy in continual learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>We evaluate our approach on four experiments, whose settings are intensively used in the previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12]</ref>. For more details and experimental results, see Appendix D. The source code for the experiments is available in Github repository <ref type="foot" target="#foot_0">1</ref> .</p><p>Disjoint MNIST Experiment. The first experiment is the disjoint MNIST experiment <ref type="bibr" target="#b3">[4]</ref>. In this experiment, the MNIST dataset is divided into two datasets: the first dataset consists of only digits {0, 1, 2, 3, 4} and the second dataset consists of the remaining digits {5, 6, 7, 8, 9}  <ref type="figure">4</ref>: Test accuracies of IMM with various transfer techniques on the disjoint MNIST. Both L2transfer and drop-transfer boost the performance of IMM and make the optimal value of α larger than 1/2. However, drop-transfer tends to make the accuracy curve more smooth than L2-transfer does. class joint categorization, unlike the setting in the previous work, which considers two independent tasks of 5-class categorization. Because the inference should decide whether a new instance comes from the first or the second task, our task is more difficult than the task of the previous work.</p><p>We evaluate the models both on the untuned setting and the tuned setting. The untuned setting refers to the most natural hyperparameter in the equation of each algorithm. The tuned setting refers to using heuristic hand-tuned hyperparameters. Consider that tuned hyperparameter setting is often used in previous works of continual learning as it is difficult to define a validation set in their setting. For example, when the model needs to learn from the new task after learning from the old task, a low learning rate or early stopping without a validation set, or arbitrary hyperparameter for balancing is used <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref>. We discover hyperparameters in the tuned setting not only to find the oracle performance of each algorithm, but also to show that there exist some paths consisting of the point that performs reasonably for both tasks. Hyperparam in Table <ref type="table">1</ref> denotes hyperparameter mainly searched in the tuned setting. Table <ref type="table">1</ref> (Top) and Figure <ref type="figure">3</ref> (Left) shows the experimental results from the disjoint MNIST experiment.</p><p>In our experimental setting, the usual SGD-based optimizers always perform less than 50%, because the biases of the output layer for the old task are always pushed to large negative values, which implies that our task is difficult. Figure <ref type="figure">4</ref> also shows that mode-IMM is robust with α and the optimal α of mean-IMM is larger than 1/2 in the disjoint MNIST experiment.</p><p>Shuffled MNIST Experiment. The second experiment is the shuffled MNIST experiment <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref> of three sequential tasks. In this experiment, the first dataset is the same as the original MNIST dataset. However, in the second dataset, the input pixels of all images are shuffled with a fixed, random permutation. In previous work, EWC reaches the performance level of the batch learner, and it is argued that EWC overcomes catastrophic forgetting in some domains. The experimental details are similar to the disjoint MNIST experiment, except all models are allowed to use dropout regularization. In the experiment, the first dataset is the same as the original MNIST dataset. However, in the second and the third dataset, the input pixels of all images are shuffled with a fixed, random permutation, respectively. Therefore, the difficulty of the three datasets is the same, though a different solution is required for each dataset.</p><p>Table <ref type="table">1</ref> (Bottom) and Figure <ref type="figure">3</ref> (Middle) shows the experimental results from the shuffled MNIST experiment. Notice that accuracy of drop-transfer (p = 0.2) alone is 96.86 (± 0.21) and L2-transfer (λ = 1e-4) + drop-transfer (p = 0.4) alone is 97.61 (± 0.15). These results are competitive to EWC without dropout, whose performance is around 97.0.</p><p>ImageNet to CUB Dataset. The third experiment is the ImageNet2CUB experiment <ref type="bibr" target="#b6">[7]</ref>, the continual learning problem from the ImageNet dataset to the Caltech-UCSD Birds-200-2011 finegrained classification (CUB) dataset <ref type="bibr" target="#b27">[28]</ref>. The numbers of classes of ImageNet and CUB dataset are around 1K and 200, and the numbers of training instances are 1M and 5K, respectively. In the ImageNet2CUB experiment, the last-layer is separated for the ImageNet and the CUB task. The structure of AlexNet is used for the trained model of ImageNet <ref type="bibr" target="#b28">[29]</ref>. In our experiment, we match the moments of the last-layer fine-tuning model and the LwF model, with mean-IMM and mode-IMM.</p><p>Figure <ref type="figure">3</ref> (Right) shows that mean-IMM moderately balances the performance of two tasks between two networks. However, the balanced hyperparameter of mode-IMM is far from α = 0.5. We think that it is because the scale of the Fisher matrix F is different between the ImageNet and the CUB task. Since the number of training data of the two tasks is different, the mean of the square of the gradient, which is the definition of F , tends to be different. This implies that the assumption of mode-IMM does not always hold for heterogeneous tasks. See Appendix D.3 for more information including the learning methods of IMM where a different class output layer or a different scale of the dataset is used.</p><p>Our results of IMM with LwF exceed the previous state-of-the-art performance, whose model is also LwF. This is because, in the previous works, the LwF model is initialized by the last-layer finetuning model, not directly by the original AlexNet. In this case, the performance loss of the old task is not only decreased, but also the performance gain of the new task is decreased. The accuracies of our mean-IMM (α = 0.5) are 56.20 and 56.73 for the ImageNet task and the CUB task, respectively. The gains compared to the previous state-of-the-art are +1.13 and -1.14. In the case of mean-IMM (α = 0.8) and mode-IMM (α = 0.99), the accuracies are 55.08 and 59.08 (+0.01, +1.12), and 55.10 and 59.12 (+0.02, +1.35), respectively.</p><p>Lifelog Dataset. Lastly, we evaluate the proposed methods on the Lifelog dataset <ref type="bibr" target="#b11">[12]</ref>. The Lifelog dataset consists of 660,000 instances of egocentric video stream data, collected over 46 days from three participants using Google Glass <ref type="bibr" target="#b29">[30]</ref>. Three class categories, location, sub-location, and activity, are labeled on each frame of video. In the Lifelog dataset, the class distribution changes continuously and new classes appear as the day passes. Table <ref type="table" target="#tab_2">2</ref> shows that mean-IMM and mode-IMM are competitive to the dual-memory architecture, the previous state-of-the-art ensemble model, even though IMM uses single network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>A Shift of Optimal Hyperparameter of IMM. The tuned setting shows there often exists some α which makes the performance of the mean-IMM close to the mode-IMM. However, in the untuned hyperparameter setting, mean-IMM performs worse when more transfer techniques are applied. Our Bayesian interpretation in IMM assumes that the SGD training of the k-th network µ k is mainly affected by the k-th task and is rarely affected by the information of the previous tasks. However, transfer techniques break this assumption; thus the optimal α is shifted to larger than 1/k. Fortunately, mode-IMM works more robustly than mean-IMM where transfer techniques are applied.</p><p>Figure <ref type="figure">4</ref> illustrates the change of the test accuracy curve corresponding to the applied transfer techniques and the following shift of the optimal α in mean-IMM and mode-IMM.</p><p>Bayesian Approach on Continual Learning. Kirkpatrick et al. <ref type="bibr" target="#b7">[8]</ref> interpreted that the Fisher matrix F as weight importance in explaining their EWC model. In the shuffled MNIST experiment, since a large number of pixels always have a value of zero, the corresponding elements of the Fisher matrix are also zero. Therefore, EWC does work by allowing weights to change, which are not used in the previous tasks. On the other hand, mode-IMM also works by selectively balancing between two weights using variance information. However, these assumptions on weight importance do not always hold, especially in the disjoint MNIST experiment. The most important weight in the disjoint MNIST experiment is the bias term in the output layer. Nevertheless, these bias parts of the Fisher matrix are not guaranteed to be the highest value nor can they be used to balance the class distribution between the first and second task. We believe that using only the diagonal of the covariance matrix in Bayesian neural networks is too naïve in general and that this is why EWC failed in the disjoint MNIST experiment. We think it could be alleviated in future work by using a more complex prior, such as a matrix Gaussian distribution considering the correlations between nodes in the network <ref type="bibr" target="#b30">[31]</ref>.</p><p>Balancing the Information of an Old and a New Task. The IMM procedure produces a neural network without a performance loss for kth task µ k , which is better than the final solution µ 1:k in terms of the performance of the kth task. Furthermore, IMM can easily weigh the importance of tasks in IMM models in real time. For example, α t can be easily changed for the solution of mean-IMM µ 1:k = k t α t µ t . In actual service situations of IT companies, the importance of the old and the new task frequently changes in real time, and IMM can handle this problem. This property differentiates IMM from the other continual learning methods using the regularization approach, including LwF and EWC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Our contributions are four folds. First, we applied mean-IMM to the continual learning of modern deep neural networks. Mean-IMM makes competitive results to comparative models and balances the information between an old and a new network. We also interpreted the success of IMM by the Bayesian framework with Gaussian posterior. Second, we extended mean-IMM to mode-IMM with the interpretation of mode-finding in the mixture of Gaussian posterior. Mode-IMM outperforms mean-IMM and comparative models in various datasets. Third, we introduced drop-transfer, a novel method proposed in the paper. Experimental results showed that drop-transfer alone performs well and is similar to the EWC without dropout, in the domain where EWC rarely forgets. Fourth, We applied various transfer techniques in the IMM procedure to make our assumption of Gaussian distribution reasonable. We argued that not only the search space of the loss function among neural networks can easily be nearly convex, but also regularizers, such as dropout, make the search space smooth, and the point in the search space have good accuracy. Experimental results showed that applying transfer techniques often boost the performance of IMM. Overall, we made state-of-theart performance in various datasets of continual learning and explored geometrical properties and a Bayesian perspective of deep neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 IMM with weight-transfer, L2-transfer</head><p>Input: data {(X 1 , y 1 ),...,(X K , y K )}, balancing hyperparameter α Output:</p><formula xml:id="formula_11">w 1:K w 0 ← InitializeNN() for k = 1:K do w k * ← w k-1 Train(w k * , X k , y k ) with L(w k * , X k , y k ) + λ • ||w k * -w k-1 || 2 2 if type is mean-IMM then w 1:k ← k t α t w t * else if type is mode-IMM then F k * ← CalculateFisherMatrix(w k * , X k , y k ) Σ 1:k ← ( k t α t F t * ) -1 w 1:k ← Σ 1:k • ( k t α t F t * w t * ) end if end for</formula><p>have argued that BNN regularizes better than NN, and provides a confidence interval for the output estimation of each input instance. Current research on BNN, to the best of our knowledge, uses Gaussian distributions as the posteriors of the parameters. In the Gaussian assumption, because tracking the entire information of a covariance matrix is too expensive, researchers usually use only the diagonal term for the covariance matrix, where the posterior distribution is fully factorized for each parameter. However, the methods using full covariance were also suggested recently <ref type="bibr" target="#b30">[31]</ref>. To estimate a covariance matrix most studies use stochastic gradient variational Bayes (SGVB), where a sampled point from the posterior distribution by Monte Carlo is used in the training phases <ref type="bibr" target="#b33">[34]</ref>. Alternatively, Kirkpatrick et al. <ref type="bibr" target="#b7">[8]</ref> approximated the covariance matrix as an inverse of a Fisher matrix. This approximation makes the computational cost of the inference of a covariance matrix cheaper when the update of covariance information is not needed in the training phase. Our method follows the approach using the Fisher matrix.</p><p>Elastic Weight Consolidation. We compare the work of Kirkpatrick et al. <ref type="bibr" target="#b7">[8]</ref> to the results of our framework. The mechanism of EWC follows sequential Bayesian estimation. EWC maximizes the following terms by gradient descent to get the solution µ 1:K .</p><formula xml:id="formula_12">log p 1:K ≈ log p(y K |X K , θ) + λ • log p 1:(K-1) + C ≈ log p(y K |X K , θ) + λ • K-1 k=1 log q 1:k + C = log p(y K |X K , θ) - λ 2 • K-1 k=1 (θ -µ 1:k ) T Σ-1 k (θ -µ 1:k ) + C<label>(19)</label></formula><p>p k is empirical posterior distribution of kth task, and q k ∼ N (µ k , Σ k ) is an approximation of p k . In EWC, Σ-1 k is also approximated by the diagonal term of Fisher matrix Fk with respect to µ 1:k and X k .</p><p>When moving to a third task, EWC uses the penalty term of both first and second network (i.e., µ 1 and µ 1:2 ). Although this heuristic works reasonably in the experiments in their paper, it does not match to the philosophy of Bayesian.</p><p>Learning without Forgetting. We compare the work of Li and Hoiem <ref type="bibr" target="#b6">[7]</ref>. Although LwF does not explicitly assume Bayesian, the approach can be represented nonetheless as follows:</p><formula xml:id="formula_13">log p 1:K ≈ log p(y K |X K , θ) + λ • K-1 k=1 log p(ŷ k |X K , θ)<label>(20)</label></formula><p>Where ŷk is the output from µ k with input X K . This framework is promising where the properties of a pseudo training set of kth task (X K , ŷk ) is similar to the ideal training set (X k , y k ).  In our IMM framework, weight-transfer, L2-transfer, and drop-transfer all take µ k-1 as the reference models of the transfer for training µ k . In other words, weight-transfer initializes µ k with µ k-1 , L2transfer uses a regularization term to minimize the Euclidean distance between µ k-1 and µ k , and drop-transfer uses a µ k-1 as the zero point of the dropout procedure. All three transfer techniques can be considered to change the reference point to, for example, µ mean 1:(k-1) or µ mode 1:(k-1) , as previous works do <ref type="bibr" target="#b7">[8]</ref>. However, all these alternatives make performances worse in our shuffled MNIST experiment. We argued that our utilization of transfer techniques is devised not to minimize the distance between µ k-1 and µ k , but to help find a µ k with a smooth and convex-like loss space between µ k-1 and µ k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 ImageNet to Other Image Datasets</head><p>When each task needs a different class output layer, IMM requires additional techniques. There is no counterpart weight matrix in the last-layer of the first network representing the second task, nor the second last-layer of the first network. To tackle this problem, we add the training process of the last-layer fine-tuning model to the IMM procedure; we match the moments of the last-layer finetuning model with the original new network for the new task. Last-layer fine-tuning is the model the last-layer is only fine-tuned for each new task; thus it does not make a performance loss for the first task, but does not often learn enough for new tasks.</p><p>The technique utilizing the last-layer fine-tuning model makes mean-IMM work in the case of different class output layers, but it is not enough for mode-IMM. It is not possible to calculate a proper Fisher matrix of the second last-layer in the first network for the first dataset. As the Fisher matrix is defined with the gradient from the loss of the first task, elements of the Fisher matrix have a value of zero. However, a zero matrix not only is what we do not want but also degenerates the performance of mode-IMM. To tackle this problem, we apply mean-IMM for the last-layer with a re-scaling. We change the mixing ratios α 1 : α 2 to α1 : α2 = α 1 :</p><formula xml:id="formula_14">α 2 • | ŵ1|</formula><p>| ŵ1|+| ŵ2| for the re-scaling, where | ŵ1 | and | ŵ2 | is the average of the whole element of weight matrix in the layer before the last-layer, in the first and the second task.</p><p>In our ImageNet2CUB experiment, the moments of the last-layer fine-tuning model and the LwF model are matched. Though LwF does not perform well in our previous experiments, it is known that LwF performs well when the size of a new dataset is small relative to the old dataset, as in the ImageNet2CUB experiment.</p><p>Figure <ref type="figure" target="#fig_2">5</ref> (Left) compares the performances of mode-IMM models with different assumptions on the Fisher matrix. In naïve mode-IMM, the Fisher matrix of the second last-layer of the first network is a zero matrix. In other words, the second last-layer of the final naïve mode-IMM is the second last-layer of the second network. Naïve mode-IMM does not yield a good performance as we expect.</p><p>Table <ref type="table">3</ref>: Experimental results on the Lifelog dataset. Mean-IMM uses weight-transfer. Classification accuracies among different classes (Top) and different subjects (Bottom). In the experiment, our IMM paradigm achieves competitive results with the approach using an ensemble network, without additional cost for inference and learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm</head><p>Location Sub-location Activity Dual memory architecture <ref type="bibr" target="#b11">[12]</ref> 78 In Figure <ref type="figure" target="#fig_2">5</ref>, scaled mode-IMM denotes the results of mode-IMM re-plotted by the α as we defined above. The result shows that re-scaled mode-IMM performs similarly to mean-IMM in the ImageNet2CUB experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Lifelog Dataset</head><p>The Lifelog dataset is the dataset recorded from Google Glass over 46 days from three participants. The 660,000 seconds of the egocentric video stream data reflects the behaviors of the participants. The dataset consists of 10 days of training data and 4 days of test data in order of time for each participant respectively. In the framework of Lee et al. <ref type="bibr" target="#b11">[12]</ref>, the network can be updated every day, but a new network can be made for the 3rd, 7th, and 10th day, with training data of 3, 4, and 3 days, respectively. Following this framework, our network is made in the 3rd, 7th, and 10th day, and then merged to previously trained networks. Our IMM used AlexNet pretrained by the ImageNet dataset <ref type="bibr" target="#b28">[29]</ref> as the initial network. The experimental results on the Lifelog dataset are in Table <ref type="table">3</ref>, where the performance of models is from Lee et al. <ref type="bibr" target="#b11">[12]</ref> except IMM.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Find ! 2 Figure 1 :</head><label>21</label><figDesc>Figure1: Geometric illustration of incremental moment matching (IMM). Mean-IMM simply averages the parameters of two neural networks, whereas mode-IMM tries to find a maximum of the mixture of Gaussian posteriors. To make IMM be reasonable, the search space of the loss function between the posterior means µ 1 and µ 2 should be reasonably smooth and convex-like. To find a µ 2 which satisfies this condition of a smooth and convex-like path from µ 1 , we propose applying various transfer techniques for the IMM procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Experimental results on visualizing the effect of weight-transfer. The geometric property of the parameter space of the neural network is analyzed. Brighter is better. θ 1 , θ 2 , and θ 3 are the vectorized parameters of trained networks from randomly selected subsets of the CIFAR-10 dataset. This figure shows that there are better solutions between the three locally optimized parameters.</figDesc><graphic coords="5,108.00,72.00,395.99,114.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: (Left) Illustration of the effect of the strategy of re-weighing on the new last-layer. Mode-IMM refers to the original mode-IMM devised for the ImageNet2CUB experiments. In naïve mode-IMM, the second last-layer of the second network is used for the second last-layer of the final IMM model. (Right) The results of mode-IMM with changing the balancing hyperparameter α to the re-scaled balancing hayperparameter α with the scale of the Fisher matrix of each network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. Our task is 10-</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">The disjoint MNIST experiment</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">The shuffled MNIST experiment</cell><cell></cell><cell></cell><cell></cell><cell cols="3">The ImageNet2CUB experiment</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">First Task, Mean-IMM Second Task, Mean-IMM First Task, Mode-IMM</cell><cell></cell><cell>0.995</cell><cell></cell><cell></cell><cell></cell><cell cols="3">First Task, Mean-IMM Second Task, Mean-IMM First Task, Mode-IMM</cell><cell></cell><cell>0.62</cell><cell></cell><cell></cell><cell>First Task, Mean-IMM Second Task, Mean-IMM First Task, Mode-IMM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Second Task, Mode-IMM</cell><cell></cell><cell>0.99</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Second Task, Mode-IMM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Second Task, Mode-IMM</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.985</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Test Accuracy</cell><cell>0.6 0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Test Accuracy</cell><cell>0.97 0.975 0.98</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Test Accuracy</cell><cell>0.56 0.58</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.965</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.54</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.96</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.955</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.52</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0</cell><cell>0.2</cell><cell cols="2">0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell><cell>0.95</cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell><cell></cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">alpha, for weighing two networks</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">alpha, for weighing two networks</cell><cell></cell><cell></cell><cell></cell><cell cols="3">alpha, for weighing two networks</cell></row><row><cell cols="21">Figure 3: Test accuracies of two IMM models with weight-transfer on the disjoint MNIST (Left),</cell></row><row><cell cols="21">the shuffled MNIST (Middle), and the ImageNet2CUB experiment (Right). α is a hyperparameter</cell></row><row><cell cols="16">that balances the information between the old and the new task.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">The disjoint MNIST experiment</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">The disjoint MNIST experiment</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Test Accuracy</cell><cell>0.7 0.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Test Accuracy</cell><cell>0.7 0.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.65</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Mean-IMM</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Mode-IMM</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Mean-IMM</cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Drop-transfer + Mean-IMM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Mode-IMM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Drop-transfer + Mode-IMM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.55</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">L2-transfer + Mean-IMM</cell><cell></cell><cell>0.55</cell><cell></cell><cell></cell><cell></cell><cell cols="3">L2, Drop-transfer + Mean-IMM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">L2-transfer + Mode-IMM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">L2, Drop-transfer + Mode-IMM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.5</cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell></cell><cell>0.8</cell><cell>1</cell><cell></cell><cell>0.5</cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">alpha, for weighing two networks</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">alpha, for weighing two networks</cell><cell></cell></row><row><cell cols="3">Figure</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Experimental results on the Lifelog dataset among different classes (location, sub-location, and activity) and different subjects (A, B, C). Every IMM uses weight-transfer.</figDesc><table><row><cell></cell><cell cols="3">Location Sub-location Activity</cell><cell>A</cell><cell>B</cell><cell>C</cell></row><row><cell>Dual memory architecture [12]</cell><cell>78.11</cell><cell>72.36</cell><cell>52.92</cell><cell>67.02</cell><cell>58.80</cell><cell>77.57</cell></row><row><cell>Mean-IMM</cell><cell>77.60</cell><cell>73.78</cell><cell>52.74</cell><cell>67.03</cell><cell>57.73</cell><cell>79.35</cell></row><row><cell>Mode-IMM</cell><cell>77.14</cell><cell>75.76</cell><cell>54.07</cell><cell>67.97</cell><cell>60.12</cell><cell>78.89</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/btjhjeon/IMM_tensorflow</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Jiseob Kim, Min-Oh Heo, Donghyun Kwak, Insu Jeon, Christina Baek, and Heidi Tessmer for helpful comments and editing. This work was supported by the Naver Corp. and partly by the Korean government (IITP-R0126-16-1072-SW.StarLab, IITP-2017-0-01772-VTT, KEIT-10044009-HRI.MESSI, KEIT-10060086-RISF). Byoung-Tak Zhang is the corresponding author.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A. Modes in the Mixture of Gaussian</head><p>According to Ray and Lindsay <ref type="bibr" target="#b20">[21]</ref>, all the critical points θ of a mixture of Gaussian (MoG) with two components are in one curve as the following equation with 0 &lt; α &lt; 1.</p><p>The proof is as follows. Imagine two Gaussian distribution q 1 and q 2 , such as in Equation <ref type="formula">2</ref>.</p><p>D is the dimension of the Gaussian distribution. Mixture of two Gaussian q 1 and q 2 with the equal mixing ratio (i.e., 1:1) is q 1 /2 + q 2 /2. The derivation of the MoG is as follows:</p><p>If we set Equation 15 to 0, to find all critical points, the following equation holds:</p><p>When α is set to q2 q1+q2 , Equation 12 holds. Note that α k is a function of θ, so θ cannot be calculated in a closed-form from Equation <ref type="formula">16</ref>. However, the optimal θ is in the set {θ|θ = ((1 -α)Σ -1</p><p>2 ), 0 &lt; α &lt; 1}, which motivates our mode-IMM method.</p><p>In our study IMM uses diagonal covariance matrices, which means that there is no correlation between parameters. This diagonal assumption is useful, since it decreases the number of parameters for each covariance matrix from O(D 2 ) to O(D). Based on this, the θ in Equation 12 is defined as follows:</p><p>v denotes an index of the parameter vector. µ •,v and σ 2 •,v are scalar. For MoG with two components in K dimension, the number of modes can be at most K + 1 <ref type="bibr" target="#b31">[32]</ref>. Therefore, it is hard to find all modes in high-dimensional Gaussian in general.</p><p>The property of critical points of a MoG with two components can be extended to the case of K components. The following equation holds:</p><p>where 0 &lt; α k &lt; 1 for all k and k α k = 1. There is no tight upper bound on the number of modes of MoG in general. There is a guess that, for all D, K ≥ 1, the upper bound is (D+K-1) C D <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B. Bayesian Neural Networks and Continual Learning</head><p>Bayesian Neural Networks. Bayesian neural networks (BNN) assume an uncertainty for the whole parameter in neural networks so that the posterior distribution can be obtained <ref type="bibr" target="#b9">[10]</ref>. Previous studies</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C. Example Algorithms of Incremental Moment Matching</head><p>Two moment matching methods: mean-IMM and mode-IMM, and three transfer learning techniques: weight-transfer, L2-transfer, and drop-transfer, are combined to make various continual learning algorithms in our study. Algorithm 1 describes mean-IMM and mode-IMM with weighttransfer and L2-transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX D. Experimental Details</head><p>Appendix D further explains following issues, 1) additional explanation of the untuned setting and tuned setting 2) techniques for IMM with a different class output layer for each task 3) other experimental details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Disjoint MNIST Experiment</head><p>We first explain the untuned setting and the tuned setting in detail. The untuned setting refers to the most natural hyperparameter in the equation of each algorithm, whereas the tuned setting refers to using heuristic hand-tuned hyperparameters. For mean-IMM, it is most natural to evenly average K models and 1/K is the most natural α k value for K sequential tasks. For EWC, 1 is the most natural λ value in Equation <ref type="formula">19</ref>, because EWC is derived from the equation of sequential Bayesian. For L2-transfer, there is no natural hyperparameter value in Equation <ref type="formula">10</ref>, so we need to heuristically choose a λ value, which is not too small but does not damage the performance of the new network for the new task.</p><p>In the SGD, the number of epochs for the dataset (epoch per dataset) for the second task corresponds to the hyperparameter. The unit is how much of the network is trained from the whole data at once. In the L2-transfer and EWC, λ in Equations 10 and 19 corresponds to their hyperparameter. In the mean-IMM and mode-IMM, α K in Equations 4 and 7 corresponds to the hyperparameter. In the drop-transfer, dropout ratio p in Equation 11 corresponds to the hyperparameter.</p><p>All of the explained hyperparameters are devised to balance the information between the old and new tasks. If λ/(1 + λ) = 1 or α 1 = 1, the final network of the algorithms is the same as the network for the first task. If 1/(1 + λ) = 1 or α K = 1, the final network is the same as the network for the last task.</p><p>We used multi-layer perceptrons (MLP) with [784-800-800-10] as the number of nodes, ReLU as the activation function, and vanilla SGD as the optimizer for all of the experiments. We set the epoch per dataset to 10, unless otherwise noted. The entire IMM model uses weight-transfer to smooth the loss surface of the model. Without weight-transfer, our IMM model does not work at all. In our experiments, all models only use one 10-way softmax output layer. For only SGD, dropout is used as proposed in Goodfellow et al. <ref type="bibr" target="#b2">[3]</ref>, but dropout does not help much.</p><p>Each accuracy was measured by averaging the results of 10 experiments. In the experiment, IMM outperforms comparative models by a significant margin. In the tuned experiment, the performance of the IMM models exceeds 90%, and the performance increases more when more transfer techniques are applied. Among all the models, weight-transfer + L2-transfer + drop-transfer + mode-IMM performs the best and its performance is greater than 94%. However, the comparative models fail to reach greater than 90%. Existing regularizer including dropout does not improve the comparative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Shuffled MNIST Experiment</head><p>The second experiment is the shuffled MNIST experiment for three sequential tasks. For the hyperparameter of IMM, we set α 1 and α 2 as the same value, and tune only α 3 . Table <ref type="table">1</ref> (Bottom) shows the experimental results. The performance of SGD + dropout and EWC + dropout comes from the report in <ref type="bibr" target="#b7">[8]</ref>. Changing only the epoch does not significantly increase the performance in SGD. The results show that our IMM paradigm performs similarly to EWC in a case where EWC performs well. Dropout regularization in the task makes both our models and comparative models perform better.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neal</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology of learning and motivation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="165" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting in connectionist networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><surname>French</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="128" to="135" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An empirical investigation of catastrophic forgetting in gradient-based neural networks</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6211</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Compete to compute</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rupesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sohrob</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faustino</forename><surname>Kazerounian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2310" to="2318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Online variational bayesian learning</title>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on Online Learning</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Streaming variational bayes</title>
		<author>
			<persName><forename type="first">Tamara</forename><surname>Broderick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Wibisono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Ashia C Wilson</surname></persName>
		</author>
		<author>
			<persName><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1727" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName><forename type="first">Zhizhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="614" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Proceedings of the National Academy of Sciences</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A practical bayesian framework for backpropagation networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="448" to="472" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weight uncertainty in neural network</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1613" to="1622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hierarchical clustering of a mixture model</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="505" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dual-memory deep learning architectures for lifelong learning of everyday human behaviors</title>
		<author>
			<persName><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Yeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Hyun Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fifth International Joint Conference on Artificial Intelligencee</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1669" to="1675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04671</idno>
		<title level="m">Razvan Pascanu, and Raia Hadsell. Progressive neural networks</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Pathnet: Evolution channels gradient descent in super neural networks</title>
		<author>
			<persName><forename type="first">Chrisantha</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Banarse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yori</forename><surname>Zwols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08734</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Feature space maximum a posteriori linear regression for adaptation of deep neural networks</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabato</forename><surname>Marco Siniscalchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I-Fan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chin-Hui</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Maximum a posteriori adaptation of network parameters in deep models</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabato</forename><surname>Marco Siniscalchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I-Fan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiadong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chin-Hui</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Online and distributed bayesian moment matching for parameter learning in sum-product networks</title>
		<author>
			<persName><forename type="first">Abdullah</forename><surname>Rashwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 19th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1469" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Simplifying mixture models through function approximation</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="644" to="658" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multiparty differential privacy via aggregation of locally trained classifiers</title>
		<author>
			<persName><forename type="first">Manas</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Rane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1876" to="1884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Understanding dropout</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Sadowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2814" to="2822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The topography of multivariate normal mixtures</title>
		<author>
			<persName><forename type="first">Surajit</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><forename type="middle">G</forename><surname>Lindsay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="2042" to="2065" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Revisiting natural gradient for deep networks</title>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3584</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6544</idno>
		<title level="m">Qualitatively neural network optimization problems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Regularized multi-task learning</title>
		<author>
			<persName><forename type="first">Theodoros</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Personalized handwriting recognition via biased regularization</title>
		<author>
			<persName><forename type="first">Wolf</forename><surname>Kienzle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kumar</forename><surname>Chellapilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="457" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<author>
			<persName><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Yeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong-Hyun</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dual-memory neural networks for modeling cognitive activities of humans via wearable sensors</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Structured and efficient variational deep learning with matrix gaussian posteriors</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04733</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the upper bound of the number of modes of a multivariate normal mixture</title>
		<author>
			<persName><forename type="first">Surajit</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multivariate Analysis</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="41" to="52" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Améndola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Engström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Haase</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05066</idno>
		<title level="m">Maximum number of modes of gaussian mixtures</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
