<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reducing the Risk of Query Expansion via Robust Constrained Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Kevyn</forename><surname>Collins-Thompson</surname></persName>
							<email>kevynct@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft</orgName>
								<address>
									<addrLine>Research 1 Microsoft Way Redmond</addrLine>
									<postCode>98052-6399</postCode>
									<region>WA</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reducing the Risk of Query Expansion via Robust Constrained Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1BF9FDE271ACFBF78D47EB5694D8BC09</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Retrieval]: Retrieval Models Algorithms</term>
					<term>Experimentation Query expansion</term>
					<term>convex optimization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a new theoretical derivation, evaluation methods, and extensive empirical analysis for an automatic query expansion framework in which model estimation is cast as a robust constrained optimization problem. This framework provides a powerful method for modeling and solving complex expansion problems, by allowing multiple sources of domain knowledge or evidence to be encoded as simultaneous optimization constraints. Our robust optimization approach provides a clean theoretical way to model not only expansion benefit, but also expansion risk, by optimizing over uncertainty sets for the data. In addition, we introduce risk-reward curves to visualize expansion algorithm performance and analyze parameter sensitivity. We show that a robust approach significantly reduces the number and magnitude of expansion failures for a strong baseline algorithm, with no loss in average gain. Our approach is implemented as a highly efficient post-processing step that assumes little about the baseline expansion method used as input, making it easy to apply to existing expansion methods. We provide analysis showing that this approach is a natural and effective way to do selective expansion, automatically reducing or avoiding expansion in risky scenarios, and successfully attenuating noise in poor baseline methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Despite decades of research on automatic query expansion <ref type="bibr" target="#b11">[12]</ref>, even state-of-the-art methods suffer from drawbacks that have limited their deployment in real-world scenarios. One example of this is their failure to account for the tradeoff between risk and reward: current techniques are optimized to perform well on average, but are unstable and have high variance across queries <ref type="bibr" target="#b9">[10]</ref>. They are ineffective at avoiding risk by operating selectively, reducing or avoiding expansion when it is likely to dramatically decrease performance. In addition, Web search engines must support an increasingly complex decision environment in which potential enhancements to a query may be influenced and constrained by multiple factors including personalization <ref type="bibr" target="#b25">[26]</ref>, implicit and explicit relevance information, and computation budgets. Expansion terms are usually selected individually, when the correct approach should be to optimize over the entire solution as a set. This is particularly problematic for Web search, where the set of related words is forced to be very small for speed and reliability reasons.</p><p>Existing closed-form term-weighting formulas for query expansion find such scenarios difficult or impossible to handle, and cannot easily balance risk and reward. For automatic expansion algorithms to be widely used, is important to find techniques that maintain the good average performance of existing methods, but which are more general and more reliable.</p><p>Convex optimization methods <ref type="bibr" target="#b2">[3]</ref> are a special class of optimization technique that includes well-known algorithms such as least-squares, but with more general capabilities, so that they can handle a much wider set of problems. They provide an attractive starting point for several reasons. Their ability to integrate different criteria for expansion, such as risk and reward, as optimization constraints and objectives allows us to express an extremely rich set of expansion scenarios in simple form. Convex programs also have unique, globally optimal solutions and reliable, highly efficient solution methods. For example, the quadratic programs we develop below can now be solved in just tens of milliseconds for a few hundred variables. Finally, many widely-used functions used in IR tasks, such as vector dot products and KL-divergence, are convex functions, making it possible to cast realistic IR problems as convex programs.</p><p>We contribute a new general formulation of query expansion as a convex robust optimization problem that produces more conservative solutions by optimizing with respect to uncertainty sets defined around the observed data. Because our goal is to mitigate the risk-reward tradeoff of expansion, we introduce risk-reward curves as an evaluation tool. We make a detailed study of algorithm constraints, and demonstrate that ignoring risk leads to less stable expansion algorithms. Our approach significantly reduces the number and magnitude of expansion failures of a strong baseline method with no loss in average gain, while successfully attenuating noise when the baseline is poor quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Theoretical model</head><p>Our goal in this section is to show how query expansion can be cast as a robust convex optimization problem. Instead of deriving a term weighting formula in closed form, we will define a set of objectives and constraints that good expansion models would be expected to satisfy. We then give the resulting convex program to a solver, which either finds an optimal point x ⋆ satisfying the constraints, or determines that no solution is feasible, i.e. can satisfy all constraints.</p><p>We define a query expansion for a query Q of length K as a set of weights x = (x1, . . . , xn) over terms in a vocabulary V, with xi ∈ [0, 1]. Each xi represents a weight for term wi ∈ V. If used with a rounding threshold, xi can also be thought of as a soft decision to include or exclude the term from the expansion 1 . We assume our observations are the initial query Q, and a set of resulting top-ranked documents for the query D.</p><p>For query expansion, the key idea is that we want to balance two criteria: reward and risk. The reward criterion for a feasible expansion x, which we denote R(x), reflects the selection of 'good' individual expansion terms, while the risk criterion, denoted V (x) penalizes choices for which there is greater uncertainty, both for individual terms and the entire expansion set. We now develop these in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Reward objectives</head><p>In general, obtaining an appropriate objective function for query expansion would seem to be very difficult, especially since we want general-purpose methods that can be easily applied to existing algorithms. We want to make few assumptions about the nature of the retrieval model.</p><p>Our solution is to view query expansion in two phases: the first step obtains initial candidate weights p = Φ(Q, D) using a baseline expansion method Φ, that we treat as a black box. The second step is a constrained optimization phase applied to the initial candidates p to obtain the final expansion solution x ⋆ . In that sense, our approach can be seen as a general-purpose wrapper method. Note that we do not assume the baseline method is 'reasonable': in fact, we show in Section 4.6 that our approach can tolerate even the most ill-behaved baselines.</p><p>With this view, choosing a reward objective becomes much easier: we want to bias the solution toward those words with the highest pi values. One simple, effective function is the expected relevance of a solution x: the weighted sum R(x) = p • x = k p k x k . Other choices for R(x) are certainly possible, although we do not explore them here. For example, if p and x represent probability distributions over terms terms, then we could choose KL-divergence R(x) = KL(p||x) as an objective since it is convex.</p><p>For Indri's language model-based expansion, we have Relevance Model estimates p(w|R) over the highest-ranking k documents, where the symbols R and N represent relevance and non-relevance respectively. We can also estimate a nonrelevance model p(w|N ) to approximate non-relevant documents using either the collection, or the bottom-ranked k documents from the initial query Q. To set pi, we first compute p(R | w) for each word w via Bayes Theorem, p(R|w) = p(w|R) p(w|R)+p(w|N ) assuming p(R) = p(N ) = 1/2. Then, using p(R|Q) and p(R| Q) to denote the probability that any query word or non-query word respectively should 1 Useful for search engines without term weighting operators. be included in the expansion, the expected value is then</p><formula xml:id="formula_0">pi = p(R|Q) + (1 -p(R|Q)) • p(R|wi) wi ∈ Q p(R| Q) • p(R|wi) wi / ∈ Q.<label>(1)</label></formula><p>We choose p(R|Q) = 0.75 and p(R| Q) = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Risk objectives</head><p>If reward were our only objective, maximizing the objective R(x) would simply correspond to preferring the expansion terms with highest pi values assigned by the baseline expansion algorithm, given whatever constraints or other conditions, such as sparsity, that were in force. These baseline pi weights, however, are inherently uncertain, and the final expansion solution may vary considerably for different hypotheses about the 'true' pi values. To account for this uncertainty, we invoke robust optimization methods. Robust methods model uncertainty in p by defining an uncertainty set U around p, and then applying a minimax approachthat is, minimizing the worst-case solution over all possible expansions realized by p varying in its uncertainty set U. The result is a more conservative algorithm that avoids relying on data in directions of higher uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Uncertainty in expansion term weights</head><p>We treat p as a random variable varying within an uncertainty set U distributed with mean p and covariance Σ. We define Uκ to be an ellipsoidal region around p</p><formula xml:id="formula_1">Uκ = {u ∈ R n |(u -p) T Σ -1 (u -p) ≤ κ 2 }.<label>(2)</label></formula><p>We set the covariance matrix entries Σij by choosing a term similarity measure σ(wi, wj) and define Σij = d(wi, wj) = exp(-η • σ(wi, wj)), where the constant η is a normalization factor that depends on the measure chosen for σ(wi, wj).</p><p>In this study, σ(wi, wj) was defined using a word similarity measure based on query perturbations described in <ref type="bibr" target="#b5">[6]</ref>. In practice, however, we have also used simpler methods for σ(wi, wj), such as the Jaccard similarity<ref type="foot" target="#foot_0">2</ref> over the 2-by-2 contingency table of term frequencies for wi and wj in the top-ranked documents, with only small reductions in effectiveness.</p><p>The adjustable parameter κ represents our tolerance of risk, with larger κ indicating a higher aversion to risk. When κ = 0, the uncertainty set U0 is a single point, p, and we have our original reward-only problem. If Σ = I, Uκ is the ellipsoid of largest volume inside box B = {u | |ui -pi| ≤ κ}.</p><p>The key result we now use is the following theorem from Ben-Tal &amp; Nemirovski <ref type="bibr" target="#b1">[2]</ref>.</p><p>Theorem 1. The robust counterpart of an uncertain linear program with general ellipsoidal uncertainty can be converted to a conic quadratic program.</p><p>As an example, applying this theorem to the very simple reward-only linear program minimize</p><formula xml:id="formula_2">-p T x (3) subject to 0 ≤ xi ≤ 1 (4) results in the robust quadratic program version minimize -p T x + κ 2 x T Σx (5) subject to 0 ≤ xi ≤ 1 (6)</formula><p>when p varies within the uncertainty set Uκ as defined above. We thus have our risk objective V (x) = κ 2 x T Σx. Combining risk and reward gives the bi-objective function U (x) = -p T x + κ 2 x T Σx.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Uncertainty in the covariance matrix Σ</head><p>We can make the solution even more conservative by defining an uncertainty set for Σ itself. We do this using a simple diagonal matrix W , with Wii reflecting uncertainty in the estimate of Σii. To set Wii, we introduce the idea of term centrality. Previous work <ref type="bibr" target="#b8">[9]</ref>[29] has shown that terms are more reliable for expansion if they are related to multiple query terms. We thus define Wii in terms of the vector di of all similarities of wi with all query terms. This gives Wii = di 2 2 = wq ∈Q d 2 (wi, wq). It can be shown <ref type="bibr" target="#b14">[15]</ref> that the effect of this regularization is a modified covariance matrix Σγ = Σ + γ -1 W , where γ controls the relative influence of the diagonal W . Our joint reward and risk objective becomes</p><formula xml:id="formula_3">U (x) = -p T x + κ 2 x T (Σ + γ -1 W )x. (<label>7</label></formula><formula xml:id="formula_4">)</formula><p>We discuss the settings for κ and γ in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Constraints for query expansion</head><p>We now add domain-specific constraints for query expansion: aspect balance, aspect coverage, and query support. Aspect balance.</p><p>Intuitively, we want an expansion that does not contain words that are all related to just one part of a query. Instead, there should be a balance among its component aspects. We assume a simplistic model where each query term represents a different aspect of the user's information need. We represent each query term q k as a vector φ k (wi) = Σ ik of its similarities to all words in V. We then form a matrix A with K rows A k = φ k . One way to express a 'balanced' solution x with respect to the aspect vectors φ k is to require that the mean of the projection Ax be within a tolerance ζµ of the centroid µ = 1/K φ k , i.e. Ax -µ ζµ where indicates component-wise comparison. Query support. We also want the initial query terms q k to have high weight in the optimal solution. We express this mathematically with simple box constraints, requiring xi to lie above a threshold threshold i for xi ∈ Q as well as below the upper limit ui, which is 1 for all terms. Termspecific values for li may also be desirable to reflect the rarity or ambiguity of individual query terms. We note that we could also implement query support using KL-divergence KL(θQ||x) in a probabilistic model.</p><p>The role of query support constraints differs from interpolation using α in model-based feedback since constraining feedback solutions to have high query support doesn't preclude significant expansion term support if there are many strongly related terms. However, as Section 4 shows, our risk framework is effective at finding expansion models that are closer to the ideal in which the 'right' amount of emphasis on the initial query terms is determined automatically. Our ultimate goal is to make model interpolation unnecessary, but we continue using it here to study the potential for further improvements.</p><formula xml:id="formula_5">minimize -p T x + κ 2 x T Σγx + λy Reward &amp; risk (8) subject to Ax -µ ζµ Aspect balance (9) gi T x ≥ ζi, wi ∈ Q Aspect coverage (10) li ≤ xi ≤ ui, wi ∈ Q Query support (11) w T x ≤ y Budget/sparsity constraint (12) 0 x 1 Label consistency (13)</formula><p>Figure <ref type="figure">1</ref>: The basic constrained quadratic program REXP used for query expansion.</p><p>Aspect coverage. This constraint is useful for retrieval oriented toward recall: it controls the absolute number of related words that are acceptable per query term. Similar to aspect balance, we denote the set of distances to neighboring words of query term q k by the vector g k = φ k . The projection g k T x gives us the aspect coverage, or how well the words selected by the solution x 'cover' term q k . The more expansion terms near q k that are given higher weights, the larger this value becomes. We want the aspect coverage for each of the vectors g k to exceed a threshold ζ k , resulting in the constraint g k T x ≥ ζ k , for all query terms q k . Putting together reward and risk objectives with the above constraints we obtain the final quadratic program, which we call REXP and is shown in Figure <ref type="figure">1</ref>. To show the flexibility of our framework we give two useful extensions: setting budget constraints and finding top-k expansions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Budget constraints</head><p>Since every expansion term with non-zero weight adds runtime cost for the search engine, we may prefer sparse solutions that minimize the number of expansion terms (while still respecting program constraints). More generally, some terms, such as those with very high frequency, may have a higher computation cost than rarer terms due to time required to load inverted lists from disk. All else being equal, we prefer solutions with lower probable computation cost. We handle such scenarios by introducing a weighted ℓ1-norm penalty constraint with weight vector w. Non-zero entries for xi will be discouraged when the corresponding wi value is large. Since x ≥ 0 this leads to the budget constraint w T x ≤ y. We can either set y to a fixed upper bound, or we can add it as a 'soft' constraint in the updated objective U (x) = R(x) + κV (x) + λy where λ controls the relative importance of sparsity against risk and reward objectives. Setting w = 1 gives a standard ℓ1-norm solution. We can penalize common terms by setting wi ∝ f (ti), where f (t) is some function of term frequency of t in the index. If sparsity is critical, with more computation time we can use reweighted ℓ1-norm minimization <ref type="bibr" target="#b12">[13]</ref> to improve the initial ℓ1 solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Producing top-k candidate lists</head><p>In some applications, we may wish to find a set of the best k alternatives, either for presenting alternative expansions to a user, or as a measure of confidence in the optimal expansion: if the sub-optimal expansion scores are far from the optimal we have more confidence in our estimate.</p><p>To do this, given an optimal solution x ⋆ , we gather a set of near-optimal points x η by varying a rounding threshold η from 0 to 1 and setting entries with x η i ≤ η to zero. We then calculate the objective function U (x η ) for each of these alternate expansion solutions and sort by descending U (x η ), selecting the top k expansions from this list 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related work</head><p>Our work was initially inspired by the idea that query expansion can be viewed as a type of portfolio allocation problem under uncertainty, in which a set of financial securities must be selected that maximizes the expected return of the portfolio but also reduces risk by diversifying among different industry sectors <ref type="bibr" target="#b17">[18]</ref>. Typically, there is the option of buying a safe 'risk-free' asset whose return is fixed and known in advance. Applied to IR, the user's query (in theory) represents the 'risk-free asset', and we can place bets on expansion terms 4 . IR has different task-specific constraints and estimating a 'rate of return' for the user's query may be problematic. Robust portfolio allocation methods are welldeveloped in the computational finance community but we have not seen much work in IR fully exploit this connection.</p><p>Our optimization constraints and objectives bring together several previous studies on the nature and causes of query drift <ref type="bibr" target="#b20">[21]</ref> <ref type="bibr" target="#b13">[14]</ref>. Our constraint for aspect balance is based on observations from the 2003 RIA Workshop <ref type="bibr" target="#b3">[4]</ref>. The empirically-derived Local Context Analysis (LCA) <ref type="bibr" target="#b28">[29]</ref> includes a weighting factor preferring expansion terms that co-occur with multiple query terms. This corresponds to the term centrality criterion in our model. Downweighting the contribution of correlated terms has been shown to improve results <ref type="bibr" target="#b20">[21]</ref>, and our correlation matrix Σ plays a similar role. Our query support constraint keeps the expansion model 'close' to the query model, a condition that has been shown to be effective in other approaches <ref type="bibr" target="#b27">[28]</ref> <ref type="bibr" target="#b24">[25]</ref>. The downside risk of query expansion has been noted for decades <ref type="bibr" target="#b22">[23]</ref>: recently this problem has started to get some attention in evaluations <ref type="bibr" target="#b9">[10]</ref> <ref type="bibr" target="#b19">[20]</ref>[1] and we focus extensively on it here.</p><p>Optimization methods have been used implicitly and explicitly for IR problems. Implicit use has been via the use of machine learning techniques such as Support Vector Machines (SVM) Cao et al. <ref type="bibr" target="#b4">[5]</ref> used an SVM to find good individual expansion terms but did not optimize over the expansion terms as a set, instead picking terms using a threshold. Explicitly, unconstrained optimization has been used for query expansion, typically using specialized code for a specific objective. Tao &amp; Zhai <ref type="bibr" target="#b24">[25]</ref> used EM with a non-convex, unconstrained likelihood objective to find a locally optimal expansion model, regularized using a prior that preferred models close to the initial query. Related work in smoothing <ref type="bibr" target="#b18">[19]</ref> used gradient descent to smooth language models by minimizing a graph-based quadratic objective. To our knowledge, the use of robust optimization for IR-related problems has been limited to text classification <ref type="bibr" target="#b14">[15]</ref>.</p><p>In previous work <ref type="bibr" target="#b6">[7]</ref> we introduced an early version of a Markowitz-type optimization framework for more reliable query expansion based on portfolio theory. Our work here greatly extends that initial study by providing the following contributions:</p><p>• A novel theoretical derivation in terms of robust optimization that allows us to define explicit uncertainty 3 This problem has close connections to finding ambiguity groups in fault detection <ref type="bibr" target="#b29">[30]</ref>. 4 Naturally, the actual query may contain typos, misspellings or verbal disagreement. Here we assume that alterations like spelling correction have already been performed. sets around variables of interest, including both individual term relevance scores and the term covariance matrix.</p><p>• New evaluation methods, including risk-reward curves and R-Loss measures for quantifying and visualizing risk-reward tradeoffs of query expansion algorithms.</p><p>• An extensive empirical evaluation, including general IR performance, risk-reward analysis, and parameter sensitivity, across 700 queries from six standard TREC collections.</p><p>• Novel budget constraints for query expansion and an algorithm for finding the k-best expansions.</p><p>More generally, an extensive development of risk-aware theoretical models, algorithms, and evaluation methods was given in the author's doctoral dissertation <ref type="bibr" target="#b7">[8]</ref>. That work introduced the risk framework for query expansion described here and also discussed extensions to other areas of information retrieval. Recently, we note that Wang <ref type="bibr" target="#b26">[27]</ref> applied a Markowitz-type mean-variance objective to balance risk and reward for document ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>In this section we give an extensive analysis showing that applying REXP to a strong, state-of-the-art baseline expansion algorithm (Indri 2.2) not only consistently and substantially reduces the worst-case performance (downside risk) of the baseline expansion algorithm across queries, but does so without reducing its strong average performance. In other words, REXP greatly improves the stability of the baseline expansion algorithm without hurting its overall effectiveness. Moreover, we show that REXP is effective when applied to alternate baseline expansion algorithms: most notably, REXP is effective at attenuating noise when a very poor quality baseline is used as input to REXP instead (Sec. 4.6). Furthermore, we analyze the sensitivity of the algorithms's risk-reward profile to changes in the various parameters and show that a single, consistent set of parameter settings for REXP works well for all collections in the study (Sec. 4.5).</p><p>Because our approach is a post-process that assumes little about the baseline expansion method used as input, we emphasize that the key performance question here is not how absolute expansion gain compares across other studies, but how much relative improvement we gain from applying REXP to an already strong baseline expansion algorithm.</p><p>We report results using standard retrieval measures, robustness histograms, and risk-reward curves. Our evaluation uses six TREC topic sets, totaling 700 unique queries: TREC 1&amp;2 (topics 51-200), TREC 7 (topics 351-400), TREC 8 (topics 401-450), wt10g (topics 451-550), robust2004 (topics 301-450, 601-700), and gov2 (topics 701-850). We chose these corpora for their varied content and document properties. Indexing and retrieval were performed using the Indri system in the Lemur toolkit <ref type="bibr" target="#b23">[24]</ref> <ref type="bibr" target="#b16">[17]</ref>. Our queries were derived from the title field of the TREC topics and phrases were not used. We wrapped the initial query terms with Indri's #combine operator, performed Krovetz stemming, and used a stoplist of 419 common English words.</p><p>For our baseline expansion method, we used the default expansion method in Indri 2.2, which first selects terms using a log-odds calculation, then assigns final term weights using the Relevance Model <ref type="bibr" target="#b15">[16]</ref>: document models were Dirichletsmoothed with µ = 1000. We chose this baseline for its consistently strong average performance: for example, in a TREC evaluation using the GOV2 corpus <ref type="bibr" target="#b10">[11]</ref>, the Indri expansion method gave a 19.8% gain in MAP over unexpanded queries, and achieved an average MAP gain of 14.4% over the six collections in this study. Indri's feedback model is linearly interpolated with the original query model weighted by a parameter α. By default we used the top 50 documents for feedback and the top 20 expansion terms, with the feedback interpolation parameter α = 0.5 unless otherwise stated.</p><p>We set the inputs to REXP as follows. For efficiency, we limited our vocabulary V to the top n = 100 expansion term candidates based on their Relevance Model probability. With these Indri term scores, the entries of the vector p were set using Eq. 1. The matrices Σ, A, and gi were also calculated dynamically for each query using the definitions given in Section 2. The entries of Σ and gi are determined by the definition of the distance function d(wi, wj), which in turn is defined in terms of the similarity function σ(wi, wj).</p><p>Here, experiments used the perturbation kernel measure <ref type="bibr" target="#b5">[6]</ref>, but as discussed earlier a simpler method such as the Jaccard similarity may also be used. The matrix A is a |Q| × K matrix, with each row being the feature vector φ(qi) for query term qi. There is one vector gi for each query term qi ∈ Q, as defined in Sec. 2.3. We set κ = 1.0 and γ = 0.75 after experimenting with a subset of queries from the TREC 1&amp;2 and TREC 7&amp;8 collections: in general, a unified set of parameters appears to work well across all collections in this study, and this is discussed further in Section 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Risk-reward performance</head><p>Informally, a reward measure reflects the quality or relevance of results for a given query, in a way that is appropriate for the task <ref type="foot" target="#foot_1">5</ref> . Because we are interested in ad-hoc retrieval we use Average Precision (AP) as our default reward measure. We call an expansion failure a case where the reward measure from applying expansion to a query is worse than the reward from the initial query results. Mathematically, we denote RI (Q) as the initial reward obtained with the query Q with no expansion, and RF (Q) as the final reward obtained when a query expansion algorithm is applied to Q.</p><p>The key aspects of a risk measure are: 1) that it captures variance or related negative aspect of retrieval performance across queries and 2) this variance/risk is based on the underlying 'reward' measure chosen. To evaluate expansion algorithms, we assume the results from the initial query represent our minimal acceptable retrieval performance: we do not want to obtain worse results than the initial query <ref type="foot" target="#foot_2">6</ref> . We are particularly interested in the downside risk of an algorithm, which we define as the reduction in reward due to expansion failures. Then the downside risk FF AIL(Q) for query Q is simply</p><formula xml:id="formula_6">FF AIL(Q) = RI (Q) -RF (Q) if RI (Q) -RF (Q) &gt; 0 0 otherwise.</formula><p>When the reward measure is precision at the top k documents (P@k) we define R-Loss at k as the net loss of relevant documents in the top-k due to failure. When the reward measure RI (Q) is average precision we refer to this simply as R-Loss, setting k to the size of the retrieved document set (k = 1000 unless otherwise specified). Just as AP gives a combined picture of precision results averaged over multiple values of k, so the R-Loss measure gives an averaged net loss of relevant documents due to failure<ref type="foot" target="#foot_3">7</ref> . We use R-Loss as our default risk measure in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Risk-reward curves</head><p>A risk-reward curve is generated by plotting a risk measure on the x axis, and a reward measure on the y axis, so that we can view how they trade off as the amount of expansion, controlled by the interpolation parameter α, is increased from α = 0 (no expansion at the origin) to α = 1 (all expansion, no initial query). The x-axis summarizes downside risk with R-Loss, the net loss in relevant documents lost due to expansion failures. To emphasize the difference over using no expansion, the y-axis summarizes the change in reward averaged over all queries, that is the percentage MAP gain over using no expansion, so that all curves start at the origin (α = 0). We will typically plot in α increments of 0.1. Risk-reward curves for both the Indri expansion baseline and the robust REXP algorithm are shown in Figure <ref type="figure">2</ref> for all six collections, using MAP as the reward measure with percentage MAP gain on the y-axis. The dashed line is the curve given by the strong baseline Indri expansion algorithm. The solid line is the curve of the resulting robust expansion after REXP is applied to the strong baseline Indri expansion algorithm. We enlarge the point at α = 0.5 since this is our default setting. As discussed further in Section 4.5, all results use the same unified set of parameters that was found to work well across all collections.</p><p>We say that a tradeoff curve A dominates a second curve B if A is higher and to the left of B. An algorithm with a dominant curve gives the same or better reward for any given level of risk. It is evident that, except for one brief segment at the end of the Robust2004 curve, the REXP tradeoff curve dominates the corresponding baseline curve for every topic set. For α = 0.5, the robust algorithm loses fewer relevant documents for all collections, while achieving comparable or higher MAP gain compared to the baseline. Also, the optimal MAP gain for REXP is always higher than the corresponding optimal baseline MAP gain.</p><p>For an alternate perspective, the risk-reward curves using Precision at 20 (P20) as reward measure instead of MAP are shown in Figure <ref type="figure">3</ref> for the same collections and algorithms. Note that while the baseline algorithm significantly hurts performance (negative P20 gain) for values of α close to 1 on five out of six collections, applying REXP results in a far more reliable expansion model that virtually never hurts P20 at any setting of α -only on TREC 7, at the extreme α = 1, does it give a very small loss. Overall, applying REXP consistently (and sometimes dramatically) improves the P20 risk-reward tradeoff across all values of α for five out of six collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">General retrieval measures</head><p>As a general summary statistic for robustness we employ a very simple measure called the robustness index (RI). For a set of queries Q, the RI measure is defined as: RI(Q) = (n+ -n-)/|Q| where n+ is the number of queries helped (i.e. with positive AP gain after expansion) nis the number of queries hurt, and |Q| the total number of queries. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R-Loss Percent MAP Gain</head><p>Robust Baseline (f) gov2</p><p>Figure <ref type="figure">2</ref>: Risk-reward tradeoff curves for six TREC topic sets, showing how the robust REXP optimization step consistently improves the entire risk-reward profile for a strong baseline query expansion method (Indri 2.2). The dashed curve gives the risk-reward profile for the original strong baseline expansion method, and the solid curve shows the resulting robust REXP version. Tradeoff curves that are higher and to the left are better. Points are plotted in α-increments of 0.1, starting with α = 0 at the origin and increasing to α = 1.0. The point at the widely-used setting α = 0.5 is show enlarged on both curves for comparison.  <ref type="table">1</ref>: Performance comparison of baseline (Base-FB) and robust (REXP-FB) feedback. Precision improvement shown for both the Indri expansion baseline Base-FB and the robust version of the expansion baseline REXP-FB is relative to the initial query performance. R-Loss change for REXP is relative to baseline expansion (negative change is good). For Robustness Index (RI), higher is better. Significant differences at the 0.05 level using the Wilcoxon signed-rank test are marked by N and E superscripts, for improvement over NoExp and Base-FB respectively. (f) gov2</p><p>Figure <ref type="figure">3</ref>: Risk-reward tradeoff curves for the same six TREC topic sets as Fig. <ref type="figure">2</ref> above, but using R-Loss@20 as the risk measure on the x-axis, and percentage gain in precision at 20 (P@20) on the y-axis as the reward measure instead of percentage MAP gain. As in Fig. <ref type="figure">2</ref>, the point at the widely-used interpolation setting α = 0.5 is enlarged for comparison on both curves: higher and to the left is better.  Table <ref type="table">1</ref> compares average precision, R-Loss, and RI statistics for the initial, baseline, and REXP feedback methods for specific choices of α = 0.5 (the standard setting). For all six collections, at α = 0.5 the average precision and P20 for REXP are statistically equal or superior to the Indri baseline expansion, while REXP also reduces the number of relevant documents in the top 20 lost to failures (R-Loss@20) by amounts ranging from 34.5% (TREC 8) to 76.9% (TREC 1&amp; 2). (Note that R-Loss is relative to initial retrieval and thus always zero for the no-expansion case.) The total number of relevant documents is shown in the denominator of the R-Loss fraction. Looking at the simple fraction of net queries helped using the Robustness Index (RI), REXP at α = 0.5 outperforms the baseline at α = 0.5 on 5 out of 6 collections, and has equal performance for TREC 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Robustness histograms</head><p>Robustness histograms provide a more detailed look at how badly queries were hurt and helped by an expansion algorithm. Figure <ref type="figure" target="#fig_3">5</ref> gives the combined robustness histogram across the six topic sets for REXP (dark) and the baseline (light). The worst failures -cases where a query's average precision was hurt by more than 60% -have been virtually eliminated by the REXP algorithm, while the upside gain distribution remains very similar to the baseline gains.</p><p>The most noticeable differences in gains are a reduction in the highest category (more than 100% AP gain) and an increase in the lowest gains (0 to 10%). Both of these are due to the selective expansion mechanism of the REXP algorithm: queries that are deemed too risky to expand are not expanded, resulting in a zero AP gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Parameter and constraint sensitivity</head><p>Because the REXP program uses several parameters, we provide a detailed study of how different choices in this parameter space affect performance and how sensitive the quality of the expansion solution is to changes in the parameters. We also show that there is a single, consistent choice of parameters that works well for all six collections we tried.</p><p>Figure <ref type="figure" target="#fig_2">4</ref> summarizes the sensitivity of tradeoff curves to different constraint parameter values. The most dominant tradeoff curves were obtained using an intermediate mix of risk and reward objectives, with all constraints active. Some constraints had a more dramatic effect on the tradeoff curve than others. Query support (li) was a highly influential constraint: it had a strong effect on MAP gain, but little effect on risk. Conversely, the use of off-diagonal Σij covariance entries (γ) had a larger effect on risk reduction (and a weaker effect on MAP). Activating both of these together resulted in most of the improvement in the REXP tradeoff curve. Other constraints such as the aspect balance constraint ζµ were less critical but acted to further shrink the risk of the tradeoff curve with little reduction in MAP. Increasing the aspect coverage parameter ζ k also acted to increase the conservativism of the solution. The role of κ is similar to that of the interpolation α, controlling the mix between a solution close to the original query and one using all expansion terms. We used a setting for REXP parameter values that is effective on all collections: high query support (i = 0.95), moderately relaxed aspect balance (ζµ = 2.0), minimal aspect coverage constraint (ζ k = 0.1 for all q k ), medium covariance regularization (γ = 0.75) and equal objective weighting (κ = 1.0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Alternative expansion baselines</head><p>To show the generality of REXP's black-box approach and its tolerance to noise, we replaced the Indri baseline algorithm with a strong Rocchio-type method <ref type="bibr" target="#b21">[22]</ref> based on a vector-space model, and a noisy idf-only version.</p><p>Rocchio-type.</p><p>We used a Rocchio-style vector space baseline in which the top k document vectors were given equal weight and used a tf.idf representation. For space reasons the tradeoff curves for two representative collections are shown in Figure <ref type="figure">6</ref>: others are similar. As it did with the Relevance model baseline, REXP dominates the Rocchio tf.idf baseline for wt10g. It also reduces R-Loss for trec12, while keeping average MAP gain comparable. High-noise Rocchio. When faced with a poor-quality expansion baseline, a good selective algorithm should usually avoid expansion altogether and revert to the original query: REXP does indeed behave exactly this way. This baseline method is a noisy version of the Rocchio scheme that ignores term frequency (tf ) and uses only idf in the term weighting, which results in a noisy expansion model dominated by rare terms that are poor discriminators for relevance. The results for the same two representative collections, TREC 7 and wt10g, are shown in Figure <ref type="figure">7</ref>. This idf -only baseline has terrible performance, with MAP loss at α = 1.0 worse than -80%. However, REXP using this baseline almost completely attenuates the damage by scaling back to very conservative expansion. At α = 0.5, for TREC 7a, MAP loss is reduced from -11.8% to almost zero (0.88%) with reduction in R-Loss from 1136 to 390. For wt10g, MAP loss is reduced from -35.1% to -6.1% with reduction in R-Loss from 5485 to 1703. The other four standard collections have similar results: REXP MAP loss at α = 0.5 is between 0% and -5%, versus baseline MAP loss of -20% to -40%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Based on our evaluation and observations, we believe there are three distinct capabilities that any expansion algorithm should have to be both reliable and effective. First, uncertainty in the data should be captured and applied to adjust the conservativeness of the solution from query-to-query. In our model this done via the robust problem's uncertainty set U. Second, a Śhard Š selection process should eliminate implausible models completely: if necessary, all hypotheses except the observed query may be rejected. We implement this property by defining a feasible set using hard constraints, such as query support. Such constraints are very effective in attenuating noise when the baseline expansion model is very poor. With sparsity or budget constraints included, this dynamically chooses the number of top-k final expansion terms (including zero terms), rather than forcing us to choose a fixed k in advance. Third, a final process searches for the optimal model based on the objective function over the remaining good (feasible) models, effectively performing a kind of model combination over the space of feasible solutions. Current algorithms implement some of these, but beyond the present work, no existing algorithms that we are aware of effectively address all three requirements at once. The result of combining them is a reliable, selective, effective expansion algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper contributes fundamental new tools for the development and evaluation of query expansion algorithms. By applying concepts from computational finance to cast query expansion as a robust constrained convex optimization problem, we can bring the full power of this technology to bear on expansion problems in order to tradeoff risk and reward and handle domain constraints that would be difficult or impossible using traditional expansion methods. We showed how we can model our uncertainty in important constraints by defining uncertainty sets and minimizing the optimal loss over the uncertainty set. This leads to conservative solutions by using robust optimization versions of the basic program, which turn out to have a simple, efficient analytical form. While most proposed improvements to query expansion only apply to a particular retrieval model, our algorithms treat the retrieval model as a black box which could be implemented using vector space models, inference networks, statistical language modeling, or other approaches. Thus, the approach we have described is broadly applicable.</p><p>We also introduced risk-reward tradeoff curves, which we believe should be a standard evaluation method for query expansion algorithms. With these curves and other evaluation measures, we showed how the downside risk of existing algorithms can be significantly improved, with no loss of average upside gain. Furthermore, we showed that our robust optimization method almost completely attenuates the damage caused by a poor baseline algorithm.</p><p>We also described extensions such as budget constraints and k-best expansions that fit easily into this framework. This work opens new research directions to explore further constraints and objectives, such as biasing expansions with personalization models or implicit and explicit relevance feedback. Finally, further gains may be possible with data-driven learning of constraints or objective parameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The sensitivity of risk-reward tradeoff curves to varying different optimization parameters: aspect coverage (AC: ζi), aspect balance (AB: ζµ), covariance (COV: γ), query support (QT: li). For space reasons, two representative corpora are shown: TREC 1&amp;2 (left) and wt10g (right). The baseline expansion tradeoff curve is also shown (dotted line).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Histograms combining results for all six TREC collections, showing the robust REXP version hurts significantly fewer queries, seen by the greatly reduced tail on the left half (queries hurt). (Recall that MAP performance of REXP is also as good or better than the strong expansion baseline.) The histograms show counts of queries, binned by percent change in MAP, for the REXP algorithm (dark) and baseline (lighter).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: The effect on risk-reward tradeoff curves of applying REXP (solid line) to a Rocchio-style expansion algorithm (dotted line) instead of the default Relevance model baseline. Tradeoff curves that are higher and to the left are better. Points are plotted in α-increments of 0.1, starting with α = 0 at the origin and increasing to α = 1.0.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>The Jaccard similarity is defined as σJ (wi, wj) = M11/(M01 + M10 + M11) where the M jk are the cells of the contigency table containing total counts in the top-ranked documents, and j, k are the binary variables indicating presence or absence of wi and wj respectively in a document.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>For example, Web search might use the average relevance of the top-ranked document (P1) while legal applications may focus on Recall.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>Hypothetically, if we could reliably estimate the quality of the initial results, we could modify this assumption.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3"><p>This weights relevant documents equally, giving more weight to queries with more relevant documents. As with micro/macro-averaging, we could also define a normalized variant of R-Loss to weight all queries equally.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Stephen Boyd for valuable discussions on related work and real-time implementation issues; Jamie Callan, William Cohen, Susan Dumais, and John Lafferty for their extensive feedback on many aspects of this research; and Paul Bennett and Jaime Teevan for their helpful comments and editing suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Query difficulty, robustness, and selective application of query expansion</title>
		<author>
			<persName><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Romano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECIR 2004</title>
		<imprint>
			<biblScope unit="page" from="127" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust solutions of uncertain linear programs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nemirovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OR Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Why current IR engines fail</title>
		<author>
			<persName><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR 2004</title>
		<meeting>SIGIR 2004</meeting>
		<imprint>
			<biblScope unit="page" from="584" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Selecting good expansion terms for pseudo-relevance feedback</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR 2008</title>
		<meeting>SIGIR 2008</meeting>
		<imprint>
			<biblScope unit="page" from="243" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust word similarity estimation using perturbation kernels</title>
		<author>
			<persName><forename type="first">K</forename><surname>Collins-Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Theoretical Information Retrieval (ICTIR)</title>
		<meeting>the International Conference on Theoretical Information Retrieval (ICTIR)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Estimating robust query models using convex optimization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Collins-Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 21 (NIPS)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="329" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Robust model estimation methods for information retrieval</title>
		<author>
			<persName><forename type="first">K</forename><surname>Collins-Thompson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Query expansion using random walk models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Collins-Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM 2005</title>
		<imprint>
			<biblScope unit="page" from="704" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Estimation and use of uncertainty in pseudo-relevance feedback</title>
		<author>
			<persName><forename type="first">K</forename><surname>Collins-Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR 2007</title>
		<meeting>SIGIR 2007</meeting>
		<imprint>
			<biblScope unit="page" from="303" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Initial results with structured queries and language models on half a terabyte of text</title>
		<author>
			<persName><forename type="first">K</forename><surname>Collins-Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ogilvie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC 2005</title>
		<imprint>
			<publisher>NIST Special Publication</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Query expansion</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Efthimiadis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="121" to="187" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Enhancing sparsity by reweighted ℓ1 minimization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P B</forename><surname>Emmanuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">B</forename><surname>Wakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Fourier Analysis and Applications</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The NRRC Reliable Information Access (RIA) workshop</title>
		<author>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR 2004</title>
		<imprint>
			<biblScope unit="page" from="528" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A robust minimax approach to classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="555" to="582" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A Generative Theory of Relevance</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Lemur toolkit for language modeling &amp; retrieval</title>
		<author>
			<persName><surname>Lemur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Markowitz. Portfolio selection</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Finance</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="91" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A general optimization framework for smoothing language models on graph structures</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR 2008</title>
		<meeting>SIGIR 2008</meeting>
		<imprint>
			<biblScope unit="page" from="611" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Latent concept expansion using markov random fields</title>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR 2007</title>
		<imprint>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving automatic query expansion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR 1998</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="206" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The SMART Retrieval System, chapter Relevance Feedback in Information Retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rocchio</surname></persName>
		</author>
		<editor>G. Salton</editor>
		<imprint>
			<date type="published" when="1971">1971</date>
			<publisher>Prentice-Hall</publisher>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The retrieval effects of query expansion on a feedback document retrieval system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="239" to="246" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Indri: A language model-based search engine for complex queries</title>
		<author>
			<persName><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Intel. Analysis</title>
		<meeting>Int. Conf. on Intel. Analysis</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Regularized estimation of mixture models for robust pseudo-relevance feedback</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR 2006</title>
		<imprint>
			<biblScope unit="page" from="162" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Personalizing search via automated analysis of interests and activities</title>
		<author>
			<persName><forename type="first">J</forename><surname>Teevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR 2005</title>
		<imprint>
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mean-variance analysis: A new document ranking theory in information retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECIR 2009</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="4" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards robust query expansion: model selection in the language modeling framework</title>
		<author>
			<persName><forename type="first">M</forename><surname>Winaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kurland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Domshlak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR 2007</title>
		<imprint>
			<biblScope unit="page" from="729" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving the effectiveness of information retrieval with local context analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Information Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="112" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Relaxed maximum a posteriori fault identification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zymnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gorinevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="989" to="999" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
