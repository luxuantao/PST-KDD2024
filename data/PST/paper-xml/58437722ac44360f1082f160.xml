<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VERY DEEP CONVOLUTIONAL NETWORKS FOR END-TO-END SPEECH RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
							<email>yzhang87@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">William</forename><surname>Chan</surname></persName>
							<email>williamchan@cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
							<email>ndjaitly@google.com</email>
						</author>
						<title level="a" type="main">VERY DEEP CONVOLUTIONAL NETWORKS FOR END-TO-END SPEECH RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Automatic Speech Recognition</term>
					<term>End-to-End Speech Recognition</term>
					<term>Very Deep Convolutional Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sequence-to-sequence models have shown success in end-to-end speech recognition. However these models have only used shallow acoustic encoder networks. In our work, we successively train very deep convolutional networks to add more expressive power and better generalization for end-to-end ASR models. We apply network-in-network principles, batch normalization, residual connections and convolutional LSTMs to build very deep recurrent and convolutional structures. Our models exploit the spectral structure in the feature space and add computational depth without overfitting issues. We experiment with the WSJ ASR task and achieve 10.5% word error rate without any dictionary or language model using a 15 layer deep network.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The sequence-to-sequence (seq2seq) model with attention <ref type="bibr" target="#b0">[1]</ref> has recently demonstrated a promising new direction for ASR that entirely sidesteps the complicated machinery developed for classical ASR <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. It is able to do this because it is not restricted by the classical independence assumptions of Hidden Markov Model (HMM) and Connectionist Temporal Classification (CTC) <ref type="bibr" target="#b6">[7]</ref> models. As a result, a single end-to-end model can jointly accomplish the ASR task within one single large neural network.</p><p>The foundational work on seq2seq models, however, has relied on simple neural network encoder and decoder models using recurrent models with LSTMs <ref type="bibr" target="#b5">[6]</ref> or GRUs <ref type="bibr" target="#b3">[4]</ref>. However, their use of hierarchy in the encoders demonstrates that better encoder networks in the model should lead to better results. In this work we significantly extend the state of the art in this area by developing very deep hybrid convolutional and recurrent models, using recent developments in the vision community.</p><p>Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b7">[8]</ref> have been successfully applied to many ASR tasks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. Unlike Deep Neural Networks (DNNs) <ref type="bibr" target="#b11">[12]</ref>, CNNs explicitly exploit structural locality in the spectral feature space. CNNs use shared weight filters and pooling to give the model better spectral and temporal invariance properties, thus typically yield better generalized and more robust models compared to DNNs <ref type="bibr" target="#b12">[13]</ref>. Recently, very deep CNNs architectures <ref type="bibr" target="#b13">[14]</ref> have also been shown to be successful in ASR <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>, using more non-linearities, but fewer parameters. Such a strategy can lead to more expressive models with better generalization.</p><p>While very deep CNNs have been successfully applied to ASR, recently there have been several advancements in the computer vision community on very deep CNNs <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18]</ref> that have not been * Work done as Google Brain interns. explored in the speech community. We explore and apply some of these techniques in our end-to-end speech model:</p><p>1. Network-in-Network (NiN) <ref type="bibr" target="#b18">[19]</ref> increases network depth through the use of 1x1 convolutions. This allows us to increase the depth and expressive power of a network while reducing the total number of parameters that would have been needed otherwise to build such deeper models. NiN has seen great success in computer vision, building very deep models <ref type="bibr" target="#b17">[18]</ref>. We show how to apply NiN principles in hierarchical Recurrent Neural Networks (RNNs) <ref type="bibr" target="#b19">[20]</ref>.</p><p>2. Batch Normalization (BN) <ref type="bibr" target="#b20">[21]</ref> normalizes each layer's inputs to reduce internal covariate shift. BN speeds up training and acts as an regularizer. BN has also seen success in endto-end CTC models <ref type="bibr" target="#b21">[22]</ref>. The seq2seq attention mechanism <ref type="bibr" target="#b0">[1]</ref> has high variance in the gradient (especially from random initialization); without BN we were unable to train the deeper seq2seq models we demonstrate in this paper. We extend on previous work and show how BN can be applied to seq2seq acoustic model encoders.</p><p>3. Residual Networks (ResNets) <ref type="bibr" target="#b22">[23]</ref> learns a residual function of the input through the usage of skip connections. ResNets allow us to train very deep networks without suffering from poor optimization or generalization which typically happen when the network is trapped at a local minima. We explore these skip connections to build deeper acoustic encoders.</p><p>4. Convolutional LSTM (ConvLSTM) <ref type="bibr" target="#b23">[24]</ref> use convolutions to replace the inner products within the LSTM unit. ConvL-STM allows us to maintain structural representations in our cell state and output. Additionally, it allows us to add more compute to the model while reducing the number of parameters for better generalization. We show how ConvLSTMs can be beneficial and replace LSTMs.</p><p>We are driven by same motivation that led to the success of very deep networks in vision <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref> -add depth of processing using more non-linearities and expressive power, while keeping the number of parameters manageable, in effect increasing the amount of computation per parameter. In this paper, we use very deep CNN techniques to significantly improve over previous shallow seq2seq speech recognition models <ref type="bibr" target="#b3">[4]</ref>. Our best model achieves a WER of 10.53% where our baseline acheives a WER of 14.76%. We present detailed analysis on how each technique improves the overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MODEL</head><p>In this section, we will describe the details of each component of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Listen, Attend and Spell</head><p>Listen, Attend and Spell (LAS) <ref type="bibr" target="#b2">[3]</ref> is an attention-based seq2seq model which learns to transcribe an audio sequence to a word sequence, one character at a time. Let x = (x1, . . . , xT ) be the input sequence of audio frames, and y = (y1, . . . , yS) be the output sequence of characters. The LAS models each character output yi using a conditional distribution over the previously emitted characters y&lt;i and the input signal x. The probability of the entire output sequence is computed using the chain rule of probabilities:</p><formula xml:id="formula_0">P (y|x) = i P (yi|x, y &lt;i )</formula><p>The LAS model consists of two sub-modules: the listener and the speller. The listener is an acoustic model encoder and the speller is an attention-based character decoder. The encoder (the Listen function) transforms the original signal x into a high level representation h = (h1, . . . , hU ) with U â‰¤ T . The decoder (the AttendAndSpell function) consumes h and produces a probability distribution over character sequences:</p><formula xml:id="formula_1">h = Listen(x)<label>(1)</label></formula><formula xml:id="formula_2">P (y|x) = AttendAndSpell(h)<label>(2)</label></formula><p>The Listen is a stacked Bidirectional Long-Short Term Memory (BLSTM) <ref type="bibr" target="#b24">[25]</ref> network with hierarchical subsampling as described in <ref type="bibr" target="#b2">[3]</ref>. In our work, we replace Listen with a network of very deep CNNs and BLSTMs. The AttendAndSpell is an attention-based transducer <ref type="bibr" target="#b0">[1]</ref>, which generates one character yi at a time:</p><formula xml:id="formula_3">si = DecodeRNN([yiâˆ’1, ciâˆ’1], siâˆ’1) (3) ci = AttentionContext(si, h) (4) p(yi|x, y &lt;i ) = TokenDistribution(si, ci)<label>(5)</label></formula><p>The DecodeRNN produces a transducer state si as a function of the previously emitted token yiâˆ’1, the previous attention context ciâˆ’1, and the previous transducer state siâˆ’1. In our implementation, DecodeRNN is a LSTM <ref type="bibr" target="#b25">[26]</ref> function without peephole connections.</p><p>The AttentionContext function generates ci with a contentbased Multi-Layer Perceptron (MLP) attention network <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Network in Network</head><p>In our study, we add depth through NiN modules in the hierarchical subsampling connections between LSTM layers. We introduce a projected subsampling layer, wherein we simply concatenate two time frames to a single frame, project into a lower dimension and apply BN and ReLU non-linearity to replace the skip subsampling connections in <ref type="bibr" target="#b2">[3]</ref>. Moreover, we further increase the depth of the network by adding more NiN 1 Ã— 1 convolution modules in between each LSTM layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Convolutional Layers</head><p>Unlike fully connected layers, Convolutional Neural Networks (CNNs) take into account the input topology, and are designed to reduce translational variance by using weight sharing with convolutional filters. CNNs have shown improvement over traditional fully-connected deep neural networks on many ASR tasks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b10">11]</ref>, we investigate the effect of convolutional layers in seq2seq models. In a hybrid system, convolutions require the addition of context window for each frame, or a way to treat the full utterance as a single sample <ref type="bibr" target="#b15">[16]</ref>. One advantage of the seq2seq model is that the encoder can compute gradients over an entire utterance at once. Moreover, strided convolutions are an essential element of CNNs. For LAS applying striding is also a natural way to reduce temporal resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Batch Normalization</head><p>Batch normalization (BN) <ref type="bibr" target="#b20">[21]</ref> is a technique to accelerate training and improve generalization, which is widely used in the computer vision community. Given a layer with output x, BN is implemented by normalizing each layer's inputs:</p><formula xml:id="formula_4">BN(x) = Î³ x âˆ’ E[x] (Var[x] + ) 1 2 + Î²<label>(6)</label></formula><p>where Î³ and Î² are learnable parameters. The standard formulation of BN for CNNs can be readily applied to DNN acoustic models and cross-entropy training. For our seq2seq model, since we construct a minibatch containing multiple utterances, we follow the sequencewise normalization <ref type="bibr" target="#b21">[22]</ref>. For each output channel, we compute the mean and variance statistics across all timesteps in the minibatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Convolutional LSTM</head><p>x t</p><formula xml:id="formula_5">h t 1 , c t 1 h t , c t h t+1 , c t+1 x t+1</formula><p>Frequency bands Time Fig. <ref type="figure">1</ref>: The Convolutional LSTM (ConvLSTM) maintains spectral structural locality in its representation. We replace the inner product of the LSTM with convolutions.</p><p>The Convolutional LSTM (ConvLSTM) was first introduced in <ref type="bibr" target="#b23">[24]</ref>. Although the fully connected LSTM layer has proven powerful for handling temporal correlations, it cannot maintain structural locality, and is more prone to overfitting. ConvLSTM is an extension of FC-LSTM which has convolutional structures in both the inputto-state and state-to-state transitions:</p><formula xml:id="formula_6">it = Ïƒ(Wxi * xt + W hi * htâˆ’1 + bi) ft = Ïƒ(W xf * xt + W hf * htâˆ’1 + b f ) ct = ft ctâˆ’1 + it tanh(Wxc * xt + W hc * htâˆ’1 + bc) ot = Ïƒ(Wxo * xt + W ho * htâˆ’1 + bo) ht = ot tanh(ct)<label>(7)</label></formula><p>iteratively from t = 1 to t = T , where Ïƒ( ) is the logistic sigmoid function, it, ft, ot, ct and ht are vectors to represent values of the input gate, forget gate, output gate, cell activation, and cell output at time t, respectively. denotes element-wise product of vectors. W * are the filter matrices connecting different gates, and b * are the corresponding bias vectors. The key difference is that * is now a convolution, while in a regular LSTM * is a matrix multiplication. Figure <ref type="figure">1</ref> shows the internal structure of a convolutional LSTM.</p><p>The state-to-state and input-to-state transitions can be achieved by a convolutional operation (here we ignore the multiple input/output channels). To ensure the attention mechanism can find the relation between encoder output and the test embedding, FC-LSTM is still necessary. However, we can use these ConvLSTMs to build deeper convolutional LSTM networks before the FC-LSTM layers. We expect this type of layer to learn better temporal representations compared to purely convolutional layers while being less prone to overfitting than FC-LSTM layers. We found bidirectional convolutional LSTMs to consistently perform better than unidirectional layers. All experiments reported in this paper used bidirectional models; here on we use convLSTM to mean bidirectional convLSTM.  Deeper networks usually improve generalization and often outperform shallow networks. However, they tend to be harder to train and slower to converge when the model becomes very deep. Several architectures have been proposed recently to enable training of very deep networks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. The idea behind these approaches is similar to the LSTM innovation -the introduction of linear or gated linear dependence between adjacent layers in the NN model to solve the vanishing gradient problem. In this study, we use a residual CNN/LSTM, to train deeper networks. Residual network <ref type="bibr" target="#b22">[23]</ref> contains direct links between the lower layer outputs and the higher layer inputs. It defines a building block:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Residual Network</head><formula xml:id="formula_7">y = F(x, Wi) + x<label>(8)</label></formula><p>where x and y are the input and output vectors of the layers considered. The function F can be one or more convolutional or convL-STM layers. The residual block for different layers is illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. In our experiments, the convolutional based residual block always has a skip connection. However, for the LSTM layers we did not find skip connections necessary. All of the layers use the identity shortcut, and we did not find projection shortcuts to be helpful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head><p>We experimented with the Wall Street Journal (WSJ) ASR task. We used the standard configuration si284 dataset for training, dev93 for validation and eval92 for test evaluation. Our input features were 80 dimensional filterbanks computed every 10ms with delta and deltadelta acceleration normalized with per speaker mean and variance.</p><p>The baseline EncodeRNN function is a 3 layer BLSTM with 256 LSTM units per-direction (or 512 total) and 4 = 2 2 time factor reduction. The DecodeRNN is a 1 layer LSTM with 256 LSTM units. All the weight matrices were initialized with a uniform distribution U(âˆ’0.1, 0.1) and bias vectors to 0. For the convolutional model, all the filter matrices were initialized with a truncated normal distribution N (0, 0.1), and used 32 output channels. Gradient norm clipping to 1 was applied, together with Gaussian weight noise N (0, 0.075) and L2 weight decay 1eâˆ’5 <ref type="bibr" target="#b29">[30]</ref>. We used ADAM with the default hyperparameters described in <ref type="bibr" target="#b30">[31]</ref>, however we decayed the learning rate from 1eâˆ’3 to 1eâˆ’4 after it converged. We used 10 GPU workers for asynchronous SGD under the TensorFlow framework <ref type="bibr" target="#b31">[32]</ref>. We monitor the dev93 Word Error Rate (WER) until convergence and report the corresponding eval92 WER. The models took O( <ref type="formula" target="#formula_3">5</ref>) days to converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Acronyms for different type of layers</head><p>All the residual block follow the structure of Fig. <ref type="figure" target="#fig_1">2</ref>. Here are the acronyms for each component we use in the following subsections: P / 2 subsampling projection layer.</p><p>C (f Ã— t) convolutional layer with filter f and t under frequency and time axis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B batch normalization</head><p>L bidirectional LSTM layer.</p><p>ResCNN residual block with convolutional layer inside.</p><p>ResConvLSTM residual block with convolutional LSTM layer inside.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network in Network for Hierarchical Connections</head><p>We first begin by investigating the acoustic encoder depth of the baseline model without using any convolutional layers. Our baseline model follows <ref type="bibr" target="#b3">[4]</ref> using the skip connection technique in its time reduction. The baseline L Ã— 3 or 3 layer BLSTM acoustic encoder, model achieves a 14.76% WER. When we simply increase the acoustic model encoder depth (i.e., to depth 8), the model does not converge well and we suspect the network to be trapped in poor local minimas. By using the projection subsampling layer as discussed in Section 2.2, we improves our WER to 13.61% WER or a 7.8% relative gain over the baseline.</p><p>We can further increase the depth of the network by adding more NiN 1 Ã— 1 convolution modules in between each LSTM layer. This improves our model's performance further to 12.88% WER or 12.7% relative over the baseline. The BN layers were critical, and without them we found the model did not converge well. Table <ref type="table" target="#tab_0">1</ref> summarizes the results of applying network-in-network modules in the hierarchical subsampling process.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Going Deeper with Convolutions and Residual Connections</head><p>In this subsection, we extend on Section 3.2 and describe experiments in which we build deeper encoders by stacking convolutional layers and residual blocks in the acoustic encoder before the BLSTM. Unlike computer vision applications or truncated BPTT training in ASR, seq2seq models need to handle very long utterances (i.e., &gt;2000 frames). If we simply stack a CNN before the BLSTMs, we quickly run out of GPU memory for deep models and also have excessive computation times. Our strategy to alleviate this problem is to apply striding in the first and second layer of the CNNs to reduce the time dimensionality and memory footprint. We found no gains by simply stacking additional ResLSTM blocks even up to 8 layers. However, we do find gains if we use convolutions. If we stack 2 additional layers of 3 Ã— 3 convolutions our model improves to 11.80% WER or 20% relative over the baseline. If we take this model and add 8 residual blocks (for a total of (2 + (8)2 + 5) = 23 layers in the encoder) our model further improves to 11.11% WER, or a 24.7% relative improvement over the baseline. We found that using 8 residual blocks a slightly outperform 4 residual blocks. Table <ref type="table" target="#tab_2">2</ref>   In this subsection, we investigate the effectiveness of the convolutional LSTM. Table <ref type="table" target="#tab_3">3</ref> compares the effect of using convolutional LSTM layers. It can be observed that a pure ConvLSTM performs much worse than the baseline -we still need the fully connected LSTM <ref type="foot" target="#foot_1">1</ref> . However, replacing the ResConv block with ResConvL-STM as shown in Figure <ref type="figure">3</ref> give us additional 7% relative gains. In our experiments, we always use 3Ã—1 filters for ConvLSTM because the recurrent structure captures temporal information while the convolutions capture spectral structure. We conjecture that the gain is because the convolutional recurrent state maintains spectral structure and reduces overfitting.</p><p>Table <ref type="table" target="#tab_4">4</ref> compares our WSJ results with other published end-toend models. To our knowledge, the previous best reported WER on WSJ without an LM was the seq2seq model with Task Loss Estimation achieving 18.0% WER in <ref type="bibr" target="#b4">[5]</ref>. Our baseline, also a seq2seq model, achieved 14.76% WER. Our model is different from that of <ref type="bibr" target="#b4">[5]</ref> in that we did not use location-based priors on the attention model and we used weight noise. Our best model, shown in Figure <ref type="figure">3</ref>, achieves a WER of 10.53%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>We explored very deep CNNs for end-to-end speech recognition. We applied Network-in-Network principles to add depth and nonlinearities to hierarchical RNNs. We also applied Batch Normalization and Residual connections to build very deep convolutional towers to process the acoustic features. Finally, we also explored Convolutional LSTMs, wherein we replaced the inner product of LSTMs with convolutions to maintain spectral structure in its representation. Together, we added more expressive capacity to build a very deep model without substantially increasing the number of parameters. On the WSJ ASR task, we obtained 10.5% WER without a language model, an 8.5% absolute improvement over published best result <ref type="bibr" target="#b3">[4]</ref>. While we demonstrated our results only on the seq2seq task, we believe this architecture should also significantly help CTC and other recurrent acoustic models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Residual block for different layers. ResCNN is a CNN block with CNN or ConvLSTM, Batch Normalization (BN) and ReLU non-linearities. The ResLSTM is a LSTM block with residual connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>P / 2 + B + R) Ã— 2 + L 13.61 (L + P / 2 + B + R + C(1Ã—1) + BN + R) Ã— 2 + L 12.88</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>We build deeper encoder networks by adding NiN modules in between LSTM layers.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>summarizes the results of these experiments.</figDesc><table><row><cell>Model</cell><cell></cell><cell>WER</cell></row><row><cell>L Ã— 3</cell><cell></cell><cell>14.76</cell></row><row><cell cols="2">NiN (from Section 3.2)</cell><cell>12.88</cell></row><row><cell cols="2">ResLSTM Ã— 8</cell><cell>15.00</cell></row><row><cell>( C(3Ã—3) 2 ( C(3Ã—3) 2 ( C(3Ã—3) 2</cell><cell cols="2">) Ã— 2 + NiN ) Ã— 2 + ResCNN Ã— 4 + NiN 11.30 11.80 ) Ã— 2 + ResCNN Ã— 8 + NiN 11.11</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>We build deeper encoder networks by adding convolution and residual network blocks. The NiN block equals (L + C (1Ã—1) + B + R) Ã— 2 + L). Our best model: includes two convolutional layer at the bottom and followed by four residual block and LSTM NiN block. Each residual block contains one convolutional LSTM layer and one convolutional layer.</figDesc><table><row><cell>3.4. Convolutional LSTM</cell><cell></cell></row><row><cell>LSTM</cell><cell></cell></row><row><cell>Conv (1x1) + BN</cell><cell></cell></row><row><cell>LSTM</cell><cell>Network in Network</cell></row><row><cell>Conv (1x1) + BN</cell><cell></cell></row><row><cell>LSTM</cell><cell></cell></row><row><cell>ResConvLSTM</cell><cell></cell></row><row><cell>ResConvLSTM</cell><cell>Residual block and</cell></row><row><cell></cell><cell>Convolutional LSTM</cell></row><row><cell>ResConvLSTM</cell><cell></cell></row><row><cell>ResConvLSTM</cell><cell></cell></row><row><cell>Conv/2 + BN</cell><cell></cell></row><row><cell></cell><cell>Convolutional Layer</cell></row><row><cell>Conv/2 + BN</cell><cell>with time reduction</cell></row><row><cell>Input</cell><cell></cell></row><row><cell>Fig. 3:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance of models with convolutional LSTM layers. The NiN block equals (L + C (1Ã—1) + B + R) Ã— 2 + L).</figDesc><table><row><cell>Model</cell><cell>WER</cell></row><row><cell>CTC (Graves et al., 2014) [33]</cell><cell>27.3</cell></row><row><cell>seq2seq (Bahdanau et al., 2016) [5]</cell><cell>18.0</cell></row><row><cell>seq2seq (our work)</cell><cell>14.76</cell></row><row><cell cols="2">seq2seq + deep convolutional (our work) 10.53</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Wall Street Journal test eval92 Word Error Rate (WER) results across Connectionist Temporal Classification (CTC) and Sequence-tosequence (seq2seq) models. The models were decoded without a dictionary or language model.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="978" xml:id="foot_0">-1-5090-4117-6/17/$31.00 Â©2017 IEEE ICASSP 2017</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">We only use 32 output channels thus it can be improved if we increase the channel size.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attention-Based Models for Speech Recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-end Attention-based Large Vocabulary Speech Recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Task Loss Estimation for Sequence Prediction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">On Online Attention-based Speech Recognition and Joint Mandarin Character-Pinyin Training</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lane</surname></persName>
		</author>
		<editor>INTERSPEECH</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sequence Transduction with Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML: Representation Learning Workshop</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
				<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998-11-11">11 Nov. 1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Exploring Convolutional Neural Network Structures and Optimization Techniques for Speech Recognition</title>
		<author>
			<persName><forename type="first">O</forename><surname>Abdel-Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<editor>INTERSPEECH</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Convolutional Neural Networks for LVCSR</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Convolutional Neural Networks for Acoustic Modeling in Low Resource Languages</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improvements to Deep Convolutional Neural Networks for LVCSR</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Beran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Aravkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASRU</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Very deep multilingual convolutional neural networks for LVCSR</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Advances in Very Deep Convolutional Neural Networks for LVCSR</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<editor>INTERSPEECH</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Very deep convolutional neural networks for noise robust speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2263" to="2276" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Going Deeper with Convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical Recurrent Neural Networks for Long-Term Dependencies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hihi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fougner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Seetapun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<title level="m">Deep Speech 2: End-to-End Speech Recognition in English and Mandarin</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Woo</surname></persName>
		</author>
		<title level="m">Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Hybrid Speech Recognition with Bidirectional LSTM</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<editor>ASRU</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11">Nov. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Grid long short-term memory</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Highway Long Short-Term Memory RNNs for Distant Speech Recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Practical Variational Inference for Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>ManÃ©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>ViÃ©gas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="http://tensorflow.org/" />
		<title level="m">TensorFlow: large-scale machine learning on heterogeneous systems, Software available from tensorflow.org</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards End-to-End Speech Recognition with Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
