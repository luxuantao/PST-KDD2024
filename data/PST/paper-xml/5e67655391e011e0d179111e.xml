<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Offline Quintuplet Loss for Image-Text Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-03-14">14 Mar 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tianlang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Rochester</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiajun</forename><surname>Deng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Uiversity of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
							<email>jluo@cs.rochester.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Rochester</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive Offline Quintuplet Loss for Image-Text Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-03-14">14 Mar 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2003.03669v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image-text matching</term>
					<term>Triplet loss</term>
					<term>Hard negative mining</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing image-text matching approaches typically leverage triplet loss with online hard negatives to train the model. For each image or text anchor in a training mini-batch, the model is trained to distinguish between a positive and the most confusing negative of the anchor mined from the mini-batch (i.e. online hard negative). This strategy improves the model's capacity to discover fine-grained correspondences and non-correspondences between image and text inputs. However, the above training approach has the following drawbacks: (1) the negative selection strategy still provides limited chances for the model to learn from very hard-to-distinguish cases. (2) The trained model has weak generalization capability from the training set to the testing set. (3) The penalty lacks hierarchy and adaptiveness for hard negatives with different "hardness" degrees. In this paper, we propose solutions by sampling negatives offline from the whole training set. It provides "harder" offline negatives than online hard negatives for the model to distinguish. Based on the offline hard negatives, a quintuplet loss is proposed to improve the model's generalization capability to distinguish positives and negatives. In addition, a novel loss function that combines the knowledge of positives, offline hard negatives and online hard negatives is created. It leverages offline hard negatives as intermediary to adaptively penalize them based on their distance relations to the anchor. We evaluate the proposed training approach on three state-of-the-art image-text models on the MS-COCO and Flickr30K datasets. Significant performance improvements are observed for all the models, demonstrating the effectiveness and generality of the proposed approach. Our code is available at https://github.com/sunnychencool/AOQ.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image-text matching is the core task in cross-modality retrieval to measure the similarity score between an image and a text. By image-text matching, a system can retrieve the top corresponding images of a sentence query, or retrieve the top corresponding sentences of an image query.</p><p>To train an image-text matching model to predict accurate similarity score, triplet loss is widely used <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14]</ref>. Each given image or text of a training mini-batch is referred to as an anchor. For each image/text anchor, a text/image that corresponds to the anchor is called a positive while one that does not correspond to the anchor is called a negative. Clearly, the anchor and its positives/negatives belong to two modalities. A triplet loss is applied to encourage the model to predict higher similarity scores between the anchor and its positives (i.e. positive pairs) than those between the anchor and its negatives (i.e. negative pairs).</p><p>To utilize negative pairs to train the model, early approaches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10]</ref> adopt an all-in strategy. For each anchor, all its negatives in the mini-batch participate in the loss computing process. However, in most situations, the semantic meanings of an anchor and its negatives are totally different. With this strategy, the overall training difficulty is relatively low for the model to distinguish between positive and negative pairs. The model only needs to focus on each pair's global semantic meaning difference and may ignore the local matching details. Faghri et al. <ref type="bibr" target="#b5">[6]</ref> propose a triplet loss with online hard negatives (i.e. online triplet loss) as a more effective training approach. Specifically, for each anchor in a mini-batch, the model computes its similarity score to all the negatives in the same mini-batch online, and selects the negative with the highest score to the anchor as online hard negative of the anchor. The new triplet loss guides the model to only distinguish between the positives and online hard negatives of the anchor. Compared with the all-in strategy, the models trained by this approach commonly achieve better performance in distinguishing between positives and confusing negatives that have similar semantic meanings to the anchor. This training approach is employed by all the state-of-the-art models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>Even with its effectiveness, we argue that the online triplet loss still have three drawbacks in negative selection strategy, distinguishing strategy, and penalization strategy: <ref type="bibr" target="#b0">(1)</ref> for the negative selection strategy, the "hardness" degree of online hard negatives is still not sufficient. Given the MS-COCO dataset as example, the training set contains 500K corresponding image-text pairs. When we set the mini-batch size to 128 as in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27]</ref>, for each online hard negative of an anchor mined from the mini-batch, we can prove that its similarity score rank expectation to the anchor in the whole training set is about 4000 (i.e. 500K  128 ). The probability of its rank in the top-100 is only about 2.2%. In other words, a very hard negative with a top-100 similarity score rank for the anchor will rarely be sampled to train the model. This decreases the model's capacity to distinguish between the positives and those very confusing negatives. Increasing the minibatch size could be helpful. However, the mini-batch computational complexity grows sharply. <ref type="bibr" target="#b1">(2)</ref> For the distinguishing strategy, the triplet loss only focuses on obtaining the correct rank orders between the positives and negatives of the same anchor. However, it does not guide the model to rank among positive pairs and negative pairs that contain no common samples. Actually, this guidance is essential to improve the model's generalization capability from training to testing, especially when we apply the guidance on the very hard negative pairs. (3) For the penalization strategy, the triplet loss lacks a hierarchy. Ideally, the loss function should guide the model to maintain remarkable score gaps among the pairs of different classes. For example, the positive pairs should obtain far higher similarity scores than very hard negative pairs, and the very hard negative pairs should also obtain far higher similarity scores than ordinary hard negative pairs. When a pair's predicted score is close or beyond the boundary of its pair class, the loss function should give it a larger penalty to update the model. However, the current online triplet loss only defines positive and online hard negative pairs. More importantly, it gives equal penalty to all the pairs when the margin conditions are not satisfied. To overcome the above drawbacks, we propose a new training approach that can be generally applied on all existing models. Specifically, we utilize a tworound training to additionally sample "harder" negatives offline. In the first round, we train the model by the original online triplet loss. After that, for each image and text anchor in the training set, the model predicts its similarity score to all its negatives in the training set and ranks them. In the second round, given each anchor in a mini-batch, we sample its offline hard negatives directly from its top negative list with the highest similarity score in the whole training set. In this process, multiple kinds of offline hard negative pairs are constructed which share/do not share common elements with the positive pairs. The model is trained by a combination of online triplet loss and offline quintuplet loss to successfully overcome the first two drawbacks. Furthermore, we modify the loss function and feed information of offline hard negative pairs into the online triplet loss term. Combined with the offline quintuplet loss term, the complete training loss achieves hierarchical and adaptive penalization for the positive pairs, offline hard negative pairs and online hard negative pairs with different "hardness" degrees. The framework of the proposed training approach is shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>Our main contributions are summarized as follows: -We propose a novel and general training approach for image-text matching models. A new offline quintuplet loss is introduced that can effectively cooperate with the original online triplet loss. -We skillfully feeds the similarity score of offline hard negative pair into online loss term, it serves as a criterion to adaptively penalize different kinds of pairs. We analyze how it works mathematically.</p><p>-We evaluate our training approach on three state-of-the-art image-text matching models. Quantitative and qualitative experiments conducted on two publicly available datasets demonstrate its strong generality and effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Image-text matching has received much attention in recent years. Most of the previous works focus on the improvement of feature extraction and model design.</p><p>Early image-text matching approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b34">35]</ref> directly capture the visualtextual alignment at the level of image and text. Typically, they extract the global image feature by convolutional neural network (CNN), and extract the global text feature by language model such as Skip-gram model <ref type="bibr" target="#b21">[22]</ref> or recurrent neural network (RNN). The image-text similarity score is then computed as the inner product <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b5">6]</ref> or cosine similarity <ref type="bibr" target="#b34">[35]</ref> of the image and text features. The success of attention models for joint visual-textual learning tasks, such as visual question answering (VQA) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b11">12]</ref> and image captioning <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b2">3]</ref>, leads to the transition to capture image-text correspondence at the level of image regions and words <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b35">36]</ref>. Typically, these approaches extract the image region feature and word feature from the last pooling layer of CNN and temporal outputs of RNN. They focus on designing effective upper networks that can automatically find, align and aggregate corresponding regions and words to compute the final similarity score. Recently, Anderson et al. <ref type="bibr" target="#b0">[1]</ref> extract the image object features by the combination of Faster R-CNN <ref type="bibr" target="#b24">[25]</ref> and ResNet <ref type="bibr" target="#b7">[8]</ref> for the task of image captioning and VQA. Based on <ref type="bibr" target="#b0">[1]</ref>, recent approaches <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b10">11]</ref> further construct the connection between words and image objects. On one hand, they propose new mechanisms for object feature extraction, such as feeding saliency information <ref type="bibr" target="#b10">[11]</ref> or extracting joint feature among objects by constructing object graph <ref type="bibr" target="#b14">[15]</ref>. On the other hand, different cross-modality aggregation networks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9]</ref> are proposed to improve the aggregation process from object and word features to the image-text similarity score.</p><p>Even though the network design is widely studied, relatively fewer works focus on the training approach. Early image-text matching approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b31">32]</ref> commonly apply a standard triplet loss whose early form can be found in <ref type="bibr" target="#b27">[28]</ref> for word-image embedding. On the other hand, Zhang et al <ref type="bibr" target="#b34">[35]</ref> improve the triplet loss and propose a norm-softmax loss to achieve cross-modal projection. For both losses, all the negatives of an anchor in the same mini-batch are utilized for loss computing. Significant improvement is observed as Faghri et al. <ref type="bibr" target="#b5">[6]</ref> propose the triplet loss with online hard negatives. Online triplet mining is first introduced in <ref type="bibr" target="#b25">[26]</ref> for face recognition. For image-text matching, it mines the online hard negatives of the anchors from the mini-batch and makes the model only pay attention to these confusing negatives. Almost all the current models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27]</ref> apply this online triplet loss. To the best of our knowledge, our work is the first that introduces offline hard negatives for image-text matching. They are mined offline from the whole training set. Motivated by <ref type="bibr" target="#b3">[4]</ref> for person re-identification, we propose a quintuplet loss based on offline hard negatives to</p><p>The huge clock on the wall is near a wooden table <ref type="table">.</ref> A grandfather clock sitting in a store of antiques.</p><p>An old style clock on a table near a wall.</p><p>A small white clock sitting on top of a wooden table  effectively cooperate with an online triplet loss, leading to significant improvement. It should be noticed that Liu et al. <ref type="bibr" target="#b18">[19]</ref> explicitly feed adaptive penalty weight into triplet loss for image-text matching. However, they use it to solve the hubness problem, while we implicitly feed hierarchical information into the model to enlarge the similarity score differences among different pair classes.</p><formula xml:id="formula_0">I#1 T#1 Online Negative Mining</formula><formula xml:id="formula_1">I#1, T#1 I#1, T#1 I#1, T#1 I#1, T#1 I#1, T#1 I#1, T#1 I#2, T#1 I#1, T#2 I#3, T#1 I#1, T#3 I#3, T#3 I#4, T#4 Postive Pair Postive Pair I#3, T#1 I#2, T#1 I#1, T#3 I#1, T#2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>In this section, we formally present our training approach for image-text matching. In Section 3.1, we introduce the margin-based standard and online triplet losses that are used in previous works. In Section 3.2, we present offline quintuplet loss as an effective complement to online triplet loss to significantly improve the performance. In Section 3.3, we propose our final loss function with adaptive penalization and mathematically show how it works. The overall training process and the involved pairs are illustrated in Figure <ref type="figure" target="#fig_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Triplet Loss for Image-text Matching</head><p>Given an input image-text pair, image-text matching models aim to predict the pair's similarity score as a criterion for cross-modality retrieval. To achieve this, positive pairs (i.e. corresponding image-text pairs) and negative pairs (i.e. noncorresponding image-text pairs) are constructed. The model is trained to predict higher similarity score for the positive pairs than the negative ones.</p><p>Because the metrics of cross-modality retrieval are based on the ranking performance of multiple candidates on a single query, triplet loss is widely applied to train the model. It holds a common sample for each positive pair and negative pair as an anchor. The other sample in the positive pair is called the anchor's positive while the other sample in the negative pair is called the anchor's negative. In essence, triplet loss encourages the model to predict higher similarity scores from the anchor to its positives. This is consistent with the retrieval process of finding the corresponding candidates of a query with the high similarity scores.</p><p>Early image-text matching works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b31">32]</ref> typically apply a standard triplet loss without hard negative mining. Given a training mini-batch that contains a set of positive pairs, the standard triplet loss is defined as:</p><formula xml:id="formula_2">L std = (i,t)∈P ( t∈T /t [γ − S(i, t) + S(i, t)] + + i∈I/i [γ − S(i, t) + S(i, t)] + ) (1)</formula><p>Here γ is the margin of the triplet loss, [x] + ≡ max(x, 0). I, T and P are the image, text and positive pair sets of the mini-batch, respectively. i and t are the anchors of the two terms, respectively. (i, t) represents the positive pair, while (i, t) and (i, t) represent the negative pairs available in the mini-batch.</p><p>On the other hand, to overcome the drawback of standard triplet loss mentioned in Section 1, Faghri et al. <ref type="bibr" target="#b5">[6]</ref> present triplet loss with online hard negatives (i.e. online triplet loss). In particular, for a positive pair (i, t) in a mini-batch, the hard negatives of the anchor i and t are given by t on = argmax c∈T /t S(i, c) and i on = argmax b∈I/i S(b, t), respectively. The online triplet loss is defined as:</p><formula xml:id="formula_3">L online = (i,t)∈P ([γ − S(i, t) + S(i, t on )] + + [γ − S(i, t) + S(i on , t)] + )<label>(2)</label></formula><p>Compared with the standard triplet loss, online triplet loss forces the model to only learn to distinguish between the positive and the most confusing negative of an anchor in the mini-batch. This guides the model to not only consider the overall semantic meaning difference of a pair, but also discover correspondences and non-correspondences from the details hidden in local regions and words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Offline Quintuplet Loss</head><p>One problem of online triplet loss in Section 3.1 is that the "hardness" degree of most online hard negatives is still not sufficient, especially when the training involves a large-scale training set and a relatively small batch size. As mentioned in Section 1, the rank of an anchor's online hard negative in the whole training set is commonly not very high. Qualitatively, as shown in Figure <ref type="figure" target="#fig_3">3</ref>, the online hard negatives of an anchor typically contain a few related words, objects or scenes to the anchor. However, there exist obvious non-correspondences between the anchor and the negatives. Indeed, the model only needs to find these noncorrespondences and strengthen their influence, which is sufficient for the score difference between the positive pair and negative pair to exceed the margin γ in Equation <ref type="formula" target="#formula_3">2</ref>. However, during inference, when the model encounters "harder" negatives like the offline hard negative examples of Figure <ref type="figure" target="#fig_3">3</ref>, the model may not be able to distinguish them from the positives. The non-corresponding parts of these "harder" negatives to the anchor are subtle, and their influence on the predicted score can be offset by the perfectly corresponding parts.</p><p>To overcome the problem, we additionally mine "harder" negatives in an offline fashion. In particular, it involves a two-round training. In the first round, A black bicycle leaning against the kitchen cabinets.</p><p>Two people in a food truck, one looking at an order.</p><p>1. A man putting food in a cart in a kitchen. 2. A man putting something in a container in an industrial kitchen. 3. A man standing at a counter preparing food. 4. A man in a blue shirt and apron stands near a counter that has food stacked on it.</p><p>1. A man with glasses is in an office setting. 2. Two people looking at the food in a fridge. the model is trained by the online triplet loss. After that, it performs global similarity score prediction -for each image/text in the training set, the model predicts its similarity score to all its non-corresponding texts/images in the training set, ranks them by their scores and stores the list of the top-h. In the second round, for each anchor in a mini-batch, its offline hard negatives are uniformly sampled from the top-h negatives of the anchor in the whole training set. The model is trained from scratch again by the following loss function:</p><formula xml:id="formula_4">L = (i,t)∈P (([γ 1 − S(i, t) + S(i, t on )] + + [γ 2 − S(i, t) + S(i, t of f )] + ) +([γ 1 − S(i, t) + S(i on , t)] + + [γ 2 − S(i, t) + S(i of f , t)] + ))<label>(3)</label></formula><p>Here t of f and i of f are the offline hard negatives of i and t, γ 1 and γ 2 are the margins of the online and offline triplet losses. It should be noticed that for models with relatively low inference speed, the above mentioned global similarity score prediction step can be time-consuming. In Section 4, we demonstrate that a model can safely utilize the prediction of another efficient model to mine offline hard negatives, which still sharply benefits the training process.</p><p>Because the offline hard negatives are very confusing, to make them benefit the training, we should set γ 2 to a lower margin than γ 1 , e.g. 0. However, in this situation, if the positive and offline hard negative pairs share a same anchor, the model will merely learn how to find the subtle non-corresponding parts of the offline hard negative pair, but still does not learn how to deal with the situation when the negative pair's perfect matching parts offset the score influence of non-corresponding parts. We attribute it to the fact that the positive and offline hard negative get close similarity score for their corresponding parts to the same anchor. The model only needs to find the the non-corresponding parts of the negative pair to satisfy the margin condition of γ 2 . Also, as claimed in <ref type="bibr" target="#b3">[4]</ref>, this setting weakens the model's generalization capability from training to testing.</p><p>Considering this, we additionally derive two offline hard negative pairs and modify Equation 3 for the second-round training as follows:</p><formula xml:id="formula_5">L = (i,t)∈P (([γ 1 − S(i, t) + S(i, t on )] + + [γ 2 − S(i, t) + S(i, t of f )] + + [γ 2 − S(i, t) + S(i of f , t of f )] + ) +([γ 1 − S(i, t) + S(i on , t)] + + [γ 2 − S(i, t) + S(i of f , t)] + + [γ 2 − S(i, t) + S( i of f , t of f )] + ))<label>(4)</label></formula><p>Here i of f and t of f are the corresponding image and text of t of f and i of f , respectively. Because t of f and i of f are offline hard negatives of corresponding i and t, both (i of f , t of f ) and ( i of f , t of f ) can be also regarded as offline hard negative pairs (we re-sample i of f and t of f if they occationally correspond to each other). The samples of each pair are non-corresponding but share very similar semantic meanings to each other, and also to i and t. This two new terms guide the model to distinguish between positive and negative pairs without common elements. In Section 4, we prove the effectiveness of deriving the new terms based on i of f , t of f instead of i on , t on . The complete offline loss terms based on anchor i and t contain 4 and 5 elements. Following <ref type="bibr" target="#b3">[4]</ref>, we define it as an offline quintuplet loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adaptive and Hierarchical Penalization</head><p>In Section 3.2, we introduce offline hard negatives which cooperate with online hard negatives to train the model as Equation <ref type="formula" target="#formula_5">4</ref>. During the training process, it is natural that we should give different penalty weights to negative pairs with different "hardness" degrees. For example, if the similarity score between a positive pair and a hard negative pair is close, both pairs should obtain higher penalty weight which guides the model to distinguish between them better. However, when we derive each loss term with respect to its contained pairs' similarity scores, the gradients are always constant. This indicates that when the margin condition is not satisfied, the penalty weight is consistent regardless of the closeness degree between the positive and negative pairs. One simple solution is modifying each loss term to a form of square so that the penalty weight is related to the score difference between the positive and negative pairs. However, we find that the improvement is limited as there are no hierarchical knowledge provided by the loss function. Ideally, we expect that the positive pairs to obtain higher scores than offline hard negative pairs, and that the offline hard negative pairs obtain higher scores than online hard negative pairs. To this end, we feed the information of offline hard negatives into the online loss term. The final loss function for the second-round training is as follows:</p><formula xml:id="formula_6">L = (i,t)∈P (((β − S(i, toff ) − S(i, ton) α )[γ1 − S(i, t) + S(i, ton)]+ + [γ2 − S(i, t) + S(i, toff )]+ + [γ2 − S(i, t) + S(ioff , toff )]+) +((β − S(ioff , t) − S(ion, t) α )[γ1 − S(i, t) + S(ion, t)]+ + [γ2 − S(i, t) + S(ioff , t)]+ + [γ2 − S(i, t) + S( ioff , toff )]+))</formula><p>(5) Here α and β are hyper-parameters. In Section 4, we present that they can be set to consistent values for different models on different datasets.</p><p>To better understand how the proposed loss function works, we focus on the first part (line) of Equation <ref type="formula">5</ref>which is symmetrical to the second part, and compute its gradient with respect to S(i, t), S(i, t of f ) and S(i, t on ) as follows:</p><formula xml:id="formula_7">∂L ∂S(i, t) = ( S(i, t of f ) − S(i, t on ) α − β)I(γ 1 − S(i, t) + S(i, t on ) &gt; 0) − I(γ 2 − S(i, t) + S(i, t of f ) &gt; 0) − I(γ 2 − S(i, t) + S(i of f , t of f ) &gt; 0), ∂L ∂S(i, t of f ) = ( S(i, t) − S(i, t on ) α − γ 1 α ))I(γ 1 − S(i, t) + S(i, t on ) &gt; 0) + I(γ 2 − S(i, t) + S(i, t of f ) &gt; 0), ∂L ∂S(i, t on ) = ( 2S(i, t on ) − S(i, t) − S(i, t of f ) α + β + γ 1 α )I(γ 1 − S(i, t) + S(i, t on ) &gt; 0)<label>(6)</label></formula><p>Here I(A) is the indicator function: I(A) = 1 if A is true, and 0 otherwise. When the margin conditions are not satisfied, the gradient of S(i, t on ) becomes larger when S(i, t on ) is close to the average of S(i, t of f ) and S(i, t), which indicates a larger penalty to make S(i, t on ) lower. For the gradient of S(i, t), the second and third terms indicate a negative constant which pushes S(i, t) to be higher than S(i, t of f ). In addition, the first term indicates an additional adaptive penalty for S(i, t) to be far away from S(i, t on ). When S(i, t on ) is remarkably lower than S(i, t of f ), the penalty drops since S(i, t on ) is sufficiently lower. As for S(i, t of f ), its gradient is subtle as the second term indicates a positive constant that penalizes S(i, t of f ) to be lower than S(i, t). However, this penalty could be neutralized when S(i, t) and S(i, t on ) are close to each other. In this situation, it prevents the penalty from incorrectly making S(i, t of f ) lower than S(i, t on ).</p><p>Overall, the proposed loss function applies adaptive and hierarchical penalties to the positive, offline hard negative and online hard negative pairs based on the differences among their predicted scores. Essentially, the pairs that are close to the boundary of its pair class obtain larger penalty weights, thus the inter-class score gaps can be enlarged among these three kinds of pairs. In Section 4, we demonstrate its strong effectiveness to improve the model's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Extensive experiments are performed to evaluate the proposed training approach. The performance of retrieval is evaluated by the standard recall at K (R@K). It is defined as the fraction of queries for which the correct item belongs to the top-K retrieval items. We first present the datasets, experiment settings and implementation details. We then compare and analyze the performance of the proposed approach with others quantitatively and qualitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Experiment Settings</head><p>We evaluate our model on two well-known datasets, MS-COCO and Flickr30K. The original MS-COCO dataset <ref type="bibr" target="#b16">[17]</ref> contains 82,783 training and 40,504 validation images, each image is annotated with five descriptions. Following the splits of <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>, we divide the dataset into 113,283 training images, 5,000 validation images and 5,000 test images. Following <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>, we report the results by averaging over 5 folds of 1K test images or testing on the full 5K test images. Flickr30k <ref type="bibr" target="#b32">[33]</ref> consists of 31K images collected from the Flickr website. Each image also corresponds to five human-annotated sentences. Following the split of <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>, we randomly select 1,000 images for validation and 1,000 images for testing, and use other images to train the model.</p><p>To evaluate the effectiveness and generality of the proposed approach, we apply it to the following current state-of-the-art image-text matching models:</p><p>-SCAN <ref type="bibr" target="#b13">[14]</ref>. The first model that captures image-text correspondence at the level of objects and words. The word and object features are extracted by bidirectional GRU and the combination of Faster R-CNN <ref type="bibr" target="#b24">[25]</ref> and ResNet-101 <ref type="bibr" target="#b7">[8]</ref>, respectively. Stacked cross attention is fed into the network to discover the full latent alignments using both objects and words as context. -BFAN <ref type="bibr" target="#b17">[18]</ref>. A novel Bidirectional Focal Attention Network based on SCAN that achieves remarkable improvement. Compared with SCAN, it focuses additionally on eliminating irrelevant fragments from the shared semantics.</p><p>-VSRN <ref type="bibr" target="#b14">[15]</ref>. The current state-of-the-art image-text matching models without leveraging extra supervision (the model in <ref type="bibr" target="#b10">[11]</ref> is trained by extra saliencyannotated data). It generates object representation by region relationship reasoning and global semantic reasoning.</p><p>All the three models are originally trained by triplet loss with online hard negatives. We replace it with the proposed training approach for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>To perform a fair comparison, for SCAN, BFAN and VSRN, we completely preserve their network structures and model settings (e.g. training batch size, feature dimension and other model-related hyper-parameter settings) as described in their original work. We only replace the online triplet loss by the proposed one to train them. For all the situations, the margins for online and offline ranking losses γ 1 and γ 2 are set to 0.2 and 0, the hyper-parameters β and α in Equation <ref type="formula">5</ref>are set to 1.5 and 0.3. The top list size h is set to 300 and 60 to sample offline hard negative texts and images (the training texts are 5 times as many training images for both datasets). As mentioned in Section 3.2, for VSRN, it takes 3,400s/620s to perform global similarity score prediction on MS-COCO/Flickr30K. However, for SCAN and BFAN, they hold complex upper networks which make this step extremely time-consuming. Therefore, we skip the first-round training of SCAN and BFAN. The similarity scores predicted by VSRN are also used as a basis for the second-round training of SCAN and BFAN to sample offline hard negatives. We consider this setting valid, because after the second-round training, the final prediction is still made by SCAN or BFAN without the participating of VSRN, which can be regarded as a teacher model. For the first-round training on MS-COCO/Flickr30K, as <ref type="bibr" target="#b14">[15]</ref>, VSRN is trained by a start learning rate of 0.0002 for 15/10 epochs, and then trained by a lower learning rate of 0.00002 for another 15/10 epochs. For the second-round training on both datasets, SCAN, BFAN and VSRN are trained by a start learning rate of 0.0005, 0.0005 and 0.0002 for 10 epochs, and then trained by a lower learning rate of 0.00005, 0.00005 and 0.00002 for another 5, 5 and 10 epochs, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on MS-COCO and Flickr30K</head><p>Table <ref type="table">1</ref> shows the performance comparison of models trained by different approaches on MS-COCO. We can see that all the three models are significantly improved on all the settings when trained by our proposed training approach. As mentioned in Section 4.2, for all the models, the offline hard negatives in their second-round training are sampled from the prediction of the first-round trained VSRN. It indicates that the proposed training approach is insensitive to the Table <ref type="table">1</ref>. Quantitative evaluation results of image-to-text (sentence) retrieval and textto-image (image) retrieval on MS-COCO 1K/5K test set. The baseline models (first row) are trained by the triplet loss with online hard negatives. "+ OffTri", "+ OffQuin", "+ AdapOffQuin" represent training the model by Equation 3, 4, 5, respectively.</p><p>Sentence Retrieval Image Retrieval Model R@1 R@5 R@10 R@1 R@5 R@10 rsum 1K Test Images SCAN <ref type="bibr" target="#b13">[14]</ref> 72 model consistency of the two-round training. When the global similarity score prediction step is intractable for the current model, we can train it by sampling offline hard negatives based on the prediction of another more efficient model. Overall, we achieve most significant improvement on BFAN. In particular, on the more reliable 5K test set, it outperforms the baseline by 8.3% and 4.7% in top-1 sentence retrieval and top-1 image retrieval.</p><p>Table <ref type="table">2</ref> shows the performance comparison on Flickr30K. It should be noted that Flickr30K is much smaller than MS-COCO as it contains fewer very confusing negative image-text pairs to be served as high-quality offline hard negative pairs. However, significant improvements are still observed for all the models. In Section 4.4, we show that our proposed training approach has strong robustness for the quality of offline hard negatives.</p><p>Table <ref type="table">2</ref>. Quantitative evaluation results of sentence retrieval and image retrieval on the Flickr30K test set.</p><p>Sentence Retrieval Image Retrieval Model R@1 R@5 R@10 R@1 R@5 R@10 rsum 1K Test Images SCAN <ref type="bibr" target="#b13">[14]</ref> 67.  We look deeper into different training approaches by examining VSRN BFAN's training behaviours<ref type="foot" target="#foot_0">3</ref> on the widely-used MS-COCO 1K validation set <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14]</ref> (i.e. the first fold of the 5K validation set). As shown in Figure <ref type="figure" target="#fig_4">4</ref>, both models' performance obtains continuous improvement as we feed different proposed mechanisms into the training process. When the models are trained by Equation <ref type="formula">5</ref>, they converge significantly faster than the baselines as it takes less than 10 epochs for them to outperform the highest R@1 of their baselines.</p><p>For our cross-modality retrieval task, a corresponding positive image-text pair may perform well on one modality but poorly on the other (e.g. obtain high rank against the negative pairs that share the same image, but low rank against the negative pairs that share the same text). We next prove that our training approach does not exacerbate this unbalance. On the full MS-COCO 5K test set that contains 5,000 images, 25,000 texts and 25,000 positive imagetext pairs, for each pair, the trained models predict its rank against the 4,999 negative pairs that share the same text and 24,995 negative pairs that share the same image as r i and r t . For fair weighting between r i and r i with different negative pair numbers, the cross-retrieval rank of each positive pair is defined as: max(5r i − 4, r t ). It records the lower rank of the positive pair against the two kinds of negative pairs. Figure <ref type="figure">5</ref> shows the 25,000 positive pairs' cross-retrieval rank frequency distribution of different rank intervals. It can be seen that for both VSRN and BFAN, the number of positive pairs with cross-retrieval rank of 1 (i.e. highest rank against all the considered negative pairs) increases significantly when the proposed approach is applied. Meanwhile, the number drops for the pairs with cross-retrieval rank larger than 200. This indicates a comprehensive improvement for the overall ranking of positive pairs in the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study and Visualization</head><p>First, we validate whether the offline hard negatives can completely replace online hard negatives to train the model. Specifically, we remove the online loss term in Equation <ref type="formula" target="#formula_5">4</ref>to train VSRN and BFAN. As shown in Table <ref type="table" target="#tab_2">3</ref>, the training process fails as it is too difficult for the model to directly learn to distinguish between the positive pairs and these extremely confusing negative pairs. Also, we demonstrate the usefulness of re-training the model from scratch in the second round. As shown in Table <ref type="table" target="#tab_2">3</ref>, when we apply Equation 5 to fine-tune the model that has already been trained by the online triplet loss and get trapped in a local optimum, it cannot obtain additional improvement. In Equation <ref type="formula" target="#formula_5">4</ref>, we create two new terms based on offline negatives. Indeed, we can instead apply them based on online negatives. However, the performance of "OnlineQuin" models are remarkably worse than the models train by Equation <ref type="formula" target="#formula_5">4</ref>, this supports our claim of the second problem in Section 1. On the other hand, in Equation <ref type="formula">5</ref>, we feed the offline hard negative information into online term for hierarchical penalization. To validate its effectiveness, we replace S(i, t of f ) and S(i of f , t) by S(i, t) for the new added terms in Equation 5 to break this hierarchical relation. α and β are re-adjusted to achieve best performance on the validation set. The performance drops to the same level of using Equation <ref type="formula" target="#formula_5">4</ref>to train the models, indicating the effectiveness. In the end, for VSRN, we present the model's per-  <ref type="formula" target="#formula_5">4</ref>(i.e replace S(i of f , t of f ))), S( i of f , t of f )) with S(ion, ton))), S( ion, ton))) to train the model. "w/o OfflineAdap" represents that we replace S(i, t of f ) and S(i of f , t) by S(i, t) for the new added terms in Equation 5 to train the model. Performance of selecting different top list size h for offline hard negative text sampling is also studied. The values in parentheses indicate the performance difference between the models trained by the variant and by the proposed approach with the final settings.</p><p>Sentence Retrieval Image Retrieval Model R@1 R@5 R@10 R@1 R@5 R@10 1K Test Images BFAN (OnlyOffline)</p><p>1.1(-76.2) 2.5 (-93.5) 4.9 (-93.6) 0.5 (-60. formance when selecting different top list size h for offline hard negative text sampling (we always keep it 5 times larger than the top list size for offline hard negative image sampling). We can find that even when h is set to 1000 which indicates significant drops of "hardness" degree of offline hard negatives, the model still achieve great performance. This is consistent with the excellent performance on Flickr30K and proves the robustness of our training approach on smaller dataset when very confusing hard negative pairs are limited. We present other ablation studies in the supplementary materials. Figure <ref type="figure">6</ref> shows the qualitative comparison between the models trained by different approaches on MS-COCO. For sentence retrieval, given an image query, we show the top-5 retrieved sentences predicted by the models. For image retrieval, given a sentence query, we show the top-3 retrieved images ranking from upper to bottom. The correct retrieval items for each query are ticked off. Overall, our training approach guides the model to better find and attend to the detailed non-correspondences of negative image-text pairs such as "snow covered field", "rhiho", "blowing out a candle" and "poster".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In contrast to existing works that make improvements on model architecture, we present a novel training approach for image-text matching. It starts by mining "harder" negatives offline from the whole training set. Based on the mined offline hard negatives, an effective quintuplet loss is proposed to complement the online triplet loss to better distinguish positive and negative pairs. Further more, we take the distance relations among positive, offline hard negative and online hard negative pairs into consideration and effectively achieve adaptive penalization for different pairs. Extensive experiments demonstrate the effectiveness and generality of the proposed approach. In the future, we will focus on building extra supervision at the object-word level to cooperate with existing training approaches and further benefit the training process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Overview of the proposed training approach. For each anchor, we sample its positives, offline hard negatives and online hard negatives. The training approach gives adaptive penalty to enlarge the similarity score differences among positive pairs, offline hard negative pairs and online hard negative pairs (i.e. the blue, green and brown arrows). On the other hand, extra penalty is added to enlarge the similarity score difference between positive pairs and offline hard negative pairs with different anchors that share similar semantic meanings (i.e. the cyan arrow).</figDesc><graphic url="image-3.png" coords="3,170.68,112.08,93.28,93.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Pos. Pair Neg. Pair Adap. Pos. Pair Adap. Neg. Pair</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Training process illustration. Given a positive image-text pair (I#1, T #1), 6 margin-based ranking losses are applied to enlarge its similarity score differences from the online hard negative pairs (I#2, T #1), (I#1, T #2), the offline hard negative pairs (I#3, T #1), (I#1, T #3) (with the common anchor), and the derived offline hard negative pairs (I#3, T #3), (I#4, T #4) (without the common anchor). Adaptive penalization is imposed via the online losses to adaptively penalize positive and negative pairs with different strengths and directions. The involved samples of each loss are marked by the corresponding squares.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Two example anchors, their corresponding positives, their sampled online hard negatives and offline hard negatives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Plotting training epoch against R@1 on the MS-COCO validation set for different training approaches applied on VSRN and BFAN. For the proposed approaches, the training curves correspond to the second-round training. "t2i" and "i2t" represents image retrieval and sentence retrieval, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 . 1 .Fig. 6 .</head><label>516</label><figDesc>Fig. 5. Comparison of positive pairs' cross-retrieval rank frequency on the MS-COCO test set for different training approaches applied on VSRN and BFAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>7) 1.4 (-87.8) 2.6(-92.4) VSRN (OnlyOffline) 0.7(-76.8) 2.1(-93.4) 3.8(-94.8) 0.4(-63.1) 1.2(-89.3) 2.3(-93.5) BFAN (Fine-tune) 74.3 (-3.0) 94.7 (-1.3) 98.2 (-0.3) 58.7 (-2.5) 88.1 (-1.1) 94.2(-0.8) VSRN (Fine-tune) 74.5 (-3.0) 94.3 (-1.2) 98.1 (-0.5) 62.0 (-1.5) 89.3(-1.2) 94.8(-1.0) BFAN (OnlineQuin) 75.3 (-2.0) 95.8 (-0.2) 98.5 (+0.0) 59.8 (-1.4) 88.6 (-0.6) 94.6(-0.4) VSRN (OnlineQuin) 76.4 (-1.1) 94.9 (-0.6) 98.2 (-0.4) 62.8 (-0.7) 89.9(-0.6) 95.2(-0.6) BFAN (w/o OfflineAdap) 76.6 (-0.7) 95.8(-0.2) 98.4 (-0.1) 60.8 (-0.4) 89.1 (-0.1) 94.8(-0.2) VSRN (w/o OfflineAdap) 77.1 (-0.4) 95.4(-0.1) 98.4 (-0.2) 63.4 (-0.1) 90.2 (-0.3) 95.5(-0.3) VSRN (h = 200) 77.1 (-0.4) 95.3(-0.2) 98.4 (-0.2) 63.3 (-0.2) 90.4 (-0.1) 95.6(-0.2) VSRN (h = 500) 77.4 (-0.1) 95.6(+0.1) 98.6 (+0.0) 63.5 (+0.0) 90.4 (-0.1) 95.7(-0.1) VSRN (h = 1000) 77.3 (-0.2) 95.4(-0.1) 98.6 (+0.0) 63.3 (-0.2) 90.3 (-0.2) 95.6(-0.2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Performance of different training approach variants on MS-COCO 1K test set. "OnlyOffline" represents the model that is only trained by the offline term. "Finetune" represents the model that is fine-tuned in the second-round instead of re-trained from scratch. "OnlineQuin" indicates that we apply online quintuplet loss instead of offline in Equation</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0">The final BFAN model is an ensemble of two independently trained models BFANequal and BFAN-prob<ref type="bibr" target="#b17">[18]</ref>, here we show the behaviours of BFAN-prob.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Expressing objects just like words: Recurrent visual embedding for image-text matching</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08510</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">factual&quot;or&quot;emotional&quot;: Stylized image captioning with adaptive learning and attention</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="519" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Beyond triplet loss: a deep quadruplet network for person re-identification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Linking image and text with 2-way nets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Eisenschtat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4601" to="4611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Vse++: Improved visual-semantic embeddings</title>
		<author>
			<persName><forename type="first">F</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05612</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Acmm: Aligned cross-modal memory for few-shot image and sentence matching</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5774" to="5783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Instance-aware image and sentence matching with selective multimodal lstm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2310" to="2318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Saliency-guided attention network for imagesentence matching</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09471</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multimodal residual learning for visual qa</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="361" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2539</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stacked cross attention for imagetext matching</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="201" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual semantic reasoning for image-text matching</title>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4654" to="4662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Identity-aware textual-visual matching with latent co-attention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1890" to="1899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">coco: Common objects in context</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Focus your attention: A bidirectional focal attention network for image-text matching</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
				<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Hal: Improved text-image matching by mitigating visual semantic hubs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10097</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Knowing when to look: Adaptive attention via a visual sentinel for image captioning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dual attention networks for multimodal reasoning and matching</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="299" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Areas of attention for image captioning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1242" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Camp: Crossmodal adaptive message passing for text-image retrieval</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5764" to="5773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large scale image annotation: learning to rank with joint word-image embeddings</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="35" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4651" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">End-to-end convolutional semantic embeddings</title>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5735" to="5744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-level attention networks for visual question answering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4709" to="4717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep cross-modal projection learning for image-text matching</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="686" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">D</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05535</idno>
		<title level="m">Dual-path convolutional image-text embedding with instance loss</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
