<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pareto Front Feature Selection based on Artificial Bee Colony Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-09-11">September 11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Emrah</forename><surname>Hancer</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Bing</forename><surname>Xue</surname></persName>
							<email>bing.xue@ecs.vuw.ac.nz</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Erciyes University</orgName>
								<address>
									<postCode>38039</postCode>
									<settlement>Kayseri</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mengjie</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Erciyes University</orgName>
								<address>
									<postCode>38039</postCode>
									<settlement>Kayseri</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dervis</forename><surname>Karaboga</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Technology and Information Systems</orgName>
								<orgName type="institution">Mehmet Akif Ersoy University</orgName>
								<address>
									<postCode>15030</postCode>
									<settlement>Burdur</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Erciyes University</orgName>
								<address>
									<postCode>38039</postCode>
									<settlement>Kayseri</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bahriye</forename><surname>Akay</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Technology and Information Systems</orgName>
								<orgName type="institution">Mehmet Akif Ersoy University</orgName>
								<address>
									<postCode>15030</postCode>
									<settlement>Burdur</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Technology and Information Systems</orgName>
								<orgName type="institution">Mehmet Akif Ersoy University</orgName>
								<address>
									<postCode>15030</postCode>
									<settlement>Burdur</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Engineering and Computer Science</orgName>
								<orgName type="institution">Victoria University of Wellington</orgName>
								<address>
									<postBox>PO Box 600</postBox>
									<postCode>6140</postCode>
									<settlement>Wellington</settlement>
									<country key="NZ">New Zealand</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pareto Front Feature Selection based on Artificial Bee Colony Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-09-11">September 11, 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">333E4F401DF017E93A0B75A95B5B7B20</idno>
					<idno type="DOI">10.1016/j.ins.2017.09.028</idno>
					<note type="submission">Received date: 11 October 2016 Revised date: 6 September 2017 Accepted date: 9 September 2017 Preprint submitted to Information Sciences</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Feature selection</term>
					<term>classification</term>
					<term>multi-objective optimization</term>
					<term>artificial bee colony</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Feature selection has two major conflicting aims, i.e. to maximize the classification performance and to minimize the number of selected features to overcome the curse of dimensionality. To balance their trade-off, feature selection can be handled as a multi-objective problem. In this paper, a feature selection approach is proposed based on a new multi-objective artificial bee colony algorithm integrated with non-dominated sorting procedure and genetic operators. Two different implementations of the proposed approach are developed: ABC with binary representation and ABC with continuous representation. Their performance are examined on 12 benchmark datasets and the results are compared with those of linear forward selection, greedy stepwise backward selection, two single objective ABC algorithms and three well-known multi-objective evolutionary computation algorithms. The results show that the proposed approach with the binary representation outperformed the other methods in terms of both the dimensionality reduction and the classification accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>valuable knowledge embedded in data and then transforming the knowledge into an understandable format for users through the steps, such as data pre-processing, management, post-processing and visualization <ref type="bibr" target="#b13">[14]</ref>. Data mining and machine learning techniques can be mainly divided into unsupervised (e.g. clustering), supervised (e.g. classification) and reinforcement learning <ref type="bibr" target="#b13">[14]</ref>. This paper focuses mainly on classification, which aims to learn a model based on a training set of instances and predict the class labels of unseen instances in the test set. Classification has been used in various real-world applications such as medical healthcare, image analysis, marketing and statistical problems <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b25">26]</ref>. However, the datasets, especially large dimensional ones, may comprise redundant, irrelevant and relevant features. This brings the problems of high complexity and poor learning performance in real-world applications <ref type="bibr" target="#b42">[43]</ref>.</p><p>One of the most common ways to overcome these problems is to apply feature selection <ref type="bibr" target="#b36">[37]</ref>. Feature selection aims to select the most relevant/useful features which contribute to the constructed model more efficiently and effectively. Not only for the classification performance, it is also beneficial for simplifying the learned models and shortening the training time. However, finding relevant/useful features is not an easy task due to the huge search space and the complex interactions among features. Feature interaction may occur in two ways, three ways or more than three ways. An individually irrelevant feature may be beneficial for the classification/learning performance while being interacted with other features. An individually relevant feature may become redundant when it is interconnected with other features. Furthermore, there exist 2 n possible feature subsets for a n-dimensional dataset. It is impractical to intimately search all possible solutions for a large n. Accordingly, feature selection is an NP-hard combinatorial problem <ref type="bibr" target="#b36">[37]</ref>. Even though a number of search techniques such as sequential forward and backward feature selection (SFS, SBS) <ref type="bibr" target="#b25">[26]</ref> have been proposed, they may have premature convergence problems or intensive computational complexity. To alleviate these problems, evolutionary computation (EC) techniques which are population based solvers in the subclass of global optimization and artificial intelligence have been applied due to their global search potential. The mostly commonly applied techniques for feature selection are genetic programming (GP) <ref type="bibr" target="#b35">[36]</ref>, genetic algorithms (GAs) <ref type="bibr" target="#b31">[32]</ref> and particle swarm optimization (PSO) <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35]</ref>. EC techniques are particularly good at multi-objective optimization because their population based search mechanism can produce multiple trade-off solutions in a single run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>It can be inferred from the two main conflicting objectives of feature selection, i.e. the maximization of the classification accuracy and the minimization of the feature subset size, that feature selection can be treated as a multi-objective problem. Unfortunately, there exist just a few studies concerning multi-objective feature selection in the literature <ref type="bibr" target="#b42">[43]</ref>, i.e., most of the existing approaches are based on a single objective of maximizing the classification accuracy. One of the recent metaheuristics, artificial bee colony (ABC) <ref type="bibr" target="#b18">[19]</ref> is an EC technique with many successful applications to solve different problems, which is a motivation to design ABC for multi-objective feature selection. Furthermore, ABC is easy implement, robust against initialization, and has the ability to explore local solutions with the low risk of local convergence. Our recent study <ref type="bibr" target="#b15">[16]</ref> has shown that ABC can be used for multi-objective feature selection, but the method in <ref type="bibr" target="#b15">[16]</ref> is for filter feature selection and the number of features in the datasets is small. The potential of ABC for multi-objective wrapper feature selection, which requires often a different approach from filters <ref type="bibr" target="#b20">[21]</ref>, and with a large number of features, has not been investigated yet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Goals</head><p>The main goal of this paper is to improve an ABC-based feature selection approach to searching for a set of Pareto optimal solutions yielding a smaller feature subset size and a lower classification error percentage than the case that all features are used. To fulfill this goal, a new multi-objective ABC approach based on non-dominated sorting and genetically inspired search is proposed, and two different implementations of the proposed approach are developed: Bin-MOABC (binary version) and Num-MOABC (continuous version). Bin-MOABC and Num-MOABC are compared with two traditional approaches, two single objective ABC variants and three well-known multiobjective feature selection approaches on 12 benchmark datasets including a variety of features, classes and instances.</p><p>Specifically, the following objectives are investigated:</p><p>1. the performance of single objective ABC approaches on reducing the feature subset size and increasing the classification performance, 2. the performance of the proposed multi-objective ABC implementations on obtaining Pareto optimal solutions and comparisons with two traditional and two single objective ABC approaches, 3. the performance analysis of the proposed multi-objective ABC implementations versus existing multi-objective approaches, and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T 4. the effect of considering feature selection in binary domain (Bin-MOABC) and continuous domain (Num-MOABC) on the classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Organization</head><p>The organization of the rest of the paper is as follows. A general knowledge concerning the standard ABC algorithm and the recent studies on feature selection is provided in Section 2. The proposed feature selection approaches are explained in Section 3. The experimental design is described in Section 4 and the experimental results are presented with discussions in Section 5. Finally, the conclusions and the future trends are introduced in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>In this section, ABC is described, the definition of multi-objective optimization is given, and then the recent research of the feature selection is briefly reviewed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Artificial Bee Colony</head><p>ABC is a swarm intelligence algorithm that simulates the foraging behavior of a honey bee colony <ref type="bibr" target="#b18">[19]</ref>. In the hive, three types of bees are assigned to the foraging task: employed bees, onlooker bees and scout bees. Employed bees are responsible from loading the nectar of discovered sources to the hive and dancing in the hive to share their information about the profitable sources with onlooker bees waiting in the hive. The onlooker bees watch the dances of the employed bees and choose a source to exploit. Scout bees search for undiscovered sources based on internal motivation or an external clue. In other words, employed and onlooker bees are responsible for exploiting food sources and scout bees are responsible for exploring new food sources. From the optimization perspective, each food source corresponds to a solution (x i = {x i1 , x i2 , ..., x iD }) in a D dimensional optimization problem and the nectar amount of a source represents the fitness value of the solution.</p><p>In the exploration-exploitation process of food sources, each employed bee searches in the neighborhood of the food source in her memory, while each onlooker bee searches in the neighborhood of the food source according to the information shared by employed bees through waggle dance. The basic steps of ABC are as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T 1. Food sources are initialized by Eq. ( <ref type="formula" target="#formula_0">1</ref>):</p><formula xml:id="formula_0">x ij = x min j + rand(0, 1)(x max j -x min j ) (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where i is the index of a food source in the range of 1 and SN and SN is the population size; j is the position of a food source in the range of 1 and D and D is the dimensionality of the search space; x min j and x max j are lower and upper bounds of position j. 2. Each employed bee i evolves its concerning food source by Eq. ( <ref type="formula">2</ref>):</p><formula xml:id="formula_2">υ ij = x ij + φ ij (x ij -x kj ) (2)</formula><p>where x i is the current food source; x k is the selected food source for x i ; j is the randomly selected position to be perturbated; υ i is the evolved food source by evolving jth parameter of x i ; and φ ij is a uniformly generated value in the range of -1 and 1. 3. Apply greedy selection between υ i and x i . If f (υ i ) &gt; f (x i ), the employed bee leaves x i and memorizes υ i as the current source. 4. Each food source is assigned a probability by Eq. <ref type="bibr" target="#b2">(3)</ref>.</p><formula xml:id="formula_3">p i = f itness i SN i=1 f itness i<label>(3)</label></formula><p>where f itness i is the fitness value of source x i and SN is the population size. 5. Each onlooker bee chooses a food source in a probabilistic manner, and then carries out searching as in the employed bee phase. 6. If there exists any exhausted food source which is determined by a 'limit' value, the scout bee generates a new food source using Eq. ( <ref type="formula" target="#formula_0">1</ref>) instead of abandoned one. 7. Repeat steps 2 to 6 until the maximum number of cycles is met.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multi-Objective Optimization</head><p>Many problems involve two or more conflicting objectives, called multiobjective optimization problems. This type of problems are typically with many solutions known as Pareto-optimal solutions.</p><p>Let f (x) = (f 1 (x), f 2 (x), ..., f no (x)) ∈ O ⊆ R n 0 be an objective vector comprising of multiple (n 0 ) conflicting functions and let F ⊆ S (where S is</p><formula xml:id="formula_4">A C C E P T E D M A N U S C R I P T</formula><p>the search space) represents the feasible space constrained by n g inequalities and n h equality constraints;</p><formula xml:id="formula_5">F = {x : g m (x) ≤ 0, h l (x) = 0, m = 1, ..., n g ; l = 1, ..., n h }<label>(4)</label></formula><p>where g m (x) and h l (x) are constraints. Using this notation, a multi-objective (minimization) problem can be formulated as follows:</p><formula xml:id="formula_6">minimize f (x) subject to x ∈ F<label>(5)</label></formula><p>When there are multiple objectives, for two solutions y and z, y dominates z iff y is is not worse than z in all objectives and better than z in at least one objective:</p><formula xml:id="formula_7">∀k : f k (y) ≤ f k (z) ∧ ∃k : f k (y) &lt; f k (z)<label>(6)</label></formula><p>A solution x * ∈ F is defined as a Pareto optimal (non-dominated) solution if there does not exist a solution x = x * ∈ F that dominates x * . The set of all non-dominated solutions form a Pareto-optimal front surface, known as Pareto front.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Existing Feature Selection Approaches</head><p>Feature selection approaches can be categorized into wrapper, filter and embedded approaches <ref type="bibr" target="#b25">[26]</ref>. While wrapper approaches use a classification algorithm to select a feature subset according to the classification performance, filter approaches generally use statistical or probabilistic properties of datasets and do not depend on any classifier or learning system. Since filter approaches do not employ any classifier or learning system, they are computationally less intensive and more general than wrappers. However, wrappers are able to get more promising results than filters. On the other hand, embedded approaches try to find an optimal feature subset in the learning process, i.e., they are dependent on the nature of classification model. Although embedded approaches are computationally less intensive than wrappers, they are conceptually more complex, and it is not easy to make a modification in the classification model to get higher performance <ref type="bibr" target="#b25">[26]</ref>. Therefore, this paper focuses on wrapper approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">Non-EC Approaches</head><p>The most well-known traditional wrapper approaches are sequential forward selection (SFS) <ref type="bibr" target="#b39">[40]</ref> and sequential backward selection (SBS) <ref type="bibr" target="#b27">[28]</ref>. SFS starts with an empty feature subset and sequentially selects features for this subset until no improvement is received on the classification performance. In contrast to SFS, SBS starts with a feature subset including all available features in the dataset and then sequentially eliminates features from this set until no improvement is received on the classification performance via further elimination. Although both SFS and SBS are simple to implement, they may converge to local minima and are computationally expensive in high-dimensional datasets. Based on SFS and SBS, the sequential forward floating selection (SFFS) and sequential backward floating selection (SFBS) <ref type="bibr" target="#b30">[31]</ref> were introduced to sort out the common limitation of SFS and SBS, in which a feature selected or removed in earlier steps cannot be updated later. Unfortunately, these attempts to overcome local-minima were not sufficient.</p><p>In traditional filter approaches, FOCUS <ref type="bibr" target="#b11">[12]</ref> exhaustively examines all possible feature subsets and then selects the smallest subset through correlation. However, exhaustive search is computationally intensive when concerned with a great number of features. Relief <ref type="bibr" target="#b19">[20]</ref> ranks features according to their weights obtained by randomly sampling instances from the data. Each weight reflects the relevance of its associated feature with the class labels. However, it does not address the redundancy among features. In contrast to FOCUS and Relief, information theoretic approaches such as MIFS <ref type="bibr" target="#b4">[5]</ref>, mRmR <ref type="bibr" target="#b29">[30]</ref> and MIFS-U <ref type="bibr" target="#b21">[22]</ref> considers both the relevance of each feature with the class labels and the redundancy within the feature subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">Single Objective EC based Approaches</head><p>To address the drawbacks of traditional approaches, researchers have also applied EC techniques, including GAs <ref type="bibr" target="#b28">[29]</ref>, GP <ref type="bibr" target="#b35">[36]</ref>, PSO <ref type="bibr" target="#b26">[27]</ref> and ABC <ref type="bibr" target="#b33">[34]</ref> to feature selection problems.</p><p>Raymer et al. <ref type="bibr" target="#b31">[32]</ref> introduced a GA based approach performing feature selection and feature extraction processes simultaneously, which achieved better results than the SFFS <ref type="bibr" target="#b30">[31]</ref> and linear discriminant analysis (LDA) approaches. Oh et al. <ref type="bibr" target="#b28">[29]</ref> hybridized GA (HGA) through embedding local search operations. The results showed that HGA performed better than standard GA.</p><p>Liu et al. <ref type="bibr" target="#b26">[27]</ref> introduced a multi-swarm PSO based approach using the classification accuracy and the F-score in a weighted manner. Different from</p><formula xml:id="formula_8">A C C E P T E D M A N U S C R I P T</formula><p>the existing studies, it considers the population as sub-populations. However, it is computationally inefficient. Huang and Dun <ref type="bibr" target="#b16">[17]</ref> proposed an efficient distributed PSO-SVM approach, which includes two components: 1) binary PSO for feature selection and 2) standard PSO for parameter optimization of SVM. Chuang et al. <ref type="bibr" target="#b6">[7]</ref> proposed an improved binary PSO algorithm based on catfish effect. In the proposed algorithm, when the global best particle could not be improved for a predefined number of iterations, 10% of particles with low quality are exchanged with new generated ones. According to the results, catfish based PSO performed better than Oh's HGA <ref type="bibr" target="#b28">[29]</ref>. Unler et al. <ref type="bibr" target="#b36">[37]</ref> proposed a hybrid filter-wrapper PSO approach to bring the advantages of filters and wrappers together. The effectiveness of the wrapperfilter approach was demonstrated by comparing it with another hybrid filter and wrapper approach based on GA.</p><p>ABC has been successfully applied to a wide range of fields <ref type="bibr" target="#b18">[19]</ref>, such as color quantization, automatic clustering, image analysis, and parameter optimization. Recently, researchers have also tried to address the feature selection problem using ABC in a single objective manner. Uzer et al. <ref type="bibr" target="#b37">[38]</ref> introduced a combined ABC-SVM feature selection approach for medical datasets. Subanya and Rajalaxmi <ref type="bibr" target="#b33">[34]</ref> proposed a hybridization of ABC and Naive Bayes, and it was tested on the Cleveland Heart disease dataset. However, the proposed approach was not compared with existing studies. Schiezaro and Pedrini <ref type="bibr" target="#b32">[33]</ref> proposed a feature selection approach using ABC based on single modification rate (MR). The results indicated that the proposed ABC algorithm outperformed the standard ABC, PSO and GA algorithms. Hancer et al. <ref type="bibr" target="#b14">[15]</ref> improved an advanced similarity based discrete ABC wrapper approach. The superiority of the discrete ABC wrapper approach was demonstrated by making comparisons with six well-known binary ABC and PSO variants on 10 benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3.">Multi-Objective EC Based Approaches</head><p>Hamdani et al. <ref type="bibr" target="#b12">[13]</ref> proposed a non-dominated sorting GA II (NSGAII) based approach. Waqas et al. <ref type="bibr" target="#b38">[39]</ref> also proposed a multi-objective GA based wrapper approach, in which a decision tree was chosen as the classifier. Xue et al. <ref type="bibr" target="#b44">[45]</ref> introduced a multi-objective PSO based wrapper approach (CMDPSO) inspired by crowding distance, non-dominated sorting and mutation for feature selection. In this work, the classification error rate and the number of features were chosen as objective functions. The results showed that CMDPSO outperformed NSGAII and strength Pareto evolutionary al-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T gorithm 2 (SPEA2). Xue et al. <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref> also used multi-objective PSO for filter feature selection with objective functions formed by mutual information and rough set theory.</p><p>Despite a number of existing approaches, most of them are single objective approaches considering the classification accuracy as a single objective. It is not possible to find a sufficient number of studies in the literature approaching feature selection as a multi-objective problem, i.e., this issue has just recently come into consideration. Furthermore, our recent study <ref type="bibr" target="#b15">[16]</ref> has shown that ABC can be used for multi-objective feature selection, but the method is for filter feature selection and the number of features in the datasets is small. The potential of ABC for multi-objective wrapper feature selection, which often requires a different approach from filters <ref type="bibr" target="#b20">[21]</ref>, and with a large number of features, has not been investigated yet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Multi-Objective ABC Approach</head><p>As mentioned in previous section, feature selection can be considered as a multi-objective problem through two main conflicting objectives: 1) minimizing the feature subset size and 2) maximizing the classification accuracy. Despite the success of ABC in different fields, there is no multi-objective ABC based wrapper approach in the literature. To cover this issue, an ABC based multi-objective feature selection approach with its two implementations are proposed in this section.</p><p>The standard ABC algorithm was proposed for single objective problems, and cannot be used for multi-objective feature selection. So modifications/adaptations are required on probability calculation scheme, solution update scheme and solution generation scheme to deal with multiobjective problems. Inspired by the concept and ideas of NSGAII <ref type="bibr" target="#b8">[9]</ref> and non-dominated sorting synchronous ABC (NSSABC) <ref type="bibr" target="#b1">[2]</ref>, we develop and implement both the binary and continuous versions of the multi-objective ABC approach, named Bin-MOABC and Num-MOABC respectively. For the clarity of the presentation purpose, we first present the structure of Bin-MOABC and Num-MOABC in Algorithm 1 to give an overall idea of the proposed methods, then describe more details of the key components.</p><p>A. How to Calculate Probabilities for Onlookers: For a single objective problem, a probability is simply assigned to a food source according to Eq. (3). However, Eq. ( <ref type="formula" target="#formula_3">3</ref>) is not suitable for multi-objective problems since</p><formula xml:id="formula_9">A C C E P T E D M A N U S C R I P T begin Generate initial population X = X 1 , X 2 , .</formula><p>.., X n by Eq. (1); Evaluate initial population X (i.e. error rate and number of features); Apply non-dominated sorting to solutions; for cycle ← 1 to M CN do foreach employed bee i do Randomly select a solution X k for X i ; Generate solutions by applying BSG (or NSG) between X i and X k ; Evaluate the generated solutions and add them to set S; end Rank the union set X ∪ S via non-dominated sorting; Update X by selecting the best SN solutions through ranking and crowding distance scores; S = ∅; foreach onlooker bee i do Select a solution X i using thermodynamic principles by Eq. ( <ref type="formula" target="#formula_11">8</ref>); Randomly select a solution X k for X i ; Generate solutions by applying BSG (or NSG) between X i and X k ; Evaluate generated solutions and add them to set S; end Rank the union set X ∪ S via non-dominated sorting; Update X by selecting the best SN solutions through ranking and crowding distance scores; if any abandoned solution then</p><p>Generate a new solution instead of abandoned one by Eq. ( <ref type="formula" target="#formula_0">1</ref>); end end Compute the classification accuracy of population X on the test set; Rank the population using non-dominated sorting and return the population; end Algorithm 1: Pseudo code of Bin-MOABC and Num-MOABC.</p><p>they have more than one objectives. Therefore, the following probability assignment scheme is employed:</p><formula xml:id="formula_10">p i = N ewf itness i SN i=1 N ewf itness i<label>(7)</label></formula><p>where N ewf itness i (calculated by Eq. ( <ref type="formula" target="#formula_11">8</ref>)) is based on Gibbs distribution <ref type="bibr" target="#b22">[23,</ref><ref type="bibr">48]</ref> and Pareto rank value. In statistical physics, the Gibbs distribution designs a framework in thermo-dynamical equilibrium at a given temperature and minimizes the free energy (the principle of the minimal free energy).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Since the key goals of multi-objective optimization (convergence towards the Pareto-optimal set and the maximization of diversity) are analogous to the principle of finding the minimum free energy state in a thermodynamic system, in MOABC, a fitness assignment technique <ref type="bibr" target="#b7">(8)</ref> proposed in [48] is used to compute the fitness of an individual.</p><formula xml:id="formula_11">N ewf itness i = 1 R(i) -T * S(i) -d(i)<label>(8)</label></formula><p>where R(i) is the Pareto rank value of the individual i, T &gt; 0 is a predefined constant value referred as temperature, d(i) is the crowding distance determined by the crowding distance assignment scheme <ref type="bibr" target="#b8">[9]</ref>, and</p><formula xml:id="formula_12">S(i) = -p T (i) log p T (i)<label>(9)</label></formula><p>where</p><formula xml:id="formula_13">p T (i) = (1/Z) exp(-R(i)/T ),</formula><p>and</p><formula xml:id="formula_14">Z = SN 1 exp(-R(i)/T )</formula><p>where p T (i) is the Gibbs distribution, Z is the partition function and SN is the population size. This fitness assignment scheme helps to converge to the Pareto-optimal solutions with a high diversity among the solutions, based on the principle of thermodynamics <ref type="bibr">[47]</ref>.</p><p>B. How to Update Individuals: To update individuals, greedy selection is applied between the current and newly generated individuals through mutation and crossover. However, the individuals do not always dominate the other individuals in multi-objective scenario. Therefore, a fast nondominated sorting scheme instead of greedy selection is applied to select better individuals with lower cost to be retained in the population. The purpose of this scheme is to sort individuals according to the level of nondomination. Each solution is compared with other solutions to determine whether it is dominated. Solutions that are not dominated by any other solution form the first non-dominated Pareto front. To find the solutions in the next front, the solutions appeared in the first front are temporarily discounted and the same procedure is repeated. For each solution p, two entities are calculated: the number of solutions dominating solution p (referred</p><formula xml:id="formula_15">A C C E P T E D M A N U S C R I P T begin foreach p ∈ P do foreach q ∈ P do if p dominates q then S p = S p + {q}; else n p = n p + 1; end end if n p = 0 then F 1 = F 1 ∪ {p}; end end i=1; while F i = ∅ do H = ∅; foreach p ∈ F i do foreach q ∈ F i do n q = n q -1; if n q = 0 then H = ∪ {q}; end end end i = i + 1; F i = H; end end</formula><p>Algorithm 2: Pseudo code of Fast Non-Dominated Sorting.</p><p>as n p ) and the number of solutions dominated by solution p (referred as S p ). In the non-dominated sorting, good solutions are determined by a ranking selection method, and a niche method is applied to keep sub-populations of good points stable. The fast non-dominated sorting for set P is presented in Algorithm 2 <ref type="bibr" target="#b8">[9]</ref>.</p><p>C. How to Generate New Individuals: Due to the large dimensionality and the complex interactions among features, some improvements in the algorithm are also required to overcome the curse of dimensionality and to increase the classification accuracy together. To search the solution space more deeply and to maintain diversity in the population, a deeper search is required. To achieve this, each solution of the population should be evaluated in different perspectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Bin-MOABC and Num-MOABC use different representations which are binary domain and continuous domain, respectively, so they use different ways to generate new solutions. In Bin-MOABC, for each solution x i , a neighborhood solution x k is selected via random selection in the employed bee phase or via probabilistic selection in the onlooker bee phase. After selection, the two-point crossover and two-way mutation are sequentially applied to generate new offsprings (defined as binary solution generator (BSG)):</p><p>1. Two-point crossover: Two positions are randomly determined on binary parents x i and x k . Everything between the positions of x i is copied to x k to generate the first offspring. Then, everything between the positions of x k is copied to x i to generate the latter one. In this way, two offsprings are generated. 2. Two-way mutation: A new mutation scheme is applied in this study.</p><p>First, a number within the range of 0 and 1 is uniformly generated.</p><p>If the generated number is greater than 0.5, a position with value 1 is chosen and its position is set to 0. Otherwise, a position with value 0 is chosen and its position is set to 1. In this way, diversity is satisfied in solution generation and two offsprings are generated. An illustrative sample of two-way mutation is presented in Fig. <ref type="figure" target="#fig_1">1</ref>. In Num-MOABC, the simulated binary crossover (SBX) <ref type="bibr" target="#b0">[1]</ref> and polynomial mutation <ref type="bibr" target="#b24">[25]</ref> are sequentially applied to the current and neighborhood solutions (defined as numeric solution generator (NSG)):</p><p>1. Simulated Binary Crossover (SBX) generates two offsprings in the following way <ref type="bibr" target="#b0">[1]</ref>:</p><formula xml:id="formula_16">off 1,k = 1 2 [(1 -β k )x i,k + (1 + β k )x j,k ] off 2,k = 1 2 [(1 + β k )x i,k + (1 -β k )x j,k ]<label>(11)</label></formula><p>where of f 1,k is the offspring with kth dimension, x i,k and x j,k are the ith and jth solutions with kth dimension, and β k is the uniformly distributed sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T 2. Polynomial Mutation generates offsprings in the following way <ref type="bibr" target="#b24">[25]</ref>:</p><formula xml:id="formula_17">off = x i,j + (x max i,j -x min i,j )δ j (<label>12</label></formula><formula xml:id="formula_18">)</formula><p>where δ j is a variation calculated through polynomial distribution:</p><formula xml:id="formula_19">δ j = (2U (0, 1)) 1 ηm+1 -1, if U (0, 1) &lt; 0.5 δ j = 1 -[2(1 -U (0, 1))] 1 ηm+1 , otherwise<label>(13)</label></formula><p>where U (0, 1) is a uniformly generated number between 0 and 1, and η m is mutation distribution index.</p><p>Therefore, totally four new offsprings are generated for each parent.</p><p>Based on the overall structure shown in Algorithm 1 and the above mentioned schemes, one can see that in the proposed algorithms, a solution (referred as neighborhood solution) is randomly chosen for each current solution in the employed bee phase. Between the current solution and its neighborhood solution, the proposed solution generator is applied to form a new solution set S. In this way, four offsprings are generated for each solution. Note that if the applied algorithm is Bin-MOABC, BSG is used; otherwise, NSG is applied. After the employed bee phase is completed, the solutions in the union set of X and S are ranked using non-dominated sorting, and SN number of solutions are selected to update the population set X through rank and crowding distance. Then, the onlooker bee phase is carried out. In the onlooker bee phase, a neighbor is randomly chosen using thermodynamic principles formulated into Eq. ( <ref type="formula" target="#formula_11">8</ref>), and then genetically inspired NSG or BSG generators are applied to generate new solutions as in employed bee phase. After that, the population set X is updated by selecting the SN highest ranked solutions from the union set of X and S.</p><p>D. Representation and Fitness Function: Each solution represents the activation code (selected or unselected) of the corresponding feature. While activation codes vary in the range between 0 and 1 in Num-MOABC, they are shown through discrete values 0 and 1 in Bin-MOABC. If the activation code of a position is greater than a user specified threshold value, its corresponding feature is selected; otherwise, it is not selected. In this study, the threshold value is defined as 0.5 as in <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b41">42]</ref>. The classification error rate of a feature subset is calculated by:</p><formula xml:id="formula_20">ErrorRate = F P + F N F P + F N + T P + T N (14) A C C E P T E D M A N U S C R I P T</formula><p>where F P and F N are false positives and false negatives, T P and T N are true positives and true negatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment Design</head><p>Twelve datasets comprising of various numbers of features (from 24 to 657), classes (from 2 to 26) and samples (from 351 to 6598) are chosen from UCI machine learning repository <ref type="bibr" target="#b3">[4]</ref> and are shown in Table <ref type="table" target="#tab_0">1</ref>, where the Multiple Features and Optic Characters datasets are referred as 'Multi' and 'Optic', respectively. Each dataset is randomly divided into two sets: 70% as the training set and 30% as the test set, where the partition is stratified to make sure the same class distribution in both sets. The classification performance the feature subsets is evaluated using K Nearest Neighbor (KNN) with K = 5. During the feature selection process, the training set is further partitioned to 10 folds in a stratified way, and 10-fold cross-validation with 5NN is applied as an inner on the training set to evaluate the classification performance of the selected features, i.e. to be used in the fitness function. The inner loop of 10-fold cross-validation is used to avoid feature selection bias, and a detailed discussion on why and how they should be applied in this way is given in <ref type="bibr" target="#b20">[21]</ref>. Note that for the proposed wrapper feature selection methods, any classification algorithm can be used here. We chose KNN because it is simple and relatively cheap, which is particularly important for feature selection problems. Since two main disadvantages of wrapper feature selection are being computationally expensive and less general to other classification methods, using a relatively cheap and simple method can avoid such issues to some extent. Previous research <ref type="bibr">[46]</ref> has shown that using a simple and relatively cheap classification algorithm (like KNN) in a wrapper approach can select a good (near-optimal) feature subset for other complex learning/classification algorithms (e.g. SVM), which are computationally expensive but able to achieve better classification performance.</p><p>To evaluate the performance of the proposed multi-objective ABC based feature selection methods, two traditional, two single objective and three multi-objective algorithms are employed in the experimental studies. The two traditional approaches are linear forward selection (LFS) <ref type="bibr" target="#b9">[10]</ref> and greedy stepwise backward selection (GSBS) <ref type="bibr" target="#b5">[6]</ref> based on SFS and SBS, respectively. They are computationally more efficient and can achieve better performance than SFS and SBS. The experiments of LFS and GSBS are performed via Waikato Environment for Knowledge Analysis (WEKA) <ref type="bibr" target="#b10">[11]</ref> and the feature sets obtained by the approaches are evaluated on the test sets using 5NN.</p><formula xml:id="formula_21">A C C E P T E D M A N U S C R I P T</formula><p>The single objective feature selection approaches are based on standard ABC (ABC-ER and ABC-F it 2C ) using only the classification error rate (Eq. ( <ref type="formula">14</ref>)), and the classification error rate and the number of features together (Eq. ( <ref type="formula" target="#formula_22">15</ref>)) in a weighted manner defined by the parameter α. As in Num-MOABC, solutions representing feature subsets are within the range of 0 and 1. If a dimension in a solution is greater than 0.5, the corresponding feature is selected; otherwise, it is not selected.</p><formula xml:id="formula_22">F it 2C = α * SubsetSize AllSetSize + (1 -α) * ErrorRate ER (<label>15</label></formula><formula xml:id="formula_23">)</formula><p>where α is the predefined value within 0 and 1; SubsetSize is the feature subset size; AllSetSize is the number of all available features in the dataset; ErrorRate is the classification error rate calculated through the selected feature subset; and ER is the error rate calculated through all available features in the dataset. The employed multi-objective feature selection approaches are as follows: NSGAII <ref type="bibr" target="#b8">[9]</ref>, NSSABC <ref type="bibr" target="#b1">[2]</ref> and multi-objective PSO (MOPSO) <ref type="bibr" target="#b7">[8]</ref>. Previous research shows that MOPSO with a continuous representation achieved better performance than with a binary representation <ref type="bibr" target="#b43">[44]</ref> and NSGAII with a binary representation achieved worse performance than MOPSO <ref type="bibr" target="#b44">[45]</ref>. Therefore, we use a continuous representation in both MOPSO and NSGAII as benchmark methods for comparison. NSGAII uses non-dominated sorting, where the population is sorted based on non-dominance relationship. While doing this, crowding distance measure defining how close individual pairs are to each other is used to satisfy diversity in population. NSSABC <ref type="bibr" target="#b1">[2]</ref> A C C E P T E D M A N U S C R I P T is inspired by the non-dominated sorting concept of NSGAII. In NSSABC, non-dominated sorting is applied after mutants are generated, and a mutant solution is generated by Eq. <ref type="bibr" target="#b15">(16)</ref>.</p><formula xml:id="formula_24">υ ij = x ij + φ ij (x ij -x kj ), if U (0, 1) &lt; M R x ij otherwise (<label>16</label></formula><formula xml:id="formula_25">)</formula><p>where M R is the predefined parameter which controls the number of parameters to be modified. MOPSO <ref type="bibr" target="#b7">[8]</ref> uses an external repository to keep a historical record of the non-dominated vectors detected during the search process. A mutation operator is also integrated to the algorithm to avoid premature convergence. Although there exist various multi-objective PSO algorithms, the reason of selecting Coello's MOPSO <ref type="bibr" target="#b7">[8]</ref> for comparisons is that this variant is one of the most well-known multi-objective PSO algorithms.</p><p>For the experiments of multi-objective algorithms, the defined parameter values are as follows: the number of individuals (particles, foods and chromosomes) is set to 30; the maximum number of evaluations is empirically defined as 6000; the parameters of MOPSO are selected as in <ref type="bibr" target="#b7">[8]</ref> where c 1 = 1.49, c 2 = 1.49 and inertial weight= 0.72; the parameters of NSGAII are selected according to <ref type="bibr" target="#b17">[18]</ref> where crossover rate and mutation rate are set to 0.8 and 0.3, respectively; the limit parameter of all ABC based feature selection approaches is set to 100; the T parameter of multi-objective ABC variants is set to 10000; and the MR parameter of NSSABC is chosen 0.5 as in <ref type="bibr" target="#b1">[2]</ref>. Lastly, the α parameter of Eq. ( <ref type="formula" target="#formula_22">15</ref>) is set to 0.2 as in <ref type="bibr" target="#b44">[45]</ref>.</p><p>The results of the feature selection approaches are presented over 30 independent runs in terms of the classification accuracy and feature subset size in Section 5. Note that, LFS and GSBS obtain a unique feature subset on each dataset, and standard ABC obtains a single best result in each of the 30 runs on each dataset, while multi-objective approaches obtain a set of feature subsets in each run. The results obtained by multi-objective approaches are collected into a union set. In the union set, the classification accuracy of the feature subsets including same subset size are averaged. These mean classification accuracy of the same sized feature subsets are called the average Pareto front. In addition to the "average" Pareto front, the non-dominated solutions in the union set are also used for the comparison of different algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>The results are mainly presented in three subsections: 1) Single objective ABC vs. Traditional Approaches, 2) Multi-objective ABC vs. Single objective ABC and Traditional Approaches, and 3) Comparisons of multiobjective approaches. In addition to these subsections, the computational CPU times, and comparisons via the Hypervolume indicator are reported to investigate the effectiveness and search ability of the approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Single Objective ABC vs. Traditional Approaches</head><p>The experimental results of ABC-ER, ABC-F it 2C , LFS and GSBS are presented in Table <ref type="table" target="#tab_1">2</ref> in terms of the classification accuracy ('CAcc') and the number of features ('NOF'). Furthermore, the results obtained by 5-NN using all features are also presented in Table <ref type="table" target="#tab_1">2</ref>, denoted as 'All'. As LFS and GSBS generate a unique solution, there is no standard deviation value for their results.</p><p>For GSBS and LFS, it is seen that LFS can reduce at least half of the available features for each dataset, but it obtains poor classification accuracies for 5 out of 12 datasets. On the other hand, GSBS selects a larger number of features, but it can perform much better than LFS in terms of the classification accuracy rate. However, the feature subsets obtained by GSBS may still include irrelevant and redundant features.</p><p>Table <ref type="table" target="#tab_1">2</ref> shows that ABC-ER (only based on the classification error rate) almost always achieves higher classification performance than the case that all features are used, and it can select around half of the available features. Further, it obtains small feature subset size and similar or high classification accuracy in most cases when compared to GSBS.</p><p>According to Table <ref type="table" target="#tab_1">2</ref>, ABC-F it 2C can get a feature subset the size of which is around half or less than half of the available features, and performs better than the case that all feature are used. ABC-F it 2C also gets higher classification accuracy than LFS in most cases. Furthermore, ABC-F it 2C performs similar or slightly better than GSBS except for the Madelon dataset in terms of the classification performance, and reduces feature subset size more effectively than GSBS. When compared with the ABC-ER approach, ABC-F it 2C generally obtains similar classification performances, but eliminates irrelevant and redundant features effectively. To improve the classification accuracy and reduce the feature subset size simultaneously, Pareto front multi-objective algorithms are needed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Multi-objective ABC vs. Single objective ABC</head><p>To investigate whether considering feature selection problem in a multiobjective ABC manner can perform better than considering in a single objective ABC manner, the experimental results of Bin-MOABC, Num-MOABC, ABC-ER and ABC-F it 2C are presented through charts in Fig. <ref type="figure">2</ref>. Each chart concerns with one of the datasets considered in the experimental study. In each chart, the horizontal and vertical axes represent the feature subset size and the classification accuracy, respectively. On top of each chart, the numbers in the brackets correspond to the number of available features and the classification accuracy using all features. On the corner side of each chart, '-A' and -B' represent the "average" Pareto front and the non-dominated solutions, respectively. Single objective approaches may converge to the same solution (feature subset) in different runs in some datasets. Therefore, the appeared points on some charts for single objective approaches may be fewer than 30 points. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>From Fig. <ref type="figure">2</ref>, it can be observed that Bin-MOABC and Num-MOABC can reduce the feature subset size, and can perform better than using all features in all cases. In almost all datasets, the number of features obtained by Bin-MOABC and Num-MOABC is smaller than 50% of all available features. For instance, on the Musk1 dataset, Bin-MOABC and Num-MOABC reduce the dimensionality from 166 to 40, but increase the classification accuracy from 80% to 90% and 91%, respectively. Accordingly, it can be inferred from the results that considering feature selection in the multi-objective ABC framework is useful and successful versus using all features.</p><p>When comparing Bin-MOABC and Num-MOABC with single objective ABC approaches (ABC-ER and ABC-F it 2C ), it is seen that the lines obtained by Bin-MOABC and Num-MOABC mostly dominate the points representing the results of single objective approaches, which means that ABC-ER and ABC-F it 2C mostly cannot remove irrelevant or redundant features as well as Bin-MOABC and Num-MOABC. Although ABC-ER and ABC-F it 2C reach similar feature subset size with Bin-MOABC and Num-MOABC in some cases, they cannot obtain higher classification performance than Bin-MOABC and Num-MOABC. For instance, on the Madelon dataset, Bin-MOABC and Num-MOABC obtain 83.26% and 82.71% accuracies using 154 features, but ABC-ER and ABC-F it 2C cannot reduce feature subset size and improve the classification performance as well as Bin-MOABC and Num-MOABC. Therefore, the comparisons suggest that the proposed Bin-MOABC and Num-MOABC approaches can explore the search space more effectively than the single objective ABC approaches to detecting better feature subsets. In addition, the weight (α) between the classification error rate and the feature subset size shown in Eq. <ref type="bibr" target="#b14">(15)</ref> does not need to be fine tuned in multi-objective approaches as in single objective approaches.</p><p>Comparisons With Recent Single Objective Approaches: To clarify the performance of Bin-MOABC and Num-MOABC versus singleobjective approaches, we futher compare them with quantum binary inspired PSO (QBPSO), discrete binary ABC (DisABC) and advanced similarity based discrete binary ABC (MDisABC) feature selection approaches. The experimental results of Bin-MOABC, Num-MOABC, QBPSO, DisABC and MDisABC are presented over 7 common datasets in Fig. <ref type="figure">3</ref>.</p><p>From Fig. <ref type="figure">3</ref>, it can be seen that Bin-MOABC and Num-MOABC also perform better than QBPSO, DisABC and MDisABC in terms of reducing the feature subset size and increasing the classification accuracy in most cases. For the cases where recent single-objective ABC and PSO variants achieve</p><formula xml:id="formula_26">A C C E P T E D M A N U S C R I P T</formula><p>similar classification results, multi-objective feature selection approaches successfully eliminates the irrelevant and redundant features compared to ABC and PSO variants. For instance, on the Ionosphere datasets, Bin-MOABC and Num-MOABC obtain 97.14% accuracy using 3 features, MDisABC obtains the same accuracy using 5 and 7 features. Only on the Madelon dataset, DisABC obtains smaller feature subsets. It therefore suggests that considering feature selection in the multi-objective framework is successful and useful versus considering in the single objective framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparisons Between Multi-Objective Approaches</head><p>To test the performance of Bin-MOABC and Num-MOABC with multiobjective approaches, NSGAII, NSSABC and MOPSO are employed. We first present the overall results of using the Hypervolume indicator to give an overall idea of the performance difference. Then the results of non-dominated and average Pareto fronts are presented on the test sets in Figs. <ref type="figure">4</ref> and<ref type="figure">5</ref>, and on the training sets in Figs. <ref type="figure">6</ref> and<ref type="figure" target="#fig_4">7</ref>.</p><p>Comparisons via Hypervolume Indicator: In order to measure the quality of the obtained Pareto fronts, hypervolume indicator <ref type="bibr" target="#b2">[3]</ref> is employed to further compare the approaches. Hypervolume metric defined by Eq. ( <ref type="formula" target="#formula_27">17</ref>) gives the volume of hypercube covered by the members of Pareto-solutions.</p><formula xml:id="formula_27">HV = volume |P | i=1 v i<label>(17)</label></formula><p>In each run, each approach obtains two Pareto fronts: training Pareto front based on the training classification accuracy and feature subset size, and testing Pareto front based on the testing classification accuracy and feature subset size. For each approach, 30 hypervolume values are calculated based on the training results, and 30 hypervolume values are calculated based on the test results. The obtained hypervolume values are normalized into the range of 0 and 1 and then Wilcoxon Rank Sum test (in which the confidence level is 95%) is applied to measure the differences between the proposed and existing approaches. To determine whether there exists any difference between approaches, the following markers are used in the tables:</p><p>• "+" indicates that Bin-MOABC (Num-MOABC) is significantly better than another corresponding approach, while "-" indicates that corresponding approach is better than Bin-MOABC (Num-MOABC).</p><p>Table <ref type="table">3</ref> shows the results of Wilcoxon Rank Sum test on the hypervolume ratio in the training process, in which 'Bin' and 'Num' refer to Bin-MOABC and Num-MOABC, respectively. Note that the comparisons are processed from the top side (Num and Bin) to the left side. The results indicate that Bin-MOABC is superior to the other approaches in most cases. Only for 1 out of 60 cases (5 algorithms × 12 datasets), Bin-MOABC gets significantly worse results than MOPSO. However, the same case cannot be suggested for Num-MOABC. For instance, Num-MOABC obtains worse results than NSGAII in most cases.</p><p>Table <ref type="table" target="#tab_2">4</ref> presents the results of Wilcoxon Rank Sum test on the hypervolume ratio in the testing process. According to Table <ref type="table" target="#tab_2">4</ref>, for low dimensional datasets, Vehicle, German and Ionosphere, Bin-MOABC achieves similar results with Num-MOABC, NSGAII and MOPSO, but gets significantly better results than NSSABC. For the high dimensional datasets, Bin-MOABC achieves significantly better results than the other approaches in all cases. Num-MOABC generally obtains significantly better results than NSSABC, and similar or worse results than NSGAII and MOPSO.</p><p>Detailed Comparisons: According to Fig. <ref type="figure">4</ref>, for the datasets such as Vehicle, German and Ionosphere, there is no significant difference between the non-dominated results of all algorithms in most cases. Except for these </p><formula xml:id="formula_28">A C C E P T E D M A N U S C R I P T</formula><formula xml:id="formula_29">= = = - - - Num-MOABC = = = + + + NSSABC = = + + + + + + + + + + NSGAII = = = = = = + - + - + = MOPSO = = - - = = + + + = + = Musk 1 Musk2 Semeion Madelon Isolet Multiple Bin Num Bin Num Bin Num Bin Num Bin Num Bin Num Bin-MOABC - - - - - = Num-MOABC + + + + + = NSSABC + = + + + = + + + + + + NSGAII + - + = + - + = + = + + MOPSO + = + = + = + - = - = =</formula><p>low dimensional datasets, the differences between the algorithms can be easily illustrated such that the proposed Bin-MOABC outperforms the others in almost all cases in terms of the classification performance and the number of features. For instance, on the Madelon dataset, Bin-MOABC reduces the feature subset size from 500 to 148 and obtains 82.56% classification accuracy. However, the other approaches cannot remove features and increase the classification accuracy as in Bin-MOABC. It can be extracted from Fig. <ref type="figure">4</ref> that NSGAII, Num-MOABC, NSSABC and MOPSO are ranked as second, third, fourth and the last order, respectively. It is seen that NSSABC and especially MOPSO are not good at eliminating irrelevant and redundant features. When taking a look at the overall distribution of solutions (average Pareto fronts) in Fig. <ref type="figure">5</ref>, the results indicate that the success of Bin-MOABC and Num-MOABC carry on in most cases, especially in Movement, Hill Valley, Musk1, Musk2, Madelon and Isolet. Not only on the test sets, but also on the training sets Bin-MOABC outperforms the others in terms of both non-dominated solutions and average Pareto fronts, as shown in Figs. <ref type="figure">6</ref> and<ref type="figure" target="#fig_4">7</ref>. Most of the lines representing the results of other approaches are appeared under the lines of Bin-MOABC, which reflects that Bin-MOABC also has the potential to significantly minimize the number of features and increase the training classification accuracy together. As in the test sets, the performances of the NSGAII, NSSABC and MOPSO approaches can be ranked as second, third and last positions in the training sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Further Comparisons using the Quadratic Classifier</head><p>In order to see whether the proposed approaches can carry on their successful performances through different classifiers against other approaches, we</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T use quadratic discriminant analysis <ref type="bibr" target="#b23">[24]</ref> which is a more general version of linear discriminant analysis. Quadratic discriminant analysis first computes the sample mean of each class. Then, it evaluates the sample covariances by first subtracting the sample mean of each class from the observations of that class, and taking the empirical covariance matrix of each class. The results of multi-objective approaches over quadratic discriminant analysis are presented in Figs. <ref type="figure">8</ref> and<ref type="figure" target="#fig_6">9</ref> on the test sets. In each chart, the horizontal axes represent the number of features and vertical axes represent the classification accuracy. On top of each chart, the numbers in the brackets correspond to the number of available features and the classification accuracy obtained by quadratic discriminant analysis using all features. Note that it could not be applied to the other 5 datasets since the computed covariance matrix of each group must be positive.</p><p>According to Fig. <ref type="figure">8</ref>, for the Vehicle and German datasets which are low-dimensional problems, the non-dominated results obtained by the multiobjective approaches are similar to each other. On the other hand, as for the Musk1, Hill Valley and Madelon datasets, the non-dominated results obtained by Bin-MOABC are strongly better than other approaches in almost all cases in terms of the classification performance and the number of features. For instance, on the Hill Valley dataset, Bin-MOABC achieves 89.28% accuracy for 19 features, while NSGAII obtains 84.28% accuracy for the same number of features. According to Fig. <ref type="figure" target="#fig_6">9</ref>, Bin-MOABC performs better than others also in average Pareto fronts. Although NSGAII generally performs better than Num-MOABC in terms of the non-dominated results, Num-MOABC mostly achieves more successful results than NSGAII in terms of average Pareto fronts. Therefore, it can be inferred that the success of the proposed approaches also carries on using quadratic discriminant analysis.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Computational Time Analysis</head><p>The experiments are implemented in MATLAB 2013a and are executed on a computer with an Intel Core i7-4700HQ 2.40 GHz CPU and 8 GB RAM, and the computational time is presented in terms of mean values over the 30 runs in Table <ref type="table">5</ref>. According to Table <ref type="table">5</ref>, the computational time is increased proportional to the dimensionality and sample size. For example, it takes only a few minutes for the datasets which have a small number of features or samples such as Vehicle, German and Ionosphere. Bin-MOABC is more efficient than the other approaches in terms of the CPU computational time in most cases, i.e, it can complete the training process in shorter time than other approaches.</p><p>Considering the other approaches, it is seen that MOPSO consumes more time in high dimensional datasets perhaps due to its external archive mechanism. NSGAII, NSSABC and Num-MOABC perform similar or slightly worse than Bin-MOABC in terms of the CPU time. The reason why Bin-MOABC generally consumes less time than the other approaches may be that Bin-MOABC depends on simple binary crossover and mutation exchanging techniques, i.e., it does not depend on numerical crossover and mutation techniques which requires more calculations. The other reason is that Bin-MOABC tends to choose smaller feature subsets than the other approaches during the training process. Hence, the objective function evaluation overhead is less. Therefore, it can be concluded that not only in the classification rate and feature subset size, but also in the CPU computational time the proposed Bin-MOABC approach performs well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Further Discussion</head><p>As can be seen from the results, Bin-MOABC outperforms the other approaches in terms of the classification rate, feature subset size and computational time. The factors of Bin-MOABC resulting better performance than the others are as follows. First, searching in binary domain is more suitable than searching in continuous domain for feature selection which is a binary NP-hard problem. However, this may not be individually sufficient to achieve convincing results. In other words, the suitability of search operators on the problem structure is also very crucial to get high classification performance and small feature subset size. For instance, although binary PSO (BPSO) searches in binary domain, it generally cannot achieve better results than standard PSO in feature selection problems <ref type="bibr" target="#b44">[45]</ref>. In Bin-MOABC, binary search operators such as two-way mutation and generation strategy are designed for the effective and efficient search in feature selection problems.</p><p>Another factor is the positive feedback in the phase of onlooker bees that increases the possibility of selecting high quality food sources for the exploration-exploitation process. Although high quality food sources have more chance than others to be processed, other food sources can also be selected in a probabilistic manner. Accordingly, diversity among sources is tried to be preserved. The last supporting factor is that there exists a balance between exploration and exploitation processes through the 'limit' parameter in ABC. If any food source is exhausted, it is left and a new food source is generated. This property brings innovation and diversity to the population and counterbalance the saturation in the population due to positive feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>The general goal of this paper was to demonstrate an effective and efficient multi-objective feature selection approach for classification. This goal was fulfilled by introducing two multi-objective ABC frameworks (Bin-MOABC and Num-MOABC). The performance analysis of the proposed algorithms was conducted by making comparisons with the single objective ABC algorithms (ABC-ER and ABC-F it 2C ), traditional algorithms (LFS and GSBS) and multi-objective algorithms (NSGAII, NSSABC and MOPSO) on 12 benchmark datasets, most of which are high dimensional. The experimental results show that Bin-MOABC and Num-MOABC outperform ABC-ER, ABC-F it 2C , LFS and GSBS in terms of the classification performance and feature subset size almost in all cases. Therefore, the proposed This paper represents an early work on ABC-based multi-objective approach to feature selection. Despite the good performance, there are also some drawbacks with the proposed algorithms, for example it is computationally expensive, and their scalability to datasets with thousands of features is still unknown. In the future, we will carry on developing multi-objective ABC based approaches for feature selection, which can better search the Pareto front of non-dominated solutions in possible solution space. We also would like to investigate multi-objective feature selection methods on largescale datasets with thousands or even tens of thousands of features, which may requires a very different design of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">References</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Data mining is in the intersection of artificial intelligence, machine learning, statistics and database systems. It is basically the process of extracting A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustrative representation on how two-way mutation is applied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Results of Bin-MOABC, Num-MOABC, ABC-ER and ABC-F it 2C (in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :Figure 6 :</head><label>456</label><figDesc>Figure 4: Non-dominated results of multi-objective approaches on the test sets (in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Average Pareto fronts of multi-objective approaches on the training sets (in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Average Pareto fronts of multi-objective approaches on the training sets (in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Average Pareto fronts of multi-objective approaches over quadratic discriminant analysis on the test sets (in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Average Pareto fronts of multi-objective approaches over quadratic discriminant analysis on the test sets (in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>S C R I P Tmulti-objective algorithms can be used for feature selection instead of single objective and traditional algorithms. The results also indicate that Bin-MOABC outperformed Num-MOABC, NSGAII, NSSABC and MOPSO in both test set and training set. Furthermore, Bin-MOABC completes the feature selection process more efficiently than the other multi-objective algorithms. The Num-MOABC approach generally cannot obtain as good results as Bin-MOABC and NSGAII, although it employs the similar mechanism with Num-MOABC and NSGAII.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Datasets</figDesc><table><row><cell>Datasets</cell><cell cols="2">Features Classes</cell><cell>Samples</cell></row><row><cell>Vehicle</cell><cell>18</cell><cell>4</cell><cell>846</cell></row><row><cell>German</cell><cell>24</cell><cell>2</cell><cell>1000</cell></row><row><cell>Ionosphere</cell><cell>34</cell><cell>2</cell><cell>351</cell></row><row><cell>Optical Recognition</cell><cell></cell><cell></cell><cell></cell></row><row><cell>of Handwritten Digits</cell><cell>64</cell><cell>10</cell><cell>5620</cell></row><row><cell>Libras Movement</cell><cell>90</cell><cell>15</cell><cell>360</cell></row><row><cell>Hill Valley</cell><cell>100</cell><cell>2</cell><cell>606</cell></row><row><cell>Musk 1</cell><cell>166</cell><cell>2</cell><cell>476</cell></row><row><cell>Musk 2</cell><cell>166</cell><cell>2</cell><cell>6598</cell></row><row><cell>Semeion</cell><cell>256</cell><cell>10</cell><cell>1593</cell></row><row><cell>Madelon</cell><cell>500</cell><cell>2</cell><cell>2600</cell></row><row><cell>Isolet</cell><cell>617</cell><cell>26</cell><cell>1559</cell></row><row><cell>Multiple Features</cell><cell>649</cell><cell>10</cell><cell>2000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results of ABC-ER, ABC-F it 2C , LFS and GSBS</figDesc><table><row><cell>Datasets</cell><cell>ABC-ER</cell><cell cols="2">ABC-F it 2C LFS</cell><cell>GSBS ALL</cell></row><row><cell>Vehicle</cell><cell cols="2">CAcc 79.53(1.67) 77.88(1.87) NOF 9.86 7.73</cell><cell cols="2">72.11 75.3 9 16</cell><cell>76.10 18</cell></row><row><cell>German</cell><cell cols="2">CAcc 70.17(1.14) 70.1(1.94) NOF 10.76 9.13</cell><cell cols="2">68.33 69.33 5 20</cell><cell>68 24</cell></row><row><cell>Ionosphere</cell><cell cols="2">CAcc 92.12(1.80) 91.74(2.02) NOF 12 11.53</cell><cell cols="2">90.48 89.52 6 29</cell><cell>89.52 34</cell></row><row><cell>Optical</cell><cell cols="2">CAcc 98.10(0.31) 98.22(0.24) NOF 41.13 37.43</cell><cell cols="2">97.86 98.75 32 38</cell><cell>98.87 64</cell></row><row><cell>Movement</cell><cell cols="2">CAcc 77.58(2.21) 77.46(2.59) NOF 42.56 40.23</cell><cell cols="2">71.43 77.14 10 79</cell><cell>80.00 90</cell></row><row><cell>Hill Valley</cell><cell cols="2">CAcc 54.13(2.11) 54.92(1.78) NOF 47.63 44.96</cell><cell cols="2">55.49 54.40 9 95</cell><cell>52.75 100</cell></row><row><cell>Musk 1</cell><cell cols="2">CAcc 83.11(2.42) 82.32(2.95) NOF 83.03 80.56</cell><cell cols="2">80.71 82.86 12 124</cell><cell>80.00 166</cell></row><row><cell>Musk 2</cell><cell cols="2">CAcc 81.52(2.93) 81.54(2.55) NOF 82.26 81.26</cell><cell cols="2">82.87 80.24 8 122</cell><cell>79.99 166</cell></row><row><cell>Semeion</cell><cell cols="2">CAcc 87.96(0.84) 86.56(1.09) NOF 131.96 132.2</cell><cell cols="2">77.33 91.04 27 237</cell><cell>90.83 256</cell></row><row><cell>Madelon</cell><cell cols="2">CAcc 72.91(1.74) 72.20(2.08) NOF 252.46 248.03</cell><cell cols="2">71.03 74.88 7 250</cell><cell>71.79 500</cell></row><row><cell>Isolet</cell><cell cols="2">CAcc 82.52(1.23) 82.71(1.14) NOF 312.93 306.80</cell><cell cols="2">76.28 80.77 27 585</cell><cell>80.98 617</cell></row><row><cell>Multiple</cell><cell cols="2">CAcc 96.57(0.28) 96.78(0.28) NOF 322.83 315.50</cell><cell cols="2">82.33 93.20 20 472</cell><cell>95.17 649</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Wilcoxon Rank Sum Test of Hypervolume Ratios on Testing</figDesc><table><row><cell>Vehicle</cell><cell>German Ionosphere</cell><cell>Optical</cell><cell>Movement Hill Valley</cell></row><row><cell cols="4">Bin Num Bin Num Bin Num Bin Num Bin Num Bin Num</cell></row><row><cell>Bin-MOABC</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>• "=" indicates that the results of Bin-MOABC (Num-MOABC) are similar to the results of corresponding approach.</p><p>• the empty cells indicate that Bin-MOABC (Num-MOABC) is "nonapplicable" with itself.   [47] X. Zou, Y. Chen, M. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Simulated binary crossover for continuous search space</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. rep</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Synchronous and asynchronous pareto-based multi-objective artificial bee colony algorithms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Akay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Global Optimization</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="415" to="445" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Theory of the hypervolume indicator: optimal µ-distributions and the choice of the reference point</title>
		<author>
			<persName><forename type="first">A</forename><surname>Auger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brockhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zitzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM SIGEVO Workshop on Foundations of Genetic Algorithms, FOGA &apos;09</title>
		<meeting>the 10th ACM SIGEVO Workshop on Foundations of Genetic Algorithms, FOGA &apos;09</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">K</forename><surname>Bache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
	</analytic>
	<monogr>
		<title level="j">UCI machine learning repository</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using mutual information for selecting features in supervised neural net learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Battiti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="537" to="550" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Greedy attribute selection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Machine Learning</title>
		<meeting>the Eleventh International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improved binary particle swarm optimization using catfish effect for feature selection</title>
		<author>
			<persName><forename type="first">L.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="12699" to="12707" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Handling multiple objectives with particle swarm optimization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Coello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pulido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lechuga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="256" to="279" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A fast and elitist multiobjective genetic algorithm: Nsga-II</title>
		<author>
			<persName><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Meyarivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="182" to="197" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large-scale attribute selection using wrappers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gutlein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karwath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Computational Intelligence and Data Mining (CIDM &apos;09)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The weka data mining software: An update</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Correlation-based feature selection for machine learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>The University of Waikato</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-objective feature selection with nsga ii</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hamdani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Karray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adaptive and Natural Computing Algorithms</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer Berlin Heidelberg</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4431</biblScope>
			<biblScope unit="page" from="240" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<title level="m">Data mining: concepts and techniques</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A binary ABC algorithm based on advanced similarity scheme for feature selection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hancer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Karaboga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="334" to="348" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A multi-objective artificial bee colony approach to feature selection using fuzzy mutual information</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hancer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Karaboga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Akay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>IEEE Congress on Evolutionary Computation (CEC)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A distributed pso-svm hybrid system with feature selection and parameter optimization</title>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Dun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1381" to="1391" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Analysis of the optimal treatment methods of aids using non-dominated sorting genetic algorithm II (nsga-II)</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kalami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khaloozadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of Control</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Instrumentation and Automation</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A comprehensive survey: artificial bee colony (ABC) algorithm and applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Karaboga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gorkemli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ozturk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karaboga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="57" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A practical approach to feature selection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Rendell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Workshop on Machine Learning</title>
		<meeting>the Ninth International Workshop on Machine Learning<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Wrappers for feature subset selection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="273" to="324" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Input feature selection for classification problems</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="143" to="159" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Landau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Lifshitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Physics. Course of Theoretical Physics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="1980">1980</date>
			<publisher>Pergamon Press</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
	<note>rd ed.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Shrunk the sample covariance matrix, The Journal of Portfolio Management</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ledoit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An elitist polynomial mutation operator for improved performance of moeas in computer networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Liagkouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Metaxiotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd International Conference on Computer Communications and Networks (ICCCN&apos;2013)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Toward integrating feature selection algorithms for classification and clustering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="491" to="502" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An improved particle swarm optimization for feature selection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Bionic Engineering</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="191" to="200" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On the effectiveness of receptors in recognition systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Marill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="17" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hybrid genetic algorithms for feature selection</title>
		<author>
			<persName><forename type="first">I.-S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-R</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1424" to="1437" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Feature selection based on mutual information: criteria of max-dependency, max-relevance, and min-redundancy</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1226" to="1238" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Floating search methods in feature selection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pudil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Novovicova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1119" to="1125" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dimensionality reduction using genetic algorithms</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Raymer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Punch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="164" to="171" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Data feature selection based on artificial bee colony algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schiezaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pedrini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Artificial bee colony based feature selection for effective cardiovascular disease diagnosis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Subanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rajalaxmi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Scientific &amp; Engineering Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="606" to="612" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Bare-Bone Particle Swarm Optimisation forSimultaneously Discretising and Selecting Features for High-Dimensional Classification</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9597</biblScope>
			<biblScope unit="page" from="701" to="718" />
		</imprint>
	</monogr>
	<note>chap. 19th European Conference on Applications of Evolutionary Computation, EvoApplications 2016 Part I</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Genetic programming for feature construction and selection in classification on high-dimensional data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memetic Computing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">mr2pso: A maximum relevance minimum redundancy feature selection method based on swarm intelligence for support vector machine classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Unler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Murat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Chinnam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="4625" to="4641" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Feature selection method based on artificial bee colony algorithm and support vector machines for medical datasets classification</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Uzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nihat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Inan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Scientific World Journal</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Feature subset selection using multi-objective genetic algorithms</title>
		<author>
			<persName><forename type="first">K</forename><surname>Waqas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th IEEE International Multitopic Conference</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A direct method of nonparametric measurement selection</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Whitney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1100" to="1103" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A multi-objective particle swarm optimisation for filter-based feature selection in classification problems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cervante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection Science</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="91" to="116" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Binary PSO and rough set theory for feature selection: A multi-objective filter based approach</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cervante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">N</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computational Intelligence and Applications</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page">1450009</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A survey on evolutionary computation approaches to feature selection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="606" to="626" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multi-objective particle swarm optimisation (pso) for feature selection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">N</forename><surname>Browne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Annual Conference on Genetic and Evolutionary Computation</title>
		<meeting>the 14th Annual Conference on Genetic and Evolutionary Computation<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Particle swarm optimization for feature selection in classification: A multi-objective approach</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">N</forename><surname>Browne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1656" to="1671" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
