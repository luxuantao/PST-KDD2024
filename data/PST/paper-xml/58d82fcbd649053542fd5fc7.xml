<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepCas: an End-to-end Predictor of Information Cascades</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
							<email>lichengz@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiaqi</forename><surname>Ma</surname></persName>
							<email>jiaqima@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of EECS</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
							<email>qmei@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of EECS</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DeepCas: an End-to-end Predictor of Information Cascades</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">028ECAD53355AE9EDA6740EA1F8A5C88</idno>
					<idno type="DOI">10.1145/3038912.3052643</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Information cascades, effectively facilitated by most social network platforms, are recognized as a major factor in almost every social success and disaster in these networks. Can cascades be predicted? While many believe that they are inherently unpredictable, recent work has shown that some key properties of information cascades, such as size, growth, and shape, can be predicted by a machine learning algorithm that combines many features. These predictors all depend on a bag of hand-crafting features to represent the cascade network and the global network structures. Such features, always carefully and sometimes mysteriously designed, are not easy to extend or to generalize to a different platform or domain.</p><p>Inspired by the recent successes of deep learning in multiple data mining tasks, we investigate whether an end-to-end deep learning approach could effectively predict the future size of cascades. Such a method automatically learns the representation of individual cascade graphs in the context of the global network structure, without hand-crafted features or heuristics. We find that node embeddings fall short of predictive power, and it is critical to learn the representation of a cascade graph as a whole. We present algorithms that learn the representation of cascade graphs in an end-to-end manner, which significantly improve the performance of cascade prediction over strong baselines including feature based methods, node embedding methods, and graph kernel methods. Our results also provide interesting implications for cascade prediction in general.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Most modern social network platforms are designed to facilitate fast diffusion of information. Information cascades are identified to be a major factor in almost every plausible or disastrous social network phenomenon, ranging from viral marketing, diffusion of innovation, crowdsourcing, rumor spread, cyber violence, and various types of persuasion campaigns.</p><p>If cascades can be predicted, one can make wiser decisions in all these scenarios. For example, understanding which types of Tweets will go viral helps marketing specialists to design their strategies; predicting the potential influence of a rumor enables administrators to make early interventions to avoid serious consequences. A prediction of cascade size benefits business owners, investors, journalists, policy makers, national security agents, and many others.</p><p>Can cascades be predicted? While many believe that cascades are inherently unpredictable, recent work has shown that some key properties of information cascades, such as size, growth, and shape, can be predicted through a mixture of signals <ref type="bibr" target="#b10">[10]</ref>. Indeed, cascades of microblogs/Tweets <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b37">37]</ref>, photos <ref type="bibr" target="#b10">[10]</ref>, videos <ref type="bibr" target="#b2">[2]</ref> and academic papers <ref type="bibr" target="#b31">[31]</ref> are proved to be predictable to some extent. In most of these studies, cascade prediction is cast as classification or regression problems and is solved with machine learning techniques that incorporate many features <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b20">20]</ref>. On one hand, many of these features are specific to the particular platform or to the particular type of information being diffused. For example, whether a photo was posted with a caption is shown to be predictive of how widely it spread on Facebook <ref type="bibr" target="#b10">[10]</ref>; specific wording on Tweets is shown to help them gain more retweets <ref type="bibr" target="#b33">[33]</ref>. These features are indicative but cannot be generalized to other platforms or to other types of cascades. On the other hand, a common set of features, those extracted from the network structure of the cascade, are reported to be predictive by multiple studies <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b39">39]</ref>.</p><p>Many of these features are carefully designed based on the prior knowledge from both network theory and empirical analyses, such as centrality of nodes, community structures, tie strength, and structural holes. There are also ad hoc features that appear very predictive, but their success is intriguing and sometimes magical. For example, Cheng et al. <ref type="bibr" target="#b10">[10]</ref> found that one of the most indicative feature to the growth of a cascade is whether any of the first a few reshares are not directly connected to the root of the diffusion.</p><p>We consider this as a major deficiency of these machine learning approaches: their performance heavily depends on the feature representations, yet there is no common principle of how to design and measure the features. Is degree the correct measure of centrality? Which algorithm should we use to extract communities, out of the hundreds available? How accurately can we detect and measure structural holes? How do we systematically design those "magical" features, and how do we know we are not missing anything important? Chances are whichever decisions we make we'll be losing information and making mistakes, and these mistakes will be accumulated and carried through to the predictions.</p><p>Can one overcome this deficiency? The recent success of deep learning in different fields inspires us to investigate an end-to-end learning system for cascade prediction, a system that pipes all the way through the network structures to the final predictions without making arbitrary decisions about feature design. Such a deep learning pipeline is expected to automatically learn the representations of the input data (cascade graphs in our case) that are the most predictive of the output (cascade growth), from a finer-granularity to increasingly more abstract representations, and allow the lower-level representations to update based on the feedback from the higher levels. A deep neural network is particularly good at learning a nonlinear function that maps these representations to the prediction, in our case the future size of a cascade. While deep learning models have shown their great power of dealing with image, text, and speech data, how to design a suitable architecture to learn the representations of graphs remains a major challenge. In the context of cascade prediction, the particular barrier is how to go from representations of nodes to representing a cascade graph as a whole.</p><p>We present a novel, end-to-end deep learning architecture named the DeepCas, which first represents a cascade graph as a set of cascade paths that are sampled through multiple random walk processes. Such a representation not only preserves node identities but also bounds the loss of structural information. Analogically, cascade graphs are represented as documents, with nodes as words and paths as sentences. The challenge is how to sample the paths from a graph to assemble the "document," which is also automatically learned through the end-to-end model to optimize the prediction of cascade growth. Once we have such a "document" assembled, deep learning techniques for text data could be applied in a similar way here. We evaluate the performance of the proposed method using real world information cascades in two different domains, Tweets and scientific papers. DeepCas is compared with multiple strong baselines, including feature based methods, node embedding methods, and graph kernel methods. DeepCas significantly improves the prediction accuracy over these baselines, which provides interesting implications to the understanding of information cascades. To ease the reproduction of our results, we have made the source code of DeepCas publicly available<ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>In a networked environment, people tend to be influenced by their neighbors' behaviors and decisions <ref type="bibr" target="#b13">[13]</ref>. Opinions, product advertisements, or political propagandas could spread over the network through a chain reaction of such influence, a process known as the information cascade <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b1">1]</ref>. We present the first deep learning method to predict the future size of information cascades.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Cascade Prediction</head><p>Cascades of particular types of information are empirically proved to be predictable to some extent, including Tweets/microblogs <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b44">44]</ref>, photos <ref type="bibr" target="#b10">[10]</ref>, videos <ref type="bibr" target="#b2">[2]</ref> and academic papers <ref type="bibr" target="#b31">[31]</ref>. In literature, cascade prediction is mainly formulated in two ways. One treats cascade prediction as a classification problem <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b11">11]</ref>, which predicts whether or not a piece of information will become popular and wide-spread (above a certain threshold). The other formulates cascade prediction as a regression problem, which predicts the numerical properties (e.g., size) of a cascade in the future <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b35">35]</ref>. This line of work can be further categorized by whether it outputs the final size of a cascade <ref type="bibr" target="#b44">[44]</ref> or the size as a function of time (i.e., the growth of the cascade) <ref type="bibr" target="#b43">[43]</ref>. Either way, most of the methods identified temporal properties, topological structure of the cascade at the early stage, source and early adopters of the information, and the content being spread as the most predictive factors.</p><p>These factors are utilized for cascade prediction in two fashions. The first mainly designs generative models of the cascade process based on temporal or structural features, which can be as simple as certain macroscopic distributions (e.g., of cascade size over time) <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b3">3]</ref> or stochastic processes that explain the microscopic actions of passing along the information <ref type="bibr" target="#b43">[43]</ref>. These generative models make various strong assumptions and oversimplify the reality. As a result, they generally underperform in real prediction tasks.</p><p>Alternatively, these factors may be represented through handcrafted features, which are extracted from the data, combined, and weighted by discriminative machine learning algorithms to perform the classification or the regression tasks <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b11">11]</ref>. Most work in this fashion uses learning methods without end-to-end training, whose performance heavily relies on the quality of the features. In general, there is no principled and systematic way to design these features. Some most predictive features are tied to particular platforms or particular cascades and are hard to be generalized, such as the ones mentioned in the introduction. Some features are closely related to the structural properties of the social network, such as degree <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b39">39]</ref>, density <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b17">17]</ref>, and community structures <ref type="bibr" target="#b39">[39]</ref>. These features could generalize over domains and platforms, but many may still involve arbitrary and hard decisions in computation, such as what to choose from hundreds of community detection algorithms <ref type="bibr" target="#b14">[14]</ref> and how to detect structural holes <ref type="bibr" target="#b42">[42]</ref>. Besides, there are also heuristic features that perform very well in particular scenarios but it is hard to explain why they are designed as is.</p><p>Our work differs from this literature as we take an end-to-end view of cascade prediction and directly learn the representations of a cascade without arbitrary feature design. We focus on the structures (including node identities) of cascades as content features are reported to be much weaker predictors than structural features <ref type="bibr" target="#b10">[10]</ref>. Using temporal signals to predict future trend is a standard problem in time series, and these signals might not be available under certain circumstances, where only cascade snapshots can be observed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning the Representation of Graphs</head><p>Our work is also related to the literature of representation learning for graphs. Networks are traditionally represented as affiliation matrices or discrete sets of nodes and edges. Modern representation learning methods attempt to represent nodes as high-dimensional vectors in a continuous space (a.k.a., node embeddings) so that nodes with similar embedding vectors share similar structural properties (e.g., <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b16">16]</ref>). Rather than learning the representation of each node, recent work also attempts to learn the representation of subgraph structures <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b41">41]</ref>. Much of this work is inspired by the huge success of representation learning and deep learning applied to various domains such as text <ref type="bibr" target="#b4">[4]</ref> and image <ref type="bibr" target="#b21">[21]</ref>. For example, DeepWalk <ref type="bibr" target="#b29">[29]</ref> makes an analogy between the nodes in networks and the words in natural language and uses fixed-length random walk paths to stimulate the "context" of a node so that node representations can be learned using the same method of learning word representations <ref type="bibr" target="#b25">[25]</ref>. The representation of a graph can then be calculated by averaging the embeddings of all nodes.</p><p>Another line of related work comes from the domain of graph kernels, which computes pairwise similarities between graphs <ref type="bibr">[7,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b32">32]</ref>. For example, the Weisfeiler-Lehman subtree kernel (WL) <ref type="bibr" target="#b32">[32]</ref> computes the graph similarity based on the sub-trees in each graph. Some studies have applied deep learning techniques to improving graph kernels <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b27">27]</ref>. Though graph kernels are good at extracting structural information from a graph, it is hard for them to incorporate node identity information.</p><p>Another analogy connects graph structures to images. Motivated by representation learning of images, the topological structures of networks are first represented using locally connected regions <ref type="bibr" target="#b28">[28]</ref>, spectral methods <ref type="bibr" target="#b12">[12]</ref>, and heat kernel signatures <ref type="bibr" target="#b24">[24]</ref>, which could then be passed through convolutional neural networks. These approaches are insensitive to orders of nodes and have an advantage of generating the same representation for isomorphic graphs. As the  expense, it is hard to incorporate the identities of nodes, which are critical to the prediction of cascades. Indeed, a remarkable study recently shows that mentioning certain users in a Tweet could lead to a bursting diffusion of that Tweet <ref type="bibr" target="#b37">[37]</ref>.</p><p>Starting in next section, we present a novel end-to-end architecture that learns the representation of cascade graphs to optimize the prediction accuracy of their future sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHOD</head><p>In reality, we observe snapshots of the social network but may or may not observe the exact time when nodes and edges are introduced. Similarly, we may observe snapshots of a cascade but not its complete history. In other words, at a given time we know who have adopted the information but not when or through whom the information was passed through <ref type="bibr" target="#b10">[10]</ref> (e.g., we know who cited a paper but not when and where she found the paper). Below we define the problem so that it is closely tied to the reality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>Given a snapshot of a social network at time t0, denote it as G = (V, E) where V is the set of nodes and E ⊂ V × V is the set of edges. A node i ∈ V represents an actor (e.g., a user in Twitter or an author in the academic paper network) and an edge (i, j) ∈ E represents a relationship tie (e.g., retweeting or citation) between node i and j up to t0.</p><p>Let C be the set of cascades which start in G after time t0. A snapshot of cascade c ∈ C with a duration t after its origination is characterized by a cascade graph</p><formula xml:id="formula_0">g t c = (V t c , E t c ), where V t c</formula><p>is a subset of nodes in V that have adopted the cascade c within duration t after its origination and</p><formula xml:id="formula_1">E t c = E ∩ (V t c × V t c</formula><p>), which is the set of edges in E with both ends inside V t c . We consider the problem of predicting the increment of the size of cascade c after a given time interval ∆t, which is denoted as</p><formula xml:id="formula_2">∆sc = |V t+∆t c |-|V t c |.</formula><p>The cascade prediction is to find a function f that maps g t c to ∆sc, f : g t c → ∆sc. In the definition, t indicates the earliness of the prediction and ∆t indicates the horizon of the prediction. When t is smaller, we are making predictions at the early stage of a cascade; when ∆t is larger, we are predicting the size of cascade that is closer to its final status. These scenarios are particularly valuable but inherently harder in reality. It is worth noting that we consider the social network structure G as static in the prediction task. While in reality the global network does change over time, we are doing this to control for the effect of cascades on the network structure in this study -new edges may form due to a particular information cascade.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DeepCas: the End-to-End Pipeline</head><p>We propose an end-to-end neural network framework that takes as input of the cascade graph gc and predicts the increment of cascade size ∆sc. The framework (shown in figure <ref type="figure" target="#fig_2">1</ref>) first samples node sequences from cascade graphs and then feeds the sequences into a gated recurrent neural network, where attention mechanisms are specifically designed to learn how to assemble sequences into "documents," so that the future cascade size could be predicted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cascade Graph as Random Walk Paths</head><p>Given a cascade graph gc, the first component in DeepCas generates an initial representation of gc using a set of node sequences.</p><p>Naturally, the future size of a cascade highly depends on who the information "propagators" are, which are the nodes in the current cascade graph. Therefore, a straightforward way to represent a graph is to treat it as a bag of nodes. However, this method apparently ignores both local and global structural information in gc, which have been proven to be critical in the prediction of diffusions <ref type="bibr" target="#b10">[10]</ref>. To remedy this issue, we sample from each graph a set of paths, instead of individual nodes. If we make a analogy between nodes and words, paths would be analogous to sentences, cascade graphs to documents, and a set of graphs to a document collection.</p><p>Similar to DeepWalk, the sampling process could be generalized as performing a random walk over a cascade graph gc, the Markov chain of which is shown in Figure <ref type="figure" target="#fig_3">2</ref>. The random walk for each diffusion graph starts from the starting state S, which is always followed by the state N , where the walker transits to a neighbor of the current node. With probability 1 -pj, it goes on walking to the neighbor. With a jumping probability pj, it jumps to an arbitrary node in the cascade graph, leading the walker to the jump state J. With continuation probability po, it walks to a neighbor of the current node, thus going back to state N . With probability 1 -po, it goes to the terminal state T , terminating the entire random walk process. Suppose the walker is at state N in the Markov chain and is currently visiting a node v, it follows a transition probability p(u ∈ Nc(v)|v) to go to one of its outgoing neighbor u ∈ Nc(v), where Nc(v) denotes the set of v's outgoing neighbors in diffusion graph gc. There are multiple strategies for setting transition probabilities. Given a specific choice of scoring function sct(u) to transit to node u, the neighbor u could be sampled in proportion to its score:</p><formula xml:id="formula_3">p(u ∈ Nc(v)|v) = sct(u) + α s∈Nc(v) (sct(s) + α) ,<label>(1)</label></formula><p>where α is a smoother. The scoring function sct(u) could be instantiated by but is not limited to (1) degc(u), the out-degree of node u in gc, (2) degG(u), the degree of u in the global graph G, or (3) weight(v, u), the weight of the edge between the current node v and its neighbor u. Likewise, when the walker is at state J and is to select a node to jump to, the scoring function scj(u) could be set correspondingly:</p><formula xml:id="formula_4">p(u) = scj(u) + α s∈Vc (scj(s) + α) , (<label>2</label></formula><formula xml:id="formula_5">)</formula><p>where Vc is the set of nodes in gc, and scj(u) could be ( <ref type="formula" target="#formula_3">1</ref>) degc(u),</p><p>(2) degG(u), or (3) s∈Nc(u) weight(u, s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Sampling Sequences from a Graph</head><p>The probability po of whether to perform another random jump or to go to the terminal state essentially determines the expected number of sampled sequences, while the probability pj of whether to perform a random jump or to transit to neighbors corresponds to the sequence length. The two factors play a key role in determining the representations of cascade graphs.</p><p>Naturally, different cascade graphs may require different parameters po and pj, as some are intrinsically more complex than others. Instead of fixing or manually tuning these two hyper-parameters, we propose to learn the two probabilities in an end-to-end manner by incorporating them into our deep learning framework. To do this, as Figure <ref type="figure" target="#fig_2">1 (b)</ref> shows, for all cascade graphs we sample a sufficient number of sequences that are long enough. Denote T the sampled sequence length, K the sampled number of sequences, where T and K are the same for all diffusion graphs, we want to learn the actual length tc and the actual number of sequences kc that are actually needed for each graph gc, essentially a different parameterization of po and pj.</p><p>Note that existing work of using random walk paths to represent graphs such as DeepWalk and Node2Vec use fixed, predefined T and K. Automatically learning graph-specific path counts and lengths is a major technical contribution. We leave the learning of tc and kc to the next subsection.</p><p>An example showing how to sample from a graph is displayed in Figure <ref type="figure" target="#fig_2">1</ref> (a) and (b). For each sequence, the starting node is sampled with probability according to Equation 2 without replacement. When all nodes are sampled once as the starting node, a new iteration of sampling starts. Following the starting node, neighbors are sampled iteratively with probability according to Equation <ref type="formula" target="#formula_3">1</ref>. The sampling of one sequence is stopped either when we reach the predefined length T , or when we reach a node without any outgoing neighbors. In this case, sequences with length less than T nodes are padded by a special node '+.' This process of sampling sequences continues until we sample K sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Neural Network Models</head><p>Once we have sampled K sequences with T nodes for each diffusion graph, any effective neural network for sequences could be applied to the random walk paths in a similar way as to text documents. The output of the neural network gives us the hidden representation of individual sequences. Unlike documents whose sentences are already written, we have to learn how to "assemble" these individual sequences into a "document," so that it can best represent the graph and predict its growth.</p><p>Node Embedding. Each node in a sequence is represented as a one-hot vector, q ∈ R N node , where N node is the number of nodes in G. All nodes share an embedding matrix A ∈ R H×N node , which converts a node into its embedding vector x = Aq, x ∈ R H . GRU-based Sequence Encoding. The sampled sequences represent the flow of information of a specific diffusion item. To capture this information flow, we use a Gated Recurrent Unite (GRU) <ref type="bibr" target="#b19">[19]</ref>, a specific type of recurrent neural network (RNN), which is known to be effective for modeling sequences. When applying GRU recursively to a sequence from left to right, the sequence representation will be more and more enriched by information from later nodes in this sequence, with the gating mechanism deciding the amount of new information to be added and the amount of history to be preserved, which simulates the process of information flow during a diffusion. Specifically, denote step i the i-th node in a sequence, for each step i with input node embedding xi ∈ R H and previous hidden state hi-1 ∈ R H as inputs, GRU computes the updated hidden state hi = GRU(xi, hi-1), hi ∈ R H by:</p><formula xml:id="formula_6">ui = σ(W (u) xi + U (u) hi-1 + b (u) ), ri = σ(W (r) xi + U (r) hi-1 + b (r) ), ĥi = tanh(W (h) xi + ri • U (h) hi-1 + b (h) ), hi = ui • ĥi-1 + (1 -ui) • hi-1,<label>(3)</label></formula><p>where σ(.) is the sigmoid activation function, • represents an elementwise product.</p><formula xml:id="formula_7">W (u) , W (r) , W (h) , U (u) , U (r) , U (h) ∈ R H×H and b (u) , b (r) , b (h) ∈ R H are</formula><p>GRU parameters that are learned during training and H is the hidden size.</p><p>For now we have read the sequence from left to right. We could also read the sequence from right to left, so that earlier nodes in the sequence could be informed by which nodes have been affected by a cascading item passed from them. To this end, we adopt the bi-directional GRU, which applies a forward GRU f wd that reads the sequence from left to right, and a backward GRU bwd that reads from right to left. As Figure <ref type="figure" target="#fig_2">1 (c)</ref> shows, the presentation of the i-th node in k-th sequence, ← → h k i ∈ R 2H , is computed as the concatenation of the forward and backward hidden vectors:</p><p>-→</p><formula xml:id="formula_8">h k i = GRU f wd (xi, -→ h k i-1 ), ← - h k i = GRU bwd (xi, ← - h k i+1 ), ← → h k i = -→ h k i ⊕ ← - h k i ,<label>(4)</label></formula><p>where ⊕ denotes the concatenation operation.  From sequence to graph representation. Given a collection of sequence representations, where the k-th sequence with length T</p><formula xml:id="formula_9">is represented as [ ← → h k 1 , ..., ← → h k i , ... , ← → h k T ],</formula><p>as displayed in Figure <ref type="figure" target="#fig_2">1  (d)</ref>, we attempt to learn the representation of the cascade graph as a whole, so that it best predicts its future size. Analogically, we are assembling a document (graph) from a large number of very long sentences. We do this by learning the number of sentences and length of sentences per document, through an attention mechanism.</p><p>In particular, the random walk on a graph terminates with probability 1 -po. From the learning perspective, we could learn the value of po by examining whether the sampled number of sequences could represent the graph well, which in turn decides whether the prediction task is well performed. Intuitively, we could partition the sampled K sequences into "mini-batches." We want to read in more mini-batches until we could learn the graph well, simulating the action of jumping to the terminal state in the random walk. To implement this intuition, we assume a geometric distribution of attentions over mini-batches. If sequences in the first mini-batch of cascade gc share attention weight p c geo , the next mini-batch will have attention (1-p c geo )p c geo , so on and so forth as Figure <ref type="figure" target="#fig_5">3</ref> shows. In theory, if we sample infinite number of sequences with the geometric distribution so that K → ∞, the number of expected minibatches to learn will be 1/p c geo . With this expectation, learning the parameter p c geo could help us decide how many sequences to read in. Note that the degree of freedom is too high if we fit a free parameter p c geo per cascade. Instead, we rely on an observation that the number of sequences we need to represent a cascade graph is correlated with its size. Therefore, we condition p c geo on the size of graph sz(gc), more specifically log 2 (sz(gc) + 1) , where • takes the floor of a floating number. As a result, p c geo is replaced with p log 2 (sz(gc)+1) geo . We could apply similar procedure to learn sequence length. In practice, we found that the standard multinomial distribution of attentions already works well. So we simply assume multinomial distribution λ1, ..., λT over T nodes so that i (λi) = 1, where {λi} are shared across all cascade graphs.</p><p>To sum up and to give a mathematical representation, suppose the mini-batch size is B sequences, then the k-th sequence will fall into the ( k/B + 1)-th mini-batch. The attention mechanism then outputs the representation for graph gc, a vector of length 2H:</p><formula xml:id="formula_10">h(gc) = K k=1 T i=1 (1 -ac) k/B ac λi ← → h t i ,<label>(5)</label></formula><p>where the term in the big parentheses corresponds to the attention over sequences with geometric distribution, and ac = p log 2 (sz(gc)+1) geo . Both ac and λi are learned through the deep learning process.</p><p>Output module. Our output module consists of one fully connected layer with one final output unit: f (gc) = MLP(h(gc)), where MLP stands for a multi-layer perceptron.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENT SETUP</head><p>We present comprehensive empirical experiments using real world data sets to evaluate the performance of DeepCas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Sets</head><p>Most existing work evaluates their methods of predicting diffusions on a single social network data set (e.g., <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b18">18]</ref>). We add another completely different, publicly available data set to demonstrate the effectiveness and generalizability of DeepCas and to allow readers to reproduce our results.</p><p>One of the scenario is the cascade of Tweets on Twitter. Following the practice in existing work <ref type="bibr" target="#b30">[30]</ref>, we collect the TWITTER data set which contains the cascades of Tweets (i.e., through retweeting) in June, 2016 from the official Decahose API (10% sample of the entire Tweet stream). All original English tweets that are published from June 1 to June 15 and retweeted at least once in 10 days are used for training. Those with only one retweet are downsampled to 5%. Cascades originated on June 16 are used for validation, and cascades originated from June 17 to June 20 are used for testing. A cascade contains the authors of the original Tweet and its retweets.</p><p>We construct the global social network G using the same Tweet stream in April and May 2016. As the follower/followee relations are not available in the data and Twitter does not disclose the retweet paths, we follow existing work <ref type="bibr" target="#b30">[30]</ref> and draw an edge from Twitter user A to B if either B retweeted a message of A or A mentioned B in a Tweet. Comparing to a follower/followee network, this network structure accumulates all information cascades and reflects the truly active connections between Twitter users. We weigh an edge based on the number of retweeting/mentioning events between the two users. To construct cascade graphs, we choose t, the duration of cascade since the original Tweet was posted, from a range of t = 1, 3, 5 days. We compute the increment of cascade size after t for the next ∆t days, where ∆t = 1, 3, 5 days. The combination of t and ∆t yields a total of 3 × 3 = 9 configurations.</p><p>In the second scenario, we evaluate the prediction of the cascades of scientific papers. We collect the AMINER data set using the DBLP citation network released by ArnetMiner<ref type="foot" target="#foot_2">2</ref> . We construct the global network G based on citations between 1992 and 2002. That is, an edge draws from node A to B if author A is ever cited by B (which indicates that B might have found a reference from reading A's papers). A cascade of a given paper thus involves all authors who have written or cited that paper. Papers published between 2003 and 2007 are included in the training set. Papers published in 2008 and 2009 are used for validation and testing, respectively. For the earliness and horizon of predictions, we set t = 1, 2, 3 years and ∆t = 1, 2, 3 years respectively.</p><p>In both scenarios, we notice that the growth of all the cascades follows a power-law distribution, where a large number of cascades did not grow at all after t. Therefore we downsample 50% graphs with zero growth (to the numbers shown in Table <ref type="table" target="#tab_0">1</ref>) and apply a logarithm transformation of the outcome variable (increment of cascade size), following existing literature <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b35">35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metric</head><p>We use the mean squared error (MSE) to evaluate the accuracy of predictions, which is a common choice for regression tasks and used in previous work of cascade prediction <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b22">22]</ref>. Denote ŷ a prediction value, and y the ground truth value, the MSE is:</p><formula xml:id="formula_11">MSE = 1 n n i=1 (ŷi -yi) 2<label>(6)</label></formula><p>As noted in Section 3.1, we predict a scaled version of the actual increment of the cascade size, i.e., yi = log 2 (∆si + 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baseline methods</head><p>We compare DeepCas with a set of strong baselines, including feature-based methods used for cascade prediction, methods based on nodes embeddings, and alternative deep learning methods to learn graph representations.</p><p>Features-*. We include all structural features that could be generalized across data sets from recent studies of cascade prediction <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b36">36]</ref>. These features include: Centrality and Density. Degree of nodes in the cascade graph g and the global network G, the mean and the 90th percentile of the local and global degrees of nodes in g, number of leaf nodes in g, edge density of g, and the number of nodes and edges in the frontier graph of the cascade, which is composed of nodes that are not in g but are neighbors of nodes in g.</p><p>Node Identity. The presence of node ids in g is used as features.</p><p>Communities. From both the cascade graph and the frontier graph, we compute the number of communities <ref type="bibr" target="#b6">[6]</ref>, the overlap of communities, and Gini impurity of communities <ref type="bibr" target="#b18">[18]</ref>.</p><p>Substructures. We count the frequency of k-node substructures (k ≤ 4) <ref type="bibr" target="#b36">[36]</ref>. These include nodes (k = 1), edges (k = 2), triads (e.g., the number of closed and open triangles) and quads from both the cascade graph and the frontier graph.</p><p>*-linear and *-deep. Once the cascade is represented as a set of features above, they are blended together using linear regression (denoted as Features-linear) with L2 regularization, as other linear regressors such as SVR empirically perform worse on our task. To obtain an even stronger baseline, we feed the feature vectors to a multi-layer perceptron (MLP), denoted as Features-deep.</p><p>OSLOR selects important nodes as sensors, and predict the outbreaks based on the cascading behaviors of these sensors <ref type="bibr" target="#b11">[11]</ref>.</p><p>Node2vec <ref type="bibr" target="#b16">[16]</ref> is selected as a representative of node embedding methods. Node2vec is a generalization of DeepWalk <ref type="bibr" target="#b29">[29]</ref>, which is reported to be outperforming alternative methods such as Deep-Walk and LINE <ref type="bibr" target="#b34">[34]</ref>. We generate walks from two sources: (1) the set of cascade graphs {g} (2) the global network G. The two sources lead to two embedding vectors per node, which are concatenated to form the final embedding of each node. The average of embeddings of all nodes in a cascade graph is fed through MLP to make the prediction.</p><p>Embedded-IC <ref type="bibr" target="#b8">[8]</ref> represents nodes by two types of embeddings: as a sender or as a receiver. For prediction, the original paper used Monte-Carlo simulations to estimate infections probabilities of each individual user. To predict cascade size, we experiment with two settings: (1) learn a linear mapping function between the number of infected users and the cascade size; (2) follow the setting of Node2Vec by using the average of embeddings of all nodes in the cascade graph, which is then piped through MLP. We find that the second setting empirically performs better than the first one. We therefore report the performance of the latter.</p><p>PSCN applies convolutional neural networks (CNN) to locally connected regions from graphs <ref type="bibr" target="#b28">[28]</ref>. We apply PSCN to both the cascade graphs and the frontier graphs. The last hidden layer of the cascade graph and that of the frontier graph are concatenated to make the final prediction.</p><p>Graph kernels. There are a set of state-of-the-art graph kernels <ref type="bibr" target="#b28">[28]</ref>: the shortest-path kernel (SP) <ref type="bibr">[7]</ref>, the random walk kernel (RW) <ref type="bibr" target="#b15">[15]</ref>, and the Weisfeiler-Lehman subtree kernel (WL) <ref type="bibr" target="#b32">[32]</ref>. The RW kernel and the SP kernel are too computationally expensive, which did not complete after 10 days for a single data set in our experiment. We therefore exclude them from the comparison, a decision also made in <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b41">41]</ref>. For the WL kernel, we experiment with two settings: WL-degree, where node degree is used as the node attribute to build subgraphs for each cascade graph and frontier graph; WL-id, where node id is used as the attribute. The second setting is to test whether node identity information could be incorporated into graph kernel methods.</p><p>Hyper-parameters. All together we have 8 baselines. All their hyper-parameters are tuned to obtain the best results on validation set for each configuration (9 in total) of each data set. For linear regressions, we chose the L2-coefficient from {1, 0.5, 0.1, 0.05, ..., 10 -8 }. For neural network regression, the initial learning rate is selected from {0.1, 0.05, 0.01, ..., 10 -4 }, the number of hidden layers from {1, 2, ..., 4}, the hidden layer size from {32, 64, ..., 1024}, and L1-and L2-coefficient both from {1, 0.5, 0.1, 0.05, ..., 10 -8 }. Following <ref type="bibr" target="#b28">[28]</ref> for PSCN, the width is set to the average number of nodes, and the receptive field size is chosen between 5 and 10. The height parameter of WL is chosen from {2, 3, 4}. The candidate embedding size set is selected from {50, 100, 200, 300} for all methods that learn embeddings for nodes. For node2vec, we follow <ref type="bibr" target="#b16">[16]</ref>, p, q are selected from {0.25, 0.50, 1, 2, 4}, the length of walk is chosen from {10, 25, 50, 75, 100}, and the number of walks per node is chosen from {5, 10, 15, 20}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">DeepCas and the Variants</head><p>We compare a few variants of DeepCas with the 8 baselines. We sample K = 200 paths each with length T = 10 from the cascade graph without tuning these parameters. As described in Section 3.4 and 3.5, the attention mechanism will automatically decide when and where to stop using the sequences. The minibatch size is set to 5. The smoother α is set to 0.01. The embedding sizes for the TWITTER and AMINER data set are set to 100 and 50 respectively. The embeddings are initialized by concatenating embedding learned by Node2Vec from both all cascade graphs {g} in training set and the global network G. The node2vec hyperparameters p and q are set to 1.</p><p>We use DeepCas-edge, DeepCas-deg, and DeepCas-DEG to denote three version of DeepCas, which randomly walk with transition probabilities proportional to edge weights, node degree in the cascade graph, and node degree in the global network. For comparison, we also include three simplified versions of DeepCas:</p><p>GRU-bag represents a cascade graph as a bag of nodes and feeds them through our GRU model. This is similar to setting the length of random walk paths to 1, which examines whether sequential information is important for cascade prediction.</p><p>GRU-fixed uses a fixed path length t and a fixed number of sequences k, without using the attention mechanism to learn them adaptively. Hyper-parameters t and k are tuned to optimal on the validation sets, the values of which are selected from {2, 3, 5, 7, 10} and from {50, 100, 150, 200}, respectively.</p><p>GRU-root uses the attention mechanism, but starts sampling a random walk path only from the root of the cascade, which are nodes who started the diffusion. If there are multiple roots, we take turns to sample from them. This examines whether it is important to perform random jumps in the walks over the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENT RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Overall performance</head><p>The overall performance of all competing methods across data sets is displayed in Table <ref type="table" target="#tab_2">2</ref>. The last three rows of each table show the performance of the complete versions of our methods, which outperform all eight baseline methods with a statistically significant drop of MSE. Please note that the numbers in Table <ref type="table" target="#tab_2">2</ref> are errors of log-transformed outcomes. If we translate them back to raw sizes, the numerical differences between the methods would look larger.</p><p>The difference between Features-deep and Features-linear is intriguing, which shows that deep learning does not always perform better than linear methods if we have already found a set of good features. The benefit of deep learning really comes from end-to-end learning from the data to the predictions.</p><p>Node2Vec and Embedded-IC do not perform well in cascade prediction. Taking the average of node embeddings as the graph representation is not as informative as representing the graph as a set of paths, even if the node embeddings are also fed into a deep neural net to make predictions. By comparing WL-degree and WLid, we can see that it is hard for graph kernels to incorporate node identities. Simply using identities as node labels degenerates performance. This is because graph kernels rely on node labels to compute similarity between graphs. Using node ids to measure similarity could cause serious sparsity problem.</p><p>The three simplified versions of DeepCas, GRU-bag, GRU-fixed, and GRU-root all lead to certain degradation of performance, comparing to the three DeepCas models. This empirically proves the effectiveness of the three important components of DeepCas. First, sampling a set of paths to represent a graph instead of averaging the representations of nodes is critical, as it facilitates the learning of structural information. Second, learning the random walks by adaptively deciding when to stop sampling from a particular path and when to stop sampling more paths is more effective than using a fixed number of fixed-length paths (which is what DeepWalk does). The suitable numbers and lengths might be associated with the complexity and the influence power of a cascade graph. If a cascade graph is more complex and more "influential," it needs more and longer paths to represent its power. Third, sampling paths only from the root is not adequate (which is what most generative models do). Randomly jumping to other nodes could make the graph representation carry more information of the cascade structure and handle missing data better. In a way, this is related to the "mysterious" feature used in Cheng et al. <ref type="bibr" target="#b10">[10]</ref>, i.e., whether some early adopters are not directly connected to the root.</p><p>Comparing the performance of using different t and ∆t, we see a general pattern that can be applied to all methods: the larger the earliness t is, the easier to make a good prediction. This is because longer t makes more information available. While for ∆t, it is the opposite, as it is always harder to make long-term predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Computational Cost</head><p>Training DeepCas is quite efficient. On a machine with 2.40 GHz CPU, 120G RAM and a single Titan X GPU, it takes less than 20 minutes to generate random walk paths for a complete data set and less than 10 minutes for the DeepCas model to converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Error Analysis</head><p>We also investigate cascades for which DeepCas makes more mistakes than the baselines, and also the other way around. Here we use the strongest baseline, Features-linear as our reference. The procedure is as follows: among graphs where DeepCas has smaller MSE than the baseline (and the other way around), we select the top 100 with the largest MSE differences between the two methods. For these top graphs, we compute the average of certain network properties of the cascade graphs. This average is taken across configurations, as we do not observe significant differences between configurations. As Table <ref type="table" target="#tab_1">3</ref> shows, DeepCas tends to perform better on larger and denser graphs. These structures are more complex and harder to be represented as a bag of hand-crafted features. An end-to-end predictor without explicit feature design works very well in these cases. On the other hand, both methods perform reasonably well on smaller graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Interpreting the Representations</head><p>We have empirically shown that DeepCas could learn the representation of cascade graphs that incorporates both structures and node identities. Qualitatively, we have not assessed what the learned representation actually captures from these information. Indeed, one concern of applying deep learning to particular domains is that the models are black-boxes and not easy to interpret. For us, it is intriguing to know whether the learned representation corresponds to well-known network properties and structural patterns in literature.</p><p>To do this, we select a few hand-crafted features which are computed for the feature based baselines. These features characterize either global or local network properties, and are listed in Figure <ref type="figure" target="#fig_7">4</ref>. In each subfigure, we layout the cascade graphs as data points in the test set to a 2-D space by feeding their vector representations output by the last hidden layer of DeepCas to t-SNE <ref type="bibr" target="#b6">[6]</ref>, a commonly used visualization algorithm. Cascade graphs with similar vector representations are placed closely. To connect the hand-crafted features with the learned representations, we color each cascade graph (a point in the 2-D visualization) by the values of each feature (e.g., network density). If we eyeball a pattern of the distribution of colors in this 2-D layout, it suggests a connection between the learned representation and that network property. We also color the layout by the ground-truth labels (increment of cascade size). If the color distribution of labels looks somewhat correlated with the color dis- tribution of a network property, we know this property attributes to cascade prediction, although not through a hand-crafted feature.</p><p>As we observe, DeepCas could capture structural properties like the number of open, closed triangles, and the number of communities. For example, in the Figure <ref type="figure" target="#fig_7">4</ref> (e), the points (cascade graphs) clustered to the bottom right have the fewest communities, while graphs in the top left have the most. Cascade graph with a larger number of communities implies that many early adopters may lie in between bigger communities, which are likely to be structural holes in the global network. In literature <ref type="bibr" target="#b9">[9]</ref>, nodes spanning structural holes are likely to gain social capital, promoting the growth of its ego-net. Indeed, when we compare the color scheme of 4(g) with 4(i), we can see that the number of communities in a cascade graph is indeed positively correlated with its growth.</p><p>Figure <ref type="figure" target="#fig_7">4</ref> (f) plots the average global degree of nodes in each cascade graph. The pattern suggests that DeepCas not only captures the structural information from individual cascade graphs, but also incorporates the global information into the graph representation. How did this happen? Although we did not explicitly represent the global network G (or the frontier graphs), DeepCas is likely to learn useful global network information from the many cascade graphs in training (similar to a model that captures collection-level informa-tion from the input of many individual documents), and incorporate it into the high-level representation of a cascade graph. Some additional observations can be made from Figure <ref type="figure" target="#fig_7">4</ref>. First, as the number of open and closed triangles are actually important features used for graph prediction tasks <ref type="bibr" target="#b36">[36]</ref>, we can see that Deep-Cas has automatically learned these useful features without human input. Second, since edge density is a function of the number of edges and nodes, DeepCas learns not only the number of edges and nodes (we do not show the node property in Figure <ref type="figure" target="#fig_7">4</ref>, but this is true), but also their none-linear relationship that involves division.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">DISCUSSION AND CONCLUSION</head><p>We present the first end-to-end, deep learning based predictor of information cascades. A cascade graph is first represented as a set of random walk paths, which are piped through a carefully designed GRU neural network structure and an attention mechanism to predict the future size of the cascade. The end-to-end predictor, Deep-Cas, outperforms feature-based machine learning methods and alternative node embedding and graph embedding methods.</p><p>While the study adds another evidence to the recent successes of deep learning in a new application domain, social networks, we do wish to point the readers to a few more interesting implications. First, we find that linearly combined, hand-crafted features per-  form reasonably well in cascade prediction, which outperform a series of node embedding, graph embedding, and suboptimal deep learning methods. Comparing to other data mining domains, social network is a field where there exists rich theoretical and empirical domain knowledge. Carefully designed features inherited from the literature are already very powerful in capturing the critical properties of networks. The benefit of deep learning in this case really comes from the end-to-end procedure, which is likely to have learned high-level features that just better represent these network properties. Comparing to deep learning methods, featurebased methods do have their advantages (if the right features are identified), as both the results and the importance of features are easier to interpret. For social network researchers, it is perhaps a good idea to interpret DeepCas as a way to test the potential room to improve cascade prediction, instead of as a complete overturn of the existing practice. Indeed, it is intriguing to pursue how to design better measurements of the classical network concepts (e.g., communities and centrality), based on the results of DeepCas. Another interesting finding is that different random walk strategies perform better and worse in different scenarios, and all better than bag of node embeddings. This is where prior knowledge in social networks literature may kick in, by incorporating various contagion/diffusion processes to generate initial representations of cascade networks.</p><p>Finally, to make our conclusion clean and generalizable, we only utilized the network structure and node identities in the prediction. It is interesting to incorporate DeepCas with other types of information when they are available, e.g., content and time series, to optimize the prediction accuracy on a particular domain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>c 2017 International World Wide Web Conference Committee (IW3C2), published under Creative Commons CC BY 4.0 License. WWW 2017, April 3-7, 2017, Perth, Australia. ACM 978-1-4503-4913-0/17/04. http://dx.doi.org/10.1145/3038912.3052643 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The end-to-end pipeline of DeepCas.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The Markov chain of random walk.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Attention to assemble the representation of the graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) # closed triangles. (b) # open triangles. High Low (e) # communities. (d) # communities. (c) Edge density. (f) Avg. degree in G. (g) # leaf nodes. (h) # edges.(i) Increment of diffusion size on TWITTER.(j) Increment of diffusion size on AMINER.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Feature visualization. Every point is a cascade graph in test set. Every layout is colored (red: high, blue: low) using hand-crafted network properties or the ground-truth, labeled under each subfigures. The left column displays graphs from TWITTER, while the right column shows AMINER.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the data sets.Avg. scaled growth scales the actual increment of the cascade size s to log 2 (∆s + 1) ∆t 1 , ∆t 2 , ∆t 3 are respectively 1,3,5 days for Tweets, and 1,2,3 years for papers.</figDesc><table><row><cell></cell><cell>Set</cell><cell></cell><cell>TWITTER</cell><cell></cell><cell></cell><cell>AMINER</cell><cell></cell></row><row><cell cols="2"># nodes in G All</cell><cell></cell><cell>354,634</cell><cell></cell><cell></cell><cell>131,415</cell><cell></cell></row><row><cell cols="2"># edges in G All</cell><cell></cell><cell>27,929,863</cell><cell></cell><cell></cell><cell>842,542</cell><cell></cell></row><row><cell>t</cell><cell></cell><cell cols="6">1 day 3 days 5 days 1 year 2 years 3 years</cell></row><row><cell></cell><cell cols="7">train 25,720 26,621 26,871 3,044 17,023 34,347</cell></row><row><cell># cascades</cell><cell cols="4">val 1,540 1,563 1,574</cell><cell>509</cell><cell cols="2">3,665 7,428</cell></row><row><cell></cell><cell cols="4">test 6,574 6,656 6,663</cell><cell>517</cell><cell cols="2">3,512 7,337</cell></row><row><cell></cell><cell cols="2">train 26.2</cell><cell>34.9</cell><cell>39.1</cell><cell>16.4</cell><cell>16.8</cell><cell>19.7</cell></row><row><cell cols="2">Avg. nodes val</cell><cell>46.1</cell><cell>62.1</cell><cell>69.7</cell><cell>10.6</cell><cell>13.6</cell><cell>17.2</cell></row><row><cell>per gc</cell><cell>test</cell><cell>50.8</cell><cell>65.8</cell><cell>72.8</cell><cell>8.8</cell><cell>12.6</cell><cell>16.2</cell></row><row><cell></cell><cell cols="2">train 99.0</cell><cell cols="2">153.8 188.3</cell><cell>56.8</cell><cell>54.9</cell><cell>68.5</cell></row><row><cell cols="5">Avg. edges val 167.0 241.4 296.5</cell><cell>29.5</cell><cell>40.9</cell><cell>55.3</cell></row><row><cell>per gc</cell><cell cols="4">test 162.3 242.2 289.0</cell><cell>22.6</cell><cell>32.9</cell><cell>44.5</cell></row><row><cell></cell><cell>train</cell><cell>1.1</cell><cell>0.6</cell><cell>0.5</cell><cell>1.8</cell><cell>2.2</cell><cell>2.0</cell></row><row><cell cols="2">Avg. scaled val</cell><cell>1.4</cell><cell>0.9</cell><cell>0.7</cell><cell>1.9</cell><cell>2.0</cell><cell>1.8</cell></row><row><cell cols="2">growth ∆t 1 test</cell><cell>1.3</cell><cell>0.8</cell><cell>0.7</cell><cell>1.7</cell><cell>2.0</cell><cell>1.8</cell></row><row><cell></cell><cell>train</cell><cell>1.6</cell><cell>1.2</cell><cell>1.1</cell><cell>2.5</cell><cell>3.0</cell><cell>2.8</cell></row><row><cell cols="2">Avg. scaled val</cell><cell>2.1</cell><cell>1.6</cell><cell>1.3</cell><cell>2.7</cell><cell>2.8</cell><cell>2.6</cell></row><row><cell cols="2">growth ∆t 2 test</cell><cell>1.9</cell><cell>1.4</cell><cell>1.3</cell><cell>2.4</cell><cell>2.8</cell><cell>2.5</cell></row><row><cell></cell><cell>train</cell><cell>1.9</cell><cell>1.5</cell><cell>1.3</cell><cell>3.0</cell><cell>3.5</cell><cell>3.2</cell></row><row><cell cols="2">Avg. scaled val</cell><cell>2.4</cell><cell>1.9</cell><cell>1.7</cell><cell>3.1</cell><cell>3.3</cell><cell>3.0</cell></row><row><cell cols="2">growth ∆t 3 test</cell><cell>2.2</cell><cell>1.8</cell><cell>1.6</cell><cell>2.9</cell><cell>3.2</cell><cell>2.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Statistics of graphs for which DeepCas outperformsFeatures-linear (first rows), and vise versa (second rows). The third row of each data set shows the average statistics on its test set.</figDesc><table><row><cell>Data set</cell><cell>Method</cell><cell cols="3"># nodes # edges Avg. degree Edge density</cell></row><row><cell></cell><cell>DeepCas</cell><cell>113.27 689.81</cell><cell>6.09</cell><cell>0.36</cell></row><row><cell cols="3">TWITTER Features-linear 89.94 455.78</cell><cell>4.22</cell><cell>0.25</cell></row><row><cell></cell><cell>Avg.</cell><cell>63.16 231.26</cell><cell>3.18</cell><cell>0.30</cell></row><row><cell></cell><cell>DeepCas</cell><cell>13.89 36.97</cell><cell>2.25</cell><cell>0.48</cell></row><row><cell cols="3">AMINER Features-linear 12.72 35.51</cell><cell>2.23</cell><cell>0.48</cell></row><row><cell></cell><cell>Avg.</cell><cell>12.53 33.35</cell><cell>2.21</cell><cell>0.49</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance measured by MSE (the lower the better), where original label ∆s is scaled to y = log 2 (∆s + 1). " means the result is significantly better or worse over Features-deep according to paired t-test test at level 0.01(0.1).</figDesc><table><row><cell>(a) TWITTER</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/chengli-um/DeepCas, retrieved onFebruary 19</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2017" xml:id="foot_1"><p>, 2017.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>https://aminer.org/citation, DBLP-Citationnetwork V8, retrieved in August 2016.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work is partially supported by the National Science Foundation under grant numbers IIS-1054199 and SES-1131500.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A simple model of herd behavior</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Economics</title>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">How viral are viral videos</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hadiji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICWSM</title>
		<meeting>of ICWSM</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mathematical models of fads explain the temporal dynamics of internet memes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hadiji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICWSM</title>
		<meeting>of ICWSM</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A theory of fads, fashion, custom, and cultural change as informational cascades</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bikhchandani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hirshleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Welch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of political Economy</title>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast unfolding of communities in large networks</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lambiotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lefebvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JSTAT</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Shortest-path kernels on graphs</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICDM</title>
		<meeting>of ICDM</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Representation learning for information diffusion through social networks: an embedded cascade model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bourigault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lamprier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WSDM</title>
		<meeting>of WSDM</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The network structure of social capital</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Burt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research in organizational behavior</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Can cascades be predicted?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Adamic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Dow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cascading outbreak prediction in networks: a data-driven approach</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGKDD</title>
		<meeting>of SIGKDD</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.09375</idno>
		<title level="m">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Easley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<title level="m">Networks, crowds, and markets</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Community detection in graphs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fortunato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics reports</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On graph kernels: Hardness results and efficient alternatives</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning Theory and Kernel Machines</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGKDD</title>
		<meeting>of SIGKDD</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A predictive model for the temporal dynamics of information diffusion in online social networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Guille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hacid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Toward order-of-magnitude cascade prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shaabani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shakarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASONAM</title>
		<meeting>of ASONAM</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Analyzing and predicting viral tweets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jenders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Naumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Prediction of retweet cascade size over time</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kupavskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ostroumova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Umnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Usachev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Serdyukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gusev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kustarev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CIKM</title>
		<meeting>of CIKM</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Information contagion: An empirical study of the spread of news on digg and twitter social networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICWSM</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="90" to="97" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><surname>Deepgraph</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.06251</idno>
		<title level="m">Graph structure predicts network growth</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m">Efficient estimation of word representations in vector space</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">subgraph2vec: Learning distributed representations of rooted sub-graphs from large graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandramohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saminathan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08928</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">subgraph2vec: Learning distributed representations of rooted sub-graphs from large graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandramohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saminathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Mining and Learning with Graphs</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGKDD</title>
		<meeting>of SIGKDD</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Truthy: mapping the spread of astroturf in microblog streams</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ratkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Conover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gonçalves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Flammini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Menczer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barabási</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.0778</idno>
		<title level="m">Modeling and predicting popularity dynamics via reinforced poisson processes</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The effect of wording on message propagation: Topic-and author-controlled natural experiments on twitter</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.1438</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">What&apos;s in a hashtag?: content based prediction of the spread of ideas in microblogging communities</title>
		<author>
			<persName><forename type="first">O</forename><surname>Tsur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WSDM</title>
		<meeting>of WSDM</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Structural diversity in social contagion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ugander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Backstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Marlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PNAS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Whom to mention: expand the diffusion of tweets by @ recommendation on micro-blogging systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd International World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1331" to="1340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sequential sales, learning, and cascades</title>
		<author>
			<persName><forename type="first">I</forename><surname>Welch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of finance</title>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Menczer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Ahn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1403.6199</idno>
		<title level="m">Predicting successful memes using network and community structure</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGKDD</title>
		<meeting>of SIGKDD</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A structural smoothing framework for robust graph comparison</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rain: Social role-aware information diffusion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>-K. Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">From micro to macro: Uncovering and predicting information cascading process with behavioral dynamics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICDM</title>
		<meeting>of ICDM</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Seismic: A self-exciting point process model for predicting tweet popularity</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Erdogdu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rajaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGKDD</title>
		<meeting>of SIGKDD</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
