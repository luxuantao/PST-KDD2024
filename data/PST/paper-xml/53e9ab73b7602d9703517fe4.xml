<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Active Ranking using Pairwise Comparisons</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kevin</forename><forename type="middle">G</forename><surname>Jamieson</surname></persName>
							<email>kgjamieson@wisc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Wisconsin Madison</orgName>
								<address>
									<postCode>53706</postCode>
									<region>WI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><forename type="middle">D</forename><surname>Nowak</surname></persName>
							<email>nowak@engr.wisc.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Wisconsin Madison</orgName>
								<address>
									<postCode>53706</postCode>
									<region>WI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Active Ranking using Pairwise Comparisons</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B9ACC04B1473A9632542042AD5197E8F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper examines the problem of ranking a collection of objects using pairwise comparisons (rankings of two objects). In general, the ranking of n objects can be identified by standard sorting methods using n log 2 n pairwise comparisons. We are interested in natural situations in which relationships among the objects may allow for ranking using far fewer pairwise comparisons. Specifically, we assume that the objects can be embedded into a d-dimensional Euclidean space and that the rankings reflect their relative distances from a common reference point in R d . We show that under this assumption the number of possible rankings grows like n 2d and demonstrate an algorithm that can identify a randomly selected ranking using just slightly more than d log n adaptively selected pairwise comparisons, on average. If instead the comparisons are chosen at random, then almost all pairwise comparisons must be made in order to identify any ranking. In addition, we propose a robust, error-tolerant algorithm that only requires that the pairwise comparisons are probably correct. Experimental studies with synthetic and real datasets support the conclusions of our theoretical analysis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper addresses the problem of ranking a set of objects based on a limited number of pairwise comparisons (rankings between pairs of the objects). A ranking over a set of n objects Θ = (θ 1 , θ 2 , . . . , θ n ) is a mapping σ : {1, . . . , n} → {1, . . . , n} that prescribes an order</p><formula xml:id="formula_0">σ(Θ) := θ σ(1) ≺ θ σ(2) ≺ • • • ≺ θ σ(n-1) ≺ θ σ(n)<label>(1)</label></formula><p>where θ i ≺ θ j means θ i precedes θ j in the ranking. A ranking uniquely determines the collection of pairwise comparisons between all pairs of objects. The primary objective here is to bound the number of pairwise comparisons needed to correctly determine the ranking when the objects (and hence rankings) satisfy certain known structural constraints. Specifically, we suppose that the objects may be embedded into a low-dimensional Euclidean space such that the ranking is consistent with distances in the space. We wish to exploit such structure in order to discover the ranking using a very small number of pairwise comparisons. To the best of our knowledge, this is a previously open and unsolved problem.</p><p>There are practical and theoretical motivations for restricting our attention to pairwise rankings that are discussed in Section 2. We begin by assuming that every pairwise comparison is consistent with an unknown ranking. Each pairwise comparison can be viewed as a query: is θ i before θ j ? Each query provides 1 bit of information about the underlying ranking. Since the number of rankings is n!, in general, specifying a ranking requires Θ(n log n) bits of information. This implies that at least this many pairwise comparisons are required without additional assumptions about the ranking. In fact, this lower bound can be achieved with a standard adaptive sorting algorithm like binary sort <ref type="bibr" target="#b0">[1]</ref>. In large-scale problems or when humans are queried for pairwise comparisons, obtaining this many pairwise comparisons may be impractical and therefore we consider situations in which the space of rankings is structured and thereby less complex.</p><p>A natural way to induce a structure on the space of rankings is to suppose that the objects can be embedded into a d-dimensional Euclidean space so that the distances between objects are consistent with the ranking. This may be a reasonable assumption in many applications, and for instance the audio dataset used in our experiments is believed to have a 2 or 3 dimensional embedding <ref type="bibr" target="#b1">[2]</ref>. We further discuss motivations for this assumption in Section 2. It is not difficult to show (see Section 3) that the number of full rankings that could arise from n objects embedded in R d grows like n 2d , and so specifying a ranking from this class requires only O(d log n) bits. The main results of the paper show that under this assumption a randomly selected ranking can be determined using O(d log n) pairwise comparisons selected in an adaptive and sequential fashion, but almost all n 2 pairwise rankings are needed if they are picked randomly rather than selectively. In other words, actively selecting the most informative queries has a tremendous impact on the complexity of learning the correct ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Problem statement</head><p>Let σ denote the ranking to be learned. The objective is to learn the ranking by querying the reference for pairwise comparisons of the form q i,j := {θ i ≺ θ j }.</p><p>(</p><formula xml:id="formula_1">)<label>2</label></formula><p>The response or label of q i,j is binary and denoted as y i,j := 1{q i,j } where 1 is the indicator function; ties are not allowed. The main results quantify the minimum number of queries or labels required to determine the reference's ranking, and they are based on two key assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1 Embedding:</head><p>The set of n objects are embedded in R d (in general position) and we will also use θ 1 , . . . , θ n to refer to their (known) locations in R d . Every ranking σ can be specified by a reference point r σ ∈ R d , as follows. The Euclidean distances between the reference and objects are consistent with the ranking in the following sense: if the σ ranks θ i ≺ θ j , then θ ir σ &lt; θ jr σ . Let Σ n,d denote the set of all possible rankings of the n objects that satisfy this embedding condition.</p><p>The interpretation of this assumption is that we know how the objects are related (in the embedding), which limits the space of possible rankings. The ranking to be learned, specified by the reference (e.g., preferences of a human subject), is unknown. Many have studied the problem of finding an embedding of objects from data <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. This is not the focus here, but it could certainly play a supporting role in our methodology (e.g., the embedding could be determined from known similarities between the n objects, as is done in our experiments with the audio dataset). We assume the embedding is given and our interest is minimizing the number of queries needed to learn the ranking, and for this we require a second assumption.</p><p>A2 Consistency: Every pairwise comparison is consistent with the ranking to be learned. That is, if the reference ranks θ i ≺ θ j , then θ i must precede θ j in the (full) ranking.</p><p>As we will discuss later in Section 3.2, these two assumptions alone are not enough to rule out pathological arrangements of objects in the embedding for which at least Ω(n) queries must be made to recover the ranking. However, because such situations are not representative of what is typically encountered, we analyze the problem in the framework of the average-case analysis <ref type="bibr" target="#b5">[6]</ref>.</p><p>Definition 1. With each ranking σ ∈ Σ n,d we associate a probability π σ such that σ∈Σ n,d π σ = 1. Let π denote these probabilities and write σ ∼ π for shorthand. The uniform distribution corresponds to π σ = |Σ n,d | -1 for all σ ∈ Σ n,d , and we write σ ∼ U for this special case. Definition 2. If M n (σ) denotes the number of pairwise comparisons requested by an algorithm to identify the ranking σ, then the average query complexity with respect to π is denoted by</p><formula xml:id="formula_2">E π [M n ].</formula><p>The main results are proven for the special case of π = U , the uniform distribution, to make the analysis more transparent and intuitive. However the results can easily be extended to general distributions π that satisfy certain mild conditions <ref type="bibr" target="#b6">[7]</ref>. All results henceforth, unless otherwise noted, will be given in terms of (uniform) average query complexity and we will say such results hold "on average."</p><p>Our main results can be summarized as follows. If the queries are chosen deterministically or randomly in advance of collecting the corresponding pairwise comparisons, then we show that almost all n 2 pairwise comparisons queries are needed to identify a ranking under the assumptions above. However, if the queries are selected in an adaptive and sequential fashion according to the algorithm Query Selection Algorithm input: n objects in R d initialize: objects θ 1 , . . . , θ n in uniformly random order for j=2,. . . ,n for i=1,. . . ,j-1 if q i,j is ambiguous, request q i,j 's label from reference; else impute q i,j 's label from previously labeled queries. output: ranking of n objects  </p><formula xml:id="formula_3">1 θ 2 θ 3 q 1 , 2 q1 , 3 q 2,3</formula><p>Figure <ref type="figure" target="#fig_4">2</ref>: Objects θ 1 , θ 2 , θ 3 and queries. The r σ lies in the shaded region (consistent with the labels of q 1,2 , q 1,3 , q 2,3 ). The dotted (dashed) lines represent new queries whose labels are (are not) ambiguous given those labels.</p><p>in Figure <ref type="figure" target="#fig_0">1</ref>, then we show that the number of pairwise rankings required to identify a ranking is no more than a constant multiple of d log n, on average. The algorithm requests a query if and only if the corresponding pairwise ranking is ambiguous (see Section 4.2), meaning that it cannot be determined from previously collected pairwise comparisons and the locations of the objects in R d . The efficiency of the algorithm is due to the fact that most of the queries are unambiguous when considered in a sequential fashion. For this very same reason, picking queries in a non-adaptive or random fashion is very inefficient. It is also noteworthy that the algorithm is also computationally efficient with an overall complexity no greater than O(n poly(d) poly(log n)) <ref type="bibr" target="#b6">[7]</ref>. In Section 5 we present a robust version of the algorithm of Figure <ref type="figure" target="#fig_0">1</ref> that is tolerant to a fraction of errors in the pairwise comparison queries. In the case of persistent errors (see <ref type="bibr">Section 5)</ref> we show that at least O(n/ log n) objects can be correctly ranked in a partial ranking with high probability by requesting just O(d log 2 n) pairwise comparisons. This allows us to handle situations in which either or both of the assumptions, A1 and A2, are reasonable approximations to the situation at hand, but do not hold strictly (which is the case in our experiments with the audio dataset).</p><p>Proving the main results involves an uncommon marriage of ideas from the ranking and statistical learning literatures. Geometrical interpretations of our problem derive from the seminal works of <ref type="bibr" target="#b7">[8]</ref> in ranking and <ref type="bibr" target="#b8">[9]</ref> in learning. From this perspective our problem bears a strong resemblance to the halfspace learning problem, with two crucial distinctions. In the ranking problem, the underlying halfspaces are not in general position and have strong dependencies with each other. These dependencies invalidate many of the typical analyses of such problems <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. One popular method of analysis in exact learning involves the use of something called the extended teaching dimension <ref type="bibr" target="#b11">[12]</ref>. However, because of the possible pathological situations alluded to earlier, it is easy to show that the extended teaching dimension must be at least Ω(n) making that sort of worst-case analysis uninteresting. These differences present unique challenges to learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation and related work</head><p>The problem of learning a ranking from few pairwise comparisons is motivated by what we perceive as a significant gap in the theory of ranking and permutation learning. Most work in ranking assumes a passive approach to learning; pairwise comparisons or partial rankings are collected in a random or non-adaptive fashion and then aggregated to obtain a full ranking (cf. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>). However, this may be quite inefficient in terms of the number of pairwise comparisons or partial rankings needed to learn the (full) ranking. This inefficiency was recently noted in the related area of social choice theory <ref type="bibr" target="#b16">[17]</ref>. Furthermore, empirical evidence suggests that, even under complex ranking models, adaptively selecting pairwise comparisons can reduce the number needed to learn the ranking <ref type="bibr" target="#b17">[18]</ref>. It is cause for concern since in many applications it is expensive and time-consuming to obtain pairwise comparisons. For example, psychologists and market researchers collect pairwise comparisons to gauge human preferences over a set of objects, for scientific understanding or product placement. The scope of these experiments is often very limited simply due to the time and expense required to collect the data. This suggests the consideration of more selective and judicious approaches to gathering inputs for ranking. We are interested in taking advantage of underlying structure in the set of objects in order to choose more informative pairwise comparison queries. From a learning perspective, our work adds an active learning component to a problem domain that has primarily been treated from a passive learning mindset.</p><p>We focus on pairwise comparison queries for two reasons. First, pairwise comparisons admit a halfspace representation in embedding spaces which allows for a geometrical approach to learning in such structured ranking spaces. Second, pairwise comparisons are the most common form of queries in many applications, especially those involving human subjects. For example, consider the problem of finding the most highly ranked object, as illustrated by the following familiar task. Suppose a patient needs a new pair of prescription eye lenses. Faced with literally millions of possible prescriptions, the doctor will present candidate prescriptions in a sequential fashion followed by the query: better or worse? Even if certain queries are repeated to account for possible inaccurate answers, the doctor can locate an accurate prescription with just a handful of queries. This is possible presumably because the doctor understands (at least intuitively) the intrinsic space of prescriptions and can efficiently search through it using only binary responses from the patient.</p><p>We assume that the objects can be embedded in R d and that the distances between objects and the reference are consistent with the ranking (Assumption A1). The problem of learning a general function f : R d → R using just pairwise comparisons that correctly ranks the objects embedded in R d has previously been studied in the passive setting <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. The main contributions of this paper are theoretical bounds for the specific case when f (x) = ||xr σ || where r σ ∈ R d is the reference point. This is a standard model used in multidimensional unfolding and psychometrics <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref>. We are unaware of any existing query-complexity bounds for this problem. We do not assume a generative model is responsible for the relationship between rankings to embeddings, but one could. For example, the objects might have an embedding (in a feature space) and the ranking is generated by distances in this space. Or alternatively, structural constraints on the space of rankings could be used to generate a consistent embedding. Assumption A1, while arguably quite natural/reasonable in many situations, significantly constrains the set of possible rankings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Geometry of rankings from pairwise comparisons</head><p>The embedding assumption A1 gives rise to geometrical interpretations of the ranking problem, which are developed in this section. The pairwise comparison q i,j can be viewed as the membership query: is θ i ranked before θ j in the (full) ranking σ? The geometrical interpretation is that q i,j requests whether the reference r σ is closer to object θ i or object θ j in R d . Consider the line connecting θ i and θ j in R d . The hyperplane that bisects this line and is orthogonal to it defines two halfspaces: one containing points closer to θ i and the other the points closer to θ j . Thus, q i,j is a membership query about which halfspace r σ is in, and there is an equivalence between each query, each pair of objects, and the corresponding bisecting hyperplane. The set of all possible pairwise comparison queries can be represented as n 2 distinct halfspaces in R d . The intersections of these halfspaces partition R d into a number of cells, and each one corresponds to a unique ranking of Θ. Arbitrary rankings are not possible due to the embedding assumption A1, and recall that the set of rankings possible under A1 is denoted by Σ n,d . The cardinality of Σ n,d is equal to the number of cells in the partition. We will refer to these cells as d-cells (to indicate they are subsets in d-dimensional space) since at times we will also refer to lower dimensional cells; e.g., (d -1)-cells.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Counting the number of possible rankings</head><p>The following lemma determines the cardinality of the set of rankings, Σ n,d , under assumption A1. </p><formula xml:id="formula_4">Q(n, d) = Q(n -1, d) + (n -1)Q(n -1, d -1) , where Q(1, d) = 1 and Q(n, 0) = 1. (3)</formula><p>In the hyperplane arrangement induced by the n objects in d dimensions, each hyperplane is intersected by every other and is partitioned into Q(n -1, d -1) subsets or (d -1)-cells. The recursion, above, arises by considering the addition of one object at a time. Using this lemma in a straightforward fashion, we prove the following corollary in <ref type="bibr" target="#b6">[7]</ref>.</p><p>Corollary 1. Assume A1-2. There exist positive real numbers k 1 and k 2 such that  If each query provides a full bit of information about the ranking, then we achieve this lower bound. For example, in the one-dimensional case (d = 1) the objects can be ordered and binary search can be used to select pairwise comparison queries, achieving the lower bound. This is generally impossible in higher dimensions. Even in two dimensions there are placements of the objects (still in general position) that produce d-cells in the partition induced by queries that have n -1 faces (i.e., bounded by n -1 hyperplanes) as shown in <ref type="bibr" target="#b6">[7]</ref>. It follows that the worst case situation may require at least n -1 queries in dimensions d ≥ 2. In light of this, we conclude that worst case bounds may be overly pessimistic indications of the typical situation, and so we instead consider the average case performance introduced in Section 1.1.</p><formula xml:id="formula_5">k 1 n 2d 2 d d! &lt; Q(n, d) &lt; k 2 n 2d 2 d d! for n &gt; d + 1. If n ≤ d + 1 then Q(n, d) = n!.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inefficiency of random queries</head><p>The geometrical representation of the ranking problem reveals that randomly choosing pairwise comparison queries is inefficient relative to the lower bound above. To see this, suppose m queries were chosen uniformly at random from the possible n</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>. The answers to m queries narrows the set of possible rankings to a d-cell in R d . This d-cell may consist of one or more of the d-cells in the partition induced by all queries. If it contains more than one of the partition cells, then the underlying ranking is ambiguous. </p><formula xml:id="formula_6">-d m -d N m = m d N d ≤ m d d! d d N d ≤ m N d d d d! ≤ em N d . Note that m d / N d &lt; 1/2 unless m = Ω(n 2</formula><p>). Therefore, if the queries are randomly chosen, then we will need to ask almost all queries to guarantee that the inferred ranking is probably correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis of sequential algorithm for query selection</head><p>Now consider the basic sequential process of the algorithm in Figure <ref type="figure" target="#fig_0">1</ref>. Suppose we have ranked k -1 of the n objects. Call these objects 1 through k -1. This places the reference r σ within a d-cell (defined by the labels of the comparison queries between objects 1, . . . , k -1). Call this d-cell C k-1 . Now suppose we pick another object at random and call it object k. A comparison query between object k and one of objects 1, . . . , k -1 can only be informative (i.e., ambiguous) if the associated hyperplane intersects this d-cell C k-1 (see Figure <ref type="figure" target="#fig_4">2</ref>). If k is significantly larger than d, then it turns out that the cell C k-1 is probably quite small and the probability that one of the queries intersects C k-1 is very small; in fact the probability is on the order of 1/k 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Hyperplane-point duality</head><p>Consider a hyperplane h = (h 0 , h 1 , . . . , h d ) with (d + 1) parameters in R d and a point p = (p 1 , . . . , p d ) ∈ R d that does not lie on the hyperplane. Checking which halfspace p falls in, i.e.,</p><formula xml:id="formula_7">h 1 p 1 + h 2 p 2 + • • • + h d p d + h 0 ≷ 0,</formula><p>has a dual interpretation: h is a point in R d+1 and p is a hyperplane in R d+1 passing through the origin (i.e., with d free parameters).</p><p>Recall that each possible ranking can be represented by a reference point r σ ∈ R d . Our problem is to determine the ranking, or equivalently the vector of responses to the n 2 queries represented by hyperplanes in R d . Using the above observation, we see that our problem is equivalent to finding a labeling over n 2 points in R d+1 with as few queries as possible. We will refer to this alternative representation as the dual and the former as the primal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Characterization of an ambiguous query</head><p>The characterization of an ambiguous query has interpretations in both the primal and dual spaces. We will now describe the interpretation in the dual which will be critical to our analysis of the sequential algorithm of Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3. [9]</head><p>Let S be a finite subset of R d and let S + ⊂ S be points labeled +1 and S -= S \ S + be the points labeled -1 and let x be any other point except the origin. If there exists two homogeneous linear separators of S + and S -that assign different labels to the point x, then the label of x is said to be ambiguous with respect to S. Lemma 2. [9, Lemma 1] The label of x is ambiguous with respect to S if and only if S + and S - are homogeneously linearly separable by a (d -1)-dimensional subspace containing x.</p><p>Let us consider the implications of this lemma to our scenario. Assume that we have labels for all the pairwise comparisons of k -1 objects. Next consider a new object called object k. In the dual, the pairwise comparison between object k and object i, for some i ∈ {1, . . . , k-1}, is ambiguous if and only if there exists a hyperplane that still separates the original points and also passes through this new point. In the primal, this separating hyperplane corresponds to a point lying on the hyperplane defined by the associated pairwise comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The probability that a query is ambiguous</head><p>An essential component of the sequential algorithm of Figure <ref type="figure" target="#fig_0">1</ref> is the initial random order of the objects; every sequence in which it could consider objects is equally probable. This allows us to state a nontrivial fact about the partial rankings of the first k objects observed in this sequence. Lemma 3. Assume A1-2 and σ ∼ U. Consider the subset S ⊂ Θ with |S| = k that is randomly selected from Θ such that all n k subsets are equally probable. If Σ k,d denotes the set of possible rankings of these k objects then every σ ∈ Σ k,d is equally probable.</p><p>Proof. Let a k-partition denote the partition of R d into Q(k, d) d-cells induced by k objects for 1 ≤ k ≤ n. In the n-partition, each d-cell is weighted uniformly and is equal to 1/Q(n, d). If we uniformly at random select k objects from the possible n and consider the k-partition, each d-cell in the k-partition will contain one or more d-cells of the n-partition. If we select one of these d-cells from the k-partition, on average there will be Q(n, d)/Q(k, d) d-cells from the n-partition contained in this cell. Therefore the probability mass in each d-cell of the k-partition is equal to the number of cells from the n-partition in this cell multiplied by the probability of each of those cells from the n-partition:</p><formula xml:id="formula_8">Q(n, d)/Q(k, d) × 1/Q(n, d) = 1/Q(k, d), and |Σ k,d | = Q(k, d).</formula><p>As described above, for 1 ≤ i ≤ k some of the pairwise comparisons q i,k+1 may be ambiguous. The algorithm chooses a random sequence of the n objects in its initialization and does not use the labels of q 1,k+1 , . . . , q j-1,k+1 , q j+1,k+1 , . . . , q k,k+1 to make a determination of whether or not q j,k+1 is ambiguous. It follows that the events of requesting the label of q i,k+1 for i = 1, 2, . . . , k are independent and identically distributed (conditionally on the results of queries from previous steps). Therefore it makes sense to talk about the probability of requesting any one of them. Lemma 4. Assume A1-2 and σ ∼ U. Let A(k, d, U ) denote the probability of the event that the pairwise comparison q i,k+1 is ambiguous for i = 1, 2, . . . , k. Then there exists positive, real number constants a 1 and a 2 independent of k such that for k &gt; 2d, a 1 2d</p><formula xml:id="formula_9">k 2 ≤ A(k, d, U ) ≤ a 2 2d k 2 .</formula><p>Proof. By Lemma 2, a point in the dual (pairwise comparison) is ambiguous if and only if there exists a separating hyperplane that passes through this point. This implies that the hyperplane representation of the pairwise comparison in the primal intersects the cell containing r σ (see Figure <ref type="figure" target="#fig_4">2</ref> for an illustration of this concept). Consider the partition of R d generated by the hyperplanes corresponding to pairwise comparisons between objects 1, . . . , k. Let P (k, d) denote the number of d-cells in this partition that are intersected by a hyperplane corresponding to one of the queries q i,k+1 , i ∈ {1, . . . , k}. Then it is not difficult to show that P (k, d) is bounded above and below by constants independent of n and k times k 2(d-1) 2 d-1 (d-1)! <ref type="bibr" target="#b6">[7]</ref>. By Lemma 3, every d-cell in the partition induced by the k objects corresponds to an equally probable ranking of those objects. Therefore, the probability that a query is ambiguous is the number of cells intersected by the corresponding hyperplane divided by the total number of d-cells, and therefore A(k, d, U ) = P (k,d) Q(k,d) . The result follows immediately from the bounds on P (k, d) and Corollary 1.</p><p>Because the individual events of requesting each query are conditionally independent, the total number of queries requested by the algorithm is just M n = n-1 k=1 k i=1 1{Request q i,k+1 }. Using the results above, it straightforward to prove the main theorem below (see <ref type="bibr" target="#b6">[7]</ref>).</p><p>Theorem 3. Assume A1-2 and σ ∼ U. Let the random variable M n denote the number of pairwise comparisons that are requested in the algorithm of Figure <ref type="figure" target="#fig_0">1</ref>, then</p><formula xml:id="formula_10">E U [M n ] ≤ 2d log 2 2d + 2da 2 log n. Furthermore, if σ ∼ π and max σ∈Σ n,d π σ ≤ c|Σ n,d | -1 for some c &gt; 0, then E π [M n ] ≤ cE U [M n ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Robust sequential algorithm for query selection</head><p>We now extend the algorithm of Figure <ref type="figure" target="#fig_0">1</ref> to situations in which the response to each query is only probably correct. If the correct label of a query q i,j is y i,j , we denote the possibly incorrect response by Y i,j . The probability that Y i,j = y i,j is at least 1p, p &lt; 1/2. The robust algorithm operates in the same fashion as the algorithm in Figure <ref type="figure" target="#fig_0">1</ref>, with the exception that when an ambiguous query is encountered several (equivalent) queries are made and a decision is based on the majority vote. This voting procedure allows us to construct a ranking (or partial ranking) that is correct with high probability by requesting just O(d log 2 n) queries where the extra log factor comes from voting. First consider the case in which each query can be repeated to obtain multiple independent responses (votes) for each comparison query. This random noise model arises, for example, in social choice theory where the "reference" is a group of people, each casting a vote. The elementary proof of the next theorem is given in <ref type="bibr" target="#b6">[7]</ref>.</p><p>Theorem 4. Assume A1-2 and σ ∼ U but that each query response is a realization of an i.i.d. Bernoulli random variable Y i,j with P (Y i,j = y i,j ) ≤ p &lt; 1/2. If all ambiguous queries are decided by the majority vote of R independent responses to each such query, then with probability greater than 1 -2n log 2 (n) exp(-1 2 (1 -2p) 2 R) this procedure correctly identifies the correct ranking and requests no more than O(Rd log n) queries on average.</p><p>In other situations, if we ask the same query multiple times we may get the same, possibly incorrect, response each time. This persistent noise model is natural, for example, if the reference is a single human. Under this model, if two rankings differ by only a single pairwise comparison, then they cannot be distinguished with probability greater than 1p. So, in general, exact recovery of the ranking cannot be guaranteed with high probability. The best we can hope for is to exactly recover a partial ranking of the objects (i.e. the ranking over a subset of the objects). Henceforth, we will assume the noise is persistent and aim to exactly recover a partial ranking of the objects.</p><p>The key ingredient in the persistent noise setting is the design of a voting set for each ambiguous query encountered. Suppose that at the jth object in the algorithm in Figure <ref type="figure" target="#fig_0">1</ref> the query q i,j is ambiguous. In principle, a voting set could be constructed using objects ranked between i and j. If object k is between i and j, then note that y i,j = y i,k = y k,j . In practice, we cannot identify the subset of objects ranked between i and j, but it is contained within the set T i,j , defined to be the subset of objects θ k such that q i,k , q k,j , or both are ambiguous. Furthermore, Lemma 3 implies that each object in T i,j is ranked between i and j with probability at least 1/3 <ref type="bibr" target="#b6">[7]</ref>. T i,j will be our voting set. Note however, if objects i and j are closely ranked, then T i,j may be rather small, and so it is not   always possible to find a sufficiently large voting set. Therefore, we must specify a size-threshold R ≥ 1. If the size of T i,j is at least R, then we decide the label for q i,j by voting over the responses to {q i,k , q k,j : k ∈ T i,j } and q i,j ; otherwise we pass over object j and move on to the next object in the list. This allows us to construct a probably correct ranking of the objects that are not passed over. The theorem below proves that a large portion of objects will not be passed over. At the end of the process, some objects that were passed over may then be unambiguously ranked (based on queries made after they were passed over) or they can be ranked without voting (and without guarantees). The proof of the next theorem is provided in the longer version of this paper <ref type="bibr" target="#b6">[7]</ref>.</p><p>Theorem 5. Assume A1-2, σ ∼ U, and P (Y i,j = y i,j ) = p. For any size-threshold R ≥ 1, with probability greater than 1 -2n log 2 (n) exp -2 9 (1 -2p) 2 R the procedure above correctly ranks at least n/(2R + 1) objects and requests no more than O(Rd log n) queries on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Empirical results</head><p>In this section we present empirical results for both the noiseless algorithm of Figure <ref type="figure" target="#fig_0">1</ref> and the robust algorithm of Section 5. For the noiseless algorithm, n = 100 points, representing the objects to be ranked, were uniformly at random simulated from the unit hypercube [0, 1] d for d = 1, 10, 20, . . . , 100. The reference was simulated from the same distribution. For each value of d the experiment was repeated 25 times using a new simulation of points and the reference. Because responses are noiseless, exact identification of the ranking is guaranteed. The number of requested queries is plotted in Figure <ref type="figure" target="#fig_7">3</ref> with the lower bound of Theorem 1 for reference. The number of requested queries never exceeds twice the lower bound which agrees with the result of Theorem 3.</p><p>The robust algorithm in Section 5 was evaluated using a symmetric similarity matrix dataset available at <ref type="bibr" target="#b19">[20]</ref> whose (i, j)th entry, denoted s i,j , represents the human-judged similarity between audio signals i and j for all i = j ∈ {1, . . . , 100}. If we consider the kth row of this matrix, we can rank the other signals with respect to their similarity to the kth signal; we define q (k) i,j := {s k,i &gt; s k,j } and y (k) i,j := 1{q (k) i,j }. Since the similarities were derived from human subjects, the derived labels may be erroneous. Moreover, there is no possibility of repeating queries here and so the noise is persistent. The analysis of this dataset in <ref type="bibr" target="#b1">[2]</ref> suggests that the relationship between signals can be well approximated by an embedding in 2 or 3 dimensions. We used non-metric multidimensional scaling <ref type="bibr" target="#b4">[5]</ref> to find an embedding of the signals: θ 1 , . . . , θ 100 ∈ R d for d = 2 and 3. For each object θ k , we use the embedding to derive pairwise comparison labels between all other objects as follows: ỹ(k) i,j := 1{||θ kθ i || &lt; ||θ kθ j ||}, which can be considered as the best approximation to the labels y (k) i,j (defined above) in this embedding. The output of the robust sequential algorithm, which uses only a small fraction of the similarities, is denoted by ŷ(k) i,j . We set R = 15 using Theorem 5 as a rough guide. Using the popular Kendell-Tau distance d(y (k) , ŷ(k) ) = n 2 -1 i&lt;j 1{y</p><p>(k) i,j = ŷ(k) i,j } <ref type="bibr" target="#b20">[21]</ref> for each object k, we denote the average of this metric over all objects by d(y, ŷ) and report this statistic and the number of queries requested in Table <ref type="table" target="#tab_0">1</ref>. Because the average error of ŷ is only 0.07 higher than that of ỹ, this suggests that the algorithm is doing almost as well as we could hope. Also, note that 2R 2d log n/ n 2 is equal to 11.4% and 17.1% for d = 2 and 3, respectively, which agrees well with the experimental values.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sequential algorithm for selecting queries. See Figure 2 and Section 4.2 for the definition of an ambiguous query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>θ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Lemma 1 .</head><label>1</label><figDesc><ref type="bibr" target="#b7">[8]</ref> Assume A1-2. Let Q(n, d) denote the number of d-cells defined by the hyperplane arrangement of pairwise comparisons between these objects (i.e. Q(n, d) = |Σ n,d |). Q(n, d) satisfies the recursion</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>For n sufficiently large, k 1 = 1 and k 2 = 2 suffice.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3. 2</head><label>2</label><figDesc>Lower bounds on query complexity Since the cardinality of the set of possible rankings is |Σ n,d | = Q(n, d), we have a simple lower bound on the number of queries needed to determine the ranking. Theorem 1. Assume A1-2. To reconstruct an arbitrary ranking σ ∈ Σ n,d any algorithm will require at least log 2 |Σ n,d | = Θ(2d log 2 n) pairwise comparisons. Proof. By Corollary 1 |Σ n,d | = Θ(n 2d ), and so at least 2d log n bits are needed to specify a ranking. Each pairwise comparison provides at most one bit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Theorem 2 . 2 . 2 .</head><label>222</label><figDesc>Assume A1-2. Let N = n Suppose m pairwise comparison are chosen uniformly at random without replacement from the possible n Then for all positive integers N ≥ m ≥ d the probability that the m queries yield a unique ranking is m d / N d ≤ ( em N ) d . Proof. No fewer than d hyperplanes bound each d-cell in the partition of R d induced by all possible queries. The probability of selecting d specific queries in a random draw of m is equal to N</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Mean and standard deviation of requested queries (solid) in the noiseless case for n = 100; log 2 |Σ n,d | is a lower bound (dashed).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics for the algorithm robust to persistent noise of Section 5 with respect to all n 2 pairwise comparisons. Recall y is the noisy response vector, ỹ is the embedding's solution, and ŷ is the output of the robust algorithm.</figDesc><table><row><cell>Dimension</cell><cell></cell><cell>2</cell><cell>3</cell></row><row><cell>% of queries</cell><cell>mean</cell><cell cols="2">14.5 18.5</cell></row><row><cell>requested</cell><cell>std</cell><cell>5.3</cell><cell>6</cell></row><row><cell>Average error</cell><cell cols="3">d(y, ỹ) 0.23 0.21 d(y, ŷ) 0.31 0.29</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Art of Computer Programming</title>
		<author>
			<persName><forename type="first">D</forename><surname>Knuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sorting and Searching</title>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Perceptual feature identification for active sonar echoes</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Philips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Pitton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Les</forename><surname>Atlas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Partial order embedding with multiple kernels</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="721" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A latent space model for rank data</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical Network Analysis: Models, Issues, and New Directions</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="90" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multidimensional scaling. Handbook of data visualization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A A</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="315" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Information-based complexity</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Traub</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>John Wiley and Sons Ltd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">G</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">D</forename><surname>Nowak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1109.3701v1</idno>
		<title level="m">Active ranking using pairwise comparisons</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A theory of data</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Coombs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="143" to="159" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on electronic computers</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="326" to="334" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Analysis of perceptron-based active learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Kalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Monteleoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="281" to="299" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Theoretical foundations of active learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hanneke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generalized teaching dimensions and the query complexity of learning</title>
		<author>
			<persName><forename type="first">Tibor</forename><surname>Hegedüs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth annual conference on Computational learning theory, COLT &apos;95</title>
		<meeting>the eighth annual conference on Computational learning theory, COLT &apos;95<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="108" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An efficient boosting algorithm for combining preferences</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="933" to="969" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to rank using gradient descent</title>
		<author>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Deeds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hullender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on Machine learning</title>
		<meeting>the 22nd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A regression framework for learning ranking functions using relative relevance judgments</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th annual international ACM SI-GIR conference on Research and development in information retrieval</title>
		<meeting>the 30th annual international ACM SI-GIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="287" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Support vector learning for ordinal regression</title>
		<author>
			<persName><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks, 1999. ICANN 99. Ninth International Conference on (Conf. Publ</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Robust approximation and incremental elicitation in voting protocols</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boutilier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">11</biblScope>
			<pubPlace>Barcelona</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Extensions of gaussian processes for ranking: semi-supervised and active learning. Learning to Rank</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multidimensional unfolding: Determining the dimensionality of ranked preference data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="43" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Similarity</forename><surname>Learning</surname></persName>
		</author>
		<ptr target="http://idl.ee.washington.edu/SimilarityLearning" />
		<title level="m">Aural Sonar dataset</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>University of Washington Information Design Lab</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Analyzing and modeling rank data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Marden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Chapman &amp; Hall/CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
