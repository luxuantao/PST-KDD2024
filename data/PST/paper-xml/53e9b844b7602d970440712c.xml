<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KRYLOV SUBSPACE METHODS FOR LINEAR SYSTEMS WITH TENSOR PRODUCT STRUCTURE *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Kressner</surname></persName>
							<email>kressner@math.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Seminar for Applied Mathematics</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<addrLine>Raemistr. 101</addrLine>
									<postCode>D-MATH, CH-8092</postCode>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christine</forename><surname>Tobler</surname></persName>
							<email>ctobler@math.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Seminar for Applied Mathematics</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<addrLine>Raemistr. 101</addrLine>
									<postCode>D-MATH, CH-8092</postCode>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">KRYLOV SUBSPACE METHODS FOR LINEAR SYSTEMS WITH TENSOR PRODUCT STRUCTURE *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3FE5E606345C23A12EF7C37430E0358A</idno>
					<idno type="DOI">10.1137/090756843</idno>
					<note type="submission">Received by the editors April 23, 2009; accepted for publication (in revised form) by V. Simoncini January 4, 2010;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>linear systems</term>
					<term>Krylov subspace methods</term>
					<term>low tensor rank</term>
					<term>high dimensionality AMS subject classifications. Primary</term>
					<term>65F10; Secondary</term>
					<term>15A69</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The numerical solution of linear systems with certain tensor product structures is considered. Such structures arise, for example, from the finite element discretization of a linear PDE on a d-dimensional hypercube. Linear systems with tensor product structure can be regarded as linear matrix equations for d = 2 and appear to be their most natural extension for d &gt; 2. A standard Krylov subspace method applied to such a linear system suffers from the curse of dimensionality and has a computational cost that grows exponentially with d. The key to breaking the curse is to note that the solution can often be very well approximated by a vector of low tensor rank. We propose and analyze a new class of methods, so-called tensor Krylov subspace methods, which exploit this fact and attain a computational cost that grows linearly with d.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.</head><p>Introduction. This paper is concerned with certain linear systems that can be written as the sum of d Kronecker products of matrices. More specifically, we consider for d = 2, (1.1)</p><formula xml:id="formula_0">A 1 ⊗ I n2 + I n1 ⊗ A 2 x = b 1 ⊗ b 2 ,</formula><p>and for d = 3, (1.2)</p><formula xml:id="formula_1">A 1 ⊗ I n2 ⊗ I n3 + I n1 ⊗ A 2 ⊗ I n3 + I n1 ⊗ I n2 ⊗ A 3 x = b 1 ⊗ b 2 ⊗ b 3 ,</formula><p>where A s ∈ R ns×ns , b s ∈ R ns , and I ns denotes the n s × n s identity matrix. For general d ∈ N, the linear system takes the form</p><formula xml:id="formula_2">(1.3) Ax = b, with A = d s=1 I n1 ⊗ • • • ⊗ I ns-1 ⊗ A s ⊗ I ns+1 ⊗ • • • ⊗ I n d , (1.4) b = b 1 ⊗ • • • ⊗ b d . (1.5)</formula><p>Classical Krylov subspace methods for solving linear systems, such as conjugate gradient or GMRES, are not well suited for solving <ref type="bibr">(1.3)</ref>. To illustrate this, let us consider the case of constant dimensions, n s ≡ n. Then every vector in the Krylov subspace basis has length n d and a single scalar product requires 2n d operations. The purpose of this paper is to develop Krylov subspace methods having computational costs and memory requirements that scale linearly, rather than exponentially, in d.</p><p>The following model problem from <ref type="bibr" target="#b8">[9]</ref> shall illustrate the type of applications leading to <ref type="bibr">(1.3)</ref>. Consider the PDE <ref type="bibr">(1.6)</ref> u = f in Ω, u| ∂Ω = 0,</p><p>where Ω = [0, 1] d is the d-dimensional hypercube. Choosing a tensorized finite element basis for discretzing the variational formulation of (1.6) yields the mass and stiffness matrices</p><formula xml:id="formula_3">M = M 1 ⊗ • • • ⊗ M d , B = d s=1 M 1 ⊗ • • • ⊗ M s-1 ⊗ B s ⊗ M s+1 ⊗ • • • ⊗ M d .</formula><p>Hence, A = M -1/2 BM -1/2 is of the form <ref type="bibr">(1.4)</ref> with</p><formula xml:id="formula_4">A s = (M s ) -1/2 B s (M s ) -1/2 . If f is separable, f = f 1 (y 1 )f 2 (y 2 ) • • • f d (y d )</formula><p>, then the discretized right-hand side takes the form <ref type="bibr">(1.5)</ref>. Otherwise, the function f might be well approximated by a short sum of separable functions, in which case the solution of the discretized equation can be obtained from linear systems of the type (1.3) by superposition. This is possible, for example, if f is sufficiently smooth and d is moderate; see, e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9]</ref>. For d = 2, (1.1) can be reformulated as follows:</p><p>(1.7)</p><formula xml:id="formula_5">A 1 X + XA 2 = b 1 (b 2 ) ,</formula><p>where x = rowvec(X), with the rowvec operator stacking the rows of a matrix X ∈ R n1×n2 into a single column vector x ∈ R n1•n2 . The linear matrix equation (1.7) is usually called the Sylvester equation, which has been studied quite intensively, often motivated by applications in systems and control theory. In fact, most results and algorithms presented in this paper are already known for d = 2. In particular, several variants of Krylov subspace algorithms for solving (1.7) have been developed and analyzed; see <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23]</ref>. The novelty of our work is in the extension to d &gt; 2; we will point out relevant connections to the case d = 2 whenever suitable. A notable exception is the convergence bound for extended Krylov subspace methods that we give in section 6; this result addresses an open question for the case d = 2. Grasedyck <ref type="bibr" target="#b8">[9]</ref> has combined an integral representation of the solution x to (1.3) with quadrature based on sinc interpolation <ref type="bibr" target="#b25">[25]</ref> to show that x can be well approximated by vectors of low tensor rank and to develop a numerical algorithm that scales linearly with d. To the best of our knowledge, this was the first and so far the only algorithm for efficiently approximating x for high dimensions. Somewhat of a drawback, the algorithm relies on computing matrix exponentials of scalar multiples of A s , which might become expensive for larger matrices. In contrast, the approach proposed in this paper relies solely on matrix-vector multiplications with A s . If available, matrix-vector products with (A s ) -1 can be used to speed up convergence. A variant of Grasedyck's algorithm is still invoked for solving smaller subsystems.</p><p>The rest of this paper is organized as follows. Section 2 contains some preliminary results, mainly concerning tensor notation and approximations of low tensor rank to the solution of <ref type="bibr">(1.3)</ref>. In section 3, we will describe the newly proposed tensor Krylov subspace method and discuss some implementation details, such as the efficient computation of the residual. The convergence of this method is analyzed in section 4 for the (symmetric and nonsymmetric) positive definite case. Section 5 provides a discussion on solving the compressed systems needed in the course of the tensor Downloaded 01/04/13 to 150.135.135.70. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php DANIEL KRESSNER AND CHRISTINE TOBLER Krylov subspace method. In section 6, we propose an extension of the tensor Krylov subspace method, which is suitable if matrix-vector products not only with A s but also with (A s ) -1 can be performed. Section 7 contains some numerical experiments with academic examples to illustrate the theoretical results obtained in this paper. Finally, some conclusions are outlined in section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries.</head><p>The following lemma is a consequence of well-known properties of the Kronecker product <ref type="bibr" target="#b14">[15]</ref>.</p><p>Lemma 2.1. Consider the matrix A defined in <ref type="bibr">(1.4)</ref>. Then Λ(A), the set of eigenvalues of A, is given by all possible sums of eigenvalues of A 1 , A 2 , . . . , A d :</p><formula xml:id="formula_6">(2.1) Λ(A) = λ 1 + λ 2 + • • • + λ d : λ s ∈ Λ(A s ) .</formula><p>The linear system (1.3) has a unique solution if and only if Λ(A) contains no zero eigenvalues, which-by Lemma 2.1-is equivalent to (2.2)</p><formula xml:id="formula_7">λ 1 + λ 2 + • • • + λ d = 0 ∀λ s ∈ Λ(A s ).</formula><p>In the case of the Sylvester equation (1.7), this corresponds to the well-known condition</p><formula xml:id="formula_8">Λ(A 1 ) ∩ Λ(-A 2 ) = ∅.</formula><p>We recall that a nonsymmetric matrix A is called positive definite if its symmetric part (A + A )/2 is positive definite. By Lemma 2.1, the matrix A is positive definite if and only if</p><formula xml:id="formula_9">(2.3) ξ 1 + ξ 2 + • • • + ξ d &gt; 0 ∀ξ s ∈ Λ(A s + A s )/2.</formula><p>The following lemma recalls an integral representation of the solution x from <ref type="bibr" target="#b8">[9]</ref>. Lemma 2.2. If A is positive definite, then the solution of the linear system (1.3) admits the representation</p><formula xml:id="formula_10">x = - ∞ 0 exp(-tA 1 )b 1 ⊗ • • • ⊗ exp(-tA d )b d dt.</formula><p>Note that Lemma 2.2 still holds if we impose the less restrictive condition that the eigenvalues of A have positive real part, but this is not needed for the developments of our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Tensor arithmetic and decompositions.</head><p>This section provides a brief overview of tensor arithmetic concepts needed in the rest of the paper. We refer to the recent survey <ref type="bibr" target="#b19">[19]</ref> for more details.</p><p>A d-way tensor v ∈ R n1×n2×•••×n d is an element of the tensor product of the vector spaces R n1 , R n2 , . . . , R n d for fixed integers n 1 , . . . , n d . The coordinates of v (with respect to a choice of bases) form a multidimensional array. The element at the multi-index I = (i 1 , i 2 , . . . , i n ) in such an array is denoted by v I . A tensor can be represented as a vector in R n1n2•••n d by simply stacking the elements v I in lexicographical order. In the following, we will identify tensors with their vector representations. For d = 2, this means that a matrix A ∈ R n1×n2 is identified with the vector rowvec(A) ∈ R n1n2 , where rowvec stacks the transposed rows of A on top of each other. This identification of tensors with vectors is unambiguous as soon as the order d and the dimensions n 1 , . . . , n d are fixed.</p><p>A tensor v ∈ R    </p><formula xml:id="formula_11">(2.4) v = v (1) ⊗ v (2) ⊗ • • • ⊗ v (d) , v (s)</formula><formula xml:id="formula_12">= (i 1 , i 2 , . . . i n ) takes the form v I = v (1) i1 v (2) i2 • • • v (d) i d .</formula><p>A tensor is called supersymmetric if it is invariant under any permutations of the indices. For matrices, supersymmetry coincides with the usual notion of symmetry. The following lemma generalizes a well-known result on the symmetry of solutions to Lyapunov matrix equations.</p><p>Lemma 2.3. Consider the linear system Ax = b, where A takes the form (1.4)</p><formula xml:id="formula_13">with constant coefficients A = A 1 = • • • = A d . If A is invertible</formula><p>and b represents a supersymmetric tensor, then the solution x is also the representation of a supersymmetric tensor.</p><p>Proof. Assume that x is the solution of Ax = b and that x is not supersymmetric. Then there is a permutation π : {1, . . . , d} → {1, . . . , d} such that the vector x π , the representation of the tensor with elements x π(I) for every multi-index</p><formula xml:id="formula_14">I, is different from x. The structure of A implies Ax π = b π . Since b is supersymmetric, Ax π = b π = b, contradicting the unique solvability of Ax = b.</formula><p>The CANDECOMP/PARAFAC (CP) decomposition represents a tensor as a sum of rank one tensors. In vector language, this means</p><formula xml:id="formula_15">(2.5) v = k i=1 v (1) i ⊗ v (2) i ⊗ • • • ⊗ v (d) i , v (s) i ∈ R ns .</formula><p>If v admits a representation (2.5), then we say that v has tensor rank at most k.</p><p>In our context we do not need the concept of exact tensor rank, which is much more subtle than the usual rank concept for matrices. Defining the n s × k matrices</p><formula xml:id="formula_16">V s = v (s) 1 , v (s) 2 , . . . , v<label>(s) k</label></formula><p>, a more compact way of writing (2.5) is</p><formula xml:id="formula_17">v = V 1 , V 2 , . . . , V d .</formula><p>The Tucker decomposition is another popular tensor decomposition. For an integer tuple K = (k 1 , . . . , k d ), it takes the form</p><formula xml:id="formula_18">(2.6) v = I≤K c I v (1) i1 ⊗ v (2) i2 ⊗ • • • ⊗ v (d) i d =: c; V 1 , V 2 , . . . , V d ,</formula><p>where the sum is taken over all multi-indices I = (i 1 , . . . , i d ) that are elementwise not larger than K. It is worth emphasizing that</p><formula xml:id="formula_19">V s = v (s) 1 , v (s) 2 , . . . , v (s) ks</formula><p>is now an n s × k s matrix, i.e., the number of columns of V s may vary with s. The k 1 × • • • × k d tensor formed from the elements c I is called the core tensor. Note that the CP decomposition (2.5) is a special case of (2.6) for constant k s ≡ k and an "identity" core tensor that is zero except for c i,i,...,i = 1 for i = 1, . . . , k. However, the Tucker format usually assumes the matrices V s to be column-orthogonal, which is not the case for the CP format.</p><p>Alternatively, the Tucker decomposition can be written as</p><formula xml:id="formula_20">(2.7) v = V 1 ⊗ • • • ⊗ V d c,</formula><p>where c is to be understood as the vector representation of the core tensor in <ref type="bibr">(2.6)</ref>. This also reveals that v is in the subspace spanned by</p><formula xml:id="formula_21">V 1 ⊗ V 2 ⊗ • • • ⊗ V d .</formula><p>Notation 2.4. We write the multidimensional Kronecker product as : Ω s → C, j = 1, . . . , k, be analytic functions such that Ω s contains the eigenvalues of A s and</p><formula xml:id="formula_22">d s=1 v s := v 1 ⊗ • • • ⊗ v d .</formula><formula xml:id="formula_23">(2.8) 1 μ 1 + μ 2 + • • • + μ d - k i=1 f (1) i (μ 1 )f (2) i (μ 2 ) • • • f (d) i (μ d ) Ω ≤ (k),</formula><p>where</p><formula xml:id="formula_24">• Ω denotes the supremum norm on Ω = Ω 1 × • • • × Ω d Then there is a rank-k tensor x k such that x -x k 2 ≤ κ P (k) b 2 ,</formula><p>where κ P = κ 2 (P 1 )κ </p><formula xml:id="formula_25">I n1 ⊗ • • • ⊗ I ns-1 ⊗ Λ s ⊗ I ns+1 ⊗ • • • ⊗ I n d x = b,</formula><p>with x = P -1 x and b = P -1 b. This is a diagonal linear system, and the entry of the solution x at the multi-index I = (i 1 , . . . , i d ) is given by</p><formula xml:id="formula_26">xI = bI λ (1) i1 + λ (2) i2 + • • • + λ (d) i d , where λ (1) i1 + • • • + λ (d) i d = 0 from Lemma 2.1.</formula><p>Similarly, if we define the rank-k tensor as (2.9)</p><formula xml:id="formula_27">x k = k j=1 f (1) j A 1 b 1 ⊗ f (2) j A 2 b 2 ⊗ • • • ⊗ f (d) j A d b d ,</formula><p>the entry of the correspondingly transformed tensor xk = P -1 x k at I is given by bI f</p><formula xml:id="formula_28">(1) j λ (1) i1 f (2) j λ (2) i2 • • • f (d) j λ (d) i d .</formula><p>Hence, with K = (k, . . . , k), </p><formula xml:id="formula_29">x -xk 2 2 = I≤K | bI | 2 1 λ (1) i1 + λ (2) i2 + • • • + λ (d) i d - k j=1 f (1) j λ (1) i1 • • • f (d) j λ (d) i d 2 ≤ (k) 2 I≤K | bI | 2 = (k)</formula><formula xml:id="formula_30">= P(x -xk ) 2 ≤ P 2 x -xk 2 , b 2 ≤ P -1 2 b 2 ,</formula><p>and κ P = P 2 P -1 2 yields the statement of the theorem.</p><p>Theorem 2.5 provides an upper bound on the error for the best approximation of x by a rank-k tensor. To be practically useful, we still need to address the approximation problem <ref type="bibr">(2.8)</ref>. The following technical lemma by <ref type="bibr">Braess and Hackbusch [6,</ref><ref type="bibr">sect. 2]</ref> will turn out to be very helpful for this purpose.</p><p>Lemma 2.6 (see <ref type="bibr" target="#b5">[6]</ref>).</p><formula xml:id="formula_31">Let s k (μ) = k i=1 ω i exp(-α i μ), with α i , ω i ∈ R. Then there is a choice of α i &gt; 0, ω i &gt; 0 (depending on k and R &gt; 1) such that sup μ∈[1,R] 1 μ -s k (μ) ≤ 16 exp -kπ 2 log(8R)</formula><p>.</p><p>Corollary 2.7. Consider the linear system Ax = b with A and b of the form (1.4)-(1.5). If A is symmetric positive definite, then there exists an approximation x k of tensor rank at most k such that</p><formula xml:id="formula_32">(2.10) x -x k 2 ≤ 16 λ min (A) exp -kπ 2 log(8κ(A)) b 2 .</formula><p>Proof. As A is symmetric positive definite, all coefficient matrices A s are symmetric, and Ax = b has a unique solution. Therefore, Theorem 2.5 can be applied with P s orthogonal (hence, κ P = 1). The result follows directly from this theorem combined with the following observation. Applying Lemma 2.6, we use the substitution (2.12)</p><formula xml:id="formula_33">μ = μ1+•••+μ d λmin(A) , y ∈ [1, λmax(A) λmin(A) ] =: [1, R] and obtain λ min (A) 1 μ 1 + . . . + μ d - k i=1 ω i λ min (A) e -αiμ1/λmin(A)</formula><formula xml:id="formula_34">x k = k j=1 ωj d s=1 exp(-αj A s )b s ,</formula><p>with αj = α j /λ min (A), ωj = ω j /λ min (A), and α j , ω j as in Lemma 2.6. The coefficients α j , ω j depend only on k and R = κ(A) &gt; 1. The resulting method will be discussed in somewhat more detail in section 5. 3. The tensor Krylov subspace method. In the following, we will develop numerical algorithms for approximating the solution x to the linear system (1.3). Note that section 2.2, in particular Remark 2.8, already provides a rather effective method for computing low tensor rank approximations as the computational effort grows linearly with d and the convergence rate depends very mildly, at most logarithmically, on the conditioning of A. Indeed, this method is used for solving compressed systems arising in our algorithms; see section 5 below. However, for the purpose of this method, the expressions exp(-αj A s )b s must be computed rather exactly to guarantee a good accuracy of x, which may be regarded as expensive for larger matrices when compared to, say, a simple matrix-vector multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Tensorized Krylov subspaces. We let</head><formula xml:id="formula_35">K ks (A s , b s ) = span b s , A s b s , . . . , A ks-1 s b s , s = 1, . . . , d,</formula><p>denote the Krylov subspace obtained from k s -1 successive matrix-vector products of A s with b s . In view of the PDE (1.6), each K ks (A s , b s ) could be seen as a subspace corresponding to one coordinate of the domain. To obtain a subspace for all d coordinates, we tensorize and take the linear hull.</p><p>Definition 3.1. Let A, b be as in (1.3)-(1.4), and consider a multi</p><formula xml:id="formula_36">-index K = (k 1 , . . . , k d ) with k s ∈ N. Then K ⊗ K (A, b) := span K k1 (A 1 , b 1 ) ⊗ • • • ⊗ K k d (A d , b d )</formula><p>is called the tensorized Krylov subspace associated with A and b.</p><p>Equivalently, the tensorized Krylov subspace can be defined as</p><formula xml:id="formula_37">(3.1) K ⊗ K (A, b) = span A i1-1 1 b 1 ⊗ • • • ⊗ A i d -1 d b d : I ≤ K .</formula><p>A more computationally oriented definition is obtained as follows. Define d matrices U s , s = 1, . . . , d, such that the columns of each</p><formula xml:id="formula_38">U s span K ks (A s , b s ). Then K ⊗ K (A, b) is spanned by the columns of U = U 1 ⊗ • • • ⊗ U d . Combined with (2.6)-(2.7), this shows that K ⊗ K (A, b</formula><p>) is spanned by the Tucker decompositions c; U 1 , U 2 , . . . , U d for all possible core tensors c.</p><p>In the following, we discuss an extension of the well-known relation between Krylov subspaces and matrix polynomials. Given a multi-index K, we call p : R d → R a multivariate polynomial of degree less than K if p is a polynomial of degree at most Downloaded 01/04/13 to 150.135.135.70. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php k s -1 in the sth variable. The space of all such multivariate polynomials is denoted by Π ⊗ K . Each p ∈ Π ⊗ K can be written as</p><formula xml:id="formula_39">p(μ 1 , . . . , μ d ) = L≤K c L μ l1-1 1 μ l2-1 2 • • • μ l d -1 d , c L ∈ R,</formula><p>where the sum is taken over multi-indices L that satisfy 1 ≤ l s ≤ k s . The evaluation of p at the matrix A, defined in (1.4) and represented by the matrix tuple (A 1 , . . . , A d ), is defined as</p><formula xml:id="formula_40">p(A 1 , . . . , A d ) := L≤K c L Âl1-1 1 Âl2-1 2 • • • Âl d -1 d = L≤K c L A l1-1 1 ⊗ A l2-1 2 ⊗ • • • ⊗ A l d -1 d , (3.2) where Âs = I n1 ⊗ • • • ⊗ I ns-1 ⊗ A s ⊗ I ns+1 ⊗ • • • ⊗ I n d and A = Â1 + • • • + Âd . Lemma 3.2. With the notation introduced above, K ⊗ K (A, b) = span p(A 1 , . . . , A d )b : p ∈ Π ⊗ K . Proof. Let g = p(A 1 , . . . , A d )b for some p ∈ Π ⊗ K . By (3.2), this is equivalent to (3.3) g = L≤K c L A l1-1 1 b 1 ⊗ • • • ⊗ A l d -1 d b d ; i.e., g is a linear combination of elements from K k1 (A 1 , b 1 ) ⊗ • • • ⊗ K k d (A d , b d ),</formula><p>and, therefore-by definition-g ∈ K ⊗ K (A, b). For the other direction, we note that (3.1) implies that any g ∈ K ⊗ K (A, b) can be written in the form (3.3), which concludes the proof.</p><p>Remark 3.3. Lemma 3.2 reveals an important difference between standard and tensorized Krylov subspaces. For k 0 ∈ N, the standard Krylov subspace satisfies</p><formula xml:id="formula_41">K k0 (A, b) = {p(A)b : p ∈ Π k0 },</formula><p>where Π k0 denotes the space of all univariate polynomials of degree at most k 0 . For a given univariate polynomial p ∈ Π k0 , we define the multivariate polynomial as</p><formula xml:id="formula_42">(3.4) p(μ 1 , μ 2 , . . . , μ d ) := p(μ 1 + μ 2 + • • • + μ d ). By direct computation, p(A 1 , . . . , A d )b = p(A)b, which-together with Lemma 3.2- shows K ⊗ K (A, b) ⊃ K k0 (A, b)</formula><p>for K = (k 0 , . . . , k 0 ). On the other hand, it is obvious that not every multivariate polynomial takes the particular form (3.4) and hence </p><formula xml:id="formula_43">K ⊗ K (A, b) = K k0 (A, b) for d &gt;</formula><formula xml:id="formula_44">: Matrix A s ∈ R ns×ns , Vector b s ∈ R ns , k s ∈ N. Output: Matrix U s ∈ R n×ks containing an orthonormal basis of K ks (A s , b s ). u (s) 1 ← b s / b s 2 for j = 1, . . . , k s do w ← A s u (s) j h (s) 1:j,j ← u (s) 1 , . . . , u (s) j w ũ(s) j+1 ← w -u (s) 1 , . . . , u (s) j h (s) 1:j,j h (s) j+1,j = ũ(s) j+1 2 u (s) j+1 = ũ(s) j+1 / ũ(s) j+1 2 end for Set U s = u (s) 1 , . . . , u (s)</formula><p>ks .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Basic algorithm.</head><p>In this section, we present the basics of the newly proposed tensor Krylov subspace algorithm. This algorithm approximates the solution x of the linear system (1.3) by an element from K ⊗ K (A, b). To start, we require a basis of K ⊗ K (A, b). For this purpose, the standard Arnoldi method is used to compute matrices U s ∈ R ns×ks such that the columns of each U s form an orthonormal basis of the Krylov subspace K ks (A s , b s ). A brief description of the Arnoldi method is provided in Algorithm 1; more algorithmic details can be found, e.g., in <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b26">26]</ref>. We assume that a suitable reorthogonalization strategy is performed such that the columns of U s are also numerically orthonormal.</p><p>Upon successful completion of Algorithm 1, one obtains the so-called Arnoldi decomposition</p><formula xml:id="formula_45">(3.5) A s U s = U s H s + h (s) ks+1,ks u (s) ks+1 e ks ,</formula><p>where the upper Hessenberg matrix H s collects the coefficients h </p><formula xml:id="formula_46">(s) ij : (3.6) H s = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ h (s) 11 h (s) 12 • • • h (s)</formula><formula xml:id="formula_47">⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ = U s A s U s .</formula><p>Note that if A s is symmetric, then H s inherits this symmetry and becomes a tridiagonal matrix.</p><p>As discussed above, the tensor Krylov subspace</p><formula xml:id="formula_48">K ⊗ K (A, b) is spanned by the columns of U = U 1 ⊗ • • • ⊗ U d .</formula><p>To extract an approximation to the solution of the linear system (1.3) from the tensor Krylov subspace, we define x K = Uy, where y solves the compressed linear system Then, x K satisfies the following Galerkin condition:</p><formula xml:id="formula_49">Ax K -b ⊥ K ⊗ K (A, b</formula><p>). Downloaded 01/04/13 to 150.135.135.70. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p><p>The solvability of (3.7) will be discussed in more detail in section 4. Note that H and b take the form</p><formula xml:id="formula_50">H = U AU = d s=1 U s d s=1 I n1 ⊗ • • • ⊗ I ns-1 ⊗ A s ⊗ I ns+1 ⊗ • • • ⊗ I n d d s=1 U s = d s=1 I k1 ⊗ • • • ⊗ I ks-1 ⊗ H s ⊗ I ks+1 ⊗ • • • ⊗ I k d , b = U d s=1 b s = d s=1 U s b s = d s=1 b s 2 d s=1 e 1 .</formula><p>It is important to note that the compressed system Hy = b inherits the Kronecker product structure from the original linear system. Solution methods applicable to the original system can therefore also be applied to the compressed system; see also section 5. Note that the computational effort for building up and storing the bases of the tensorized Krylov subspace grows only linearly with the number of dimensions. Assuming that the cost for solving the compressed system admits the same growth, we therefore obtain a numerical method with an overall cost that scales linearly with d.</p><p>For small dimensions d, it might be feasible to store an explicit representation of the solution y to (3.7). In this case, the approximation x K is represented by the Tucker decomposition</p><formula xml:id="formula_51">x K = Uy = y; U 1 , U 2 , . . . , U d</formula><p>with core tensor y. If y itself is represented by a Tucker decomposition, then x K admits again a Tucker decomposition with the same core tensor as y. For high dimensions d, such a representation is not admissible, and we will discuss in section 5 how to represent (or rather approximate) y by a CP decomposition,</p><formula xml:id="formula_52">(3.8) y = t i=1 y (1) i ⊗ • • • ⊗ y (d) i .</formula><p>Then x K can also be represented by a CP decomposition:</p><formula xml:id="formula_53">x K ≈ U t i=1 y (1) i ⊗ • • • ⊗ y (d) i = t i=1 U 1 y (1) i ⊗ • • • ⊗ U d y (d) i .</formula><p>Algorithm 2 summarizes the proposed tensor Krylov subspace method for solving (1.3). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Computation of the residual.</head><p>To monitor the convergence of Algorithm 2, one can compute the norm of the residual r K = b -Ax K . The following lemma extends known results for Lyapunov and Sylvester equations <ref type="bibr" target="#b16">[17]</ref> to compute this norm in a cheaper way when x K is obtained by a Krylov subspace method.</p><p>Lemma 3.4. Let x K be computed by Algorithm 2. Then the residual r</p><formula xml:id="formula_54">K = Ax K -b satisfies r K 2 2 = d s=1 h (s) ks+1,ks 2 L≤K ls =ks y L 2 + Hy -b 2 2 .</formula><p>Proof.</p><formula xml:id="formula_55">For β ∈ {0, 1} d , we define U β = U β,1 ⊗ U β,2 ⊗ • • • ⊗ U β,d</formula><p>, where U β,s = U s if β(s) = 0, and the columns of U β,s form a basis of span(U s ) ⊥ otherwise. Note that span U β ⊥ span U γ unless β = γ. Hence R n1n2•••n d can be written as the orthogonal sum of all span U β and, therefore,</p><formula xml:id="formula_56">(3.9) r K 2 2 = β∈{0,1} d U β r 2 2 .</formula><p>For β ≡ 0, U β = U and U r K = U AUy -U b = Hy -b, the error in solving the compressed system. For general β,</p><formula xml:id="formula_57">U β AU = d s=1 U β,1 U 1 ⊗ • • • ⊗ U β,s-1 U s-1 ⊗ U β,s A s U s ⊗ U β,s+1 U s+1 ⊗ • • • ⊗ U β,d U d .</formula><p>Note that U β,j U j = 0 if β(j) = 1. Hence, U β AU = 0 if β contains more than one entry 1. Combined with the fact that U β b = 0 unless β ≡ 0, this implies that only terms corresponding to β with exactly one entry 1 contribute to the sum (3.9). Let us consider such a β s = (0, . . . , 0, 1, 0, . . . , 0) with a single entry 1 at the sth position. Then</p><formula xml:id="formula_58">U βs r K 2 2 = I ⊗ • • • ⊗ I ⊗ U β,s A s U s ⊗ I ⊗ • • • ⊗ I y 2 2 = I ⊗ • • • ⊗ I ⊗ h (s) ks+1,ks U β,s u (s) ks+1 e ks ⊗ I ⊗ • • • ⊗ I y 2 2 = h (s) ks+1,ks 2 I ⊗ • • • ⊗ I ⊗ e ks ⊗ I ⊗ • • • ⊗ I y 2 2 = h (s) ks+1,ks 2 L≤K ls =ks y L 2 , (3.10)</formula><p>where we used the Arnoldi decomposition (3.5), and U β,s u (s) ks+1 2 = 1. This completes the proof, as r 2  2 is obtained by summing up the terms (3.10) for s = 1, . . . , d. Although the expression provided by Lemma 3.4 reduces the cost for computing the residual norm significantly, it still scales exponentially with d simply because almost all elements of y need to be accessed. This exponential growth can be avoided if y is represented in a data sparse format. Consider, for example, a CP decomposition (3.8) of y. Then , where the cost for computing the latter expression scales linearly with d, assuming that t remains constant as d grows. Hence, the overall cost for evaluating r K 2 2 scales quadratically with d. Downloaded 01/04/13 to 150.135.135.70. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php 4. Convergence analysis. In the following, we will develop a convergence analysis for the tensor Krylov subspace method in special cases. It is clear that this can only be performed in a meaningful way if the unique solvability of the compressed system (3.7) is guaranteed. The following lemma is an extension of the usual positive definiteness condition in the convergence analysis of full orthogonalization methods (FOMs) for standard linear systems; see <ref type="bibr" target="#b24">[24]</ref> and the references therein.</p><p>Lemma 4.1. Given (1.3), suppose that the eigenvalues of the symmetric parts</p><formula xml:id="formula_59">(A s + A s )/2 are contained in intervals [α s , β s ]. Let U = U 1 ⊗ • • • ⊗ U d ,</formula><p>where the columns of each U s ∈ R ns×ks form an orthonormal basis. Then the compressed matrix U AU is invertible if</p><formula xml:id="formula_60">(4.1) d s=1 α s , d s=1 β s ∩ {0} = ∅.</formula><p>Proof. The Cauchy interlacing theorem implies that the eigenvalues of the compressed symmetric parts It is common to call a nonsymmetric matrix to be positive/negative definite if its symmetric part is positive/negative definite. By Lemma 2.1, the condition (4.1) is equivalent to the definiteness of A. For this condition to be satisfied, it is sufficient but not necessary 1 that all coefficients A s be either positive definite or negative definite. In particular, the compressed system is solvable in the special case when all A s are symmetric positive definite.</p><formula xml:id="formula_61">U s A s U s + U s A s U s = U s (A s + A s )U s are also contained in [α s ,</formula><p>For the development of our convergence analysis, it is central to note that the residual r K = Ax Kb, with x K produced by Algorithm 2, satisfies</p><formula xml:id="formula_62">U r K = U Ax K -U b = U AUy -b = Hy -b = 0.</formula><p>In other words, the following Galerkin condition holds:</p><formula xml:id="formula_63">Ax K -b ⊥ K ⊗ K (A, b). (4.2)</formula><p>For practical purposes, the compressed system cannot be solved exactly, but only a low rank approximation to the solution y is found. However, as this approximation error can be reduced at will by increasing the rank of y, we will assume in this section that the compressed system is solved exactly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The symmetric positive definite case. We first consider the case that</head><p>A is symmetric positive definite. This allows us to introduce the weighted Euclidean norm</p><formula xml:id="formula_64">c A := A 1/2 c 2 , c ∈ R n1n2•••ns ,</formula><p>and relate the Galerkin condition (4.2) to a linear least-squares problem. 1 On the other hand, it is easy to see that one can always shift the matrices As in a way such that each As inherits the definiteness of A but A itself is not altered. For positive definite A, Ãs = As + (ᾱαs)I with ᾱ = α 1 +• </p><formula xml:id="formula_65">x K = arg min x∈K ⊗ K (A,b) x -x A .</formula><p>Proof. This result is well known and can be proven by standard techniques (see, e.g., <ref type="bibr" target="#b21">[21]</ref>).</p><p>An immediate consequence of Proposition 4.2, the error in the A-norm of x K decreases monotonically as any of the individual dimensions k s in the tensorized Krylov subspace K ⊗ K (A, b) grows. The following theorem turns Proposition 4.2 into a multivariate polynomial approximation problem. Let us recall from section 3.1 that Π ⊗ K denotes the space of all multivariate polynomials of degree at most k s -1 in the sth variable.</p><p>Theorem 4.3. Under the assumptions of Proposition 4.2,</p><formula xml:id="formula_66">x K -x A ≤ A 2 b 2 min p∈Π ⊗ K max λs ∈Λ(As ) s=1,...,d p(λ 1 , . . . , λ d ) - 1 λ 1 + • • • + λ d .</formula><p>Proof. From Proposition 4.2, we have</p><formula xml:id="formula_67">x K -x A = min x∈K ⊗ K (A,b) x -x A ≤ A 2 min x∈K ⊗ K (A,b) x -x 2 .</formula><p>By Lemma 3.2, any tensor x in the tensorized Krylov subspace K ⊗ K (A, b) can be expressed as x = p(A 1 , . . . , A d )b, where p is a multivariate polynomial in p ∈ Π ⊗ K . Hence,</p><formula xml:id="formula_68">x K -x A ≤ A 2 b 2 min p∈Π ⊗ K p(A 1 , . . . , A d ) -A -1 2 .</formula><p>By a similarity transformation (parallel to the proof of Theorem 2.5), we find</p><formula xml:id="formula_69">p(A 1 , . . . , A d ) -A -1 2 = max λs ∈Λ(As ) s=1,...,d p(λ 1 , . . . , λ d ) - 1 λ 1 + • • • + λ d ,</formula><p>which concludes the proof. We follow the standard technique of relaxing the min-max problem of Theorem 4.3 such that the maximum is taken over the intervals [α s , β s ] := [λ min (A s ), λ max (A s )] instead of the discrete sets of eigenvalues. This can only increase the bound, and we therefore obtain</p><formula xml:id="formula_70">x K -x A ≤ A 2 b 2 E Ω (K),</formula><p>where</p><formula xml:id="formula_71">(4.3) E Ω (K) := min p∈Π ⊗ K p(μ 1 , . . . , μ d ) - 1 μ 1 + • • • + μ d Ω , with • Ω defined as the supremum norm on Ω := [α 1 , β 1 ] × • • • × [α d , β d ].</formula><p>There are several ways to approach the multivariate polynomial approximation problem (4.3). Inspired by work in <ref type="bibr" target="#b4">[5]</ref>, we have first followed an approach based on interpolation by tensor Chebyshev polynomials in <ref type="bibr" target="#b27">[27]</ref>. Unfortunately, the Lebesgue Downloaded 01/04/13 to 150.135.135.70. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php constants needed to be taken care of in this approach grow exponentially with d, leading to rather loose bounds for high dimensions. For example, if</p><formula xml:id="formula_72">k 1 = • • • = k d =: k, then a factor proportional to 2 π ln(k) d-1 is introduced by the Lebesgue constants.</formula><p>This factor can be avoided when following a completely different approach, essentially mimicking the proof of <ref type="bibr" target="#b23">[23]</ref> on a scalar level for arbitrary dimensions. Lemma A.1 in the appendix contains the approximation result obtained in this manner. Combining Theorem 4.3 with Lemma A.1 yields the following convergence bound. Corollary 4.4. Under the assumptions of Proposition 4.2,</p><formula xml:id="formula_73">x K -x A ≤ A 2 b 2 λ min (A) d s=1 √ κ s + 1 √ κ s • √ κ s -1 √ κ s + 1 ks ,</formula><p>where κ s = 1 + βs-αs λmin(A) . For d = 2, a bound similar to the bound of Corollary 4.4 has been obtained in <ref type="bibr" target="#b23">[23,</ref><ref type="bibr">Proposition 3.1]</ref>. In fact, the bound of <ref type="bibr" target="#b23">[23]</ref> has a somewhat smaller constant by avoiding the detour via multivariate polynomial approximations. In principle, the proof techniques <ref type="bibr" target="#b23">[23]</ref> could also be extended to d &gt; 2, but our approach has the advantage of also admitting convergence bounds for the extended Krylov subspace method; see section 6.</p><p>Remark 4.5. It is instructive to compare the error bound of Corollary 4.4 with the well-known error bound of the classical CG method applied to the linear system (1.3):</p><formula xml:id="formula_74">(4.4) x k -x A ≤ 2 b A κ(A) -1 κ(A) + 1 k ≤ 2 A 2 b 2 κ(A) -1 κ(A) + 1 k , where κ(A) = A 2 A -1 2 = β1+•••+β d α1+•••+α d . To simplify the discussion, assume α 1 = • • • = α d =: α and β 1 = • • • = β d =: β,</formula><p>in which case it is reasonable to choose K = (k, . . . , k). Then the bound of Corollary 4.4 simplifies to (4.5)</p><formula xml:id="formula_75">x K -x A ≤ d √ κ + 1 √ κ • A 2 b 2 λ min (A) √ κ -1 √ κ + 1 k , where κ = 1 + β-α dα = d-1 d + κ(A) d .</formula><p>For larger κ(A) this means that the effective condition number that determines the convergence rate in (4.5) is divided by d in comparison to <ref type="bibr">(4.4)</ref>. This indicates that the tensor Krylov subspace method can be expected to require 1/ √ d times the iterations needed by classical CG; see also Remark 3.3. More surprisingly, the convergence rate of the tensor Krylov subspace method appears to improve with increasing number of dimensions d, assuming that the condition number of A remains constant. This benefit from higher dimensions was already noted for d = 2 in <ref type="bibr" target="#b23">[23]</ref> and is confirmed by the numerical experiments in section 7.</p><p>Remark 4.6. To avoid unnecessary work, it is of interest to choose K = (k 1 , . . . , k d ) such that the summands in the convergence bound of Corollary 4.4 are balanced, i.e., the terms ( √ κs-1 √ κs+1 ) ks are nearly constant for all s. Taking the logarithm yields </p><formula xml:id="formula_76">k s log √ κ s -1 √ κ s + 1 ≈ -2 k s √</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">An approach to the nonsymmetric positive definite case.</head><p>To obtain convergence bounds in the case that A is nonsymmetric positive definite, we follow the proof technique by Simoncini and Druskin <ref type="bibr" target="#b23">[23]</ref> for Lyapunov equations. To simplify the presentation, it is assumed that b s 2 = 1 throughout the rest of this section.</p><p>Lemma 4.7. Let x denote the solution of (1.3), where A is positive definite. For the approximation x K obtained by Algorithm 2, it holds that</p><formula xml:id="formula_77">x K -x 2 ≤ d s=1 ∞ 0 e -αst x (s) ks -x (s)</formula><p>2 dt, where x (s) = e -tAs b s , x (s) ks = U s e -tHs e 1 , and αs = j =s α j with α j = λ min (A j + A j )/2. Proof. By Lemma 2.2, both x and x K admit integral representations:</p><formula xml:id="formula_78">x = ∞ 0 x (1) ⊗ • • • ⊗ x (d) dt, x K = ∞ 0 x (1) k1 ⊗ • • • ⊗ x (d) k d dt.</formula><p>Note that x (s) 2 ≤ e -αst as well as x (s) ks 2 ≤ e -αst . We have</p><formula xml:id="formula_79">x K -x 2 ≤ ∞ 0 d j=1 x (j) kj - d j=1 x (j) 2 dt = ∞ 0 d s=1 s-1 j=1 x (j) kj ⊗ x (s) ks -x (s) ⊗ d j=s+1 x (j) 2 dt ≤ d s=1 ∞ 0 s-1 j=1 x (j) kj 2 x (s) ks -x (s) 2 d j=s+1 x (j) 2 dt ≤ d s=1 ∞ 0 e -αst x (s) ks -x (s)</formula><p>2 dt, which concludes the proof. Note that the term x (s) ksx (s) 2 appearing in the bound of Lemma 4.7 corresponds to the approximation error of the usual Krylov subspace approximation to the matrix exponential e -tAs b s . Any reasonably good bound on this approximation error could be inserted to yield a bound on x K -x 2 . In the following, we demonstrate this procedure for the case that the fields of values F (A s ) = {w A s w : w ∈ C ns , w 2 = 1} for s = 1, . . . , d are contained in ellipses.</p><p>Theorem 4.8. In addition to the assumptions of Lemma 4.7, suppose that the field of values of each A s ∈ R ns×ns is contained in an ellipse of center (c s , 0), foci (c s ± f s , 0), and semiaxes a <ref type="bibr" target="#b0">(1)</ref> s and a <ref type="bibr" target="#b1">(2)</ref> s . Then</p><formula xml:id="formula_80">x K -x 2 ≤ d s=1 4 (α s + c s ) 2 -f 2 s ρ s ρ s -1 ρ -ks s ,</formula><p>where r s = a (1)  s +a (2)   s 2</p><p>, ρ s = cs+ αs 2rs + </p><formula xml:id="formula_81">x K -x 2 ≤ 4 d s=1 ∞ j=ks ∞ 0 e -( αs+cs)t I j (f s t) 2r s f s j dt = d s=1 4 (α s + c s ) 2 -f 2 s ∞ j=ks ρ -j s = d s=1 4 (α s + c s ) 2 -f 2 s ρ s ρ s -1 ρ -ks s .</formula><p>In a manner analogous to the proof of Theorem 4.8, the other results from <ref type="bibr" target="#b23">[23]</ref> for nonsymmetric positive definite matrices, dealing, for example, with a field of values contained in a wedge-shaped set, might be extended to arbitrary dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Solving the compressed system.</head><p>The tensor Krylov subspace method (see Algorithm 2) requires the solution of the compressed system Hy = b, having the same Kronecker product structure as the original system (1.3), with the coefficients H s in upper Hessenberg form. As mentioned in section 3.2, this system might be solved explicitly by a direct method for small dimensions, but for higher dimensions this will quickly become prohibitively expensive. The method already suggested in Remark 2.8 provides a viable alternative. An approximation y t to the exact solution y is calculated as</p><formula xml:id="formula_82">y t = t j=1 ω j λ min (H) d s=1 exp - α j λ min (H) H s bs , with 1/λ ≈ t j=1 ω j e -αjλ , λ ∈ Λ(H).</formula><p>The success of this approach depends of course crucially on the choice of the coefficients α j &gt; 0, ω j &gt; 0. Ideally, we would like to solve the min-max problem</p><formula xml:id="formula_83">min αj ,ωj ∈R + max μ∈Ω 1/μ - t j=1 ω j e -αj μ ,</formula><p>where Ω ⊆ C contains all eigenvalues of H scaled by some factor 1/ρ. Section 5.1 discusses the case of symmetric positive definite H, in which case ρ = λ min (H) and Ω = <ref type="bibr">[1, κ(H)</ref>]. For this purpose, a finite upper bound on the condition number κ(H) must be available, which can be determined from the eigenvalues of H s and from applying Lemma 2. be computed), we have Ω = {z ∈ C : (z) ≥ 1}, assuming that H has only eigenvalues in the right-half complex plane and ρ = min{ (λ) : λ ∈ Λ(H)}. This case is discussed in section 5.2. A comparison of the coefficients α j resulting in each case can be found in Figure <ref type="figure">5</ref>.1.</p><p>For both choices of coefficients, the choice of t needs to be determined in advance. We choose t such that the convergence bounds given below are not larger than a tolerance provided by the user. As this tolerance determines the overall quality of the approximation to the large linear system (1.3), it will usually be chosen rather small, say 10 -9 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Symmetric H and condition number known.</head><p>As already mentioned in section 2, the existence of coefficients α j &gt; 0, ω j &gt; 0 satisfying</p><formula xml:id="formula_84">1 μ - t j=1 ω j e -αjμ ≤ 16 exp -tπ 2 log(8R) ∀μ ∈ [1, R]</formula><p>is proved in <ref type="bibr" target="#b10">[11]</ref>. By Corollary 2.7,</p><formula xml:id="formula_85">y -y t 2 ≤ 16 λ min (H) exp -tπ 2 log(8κ(H)) b 2 .</formula><p>Unfortunately, there is no explicit formula for determining such coefficients α j &gt; 0, ω j &gt; 0. The optimal choice of coefficients can be calculated by a Remez-like algorithm for specific values of t and R. This has been performed for a large set of pairs (t, R) by Hackbusch <ref type="bibr" target="#b9">[10]</ref>, and the resulting coefficients were used in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Condition number unknown and/or nonsymmetric H.</head><p>If Ω is a subset of the right-half complex plane, the following bound from [12, Appendix D3.4.1] applies: </p><formula xml:id="formula_86">1 μ - t j=-t ω j e -αj μ ≤ C St exp | (μ)|/π exp(-π √ t) ∀μ ∈ C, (μ) ≥</formula><formula xml:id="formula_87">y -y 2t+1 2 ≤ C St κ P β exp(γ/π) exp(-π √ t) b 2 ,</formula><p>where β = min{ (λ) : λ ∈ Λ(H)}, γ = max{| (λ)| : λ ∈ Λ(H)}, and κ P is as defined in Theorem 2.5. In this case, there are explicit formulas for suitable coefficients:</p><formula xml:id="formula_88">h St = π/ √ t, α j = log exp(jh St ) + 1 + exp(2jh St ) , ω j = h St 1 + exp(-2jh St ) -1/2 for j = -t, . . . , t.</formula><p>6. An extended tensor Krylov subspace method. In many cases of practical interest, the convergence of the tensor Krylov subspace method can be enhanced by including matrix-vector products with A -1 s for all or some s, for example, by means of sparse direct factorizations. Although the computational cost for such an operation is significantly higher than a matrix-vector this may be offset by accelerated convergence. Moreover, the tensor rank of the resulting approximate solution is reduced, potentially offering advantages to subsequent computations.</p><p>In the following, we propose such an extended tensor Krylov subspace method, in the spirit of the closely related extended Krylov subspace methods for matrix functions <ref type="bibr" target="#b6">[7]</ref> and linear matrix equations <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b13">14]</ref>. The convergence of this method for approximating matrix functions has been recently discussed in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>In contrast to the tensor Krylov subspace method described in section 3, some (or all) of the matrices U s now represent orthonormal bases for the extended Krylov subspace</p><formula xml:id="formula_89">K ks (A s , b s ) := K ks (A s , b s ) + K ks+1 (A -1 s , b s ),</formula><p>assuming of course that A s is invertible. Generically, the dimension of the extended Krylov subspace is 2k s and hence U s ∈ R ns×2ks . Arnoldi-type algorithms for computing U s are discussed, for example, in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">22]</ref>. The rest of the extended tensor Krylov subspace method is identical to Algorithm 2.</p><p>For simplicity, we assume that all A s are symmetric positive definite and extended Krylov subspaces are used for all s = 1, . . . , d. Then</p><formula xml:id="formula_90">K ks (A s , b s ) = span{A -ks s b s , . . . , A -1 s b s , b s , A s b s , . . . , A ks-1 s b s } = span{ (A s )b s : ∈ L ks },</formula><p>where L ks denotes the linear space of Laurent polynomials (μ) = ks-1 j=-ks c j μ j . Similarly, for the tensorized extended Krylov subspace, it holds that</p><formula xml:id="formula_91">K ⊗ K (A, b) := span K k1 (A 1 , b 1 ) ⊗ • • • ⊗ K k d (A d , b d ) = span (A 1 , . . . , A s )b : ∈ L ⊗ ks . (6.1)</formula><p>Here, L ⊗ ks denotes the space of all multivariate Laurent polynomials</p><formula xml:id="formula_92">(μ 1 , . . . , μ d ) = -K≤L&lt;K c L μ l1 1 μ l2 2 • • • μ l d d , c L ∈ R, Downloaded<label>01</label></formula><p>/04/13 to 150.135.135.70. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php where -K ≤ L &lt; K is to be understood as -k s ≤ l s ≤ k s -1 for s = 1, . . . , d. The evaluation of at a matrix tuple (A 1 , . . . , A d ) is then-analogously to (3.2)-defined as</p><formula xml:id="formula_93">(A 1 , . . . , A d ) = -K≤L&lt;K c L A l1 1 ⊗ A l2 2 ⊗ • • • ⊗ A l d d , c L ∈ R.</formula><p>The identity (6.1) can be shown in a way similar to Lemma 3.2. Much of the convergence analysis of section 4 can be extended in a straightforward way. In particular, the convergence bound Theorem 4.3 becomes</p><formula xml:id="formula_94">x K -x A ≤ A 2 b 2 min ∈L ⊗ K max λs∈Λ(As) (λ 1 , . . . , λ d ) - 1 λ 1 + • • • + λ d ≤ A 2 b 2 min ∈L ⊗ K max μs∈[αs,βs] (μ 1 , . . . , μ d ) - 1 μ 1 + • • • + μ d , (6.2)</formula><p>where [α s , β s ] = [λ min (A s ), λ max (A s )]. To proceed further, we need to address the multivariate Laurent polynomial approximation problem <ref type="bibr">(6.3)</ref> E Ω := min</p><formula xml:id="formula_95">∈L ⊗ K (μ 1 , . . . , μ d ) - 1 μ 1 + • • • + μ d Ω ,</formula><p>where</p><formula xml:id="formula_96">• Ω denotes the supremum norm on Ω := [α 1 , β 1 ] × • • • × [α d , β d ]. Lemma 6.1. Consider E Ω defined in (6.</formula><p>3) for constant α s ≡ α, β s ≡ β, and K = (k, . . . , k). Then</p><formula xml:id="formula_97">E Ω ≤ 1 + β α 1 α √ κ + 1 √ κ • √ κ -1 √ κ + 1 k , where (i) for d = 2: α = 2 √ αβ, β = α + β, κ = (α + β)/(2 α); (ii) for d &gt; 2: α = d 2 (d -1) 1-d d α d-1 d β 1 d , β = d(α + β), κ = β/α. Proof. (i) Setting ξ s (μ s ) = μ s + γ/μ s for s = 1, 2, we have (6.4) 1 μ 1 + μ 2 = 1 + γμ -1 1 μ -1 2 1 ξ 1 (μ 1 ) + ξ 2 (μ 2 )</formula><p>.</p><p>The choice γ = αβ balances the maximal value of ξ s at the boundaries of the interval:</p><formula xml:id="formula_98">ξ s (α) = ξ s (β) = α + β = β. It is straightforward to see that the minimum of ξ s is attained at √ γ with ξ s ( √ γ) = 2 √ αβ = α. Hence α ≤ ξ s (μ s ) ≤ β for μ s ∈ [α, β]. By Lemma A.1, there is a bivariate polynomial p(ξ 1 , ξ 2 ) ∈ Π ⊗ K such that E Ω := p(ξ 1 , ξ 2 ) - 1 ξ 1 + ξ 2 Ω ≤ 1 α √ κ + 1 √ κ • √ κ -1 √ κ + 1 k ,</formula><p>where Ω = [α, β] d , and κ is defined as in the statement of the lemma. By (6.4), the bivariate Laurent polynomial (μ 1 , μ</p><formula xml:id="formula_99">2 ) := 1 + γμ -1 1 μ -1 2 p ξ 1 (μ 1 ), ξ 2 (μ 2 ) ∈ L ⊗ K satisfies E Ω ≤ (μ 1 , μ 2 ) - 1 μ 1 + μ 2 Ω ≤ 1 + γμ -1 1 μ -1</formula><p>2 Ω E Ω = (1 + β/α)E Ω , Downloaded 01/04/13 to 150.135.135.70. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php which concludes the proof of (i).</p><p>(ii) Trivially, (μ Since grad ξ = 0 at μ s ≡ (d-1)γ 1/d , the minimum of ξ is given by d 2 (d-1)</p><formula xml:id="formula_100">1 + • • • + μ d ) -1 = (1 + γ d s=1 μ -1 s )ξ(μ 1 , . . . , μ d ) -1 ,</formula><p>1-d d γ 1/d = α. By Lemma A.1, there is a polynomial p of degree at most k -1 such that</p><formula xml:id="formula_101">E Ω := p(ξ) -ξ -1 [ α, β] ≤ 1 α √ κ + 1 √ κ • √ κ -1 √ κ + 1 k .</formula><p>Similarly as in the proof of (i), the multivariate Laurent polynomial (μ 1 , . . . , μ d ) :=</p><formula xml:id="formula_102">(1 + γ d s=1 μ -1 s )p(ξ(μ 1 , . . . , μ d )) ∈ L ⊗ K yields E Ω ≤ (μ 1 , . . . , μ d ) - 1 μ 1 + • • • + μ d Ω ≤ 1 + γ d s=1 μ -1 s Ω E Ω = 1 + β/α E Ω ,</formula><p>which concludes the proof. For d = 2, the factor κ that determines the asymptotics of the convergence bound in Lemma 6.1 satisfies κ ≈ κ(A)/4 for β α. This compares favorably with the factor κ = κ(A)/2 that determines the asymptotics of the convergence bound (4.5) for the (standard) tensor Krylov subspace method. The linear convergence rate of the extended Krylov subspace method for solving Sylvester equations is therefore bounded by √ κ ≈ κ(A) 1/4 /2, which was also observed experimentally in <ref type="bibr" target="#b22">[22]</ref>. For d = 3, κ ≈ 2κ(A) 2/3 /3 for β α. As d increases, the bound of Lemma 6.1 becomes more pessimistic: κ d→∞ → 1 + κ(A). It is not clear to us whether the bound of Lemma 6.1 could be improved to also reflect the significantly better convergence of the extended tensor Krylov subspace method we observed for higher dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Numerical experiments.</head><p>We have implemented the (extended) tensor Krylov subspace methods in MATLAB, using the Tensor Toolbox <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> for storing tensors in CP decomposition and for multiplying matrices with tensors. For solving the compressed systems, the coefficients described in section 5 are used. A tolerance of ε = 10 -9 is chosen as an upper bound on the accuracy of the solution to the compressed system. </p><formula xml:id="formula_103">A s = 1 h 2 ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ 2 -1 -1 2 -1 . . . . . . . . . -1 2 -1 -1 2 ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ , b s = ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ g(z (s) 1 )</formula><p>. . .</p><formula xml:id="formula_104">g(z (s) n ) ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ .</formula><p>It is well known that the minimal and maximal eigenvalues of A s are given by</p><formula xml:id="formula_105">λ min = 2 h 2 1 -cos π n s + 1 , λ max = 2 h 2 1 -cos πn s n s 1 ,</formula><p>resulting in the convergence factor (see Corollary 4.4)</p><formula xml:id="formula_106">(7.1) κ = 1 + λ max -λ min dλ min = 1 + 2 cos( π n+1 ) d(1 -cos( π n+1 )) ≈ 4(n + 1) 2 π 2 d .</formula><p>For simplicity, we have used right-hand side vectors b s composed of uniformly distributed pseudorandom numbers.</p><p>The tensor Krylov subspace method was used with multi-index K = (k, . . . , k), as the size n s and condition number are identical for all A s . All convergence plots display the convergence of the relative residual Ax kb 2 / b 2 . As the solution x k cannot be stored explicitly, the norm of the residual must be calculated directly from its CP decomposition. For this purpose, an efficient method is proposed in section 3.3. As Lemma 3.4 shows, the error is ultimately bounded by the residual induced by the approximation error from the compressed system. Figures 7.1 and 7.2 show the convergence of the tensor Krylov subspace method for systems of size n s = 200 and n s = 1000, respectively, with s = 1, . . . , d and various dimensions d. The observed convergence rates correspond reasonably well to the theoretically predicted convergence rates; in particular, the convergence rates improve for higher dimensions. The plots in Figures 7.1 and 7.2 are remarkably similar apart from the different scaling of the x-axis, caused by the increase of the condition number as n s grows. The convergence curves also nicely confirm the fact that x k = x holds for k ≡ n s up to the approximation error from the compressed system. In Figure <ref type="figure">7</ref>.3, we apply the extended tensor Krylov subspace method to a system of size n s = 200. At k = 40, the method converges up to the approximation error in the compressed system for all dimensions. The observed convergence is significantly better than the convergence rate   where f is again a separable function. A standard finite-difference discretization on equidistant nodes, combined now with a second order convergent scheme for the convection term, leads to</p><formula xml:id="formula_107">A s = 1 h 2 ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ 2 -1 -1 2 -1 . . . . . . . . . -1 2 -1 -1 2 ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ + c s 4h ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ 3 -5 1 1 3 -5 . . . . . . . . . . . . 1 1 3 -5 1 3 ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ .</formula><p>Note that A is identical to the system matrix in [9, <ref type="bibr">Example 22]</ref>. Again, the righthand side is composed of vectors b s containing uniformly distributed pseudorandom numbers. Since we have chosen the system size n s = 200 to be constant over all dimensions, it is again reasonable to choose K = {k, . . . , k}. Figure <ref type="figure">7</ref>.4 displays the convergence of the tensor Krylov subspace method applied to this example for various dimensions. The convection coefficients are c s = 10 for the left plot and significantly larger, c s = 100, for the right plot. The convergence is significantly slower compared to the symmetric case but the qualitative behavior is similar: the convergence rate improves for higher dimensions. As expected, the convergence rate slightly deteriorates as the convection coefficient grows.</p><p>We have also performed experiments with the extended tensor Krylov subspace method applied to the nonsymmetric example. The observed convergence rates improve similarly as for the symmetric example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions.</head><p>A tensor Krylov subspace method that deals well with highdimensional linear systems that exhibit a certain, rather particular Kronecker product structure has been proposed. We envision the methods developed in this paper as building blocks for addressing more general high-dimensional problems in future algorithms. For example, they could be used in a preconditioned inverse iteration for high-dimensional PDE eigenvalue problems <ref type="bibr" target="#b12">[13]</ref>.</p><p>Appendix. The following lemma contains the approximation result needed in section 4 for the convergence analysis of the tensor Krylov subspace method. Downloaded 01/04/13 to 150.135.135.70. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Lemma A.1. For the multivariate polynomial approximation error E Ω (K) defined in <ref type="bibr">(4.3)</ref>, it holds that</p><formula xml:id="formula_108">E Ω (K) ≤ 1 λ min (A) d s=1 √ κ s + 1 √ κ s • √ κ s -1 √ κ s + 1 ks ,</formula><p>where κ s = 1 + βs-αs λmin(A) . Proof. Since A is assumed to be symmetric positive definite, α 1 + • • • + α d &gt; 0, and hence, every (μ 1 , . . . , μ d ) ∈ Ω also satisfies μ 1 + • • • + μ d &gt; 0. This allows us to write   </p><formula xml:id="formula_109">1 μ 1 + • • • + μ d = ∞ 0 e -(μ1+</formula><formula xml:id="formula_110">E s (k s ) = 2 λ min (A) √ κ s ∞ j=ks √ κ s -1 √ κ s + 1 j = √ κ s + 1 λ min (A) √ κ s • √ κ s -1 √ κ s + 1 ks ,</formula><p>which, together with (A.4), concludes the proof.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 . 1 .</head><label>21</label><figDesc>Fig. 2.1. Comparison of the convergence bounds (2.10) and (2.11), assuming λ min (A) = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 3 . 7 )</head><label>37</label><figDesc>Hy = b, with H = U AU and b = U b.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 2 .</head><label>2</label><figDesc>Tensor Krylov subspace method. Input: Coefficients A s ∈ R ns×ns and b s ∈ R ns of the linear system (1.3). Output: Approximation x K = Uy to the solution x of (1.3). for s = 1 to d do Apply Algorithm 1 to A s and b s to compute U s , H s . end for Compute/approximate solution y to the compressed equation Hy = b in (3.7). Downloaded 01/04/13 to 150.135.135.70. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>β s ]. Combined with Lemma 2.1, this shows that any eigenvalue of U AU + U A U can be written as μ = d s=1 μ s , μ s ∈ [α s , β s ] and, therefore, μ ∈ d s=1 α s , d s=1 β s . Then (4.1) implies that the set of all such μ is either negative or positive. Hence, the symmetric part of U AU is positive or negative definite, which concludes the proof.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 .Fig. 5 . 1 .</head><label>151</label><figDesc>Fig. 5.1. Coefficients α j proposed in sections 5.2 (leftmost plot) and 5.1 (three rightmost plots, for κ(H) = 10, 10 4 , 10 8 ), for t = 3, 5, . . . , 11 coefficients.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>1 j</head><label>1</label><figDesc>with ξ(μ 1 , . . . , μ d ) = 1 + γ for some γ &gt; 0. Note that ξ is a sum of convex functions and is therefore itself convex. The maximum of ξ is attained at the extrema of the convex polytope [α, β] d . The choice γ = α d-1 β approximately balances the values the extrema μ s ≡ α and μ s ≡ β, which leads to max μs∈{α,β} ξ(μ 1 , . . . , μ d ) = ξ(α, . . . , α) = d(α + β) = β.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>7. 1 .</head><label>1</label><figDesc>Symmetric example. As a first example, we consider the d-dimensional Poisson equation (1.6) with separable right-hand side f (μ 1 , μ 2 , . . . , μ d ) = g(μ 1 )g(μ 2 ) • • • g(μ d ). A standard finite-difference discretization on equidistant nodes leads to the Downloaded 01/04/13 to 150.135.135.70. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php linear system Ax = b, with</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>kFig. 7 . 1 .Fig. 7 . 2 .</head><label>7172</label><figDesc>Fig. 7.1. Relative residual of the tensor Krylov subspace method for the symmetric example and ns = 200 (left). Predicted convergence rate √ κ-1 √ κ+1 k (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 . 3 .</head><label>73</label><figDesc>Fig. 7.3. Relative residual of the extended tensor Krylov subspace method for the symmetric example and ns = 200 (left). Predicted convergence rate √ κ-1 √ κ+1 k (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>7. 2 .Fig. 7 . 4 .</head><label>274</label><figDesc>Fig. 7.4. Relative residual of the tensor Krylov subspace method for the nonsymmetric example and ns = 200, with factor cs = 10 (left) and cs = 100 (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>2 t 2 )t 2I j β s -α s 2 ,</head><label>222</label><figDesc>(s) ks (α s , t) = e -αs +βs j (t) ≤ e -αs +βs 2 t ∞ j=0 a (s) j (t) = e -αst . Downloaded 01/04/13 to 150.135.135.70. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php DANIEL KRESSNER AND CHRISTINE TOBLER Hence,ks (μ s , t) Ω e -(λmin(A)-αs)t . (A.3)To use these results for our multivariate interpolation problem, we setp(μ 1 , . . . , μ d ) = I≤K c I p (1) i1 (μ 1 )p (2) i2 (μ 2 ) • • • p (d) i d (μ d ),where I = (i 1 , . . . , i d ) with 1 ≤ i s ≤ k s , andp (s) is (μ s ) = T is-1 (-g s (μ s )), c I = is-1 (t) dt.With this choice,E Ω (K) ≤ p(μ 1 , . . . , μ s ) -1 μ 1 + • • • + μ s Ω λmin(A)-αs)t e -μstp (s)ks (μ s , t) Ω dt Downloaded 01/04/13 to 150.135.135.70. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.phpUsing ∞ 0 e -δt I j (γt) dt = γ j √ δ 2 -γ 2 (δ+ √ δ 2 -γ 2) j , which holds for δ &gt; γ<ref type="bibr" target="#b23">[23]</ref> and j = 0, we obtain after some algebraic manipulation ∞ 0 e -(λmin(A)+ βs-αs with κ s = 1 + βs-αs λmin(A) . Hence,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>n1×n2×</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>∈ R ns . Downloaded 01/04/13 to 150.135.135.70. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Note that this implies that the element at the multi-index I</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>2.2. Low tensor rank approximations. Solving</head><label></label><figDesc>Note that the order in which the index s is evaluated is important, as the Kronecker product does not commute. Downloaded 01/04/13 to 150.135.135.70. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php (1.3) for larger d requires us to work with a data sparse representation of x. For this purpose, x will be approximated by a low rank tensor. The following theorem provides a fundamental connection between approximations of x by low rank tensors and separable approximations to the reciprocal of a sum of d variables.</figDesc><table><row><cell>DANIEL KRESSNER AND CHRISTINE TOBLER</cell></row><row><cell>Theorem 2.5. Consider the linear system (1.3) with coefficient matrices A s ∈</cell></row><row><cell>R ns×ns , and assume that the system is solvable and that the matrices A s are diago-nalizable: P -1 (s) s A s P s = Λ s , with invertible P s and diagonal Λ s . Let f j</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>• • • e -αiμ d /λmin(A)   </figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">≤ 16 exp</cell><cell>-kπ 2 log(8κ(A))</cell><cell>,</cell></row><row><cell cols="5">yielding a bound on the quantity (k) defined in (2.8).</cell><cell></cell></row><row><cell cols="7">Corollary 2.7 shows that the solution x can be well approximated by a low rank</cell></row><row><cell cols="7">tensor, provided that A is symmetric positive definite. In comparison, the bound in</cell></row><row><cell cols="2">[12, Appendix D3.4.1] yields</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(2.11)</cell><cell>x -x 2k+1 2 ≤</cell><cell>C St λ min (A)</cell><cell>exp(-π</cell><cell>√</cell><cell>k) b 2 ,</cell></row><row><cell cols="7">where C St is independent of A and k. Experimentally, we found C St ≈ 2.75. It</cell></row><row><cell cols="7">is important to emphasize that the convergence rate predicted by (2.11) does not</cell></row><row><cell cols="7">depend on the condition number of A. However, this comes at the expense of having √ k instead of k in the exponent. It is therefore of interest to compare (2.11) with the</cell></row><row><cell cols="7">bound of Corollary 2.7 for different κ(A); see Figure 2.1. It turns out that the bound</cell></row><row><cell cols="7">of Corollary 2.7 is often significantly better, except for very large values of κ(A) and</cell></row><row><cell>smaller k.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>Remark 2.8. Note that Lemma 2.2 also suggests an algorithm for calculating x k :</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>1 and nontrivial choices of A, b. To summarize: Tensorized Krylov subspaces are richer than standard Krylov subspaces. This is no surprise when taking into account that the dimension of K ⊗ Downloaded 01/04/13 to 150.135.135.70. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Algorithm 1. Arnoldi method. Input</figDesc><table /><note><p>K (A, b) grows exponentially with d while the dimension of K k0 (A, b) grows only linearly with d.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>••+α d Downloaded 01/04/13 to 150.135.135.70. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php DANIEL KRESSNER AND CHRISTINE TOBLER Proposition 4.2. Let x denote the solution of (1.3), where A is symmetric positive definite. Then the Galerkin condition (4.2) for an approximation x K implies</figDesc><table /><note><p>d is such a shift.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>κ s Downloaded 01/04/13 to 150.135.135.70. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php for large κ s . Hence it is reasonable to choose K = (k 1 , . . . , k d ) such that ks √ κs is nearly constant across all dimensions s.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Downloaded 01/04/13 to 150.135.135.70. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Proof. By the proof of Proposition 4.1 in [23],</figDesc><table><row><cell>x (s) ks -x (s)</cell><cell>2 ≤ 4</cell><cell>∞ j=ks</cell><cell>e -cst I j (f s t)</cell><cell>2r s f s</cell><cell>j</cell><cell>,</cell></row><row><cell cols="7">where I j denotes the jth modified Bessel function; see also the proof of Lemma A.1.</cell></row><row><cell cols="2">Combined with Lemma 4.7, this yields</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>1 2rs (c s + αs ) 2f 2 s for s = 1, . . . , d.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>1, Downloaded 01/04/13 to 150.135.135.70. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php where C St does not depend on t or μ. This yields</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>••+μ d )t dt = j denotes the jth modified Bessel function. We define polynomials p (s) ks (μ s , t) in μ s by truncating the series (A.1) after the (k s -1)th term. Obviously,</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>∞</cell><cell>d</cell><cell>e -μst dt.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>s=1</cell></row><row><cell cols="9">Following [8, 23], we expand e -μst in terms of Chebyshev series:</cell></row><row><cell cols="2">(A.1)</cell><cell cols="5">e -μst = e -αs +βs 2</cell><cell>t</cell><cell>∞</cell><cell>a</cell><cell>(s)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>j=0</cell></row><row><cell cols="5">The Chebyshev coefficients a</cell><cell>(s) j</cell><cell cols="3">take the form</cell></row><row><cell></cell><cell></cell><cell></cell><cell>a</cell><cell cols="2">(s) j (t) =</cell><cell>I j 2I j</cell><cell cols="2">βs-αs 2 βs-αs t , 2 t</cell><cell>j = 0, otherwise,</cell></row><row><cell cols="2">where I (A.2)</cell><cell>p</cell><cell cols="5">(s) ks (μ s , t) -e -μst</cell><cell>Ω ≤ e -αs +βs 2</cell><cell>t</cell><cell>∞</cell><cell>a</cell><cell>(s) j (t).</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>j=ks</cell></row><row><cell cols="2">Moreover,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>p</cell><cell>(s)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p><p>j (t) T j (-g s (μ s )),</p>where T j denotes the jth Chebyshev polynomial of the first kind, and the parameter transformations g s : [α s , β s ] → [-1, 1] are defined as</p>g s (μ s ) = 2 β sα s μ s -β s + α s β sα s . ks (μ s , t) Ω ≤ p</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Downloaded 01/04/13 to 150.135.135.70. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. The authors thank Lars Grasedyck for helpful and inspiring discussions on the subject of this paper and both referees for carefully checking the paper and providing a number of helpful remarks.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>electronically March 10, 2010. This research was supported by the SNF research module Preconditioned methods for large-scale model reduction within the SNF ProDoc Efficient Numerical Methods for Partial Differential Equations.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficient MATLAB computations with sparse and factored tensors</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="205" to="231" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<ptr target="http://csmr.ca.sandia.gov/∼tgkolda/TensorToolbox/" />
		<title level="m">MATLAB Tensor Toolbox Version 2.3</title>
		<imprint>
			<date type="published" when="2009-07">July 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Error Estimation and Evaluation of Matrix Functions via the Faber Transform</title>
		<author>
			<persName><forename type="first">B</forename><surname>Beckermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Reichel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Lille, France</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Université de Lille</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Algorithms for numerical analysis in high dimensions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Beylkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Mohlenkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2133" to="2159" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">H 2 -matrices -An Efficient Tool for the Treatment of Dense Matrices, Habilitationsschrift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Börm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<pubPlace>Kiel, Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Christian-Albrechts-Universität zu Kiel</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Approximation of 1/x by exponential sums in [1, ∞)</title>
		<author>
			<persName><forename type="first">D</forename><surname>Braess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hackbusch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IMA J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="685" to="697" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extended Krylov subspaces: Approximation of the matrix square root and related functions</title>
		<author>
			<persName><forename type="first">V</forename><surname>Druskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Knizhnerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="755" to="771" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Two polynomial methods for calculating functions of symmetric matrices</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">L</forename><surname>Druskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Knizhnerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zh. Vychisl. Mat. Mat. Fiz</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1763" to="1775" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Existence and computation of low Kronecker-rank approximations for large linear systems of tensor product structure</title>
		<author>
			<persName><forename type="first">L</forename><surname>Grasedyck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing</title>
		<imprint>
			<biblScope unit="page" from="247" to="265" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Approximation of 1/x by Exponential Sums</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hackbusch</surname></persName>
		</author>
		<ptr target="http://www.mis.mpg.de/scicomp/EXPSUM/1x/tabelle" />
		<imprint>
			<date type="published" when="2008-08">August 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Hackbusch</surname></persName>
		</author>
		<ptr target="http://www.mis.mpg.de/preprints/tr/report-0405.pdf" />
		<title level="m">Entwicklungen nach Exponentialsummen</title>
		<meeting><address><addrLine>Leipzig, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>Max-Planck-Institut für Mathematik in den Naturwissenschaften</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Hackbusch</surname></persName>
		</author>
		<title level="m">Hierarchische Matrizen: Algorithmen und Analysis</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Use of Tensor Formats in Elliptic Eigenvalue Problems</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hackbusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Khoromskij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Sauter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Tyrtyshnikov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<pubPlace>Leipzig, Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Max-Planck-Institut für Mathematik in den Naturwissenschaften</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Preprint 78</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Extended Arnoldi methods for large Sylvester matrix equations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heyouni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">L.M.P.A</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Calais, France</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<title level="m">Topics in Matrix Analysis</title>
		<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Krylov-subspace methods for the Sylvester equation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Reichel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra Appl</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="283" to="313" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Krylov subspace methods for solving large Lyapunov equations</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Jaimoukha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Kasenally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="227" to="251" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A New Investigation of the Extended Krylov Subspace Method for Matrix Function Evaluations</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Knizhnerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Simoncini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Linear Algebra Appl</title>
		<imprint/>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Redistribution subject to SIAM license or copyright</title>
		<idno>Downloaded 01/04/13 to 150.135.135.70</idno>
		<ptr target="http://www.siam.org/journals/ojsa.phpDANIELKRESSNERANDCHRISTINETOBLER" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tensor decompositions and applications</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="455" to="500" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Numerical solution of large Lyapunov equations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Saad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing, Scattering and Operator Theory, and Numerical Methods</title>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="503" to="511" />
		</imprint>
	</monogr>
	<note>Birkhäuser Boston</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Saad</surname></persName>
		</author>
		<title level="m">Iterative Methods for Sparse Linear Systems</title>
		<meeting><address><addrLine>SIAM, Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A new iterative method for solving large-scale Lyapunov matrix equations</title>
		<author>
			<persName><forename type="first">V</forename><surname>Simoncini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1268" to="1288" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convergence analysis of projection methods for the numerical solution of large Lyapunov equations</title>
		<author>
			<persName><forename type="first">V</forename><surname>Simoncini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Druskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="828" to="843" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Field-of-values analysis of preconditioned iterative methods for nonsymmetric elliptic problems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Starke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Math</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="103" to="117" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Numerical methods based on sinc and analytic functions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Stenger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Springer Ser. Comput. Math.</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<date type="published" when="1993">1993</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Matrix Algorithms</title>
		<imprint>
			<biblScope unit="volume">II</biblScope>
			<date type="published" when="2001">2001</date>
			<pubPlace>Eigensystems, SIAM, Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Tobler</surname></persName>
		</author>
		<title level="m">Krylov Subspace Methods for Large Linear Systems with Tensor Product Structure, Master&apos;s thesis</title>
		<meeting><address><addrLine>Zürich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>ETH Zürich</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
