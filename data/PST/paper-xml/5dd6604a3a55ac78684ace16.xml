<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Heterogeneous Deep Graph Infomax</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-02-04">4 Feb 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuxiang</forename><surname>Ren</surname></persName>
							<email>yuxiang@ifmlab.org</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IFM lab</orgName>
								<orgName type="institution">Florida State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">JD Finance America Corporation</orgName>
								<address>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
							<email>chuang7@nd.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">JD Finance America Corporation</orgName>
								<address>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Dai</surname></persName>
							<email>peng.dai@jd.com</email>
							<affiliation key="aff1">
								<orgName type="institution">JD Finance America Corporation</orgName>
								<address>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
							<email>liefeng.bo@jd.com</email>
						</author>
						<author>
							<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
							<email>jiawei@ifmlab.org</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IFM lab</orgName>
								<orgName type="institution">Florida State University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">JD Finance America Corporation</orgName>
								<address>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Heterogeneous Deep Graph Infomax</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-02-04">4 Feb 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1911.08538v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph representation learning is to learn universal node representations that preserve both node attributes and structural information. The derived node representations can be used to serve various downstream tasks, such as node classification and node clustering. Inspired by the emerging mutual information-based learning algorithm, in this paper we propose an unsupervised graph neural network Heterogeneous Deep Graph Infomax (HDGI ) for heterogeneous graph representation learning. We use the meta-path to model the structure involving semantics in heterogeneous graphs and utilize graph convolution module and semantic-level attention mechanism to capture individual node local representations. By maximizing the local-global mutual information, HDGI effectively learns highlevel node representations. Experiments show that HDGI remarkably outperforms state-of-the-art unsupervised graph representation learning methods. We even achieve comparable performance in node classification tasks when comparing with state-ofthe-art supervised end-to-end GNN models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Numerous real-world applications, such as social networks <ref type="bibr">[Zhang, 2018]</ref> and knowledge graphs <ref type="bibr" target="#b8">[Wang et al., 2017a]</ref> exhibit the favorable property of graph data structure. Meanwhile, handling graph data is very challenging. Because each node has its unique attributes, and the connections between nodes convey important information. When learning from the attributes of individual nodes and the connection information among them simultaneously, the task becomes more challenging.</p><p>Traditional machine learning methods focus on the features of individual nodes, which obstructs their ability to process graph data. Graph neural networks (GNNs) for representation learning of graphs learn nodes' new feature vectors through a recursive neighborhood aggregation scheme <ref type="bibr" target="#b11">[Xu et al., 2018]</ref>. With the support of sufficient training samples, a rich body of supervised graph neural network models have been <ref type="bibr">developed [Kipf and Welling, 2017a;</ref><ref type="bibr">Veličković et al., 2017;</ref><ref type="bibr" target="#b12">You et al., 2018]</ref>. However, labeled data is not always available in graph representation learning tasks, and those algorithms are not applicable to the unsupervised learning settings. To alleviate the training sample scarcity problem, unsupervised graph representation learning has aroused extensive research interest. The goal of this task is to learn a lowdimensional representation of each graph node. The representation preserves graph topological structure and node content. Meanwhile, the learned representations can be applied to conventional sample-based machine learning algorithms as well.</p><p>Most of the existing unsupervised graph representation learning models can be roughly grouped into factorization-based models and edge-based models. Specifically, factorization-based models capture the global graph information by factorizing the sample affinity matrix <ref type="bibr" target="#b12">[Zhang et al., 2016;</ref><ref type="bibr" target="#b12">Yang et al., 2015;</ref><ref type="bibr">Zhang et al., 2016]</ref>. Those methods tend to ignore the node attributes and local neighborhood relationships, which usually contain important information. Edge-based models exploit the local and higher-order neighborhood information by edge connections or random-walk paths. Nodes tend to have similar representations if they are connected or co-occur in the same path <ref type="bibr">[Kipf and Welling, 2017b;</ref><ref type="bibr" target="#b1">Duran and Niepert, 2017;</ref><ref type="bibr" target="#b3">Hamilton et al., 2017a;</ref><ref type="bibr" target="#b6">Perozzi et al., 2014]</ref>. Edge-based models are prone to preserve limited order node proximity and lack a mechanism to preserve the global graph structure. The recently proposed deep graph infomax (DGI) <ref type="bibr" target="#b7">[Veličković et al., 2019</ref>] model provides a novel direction that considers both global and local graph structure. DGI maximizes the mutual information between graph patch representations and the corresponding high-level summaries of graphs. It has shown competitive performance even compared with supervised graph neural networks in benchmark homogeneous graphs.</p><p>In this paper, we explore the mutual information-based learning framework in heterogeneous graph representation problems. The networked data in the real world usually contain complex structures (involving multiple types of nodes and edges), which can be formally modeled as heterogeneous graphs (HG). Compared with homogeneous graphs, heterogeneous graphs contain more detailed information and rich semantics among multi-typed nodes. Taking the bibliographic network in Figure <ref type="figure" target="#fig_0">1</ref> as an example, it contains three types of nodes (Author, Paper and Subject) as well as two types of edges (Write and Belong-to). Besides, the individual nodes themselves also carry abundant attribute information (e.g., paper textual contents). Due to the diversity of node and edge types, the heterogeneous graph itself becomes more complex, and the diverse (direct or indirect) connections between nodes also convey more semantic information. In heterogeneous graph studies, meta-path <ref type="bibr" target="#b6">[Sun et al., 2011]</ref> has been widely used to represent the composite relations with different semantics. As illustrated in Figure <ref type="figure" target="#fig_0">1</ref>(d), the relations between paper can be expressed by PAP and PSP, which represent papers written by the same author and papers belonging to the same subject, respectively. GNNs initially proposed for homogeneous graphs may encounter great challenges to handle relations with different semantics in heterogeneous graphs.</p><p>To address the above challenges, we propose a novel metapath based unsupervised graph neural network model for heterogeneous graphs, namely Heterogeneous Deep Graph Infomax (HDGI ). In summary, our contributions in this paper can be summarized as follows:</p><p>• This paper presents the first model to apply mutual information maximization to representation learning in heterogeneous graphs. • Our proposed method, HDGI , is a novel unsupervised graph neural network. It handles graph heterogeneity by utilizing an attention mechanism on meta-paths and deals with the unsupervised settings by applying mutual information maximization.</p><p>• Our experiments demonstrate that the representations learned by HDGI are effective for both node classification and clustering tasks. Moreover, its performance can also beat state-of-the-art GNN models, where they have the additional supervised label information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Graph representation learning. Graph representation learning has become a non-trivial topic <ref type="bibr">[Cui et al., 2018]</ref> because of the ubiquity of graphs in the real world. As a data type containing rich structural information, many models <ref type="bibr" target="#b3">[Grover and Leskovec, 2016;</ref><ref type="bibr" target="#b6">Tang et al., 2015]</ref> acting on graphs learn the representations of nodes based on the structure of the graph.</p><p>DeepWalk <ref type="bibr" target="#b6">[Perozzi et al., 2014]</ref> uses the set of random walks over the graph in SkipGram to learn node embeddings. Several methods <ref type="bibr" target="#b6">[Ou et al., 2016;</ref><ref type="bibr" target="#b8">Wang et al., 2017b]</ref> attempt to retrieve structural information through the matrix factorization. However, all the above methods are proposed for homogeneous graphs. Heterogeneous graph learning. In order to handle the heterogeneity of graphs, metapath2vec <ref type="bibr" target="#b0">[Dong et al., 2017]</ref> samples random walks under the guidance of meta-paths and learns node embeddings through the skip-gram in heterogeneous graphs. HIN2Vec <ref type="bibr" target="#b2">[Fu et al., 2017]</ref> learns the embedding vectors of nodes and meta-paths simultaneously while conducts prediction tasks. <ref type="bibr" target="#b9">Wang et al. [Wang et al., 2019]</ref> consider the attention mechanism in heterogeneous graph learning, where information from multiple meta-path defined connections can be learned effectively. From the perspective of attributed graphs, SHNE <ref type="bibr" target="#b14">[Zhang et al., 2019]</ref> captures both structural closeness and unstructured semantic relations through joint optimization of heterogeneous Skip-Gram and deep semantic encoding. Many learning methods <ref type="bibr" target="#b6">[Schlichtkrull et al., 2018;</ref><ref type="bibr" target="#b8">Wang et al., 2017a]</ref> on knowledge graphs can often be applied to other heterogeneous graphs.</p><p>Graph neural network. With the success of deep learning in the recent period, graph neural networks (GNNs) <ref type="bibr" target="#b12">[Zhang, 2019]</ref> have made a lot of progress in graph representation learning. The core idea of GNN is to aggregate the feature information of the neighbors through neural networks to learn the new features that combine the independent information of the node and corresponding structural information in the graph. Most successful GNNs are based on supervised learning including <ref type="bibr">GCN [Kipf and Welling, 2017a]</ref>, GAT <ref type="bibr">[Veličković et al., 2017]</ref>, <ref type="bibr">GraphRNN [You et al., 2018]</ref>, <ref type="bibr">SplineCNN [Fey et al., 2018]</ref>, <ref type="bibr">AdaGCN [Sun et al., 2019]</ref> and AS-GCN <ref type="bibr" target="#b5">[Huang et al., 2018]</ref>. The unsupervised learning GNNs can be mainly divided into two categories, i.e., random walk-based <ref type="bibr" target="#b6">[Perozzi et al., 2014;</ref><ref type="bibr" target="#b3">Grover and Leskovec, 2016;</ref><ref type="bibr">Kipf and Welling, 2017b;</ref><ref type="bibr" target="#b1">Duran and Niepert, 2017;</ref><ref type="bibr" target="#b3">Hamilton et al., 2017a]</ref> and mutual information-based <ref type="bibr" target="#b7">[Veličković et al., 2019]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Formulation</head><p>In this section, we first introduce the concept of heterogeneous graph and the problem definition of heterogeneous graph representation learning. Next we define meta-path based adjacency matrix, which is useful in the following algorithm description. Definition 3.1 (Heterogeneous Graph (HG)) A heterogeneous graph is defined as G = (V, E) with a node type mapping function φ : V → T and an edge type mapping function ψ : E → R. Each node v ∈ V belongs to one particular node type in the node type set T : φ(v) ∈ T , and each edge e ∈ E belongs to a particular edge type in the edge type set R : ψ(e) ∈ R. Heterogeneous graphs have the property that |T | + |R| &gt; 2. The attributes and content of nodes can be encoded as initial feature matrix X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D</head><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = "</p><formula xml:id="formula_0">3 t t f G u Z Z 2 1 m Y v + 6 l 8 y + b F g j 1 s J 4 = " &gt; A A A B 9 H i c b V B N T w I x F H z F L 8 Q v 1 K O X R j D x R H b x o E e i H j x i I k g C G 9 I t X W j o d t e 2 S 0 I 2 / A 4 v H j T G q z / G m / / G L u x B w U m a T G b e y 5 u O H w u u j e N 8 o 8 L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q 6 y h R l L V o J C L V 8 Y l m g k v W M t w I 1 o k V I 6 E v 2 K M / v s n 8 x w l T m k f y w U x j 5 o V k K H n A K T F W 8 q q 9 k J g R J S K 9 n V X 7 5 Y p T c + b A q 8 T N S Q V y N P v l r 9 4 g o k n I p K G C a N 1 1 n d h 4 K V G G U 8 F m p V 6 i W U z o m A x Z 1 1 J J Q q a 9 d B 5 6 h s + s M s B B p O y T B s / V 3 x s p C b W e h r 6 d z D L q Z S 8 T / / O 6 i Q m u v J T L O D F M 0 s W h I B H Y R D h r A A + 4 Y t S I q S W E K m 6 z Y j o i i l B j e y r Z E t z l L 6 + S d r 3 m X t T q 9 / V K 4 z q v o w g n c A r n 4 M I l N O A O m t A C C k / w D K / w h i b o B b 2 j j 8 V o A e U 7 x / A H 6 P M H M s 6 R u A = = &lt; / l a t e x i t &gt; + &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l G b w 8 j c J W 3 K 0 w E s h r S Q p 8 a Q G 0 q 8 = " &gt; A A A B 6 H i c b V D J S g N B E K 2 J W x K 3 q E c v j U E Q h D C j B z 0 G v X h M w C y Y D K G n U 5 O 0 6 V n o 7 h H D k C / w 4 k G R X P 0 B / 8 W b X 6 O d 5 a C J D w o e 7 1 V R V c + L B V f a t r + s z M r q 2 v p G N p f f 3 N r e 2 S 3 s 7 d d V l E i G N R a J S D Y 9 q l D w E G u a a 4 H N W C I N P I E N b 3 A 9 8 R s P K B W P w l s 9 j N E N a C / k P m d U G 6 l 6 2 i k U 7 Z I 9 B V k m z p w U y 7 l 4 f P f x + F 3 p F D 7 b 3 Y g l A Y a a C a p U y 7 F j 7 a Z U a s 4 E j v L t R G F M 2 Y D 2 s G V o S A N U b j o 9 d E S O j d I l f i R N h Z p M 1 d 8 T K Q 2 U G g a e 6 Q y o 7 q t F b y L + 5 7 U S 7 V + 6 K Q / j R G P I Z o v 8 R B A d k c n X p M s l M i 2 G h l A m u b m V s D 6 V l G m T T d 6 E 4 C y + v E z q Z y X n v H R W N W l c w Q x Z O I Q j O A E H L q A M N 1 C B G j B A e I I X e L X u r W f r z R r P W j P W f O Y A / s B 6 / w G S B p C B &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m U K s B b 8 8 / t a 0 G a 1 0 S L x F p + 4 H i N w = " &gt; A A A B 6 H i c b V D J T g J B E K 3 B D X B D P X r p S E y 8 S G b 0 o E e i F 4 + Q y B J h Q n q a G m j p W d L d Y y Q T v s C L B 4 3 h 6 g / 4 L 9 7 8 G m 2 W g 4 I v q e T l v a p U 1 f N i w Z W 2 7 S 8 r s 7 K 6 t r 6 R z e U 3 t 7 Z 3 d g t 7 + 3 U V J Z J h j U U i k k 2 P K h Q 8 x J r m W m A z l k g D T 2 D D G 1 x P / M Y D S s W j 8 F Y P Y 3 Q D 2 g u 5 z x n V R q q e d g p F u 2 R P Q Z a J M y f F c i 4 e 3 3 0 8 f l c 6 h c 9 2 N 2 J J g K F m g i r V c u x Y u y m V m j O B o 3 w 7 U R h T N q A 9 b B k a 0 g C V m 0 4 P H Z F j o 3 S J H 0 l T o S Z T 9 f d E S g O l h o F n O g O q + 2 r R m 4 j / e a 1 E + 5 d u y s M 4 0 R i y 2 S I / E U R H Z P I 1 6 X K J T I u h I Z R J b m 4 l r E 8 l Z d p k k z c h O I s v L 5 P 6 W c k 5 L 5 1 V T R p X M E M W D u E I T s C B C y j D D V S g B g w Q n u A F X q 1 7 6 9 l 6 s 8 a z 1 o w 1 n z m A P 7 D e f w C V D p C D &lt; / l a t e x i t &gt; a 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " k F B b j L 4 7 V H C Y d s B 6 9 V Z 5 d h k o 2 / U = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B l v B V U n q Q p d F N y 4 r 2 A c 0 I U y m k 3 b o Z B L m I Z T Q 3 3 D j Q h G 3 / o w 7 / 8 Z p m 4 W 2 H r h w O O d e 7 r 0 n y j h T 2 n W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p q t R I Q j s k 5 a n s R 1 h R z g T t a K Y 5 7 W e S 4 i T i t B d N 7 u Z + 7 4 l K x V L x q K c Z D R I 8 E i x m B G s r + X U c 5 n 5 7 z E J v V g + r N b f h L o D W i V e Q G h R o h 9 U v f 5 g S k 1 C h C c d K D T w 3 0 0 G O p W a E 0 1 n F N 4 p m m E z w i A 4 s F T i h K s g X N 8 / Q h V W G K E 6 l L a H R Q v 0 9 k e N E q W k S 2 c 4 E 6 7 F a 9 e b i f 9 7 A 6 P g m y J n I j K a C L B f F h i O d o n k A a M g k J Z p P L c F E M n s r I m M s M d E 2 p o o N w V t 9 e Z 1 0 m w 3 v q t F 8 a N Z a t 0 U c Z T i D c 7 g E D 6 6 h B f f Q h g 4 Q y O A Z X u H N M c 6 L 8 + 5 8 L F t L T j F z C n / g f P 4 A A f 6 R A Q = = &lt; / l a t e x i t &gt; a 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " k F B b j L 4 7 V H C Y d s B 6 9 V Z 5 d h k o 2 / U = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B l v B V U n q Q p d F N y 4 r 2 A c 0 I U y m k 3 b o Z B L m I Z T Q 3 3 D j Q h G 3 / o w 7 / 8 Z p m 4 W 2 H r h w O O d e 7 r 0 n y j h T 2 n W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p q t R I Q j s k 5 a n s R 1 h R z g T t a K Y 5 7 W e S 4 i T i t B d N 7 u Z + 7 4 l K x V L x q K c Z D R I 8 E i x m B G s r + X U c 5 n 5 7 z E J v V g + r N b f h L o D W i V e Q G h R o h 9 U v f 5 g S k 1 C h C c d K D T w 3 0 0 G O p W a E 0 1 n F N 4 p m m E z w i A 4 s F T i h K s g X N 8 / Q h V W G K E 6 l L a H R Q v 0 9 k e N E q W k S 2 c 4 E 6 7 F a 9 e b i f 9 7 A 6 P g m y J n I j K a C L B f F h i O d o n k A a M g k J Z p P L c F E M n s r I m M s M d E 2 p o o N w V t 9 e Z 1 0 m w 3 v q t F 8 a N Z a t 0 U c Z T i D c 7 g E D 6 6 h B f f Q h g 4 Q y O A Z X u H N M c 6 L 8 + 5 8 L F t L T j F z C n / g f P 4 A A f 6 R A Q = = &lt; / l a t e x i t &gt; a p &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l y e 4 T i v g G D i A I o Y 6 C Q p I X D J j b t M = " &gt; A A A B 8 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L L a C p 5 L U g x 6 L X j x W s B / Q h L D Z b t q l m 8 2 y u x F K 6 N / w 4 k E R r / 4 Z b / 4 b t 2 0 O 2 v p g 4 P H e D D P z I s m Z N q 7 7 7 Z Q 2 N r e 2 d 8 q 7 l b 3 9 g 8 O j 6 v F J V 6 e Z I r R D U p 6 q f o Q 1 5 U z Q j m G G 0 7 5 U F C c R p 7 1 o c j f 3 e 0 9 U a Z a K R z O V N E j w S L C Y E W y s 5 N d x m P v t M Q v l r B 5 W a 2 7 D X Q C t E 6 8 g N S j Q D q t f / j A l W U K F I R x r P f B c a Y I c K 8 M I p 7 O K n 2 k q M Z n g E R 1 Y K n B C d Z A v b p 6 h C 6 s M U Z w q W 8 K g h f p 7 I s e J 1 t M k s p 0 J N m O 9 6 s 3 F / 7 x B Z u K b I G d C Z o Y K s l w U Z x y Z F M 0 D Q E O m K D F 8 a g k m i t l b E R l j h Y m x M V V s C N 7 q y + u k 2 2 x 4 V 4 3 m Q 7 P W u i 3 i K M M Z n M M l e H A N L b i H N n S A g I R n e I U 3 J 3 N e n H f n Y 9 l a c o q Z U / g D 5 / M H Y f i R Q A = = &lt; / l a t e x i t &gt; a p</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l y e 4 T i v g G D i A I o Y 6 C Q p I X D J j b t M = " &gt; A A A B 8 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L L a C p 5 L U g x 6 L X j x W s B / Q h L D Z b t q l m 8 2 y u x F K 6 N / w 4 k E R r / 4 Z b / 4 b t 2 0 O 2 v p g 4 P H e D D P z I s m Z N q 7 7 7 Z Q 2 N r e 2 d 8 q 7 l b 3 9 g 8 O j 6 v F J V 6 e Z I r R D U p 6 q f o Q 1</p><formula xml:id="formula_1">5 U z Q j m G G 0 7 5 U F C c R p 7 1 o c j f 3 e 0 9 U a Z a K R z O V N E j w S L C Y E W y s 5 N d x m P v t M Q v l r B 5 W a 2 7 D X Q C t E 6 8 g N S j Q D q t f / j A l W U K F I R x r P f B c a Y I c K 8 M I p 7 O K n 2 k q M Z n g E R 1 Y K n B C d Z A v b p 6 h C 6 s M U Z w q W 8 K g h f p 7 I s e J 1 t M k s p 0 J N m O 9 6 s 3 F / 7 x B Z u K b I G d C Z o Y K s l w U Z x y Z F M 0 D Q E O m K D F 8 a g k m i t l b E R l j h Y m x M V V s C N 7 q y + u k 2 2 x 4 V 4 3 m Q 7 P W u i 3 i K M M Z n M M l e H A N L b i H N n S A g I R n e I U 3 J 3 N e n H f n Y 9 l a c o q Z U / g D 5 / M H Y f i R Q A = = &lt; / l a t e x i t &gt; q &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P 1 0 A M b R b b w f L C A x t j k m s 6 U e h 7 C g = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s s t C T a W G K U j w Q I 2 V v m Y M P e 3 r m 7 Z 0 I u / A Q b C 4 2 x 9 R f Z + W 9 c 4 A o F X z L J y 3 s z m Z n n x 4 J r 4 7 r f T m 5 t f W N z K 7 9 d 2 N n d 2 z 8 o H h 4 1 d Z Q o h g 0 W i U i 1 f a p R c I k N w 4 3 A d q y Q h r 7 A l j + + m f m t J 1 S a R / L B T G L s h X Q o e c A Z N V a 6 L z + W + 8 W S W 3 H n I K v E</formula><p>y 0 g J M t T 7 x a / u I G J J i N I w Q b X u e G 5 s e i l V h j O B 0 0 I 3 0 R h T N q Z D 7 F g q a Y i 6 l 8 5 P n Z I z q w x I E C l b 0 p C 5 + n s i p a H W k 9 C 3 n S E 1 I 7 3 s z c T / v E 5 i g q t e y m W c G J R s s S h I B D E R m f 1 N B l w h M 2 J i C W W K 2 1 s J G 1 F F m b H p F G w I 3 v L L q 6 R Z r X g X l e p d t V S 7 z u L I w w m c w j l 4 c A k 1 u I U 6 N I D B E J 7 h F d 4 c 4 b w 4 7 8 7 H o j X n Z D P H 8 A f O 5 w + T / Y 1 T &lt; / l a t e x i t &gt; q &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j x t l i Y u 5 Z k j 9 y D f O U e X c L R m b g 1 Q = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 U r x q / q h 6 9 L B b B U 0 n q Q S 9 i s R e P F e w H p K F s t p t 2 6 W Y T d j d C C f 0 Z X j w o 0 q v / w 7 s X 8 d + 4 a X v Q 1 g c D j / d m m D c T J J w p 7 T j f V m F t f W N z q 7 h t 7 + z u</p><formula xml:id="formula_2">0 A M b R b b w f L C A x t j k m s 6 U e h 7 C g = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s s t C T a W G K U j w Q I 2 V v m Y M P e 3 r m 7 Z 0 I u / A Q b C 4 2 x 9 R f Z + W 9 c 4 A o F X z L J y 3 s z m Z n n x 4 J r 4 7 r f T m 5 t f W N z K 7 9 d 2 N n d 2 z 8 o H h 4 1 d Z Q o h g 0 W i U i 1 f a p R c I k N w 4 3 A d q y Q h r 7 A l j + + m f m t J 1 S a R / L B T G L s h X Q o e c A Z N V a 6 L z + W + 8 W S W 3 H n I K v E y 0 g J M t T 7 x a / u I G J J i N I w Q b X u e G 5 s e i l V h j O B 0 0 I 3 0 R h T N q Z D 7 F g q a Y i 6 l 8 5 P n Z I z q w x I E C l b 0 p C 5 + n s i p a H W k 9 C 3 n S E 1 I 7 3 s z c T / v E 5 i g q t e y m W c G J R s s S h I B D E R m f 1 N B l w h M 2 J i C W W K 2 1 s J G 1 F F m b H p F G w I 3 v L L q 6 R Z</formula><formula xml:id="formula_3">7 R + U D o 9 a K k 4 l o U 0 S 8 1 h 2 A q w o Z 4 I 2 N d O c d h J J c R R w 2 g 5 G 9 d x v P 1 K p W C w e 9 D i h f o Q H g o W M Y G 0 k r x t h P S S Y Z / V J r 1 R 2 K s 4 M a J W 4 C 1 K + + b C v k + m X 3 e i V P r v 9 m K Q R F Z p w r J T n O o n 2 M y w 1 I 5 x O 7 G 6 q a I L J C A + o Z 6 j A E V V + N o s 8 Q W d G 6 a M w l q a E R j P 1 9 0 S G I 6 X G U W A 6 8 4 h q 2 c v F / z w v 1 e G V n z G R p J o K M l 8 U p h z p G O X 3 o z 6 T l G g + N g Q T y U x W R I Z Y Y q L N l 2 z z B H f 5 5 F X S q l b c i 0 r 1 3 i n X b m G O I p z A K Z y D C 5 d Q g z t o Q B M I x P A E L / B</formula><p>q a e v Z e r O m 8 9 a C t Z g 5 h j + w 3 n 8 A 1 C 2 U m g = = &lt; / l a t e x i t &gt; C &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j x t l i Y u 5 Z k j 9 y <ref type="table" target="#tab_0">r O m 8 9 a C t Z g 5 h j + w 3 n 8 A 1 C 2</ref>   </p><formula xml:id="formula_4">D f O U e X c L R m b g 1 Q = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 U r x q / q h 6 9 L B b B U 0 n q Q S 9 i s R e P F e w H p K F s t p t 2 6 W Y T d j d C C f 0 Z X j w o 0 q v / w 7 s X 8 d + 4 a X v Q 1 g c D j / d m m D c T J J w p 7 T j f V m F t f W N z q 7 h t 7 + z u 7 R + U D o 9 a K k 4 l o U 0 S 8 1 h 2 A q w o Z 4 I 2 N d O c d h J J c R R w 2 g 5 G 9 d x v P 1 K p W C w e 9 D i h f o Q H g o W M Y G 0 k r x t h P S S Y Z / V J r 1 R 2 K s 4 M a J W 4 C 1 K + + b C v k + m X 3 e i V P r v 9 m K Q R F Z p w r J T n O o n 2 M y w 1 I 5 x O 7 G 6 q a I L J C A + o Z 6 j A E V V + N o s 8 Q W d G 6 a M w l q a E R j P 1 9 0 S G I 6 X G U W A 6 8 4 h q 2 c v F / z w v 1 e G V n z G R p J o K M l 8 U p h z p G O X 3 o z 6 T l G g + N g Q T y U x W R I Z Y Y q L N l 2 z z B H f 5 5 F X S q l b c i 0 r 1 3 i n X b m G O I p z A K Z y D C 5 d Q g z t o Q B M I x P A E L / B q a e v Z e</formula><formula xml:id="formula_5">U m g = = &lt; / l a t e x i t &gt; R &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " O t k L d M O + A / g L 1 v G u s g v + W E s + 9 K A = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 U r x q / q h 6 9 B I v g q S T 1 o B e x 6 M V j F f s B a S i b 7 a Z d u t k N u x u h h P 4 M L x 4 U 6 d X / 4 d 2 L + G / c t D 1 o 6 4 O B x 3 s z z J s J E 0 a V d t 1 v q 7 C y u r a + U d y 0 t 7 Z 3 d v d K + w d N J V K J S Q M L J m Q 7 R I o w y k l D U 8 1 I O 5 E E x S E j r X B 4 k / u t R y I V F f x B j x I S x K j P a U Q x 0 k b y O z H S A 4 x Y d j / u l s p u x Z 3 C W S b e n J S v P u z L Z P J l 1 7 u l z 0 5 P 4 D Q m X G O G l P I 9 N 9 F B h q S m m J G x 3 U k V S R A e o j 7 x D e U o J i r I p p H H z o l R e k 4 k p C m u n a n 6 e y J D s V K j O D S d e U S 1 6 O X i f 5 6 f 6 u g i y C h P U k 0 4 n i 2 K U u Z o 4 e T 3 O z 0 q C d Z s Z A j C k p q s D h 4 g i b A 2 X 7 L N E 7 z F k 5 d J s 1 r x z i r V O 7 d c u 4 Y Z i n A E x 3 A K H p x D D W 6 h D g 3 A I O A J X u D V 0 t a z 9 W Z N Z q 0 F a z 5 z C H 9 g v f 8 A 6 v i U q Q = = &lt; / l a t e x i t &gt; p &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W C G R 7 w 1 G 2 t H 4 J S i f B y A 2 k Y m P y e 8 = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b t Y a B m 0 s Y x g P i A 5 w t 5 m L l m y t 3 f u 7 g n h y J + w s V D E 1 r 9 j 5 7 9 x k 1 y h i Q 8 G H u / N M D M v S A T X x n W / n b X 1 j c 2 t 7 c J O c X d v / + C w d H T c 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s s H M 0 n Q j + h Q 8 p A z a q z U q f Q a I 9 5 P K v 1 S 2 a 2 6 c 5 B V 4 u W k D D k a / d J X b x C z N E J p m K B a d z 0 3 M X 5 G l e F M 4 L T Y S z U m l I 3 p E L u W S h q h 9 r P 5 v V N y b p U B C W N l S x o y V 3 9 P Z D T S e h I F t j O i Z q S X v Z n 4 n 9 d N T X j t Z 1 w m q U H J F o v C V B A T k 9 n z Z M A V M i M m l l C m u L 2 V s B F V l B k b U d G G 4 C 2 / v E p a t a p 3 W a 3 d 1 8 r 1 m z y O A p z C G V y A B 1 d Q h z t o Q B M Y C H i G V 3 h z H p 0 X 5 9 3 5 W L S u O f n M C f y B 8 / k D J M W P Y A = = &lt; / l a t e x i t &gt; … … 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W 7 Q k d F k C i V 0 7 P 6 f f A j x 3 9 0 k n C X g = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b t Y a B m 0 s Y x g P i A 5 w t 5 m L 1 m y t 3 f u z g n h y J + w s V D E 1 r 9 j 5 7 9 x k 1 y h i Q 8 G H u / N M D M v S K Q w 6 L r f z t r 6 x u b W d m G n u L u 3 f 3 B Y O j p u m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t z O / / c S 1 E b F 6 w E n C / Y g O l Q g F o 2 i l T q X X G I m + V + m X y m 7 V n Y O s E i 8 n Z c j R 6 J e + e o O Y p R F X y C Q 1 p u u 5 C f o Z 1 S i Y 5 N N i L z U 8 o W x M h 7 x r q a I R N 3 4 2 v 3 d K z q 0 y I G G s b S k k c / X 3 R E Y j Y y Z R Y D s j i i O z 7 M 3 E / 7 x u i u G 1 n w m V p M g V W y w K U 0 k w J r P n y U B o z l B O L K F M C 3 s r Y S O q K U M b U d G G 4 C 2 / v E p a t a p 3 W a 3 d 1 8 r 1 m z y O A p z C G V y A B 1 d Q h z t o Q B M Y S H i G V 3 h z H p 0 X 5 9 3 5 W L S u O f n M C f y B 8 / k D x P u P I Q = = &lt; / l a t e x i t &gt; 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W 7 Q k d F k C i V 0 7 P 6 f f A j x 3 9 0 k n C X g = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b t Y a B m 0 s Y x g P i A 5 w t 5 m L 1 m y t 3 f u z g n h y J + w s V D E 1 r 9 j 5 7 9 x k 1 y h i Q 8 G H u / N M D M v S K Q w 6 L r f z t r 6 x u b W d m G n u L u 3 f 3 B Y O j p u m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t z O / / c S 1 E b F 6 w E n C / Y g O l Q g F o 2 i l T q X X G I m + V + m X y m 7 V n Y O s E i 8 n Z c j R 6 J e + e o O Y p R F X y C Q 1 p u u 5 C f o Z 1 S i Y 5 N N i L z U 8 o W x M h 7 x r q a I R N 3 4 2 v 3 d K z q 0 y I G G s b S k k c / X 3 R E Y j Y y Z R Y D s j i i O z 7 M 3 E / 7 x u i u G 1 n w m V p M g V W y w K U 0 k w J r P n y U B o z l B O L K F M C 3 s r Y S O q K U M b U d G G 4 C 2 / v E p a t a p 3 W a 3 d 1 8 r 1 m z y O A p z C G V y A B 1 d Q h z t o Q B M Y S H i G V 3 h z H p 0 X 5 9 3 5 W L S u O f n M C f y B 8 / k D x P u P I Q = = &lt; / l a t e x i t &gt; p &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W C G R 7 w 1 G 2 t H 4 J S i f B y A 2 k Y m P y e 8 = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b t Y a B m 0 s Y x g P i A 5 w t 5 m L l m y t 3 f u 7 g n h y J + w s V D E 1 r 9 j 5 7 9 x k 1 y h i Q 8 G H u / N M D M v S A T X x n W / n b X 1 j c 2 t 7 c J O c X d v / + C w d H T c 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s s H M 0 n Q j + h Q 8 p A z a q z U q f Q a I 9 5 P K v 1 S 2 a 2 6 c 5 B V 4 u W k D D k a / d J X b x C z N E J p m K B a d z 0 3 M X 5 G l e F M 4 L T Y S z U m l I 3 p E L u W S h q h 9 r P 5 v V N y b p U B C W N l S x o y V 3 9 P Z D T S e h I F t j O i Z q S X v Z n 4 n 9 d N T X j t Z 1 w m q U H J F o v C V B A T k 9 n z Z M A V M i M m l l C m u L 2 V s B F V l B k b U d G G 4 C 2 / v E p a t a p 3 W a 3 d 1 8 r 1 m z y O A p z C G V y A B 1 d Q h z t o Q B M Y C H i G V 3 h z H p 0 X 5 9 3 5 W L S u O f n M C f y B 8 / k D J M W P Y A = = &lt; / l a t e x i t &gt; h 1 i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 E H s T J u G S B B Z q n p M u r c E C E v p l h k = " &gt; A A A B / X i c b V C 5 T s N A E F 1 z h n C Z o 6 N Z k S B R R X Y o o I y g o Q w S O a T Y W O v N O F l l f W h 3 H S l Y F r 9 C Q w F C t P w H H X / D J n E B C U 8 a 6 e m 9 G c 3 M 8 x P O p L K s b 2 N l d W 1 9 Y 7 O 0 V d 7 e 2 d 3 b N w 8 O 2 z J O B Y U W j X k s u j 6 R w F k E L c U U h 2 4 i g I Q + h 4 4 / u p n 6 n T E I y e L o X k 0 S c E M y i F j A K F F a 8 s z j q j M G m g 1 z j z 1 k T n P I P D u v e m b F q l k z 4 G V i F 6 S C C j Q 9 8 8 v p x z Q N I V K U E y l 7 t p U o N y N C M c o h L z u p h I T Q E R l A T 9 O I h C D d b H Z 9 j s + 0 0 s d B L H R F C s / U 3 x M Z C a W c h L 7 u D I k a y k V v K v 7 n 9 V I V X L k Z i 5 J U Q U T n i 4 K U Y x X j a R S 4 z w R Q x S e a E C q Y v h X T I R G E K h 1 Y W Y d g L 7 6 8 T N r 1 m n 1 R q 9 / V K 4 3 r I o 4 S O k G n 6 B z Z 6 B I 1 0 C 1 q o h a i 6 B E 9 o 1 f 0 Z j w Z L 8 a 7 8 T F v X T G K m S P 0 B 8 b n D / H 1 l O I = &lt; / l a t e x i t &gt; xi &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 2 5 6 g Q V B S a u E I X R K o P l m Z d L 8 5 d c = " &gt; A A A B 8 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s s t C T a W G I i H 8 l x I X v L H m z Y 2 7 3 s 7 h H J h Z 9 h Y 6 E x t v 4 a O / + N C 1 y h 4 E s m e X l v J j P z w o Q z b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T t p a p I r R F J J e q G 2 J N O R O 0 Z Z j h t J s o i u O Q 0 0 4 4 v p v 7 n Q l V m k n x a K Y J D W I 8 F C x i B B s r + d X e h J L s a d Z n 1 X 6 5 4 t b c B d A 6 8 X J S g R z N f v m r N 5 A k j a k w h G O t f c 9 N T J B h Z R j h d F b q p Z o m m I z x k P q W C h x T H W S L k 2 f o w i o D F E l l S x i 0 U H 9 P Z D j W e h q H t j P G Z q R X v b n 4 n + e n J r o J M i a S 1 F B B l o u i l C M j 0 f x / N G C K E s O n l m C i m L 0 V k R F W m B i b U s m G 4 K 2 + v E 7 a 9 Z p 3 V a s / 1 C u N 2 z y O I p z B O V y C B 9 f Q g H t o Q g s I S H i G V 3 h z j P P i v D s f y 9 a C k 8 + c w h 8 4 n z / u a J E E &lt; / l a t e x i t &gt; xi &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 2 5 6 g Q V B S a u E I X R K o P l m Z d L 8 5 d c = " &gt; A A A B 8 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s s t C T a W G I i H 8 l x I X v L H m z Y 2 7 3 s 7 h H J h Z 9 h Y 6 E x t v 4 a O / + N C 1 y h 4 E s m e X l v J j P z w o Q z b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T t p a p I r R F J J e q G 2 J N O R O 0 Z Z j h t J s o i u O Q 0 0 4 4 v p v 7 n Q l V m k n x a K Y J D W I 8 F C x i B B s r + d X e h J L s a d Z n 1 X 6 5 4 t b c B d A 6 8 X J S g R z N f v m r N 5 A k j a k w h G O t f c 9 N T J B h Z R j h d F b q p Z o m m I z x k P q W C h x T H W S L k 2 f o w i o D F E l l S x i 0 U H 9 P Z D j W e h q H t j P G Z q R X v b n 4 n + e n J r o J M i a S 1 F B B l o u i l C M j 0 f x / N G C K E s O n l m C i m L 0 V k R F W m B i b U s m G 4 K 2 + v E 7 a 9 Z p 3 V a s / 1 C u N 2 z y O I p z B O V y C B 9 f Q g H t o Q g s I S H i G V 3 h z j P P i v D s f y 9 a C k 8 + c w h 8 4 n z / u a J E E &lt; / l a t e x i t &gt; xj &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G 5 / C k n 7 i q g b + 5 R + b y V Q y x L q X g J Y = " &gt; A A A B / H i c b V C 7 T s N A E D z z D O F l S E l j k S B R R X Y o o I y g o Q w S e U i x Z Z 3 P 6 + T I + a G 7 c 4 R l m V + h o Q A h W j 6 E j r / h k r i A h J F W G s 3 s a n f H S x g V 0 j S / t b X 1 j c 2 t 7 c p O d X d v / + B Q P z r u i T j l B L o k Z j E f e F g A o x F 0 J Z U M B g k H H H o M + t 7 k Z u b 3 p 8 A F j a N 7 m S X g h H g U 0 Y A S L J X k 6 r W G P Q W S 2 5 I y H / L H o n A f G q 5 e N 5 v m H M Y q s U p S R y U 6 r v 5 l + z F J Q 4 g k Y V i I o W U m 0 s k x l 5 Q w K K p 2 K i D B Z I J H M F Q 0 w i E I J 5 8 f X x h n S v G N I O a q I m n M 1 d 8 T O Q 6 F y E J P d Y Z Y j s W y N x P / 8 4 a p D K 6 c n E Z J K i E i i 0 V B y g w Z G 7 M k D J 9 y I J J l i m D C q b r V I G P M M Z E q r 6 o K w V p + e Z X 0 W k 3 r o t m 6 a 9 X b 1 2 U c F X S C T t E 5 s t A l a q N b 1 E F d R F C G n t E r e t O e t B f t X f t Y t K 5 p 5 U w N / Y H 2 + Q P l Q p T s &lt; / l a t e x i t &gt; xj &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G 5 / C k n 7 i q g b + 5 R + b y V Q y x L q X g J Y = " &gt; A A A B / H i c b V C 7 T s N A E D z z D O F l S E l j k S B R R X Y o o I y g o Q w S e U i x Z Z 3 P 6 + T I + a G 7 c 4 R l m V + h o Q A h W j 6 E j r / h k r i A h J F W G s 3 s a n f H S x g V 0 j S / t b X 1 j c 2 t 7 c p O d X d v / + B Q P z r u i T j l B L o k Z j E f e F g A o x F 0 J Z U M B g k H H H o M + t 7 k Z u b 3 p 8 A F j a N 7 m S X g h H g U 0 Y A S L J X k 6 r W G P Q W S 2 5 I y H / L H o n A f G q 5 e N 5 v m H M Y q s U p S R y U 6 r v 5 l + z F J Q 4 g k Y V i I o W U m 0 s k x l 5 Q w K K p 2 K i D B Z I J H M F Q 0 w i E I J 5 8 f X x h n S v G N I O a q I m n M 1 d 8 T O Q 6 F y E J P d Y Z Y j s W y N x P / 8 4 a p D K 6 c n E Z J K i E i i 0 V B y g w Z G 7 M k D J 9 y I J J l i m D C q b r V I G P M M Z E q r 6 o K w V p + e Z X 0 W k 3 r o t m 6 a 9 X b 1 2 U c F X S C T t E 5 s t A l a q N b 1 E F d R F C G n t E r e t O e t B f t X f t Y t K 5 p 5 U w N / Y H 2 + Q P l Q p T s &lt; / l a t e x i t &gt; h p i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 l w B 9 t F b z I g K + V 2 R 9 A r a s v 5 c O Q I = " &gt; A A A B / X i c b V C 5 T s N A E B 1 z h n C Z o 6 N Z k S B R R X Y o o I y g o Q w S O a T Y W O v N O l l l f W h 3 H S l Y F r 9 C Q w F C t P w H H X / D J n E B C U 8 a 6 e m 9 G c 3 M 8 x P O p L K s b 2 N l d W 1 9 Y 7 O 0 V d 7 e 2 d 3 b N w 8 O 2 z J O B a E t E v N Y d H 0 s K W c R b S m m O O 0 m g u L Q 5 7 T j j 2 6 m f m d M h W R x d K 8 m C X V D P I h Y w A h W W v L M 4 6 o z p i Q b 5 h 5 7 y J z m k H l J X v X M i l W z Z k D L x C 5 I B Q o 0 P f P L 6 c c k D W m k C M d S 9 m w r U W 6 G h W K E 0 7 z s p J I m m I z w g P Y 0 j X B I p Z v N r s / R m V b 6 K I i F r k i h m f p 7 I s O h l J P Q 1 5 0 h V k O 5 6 E 3 F / 7 x e q o I r N 2 N R k i o a k f m i I O V I x W g a B e o z Q Y n i E 0 0 w E U z f i s g Q C 0 y U D q y s Q 7 A X X 1 4 m 7 X r N v q j V 7 + q V x n U R R w l O 4 B T O w Y Z L a M A t N K E F B B 7 h G V 7 h z X g y X o x 3 4 2 P e u m I U M 0 f w B 8 b n D 1 H + l S E = &lt; / l a t e x i t &gt; h 1 j &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c I M x h z I E B V J e S v 4 z 5 B u v 1 X e E s s c = " &gt; A A A C B X i c b V C 5 T s N A E F 2 H K 4 T L Q A m F R Y J E F d m h g D K C h j J I 5 J B i Y 6 3 X 4 3 j J + t D u O l J k u a H h V 2 g o Q I i W f 6 D j b 9 g c B S Q 8 a a S n 9 2 Y 0 M 8 9 L G R X S N L + 1 0 s r q 2 v p G e b O y t b 2 z u 6 f v H 3 R E k n E C b Z K w h P c 8 L I D R G N q S S</formula><formula xml:id="formula_6">W U + 5 G F R u A / 3 u d 0 K q W s V N V e v m n V z C m O Z W H N S R X O 0 X P 3 L 9 h O S R R B L w r A Q f c t M p Z N j L i l h U F T s T E C K y R A P o K 9 o j C M Q T j 7 9 o j B O l e I b Q c J V x d K Y q r 8 n c h w J M Y 4 8 1 R l h G Y p F b y L + 5 / U z G V w 6 O Y 3 T T E J M Z o u C j B k y M S a R G D 7 l Q C Q b K 4 I J p + p W g 4 S Y Y y J V c B U V g r X 4 8 j L p N O r W e b 1 x 2 6 g 2 r + Z x l N E R O k F n y E I X q I l u U A u 1 E U G P 6 B m 9 o j f t S X v R 3 r W P W W t J m 8 8 c o j / Q P n 8 A k L W Y m Q = = &lt; / l a t e x i t &gt; h p j &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R 6 0 / s K F 3 f 3 i O W W 6 1 W i D w n i C N s E g = " &gt; A A A C B X i c b V C 5 T s N A E F 2 H K 4 T L Q A m F R Y J E F d m h g D K C h j J I 5 J B i Y 6 3 X 4 3 j J + t D u O l J k u a H h V 2 g o Q I i W f 6 D j b 9 g c B S Q 8 a a S n 9 2 Y 0 M 8 9 L G R X S N L + 1 0 s r q 2 v p G e b O y t b 2 z u 6 f v H 3 R E k n E C b Z K w h P c 8 L I D R G N q S S</formula><formula xml:id="formula_7">W U + 5 G F R u A / 3 u d 0 K q Z s W N V e v m n V z C m O Z W H N S R X O 0 X P 3 L 9 h O S R R B L w r A Q f c t M p Z N j L i l h U F T s T E C K y R A P o K 9 o j C M Q T j 7 9 o j B O l e I b Q c J V x d K Y q r 8 n c h w J M Y 4 8 1 R l h G Y p F b y L + 5 / U z G V w 6 O Y 3 T T E J M Z o u C j B k y M S a R G D 7 l Q C Q b K 4 I J p + p W g 4 S Y Y y J V c B U V g r X 4 8 j L p N O r W e b 1 x 2 6 g 2 r + Z x l N E R O k F n y E I X q I l u U A u 1 E U G P 6 B m 9 o j f t S X v R 3 r W P W W t J m 8 8 c o j / Q P n 8 A 8 K + Y 2 A = = &lt; / l a t e x i t &gt;</formula><p>hj &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m o 9 O P r 6 3 p  </p><formula xml:id="formula_8">V Q b d 6 a 0 V L 0 f l L S t u f E = " &gt; A A A B / H i c b V C 7 T s N A E D y H V w g v Q 0 o a i w S J K r J D A W U E D W W Q y E O K L e t 8 X i d H z g / d n S N Z l v k V G g o Q o u V D 6 P g b L o k L S B h p p d H M r n Z 3 v I R R I U 3 z W 6 t s b G 5 t 7 1 R 3 a 3 v 7 B 4 d H + v F J X 8 Q p J 9 A j M Y v 5 0 M M C G I 2 g J 6 l k M E w 4 4 N B j M P C m t 3 N / M A M u a B w 9 y C w B J 8 T j i A a U Y K k k V 6 8 3 7 R m Q 3 J a U + Z B P i s J 9 b L p 6 w 2 y Z C x j r x C p J A 5 X o u v q X 7 c c k D S G S h G E h R p a Z S C f H X F L C o K j Z q Y A E k y k e w 0 j R C I c g n H x x f G G c K 8 U 3 g p i r i q S x U H 9 P 5 D g U I g s 9 1 R l i O R G r 3 l z 8 z x u l M r h 2 c h o l q Y S I L B c F K T N k b M y T M H z K g U i W K Y I J p + p W g 0 w w x 0 S q v G o q B G v 1 5 X X S b 7 e s y 1 b 7 v t 3 o 3 J R x V N E p O k M X y E J X q I P u U B f 1 E E E Z e k a v</formula><formula xml:id="formula_9">C o f H z S 1 j J V F F t U c q k 6 I d H I m c C W Y Y Z j J 1 F I 4 p D j U z i + m / t P E 1 S a S f F o p g k G M R k K F j F K j J W 6 1 d 4 E a T a a 9 V m 1 X 6 5 4 N W 8 B d 5 3 4 O a l A j m a / / N U b S J r G K A z l R O u u 7 y U m y I g y j H K c l X q p x o T Q M R l i 1 1 J B Y t R B t j h 5 5 l 5 Y Z e B G U t k S x l 2 o v y c y E m s 9 j U P b G R M z 0 q v e X P z P 6 6 Y m u g k y J p L U o K D L R V H K X S P d + f / u g C m k h k 8 t I V Q x e 6 t L R 0 Q R a m x K J R u C v / r y O m n X a / 5 V r f 5 Q r z R u 8 z i K c A b n c A k + X E M D 7 q E J L a A g 4 R</formula><formula xml:id="formula_10">− − → v 2 R2 − − → • • • Rn−1 −−−→ v n is defined as a meta-path between nodes v 1 and v n , wherein R = R 1 • R 2 • • • • • R n−1 defines the com- posite relations between node v 1 and v n [Dong et al., 2017].</formula><p>In this paper, we intend to utilize symmetric and undirected meta paths to denote the closeness among target-type nodes V t , which can help simplify the problem setting.</p><p>We represent the set of meta paths used in this paper as {Φ 1 , Φ 2 , • • • , Φ P }, where Φ i denotes the i-th meta path type. For example, in Figure <ref type="figure" target="#fig_0">1</ref>(d), Paper-Author-Paper (PAP) and Paper-Subject-Paper (PSP) are two types of meta-paths, which contain the semantic "papers written by the same author" and "papers belonging to the same subject" respectively. Definition 3.2 (Meta-path based Adjacency Matrix) For meta-path definition Φ i , if there exists a path instance between node v i ∈ V t and v j ∈ V t , we call that v i and v j are "connected neighbors" based on Φ i . Such neighborhood in-formation can be represented by a meta-path based adjacent matrix</p><formula xml:id="formula_11">A Φi ∈ R |Vt|×|Vt| , where A Φi ij = A Φi ji = 1 if v i , v j are connected by meta-path Φ i and A Φi ij = A Φi ji = 0 otherwise.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">HDGI Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">HDGI Architecture Overview</head><p>A high-level illustration of the proposed HDGI is shown in Figure <ref type="figure" target="#fig_5">2</ref>. The input of HDGI is a heterogeneous graph G containing N vertices whose initial d-dimension features are denoted by X ∈ R N ×d , and meta-path set {Φ i } P i=1 . Based on {Φ i } P i=1 we can calculate the meta-path set based adjacency matrices {A Φi } P i=1 . The meta-path based local representation encoding described in §4.2 has two steps: (1) learning individual node representation H Φi from X and each A Φi , i = 1, 2.., P and (2) generating node representation H by aggregating {H Φi } P i=1 through a semantic-level attention mechanism. A global representation encoder R is proposed to derive a graph summary vector s from H (see §4.3). The discriminator D will be trained with the objective to maximize mutual information between positive nodes and the graphlevel summary s. In §4.4 we elaborate the mutual information based discriminator D and the negative sample generator C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Meta-path based local representation encoder</head><p>Meta-path specific graph node representation learning. Each of A Φi , i = 1, 2, ..P can be viewed as a homogeneous graph. At this step our target is to derive a node representation containing the information of initial node feature X and A Φi , with a node-level encoder:</p><formula xml:id="formula_12">H Φi = a Φi (X, A Φi ) (1)</formula><p>Two kinds of encoder are considered in this work. The first is Graph Convolutional Network (GCN) <ref type="bibr">[Kipf and Welling, 2017a]</ref>. The nodes representation obtained by GCN is:</p><formula xml:id="formula_13">H Φi = (D Φi − 1 2 ÃΦi D Φi − 1 2 )XW Φi (2)</formula><p>where ÃΦi = A Φi + I, D Φi is the diagonal node degree matrix of ÃΦi . Matrix W Φi ∈ R d×F is the filter parameter matrix.</p><p>The second encoder we consider is Graph Attention module (GAT) <ref type="bibr">[Veličković et al., 2017]</ref>. For the m-th node, its K-head attention output can be computed as:</p><formula xml:id="formula_14">h Φi m = K k=1 σ( j∈N Φ i m α Φi,k mj W Φi x j )<label>(3)</label></formula><p>where is the concatenation operator, W Φi is the linear transformation parameter matrix and N Φi m is neighbor set defined by Φ i . α Φi,k mj is the normalized attention coefficient computed by the k-th attention mechanism.</p><p>After the node-level learning, we obtain the set of node representations {H Φi } P i=1 . They are aggregated to get a heterogeneous graph based node representation. Heterogeneous graph node representation learning. The representations learned based on the specific meta-path contain only the semantic-specific information. The key issue to accomplish the aggregation is exploring how much each meta-path should contribute to the final representations. Here we add a semantic attention layer L att to learn the weights:</p><p>{β Φ1 , β Φ2 , . . . , β Φ P } = L att (H Φ1 , H Φ2 , . . . , H Φ P ) (4) Specifically, the importance of the meta-path Φ i is calculated by</p><formula xml:id="formula_15">e Φi = 1 N N n=1 tanh( q T • [W sem • h Φi n + b])<label>(5)</label></formula><p>where W sem is a linear transformation parameter matrix. β Φi is obtained by normalizing {e Φi } P i=1 with a softmax function:</p><formula xml:id="formula_16">β Φi = softmax(e Φi ) = exp(e Φi ) P j=1 exp(e Φj )<label>(6)</label></formula><p>The heterogeneous graph node representation H is obtained by a linear combination of {H Φi } P i=1 , that is</p><formula xml:id="formula_17">H = P i=1 β Φi • H Φi (7)</formula><p>Our semantic attention layer is inspired by <ref type="bibr" target="#b6">HAN [Wang et al., 2019]</ref>, but there are still some differences in the learning direction. HAN utilizes classification cross-entropy as the loss function, the learning direction is guided by known labels in the training set. However, the attention weights learned in HDGI are guided by the binary cross-entropy loss which indicates whether the node belongs to the original graph. Therefore, the weights learned in HDGI serve for the existence of a node. Because no classification label involves, the weights get no bias from the known labels.</p><p>The representations H serve as the final output local features. The global representation encoder leverages the representations H to output the graph-level summary which will be described in the following part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Global Representation Encoder</head><p>The learning objective of HDGI is to maximize the mutual information between local representations and the global representation. The local representations of nodes are included in H, and we need the summary vector s to represent the global information of the entire heterogeneous graph. Based on H, we examined three candidate encoder functions: Averaging encoder function. Our first candidate encoder function is the averaging operator, where we simply take the mean of the node representations to output the graph-level summary vector s:</p><formula xml:id="formula_18">s = σ 1 N N i=1 h i (8)</formula><p>Pooling encoder function. In this pooling encoder function, each node's vector is independently fed through a fullyconnected layer. An elementwise max-pooling operator is applied to summary the information from the nodes set:</p><formula xml:id="formula_19">s pool = max({σ(W pool h i + b), i ∈ {1, 2, . . . , N }) (9)</formula><p>where max denotes the element-wise max operator and σ is a nonlinear activation function.</p><p>Set2vec encoder function. The final encoder function we examine is Set2vec [Oriol <ref type="bibr" target="#b6">Vinyals, 2016]</ref>, which is based on an LSTM architecture. Because the original set2vec in [Oriol <ref type="bibr" target="#b6">Vinyals, 2016]</ref> works on ordered node sequences, but here we need a summary of the graph concluding comprehensive information from each node instead of merely graph structure. Therefore, we apply the LSTMs to a random permutation of the node's neighbor on an unordered set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">HDGI Learning</head><p>Mutual information based discriminator. Belghazi et al. proved that the KL-divergence admits the Donsker-Varadhan representation and the f -divergence representation as dual representations in <ref type="bibr" target="#b0">[Belghazi et al., 2018]</ref>. The dual representations provide a lower-bound to the mutual information of random variables X and Y : x,y) ]) (10) Here, P XY is the joint distribution and P X ⊗ P Y is the product of margins. T ω is a deep neural network based discriminator parametrized by ω. The expectations in equ.10 can be estimated using samples from P XY and P X ⊗P Y . The expressive power of the discriminator ensures to approximate the MI with high accuracy. Here, we estimate and maximize the mutual information simultaneously by training a discriminator D to distinguish positive sample set</p><formula xml:id="formula_20">MI(X; Y ) ≥ E P XY [T ω (x, y)] − log(E P X ⊗P Y [e Tω(</formula><formula xml:id="formula_21">P os = [ h n , s] N n=1 with negative sample set N eg = [ hm , s] M m=1</formula><p>. The sample ( h i , s) is denoted as positive as node h i belongs to the original graph (the joint distribution), and ( hj , s) is negative as the node hj is the generated fake one (the product of marginals). The discriminator D is a bilinear layer:   &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 J q H z x K a P t / A Z k 7 n w h z E X j H Z j + 8   </p><formula xml:id="formula_22">D( h i , s) = σ( h T i W D s)<label>(11)</label></formula><formula xml:id="formula_23">h i Q 8 G H u / N M D M v S A T X x n W / n b X 1 j c 2 t 7 c J O c X d v / + C w d H T c 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s s H M 0 n Q j + h Q 8 p A z a q z U q f Q a I 9 5 P K v 1 S 2 a 2 6 c 5 B V 4 u W k D D k a / d J X b x C z N E J p m K B a d z 0 3 M X 5 G l e F M 4 L T Y S z U m l I 3 p E L u W S h q h 9 r P 5 v V N y b p U B C W N l S x o y V 3 9 P Z D T S e h I F t j O i Z q S X v Z n 4 n 9 d N T X j t Z 1 w m q U H J F o v C V B A T k 9 n z Z M A V M i M m l l C m u L 2 V s B F V l B k b U d G G 4 C 2 / v E p a t a p 3 W a 3 d 1 8 r 1 m z y O A p z C G V y A B 1 d Q h z t o Q B M Y C H i G V 3 h z</formula><formula xml:id="formula_24">d A K 8 T L y c V l K P Z L 3 / 1 B o q m M Z N A B T H G 9 9 w E g o x o 4 F S w W a m X G p Y Q O i Z D 5 l s q S c x M k C 1 O n u E L q w x w p L Q t C X i h / p 7 I S G z M N A 5 t Z 0 x g Z F a 9 u f i f 5 6 c Q 3 Q Q Z l 0 k K T N L l o i g V G B S e / 4 8 H X D M K Y m o J o Z r b W z E d E U 0 o 2 J R K N g R v 9 e V 1 0 q 7 X v K t a /</formula><formula xml:id="formula_25">= " &gt; A A A B 8 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k b u z 0 J J o Y 4 m J f C R w I X v L H m z Y 2 7 3 s 7 h H J h Z 9 h Y 6 E x t v 4 a O / + N C 1 y h 4 E s m e X l v J j P z w o Q z b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T l p a p I r R J J J e q E 2 J N O R O 0 a Z j h t J M o i u O Q 0 3 Y 4 v p v 7 7 Q l V m k n x a K Y J D W I 8 F C x i B B s r d a u 9 C S X Z 0 6 z v V / v l i l t z F 0 D r x M t J B X I 0 + u W v 3 k C S N K b C E I 6 1 7 n p u Y o I M K 8 M I p 7 N S L 9 U 0 w W S M h 7 R r q c A x 1 U G 2 O H m G L q w y Q J F U t o R B C / X 3 R I Z j r a d x a D t j b E Z 6 1 Z u L / 3 n d 1 E Q 3 Q c Z E k h o q y H J R l H J k J J r / j w Z M U W L 4 1 B J M F L O 3 I j L C C h N j U y r Z E L z V l</formula><formula xml:id="formula_26">M p h V u q l G h J C x 2 Q I v q W C x K C D b H H y D F 9 Y Z Y A j q W w J g x f q 7 4 m M x F p P 4 9 B 2 x s S M 9 K o 3 F / / z / N R E 1 0 H G R J I a E H S 5 K E o 5 N h L P / 8 c D p o A a P r W E U M X s r Z i O i C L U 2 J R K N g R v 9 e V 1 0 q n X v E a t f l + v N G /</formula><formula xml:id="formula_27">J I s G 9 I t X W j o t p t 2 l k g 2 / A w v H j T G q 7 / G m / / G A n t Q 8 C W T v L w 3 k 5 l 5 Y S K 4 A d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o b V S q K W t R J Z T u h M Q w w S V r A Q f B O o l m J A 4 F e w x H t z P / c c y 0 4 U o + w C R h Q U w G k k e c E r C S X + 2 O G c 2 e p r 3 L a q 9 c c W v u H H i V e D m p o B z N X v m r 2 1 c 0 j Z k E K o g x v u c m E G R E A 6 e C T U v d 1 L C E 0 B E Z M N 9 S S W J m g m x + 8 h S f W a W P I 6 V t S c B z 9 f d E R m J j J n F o O 2 M C Q 7 P</formula><formula xml:id="formula_28">Q k d F k C i V 0 7 P 6 f f A j x 3 9 0 k n C X g = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b t Y a B m 0 s Y x g P i A 5 w t 5 m L 1 m y t 3 f u z g n h y J + w s V D E 1 r 9 j 5 7 9 x k 1 y h i Q 8 G H u / N M D M v S K Q w 6 L r f z t r 6 x u b W d m G n u L u 3 f 3 B Y O j p u m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t z O / / c S 1 E b F 6 w E n C / Y g O l Q g F o 2 i l T q X X G I m + V + m X y m 7 V n Y O s E i 8 n Z c j R 6 J e + e o O Y p R F X y C Q 1 p u u 5 C f o Z 1 S i Y 5 N N i L z U 8 o W x M h 7 x r q a I R N 3 4 2 v 3 d K z q 0 y I G G s b S k k c / X 3 R E Y j Y y Z R Y D s j i i O z 7 M 3 E / 7 x u i u G 1 n w m V p M g V W y w K U 0 k w J r P n y U B o z l B O L K F M C 3 s r Y S O q K U M b U d G G 4 C 2 / v E p a t a p 3 W a 3 d 1 8 r 1 m z y O A p z C G V y A B 1 d Q h z t o Q B M Y S H i G V 3 h z H p 0 X 5 9 3 5 W L S u O f n M C f y B 8 / k D x P u P I Q = = &lt; / l a t e x i t &gt; x1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h 3 D g w o 2 a z x Q R Z 6 c / 6 N 2 F J o O G n 9 4 = " &gt; A A A B 8 n i c b V A 9 T w J B E N 3 D L 8 Q v 1 N J m I 5 h Y k T s s t C T a W G I i H 8 l x I X v L H m z Y 2 7 3 s z h H J h Z 9 h Y 6 E x t v 4 a O / + N C 1 y h 4 E s m e X l v J j P z w k R w A 6 7 7 7 R Q 2 N r e 2 d 4 q 7 p b 3 9 g 8 O j 8 v F J 2 6 h U U 9 a i S i j d D Y l h g k W A g 6 C d R P N S B w K 1 g n H d 3 O / M 2 H a c C U f Y Z q w I C Z D y S N O C V j J r / Y m j G Z P s 7 5 X 7 Z c r b s 1 d A K 8 T L y c V l K P Z L 3 / 1 B o q m M Z N A B T H G 9 9 w E g o x o 4 F S w W a m X G p Y Q O i Z D 5 l s q S c x M k C 1 O n u E L q w x w p L Q t C X i h / p 7 I S G z M N A 5 t Z 0 x g Z F a 9 u f i f 5 6 c Q 3 Q Q Z l 0 k K T N L l o i g V G B S e / 4 8 H X D M K Y m o J o Z r b W z E d E U 0 o 2 J R K N g R v 9 e V 1 0 q 7 X v K t a / a F e a d z m c R T R G T p H l 8 h D 1 6 i B 7 l E T t R B F C j 2 j V / T m g P P i v D s f y 9 a C k 8 + c o j 9 w P n 8 A m V C Q z A = = &lt; / l a t e x i t &gt; x2</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 J q H z x K a P t / A Z k 7 n w h z E X j H Z j + 8   In essence, the discriminator works to maximize the mutual information between a high-level global representation and local representations(node-level), which encourages the encoder to learn the information presenting in all globally relevant locations. The information about a class label can be one of the cases. The above loss can be optimized through the gradient descent, and the representations of nodes can be learned when the optimization is completed. Negative samples generator. The negative sample set</p><formula xml:id="formula_29">= " &gt; A A A B 8 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k b u z 0 J J o Y 4 m J f C R w I X v L H m z Y 2 7 3 s 7 h H J h Z 9 h Y 6 E x t v 4 a O / + N C 1 y h 4 E s m e X l v J j P z w o Q z b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T l p a p I r R J J J e q E 2 J N O R O 0 a Z j h t J M o i u O Q 0 3 Y 4 v p v 7 7 Q l V m k n x a K Y J D W I 8 F C x i B B s r d a u 9 C S X Z 0 6 z v V / v l i l t z F 0 D r x M t J B X I 0 + u W v 3 k C S N K b C E I 6 1 7 n p u Y o I M K 8 M I p 7 N S L 9 U 0 w W S M h 7 R r q c A x 1 U G 2 O H m G L q w y Q J F U t o R B C / X 3 R I Z j r a d x a D t j b E Z 6 1 Z u L / 3 n d 1 E Q 3 Q c Z E k h o q y H J R l H J k J J r / j w Z M U W L 4 1 B J M F L O 3 I j L C C h N j U y r Z E L z V l 9 d J y 6 9 5 V z X / w a / U b / M 4 i n A G 5 3 A J H l x D H e 6 h A U 0 g I O E Z X u H N M c 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A m t W Q z Q = = &lt; / l a t e x i t &gt; x3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E 3 b g o o u r C S z w V A K f l X b i 1 + q f + y s = " &gt; A A A B 8 n i c b V A 9 T w J B E N 3 D L 8 Q v 1 N J m I 5 h Y k T s o t C T a W G I i Y H J c y N 4 y B x v 2 d i + 7 e 0 R y 4 W f Y W G i M r b / G z n / j A l c o + J J J X t 6 b y c y 8 M O F M G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k o 2 W q K L S p 5 F I 9 h k Q D Z w L a h h k O j 4 k C E o c c u u H 4 d u 5 3 J 6 A 0 k + L B T B M I Y j I U L G K U G C v 5 1 d 4 E a P Y 0 6 z e q / X L F r b k L 4 H X i 5 a S C c r T 6 5 a / e Q N I 0 B m E o J 1 r 7 n p u Y I C P K M M p h V u q l G h J C x 2 Q I v q W C x K C D b H H y D F 9 Y Z Y A j q W w J g x f q 7 4 m M x F p P 4 9 B 2 x s S M 9 K o 3 F / / z / N R E 1 0 H G R J I a E H S 5 K E o 5 N h L P / 8 c D p o A a P r W E U M X s r Z i O i C L U 2 J R K N g R v 9 e V 1 0 q n X v E a t f l + v N G / y O I r o D J 2 j S + S h K 9 R E d 6 i F 2 o g i i Z 7 R K 3 p z j P P i v D s f y 9 a C k 8 + c o j 9 w P n 8 A n F q Q z g = = &lt; / l a t e x i t &gt; x4 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B q B a a L d W m A p r K f v y H M 0 0 x v O O N X Q = " &gt; A A A B 8 n i c b V B N T w I x E O 3 i F + I X 6 t F L I 5 h 4 I r t o o k e i F 4 + Y C J I s G 9 I t X W j o t p t 2 l k g 2 / A w v H j T G q 7 / G m / / G A n t Q 8 C W T v L w 3 k 5 l 5 Y S K 4 A d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o b V S q K W t R J Z T u h M Q w w S V r A Q f B O o l m J A 4 F e w x H t z P / c c y 0 4 U o + w C R h Q U w G k k e c E r C S X + 2 O G c 2 e p r 3 L a q 9 c c W v u H H i V e D m p o B z N X v m r 2 1 c 0 j Z k E K o g x v u c m E G R E A 6 e C T U v d 1 L C E 0 B E Z M N 9 S S W J m g m x + 8 h S f W a W P I 6 V t S c B z 9 f d E R m J j J n F o O 2 M C Q 7 P</formula><formula xml:id="formula_30">M p h V u q l G h J C x 2 Q I v q W C x K C D b H H y D F 9 Y Z Y A j q W w J g x f q 7 4 m M x F p P 4 9 B 2 x s S M 9 K o 3 F / / z / N R E 1 0 H G R J I a E H S 5 K E o 5 N h L P / 8 c D p o A a P r W E U M X s r Z i O i C L U 2 J R K N g R v 9 e V 1 0 q n X v K t a</formula><formula xml:id="formula_31">h i Q 8 G H u / N M D M v S K Q w 6 L r f z t r 6 x u b W d m G n u L u 3 f 3 B Y O j p u m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t z O / / c S 1 E b F 6 w E n C / Y g O l Q g F o 2 i l T q X X G I m + V + m X y m 7 V n Y O s E i 8 n Z c j R 6 J e + e o O Y p R F X y C Q 1 p u u 5 C f o Z 1 S i Y 5 N N i L z U 8 o W x M h 7 x r q a I R N 3 4 2 v 3 d K z q 0 y I G G s b S k k c / X 3 R E Y j Y y Z R Y D s j i i O z 7 M 3 E / 7 x u i u G 1 n w m V p M g V W y w K U 0 k w J r P n y U B o z l B O L K F M C 3 s r Y S O q K U M b U d G G 4 C 2 / v E p a t a p 3 W a 3 d 1 8 r 1 m z y O A p z C G V y A B 1 d Q h z t o Q B M Y S H i G V 3 h z</formula><formula xml:id="formula_32">[ hm , s] M m=1</formula><p>is composed of the samples that do not exist in the heterogeneous graph. We extend the negative sample geneation approach proposed in <ref type="bibr" target="#b7">[Veličković et al., 2019]</ref> to heterogeneous graph setting.</p><p>In heterogeneous graph G, we have rich and complex structural information characterized by meta-path based adjacency matrices. Our negative samples generator X, {A Φ1 , A Φ2 , . . . , A Φ P } = C(X, {A Φ1 , A Φ2 , . . . , A Φ P }) (13) keeps all meta-path based adjacency matrices unchanged, which can make the overall structure of G stable. We shuffle the rows of the initial node feature matrix X, which changes the index of nodes in order to corrupt the node-level connections among them. The structure of the whole graph does not change, but the initial feature attribute corresponding to each node is changed. We provide a simple example to illustrate the procedure of generating negative samples in Figure <ref type="figure" target="#fig_14">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We evaluate the performance of HDGI on three heterogeneous graph data, and summarize their details in Table <ref type="table" target="#tab_0">1</ref>.  • DBLP <ref type="bibr" target="#b3">[Gao et al., 2009]</ref>: This is a research paper set, which contains scientific publications and the corresponding authors. The target author node can be divided into 4 areas: database, data mining, information retrieval, and machine learning. We use the area of authors as the labels.</p><p>The initial features are generated based on authors' profiles with the bag-of-words embeddings . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiment Setup</head><p>The most commonly used tasks to measure the quality of learned representations are node classification <ref type="bibr" target="#b6">[Perozzi et al., 2014;</ref><ref type="bibr" target="#b3">Grover and Leskovec, 2016;</ref><ref type="bibr" target="#b4">Hamilton et al., 2017b]</ref> and node clustering <ref type="bibr" target="#b0">[Dong et al., 2017;</ref><ref type="bibr" target="#b9">Wang et al., 2019]</ref>. We evaluate HDGI from both two types of tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compared Baselines</head><p>HDGI is compared with the following supervised and unsupervised methods: Unsupervised methods</p><p>• Raw Feature: The initial features are used as embeddings.</p><p>• Metapath2vec (M2V) <ref type="bibr" target="#b0">[Dong et al., 2017]</ref>: A meta-path based graph embedding method for heterogeneous graph. We test all meta-paths and report the best result. classes. We do not use any label in this unsupervised learning task and make the comparison among all unsupervised learning methods. We also repeat the clustering process for 10 times and report the average NMI and ARI in Table <ref type="table">2</ref>. Deep-Walk cannot perform well because they are not able to handle the graph heterogeneity. Metapath2vec cannot handle diversitified semantic information simultaneously, which makes the representations not effective enough. The verification based on node clustering tasks also demonstrates that HDGI can learn effective representations by considering the structural information, the semantic information and the node independent information simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HDGI-A vs HDGI-C</head><p>From the comparison between HDGI-C and HDGI-A in node classification tasks, the results reflects some interesting findings. HDGI-C achieves better performance than HDGI-A in all experiments, which means that the graph convolution works better than the attention mechanism in capturing local network structures. The reason might be that the graph attention mechanism is strictly limited to the direct neighbors of nodes, the graph convolution considering hierarchical dependencies can see farther. This analysis can also be verified by the results of the clustering task. can find that this advantage is very subtle. In fact, each function can perform well on experimental datasets. But for larger and more complex heterogeneous graphs, a specified and sophisticated function may perform better. The design of the global encoder function for heterogeneous graphs with different scales and structures is an open question, which is worthy of further discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose an unsupervised graph neural network, HDGI , which learns node representations in heterogeneous graphs. HDGI combines several state-of-the-art techniques. It employs convolution-style GNNs along with a semantic-level attention mechanism to capture individual local representations of nodes. Through maximizing the localglobal mutual information, HDGI learns high-level representations containing graph-level structural information. It exploits the structure of meta-path to model the connection semantics in heterogeneous graphs. We demonstrate the effectiveness of learned representations for both node classification and clustering tasks on three heterogeneous graphs. HDGI is particularly competitive in node classification tasks with state-of-the-art supervised methods, where they have the additional supervised label information. We are optimistic that mutual information maximization is a promising future direction for unsupervised representation learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of heterogeneous bibliographic graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>r X g X l e p d t V S 7 z u L I w w m c w j l 4 c A k 1 u I U 6 N I D B E J 7 h F d 4 c 4 b w 4 7 8 7 H o j X n Z D P H 8 A f O 5 w + T / Y 1 T &lt; / l a t e x i t &gt; C</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>g a 9 l A O O P A Z d b 3 g 9 8 b s j 4 I I m 8 Z 0 c p + B E e B D T g B I s l e T q x z V 7 B C S 3 J</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>g a 9 l A O O P A Z d b 3 g 9 8 b s j 4 I I m 8 Z 0 c p + B E e B D T g B I s l e T q x z V 7 B C S 3 J</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>6 E 1 7 0 l 6 0 d + 1 j 2 V r R y p k 6 + g P t 8 w f M s p T c &lt; / l a t e x i t &gt; hi &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 1 S A i s g S H U s d j qt 4 A b C L N l W x D u 0 = " &gt; A A A B 8 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N L m I p h Y k T s s t C T a W G I iS A I X s r f M w Y a 9 3 c v u H g m 5 8 D N s L D T G 1 l 9 j 5 7 9 x g S s U f M k k L + / N Z G Z e m H C m j e d 9 O 4 W N z a 3 t n e J u a W / / 4 P</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2</head><label>2</label><figDesc>Figure 2 The high-level structure of HDGI . Local representation encoder is a hierarchical structure: learning node representations in terms of every meta-path based adjacency matrix respectively and then aggregating them through semantic-level attention. Global representation encoder R outputs a graph-level summary vector s. Negative samples generator C is responsible for generating negative nodes. The discriminator D maximizes mutual information between positive nodes and the graph-level summary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>p</head><label></label><figDesc>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W C G R 7 w 1 G 2 t H 4 J S i f B y A 2 k Y m P y e 8 = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b t Y a B m 0 s Y x g P i A 5 w t 5 m L l m y t 3 f u 7 g n h y J + w s V D E 1 r 9 j 5 7 9 x k 1 y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>H p 0 X 5 9 3 5 W L S u O f n M C f y B 8 / k D J M W P Y A = = &lt; / l a t e x i t &gt; … C &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s 4 e e B H A v 4 5 kU 4 G G e v 0 A Q O t a R d 0 0 = " &gt; A A A B 9 H i c b V B N T w I x F H y L X 4 h f q E c v j W D i i e z i Q Y 9 E L h 4 x E T C B D e m W L j R 0 2 7 X t k p A N v 8 O L B 4 3 x 6 o / x 5 r + x C 3 t Q c J I m k 5 n 3 8 q Y T x J x p 4 7 r f T m F j c 2 t 7 p 7 h b 2 t s / O D w q H 5 9 0 t E w U o W 0 i u V S P A d a U M 0 H b h h l O H 2 N F c R R w 2 g 0 m z c z v T q n S T I o H M 4 u p H + G R Y C E j 2 F j J r / Y j b M Y E 8 7 Q 5 r w 7 K F b f m L o D W i Z e T C u R o D c p f / a E k S U S F I R x r 3 f P c 2 P g p V o Y R T u e l f q J p j M k E j 2 j P U o E j q v 1 0 E X q O L q w y R K F U 9 g m D F u r v j R R H W s + i w E 5 m G f W q l 4 n / e b 3 E h D d + y k S c G C r I 8 l C Y c G Q k y h p A Q 6 Y o M X x m C Sa K 2 a y I j L H C x N i e S r Y E b / X L 6 6 R T r 3 l X t f p 9 v d K 4 z e s o w h m c w y V 4 c A 0 N u I M W t I H A E z z D K 7 w 5 U + f F e X c + l q M F J 9 8 5 h T 9 w P n 8 A M U i R t w = = &lt; / l a t e x i t &gt; x1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h 3 D g w o 2 a z x Q R Z 6 c / 6 N 2 F J o O G n 9 4 = " &gt; A A A B 8 n i c b V A 9 T w J B E N 3 D L 8 Q v 1 N J m I 5 h Y k T s s t C T a W G I i H 8 l x I X v L H m z Y 2 7 3 s z h H J h Z 9 h Y 6 E x t v 4 a O / + N C 1 y h 4 E s m e X l v J j P z w k R w A 6 7 7 7 R Q 2 N r e 2 d 4 q 7 p b 3 9 g 8 O j 8 v F J 2 6 h U U 9 a i S i j d D Y l h g k v W A g 6 C d R P N S B w K 1 g n H d 3 O / M 2 H a c C U f Y Z q w I C Z D y S N O C V j J r / Y m j G Z P s 7 5 X 7 Z c r b s 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>a F e a d z m c R T R G T p H l 8 h D 1 6 i B 7 l E T t R B F C j 2 j V / T m g P P i v D s f y 9 a C k 8 + c o j 9 w P n 8 A m V C Q z A = = &lt; / l a t e x i t &gt; x2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>9 d J y 6 9 5 V z X / w a / U b / M 4 i n A G 5 3 A J H l x D H e 6 h A U 0 gI O E Z X u H N M c 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A m t W Q z Q = = &lt; / l a t e x i t &gt; x3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E 3 b g o o u r C S z w V A K f l X b i 1 + q f + y s = " &gt; A A A B 8 n i c b V A 9 T w J B E N 3 D L 8 Q v 1 N J m I 5 h Y k T s o t C T a W G I i Y H J c y N 4 y B x v 2 d i + 7 e 0 R y 4 W f Y W G i M r b / G z n / j A l c o + J J J X t6 b y c y 8 M O F M G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k o 2 W q K L S p 5 F I 9 h k Q D Z w L a h h k O j 4 k C E o c c u u H 4 d u 5 3 J 6 A 0 k + L B T B M I Y j I U L G K U G C v 5 1 d 4 E a P Y 0 6 z e q / X L F r b k L 4 H X i 5 a S C c r T 6 5 a / e Q N I 0 B m E o J 1 r 7 n p u Y I C P K M</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>y O I r o D J 2 j S + S h K 9 R E d 6 i F 2 o g i i Z 7 R K 3 p z j P P i v D s f y 9 a C k 8 + c o j 9 w P n 8 A n F q Q z g = = &lt; / l a t e x i t &gt; x4 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B q B a aL d W m A p r K f v y H M 0 0 x v O O N X Q = " &gt; A A A B 8 n i c b V B N T w I x E O 3 i F + I X 6 t F L I 5 h 4 I r t o o k e i F 4 + Y C J I s G 9 I t X W j o t p t 2 l k g 2 / A w v H j T G q 7 / G m / / G A n t Q 8 C W T v L w 3 k 5 l 5 Y S K 4 A d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o b V S q K W t R J Z T u h M Q w w S V r A Q f B O o l m J A 4 F e w x H t z P / c c y 0 4 U o + w C R h Q U w G k k e c E r C S X + 2 O G c 2 e p r 3 L a q 9 c c W v u H H i V e D m p o B z N X v m r 2 1 c 0 j Z k E K o g x v u c m E G R E A 6 e C T U v d 1 L C E 0 B E Z M N 9 S S W J m g m x + 8 h S f W a W P I 6 V t S c B z 9 f d E R m J j J n F o O 2 M C Q 7 Ps z c T / P D + F 6 D r I u E x S Y J I u F k W p w K D w 7 H / c 5 5 p R E B N L C N X c 3 o r p k G h C w a Z U s i F 4 y y + v k n a 9 5 l 3 U 6 v f 1 S u M m j 6 O I T t A p O k c e u k I N d I e a q I U o U u g Z v a I 3 B 5 w X 5 9 3 5 W L Q W n H z m G P 2 B 8 / k D n d + Q z w = = &lt; / l a t e x i t &gt; x5 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d U c n k i 3 Q N e D g L i B z Z M m c 1 3 2 o o L U = " &gt; A A A B 8 n i c b V B N T w I x E O 3 i F + I X 6 t F L I 5 h 4 I r s Y o 0 e i F 4 + Y C</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>s z c T / P D + F 6 D r I u E x S Y J I u F k W p w K D w 7 H / c 5 5 p R E B N L C N X c 3 o r p k G h C w a Z U s i F 4 y y + v k n a 9 5 l 3 U 6 v f 1 S u M m j 6 O I T t A p O k c e u k I N d I e a q I U o U u g Z v a I 3 B 5 w X 5 9 3 5 W L Q W n H z m G P 2 B 8 / k D n 2 S Q 0 A = = &lt; / l a t e x i t &gt; 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W 7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>s z c T / P D + F 6 D r I u E x S Y J I u F k W p w K D w 7 H / c 5 5 p R E B N L C N X c 3 o r p k G h C w a Z U s i F 4 y y + v k n a 9 5 l 3 U 6 v f 1 S u M m j 6 O I T t A p O k c e u k I N d I e a q I U o U u g Z v a I 3 B 5 w X 5 9 3 5 W L Q W n H z m G P 2 B 8 / k D n d + Q z w = = &lt; / l a t e x i t &gt; x5 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d U c n k i 3 Q N e D g L i B z Z M m c 1 3 2 o o L U = " &gt; A A A B 8 n i c b V B N T w I x E O 3 i F + I X 6 t F L I 5 h 4 I r s Y o 0 e i F 4 + Y C J I s G 9 I t X W j o t p t 2 l k g 2 / A w v H j T G q 7 / G m / / G A n t Q 8 C W T v L w 3 k 5 l 5 Y S K 4 A d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o bV S q K W t R J Z T u h M Q w w S V r A Q f B O o l m J A 4 F e w x H t z P / c c y 0 4 U o + w C R h Q U w G k k e c E r C S X + 2 O G c 2 e p r 3 L a q 9 c c W v u H H i V e D m p o B z N X v m r 2 1 c 0 j Z k E K o g x v u c m E G R E A 6 e C T U v d 1 L C E 0 B E Z M N 9 S S W J m g m x + 8 h S f W a W P I 6 V t S c B z 9 f d E R m J j J n F o O 2 M C Q 7 P s z c T / P D + F 6 D r I u E x S Y J I u F k W p w K D w 7 H / c 5 5 p R E B N L C N X c 3 o r p k G h C w a Z U s i F 4 y y + v k n a 9 5 l 3 U 6 v f 1 S u M m j 6 O I T t A p O k c e u k I N d I e a q I U o U u g Z v a I 3 B 5 w X 5 9 3 5 W L Q W n H z m G P 2 B 8 / k D n 2 S Q 0 A = = &lt; / l a t e x i t &gt; x1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h 3 D g w o 2 a z x Q R Z 6 c / 6 N 2 F J o O G n 9 4 = " &gt; A A A B 8 n i c b V A 9 T w J B E N 3 D L 8 Q v 1 N J m I 5 h Y k T s s t C T a W G I i H 8 l x I X v L H m z Y 2 7 3 s z h H J h Z 9 h Y 6 E x t v 4 a O / + N C 1 y h 4 E s m e X l v J j P z w k R w A 6 7 7 7 R Q 2 N r e 2 d 4 q 7 p b 3 9 g 8 O j 8 v F J 2 6 h U U 9 a i S i j d D Y l h g k v W A g 6 C d R P N S B w K 1 g n H d 3 O / M 2 H a c C U f Y Z q w I C Z D y S N O C V j J r / Y m j G Z P s 7 5 X 7 Z c r b s 1 d A K 8 T L y c V l K P Z L 3 / 1 B o q m M Z N A B T H G 9 9 w E g o x o 4 F S w W a m X G p Y Q O i Z D 5 l s q S c x M k C 1 O n u E L q w x w p L Q t C X i h / p 7 I S G z M N A 5 t Z 0 x g Z F a 9 u f i f 5 6 c Q 3 Q Q Z l 0 k K T N L l o i g V G B S e / 4 8 H X D M K Y m o J o Z r b W z E d E U 0 o 2 J R K N g R v 9 e V 1 0 q 7 X v K t a / a F e a d z m c R T R G T p H l 8 h D 1 6 i B 7 l E T t R B F C j 2 j V / T m g P P i v D s f y 9 a C k 8 + c o j 9 w P n 8 A m V C Q z A = = &lt; / l a t e x i t &gt; x6 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W J A o z P M x N s X G t P k p J G X m D z S L g v U = " &gt; A A A B 8 n i c b V B N T w I x E O 3 i F + I X 6 t F L I 5 h 4 I r u Y q E e i F 4 + Y C J I s G 9 I t X W j o t p t 2 l k g 2 / A w v H j T G q 7 / G m / / G A n t Q 8 C W T v L w 3 k 5 l 5 Y S K 4 A d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o b V S q K W t R J Z T u h M Q w w S V r A Q f B O o l m J A 4 F e w x H t z P / c c y 0 4 U o + w C R h Q U w G k k e c E r C S X + 2 O G c 2 e p r 3 L a q 9 c c W v u H H i V e D m p o B z N X v m r 2 1 c 0 j Z k E K o g x v u c m E G R E A 6 e C T U v d 1 L C E 0 B E Z M N 9 S S W J m g m x + 8 h S f W a W P I 6 V t S c B z 9 f d E R m J j J n F o O 2 M C Q 7 P s z c T / P D + F 6 D r I u E x S Y J I u F k W p w K D w 7 H / c 5 5 p R E B N L C N X c 3 o r p k G h C w a Z U s i F 4 y y + v k n a 9 5 l 3 U 6 v f 1 S u M m j 6 O I T t A p O k c e u k I N d I e a q I U o U u g Z v a I 3 B 5 w X 5 9 3 5 W L Q W n H z m G P 2 B 8 / k D o O m Q 0 Q = = &lt; / l a t e x i t &gt; x2&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 J q H z x K a P t / A Z k 7 n w h z E X j H Z j + 8= " &gt; A A A B 8 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k b u z 0 J J o Y 4 m J f C R w I X v L H m z Y 2 7 3 s 7 h H J h Z 9 h Y 6 E x t v 4 a O / + N C 1 y h 4 E s m e X l v J j P z w o Q z b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T l p a p I r R J J J e q E 2 J N O R O 0 a Z j h t J M o i u O Q 0 3 Y 4 v p v 7 7 Q l V m k n x a K Y J D W I 8 F C x i B B s r d a u 9 C S X Z 0 6 z v V / v l i l t z F 0 D r x M t J B X I 0 + u W v 3 k C S N K b C E I 6 1 7 n p u Y o I M K 8 M I p 7 N S L 9 U 0 w W S M h 7 R r q c A x 1 U G 2 O H m G L q w y Q J F U t o R B C / X 3 R I Z j r a d x a D t j b E Z 6 1 Z u L / 3 n d 1 E Q 3 Q c Z E k h o q y H J R l H J k J J r / j w Z M U W L 4 1 B J M F L O 3 I j L C C h N j U y r Z E L z V l9 d J y 6 9 5 V z X / w a / U b / M 4 i n A G 5 3 A J H l x D H e 6 h A U 0 g I O E Z X u H N M c 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A m t W Q z Q = = &lt; / l a t e x i t &gt; x7 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M B B 8 x B r S J 7 k A P H 3 C m W 5 + 6 z O A L K Y = " &gt; A A A B 8 n i c b V A 9 T w J B E N 3 D L 8 Q v 1 N J m I 5 h Y k T s s s C T a W G I i Y H J c y N 4 y B x v 2 d i + 7 e 0 R y 4 W f Y W G i M r b / G z n / j A l c o + J J J X t 6 b y c y 8 M O F M G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k o 2 W q K L S p 5 F I 9 h k Q D Z w L a h h k O j 4 k C E o c c u u H 4 d u 5 3 J 6 A 0 k + L B T B M I Y j I U L G K U G C v 5 1 d 4 E a P Y 0 6 z e q / X L F r b k L 4 H X i 5 a S C c r T 6 5 a / e Q N I 0 B m E o J 1 r 7 n p u Y I C P K MM p h V u q l G h J C x 2 Q I v q W C x K C D b H H y D F 9 Y Z Y A j q W w J g x f q 7 4 m M x F p P 4 9 B 2 x s S M 9 K o 3 F / / z / N R E 1 0 H G R J I a E H S 5 K E o 5 N h L P / 8 c D p o A a P r W E U M X s r Z i O i C L U 2 J R K N g R v 9 e V 1 0 q n X v K t a / b 5 e a d 7 k c R T R G T p H l 8 h D D d R E d 6 i F 2o g i i Z 7 R K 3 p z j P P i v D s f y 9 a C k 8 + c o j 9 w P n 8 A o m 6 Q 0 g = = &lt; / l a t e x i t &gt; p &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W C G R 7 w 1 G 2 t H 4 J S i f B y A 2 k Y m P y e 8 = " &gt; A A A B 7 3 c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b t Y a B m 0 s Y x g P i A 5 w t 5 m L l m y t 3 f u 7 g n h y J + w s V D E 1 r 9 j 5 7 9 x k 1 y hi Q 8 G H u / N M D M v S A T X x n W / n b X 1 j c 2 t 7 c J O c X d v / + C w d H T c 0 n G q G D Z Z L G L V C a h w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s s H M 0 n Q j + h Q 8 p A z a q z U q f Q a I 9 5 P K v 1 S 2 a 2 6 c 5 B V 4 u W k D D k a / d J X b x C z N E J p m K B a d z 0 3 M X 5 G l e F M 4 L T Y S z U m l I 3 p E L u W S h q h 9 r P 5 v V N y b p U B C W N l S x o y V 3 9 P Z D T S e h I F t j O i Z q S X v Z n 4 n 9 d N T X j t Z 1 w m q U H J F o v C V B A T k 9 n z Z M A V M i M m l l C m u L 2 V s B F V l B k b U d G G 4 C 2 / v E p a t a p 3 W a 3 d 1 8 r 1 m z y O A p z C G V y A B 1 d Q h z t o Q B M Y C H i G V 3 h zH p 0 X 5 9 3 5 W L S u O f n M C f y B 8 / k D J M W P Y A = = &lt; / l a t e x i t &gt; x6 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W J A o z P M x N s X G t P k p J G X m D z S L g v U = " &gt; A A A B 8 n i c b V B N T w I x E O 3 i F + I X 6 t F L I 5 h 4 I r u Y q E e i F 4 + Y C J I s G 9 I t X W j o t p t 2 l k g 2 / A w v H j T G q 7 / G m / / G A n t Q 8 C W T v L w 3 k 5 l 5 Y S K 4 A d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o b V S q K W t R J Z T u h M Q w w S V r A Q f B O o l m J A 4 F e w x H t z P / c c y 0 4 U o + w C R h Q U w G k k e c E r C S X + 2 O G c 2 e p r 3 L a q 9 c c W v u H H i V e D m p o B z N X v m r 2 1 c 0 j Z k E K o g x v u c m E G R E A 6 e C T U v d 1 L C E 0 B E Z M N 9 S S W J m g m x + 8 h S f W a W P I 6 V t S c B z 9 f d E R m J j J n F o O 2 M C Q 7 P s z c T / P D + F 6 D r I u E x S Y J I u F k W p w K D w 7 H / c 5 5 p R E B N L C N X c 3 o r p k G h C w a Z U s i F 4 y y + v k n a 9 5 l 3 U 6 v f 1 S u M m j 6 O I T t A p O k c e u k I N d I e a q I U o U u g Z v a I 3 B 5 w X 5 9 3 5 W L Q W n H z m G P 2 B 8 / k D o O m Q 0 Q = = &lt; / l a t e x i t &gt; x7 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M B B 8 x B r S J 7 k A P H 3 C m W 5 + 6 z O A L K Y = " &gt; A A A B 8 n i c b V A 9 T w J B E N 3 D L 8 Q v 1 N J m I 5 h Y k T s s s C T a W G I i Y H J c y N 4 y B x v 2 d i + 7 e 0 R y 4 W f Y W G i M r b / G z n / j A l c o + J J J X t 6 b y c y 8 M O F M G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k o 2 W q K L S p 5 F I 9 h k Q D Z w L a h h k O j 4 k C E o c c u u H 4 d u 5 3 J 6 A 0 k + L B T B M I Y j I U L G K U G C v 5 1 d 4 E a P Y 0 6 z e q / X L F r b k L 4 H X i 5 a S C c r T 6 5 a / e Q N I 0 B m E o J 1 r 7 n p u Y I C P K M</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>/ b 5 e a d 7 k c R T R G T p H l 8 h D D d R E d 6 i F 2 o g i i Z 7 R K 3 p z j P P i v D s f y 9 a C k 8 + c o j 9 w P n 8 A o m 6 Q 0 g = = &lt; / l a t e x i t &gt; x4 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "B q B a a L d W m A p r K f v y H M 0 0 x v O O N X Q = " &gt; A A A B 8 n i c b V B N T w I x E O 3 i F + I X 6 t F L I 5 h 4 I r t o o k e i F 4 + Y C J I s G 9 I t X W j o t p t 2 l k g 2 / A w v H j T G q 7 / G m / / G A n t Q 8 C W T v L w 3 k 5 l 5 Y S K 4 A d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o b V S q K W t R J Z T u h M Q w w S V r A Q f B O o l m J A 4 F e w x H t z P / c c y 0 4 U o + w C R h Q U w G k k e c E r C S X + 2 O G c 2 e p r 3 L a q 9 c c W v u H H i V e D m p o B z N X v m r 2 1 c 0 j Z k E K o g x v u c m E G R E A 6 e C T U v d 1 L C E 0 B E Z M N 9 S S W J m g m x + 8 h S f W a W P I 6 V t S c B z 9 f d E R m J j J n F o O 2 M C Q 7 Ps z c T / P D + F 6 D r I u E x S Y J I u F k W p w K D w 7 H / c 5 5 p R E B N L C N X c 3 o r p k G h C w a Z U s i F 4 y y + v k n a 9 5 l 3 U 6 v f 1 S u M m j 6 O I T t A p O k c e u k I N d I e a q I U o U u g Z v a I 3 B 5 w X 5 9 3 5 W L Q W n H z m G P 2 B 8 / k D n d + Q z w = = &lt; / l a t e x i t &gt; x5 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d U c n k i 3 Q N e D g L i B z Z M m c 1 3 2 o o L U = " &gt; A A A B 8 n i c b V B N T w I x E O 3 i F + I X 6 t F L I 5 h 4 I r s Y o 0 e i F 4 + Y C J I s G 9 I t X W j o t p t 2 l k g 2 / A w v H j T G q 7 / G m / / G A n t Q 8 C W T v L w 3 k 5 l 5 Y S K 4 A d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3v 5 B + f C o b V S q K W t R J Z T u h M Q w w S V r A Q f B O o l m J A 4 F e w x H t z P / c c y 0 4 U o + w C R h Q U w G k k e c E r C S X + 2 O G c 2 e p r 3 L a q 9 c c W v u H H i V e D m p o B z N X v m r 2 1 c 0 j Z k E K o g x v u c m E G R E A 6 e C T U v d 1 L C E 0 B E Z M N 9 S SW J m g m x + 8 h S f W a W P I 6 V t S c B z 9 f d E R m J j J n F o O 2 M C Q 7 P s z c T / P D + F 6 D r I u E x S Y J I u F k W p w K D w 7 H / c 5 5 p R E B N L C N X c 3 o r p k G h C w a Z U s i F 4 y y + v k n a 9 5 l 3 U 6 v f 1 S u M m j 6 O I T t A p O k c e u k I N d I e a q I U o U u g Z v a I 3 B 5 w X 5 9 3 5 W L Q W n H z m G P 2 B 8 / k D n 2 S Q 0 A = = &lt; / l a t e x i t &gt; … 1&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W 7 Q k d F k C i V 0 7 P 6 f f A j x 3 9 0 k n C X g = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b t Y a B m 0 s Y x g P i A 5 w t 5 m L 1 m y t 3 f u z g n h y J + w s V D E 1 r 9 j 5 7 9 x k 1 y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The example of generating negative samples Here W D is a learnable matrix and σ is the sigmoid activation function. In our problem, based on the approximately monotonic relationship between Jensen-Shannon divergence and mutual information [Hjelm et al., 2019], we can maximize the mutual information with the binary cross-entropy loss of the discriminator: L(P os, N eg, s) = 1 N + M N</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>• ACM<ref type="bibr" target="#b9">[Wang et al., 2019]</ref>: This is another academic paper data in which target paper nodes are categorized into 3 classes: database, wireless communication and data Mining. The initial features are constructed from paper keywords with the TF-IDF based embedding techniques.• IMDB<ref type="bibr" target="#b10">[Wu et al., 2016]</ref>: It is a knowledge graph data about movies (target nodes) that can be categorized into three types: Action, Comedy, and Drama. The feature of a movie is composed of {color, title, language, keywords, country, rating, year} with a TF-IDF encoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>•</head><label></label><figDesc>DeepWalk (DW) [Perozzi et al., 2014]: A random walk based graph embedding method for homogeneous graph. • DeepWalk+Raw Feature(DW+F): It concatenates the learned DeepWalk embeddings with the raw features as the final representations. • DGI [Veličković et al., 2019]: A mutual information based graph representation method for homogeneous graph. • HDGI-C: It is a variant of HDGI which uses graph convolutional network to capture local representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The comparison between different global representation encoder functions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of experimented datasets.</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b t Y a B m 0 s Y x g P i A 5 w t 5 m L l m y t 3 f u 7 g n h y J + w s V D E 1 r 9 j 5 7 9 x k 1 y h i Q 8 G H u / N M D M v S A T X x n W / n b X 1 j c 2 t 7 c J O c X d v / + C w d H T c 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s s H M 0 n Q j + h Q 8 p A z a q z U q f Q a I 9 5 P K v 1 S 2 a 2 6 c 5 B</p><p>m 0 s Y x g P i A 5 w t 5 m L 1 m y t 3 f u z g n h y J + w s V D E 1 r 9 j 5 7 9 x k 1 y h</p><p>H p 0 X 5 9 3 5 W L S u O f n M C f y B 8 / k D x P u P I Q = = &lt; / l a t e x i t &gt; p &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W C G R 7 w 1 G 2 t H 4 J S i f B y A 2 k Y m P y e 8 = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b t Y a B m 0 s Y x g P i A 5 w t 5 m L l m y t 3 f u 7 g n h y J + w s V D E 1 r 9 j 5 7 9 x k 1 y</p><p>H p 0 X 5 9 3 5 W L S u O f n M C f y B 8 / k D J M W P Y 8 Q v 1 N J m I 5 h Y k T s s t C T a W G I i H w Y I 2 V v m Y M P u 3 W V 3 j 4 R c + B U 2 F h p j 6 8 + x 8 9 + 4 w B U K v m S S l / d m M j P P j w X X x n W / n d z G 5 t b 2 T n 6 3 s L d / c H h U P D 5 p 6 i h R D B s s E p F q + 1 S j 4 C E 2 D D c C 2 7 F C K n 2 B L X 9 8 N / d b E 1 S a R + G j m c b Y k 3 Q Y 8 o A z a q z 0 V O 5 O k K V 6 V u 4 X S 2 7 F X Y C s E y 8 j J c h Q 7 x e / u o O I J R J D w w T V u u O 5 s e m l V B n O B M 4 K 3 U R j T N m Y D r F j a U g l 6 l 6 6 O H h G L q w y I E G k b I W G L N T f E y m V W k + l b z s l N S O 9 6 s 3 F / 7 x O Y o K b X s r D O D E Y s u W i I B H E R G T + P R l w h c y I q S W U K W 5 v J W x E F W X G Z l S w I X i r L 6 t 3 9 Q P j x q 6 y h R l L V o J C L V 8 Y l m g k v W M t w I 1 o k V I 6 E v 2 K M / v s n 8 x w l T m k f y w U x j 5 o V k K H n A K T F W 8 q q 9 k J g R J S K 9 n V X 7 5 Y p T c + b A q 8 T N S Q V y N P v l r 9 4 g o k n I p K G C a N 1 1 n d h 4 K V G G U 8 F m p V 6 i W U z o m A x Z 1 1 J J Q q a 9 d B 5 6 h s + s M s B B p O y T B s / V 3 x s p C b W e h r 6 d z D L q Z S 8 T / / O 6 i Q m u v J T L O D F M 0 s W h I B H Y R D h r A A</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>tion learning on all nodes labeled with entity types in heterogeneous graphs. We report the best performance from all meta-paths and the graph ignoring type information.</p><p>• GAT <ref type="bibr">[Veličković et al., 2017]</ref>: GAT applies the attention mechanism in homogeneous graphs for node classification.</p><p>• HAN <ref type="bibr" target="#b9">[Wang et al., 2019]</ref>: HAN employs node-level attention and semantic-level attention to capture the information from all meta-paths.</p><p>For methods designed for homogeneous graphs, i.e., Deep-Walk, DGI, GCN, GAT, we do not consider graph heterogeneity and construct meta-path based adjacency matrix, then we report the best performance. We test all meta-paths for Metapath2vec and report the best result. For RGCN, because our task is to learn the representations of target-type nodes, the cross-entropy loss is calculated by the classification in target-type nodes only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproducibility</head><p>For the proposed HDGI including HDGI-C and HDGI-A, we optimize the model with <ref type="bibr">Adam [Kingma and Ba, 2015]</ref>. The dimension of node-level representations in HDGI-C is set as 512 and the dimension of q is set as 8. For HDGI-A, we set the dimension of node-level representations as 64 and the attention head is set as 4. The dimension of q is set as 8 as well. We employ Pytorch to implement our model and conduct experiments in the server with 4 GTX-1080ti GPUs. Code is available at https://github.com/YuxiangRen/Heterogeneous-Deep-Graph-Infomax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node classification task</head><p>In the node classification task, we train a logistic regression classifier for unsupervised learning methods, while the supervised methods output the classification result as end-to-end models. We conduct the experiments with two different training ratios (20% and 80%). The ratios of validation set and test set are fixed at 10%. To keep the results stable, we repeat the classification process for 10 times and report the average Macro-F1 and Micro-F1 in Table <ref type="table">2</ref>. We observe that HDGI-C outperforms all other unsupervised learning methods. When competing with the supervised learning methods (designed for homogeneous graphs like GCN and GAT ), HDGI can perform much better. This observation proves that the type and semantic information are very important and need to be handled carefully instead of directly ignoring them in heterogeneous graphs. The result of RGCN is suboptimal, because the original RGCN is a featureless approach and we follow the code to assign a one-hot vector to each node. In addition, the unified learning of all types of nodes in the same latent space, is beneficial to entity type classification but may not be applicable to label classification. HDGI is also competitive with the result reported from HAN which is designed for heterogeneous graphs. The reason should be that HDGI can capture more global structural information when exploring the mutual information in reconstructing the representation, while supervised loss based GNNs overemphasize the direct neighborhoods <ref type="bibr" target="#b7">[Veličković et al., 2019]</ref>. This observation, on the other hand, suggests that the features learned through supervised learning in graph structures may have limitations, either from the structure or a task-based preference. These limitations can affect the learning representations from a more general perspective badly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node clustering task</head><p>In the node clustering task, we use the K-Means to conduct the clustering based on the learned representations. The number of clusters K is set as the number of the target node</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Yuxiao Dong, Nitesh V Chawla, and Ananthram Swami. metapath2vec: Scalable representation learning for heterogeneous networks</title>
		<author>
			<persName><surname>Belghazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</title>
				<meeting>the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2018. 2018. 2018. 2018. 2017. 2017</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
	<note>Mine: mutual information neural estimation. ICML</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning graph representations with embedding propagation</title>
		<author>
			<persName><forename type="first">Niepert</forename><forename type="middle">;</forename><surname>Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="5119" to="5130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hin2vec: Explore meta-paths in heterogeneous information networks for representation learning</title>
		<author>
			<persName><forename type="first">Fey</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
				<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2018. 2018. 2017. 2017</date>
			<biblScope unit="page" from="1797" to="1806" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graph-based consensus maximization among multiple supervised and unsupervised models</title>
		<author>
			<persName><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009. 2009. 2016. 2016. 2017a. 2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
				<imprint>
			<date type="published" when="2017">2017b. 2017. 2019. 2019</date>
		</imprint>
	</monogr>
	<note>IEEE Data Engineering Bulletin</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
				<imprint>
			<publisher>Kingma and Ba</publisher>
			<date type="published" when="2015">2018. 2018. 2015. 2015. 2017a. 2017. 2017</date>
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
	<note>International Conference on Learning Representation</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pathsim: Meta path-based top-k similarity search in heterogeneous information networks</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Manjunath Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Ou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05081</idno>
		<idno>arXiv:1710.10903</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>International World Wide Web Conferences Steering Committee</publisher>
			<date type="published" when="2011">2016. 2016. 2016. 2016. 2014. 2014. 2018. 2018. 2011. 2011. 2019. 2015. 2015. 2017</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the 24th international conference on world wide web. Veličković et al., 2017] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><surname>Veličković</surname></persName>
		</author>
		<title level="m">Deep graph infomax. International Conference on Learning Representation</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2017">2017a. 2017. 2017b. 2017</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
		</imprint>
	</monogr>
	<note>Community preserving network embedding</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Heterogeneous graph attention network</title>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="2022" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Explaining reviews and ratings with paco: Poisson additive co-clustering</title>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="127" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Collective classification via discriminative matrix factorization on sparsely labeled networks</title>
		<author>
			<persName><forename type="first">Yang</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08773</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
				<meeting>the 25th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015. 2015. 2018. 2018. 2016. 2016. 2019. 2019</date>
			<biblScope unit="page" from="690" to="698" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, WSDM &apos;19</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09874</idno>
		<title level="m">Jiawei Zhang. Social network fusion and mining: a survey</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Graph neural networks for small graph and giant network representation learning: An overview</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.00187</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
