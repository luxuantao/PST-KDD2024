<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Combining Knowledge with Deep Convolutional Neural Networks for Short Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jin</forename><surname>Wang</surname></persName>
							<email>jinwang@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Los Angeles. ‡ Facebook Inc. Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Los Angeles. ‡ Facebook Inc. Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dawei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Los Angeles. ‡ Facebook Inc. Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
							<email>junyan@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Los Angeles. ‡ Facebook Inc. Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Combining Knowledge with Deep Convolutional Neural Networks for Short Text Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text classification is a fundamental task in NLP applications. Most existing work relied on either explicit or implicit text representation to address this problem. While these techniques work well for sentences, they can not easily be applied to short text because of its shortness and sparsity. In this paper, we propose a framework based on convolutional neural networks that combines explicit and implicit representations of short text for classification. We first conceptualize a short text as a set of relevant concepts using a large taxonomy knowledge base. We then obtain the embedding of short text by coalescing the words and relevant concepts on top of pre-trained word vectors. We further incorporate character level features into our model to capture fine-grained subword information. Experimental results on five commonly used datasets show that our proposed method significantly outperforms state-of-the-art methods. * This work is done when the authors were in MSRA.</p><p>advanced text representation models. According to the different ways of leveraging external sources, previous text representation models can be divided into two categories: explicit representation and implicit representation [Wang and Wang,  2016].</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text classification is a crucial technology in many applications, such as web search, ads matching, and sentiment analysis. Previous studies on text classification either rely on human designed features <ref type="bibr" target="#b5">[Wang and Manning, 2012]</ref> or use deep neural networks on distributed representation of texts <ref type="bibr" target="#b1">[Conneau et al., 2016]</ref>. Despite the impressive advances for sentences and documents, such methods still have limitations for short texts:</p><p>• Unlike paragraphs or documents, short texts do not always observe the syntax of natural language.</p><p>• Short texts lack of contexts.</p><p>• Short texts are usually rather ambiguous because they contain polysemes and typos.</p><p>Such characteristics pose major challenges in short text classification. In order to overcome them, researchers need to capture more semantic as well as syntax information from short texts. A crucial step to reach this goal is to use more • Explicit Representation For explicit approaches, a given text is modeled following traditional NLP steps, including chunking, labeling, and syntactic parsing. Researchers create effective features from many aspects, such as knowledge base, POS tagging and dependency parsing. Although explicit models are easily understandable by human beings, it is difficult for the machine to collect useful features for disambiguation. Besides, it also suffers from the data sparsity problem. For example, when an entity is missing in a knowledge base, we cannot obtain any feature of it and thus an explicit representation will fail to work.</p><p>• Implicit Representation In terms of implicit representations, the text is represented using Neural Language Model (NLM) <ref type="bibr" target="#b0">[Bengio et al., 2003</ref>]. An NLM maps texts to an implicit semantic space and parameterizes them as a vector. An implicit representation model can capture richer information from context and facilitate text understanding with the help of deep neural networks. However, implicit representations also have some disadvantages: they perform poorly on new and rare words. Besides, they ignore important relations other than co-occurrence, such as IsA and IsProper-tyOf. For instance, the word "angles" in text the Angles won the World Series is the name of a baseball team.</p><p>However, an implicit model cannot capture the information that Angles is a baseball team and will treat it as an animal or a new word. We need additional knowledge to fill this gap.</p><p>It is ineffective to use either explicit or implicit representations independently for short text classification. For explicit models, the syntax and semantic information in short texts is too subtle for traditional NLP approaches to capture. Sometimes implicit models do not work well either. In many real applications, rare words like proper nouns occur frequently in short texts. Moreover, as is stated in the previous work <ref type="bibr">[Hua et al., 2015]</ref>, the IsA relation, which is missing in implicit models, is crucial for short text understanding. Despite their shortcomings, explicit and implicit representations are complementary to each other as is shown in previous work <ref type="bibr" target="#b1">[Hu et al., 2016]</ref>: explicit knowledge such as logic rules can be integrated into deep neural networks to regulate the learning process. We can combine them to transform explicit knowledge into neural models.</p><p>In this work, we propose a deep neural network that makes the fusion of explicit and implicit representations of short texts. We first enrich the information of short texts with the help of an explicit knowledge base. Specifically, we associate each short text with its relevant concepts in the knowledge base. Next we combine the words and relevant concepts of the short text to generate its embedding using a pre-trained word embedding. Then we feed this word-concept embedding into a Convolutional Neural Network (CNN) to learn explicit knowledge from the joint embedding. To the best of our knowledge, this is the first work that combines explicit and implicit representation for short text understanding.</p><p>While implicit and explicit representation models can provide rich semantic features for short text understanding, they still miss some semantic information. For example, given a short text buy apple iPhone7, as "iPhone7" is a new word, neither the knowledge base nor the pre-trained word embedding will recognize it. Inspired by the character level language model <ref type="bibr" target="#b4">[Kim et al., 2016]</ref>, we combine the input wordconcept embedding with a character-level embedding to obtain more semantic features. We use a separate CNN with only character embeddings as the input and concatenate its outputs to that of the main network in the fully-connected layer as the feature vector for the output layer. In this way, we can acquire more subword information such as morphemes that is missing in word-level embedding. In the above example, the word "iPhone7" will propose characteristic features similar to "iPhone". As "i-Phone" is included in knowledge base, we can capture the meaning of the new word "iPhone7" in this way.</p><p>We use a separate Convolutional Neural Network with only character embedding as the input and concatenate its outputs to those of the main network in the fully-connected layer as the feature vector for the output layer. In this way, we can acquire more subword information such as morphemes that is missing in word-level embedding.</p><p>The main contributions of this paper are summarized as follows:</p><p>• We associate relevant concepts with short texts by leveraging explicit knowledge and generating the implicit representation of the short text. To capture fine-grained semantic information, we also integrate the character level features into the joint embedding.</p><p>• We build a jointly model using Convolutional Neural Network to learn the coalesced embedding and to perform classification.</p><p>• We conduct extensive experiments on five commonly used datasets. The results show that our model significantly outperforms state-of-the-art methods.</p><p>2 Related Work </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Text Classification</head><p>Traditional text classification methods rely on humandesigned features. The most widely used feature is to represent text as a vector of terms, namely "Bag-of-Words".</p><p>Other studies mainly focus on generating more complex features, such as POS tagging and tree kernel <ref type="bibr">[Post and Bergsma, 2013]</ref>. The classifiers can be built using machine learning algorithms such as Naive Bayes and Support Vector Machine <ref type="bibr" target="#b5">[Wang and Manning, 2012]</ref>. For short text classification task, previous studies focus on feature expanding <ref type="bibr" target="#b5">[Shen et al., 2006]</ref> by leveraging context information from search engines. However, such methods have a serious problem of data sparsity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Neural Language Model</head><p>With the rapid development of distributed word representation and deep neural networks, many new ideas for traditional NLP tasks have been proposed. The Neural Language Model solves the data sparsity problem by representing words with dense vectors <ref type="bibr" target="#b0">[Bengio et al., 2003;</ref><ref type="bibr">Pennington et al., 2014]</ref>.</p><p>The embedding vectors obtained through NLMs can capture meaningful syntactic and semantic information and map semantically similar words close in the induced vector space. Recently there have been many studies on learning the representation of texts with different granulates: <ref type="bibr" target="#b1">[Hill et al., 2016]</ref> focus on learning the embedding of a phrase, while <ref type="bibr" target="#b4">[Palangi et al., 2016]</ref>, <ref type="bibr">[Kalchbrenner et al., 2014]</ref> and <ref type="bibr" target="#b4">[Le and Mikolov, 2014]</ref> focuses on learning the embedding of sentences. The text embedding enables us to measure the similarity between different texts by simply computing the distance between embedding vectors.</p><p>With fixed-length embedding vectors as input, using deep neural networks for various types of NLP tasks has gradually become popular. <ref type="bibr" target="#b5">[Socher et al., 2011]</ref> introduced an autoencoder using Recurrent Neural Network to capture the syntax information of a sentence. <ref type="bibr">[Socher et al., 2013]</ref> proposed the Recursive Neural Tensor Network for sentiment analysis. <ref type="bibr">[Collobert et al., 2011]</ref> first use CNN with pre-trained word embedding for text classification. <ref type="bibr" target="#b4">[Kim, 2014]</ref> further improves the performance by using multi-channel embedding. <ref type="bibr" target="#b6">[Zhang et al., 2015]</ref> and <ref type="bibr" target="#b1">[Conneau et al., 2016]</ref> proposed very deep neural networks with only character level information as input. While such methods work well for large documents, they perform poorly on short texts due to the limited information provided by them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Design</head><p>In this section, we present a joint model called Knowledge Powered Convolutional Neural Network (KPCNN), using two sub-networks to extract the word-concept and character features. We first introduce the way to conceptualize short texts with the help of a knowledge base. Then we describe the model and show how to learn the features from the embeddings incorporating information from the word, concept and characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Short Text Conceptualization</head><p>The first step of our work is short text conceptualization. We reach this goal using an existing knowledge base, such as DBpedia, Yago, Freebase and Probase <ref type="bibr" target="#b6">[Wu et al., 2012]</ref>. We will use Probase in this paper<ref type="foot" target="#foot_1">1</ref> . Because compared with other knowledge bases, Probase has a much broader coverage of concepts about wordly facts. Besides, Probase contains probabilistic information with which we can quantify many measurements of short texts, such as popularity, typicality and categorization. By leveraging the large number of IsA relations in Probase, we can get a list of concepts as well as their relevance to the short text.</p><p>Here we denote the concept vector as C = {&lt; c 1 , w 1 &gt; , &lt; c 2 , w 2 &gt;, ..., &lt; c k , w k &gt;}, where c i is a concept in the knowledge base, and w i is a weight to represent the relevance of the short text associated with c i . Given a short text, we can obtain its concept vector using the conceptualization API provided by Probase. It computes the concept vector of a given short text using a novel knowledge-intensive approach <ref type="bibr">[Hua et al., 2015]</ref>. In Probase, the number of k is set to 10. If there are more than 10 concepts, the top-10 results will be returned. For example, given a short text "CNOOC Signed a PSC with ROC", we can get its concept vector as {&lt;client,0.9&gt;, &lt;channel,0.6&gt;, &lt;mythological creature,0.6&gt;, &lt;international well-known enterprise,0.2&gt;, &lt;chinese oil major,0.2&gt;}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overall Architecture of the Model</head><p>After we get the results of conceptualization, we can combine the knowledge of concepts with the embedding of short texts. For the word and concept embedding, we use pretrained word embedding and keep them static. But as there is no pre-trained embeddings for the character level, we should allow the embedding of characters to be modified during the training process. In this way, the character embedding can be well-tuned while training the model. To this end, we propose a two-branch model as shown in Figure <ref type="figure" target="#fig_0">1</ref>. It has two components: the upper sub-network is for the word and concept embedding of the short text, and the lower sub-network is for the character embedding. Both of them are CNNs. With such a model, we can learn rich features from both the word level and the character level, respectively.</p><p>The upper component consists of seven layers: one input layer, two convolution layers, two pooling layers and two hidden layers.</p><p>Input Layer. The input layer transforms the short text into a matrix of embedding, denoted as W ∈ R (k+n)×m as the input of the network, where n and k is the the maximum number of words and concepts, respectively. And m is the dimension of word embedding. We obtain W by concatenating the embedding of words and concepts together:</p><formula xml:id="formula_0">W = W w ⊕ W c .</formula><p>Here W w and W c are the embedding of the words and concepts, respectively. And ⊕ is the concatenation operation. The way to construct W w is rather straightforward: suppose the short text consists of n words, and v w i ∈ R m is an mdimensional vector of the i th word in the short text. We can get W w by simply concatenating them:</p><formula xml:id="formula_1">W w = v w 1 ⊕ v w 2 ⊕ ... ⊕ v w n (1)</formula><p>To get the representation of W c , we also need to consider the weight of concepts at the same time. For each embedding vector v c i ∈ R m of concept c i , we multiply it by the constant w i to denote the weight of a given concept. Then we have:</p><formula xml:id="formula_2">W c = w 1 v c 1 ⊕ w 2 v c 2 ⊕ ... ⊕ w k v c k (2)</formula><p>If the short text or concept vector is not long enough, we will use 0 as padding. We get the embeddings v w i and v c i by looking up the pre-trained word embedding. Convolution Layer.</p><p>The function of convolution layers is to extract higher level features from the input matrix. To get different kinds of features, we apply filters with different sizes. Similar to many previous works, we fix the width of each filter as m and treat the height h of it as a hyper parameter. Given a filter ω ∈ R h×m , a feature s i is generated from a window of words and concepts [v i : v i+h−1 ] by:</p><formula xml:id="formula_3">s i = g(ω • [v i : v i+h−1 ] + b)<label>(3)</label></formula><p>Here b ∈ R is a bias term. And g is a non-linear function. In this work we use ReLU as the non-linear function for convolution layers. The filter is applied to all possible windows of words and concepts in W to produce a feature map s ∈ R n+k−h+1 . This process can be repeated for various filters with different heights to increase the feature coverage of the model. Pooling Layer.</p><p>The function of a pooling layer is to further abstract the features generated from convolution layer by aggregating the scores for each filter. In this work, we apply a max-over-time pooling operation over each feature map. The idea is to choose the highest value on each dimension of vector to capture the most important feature. With pooling layers, we can induce a fixed-length vector from feature maps. Hidden Layer.</p><p>In order to make full use of rich features obtained from the pooling layers, we use a non-linear hidden layer to combine different pooling features. We use tanh as the activation function here in our work. In this layer, we can also apply dropout <ref type="bibr" target="#b1">[Hinton et al., 2012]</ref> as a mean of regularization by randomly setting to zero a proportion of elements of the feature vector.</p><p>Similarly, the lower subnetwork also consists of seven layers: one input layer, two convolution layers, two pooling layers and two hidden layers. The input of this subnetwork is a sequence of encoded characters in the short text. The encoding is done by first generating an alphabet of all the characters Finally, we combine the output vectors of the two subnetworks by concatenating them. Then we apply an output layer on the joint vector to convert the output numbers into probabilities for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>We define all the parameters to be trained as a set Θ. Here we denote the set of training data as X and the set of class label as Y. For each x ∈ X , the network computes a score s(y; x, Θ) for each class y ∈ Y. To transform the scores into a conditional probability distribution in the output layer, we use a softmax operation over the scores for all y ∈ Y:</p><formula xml:id="formula_4">p(y|x, Θ) = exp(s(y; x, Θ)) ∀τ ∈Y exp(s(τ ; x, Θ)) (4)</formula><p>The training target of the model is to maximize the loglikelihood over the training set with respect to Θ:</p><formula xml:id="formula_5">Θ → x∈X log p(y|x, Θ)<label>(5)</label></formula><p>We use <ref type="bibr">Adagrad [Duchi et al., 2011]</ref> to optimize the training process. At the t th epoch, the parameters are updated as:</p><formula xml:id="formula_6">Θ t = Θ t−1 − α t i=1 g i g t (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>where α is the learning rate and g t is the gradient at epoch t.</p><p>All the parameters are initialized from a uniform distribution, we follow many previous studies to make such settings.  <ref type="table" target="#tab_1">1</ref>.</p><p>TREC. This is a question answering dataset<ref type="foot" target="#foot_2">2</ref> . It involves 6 different types of questions, such as whether the question is about a location, about person or numeric information.</p><p>Twitter. This is a set of tweets with 3 kinds of sentiments: positive, neutral and negative. The labels are added by the original author of the dataset<ref type="foot" target="#foot_3">3</ref> . We have preprocessed this dataset for the ease of use in this work: We removed the spams and quotes in the tweets. For tweets written in other languages, we translate them into English using Google Translator. We also remove the # hashtags that are not located at the end of the message. AG news. This dataset is adopted from <ref type="bibr" target="#b6">[Zhang et al., 2015]</ref>.</p><p>The original data consists of both articles and descriptions of AG's corpus of news. In order to test for short texts, we remove the articles and only use the titles in our experiment.</p><p>Bing. This is a dataset of query logs adopted from <ref type="bibr">[Wang et al., 2014]</ref>. They are divided into 4 categories according to their contents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Movie Review.</head><p>This dataset consists of one sentence per comment on movies. Classification involves detecting positive/negative reviews <ref type="bibr">[Pang and Lee, 2005]</ref>. For this dataset, we randomly split 80% as the training set and the remaining 20% as test set. In this process, we keep a balance number of items with each label in the training set.</p><p>There are several hyper parameters in our model. We set them empirically. The details are in Table <ref type="table">2</ref>. We use the tool word2vec<ref type="foot" target="#foot_4">4</ref> to train word and concept embedding. If a word is unknown, we will randomly initialize its embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Methods</head><p>We compared our method with several state-of-the-art approaches: two feature-based methods and two deep learning based methods. The metric for evaluating each model is the accuracy of prediction.</p><p>Word-Concept Embedding + LR. This baseline uses the weighted word embedding as well as concept embedding to represent each short text. For the weighted word embedding V w ∈ R m , we use the tf-idf value of each word as the weight. Given the concept vector C, the concept embedding V c ∈ R m is the weighted average of embedding of each concept:</p><formula xml:id="formula_8">V c = k i=0 w i v c i k i=0 w i (7)</formula><p>And the overall embedding is the average of V w and V c . Then we use Logistic Regression over such embedding to perform classification. A similar idea has been proposed in <ref type="bibr">[Huang et al., 2012]</ref> to capture global context for classification. BoW + SVM. This baseline is proposed by <ref type="bibr" target="#b5">[Wang and Manning, 2012]</ref>. The basic idea is to use the traditional SVM algorithm to build a classifier. We use the unigrams as the feature for short texts. The weight of each feature is the frequency of each unigram. CNN. This method uses a one-layer CNN for text classification proposed by <ref type="bibr" target="#b4">[Kim, 2014]</ref>. It uses a multi-channel architecture for text embedding. We obtain its source code from the author<ref type="foot" target="#foot_5">5</ref> and use its default settings for hyper parameters. CharCNN. We also compared our work with a recently proposed method <ref type="bibr" target="#b6">[Zhang et al., 2015]</ref>. It uses a 12-layer convolutional neural network with only character level features as the input. We obtain the source code from the author<ref type="foot" target="#foot_6">6</ref> . WCCNN. This baseline is proposed by us. We use only the upper subnetwork of our proposed model. And the embedding is the concatenation of word embedding and concept embedding matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion of Results</head><p>The results on all the datasets are shown in Table <ref type="table" target="#tab_2">3</ref>. We can see that our model KPCNN significantly outperforms stateof-the-art methods. In all the five datasets, our proposed model outperforms the best baselines by 2% to 5% in accuracy. Even without the character level information, the WC-CNN model still outperforms most state-of-the-art methods. The reason is that the neural network can effectively represent the semantic content of a short text. With the help of the concept embedding, we can acquire richer features from the short text and feed them into our model.</p><p>By comparing our model with the proposed baseline WC-CNN, we can see that the character level information can help improve the performance of the model. With the help of the character features, we have improvement in accuracy on four out of five datasets.</p><p>While character-level features are helpful, it is worth noting that the CharCNN model <ref type="bibr" target="#b6">[Zhang et al., 2015]</ref> does not perform well in our experiment. The reason is that due to the shortness and sparsity of short texts, CharCNN is unable to capture enough features with only character level information. Therefore, although CharCNN works well for document-level datasets, it is not effective in solving the problem of short text classification. Moreover, as CharCNN has a fixed alphabet, it fails to take many unknown characters in the  Twitter dataset. So the performance on that dataset is rather poor.</p><p>From the above results, we can also observe that compared with traditional methods, convolutional neural network approaches achieve better results. The main reason is that CNN is able to capture richer features with various filter sizes in the convolution layer and select more discriminative features from pooling layers.</p><p>Our model can be integrated with other word embedding, too. Here we perform some experiments using two different word embeddings: Word-Concept <ref type="bibr" target="#b0">[Cheng et al., 2015]</ref> and Glove <ref type="bibr">[Pennington et al., 2014]</ref>. In order to show the contribution of different features, we test with three methods. CNN is the method that uses only the embedding of words in short text. WC is the WCCNN model with only upper subnetwork. And KP is our KPCNN model. The results are shown in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>We can see that the overall results using Glove and Word-Concept embedding are not as good as using word2vec embedding. The main reason is that compared with word2vec, these two embedding methods have too many unknown words. The number of unknown words in each dataset is shown in Table <ref type="table" target="#tab_3">4</ref>. We can see that for all the datasets, there are fewer unknown words in word2vec. In the TREC dataset, the Word-Concept embedding fails to recognize more than one third of the words. In this case, the embedding vectors of most words are initialized randomly and the quality of the short text representation will be seriously influenced.</p><p>From the results in Figure <ref type="figure" target="#fig_1">2</ref>, we can further conclude that the conceptualization and character level information does help improve the overall performance. For the Word-Concept embedding, the accuracy of KP can be up to 12% higher than that of CNN. This is because Word-Concept makes use of additional knowledge source to learn contextual word representation. And the implicit representation of words involves a combination of its intrinsic vector and the most contextappropriate concept vector. Therefore, the KP model can make full use of the joint representation of short text and achieve better accuracy than the CNN model.</p><p>Finally, we can see from Figure <ref type="figure" target="#fig_1">2</ref> that KPCNN can have up to 7% accuracy gaining than the other two models. This is because word-level embedding can not handle out-ofvocabulary words. This phenomenon is more significant as the number of unknown words in Glove and Word-Concept is much larger. With the help of character level information, we can easily address this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel model that takes advantage of both explicit and implicit representations for short text classification. We enrich the features of short texts by conceptualizing them with the help of a well known knowledge base. We combine the associated concepts with the words to generate the embedding of short text. We also utilize the character level information to enhance the embedding of short text. With such embedding as the input, we build a joint model on the basis of the CNN to perform classification. Experiments on real data show that our method achieves significant improvement over state-of-the-art methods for short text classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of the Overall Model in the dataset and then randomly initializing the embedding of each character with m c dimensions. Then the sequence of characters is transformed into a matrix W c ∈ R L×mc . Here L is a hyper parameter that limits the maximum size of the sequence. Any character exceeding length L is ignored. In this work we set the value of L to be 256.Finally, we combine the output vectors of the two subnetworks by concatenating them. Then we apply an output layer on the joint vector to convert the output numbers into probabilities for classification.</figDesc><graphic url="image-1.png" coords="4,56.70,54.00,498.60,266.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Results with different embeddings</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Len</cell></row></table><note>A Summary of Datasets Datasets #class Training/Test set Avg.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Accuracy of Composed Models on Different Datasets</figDesc><table><row><cell>Model</cell><cell>TREC</cell><cell>Twitter</cell><cell cols="2">AG news Bing</cell><cell>Movie Review</cell></row><row><cell>WC + LR</cell><cell>52.8</cell><cell>57.57</cell><cell>61.56</cell><cell>74.48</cell><cell>60.44</cell></row><row><cell>BoW + SVM</cell><cell>85.66</cell><cell>56.23</cell><cell>72.7</cell><cell>80.33</cell><cell>77.52</cell></row><row><cell>CNN</cell><cell>89.33</cell><cell>57.24</cell><cell>86.11</cell><cell>96.2</cell><cell>81.52</cell></row><row><cell>CharCNN</cell><cell>76</cell><cell>44.96</cell><cell>78.27</cell><cell>85.64</cell><cell>77.01</cell></row><row><cell>WCCNN</cell><cell>91.21</cell><cell>57.87</cell><cell>85.57</cell><cell>96.22</cell><cell>83.77</cell></row><row><cell>KPCNN</cell><cell>93.46</cell><cell>59.84</cell><cell>88.36</cell><cell>99.17</cell><cell>83.25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The number of unknown words in each embedding</figDesc><table><row><cell>Embedding</cell><cell>TREC</cell><cell>Twitter</cell><cell cols="2">AG news Bing</cell><cell>Movie Review</cell></row><row><cell>vocabulary size</cell><cell>9592</cell><cell>26840</cell><cell>51299</cell><cell>15011</cell><cell>18762</cell></row><row><cell>Word2vec</cell><cell>467</cell><cell>5133</cell><cell>5096</cell><cell>1350</cell><cell>2314</cell></row><row><cell>Word-Concept</cell><cell>3891</cell><cell>17029</cell><cell>35185</cell><cell>9709</cell><cell>2302</cell></row><row><cell>Glove</cell><cell>3715</cell><cell>16111</cell><cell>34729</cell><cell>9660</cell><cell>1153</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">The data is now public available as part of the Microsoft Concept Graph: https://concept.msra.cn/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">http://cogcomp.cs.illinois.edu/Data/QA/QC/ Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI-17)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">https://www.cs.york.ac.uk/semeval-2013/task2/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4">https://code.google.com/archive/p/word2vec/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5">https://github.com/yoonkim/CNN sentence</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6">https://github.com/zhangxiangxiao/Crepe Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence </note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contextual text understanding in distributional semantic space</title>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<editor>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pavel</forename><forename type="middle">P</forename><surname>Kuksa</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2003">2003. 2003. 2015. 2015. 2011</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
		</imprint>
	</monogr>
	<note>Journal of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName><surname>Conneau</surname></persName>
		</author>
		<idno>CoRR, abs/1207.0580</idno>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
				<editor>
			<persName><surname>Hua</surname></persName>
		</editor>
		<imprint>
			<publisher>Edward</publisher>
			<date type="published" when="2011">2016. 2016. 2011. 2011. 2016. 2016. 2012. 2012. 2016. 2016. 2015. 2012. 2012</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
	<note>ACL. Kalchbrenner et al., 2014</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName><forename type="first">Phil</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tom Kenter and Maarten de Rijke. Short text similarity with word embeddings</title>
				<imprint>
			<publisher>Kenter and de Rijke</publisher>
			<date type="published" when="2014">2014. 2015. 2015</date>
			<biblScope unit="page" from="1411" to="1420" />
		</imprint>
	</monogr>
	<note>CIKM</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="2741" to="2749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval</title>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mikolov ; Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<publisher>Matt Post and Shane Bergsma</publisher>
			<date type="published" when="2005">2014. 2014. 2014. 2014. 2013. 2013. 2016. 2016. 2005. 2014. 2014. 2013</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="866" to="872" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<meeting><address><addrLine>Haixun Wang, Xiaofeng Meng</addrLine></address></meeting>
		<imprint>
			<publisher>Kejun Zhao</publisher>
			<date type="published" when="2006">2006. 2006. 2011. 2011. 2013. 2013. 2011. 2011. 2012. 2012. 2016. 2016. 2014. 2014. 2015. 2015</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="3264" to="3270" />
		</imprint>
	</monogr>
	<note>IJCAI</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Xiang Zhang, Junbo Zhao, and Yann LeCun</title>
				<imprint>
			<date type="published" when="2012">2012. 2012. 2015. 2015</date>
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
