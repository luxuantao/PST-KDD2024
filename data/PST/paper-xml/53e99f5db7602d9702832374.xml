<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Volunteer Computing and Desktop Cloud: the Cloud@Home Paradigm</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Vincenzo</forename><forename type="middle">D</forename><surname>Cunsolo</surname></persName>
							<email>vdcunsolo@unime.it</email>
						</author>
						<author>
							<persName><forename type="first">Salvatore</forename><surname>Distefano</surname></persName>
							<email>sdistefano@unime.it</email>
						</author>
						<author>
							<persName><forename type="first">Antonio</forename><surname>Puliafito</surname></persName>
							<email>apuliafito@unime.it</email>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><surname>Scarpa</surname></persName>
							<email>mscarpa@unime.it</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Messina</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Contrada di Dio, S. Agata</orgName>
								<address>
									<postCode>98166</postCode>
									<settlement>Messina</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Volunteer Computing and Desktop Cloud: the Cloud@Home Paradigm</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7B87261CC86E0330A973076917234E41</idno>
					<idno type="DOI">10.1109/NCA.2009.41</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Only commercial Cloud solutions have been implemented so far, offering computing resources and services for renting. Some interesting projects, such as Nimbus, OpenNEbula, Reservoir, work on Cloud. One of their aims is to provide a Cloud infrastructure able to provide and share resources and services for scientific purposes. The encouraging results of Volunteer computing projects in this context and the flexibility of the Cloud, suggested to address our research efforts towards a combined new computing paradigm we named Cloud@Home. On one hand it can be considered as a generalization of the @home philosophy, knocking down the barriers of Volunteer computing, and also allowing to share more general services. On the other hand, Cloud@Home can be considered as the enhancement of the grid-utility vision of Cloud computing. In this new paradigm, users' hosts are not passive interface to Cloud services anymore, but they can interact (free or by charge) with other Clouds.</p><p>In this paper we present the Cloud@Home paradigm, highlighting its contribution to the actual state of the art on the topic of distributed and Cloud computing. We detail the functional architecture and the core structure implementing such paradigm, demonstrating how it is really possible to build up a Cloud@Home infrastructure.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Cloud computing is a distributed computing paradigm that mixes aspects of Grid computing, Internet computing, Utility computing, Autonomic computing and Green computing. The development and the success of Cloud is due to the maturity reached by both hardware and software virtualization. This made realistic the L.Kleinrock outlook <ref type="bibr" target="#b12">[12]</ref> of computing as the 5th utility.</p><p>Cloud computing comes from the service-centric perspective that is widely spreading on the IT world. From this perspective, all capabilities and resources of a Cloud are provided to users as a service, to be accessed through the Internet without any specific knowledge of, expertise with, or control over the underlying technology infrastructure supporting them. It offers a user-centric interface that acts as a unique, user friendly, point of access for users' needs and requirements. Moreover, Cloud computing provides on-demand service provision, QoS guaranteed offer, and autonomous system for managing hardware, software and data transparently to users <ref type="bibr" target="#b19">[18]</ref>.</p><p>In order to achieve such goals it is necessary to implement a level of abstraction of physical resources, uniforming their interfaces and providing means for their management, adaptively to user requirements. This is done through virtualizations, service mashups (Web 2.0) and service oriented architectures (SOA).</p><p>Virtualization allows to execute a software version of a hardware machine into a host system, in an isolated way. It "homogenizes" resources: problems of compatibility are overcome by providing heterogeneous hosts of a distributed computing environment (the Cloud) with the same virtual machine. The software implementing virtualization is named hypervisor.</p><p>The Web 2.0 provides a uniform interface to Cloud services, implementing service mashup. It is mainly based on an evolution of JavaScript with improved language constructs (late binding, clousers, lambda functions, etc) and AJAX interactions.</p><p>SOA is a paradigm for organizing and utilizing distributed capabilities that may be under the control of different ownership domains. In SOA, services are the mechanism by which needs and capabilities are brought together. SOA defines standard interfaces and protocols that allow developers to encapsulate information tools as services that clients can access without knowledge of, or control over, their internal workings <ref type="bibr" target="#b8">[8]</ref>.</p><p>A great interest on Cloud computing has been manifested and numerous projects from industry and academia have been proposed. In commercial contexts, among the others we highlight: Amazon Elastic Compute Cloud [1] IBM's Blue Cloud <ref type="bibr" target="#b11">[11]</ref>, Sun Microsystems Network.com <ref type="bibr" target="#b16">[15]</ref>, Microsoft Azure Services Platform <ref type="bibr" target="#b5">[5]</ref>. Scientific open activities and projects are: Reservoir <ref type="bibr" target="#b15">[14]</ref> Nimbus/Stratus/Wispy/Kupa <ref type="bibr" target="#b18">[17]</ref> and OpenNEbula <ref type="bibr" target="#b7">[7]</ref>. All of them support and provide an ondemand computing paradigm: users submit their requests to the Cloud that remotely, in a distributed fashion, processes them and gives back the results. This client-server model well fits aims and scopes of commercial Clouds: the business. But, on the other hand, it represents a restriction for scientific Clouds, that have a view closer to Volunteer computing.</p><p>Volunteer computing uses computers volunteered by their owners, as a source of computing power and storage to provide distributed scientific computing <ref type="bibr" target="#b2">[3]</ref>. It is behind the "@home" philosophy of sharing/donating network connected resources for supporting distributed scientific computing.</p><p>We believe that the Cloud computing paradigm is applicable also at lower scales, from the single contributing user, that shares his/her desktop/devices according to the available capabilities, to research groups, public administrations, social communities, small and medium enterprises, which make available their distributed computing resources to the Cloud. Both freesharing and pay-per-use models can be easily adopted in such scenarios.</p><p>From the utility point of view, the rise of the "technoutility complex" and the corresponding increase of computing resources demand, in some cases growing dramatically faster than Moore's Law as predicted by the Sun CTO Greg Papadopoulos in the red shift theory for IT <ref type="bibr" target="#b13">[13]</ref>, could bring, in a close future, towards an oligarchy, a lobby or a trust of few big companies controlling the whole computing resources market.</p><p>To avoid such scenario, we suggest to address the problem in a different way: instead of building costly private data centers, that the Google CEO Eric Schmidt likes to compare to the prohibitively expensive cyclotrons <ref type="bibr" target="#b3">[4]</ref>, we propose a more "democratic" form of Cloud computing, in which the computing resources of single users can be shared with the others, in order to contribute to the elaboration of complex problems.</p><p>Since this paradigm is very similar to the Volunteer computing one, it can be named Cloud@Home. Compatibility limitations of Volunteer computing can be solved in Cloud computing environments, allowing to share resources and services.</p><p>The Cloud@Home paradigm could be also applied to commercial Clouds, establishing an open computing-utility market where users can both buy and sell their services. Since the computing power can be described by a "long-tailed" distribution, in which a high-amplitude population (Cloud providers and commercial data centers) is followed by a lowamplitude population (small data centers and private users) which gradually "tails off" asymptotically, Cloud@Home can catch the Long Tail effect <ref type="bibr" target="#b1">[2]</ref>, providing similar or higher computing capabilities than commercial providers' data centers, by grouping small computing resources from many single contributors.</p><p>In the following we demonstrate how it is possible to make real all these aims through the Cloud@Home paradigm. Thus, in section 3 we describe the functional architecture of the Cloud@Home infrastructure, and in section 4 we characterize the blocks implementing the functions previously identified into the Cloud@Home core structure. Section 5 resumes the paper and discusses about challenges and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">WHY CLOUD@HOME?</head><p>The necessity of such a new computing paradigm is strictly related to the limits of existing Cloud solutions. For years Grid computing has been considered as the solution for all the computing problems: a secure, reliable, performing platform for managing geographically distributed resources. But it has some drawbacks: it is sensitive to hardware or software differences or incompatibility; it is not possible to dynamically extend a Virtual Organization (VO) by on-line enrolling resources, and consequently is not possible to share local resources, if they are not initially enrolled in the VO; it often does not face QoS and billing problems; it mainly supports data parallelism against task parallelism, making difficult the composition of services; users need to have knowledge of both the distributed system and the application requirements in order to submit and manage jobs.</p><p>These lacks have been partially faced and solved in Utility and Cloud computing, implementing service oriented paradigms with higher level user friendly interfaces, starting from on-demand models: users commission their computing, pay and get the results. Since they are mainly thought for commercial applications, QoS and business policies have to be carefully addressed. Utility and Cloud computing lack of an open, free viewpoint: as in the Grid computing, it is not possible to enroll resources or services, as also to build custom data centers by dynamically aggregating resources and services not conceived with this purpose. Moreover, each Cloud has its own interface and services, therefore it cannot communicate or interoperate with other Clouds.</p><p>On the other hand the Volunteer computing paradigm implements an open distributed environment in which resources (not services as in the Cloud) can be shared. But it manifests the same problem of Grid with regard to the compatibility among resources. Moreover, due to its purpose, it also does not implement any QoS and billing policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Aims and goals</head><p>Ian Foster summarizes the computing paradigm of the future as follows <ref type="bibr" target="#b9">[9]</ref>: "... we will need to support on-demand provisioning and configuration of integrated "virtual systems" providing the precise capabilities needed by an end-user. We will need to define protocols that allow users and service providers to discover and hand off demands to other providers, to monitor and manage their reservations, and arrange payment. We will need tools for managing both the underlying resources and the resulting distributed computations. We will need the centralized scale of today's Cloud utilities, and the distribution and interoperability of today's Grid facilities.".</p><p>We share all these requirements, but in a slightly different perspective: we want to actively involve users into such a new form of computing, allowing to create own interoperable Clouds. We believe that it is possible to export, apply and adapt the "@home" philosophy to the Cloud computing paradigm. By merging Volunteer and Cloud computing, a new paradigm can be created: Cloud@Home. This new computing paradigm gives back the power and the control to users, who can decide how to manage their resources/services in a global, geographically distributed context. They can voluntarily sustain scientific projects by free placing their resources/services at the scientific research centers' disposal (eventually according to a credit mechanism), or they can earn money by selling their resources to Cloud computing providers in a pay-per-use/share context, according to their capabilities.</p><p>Therefore, in Cloud@Home both the commercial and the volunteer viewpoints coexist: in the former case the end-user orientation of Cloud is extended to a collaborative twoway Cloud in which users can buy and/or sell their resources/services; in the latter case, the Grid philosophy of few but large computing requests is extended and enhanced to open Virtual Organizations. In both cases QoS requirements could be specified, introducing in the Grid and Volunteer philosophy (best effort) the concept of quality.</p><p>Cloud@Home can be also considered as a generalization and a maturation of the @home philosophy: a context in which users voluntarily share their resources without any compatibility problem. This allows to knock down both hardware (processor bits, endianness, architecture, network) and software (operating systems, libraries, compilers, applications, middlewares) barriers of Grid and Volunteer computing. Moreover, in Cloud@Home the term resources must be interpreted in the more general Cloud sense of services.</p><p>On the other hand, Cloud@Home can be considered as the enhancement of the Grid-Utility vision of Cloud computing. In this new paradigm, users' hosts are not passive interfaces to Cloud services, but they can be actively involved in computing. Single nodes and services can be enrolled by the Cloud@Home middleware, in order to build own-private Cloud infrastructures that can (free or by charge) interact with other Clouds.  The Cloud@Home motto is: heterogeneous hardware for homogeneous Clouds. Thus, the scenario we prefigure is composed of several coexisting and interoperable Clouds, as pictorially depicted in Fig. <ref type="figure" target="#fig_0">1</ref>. Open Clouds (yellow) identify Clouds operating for free; Commercial Clouds (blue) characterize entities or companies selling their computing resources for business; Hybrid Clouds (green) can both sell or give for free their services.</p><p>The overall infrastructure must deal with the high dynamism of its nodes/resources, allowing to move and reallocate data, tasks and jobs. It is therefore necessary to implement a lightweight middleware, specifically designed to optimize migrations. It allows to involve limited resources' devices into the Cloud, and does not influence the code writing as Grid and Volunteer computing paradigms do.</p><p>Another important goal of Cloud@Home is the security. Volunteer computing has some lacks in security concerns, while the Grid paradigm implements complex security mech-anisms. The virtualization in Clouds implements the isolation of the services, but does not provide any protection from local access. With regards data security, the specific goal of Cloud@Home is to extend the security mechanisms of Clouds to the protection of data from local access.</p><p>Last but not least, interoperability is one of the most important goal of Cloud@Home. This is an open problem in Grid, Volunteer and Cloud computing that we want to adequately face in Cloud@Home.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Application Scenarios</head><p>Different application scenarios can be imagined for Cloud@Home:</p><p>• Scientific research centers, communities -the Volunteer computing inspiration of Cloud@Home provides means for the creation of open, interoperable Clouds for supporting scientific purposes, overcoming the portability and compatibility problems highlighted by @home projects. Similar benefits could be experienced in public administrations and open communities (social network, peer-topeer, etc). • Enterprises -planting a Cloud@Home computing infrastructure in commercial locations can bring considerable benefits, especially in small and medium but also in big enterprises. It could be possible to implement own data center with local, existing, off the shelf, resources. The interoperability among Clouds allows to buy computing resources from commercial Cloud providers if needed or, otherwise, to sell the local Cloud computing resources to the same providers that can resell them. This allows to reduce and optimize business costs according to QoS/SLA policies, improving performances and reliability. For example, this paradigm allows to deal with the flow peaks economy: data centers could be sized for the medium case, and worst cases (peaks) could be managed by buying computing resources from Cloud providers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CLOUD@HOME OVERVIEW</head><p>With Cloud@Home, anyone can experience the power of Cloud computing, both actively providing his/her own resources and services, and passively submitting his/her applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Issues, Challenges and Open Problems</head><p>In order to implement a lightweight Cloud@Home middleware, the following issues have to be taken into consideration:</p><p>• Resources and Services management -a mechanism for managing resources and services offered by Clouds is mandatory. • Frontend -abstraction is needed in order to provide users with a high level service oriented point of view of the computing system, and a unique, uniform access point to the Cloud. • Security -effective mechanisms are required to provide: authentication, resources and data protection, data confidentiality and integrity.</p><p>• Reliability -it is necessary to implement redundancy of resources and services, and hosts' recovery policies. • Interoperability -Clouds have to be able to interoperate each other. • Business models -it is necessary to provide QoS and SLA management for both commercial and open volunteer Clouds in order to discriminate among the applications to be run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Basic Architecture</head><p>A possible Cloud@Home architecture is shown in Fig. <ref type="figure">2</ref>, identifying three hierarchical layers: frontend, virtual and physical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Frontend Layer</head><p>The Cloud@Home frontend layer is responsible for the resources and services management (enrolling, discovery, allocation, coordination, monitoring, scheduling, etc) from the global Cloud system's perspective. The frontend layer provides tools for translating end-user requirements into physical resources' demand, also considering QoS/SLA constraints, if specified by the user. Moreover, in commercial Clouds, it must be able to negotiate the QoS policy to be applied (SLA), therefore monitoring for its fulfillment and, in case of unsatisfactory results, adapting the computing workflow to such QoS requirements.</p><p>If the available Cloud's resources and services can not satisfy the requirements, the frontend layer provides mechanisms for requesting further resources and services to other Clouds, both open and/or commercial. In other words, the Cloud@Home frontend layer implements the interoperability among Clouds. In order to improve reliability and availability of services and resources, especially if QoS policies and constraints have been specified, it is necessary introduce redundancy.</p><p>The frontend layer is split into two parts, as shown in Fig. <ref type="figure">2</ref>: the server side, implementing the resources management and related problems, and the light client side, only providing mechanisms and tools for authenticating, accessing and interacting with the Cloud.</p><p>The frontend layer provides means, tools and policies for managing users. The best mechanism to achieve secure authentications is the Public Keys Infrastructure (PKI) <ref type="bibr" target="#b17">[16]</ref>, better if combined with smartcard devices that, through a trusted certification authority, ensure the user identification and provide Single Sign-on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Virtual Layer</head><p>The virtualization of physical resources offers to end-users a homogeneous view of Cloud's services and resources. Two basic services are provided by virtual to frontend layer and, consequently, to end-users: execution and storage services.</p><p>The execution service is the tool for creating and managing virtual machines. A user, sharing his/her resources within a Cloud@Home, allows the other users of the Cloud to execute and manage virtual machines locally at his/her node, according to policies and constraints negotiated and monitored at the frontend layer. As shown in Fig. <ref type="figure">2</ref>, from the end-user point of view an execution Cloud is seen as a set of virtual machines available and ready-to-use. The virtual machines' isolation implements protection and therefore security.</p><p>The storage service implements a storage system distributed across the storage hardware resources composing the Cloud, highly independent of them since data and files are replicated according to QoS policies and requirements to be satisfied. From the end-user point of view, a storage Cloud appears as a locally mounted remote disk.</p><p>In a distributed environment where any users can host part of private data, it is necessary to protect such data from unauthorized accesses. A way to obtain data confidentiality and integrity could be the cryptography, as better explained in the physical layer description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Physical Layer</head><p>The physical layer is composed of a "cloud" of generic nodes and/or devices geographically distributed across the Internet. They provide to the upper virtual layer both physical resources for implementing execution and storage services and mechanisms and tools for locally managing such resources.</p><p>Cloud@Home negotiates with users that want to join a Cloud about his/her contribution. This mechanism involves the physical layer that provides tools for reserving physical execution and/or storage resources for the Cloud, and monitors these resources, such that constraints, requirements and policies thus specified are not violated. This ensures reliability and availability of physical resources, avoiding to overload the local system and therefore reducing the risk of crashes.</p><p>To implement the execution service in a generic device or to enroll it into an execution Cloud, the device must have a hypervisor ready to allocate and run virtual machines, as shown in Fig. <ref type="figure">2</ref>. If a storage service is installed into the device, a portion of the local storage system must be dedicated for hosting the Cloud data. In such cases, the Cloud@Home file system is installed into the devices' shared storage space.</p><p>At physical layer it is necessary to implement data security (integrity and confidentiality) also ensuring that stored data cannot be accessed by who physically hosts them. We propose an approach that combines the inviolability of the Public Key Infrastructure asymmetric cryptography and the speed of the symmetric cryptography. Data are firstly encrypted by the symmetric key, and then stored into the selected host with the symmetric key encrypted by the user private key. This ensures that only authorized users can decrypt the symmetric key and consequently can access data.</p><p>SSH, TLS, IPSEC and other similar transmission protocols could be used to manage the connection among nodes. However, since the data stored in a Cloud@Home storage are encrypted, it is not necessary to use a secure channel for data transfers, more performant protocol, such as BitTorrent <ref type="bibr" target="#b6">[6]</ref>, can be used. The secure channel is required for sending and receiving non-encrypted messages and data to/from remote hosts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Management Subsystem</head><p>Storage Master</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Resource Subsystem</head><p>Cloud Broker Figure <ref type="figure">3</ref>. Cloud@home Core Structure Organization</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">INSIDE CLOUD@HOME</head><p>The blocks implementing the functional architecture of Cloud@Home above introduced, have been characterized in Fig. <ref type="figure">3</ref>, reporting the core structure of the server-side system, subdivided into management and resource subsystems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Management Subsystem</head><p>In order to enroll and manage the distributed resources and services of a Cloud, providing a unique point of access, it is necessary to adopt a centralized approach. This is implemented by the management subsystem that is composed of four parts: the user frontend, the Cloud broker, the resource engine and the policy manager.</p><p>The user frontend provides tools for Cloud@Home-User interactions. It collects and manages the users' requests issued by the client side. All such requests are transferred to the blocks composing the underlying layer for processing.</p><p>An important task carried out by the user frontend is the Clouds interoperability, implemented by point-to-point connecting the user frontend of the Clouds wishing to interoperate. In case one of the two Clouds to be point-topoint connected has not the Cloud@Home structure, it is necessary to translate the requests between Cloud@Home and foreign Clouds formats, task delegated by the user frontend to the Cloud broker. The Cloud broker collects and manages information about the available Clouds and the services they provide (both functional and non-functional parameters, such as QoS, costs, reliability, request formats' specifications for Cloud@Home-foreign Clouds translations, etc).</p><p>The policy manager provides and implements the Cloud's access facilities. This task falls into the security scope of identification, authentication and permission management, implemented starting from PKI as above introduced. The policy manager also manages the information about users' QoS policies and requirements.</p><p>The resource engine is the hearth of Cloud@Home. It is responsible for the resources' management. To meet this goal, the resource engine applies a hierarchical policy. It operates at higher level, in a centralized way, indexing all the resources of the Cloud. Incoming requests are delegated to VM schedulers or storage masters that, in a distributed fashion, manage the computing or storage resources respectively, coordinated by the resource engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Resource Subsystem</head><p>The resource subsystem contains all the blocks implementing the local and distributed management functionalities of Cloud@Home. This subsystem can be logically split into two parts offering different services over the same resources: the execution Cloud and the storage Cloud. The management subsystem merges them providing a unique Cloud that can offer both execution and/or storage services. The execution Cloud provides tools for managing virtual machines according to users' requests and requirements coming from the management subsystem. It is composed of four blocks: VM scheduler, VM provider, resource monitor and hypervisor.</p><p>The VM Scheduler is a peripheral resource broker of Cloud@Home infrastructure, to which the resource engine delegates the management of Cloud resources and services. From the end-user point of view a VM is allocated somewhere on the Cloud, therefore its migration is transparent to endusers.</p><p>VM provider, resource monitor and hypervisor are responsible for managing a VM locally to a physical resource. A VM provider exports functions for allocating, managing, migrating and destroying a virtual machine on the corresponding host. The resource monitor allows to take under control the local computing resources according to requirements and constraints negotiated in the setup phase with the contributing-user. If during a virtual machine execution the local resources crash or become insufficient to keep running the virtual machine, the resource monitor asks the scheduler to migrate the VM elsewhere.</p><p>In order to implement a distributed storage system, the storage Cloud, we specify the Cloud@Home file system (FS), inspired by the Google FS <ref type="bibr" target="#b10">[10]</ref>. The Cloud@Home FS splits data and files into chunks of fixed or variable size, depending on the storage resource available. The architecture of such file system is hierarchical: data chunks are physically stored on chunk providers and corresponding storage masters index the chunks.</p><p>The storage master is the directory server, indexing the data stored in the associated chunk providers. It directly interfaces with the resource engine to discover the resources storing data. The resource engine can be considered as the directory server indexing all the storage masters. To improve the storage Cloud reliability, storage masters must be replicated. Moreover, a chunk provider can be associated to more than one storage master.</p><p>Chunk providers physically store the data, that, as introduced above, are encrypted in order to achieve the confidentiality goal. Data reliability can be improved by replicating data chunks and chunk providers, consequently updating the corresponding storage masters. In this way, a corrupted data chunk can be automatically recovered and restored through the storage masters, without involving the end-user.</p><p>In order to avoid VM schedulers or storage masters become bottlenecks, once the VM/chunk providers have been located, VM interactions or data transfers are implemented by directly connecting end-users and VM or chunk providers, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">READY FOR CLOUD@HOME?</head><p>In this paper we proposed an innovative computing paradigm that represents a solution for building Clouds starting from heterogeneous and independent nodes, not specifically conceived for this purpose. This implements a generalization of both Volunteer and Cloud computing by aggregating the computational potentialities of many small, low power systems, exploiting the long tail effect of computing.</p><p>In this way Cloud@Home opens the Cloud computing world to scientific and academic research centers, as well as to communities or single users: anyone can voluntarily support projects by sharing his/her resources. On the other hand, it could open the utility computing market to the single user that wants to sell his/her computing resources. To realize this broader vision, several issues must be adequately taken into account: reliability, security, portability of resources and services, interoperability among Clouds, QoS/SLA and business models and policies.</p><p>It is necessary a common understanding, an ontology that fixes metrics and concepts such as resources, services and also overall Clouds functional and non-functional parameters, QoS, SLA, exposition format, and so on, that must be translated into specific interoperability standards. Fundamental aspect to take into account is the reliability: in a heterogeneous Cloud we can have highly reliable resources (NAS, computing servers), and barely reliable resources (temporary contributors). Cloud@Home must consider such parameter, specifying adequate management policies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Cloud@Home Scenario</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>978-0-7695-3698-9/09 $25.00 © 2009 IEEE DOI 10.1109/NCA.2009.41</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Nov</forename><surname>Amazon</surname></persName>
		</author>
		<ptr target="http://aws.amazon.com/ec2" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The Long Tail: How Endless Choice Is Creating Unlimited Demand</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Anderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006-07">July 2006</date>
			<publisher>Random House Business Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The computational and storage potential of volunteer computing</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Fedak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCGRID &apos;06</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Baker</surname></persName>
		</author>
		<title level="m">Google and the Wisdom of Clouds</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><surname>Businessweek</surname></persName>
		</author>
		<ptr target="http://www.businessweek.com/magazine/content/0752/b4064048925836.htm" />
		<imprint>
			<date type="published" when="2008-12-24">December 24 2008. Dec. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Azure services platform</title>
		<ptr target="http://www.microsoft.com/azure/default.mspx" />
		<imprint>
			<publisher>Microsoft Co</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The BitTorrent Protocol Specification</title>
		<author>
			<persName><forename type="first">Bram</forename><surname>Cohen</surname></persName>
		</author>
		<ptr target="http://www.bittorrent.org/beps/bep0003.html" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m">OpenNEbula Project</title>
		<imprint/>
		<respStmt>
			<orgName>Distributed Systems Architecture Research Group</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Service-oriented science</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">308</biblScope>
			<biblScope unit="issue">5723</biblScope>
			<biblScope unit="page" from="814" to="817" />
			<date type="published" when="2005-05">May 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">There&apos;s Grid in them thar Clouds. Ian Foster&apos;s blog</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Foster</surname></persName>
		</author>
		<ptr target="http://ianfoster.typepad.com/blog/2008/01/theres-grid-in.html" />
		<imprint>
			<date type="published" when="2008-01">Jan. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><surname>Gobioff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shun-Tak</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Google File System. SIGOPS</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="29" to="43" />
			<date type="published" when="2003-12">December 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Blue Cloud project [URL]. IBM</title>
		<ptr target="http://www-03.ibm.com/press/us/en/pressrelease/22613.wss/" />
		<imprint>
			<date type="published" when="2008-06">June 2008</date>
			<publisher>IBM Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A vision for the internet</title>
		<author>
			<persName><forename type="first">Leonard</forename><surname>Kleinrock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ST Journal of Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="5" />
			<date type="published" when="2005-11">November 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Richard</forename><surname>Martin</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="report_type">The Red Shift Theory</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">In-Formationweek</forename></persName>
		</author>
		<ptr target="http://www.informationweek.com/news/hardware/showArticle.jhtml?articleID=20180087" />
		<imprint>
			<date type="published" when="2007-08-20">August 20 2007. Aug. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Reservoir Project</title>
		<author>
			<persName><forename type="first">Reservoir</forename><surname>Consortium</surname></persName>
		</author>
		<ptr target="http://www-03.ibm.com/press/us/en/pressrelease/23448.wss/" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<ptr target="http://www.network.com" />
		<title level="m">Sun Microsystem. Network.com</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<author>
			<persName><forename type="first">S</forename><surname>Tuecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Engert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pearlman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Internet X.509 Public Key Infrastructure</title>
		<imprint>
			<date type="published" when="2004-06">June 2004</date>
			<biblScope unit="volume">3820</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<ptr target="http://meta.cesnet.cz/cms/opencms/en/docs/clouds" />
		<title level="m">Nimbus/Stratus/Wispy/Kupa Projects</title>
		<imprint>
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Chicago-University of Florida-Purdue University-Masaryk University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scientific Cloud Computing: Early Definition and Experience</title>
		<author>
			<persName><forename type="first">Lizhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Kunze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Canales Castellanos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Karl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCC &apos;08</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="825" to="830" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
