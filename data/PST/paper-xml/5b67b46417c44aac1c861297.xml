<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Low Resource Named Entity Recognition using Cross-lingual Knowledge Transfer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
							<email>xcfeng@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiachong</forename><surname>Feng</surname></persName>
							<email>xiachongfeng@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
							<email>bqin@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhangyin</forename><surname>Feng</surname></persName>
							<email>zyfeng@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
							<email>tliu@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Low Resource Named Entity Recognition using Cross-lingual Knowledge Transfer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural networks have been widely used for high resource language (e.g. English) named entity recognition (NER) and have shown state-of-the-art results. However, for low resource languages, such as Dutch and Spanish, due to the limitation of resources and lack of annotated data, NER models tend to have lower performances. To narrow this gap, we investigate cross-lingual knowledge to enrich the semantic representations of low resource languages. We first develop neural networks to improve low resource word representations via knowledge transfer from high resource language using bilingual lexicons. Further, a lexicon extension strategy is designed to address out-of lexicon problem by automatically learning semantic projections. Finally, we regard word-level entity type distribution features as an external languageindependent knowledge and incorporate them into our neural architecture. Experiments on two low resource languages (Dutch and Spanish) demonstrate the effectiveness of these additional semantic representations (average 4.8% improvement). Moreover, on Chinese OntoNotes 4.0 dataset, our approach achieves an F-score of 83.07% with 2.91% absolute gain compared to the state-of-the-art systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named entity recognition (NER) is defined as the extraction of a contiguous sequence of textual tokens, which represents the name of an object of a specified class, such as person, location or organization. It plays a vital role in the overall task of Information Extraction (IE) and serves as an intermediate step for subsequent IE tasks, like Relation Extraction and Entity Linking. Current state-of-the-art methods for English NER usually use deep learning algorithms, e.g., Feedforward neural network (FNN) or Recurrent neural network (RNN) <ref type="bibr" target="#b3">[Huang et al., 2015]</ref>, and build name taggers from annotated data with accompanying entity labels. Such models generalize well on new entities based on features automatically learned from the context. However, a neural-based NER *Corresponding author.  system could still get lower performance if its hidden feature representations cannot be learned adequately, which happens frequently when the annotated data is not enough, especially in low resource scenario <ref type="bibr" target="#b8">[Zhang et al., 2016]</ref>. In this paper, we regard English as a high resource language, and other languages, such as Dutch, Spanish, and even Chinese, as low resource languages.</p><p>To improve the performance of low resource NER, we present a neural-based sequential tagger which incorporates additional word representations learned from semantic projections based on cross-lingual knowledge. This approach is partly inspired by the previous empirical success of feature-based sequence labeling models with bilingual constraints/inferences for Chinese and English NER <ref type="bibr" target="#b1">[Che et al., 2013;</ref><ref type="bibr">Wang et al., 2013]</ref>. Our approach is built on the state-of-the-art LSTM-CRF framework <ref type="bibr" target="#b4">[Lample et al., 2016]</ref>, which models each word of the input sentence with a contextual embedding based on Bi-LSTM, and then assigns an entity label for each word using CRF. Compared with previous work which are only based on general word embeddings, we design three strategies to enrich the semantic representations, which embodies our main contributions:</p><p>(1) We build neural networks to model the external semantic representation of each low resource language word based on the translations from high resource languages. The intuition is that different languages usually contain complementary cues about entities and these cues can be further transferred through bilingual lexicons. Figure <ref type="figure" target="#fig_0">1</ref> shows a simple example for Chinese name tagging. The word "本" is common in Chinese but rarely appears as a name. However, based on a Chinese-English dictionary, one of the English translation candidates of "本" is "Ben", which provides a strong semantic clue that the word is a person name in English.</p><p>(2) The lexicon is usually limited and cannot cover all low resource language words. Thus, we further design a new strategy to extend the bilingual mappings with a linear transformation function. After generating the external semantic representations for the low resource language words based on high resource translations (the output of the previous network), we learn a linear projection function between the low resource word embedding space and the high resource language translation semantic space, accordingly, the out-oflexicon low resource language words can also be estimated with a new semantic representation.</p><p>(3) For each word, we calculate its distributional probabilities over all entity types and add them as additional features to the original word representation for both low resource and high resource languages. The rationale is that the entity type distribution can be regarded as a language-independent knowledge and it is helpful for low resource NER. For example, both the English and Chinese are describing the same entity, even probably with different spelling (e.g., "United States" in English vs. "美国" in Chinese), the entity type of that entity does not change from one language to another <ref type="bibr">[Ni and Florian, 2017]</ref>. And if we know that "United States" is a location in English space, then naturally we can predict that the entity type of Chinese word "美国" prefers location.</p><p>Experiments on two low resource languages (Dutch and Spanish) demonstrate that the additional semantic representations can bring in an average 4.8% F-score gain compared with general word embeddings. Moreover, we also conduct experiments on the Chinese portion of the OntoNotes 4.0 corpus. The results show that we achieve a 2.91% improvement compared to the state-of-the-art system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In this section, we first describe the background on LSTM-CRF model, which is the backbone of our approach. Afterwards, we present two neural networks to learn the crosslingual semantic representation of each low resource language word based on high resource language translations. Lastly, we introduce a lexicon extension strategy to alleviate the out-of-lexicon problem and describe how to learn the entity type distribution based on original word representations and entity type representations in both languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Basic Model: LSTM-CRF</head><p>LSTM-CRF model is originally introduced by <ref type="bibr" target="#b3">[Huang et al., 2015]</ref>, which takes a sequence of elements as the input and outputs a sequence of category labels corresponding to the input sequence<ref type="foot" target="#foot_1">1</ref> . The idea has been successfully applied in POS Tagging, Chunking and NER <ref type="bibr" target="#b4">[Liu et al., 2017a]</ref>.</p><p>The approach of <ref type="bibr" target="#b4">[Lample et al., 2016]</ref> is based on LSTM and CRF Tagging models. An illustration of this network is given in the left of Figure <ref type="figure">2</ref>. The LSTM is a special form of recurrent neural networks (RNNs) with three gated units, input, output and forget, which could control the passing of information along the sequence and thus improves the modeling of long-range dependencies. Following <ref type="bibr" target="#b4">[Lample et al., 2016]</ref>, they take a sequence of vectors X = {x 1 , ..., x i , ..., x n } as input and return another sequence H = {h 1 , ..., h i , ..., h n } that represents some information about the sequence at every step in the input. For brevity, the details of LSTM equations are given in <ref type="bibr" target="#b2">[Gers et al., 1999]</ref>. The conditional random field (CRF) <ref type="bibr">[Jurafsky and Martin, 2000</ref>] is a probabilistic graphical model, which works in a sequential way and predict a label sequence y = {y 1 , ..., y i , ..., y n } corresponding to the input sequence X. They define a score function as follows:</p><formula xml:id="formula_0">s(X, y) = n i=0 A yi,yi+1 + n i=1 H i,yi<label>(1)</label></formula><p>where A is a matrix of transition scores such that A i,j represents the score of a transition from the tag i to tag j. y 0 and y n are the start and end tags of a sentence. Matrix A is therefore a square matrix of size k + 2, k is the number of tags. A softmax over all possible tag sequences yields a probability for the sequence y:</p><formula xml:id="formula_1">p(y|X) = exp s(X,y) ỹ∈YX exp s(X,ỹ)<label>(2)</label></formula><p>where Y X represents all possible tag sequences for the input sequence X.</p><p>Furthermore, <ref type="bibr" target="#b4">[Lample et al., 2016</ref>] incorporated characterlevel structure into word representation. Each input vector x i consists of two parts, pre-trained word-level representation w i <ref type="bibr" target="#b5">[Mikolov et al., 2013]</ref> and task-related character-level representation c i . They adopted a bidirectional LSTM to capture information in both forward and backward directions and concatenate the outputs of these two LSTMs as c i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Improved with Bilingual Lexicon</head><p>We present an overview of the developed networks for modeling bilingual lexicons, as illustrated in the right of Figure <ref type="figure">2</ref>. Following the same setting in Section 2.1, given a low resource language sentence X = {x 1 , x 2 , ..., x i , ..., x n }, we assume that each word x i has a corresponding high resource language translation T i based on the bilingual lexicon<ref type="foot" target="#foot_3">2</ref> . The translation T i can be viewed as a combination of multiple translation items and each translation item consists of multiple high resource language words. To make better understanding of the high resource language translation of a low resource word, all translation items should be encoded into the encoder. One simple way is to take the concatenation of all translation words as the input for the vanilla RNN unit <ref type="bibr" target="#b5">[Liu et al., 2017b]</ref>. We also map each high resource language translation word into its embedding vector. Therefore, translation word vectors {t 1 , ..., t i , ..., t l } are stacked and regarded as the translation memory unit T ∈ R d×l , where l is the number of all translation words. An example is given in Figure <ref type="figure" target="#fig_0">1</ref>. The Chinese word "美联储" has two translation items in English, namely "FED" and "Federal Reserve". We can get its translation sequence as ["FED", "Federal", "Reserve" ], of which size is 3. </p><formula xml:id="formula_2">w i c i vec i e i w i-1 c i-1 vec i-1 e i-1 w i+1 c i+1 vec i+1 e i+1 vec i t j-1 t j t j+1 t j-2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSTM layer</head><p>Bilingual lexicon representations by LSTM-based Network</p><formula xml:id="formula_3">t j-1 t j t j+1 t j-2</formula><p>Bilingual lexicon representations by attention-based Network Following <ref type="bibr" target="#b5">[Liu et al., 2017b]</ref>, we first present a LSTMbased network for modeling bilingual lexicons. To better encode the structural information of different translation items, we incorporate the POS-tag information of each translation item into their corresponding translation words. In our method, each POS-tag label is also mapped to a d p (a hyperparameter) dimensional vector, which is randomly initialized and optimized by the model. Then we combine the embedding of each translation word t i and its corresponding POStag vector p i as [t i , p i ], and then feed it to the Bi-LSTM unit. Finally, we concatenate the outputs of forward LSTM and backward LSTM as translation representations vec. We name this model as LSTM-CRF+BL LST M , as illustrated in the upper-right of Figure <ref type="figure">2</ref>.</p><formula xml:id="formula_4">w i vec i =∑ 𝛼 𝑗 t 𝑗 𝑗 E O E L E N E P Entity Type Distribution w i cos ij = E 𝑗 •w 𝑖 𝑇 ∥E 𝑗 ∥×∥w 𝑖 ∥ e i</formula><p>Considering that each word in the translation does not contribute equally to the semantic meaning of the original low resource word, we further introduce an attention-based network to model the bilingual lexicons, which is similar as the attention-based memory network in question answering <ref type="bibr">[Sukhbaatar et al., 2015]</ref>. In detail, taking an external translation unit T ∈ R d×l and a low resource word vector x i ∈ R d as input, the attention model outputs a continuous vector vec ∈ R d , which is a weighted sum of each piece of memory in T:</p><formula xml:id="formula_5">vec = l j=1 α j t j (3)</formula><p>where l is the memory unit size, α j ∈ [0, 1] is the weight of t j and j α j = 1. We implement a neural network based attention model based on previous work <ref type="bibr" target="#b0">[Bahdanau et al., 2014]</ref>.</p><p>For each piece of translation memory t j , we use a feed forward neural network to compute its semantic relatedness with the low resource word. The scoring function is calculated as follows:</p><formula xml:id="formula_6">g j = tanh(W att x i + U att t j + b att )<label>(4)</label></formula><p>where</p><formula xml:id="formula_7">W att ∈ R d , U att ∈ R d and b tt ∈ R 1×1 .</formula><p>After obtaining g 1 , g 2 , ...g l , we feed them to a sof tmax function to calculate the final importance distribution α 1 , α 2 , ...α l . We name this model as LSTM-CRF+BL AT T , as illustrated in the right-middle of Figure <ref type="figure">.</ref> 2</p><formula xml:id="formula_8">α j = exp(g j ) l z=1 exp(g z )</formula><p>(5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Improved with Mapping based Lexicon Extension Strategy</head><p>In the actual situation, the bilingual lexicons can not cover all low resource language words. To overcome this challenge, we design a lexicon extension strategy to estimate the translation representations of out-of-lexicon word. Suppose there is a low resource language word set W = {w 1 , ..., w i , ..., w f }, each word has a low resource word vector w i and a high resource language translation vector vec i . We learn a linear projection function as the transformation between the two semantic space, as follows:</p><formula xml:id="formula_9">vec i = Mw i (<label>6</label></formula><formula xml:id="formula_10">)</formula><p>where M is the mapping matrix. We minimize the following objective to optimize M:</p><formula xml:id="formula_11">loss M = f i=1 ||vec i − Mw i || 2<label>(7)</label></formula><p>After obtain M, for each out-of-lexicon word o i , we can estimate the translation representation veo i as follows:</p><formula xml:id="formula_12">veo i = Mo i (8)</formula><p>2.4 Improved with Language-Independent Entity Type Distribution</p><p>In this section, we introduce the entity type-based distributional features, which denotes the probabilities of each word to be tagged as each entity type. Word embeddings have been empirically shown to preserve linguistic regularities, such as similar words tend to be close to each other in the same space <ref type="bibr" target="#b5">[Mikolov et al., 2013]</ref>. We observe that the same property also applies to the words with the same entity type. For example, the distance between the word "Microsoft" and "Bill Gates" is larger than that between "Microsoft" and "IBM". Therefore, we can learn an approximate representation of each entity type, and use the similarities between each entity type representation and each word embedding as the entity type-based distributional feature.</p><p>In this work, we focus on three most common named entity types, i.e., P (Person), L (Location), O (Organization), and discard the others. Taking low resource language as an example, we randomly select 10 entities from each entity type and average their embeddings as the entity type representation. At the same time, we randomly generate one vector representing non-entity; Therefore, four entity type vectors</p><formula xml:id="formula_13">{E P , E O , E L , E N } are constructed, each E j ∈ R d .</formula><p>Afterwards, we use standard cosine function to calculate the semantic relatedness between the low resource word embedding w i and the entity type representation E j .</p><formula xml:id="formula_14">e ij = w T i • E j ||w i || × ||E j ||<label>(9)</label></formula><p>For high resource language, we also calculate the entity distribution of each word. In the end, each low resource language word and each high resource language word are assigned with an entity distributional feature vector with dimensionality 4, e i = {e P , e O , e L , e N }, as illustrated in the bottom-right of Figure <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Low Resource NER</head><p>Now, the vectors of each word in the input low resource sentence is made up of four parts: a word embedding w i , a character-level representation c i , a high resource translation vector vec i or veo i , and an entity type distributional representation e i . we regard the concatenation vectors of these four representations as word representation</p><formula xml:id="formula_15">x i = [w i , c i , vec i , e i ],</formula><p>and feed them into the previous LSTM-CRF model for NER (Section 2.1). The model is trained in a supervised manner by minimizing the cross entropy error of sequence labeling and L2 loss:</p><formula xml:id="formula_16">loss = − X∈C y∈ ỸX p g (ỹ|X)log(p(ỹ|X)) + loss M (<label>10</label></formula><formula xml:id="formula_17">)</formula><p>where p(y|X) is the probability of predicting sequence X as tag sequence ỹ. C denotes all training sentences. X is the input sentence representations. Y X represents all possible tag sequences for the input sequence. p g (ỹ|X) is 1 or 0, indicating whether the correct sequence tag is ỹ. We use back propagation to calculate the gradients of all the parameters, and update them with stochastic gradient descent. We randomize other parameters with uniform distribution U (0. and set the learning rate as 0.01. Table <ref type="table" target="#tab_2">1</ref> illustrates the word embedding parameters used in our experiments<ref type="foot" target="#foot_4">3</ref> . For brevity, the details of other parameters are given in our codes<ref type="foot" target="#foot_5">4</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head><p>We apply our neural architecture for NER on various datasets and evaluate the effectiveness separately. In this section, we will describe the detailed experimental settings and discuss the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>We evaluate the proposed approach on two low resource languages (including Spanish and Dutch<ref type="foot" target="#foot_6">5</ref> ), and Chinese<ref type="foot" target="#foot_7">6</ref> , which is distinct from Latin-based languages. In this paper, we regard English as high resource language and all other languages as low resource languages. Table <ref type="table">2</ref> shows the detailed description of the data sets used in our experiments. In this paper, we focus on four entity types (Person, Location, Organization, None), which are commonly adopted in previous NER studies <ref type="bibr" target="#b1">[Che et al., 2013;</ref><ref type="bibr">Wang et al., 2013]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Low Resource NER</head><p>We compare with the following baseline methods on the two languages.</p><p>• LSTM-CRF <ref type="bibr" target="#b4">[Lample et al., 2016]</ref> is introduced in section 2.1. Compared with standard Bi-LSTM, it adds a CRF layer to impose several hard constraints of the "grammar". • LM-LSTM-CRF <ref type="bibr" target="#b4">[Liu et al., 2017a]</ref> is also a LSTM-CRF-based sequence labeling framework and incorporates residual network and language model to extract character-level knowledge from the self-contained order information.</p><p>• CLNER (Cross-Lingual Named Entity Recognition) <ref type="bibr" target="#b6">[Ni et al., 2017]</ref>, a weakly supervised method, which creates automatically labeled NER data for a target language via annotation projection on comparable corpora. The recognizer is a prototype-based neural model.</p><p>Our model has several variations, which are detailed below.</p><p>•  <ref type="table" target="#tab_4">3</ref>. Evaluation metric is F measure <ref type="bibr" target="#b5">[Manning and Schütze, 1999]</ref>. We can find that our method LSTM-CRF+BL+M+E LST M yields the best performance on two languages compared with many strong baselines. The performance of CLNER are relatively low because both of them utilize indirectly acquired features based on linguistic resources and cross-lingual entity mappings. LSTM-CRF obtains significant improvement over CLNER by integrating semantic representations of low resource language words and learning the constraints between entity labels. Besides, we surprisingly find that all of our variants outperform the strong baseline LSTM-CRF on two languages, which demonstrate the effectiveness of each of the additional semantic representations for low resource NER. Among the six variants of our model, LSTM-based models perform better than attention-based models, which indicates that the sequence feature is more important for modeling lexicon structure. In the last, two real examples are given in Table <ref type="table" target="#tab_5">4</ref> to demonstrate the effectiveness of the additional bilingual lexicon representations for low resource NER.  To demonstrate the effectiveness of our models on largescale corpora, we show the results on Chinese Ontonotes 4.0 NER in Table <ref type="table" target="#tab_6">5</ref>. Additionally, we add a strong baseline for Chinese NER with bilingual constraints, namely Soft-Align<ref type="foot" target="#foot_8">7</ref> . From Table <ref type="table" target="#tab_6">5</ref>, we can still get consistent improvements on Chinese NER over previous state-of-the-art methods. Specifically, we observe that LSTM-CRF+BL+M+E LST M achieves a significant gain in Recall. This is reasonable since the semantic representation of each Chinese word is much richer in LSTM-CRF+BL+M+E LST M than other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Fine-Grained Performance on Different Groups</head><p>This subsection studies the effectiveness of our cross-lingual representations. For comparison purposes, we select the baselines: LSTM-CRF <ref type="bibr" target="#b4">[Lample et al., 2016]</ref> and LM-LSTM-CRF <ref type="bibr" target="#b4">[Liu et al., 2017a]</ref>  guages, our model (LSTM-CRF+BL+M+E LST M ) yields an average 6.44% improvement, which is 2 times in situation A. This demonstrates that the cross-language representation has better ability to model non-covered entities than word-level and character-level representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>There exist two threads of related work regarding the topics in this paper, which are Monolingual NER and how to improve it with other languages (Cross-lingual NER).  <ref type="bibr" target="#b5">[Ma and Hovy, 2016]</ref>. Furthermore, character-based representations had been proved to be effective in capturing the orthographic and morphological evidence. Also, most of these models added a CRF layer, and reported significant improvement over pure RNN models. Our architecture is based on the success of LSTM-CRF model and is further modified to enrich the word representation with cross-lingual knowledge information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Monolingual NER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Cross-lingual NER</head><p>The idea of utilizing multilingual resources to improve monolingual name tagger systems has been studied extensively. <ref type="bibr" target="#b4">[Li et al., 2012]</ref> presented a cyclic CRF model and performed approximate inference using loopy belief propagation. Although, their feature-rich CRF formulation of bilingual edge is powerful, an obvious drawback of this approach is the requirement of manually annotate bilingual NER data. There-fore, <ref type="bibr" target="#b1">[Chen et al., 2010]</ref> proposed approaches to extract bilingual named entity pairs from unannotated bitext. The verification was based on bilingual named entity dictionaries. In this regard, one of the most interesting papers is <ref type="bibr">[Burkett et al., 2010]</ref>, which explored an "up-training" mechanism by using the outputs from a strong monolingual model as groundtruth, and thereby simulated a learning environment, where a bilingual model is trained to help a "weakened" monolingual model recover the results of the strong model. <ref type="bibr" target="#b4">[Kim et al., 2012]</ref> proposed a method of labeling bilingual corpora with named entity labels automatically based on Wikipedia.</p><p>[ <ref type="bibr" target="#b1">Che et al., 2013;</ref><ref type="bibr">Wang et al., 2013]</ref> tackled the problem of jointly recognizing and aligning bilingual named entities. For low resource NER, <ref type="bibr" target="#b8">[Zhang et al., 2016]</ref> proposed an expectation-driven model that designed a large number of language-specific features (rules, patterns, gazetteers, etc.) via consulting and encoding linguistic knowledge from native speakers. <ref type="bibr">[Ni and Florian, 2017;</ref><ref type="bibr" target="#b6">Ni et al., 2017]</ref> developed approaches to improve multilingual name tagging performances with Wikipedia entity type mapping and word distribution mapping. However, these methods suffer from error propagation. Moreover, the selection and collection of task related features are time-consuming and labor intensive.</p><p>Our approach differs in that it does not acquire any hand-craft features and bilingual lexicons are one of the most basic language resources for all languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Low resource NER is a very important yet challenging problem in natural language processing. In this paper, we focus on this problem by incorporating cross-lingual knowledge into a neural architecture, which guides low resource name tagging to achieve a better performance. Specifically, we use bilingual lexicons to bridge cross-lingual semantic mapping and design a lexicon extension strategy to alleviate the out-of-lexicon problem. Moreover, we regard entity type distribution as language-independent features and model them in our architecture. Experiments on three languages, namely, Dutch, Spanish and Chinese demonstrate the effectiveness of our model for low resource language NER. In the future, we will incorporate other knowledge resources, such as FrameNet and WordNet, from high resource languages into our neural architecture. We will also extend our architecture to other NLP tasks, such as event extraction, sentiment analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of NER labels with bilingual lexicon.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>LSTM-CRF+BL LST M extends LSTM-CRF by taking into account of the bilingual lexicon, and uses a LSTMbased network towards the translations. • LSTM-CRF+BL+M LST M : an extension of LSTM-CRF+BL LST M by further incorporating the lexicon extension strategy. • LSTM-CRF+BL+M+E LST M : an extension of LSTM-CRF+BL+M LST M by further concatenating the semantic representation of each word with entity type distribution features in both languages. • LSTM-CRF+BL AT T extends LSTM-CRF by taking into account of the bilingual lexicon, and uses an attentionbased network towards the translations. • LSTM-CRF+BL+M AT T : an extension of LSTM-CRF+BL +AT T by further incorporating the lexicon extension strategy. • LSTM-CRF+BL+M+E AT T : an extension of LSTM-CRF+BL+M +AT T by further concatenating the semantic representation of each word with entity type distribution features in both languages. Experimental results are given in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Embedding parameters used in our experiments on four languages.</figDesc><table><row><cell>01, 0.01),</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of different methods on low resource NER.</figDesc><table><row><cell>Example 1</cell><cell cols="6">Some are in the George Grard Foundation.</cell></row><row><cell>Dutch</cell><cell cols="6">Een aantal is in de Stichting George Grard.</cell></row><row><cell>Golden</cell><cell cols="6">O O O O O B-ORG I-ORG E-ORG</cell></row><row><cell>LSTM-CRF</cell><cell cols="6">O O O O O B-LOC I-LOC E-LOC</cell></row><row><cell>Our model</cell><cell cols="6">O O O O O B-ORG I-ORG E-ORG</cell></row><row><cell>Example 2</cell><cell cols="6">The delegate of the Andalusian Gov in Cádiz,</cell></row><row><cell>Spanish</cell><cell cols="6">El delegado del Gobierno andaluz en Cádiz,</cell></row><row><cell>Golden</cell><cell>O</cell><cell>O</cell><cell cols="2">O S-ORG</cell><cell cols="2">O O S-LOC</cell></row><row><cell cols="2">LSTM-CRF O</cell><cell>O</cell><cell>O</cell><cell>O</cell><cell>O O</cell><cell>O</cell></row><row><cell>Our model</cell><cell>O</cell><cell>O</cell><cell cols="2">O S-ORG</cell><cell cols="2">O O S-LOC</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Case study for Dutch and Spanish NER (Italic Sentences Show the English Translations for the Dutch and Spanish Examples). Our model is LSTM-CRF+BL+M LST M .</figDesc><table><row><cell>3.3 Chinese NER</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Chinese</cell><cell cols="3">Precision Recall F-score</cell></row><row><cell>Soft-Align</cell><cell>77.37</cell><cell>71.13</cell><cell>74.13</cell></row><row><cell>LSTM-CRF</cell><cell>82.58</cell><cell>76.92</cell><cell>79.65</cell></row><row><cell>LM-LSTM-CRF</cell><cell>81.90</cell><cell>78.50</cell><cell>80.16</cell></row><row><cell>LSTM-CRF+BL LST M</cell><cell>82.01</cell><cell>82.81</cell><cell>82.41</cell></row><row><cell>LSTM-CRF+BL+M LST M</cell><cell>82.05</cell><cell>83.24</cell><cell>82.64</cell></row><row><cell>LSTM-CRF+BL+M+E LST M</cell><cell>82.84</cell><cell>83.32</cell><cell>83.07</cell></row><row><cell>LSTM-CRF+BL AT T</cell><cell>81.72</cell><cell>80.68</cell><cell>81.20</cell></row><row><cell>LSTM-CRF+BL+M AT T</cell><cell>82.41</cell><cell>80.44</cell><cell>81.42</cell></row><row><cell>LSTM-CRF+BL+M+E AT T</cell><cell>82.46</cell><cell>80.97</cell><cell>81.71</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison of different methods on Chinese NER.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Comparison of the results for LSTM-CRF, LM-LSTM-CRF and our LSTM-based networks. A denotes the entities appearing in both training and test datasets, and B indicates all other cases. Evaluation metric is F measure.</figDesc><table><row><cell>and compare them with LSTM-based bilin-</cell></row><row><cell>gual lexicon models. Moreover, to prove that cross-lingual</cell></row><row><cell>representation could capture more valuable semantics, espe-</cell></row><row><cell>cially for the entities that appear in the testing data but never</cell></row><row><cell>appear in the training data, we divide the entities in the test-</cell></row><row><cell>ing data into two parts (A: appearing in both testing and train-</cell></row><row><cell>ing data with the same entity type, or B: appearing in testing</cell></row><row><cell>data only) and perform evaluations separately. Experimental</cell></row><row><cell>results are shown in Table 6 and illustrate that for all situ-</cell></row><row><cell>ations, cross-lingual representation brings in significant im-</cell></row><row><cell>provements compared with word embedding and character-</cell></row><row><cell>level representation in NER. For situation B in three lan-</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">Entity types are usually represented in BIOES format (which stand for Begin, Inside, Outside, End, and Single, indicating the position of the token in the entity) as this scheme has been reported to outperform others such as BIO [Ratinov and Roth</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2009" xml:id="foot_2">, 2009].</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3">We construct bilingual lexicons from online translators such as Bing Dict and FAIR (Facebook AI Research) dictionary.Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_4">FAIR:https://github.com/facebookresearch/MUSE</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_5">Our code is available at: https://github.com/scir-code/lrner.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_6">CoNLL: https://github.com/synalp/NER/tree/master/corpus/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_7">Ontonotes: https://catalog.ldc.upenn.edu/ldc2011t03Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_8">Che et al. proposed a novel Integer Linear Programming-based inference algorithm with bilingual constraints for English and Chinese NER.Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the National Natural Science Foundation of China (NSFC) via grant 61632011, 61772156 and 61472107.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><surname>Bahdanau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CONLL</title>
				<editor>
			<persName><forename type="first">David</forename><surname>Burkett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</editor>
		<imprint>
			<publisher>CoRR</publisher>
			<date type="published" when="2010">2014. 2014. 2015. 2015. 2010</date>
			<biblScope unit="page" from="46" to="54" />
		</imprint>
	</monogr>
	<note>Learning better monolingual models with unannotated bilingual text</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hai Leong Chieu and Hwee Tou Ng. Named entity recognition: a maximum entropy approach using global information</title>
		<author>
			<persName><forename type="first">Che</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08308</idno>
	</analytic>
	<monogr>
		<title level="m">Jason PC Chiu and Eric Nichols. Named entity recognition with bidirectional lstm-cnns</title>
				<imprint>
			<publisher>Abe Ittycheriah</publisher>
			<date type="published" when="2002">2013. 2013. 2010. 2010. 2002. 2002. 2015. 2015</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>HLT-NAACL. Florian et al., 2003] Radu Florian</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Named entity recognition through classifier combination</title>
		<author>
			<persName><forename type="first">Hongyan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Gers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
				<imprint>
			<date type="published" when="1999">2003. 1999. 1999</date>
			<biblScope unit="page" from="168" to="171" />
		</imprint>
	</monogr>
	<note>Learning to forget: Continual prediction with lstm</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
	</analytic>
	<monogr>
		<title level="m">Jurafsky and Martin, 2000] Dan Jurafsky and Jameás H Martin. Speech &amp; language processing</title>
				<imprint>
			<publisher>Pearson Education India</publisher>
			<date type="published" when="2000">2015. 2015. 2000</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01360</idno>
		<idno>arXiv:1709.04109</idno>
	</analytic>
	<monogr>
		<title level="m">Empower sequence labeling with task-aware neural language model</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2012. 2012. 2001. 2001. 2016. 2016. 2012. 2012. 2017a. 2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1727" to="1731" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Neural architectures for named entity recognition</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01354</idno>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<meeting><address><addrLine>Florian</addrLine></address></meeting>
		<imprint>
			<publisher>Jian Ni and Radu Florian</publisher>
			<date type="published" when="1999">2017b. 2017. Ma and Hovy, 2016. 2016. 1999. 1999. 2013. 2017. 2017</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Table-to-text generation by structure-aware seq2seq learning. Improving multilingual named entity recognition with wikipedia entity type mapping. EMNLP</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Jian Ni, Georgiana Dinu, and Radu Florian. Weakly supervised cross-lingual named entity recognition via effective annotation and representation projection</title>
		<author>
			<persName><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="page" from="1470" to="1480" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Effective bilingual constraints for semi-supervised learning of named entity recognizers</title>
		<author>
			<persName><surname>Peters</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00108</idno>
		<idno>arXiv:1505.05008</idno>
	</analytic>
	<monogr>
		<title level="m">Cicero Nogueira dos Santos and Victor Guimarães. Boosting named entity recognition with neural character embeddings</title>
				<imprint>
			<publisher>Wanxiang Che, and Christopher D Manning</publisher>
			<date type="published" when="2009">2017. 2017. 2009. 2009. 2015. 2015. 2015. 2013. 2013</date>
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>AAAI. Citeseer</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Name tagging for low-resource incident languages based on expectation-driven learning</title>
		<author>
			<persName><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
				<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016">2017. 2017. 2016. 2016</date>
			<biblScope unit="page" from="249" to="259" />
		</imprint>
	</monogr>
	<note>Transfer learning for sequence tagging with hierarchical recurrent networks</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
