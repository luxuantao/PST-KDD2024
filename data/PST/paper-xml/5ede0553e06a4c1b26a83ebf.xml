<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">When Does Self-Supervision Help Graph Convolutional Networks?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
							<email>&lt;yshen@tamu.edu&gt;.</email>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">When Does Self-Supervision Help Graph Convolutional Networks?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervision as an emerging technique has been employed to train convolutional neural networks (CNNs) for more transferrable, generalizable, and robust representation learning of images. Its introduction to graph convolutional networks (GCNs) operating on graph data is however rarely explored. In this study, we report the first systematic exploration and assessment of incorporating self-supervision into GCNs. We first elaborate three mechanisms to incorporate selfsupervision into GCNs, analyze the limitations of pretraining &amp; finetuning and self-training, and proceed to focus on multi-task learning. Moreover, we propose to investigate three novel selfsupervised learning tasks for GCNs with theoretical rationales and numerical comparisons. Lastly, we further integrate multi-task self-supervision into graph adversarial training. Our results show that, with properly designed task forms and incorporation mechanisms, self-supervision benefits GCNs in gaining more generalizability and robustness. Our codes are available at https: //github.com/Shen-Lab/SS-GCNs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graph convolutional networks (GCNs) <ref type="bibr" target="#b18">(Kipf &amp; Welling, 2016)</ref> generalize convolutional neural networks (CNNs) <ref type="bibr" target="#b21">(LeCun et al., 1995)</ref> to graph-structured data and exploit the properties of graphs. They have outperformed traditional approaches in numerous graph-based tasks such as node or link classification <ref type="bibr" target="#b18">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b35">Veličković et al., 2017;</ref><ref type="bibr" target="#b27">Qu et al., 2019;</ref><ref type="bibr" target="#b36">Verma et al., 2019;</ref><ref type="bibr" target="#b16">Karimi et al., 2019;</ref><ref type="bibr" target="#b44">You et al., 2020)</ref>, link prediction <ref type="bibr" target="#b46">(Zhang &amp; Chen, 2018)</ref>, and graph classification <ref type="bibr" target="#b43">(Ying et al., 2018;</ref><ref type="bibr" target="#b42">Xu et al., 2018)</ref>, many of which are semi-supervised learning tasks. In this paper, we mainly focus our discussion on transductive semisupervised node classification, as a representative testbed Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 108, 2020. Copyright 2020 by the author(s).</p><p>for GCNs, where there are abundant unlabeled nodes and a small number of labeled nodes in the graph, with the target to predict the labels of remaining unlabeled nodes.</p><p>In a parallel note, self-supervision has raised a surge of interest in the computer vision domain <ref type="bibr" target="#b14">(Goyal et al., 2019;</ref><ref type="bibr" target="#b19">Kolesnikov et al., 2019;</ref><ref type="bibr" target="#b24">Mohseni et al., 2020)</ref> to make use of rich unlabeled data. It aims to assist the model to learn more transferable and generalized representation from unlabeled data via pretext tasks, through pretraining (followed by finetuning), or multi-task learning. The pretext tasks shall be carefully designed in order to facilitate the network to learn downstream-related semantics features <ref type="bibr" target="#b31">(Su et al., 2019)</ref>. A number of pretext tasks have been proposed for CNNs, including rotation <ref type="bibr" target="#b12">(Gidaris et al., 2018)</ref>, exemplar <ref type="bibr" target="#b9">(Dosovitskiy et al., 2014)</ref>, jigsaw <ref type="bibr" target="#b25">(Noroozi &amp; Favaro, 2016)</ref> and relative patch location prediction <ref type="bibr" target="#b8">(Doersch et al., 2015)</ref>. Lately, <ref type="bibr" target="#b15">Hendrycks et al. (2019)</ref> demonstrated the promise of self-supervised learning as auxiliary regularizations for improving robustness and uncertainty estimation. <ref type="bibr" target="#b5">Chen et al. (2020)</ref> introduced adversarial training into self-supervision, to provide the first general-purpose robust pretraining.</p><p>In short, GCN tasks usually admit transductive semisupervised settings, with tremendous unlabeled nodes; meanwhile, self-supervision plays an increasing role in utilizing unlabeled data in CNNs. In view of the two facts, we are naturally motivated to ask the following interesting, yet rarely explored question:</p><p>Can self-supervised learning play a similar role in GCNs to improve their generalizability and robustness?</p><p>Contributions. This paper presents the first systematic study on how to incorporate self-supervision in GCNs, unfolded by addressing three concrete questions: Q1: Could GCNs benefit from self-supervised learning in their classification performance? If yes, how to incorporate it in GCNs to maximize the gain?</p><p>Q2: Does the design of pretext tasks matter? What are the useful self-supervised pretext tasks for GCNs?</p><p>Q3: Would self-supervision also affect the adversarial robustness of GCNs? If yes, how to design pretext tasks?</p><p>Directly addressing the above questions, our contributions are summarized as follows:</p><p>arXiv:2006.09136v2 <ref type="bibr">[cs.</ref>LG] 17 Jun 2020</p><p>A1: We demonstrate the effectiveness of incorporating selfsupervised learning in GCNs through multi-task learning, i.e. as a regularization term in GCN training. It is compared favorably against self-supervision as pretraining, or via self-training <ref type="bibr" target="#b32">(Sun et al., 2019)</ref>.</p><p>A2: We investigate three self-supervised tasks based on graph properties. Besides the node clustering task previously mentioned in <ref type="bibr" target="#b32">(Sun et al., 2019)</ref>, we propose two new types of tasks: graph partitioning and completion. We further illustrate that different models and datasets seem to prefer different self-supervised tasks.</p><p>A3: We further generalize the above findings into the adversarial training setting. We provide extensive results to show that self-supervision also improves robustness of GCN under various attacks, without requiring larger models nor additional data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Graph-based semi-supervised learning. Semi-supervised graph-based learning works with the crucial assumption that the nodes connected with edges of larger weights are more likely to have the same label <ref type="bibr" target="#b48">(Zhu &amp; Goldberg, 2009)</ref>.</p><p>There are abundance of work on graph-based methods, e.g. (randomized) mincuts <ref type="bibr" target="#b2">(Blum &amp; Chawla, 2001;</ref><ref type="bibr" target="#b3">Blum et al., 2004)</ref>, Boltzmann machines <ref type="bibr" target="#b11">(Getz et al., 2006;</ref><ref type="bibr" target="#b47">Zhu &amp; Ghahramani, 2002)</ref> and graph random walks <ref type="bibr" target="#b0">(Azran, 2007;</ref><ref type="bibr" target="#b33">Szummer &amp; Jaakkola, 2002)</ref>. Lately, graph convolutional network (GCN) <ref type="bibr" target="#b18">(Kipf &amp; Welling, 2016)</ref> and its variants <ref type="bibr" target="#b35">(Veličković et al., 2017;</ref><ref type="bibr" target="#b27">Qu et al., 2019;</ref><ref type="bibr" target="#b36">Verma et al., 2019)</ref> have gained their popularity by extending the assumption from a hand-crafted one to a data-driven fashion. A detailed review could be referred to <ref type="bibr" target="#b41">(Wu et al., 2019b)</ref>.</p><p>Self-supervised learning. Self-supervision is a promising direction for neural networks to learn more transferable, generalized and robust features in computer vision domain <ref type="bibr" target="#b14">(Goyal et al., 2019;</ref><ref type="bibr" target="#b19">Kolesnikov et al., 2019;</ref><ref type="bibr" target="#b15">Hendrycks et al., 2019)</ref>. So far, the usage of self-supervision in CNNs mainly falls under two categories: pretraining &amp; finetuning, or multi-task learning. In pretraining &amp; finetuning. the CNN is first pretrained with self-supervised pretext tasks, and then finetuned with the target task supervised by labels <ref type="bibr" target="#b34">(Trinh et al., 2019;</ref><ref type="bibr" target="#b25">Noroozi &amp; Favaro, 2016;</ref><ref type="bibr" target="#b12">Gidaris et al., 2018)</ref>, while in multi-task learning the network is trained simultaneously with a joint objective of the target supervised task and the self-supervised task(s). <ref type="bibr" target="#b7">(Doersch &amp; Zisserman, 2017;</ref><ref type="bibr" target="#b28">Ren &amp; Jae Lee, 2018)</ref>.</p><p>To our best knowledge, there has been only one recent work pursuing self-supervision in GCNs <ref type="bibr" target="#b32">(Sun et al., 2019)</ref>, where a node clustering task is adopted through self-training. However, self-training suffers from limitations including performance "saturation" and degrading (to be detailed in Sections 3.2 and 4.1 for theoretical rationales and empirical results).</p><p>It also restricts the types of self-supervision tasks that can be incorporated.</p><p>Adversarial attack and defense on graphs. Similarly to CNNs, the wide applicability and vulnerability of GCNs raise an urgent demand for improving their robustness. Several algorithms are proposed to attack and defense on graph <ref type="bibr" target="#b6">(Dai et al., 2018;</ref><ref type="bibr" target="#b49">Zügner et al., 2018;</ref><ref type="bibr" target="#b37">Wang et al., 2019a;</ref><ref type="bibr" target="#b39">Wu et al., 2019a;</ref><ref type="bibr" target="#b38">Wang et al., 2019b)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first elaborate three candidate schemes to incorporate self-supervision with GCNs. We then design novel self-supervised tasks, each with its own rationale explained. Lastly we generalize self-supervised to GCN adversarial defense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Graph Convolutional Networks</head><p>Given an undirected graph G = {V, E}, where V = {v 1 , ..., v |V| } represents the node set with |V| nodes, E = {e 1 , ..., e |E| } stands for the edge set with |E| edges, and e n = (v i , v j ) indicates an edge between nodes v i and v j . Denoting X ∈ R |V|×N as the feature matrix where x n = X[n, :] T is the N -dimensional attribute vector of the node v n , and A ∈ R |V|×|V| as the adjacency matrix where</p><formula xml:id="formula_0">a ij = A[i, j] = { 1, if (vi,vj )∈E 0, otherwise</formula><p>and a ij = a ji , the GCN model of semi-supervised classification with two layers <ref type="bibr" target="#b18">(Kipf &amp; Welling, 2016)</ref> is formulated as:</p><formula xml:id="formula_1">Z = Â ReLU( ÂXW 0 ) W 1 ,<label>(1)</label></formula><p>where Â = D− 1 2 (A + I) D− 1 2 , and D is the degree matrix of A + I. Here we do not apply softmax function to the output but treat it as a part of the loss described below.</p><p>We can treat Â ReLU( ÂXW 0 ) in (1) as the feature extractor f θ (X, Â) of GCNs in general. The parameter set θ = {W 0 } in (1) but could include additional parameters for corresponding network architectures in GCN variants <ref type="bibr" target="#b35">(Veličković et al., 2017;</ref><ref type="bibr" target="#b27">Qu et al., 2019;</ref><ref type="bibr" target="#b36">Verma et al., 2019)</ref>.</p><p>Thus GCN is decomposed into feature extraction and linear transformation as Z = f θ (X, Â)Θ where parameters θ and Θ = W 1 are learned from data. Considering the transductive semi-supervised task, we are provided the labeled node set V label ⊂ V with |V label | |V| and the label matrix Y ∈ R |V|×N with label dimension N (for a classification task N = 1). Therefore, the model parameters in GCNs are learned by minimizing the supervised loss calculated between the output and the true label for labeled nodes, which can be formulated as:</p><formula xml:id="formula_2">Z = f θ (X, Â)Θ, θ * , Θ * = arg min θ,Θ L sup (θ, Θ) = arg min θ,Θ 1 |V label | vn∈V label L(z n , y n ),<label>(2)</label></formula><p>where L(•, •) is the loss function for each example, y n = Y [n, :] T is the annotated label vector, and z n = Z[n, :] T is the true label vector for v n ∈ V label .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Three Schemes: Self-Supervision Meets GCNs</head><p>Inspired by relevant discussions in CNNs <ref type="bibr" target="#b14">(Goyal et al., 2019;</ref><ref type="bibr" target="#b19">Kolesnikov et al., 2019;</ref><ref type="bibr" target="#b15">Hendrycks et al., 2019)</ref>, we next investigate three possible schemes to equip a GCN with a self-supervised task ("ss"), given the input X ss , Âss the label Y ss and the node set V ss .</p><p>Pretraining &amp; finetuning. In the pretraining process, the network is trained with the self-supervised task as following:</p><formula xml:id="formula_3">Z ss = f θ (X ss , Âss )Θ ss , θ * ss , Θ * ss = arg min θ,Θss L ss (θ, Θ ss ) = arg min θ,Θ 1 |V ss | vn∈Vss L ss (z ss,n , y ss,n ), (3)</formula><p>where Θ ss is the linear transformation parameter, and</p><formula xml:id="formula_4">L ss (•, •) is the loss function of the self-supervised task, z ss,n = Z ss [n, :] T , y ss,n = Y ss [n, :] T . Then in the fine- tuing process the feature extractor f θ (•, •) is trained in for- mulation (2) using θ *</formula><p>ss to initialize parameters θ. Pretraining &amp; finetuning is arguably the most straightforward option for self-supervision benefiting GCNs. However, our preliminary experiment found little performance gain from it on a large dataset Pubmed (Table <ref type="table" target="#tab_1">1</ref>). We conjecture that it is due to (1) "switching" to a different objective function L sup (•, •) in finetuning from that in pretraining L ss (•, •); and ( <ref type="formula" target="#formula_2">2</ref>) training a shallow GCN in the transductive semisupervised setting, which was shown to beat deeper GCNs causing over-smoothing or "information loss" <ref type="bibr" target="#b22">(Li et al., 2018;</ref><ref type="bibr" target="#b26">Oono &amp; Suzuki)</ref>. We will systematically assess and analyze this scheme over multiple datasets and combined with other self-supervision tasks in Section 4.1.</p><p>Self-training. <ref type="bibr" target="#b32">(Sun et al., 2019)</ref> is the only prior work that pursues self-supervision in GCNs and it does so through selftraining. With both labeled and unlabeled data, a typical self-training pipeline starts by pretraining a model over the labeled data, then assigning "pseudo-labels" to highly confident unlabeled samples, and including them into the labeled data for the next round of training. The process could be repeated several rounds and can be formulated in each round similar to formulation (2) with V label updated. The authors of <ref type="bibr" target="#b32">(Sun et al., 2019)</ref> proposed a multi-stage selfsupervised (M3S) training algorithm, where self-supervision was injected to align and refine the pseudo labels for the unlabeled nodes. Despite improving performance in previous few-shot experiments, M3S shows performance gain "saturation" in Table <ref type="table" target="#tab_2">2</ref> as the label rate grows higher, echoing literature <ref type="bibr" target="#b48">(Zhu &amp; Goldberg, 2009;</ref><ref type="bibr" target="#b22">Li et al., 2018)</ref>. Further, we will show and rationalize their limited performance boost in Section 4.1.</p><p>Multi-task learning. Considering a target task and a selfsupervised task for a GCN with (2), the output and the training process can be formulated as:</p><formula xml:id="formula_5">Z = f θ (X, Â)Θ, Z ss = f θ (X ss , Âss )Θ ss , θ * , Θ * , Θ * ss = arg min θ,Θ,Θss α 1 L sup (θ, Θ) + α 2 L ss (θ, Θ ss ),<label>(4)</label></formula><p>where α 1 , α 2 ∈ R &gt;0 are the weights for the overall supervised loss L sup (θ, Θ) as defined in (2) and those for the self-supervised loss L ss (θ, Θ ss ) as defined in (3), respectively. To optimize the weighted sum of their losses, the target supervised and self-supervised tasks share the same feature extractor f θ (•, •) but have their individual linear transformation parameters Θ * and Θ * ss as in Figure <ref type="figure" target="#fig_0">1</ref>. In the problem (4), we regard the self-supervised task as a regularization term throughout the network training. The regularization term is traditionally and widely used in graph signal processing, and a famous one is graph Laplacian regularizer (GLR) <ref type="bibr" target="#b30">(Shuman et al., 2013;</ref><ref type="bibr" target="#b1">Bertrand &amp; Moonen, 2013;</ref><ref type="bibr" target="#b23">Milanfar, 2012;</ref><ref type="bibr" target="#b29">Sandryhaila &amp; Moura, 2014;</ref><ref type="bibr" target="#b40">Wu et al., 2016)</ref> which penalizes incoherent (i.e. nonsmooth) signals across adjacent nodes <ref type="bibr" target="#b4">(Chen &amp; Liu, 2017)</ref>.</p><p>Although the effectiveness of GLR has been shown in graph signal processing, the regularizer is manually set simply following the smoothness prior without the involvement of data, whereas the self-supervised task acts as the regularizer learned from unlabeled data under the minor guidance of human prior. Therefore, a properly designed task would introduce data-driven prior knowledge that improves the model generalizability, as show in Table <ref type="table" target="#tab_1">1</ref>.</p><p>In total, multi-task learning is the most general framework among the three. Acting as the data-driven regularizer during training, it makes no assumption on the self-supervised task type. It is also experimentally verified to be the most effective among all the three (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">GCN-Specific Self-Supervised Tasks</head><p>While Section 3.2 discusses the "mechanisms" by which GCNs could be trained with self-supervision, here we expand a "toolkit" of self-supervised tasks for GCNs. We show that, by utilizing the rich node and edge information in a graph, a variety of GCN-specific self-supervised tasks (as summarized in Table <ref type="table" target="#tab_3">3</ref>) could be defined and will be further shown to benefit various types of supervised/downstream tasks. They will assign different pseudo-labels to unlabeled nodes and solve formulation in (4).</p><p>Node clustering. Following M3S <ref type="bibr" target="#b32">(Sun et al., 2019)</ref>, one intuitive way to construct a self-supervised task is via the node clustering algorithm. Given the node set V with the feature matrix X as input, with a preset number of clus- </p><formula xml:id="formula_6">V clu,n = ∅ (n = 1, . . . , K), ∪ K n=1 V clu,n = V, V clu,i ∩ V clu,j = ∅ (∀i, j = 1, ..., K and i = j).</formula><p>With the clusters of node sets, we assign cluster indices as self-supervised labels to all the nodes:</p><formula xml:id="formula_7">y ss,n = k if v n ∈ V clu,k (∀n = 1, . . . , |V|, ∀k = 1, . . . , K).</formula><p>Graph partitioning. Clustering-related algorithms are node feature-based, with the rationale of grouping nodes with similar attributes. Another rationale to group nodes can be based on topology in graph data. In particular two nodes connected by a "strong" edge (with a large weight) are highly likely of the same label class <ref type="bibr" target="#b48">(Zhu &amp; Goldberg, 2009)</ref>. Therefore, we propose a topology-based self-supervision using graph partitioning.</p><p>Graph partitioning is to partition the nodes of a graph into roughly equal subsets, such that the number of edges connecting nodes across subsets is minimized <ref type="bibr" target="#b17">(Karypis &amp; Kumar, 1995)</ref>. Given the node set V, the edge set E and the adjacency matrix A as the input, with a preset number of partitions K ∈ {1, . . . , |V|} (a hyperparameter in our experiments), a graph partitioning algorithm will output a set of node sets {V par,1 , . . . , V par,K |V par,n ⊆ V, n = 1, . . . , K} such that:</p><formula xml:id="formula_8">V par,n = ∅ (∀n = 1, ..., K), ∪ K n=1 V par,n = V, V par,i ∩ V par,j = ∅ (∀i, j = 1, ..., K and i = j),</formula><p>which is similar to the case of node clustering. In addition, balance constraints are enforced for graph partitioning K </p><formula xml:id="formula_9">max k |V par,k | |V| 1 + ,</formula><formula xml:id="formula_10">a ij .</formula><p>With the node set partitioned along with the rest of the graph, we assign partition indices as self-supervised labels:</p><formula xml:id="formula_11">y ss,n = k if v n ∈ V par,k , n = 1, ..., |V|, ∀k = 1, . . . , K.</formula><p>Different from node clustering based on node features, graph partitioning provides the prior regularization based graph topology, which is similar to graph Laplacian regularizer (GLR) <ref type="bibr" target="#b30">(Shuman et al., 2013;</ref><ref type="bibr" target="#b1">Bertrand &amp; Moonen, 2013;</ref><ref type="bibr" target="#b23">Milanfar, 2012;</ref><ref type="bibr" target="#b29">Sandryhaila &amp; Moura, 2014;</ref><ref type="bibr" target="#b40">Wu et al., 2016)</ref> that also adopts the idea of "connection-prompting similarity". However, GLR, which is already injected into the GCNs architecture, locally smooths all nodes with their neighbor nodes. contrast, partitioning considers global smoothness by utilizing all connections to group nodes with heavier connection densities.</p><p>Graph completion. Motivated by image inpainting a.k.a. completion <ref type="bibr" target="#b45">(Yu et al., 2018)</ref> in computer vision (which aims to fill missing pixels of an image), we propose graph completion, a novel regression task, as a self-supervised task. As an analogy to image completion and illustrated in Figure <ref type="figure" target="#fig_1">2</ref>, our graph completion first masks target nodes by removing their features. It then aims at recovering/predicting masked node features by feeding to GCNs unmasked node features (currently restricted to second-order neighbors of each target node for 2-layer GCNs).</p><p>We design such a self-supervised task for the following reasons: 1) the completion labels are free to obtain, which is the node feature itself; and 2) we consider graph completion can aid the network for better feature representation, which teaches the network to extract feature from the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Self-Supervision in Graph Adversarial Defense</head><p>With the three self-supervised tasks introduced for GCNs to gain generalizability toward better-performing supervised learning (for instance, node classification), we proceed to examine their possible roles in gaining robustness against various graph adversarial attacks.</p><p>Adversarial attacks. We focus on single-node direct evasion attacks: a node-specific attack type on the attributes/links of the target node v n under certain constraints following <ref type="bibr" target="#b49">(Zügner et al., 2018)</ref>, whereas the trained model (i.e. the model parameters (θ * , Θ * )) remains unchanged during/after the attack. The attacker g generates perturbed feature and adjacency matrices, X and A , as: Adversarial training for graph data can then be formulated as both supervised learning for labeled nodes and recovering pseudo labels for unlabeled nodes (attacked and clean):</p><formula xml:id="formula_12">X , A = g(X, A, Y , v n , θ * , Θ * ),<label>(5)</label></formula><formula xml:id="formula_13">Z = f θ (X, Â)Θ, Z = f θ (X , A )Θ, θ * , Θ * = arg min θ,Θ L sup (θ, Θ) + α 3 L adv (θ, Θ) ,<label>(6)</label></formula><p>where α 3 is a weight for the adversarial loss L adv (•, •),</p><formula xml:id="formula_14">y pseudo,n = Y pseudo [n, :] T and z n = Z [n, :] T .</formula><p>Adversarial defense with self-supervision. With selfsupervision working in GCNs formulated as in (4) and adversarial training in (6), we formulate adversarial training with self-supervision as:</p><formula xml:id="formula_15">Z = f θ (X, Â)Θ, Z = f θ (X , A )Θ, Z ss = f θ (X ss , A ss ) θ * , Θ * , Θ * ss = arg min θ,Θ,Θss α 1 L sup (θ, Θ) + α 2 L ss (θ, Θ ss ) + α 3 L adv (θ, Θ) ,<label>(7)</label></formula><p>where the self-supervised loss is introduced into training with the perturbed graph data as input (the self-supervised label matrix Y ss is also generated from perturbed inputs).</p><p>It is observed in CNNs that self-supervision improves robustness and uncertainty estimation without requiring larger models or additional data <ref type="bibr" target="#b15">(Hendrycks et al., 2019)</ref>. We thus experimentally explore whether that also extends to GCNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we extensively assess, analyze, and rationalize the impact of self-supervision on transductive semisupervised node classification following <ref type="bibr" target="#b18">(Kipf &amp; Welling, 2016)</ref> on the aspects of: 1) the standard performances of GCN <ref type="bibr" target="#b18">(Kipf &amp; Welling, 2016)</ref> with different self-supervision schemes; 2) the standard performances of multi-task selfsupervision on three popular GNN architectures -GCN, graph attention network (GAT) <ref type="bibr" target="#b35">(Veličković et al., 2017)</ref>, and graph isomorphism network (GIN) <ref type="bibr" target="#b42">(Xu et al., 2018)</ref>; as well as those on two SOTA models for semi-supervised node classification -graph Markov neural network (GMNN) <ref type="bibr" target="#b27">(Qu et al., 2019)</ref> that introduces statistical relational learning <ref type="bibr" target="#b20">(Koller &amp; Pfeffer, 1998;</ref><ref type="bibr" target="#b10">Friedman et al., 1999)</ref> into its architecture to facilitate training and GraphMix <ref type="bibr" target="#b36">(Verma et al., 2019)</ref> that uses the Mixup trick; and 3) the performance of GCN with multi-task self-supervision in adversarial defense. Implementation details can be found in Appendix A. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Self-Supervision Helps Generalizability</head><p>Self-supervision incorporated into GCNs through various schemes. We first examine three schemes (Section 3.2) to incorporate self-supervision into GCN training: pretraining &amp; finetuning, self-training (i.e. M3S <ref type="bibr" target="#b32">(Sun et al., 2019)</ref>) and multi-task learning. The hyper-parameters of M3S are set at default values reported in <ref type="bibr" target="#b32">(Sun et al., 2019)</ref>. The differential effects of the three schemes combined with various self-supervised tasks are summarized for three datasets in Table <ref type="table" target="#tab_8">5</ref>, using the target performances (accuracy in node classification). Each combination of self-supervised scheme and task is run 50 times for each dataset with different random seeds so that the mean and the standard deviation of its performance can be reported.  finetuning provides some performance improvement for the small dataset Cora but does not do so for the larger datasets Citeseer and PubMed. This conclusion remains valid regardless of the choice of the specific self-supervised task. The moderate performance boost echos our previous conjecture: although information about graph structure and features is first learned through self-supervision (L ss as in (3)) in the pretraining stage, such information may be largely lost during finetuning while targeting the target supervised loss alone (L sup as in ( <ref type="formula" target="#formula_2">2</ref>)). The reason for such information loss being particularly observed in GCNs could be that, the shallow GCNs used in the transductive semi-supervised setting can be more easily "overwritten" while switching from one objective function to another in finetuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results in</head><p>Through the remaining two schemes, GCNs with selfsupervision incorporated could see more significant improvements in the target task (node classification) compared to GCN without self-supervision. In contrast to pretraining and finetuning that switches the objective function after selfsupervision in (3) and solves a new optimization problem in (2), both self-training and multi-task learning incorporate self-supervision into GCNs through one optimization problem and both essentially introduce an additional selfsupervision loss to the original formulation in (2).</p><p>Their difference lies in what pseudo-labels are used and how they are generated for unlabeled nodes. In the case of selftraining, the pseudo-labels are the same as the target-task labels and such "virtual" labels are assigned to unlabeled nodes based on their proximity to labeled nodes in graph embedding. In the case of multi-task learning, the pseudolabels are no longer restricted to the target-task labels and can be assigned to all unlabeled nodes by exploiting graph structure and node features without labeled data. And the target supervision and the self-supervision in multi-task learning are still coupled through common graph embedding. So compared to self-training, multi-task learning can be more general (in pseudo-labels) and can exploit more in graph data (through regularization).</p><p>Multi-task self-supervision on SOTAs. Does multi-task self-supervision help SOTA GCNs? Now that we have established multi-task learning as an effective mechanism to incorporate self-supervision into GCNs, we set out to explore the added benefits of various self-supervision tasks to SOTAs through multi-task learning. Table <ref type="table" target="#tab_9">6</ref> shows that different self-supervised tasks could benefit different network architectures on different datasets to different extents. When does multi-task self-supervision help SOTAs and why?</p><p>We note that graph partitioning is generally beneficial to all three SOTAs (network architectures) on all the three datasets, whereas node clustering do not benefit SOTAs on PubMed. As discussed in Section 3.2 and above, multi-task learning introduce self-supervision tasks into the optimization problem in (4) as the data-driven regularization and these tasks represent various priors (see Section 3.3).</p><p>(1) Feature-based node clustering assumes that feature similarity implies target-label similarity and can group distant nodes with similar features together. When the dataset is large and the feature dimension is relatively low (such as PubMed), feature-based clustering could be challenged in providing informative pseudo-labels.</p><p>(2) Topology-based graph partitioning assumes that connections in topology implies similarity in labels, which is safe for the three datasets that are all citation networks. In addition, graph partitioning as a classification task does not impose the assumption overly strong. Therefore, the prior represented by graph partitioning can be general and effective to benefit GCNs (at least for the types of the target task and datasets considered).</p><p>(3) Topology and feature-based graph completion assumes the feature similarity or smoothness in small neighborhoods of graphs. Such a context-based feature representation can greatly improve target performance, especially when the neighborhoods are small (such as Citeseer with the smallest average degree among all three datasets). However, the regression task can be challenged facing denser graphs with larger neighborhoods and more difficult completion tasks (such as the larger and denser PubMed with continuous features to complete). That being said, the potentially informative prior from graph completion can greatly benefit other tasks, which is validated later (Section 4.2).</p><p>Does GNN architecture affect multi-task self-supervision?</p><p>For every GNN architecture/model, all three self-supervised tasks improve its performance for some datasets (except for GMNN on PubMed). The improvements are more significant for GCN, GAT, and GIN. We conjecture that dataregularization through various priors could benefit these three architectures (especially GCN) with weak priors to begin with. In contrast, GMNN sees little improvement with graph completion. GMNN introduces statistical relational learning (SRL) into the architecture to model the dependency between vertices and their neighbors. Considering that graph completion aids context-based representation and acts a somewhat similar role as SRL, the self-supervised and the architecture priors can be similar and their combination may not help. Similarly GraphMix introduces a data augmentation method Mixup into the architecture to refine feature embedding, which again mitigates the power of graph completion with overlapping aims.</p><p>We also report in Appendix B the results in inductive fullysupervised node classification. Self-supervision leads to modest performance improvements in this case, appearing to be more beneficial in semi-supervised or few-shot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Self-Supervision Boosts Adversarial Robustness</head><p>What additional benefits could multi-task self-supervision bring to GCNs, besides improving the generalizability of graph embedding (Section 4.1)? We additionally perform adversarial experiments on GCN with multi-task selfsupervision against Nettack <ref type="bibr" target="#b49">(Zügner et al., 2018)</ref>, to examine its potential benefit on robustness.</p><p>We first generate attacks with the same perturbation intensity (n perturb = 2, see details in Appendix A) as in adversarial training to see the robust generalization. For each selfsupervised task, the hyper-parameters are set at the same values as in Table <ref type="table" target="#tab_9">6</ref>. Each experiment is repeated 5 times as the attack process on test nodes is very time-consuming.</p><p>What self-supervision task helps defend which types of graph attacks and why? In Tables <ref type="table" target="#tab_12">7 and 8</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Result Summary</head><p>We briefly summarize the results as follows.</p><p>First, among three schemes to incorporate self-supervision into GCNs, multi-task learning works as the regularizer and consistently benefits GCNs in generalizable standard performances with proper self-supervised tasks. Pretraining &amp; finetuning switches the objective function from selfsupervision to target supervision loss, which easily "overwrites" shallow GCNs and gets limited performance gain. Self-training is restricted in what pseudo-labels are assigned and what data are used to assign pseudo-labels. And its performance gain is more visible in few-shot learning and can be diminishing with slightly increasing labeling rates.</p><p>Second, through multi-task learning, self-supervised tasks provide informative priors that can benefit GCN in generalizable target performance. Node clustering and graph partitioning provide priors on node features and graph structures, respectively; whereas graph completion with (joint) priors on both help GCN in context-based feature representation. Whether a self-supervision task helps a SOTA GCN in the standard target performance depends on whether the dataset allows for quality pseudo-labels corresponding to the task and whether self-supervised priors complement existing architecture-posed priors.</p><p>Last, multi-task self-supervision in adversarial training improves GCN's robustness against various graph attacks. Node clustering and graph partitioning provides priors on features and links, and thus defends better against feature </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present a systematic study on the standard and adversarial performances of incorporating selfsupervision into graph convolutional networks (GCNs). We first elaborate three mechanisms by which self-supervision is incorporated into GCNs and rationalize their impacts on the standard performance from the perspective of optimization. Then we focus on multi-task learning and design three novel self-supervised learning tasks. And we rationalize their benefits in generalizable standard performances on various datasets from the perspective or data-driven regularization. Lastly, we integrate multi-task self-supervision into graph adversarial training and show their improving robustness of GCNs against adversarial attacks. Our results show that, with properly designed task forms and incorporation mechanisms, self-supervision benefits GCNs in gaining both generalizability and robustness. Our results also provide rational perspectives toward designing such task forms and incorporation tasks given data characteristics, target tasks and neural network architectures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The overall framework for self-supervision on GCN through multi-task learning. The target task and auxiliary self-supervised tasks share the same feature extractor f θ (•, •) with their individual linear transformation parameters Θ, Θss.</figDesc><graphic url="image-3.png" coords="4,343.55,173.18,107.37,68.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Graph completion for a target node. With the targetnode feature masked and neighbors' features and connections provided, GCNs will recover the masking feature based on the neighborhood information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparing performances of GCN through pretraining &amp; finetuning (P&amp;F) and multi-task learning (MTL) with graph partitioning (see Section 3.3) on the PubMed dataset. Reported numbers correspond to classification accuracy in percent.</figDesc><table><row><cell>Pipeline</cell><cell>GCN</cell><cell>P&amp;F</cell><cell>MTL</cell></row><row><cell>Accuracy</cell><cell>79.10 ± 0.21</cell><cell>79.19 ± 0.21</cell><cell>80.00 ± 0.74</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Experiments for GCN through M3S. Gray numbers are from<ref type="bibr" target="#b32">(Sun et al., 2019)</ref>.</figDesc><table><row><cell>Label Rate</cell><cell>0.03%</cell><cell>0.1%</cell><cell>0.3% (Conventional dataset split)</cell></row><row><cell>GCN</cell><cell>51.1</cell><cell>67.5</cell><cell>79.10 ± 0.21</cell></row><row><cell>M3S</cell><cell>59.2</cell><cell>70.6</cell><cell>79.28 ± 0.30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Overview of three self-supervised tasks.</figDesc><table><row><cell>Task</cell><cell>Relied Feature</cell><cell>Primary Assumption</cell><cell>Type</cell></row><row><cell>Clustering</cell><cell>Nodes</cell><cell>Feature Similarity</cell><cell>Classification</cell></row><row><cell>Partitioning</cell><cell>Edges</cell><cell>Connection Density</cell><cell>Classification</cell></row><row><cell>Completion</cell><cell>Nodes &amp; Edges</cell><cell>Context based Representation</cell><cell>Regression</cell></row></table><note>ters K ∈ {1, . . . , |V|} (treated as a hyperparameter in our experiments), the clustering algorithm will output a set of node sets {V clu,1 , ..., V clu,K |V clu,n ⊆ V, n = 1, ..., K} such that:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Dataset statistics. |V|, |V label |, |E|, and N denotes the numbers of nodes, numbers of labeled nodes, numbers of edges, and feature dimension per node, respectively.</figDesc><table><row><cell>Dataset</cell><cell>|V|</cell><cell>|V label |</cell><cell>|E|</cell><cell>N</cell><cell>Classes</cell></row><row><cell>Cora</cell><cell>2,780</cell><cell>140</cell><cell>13,264</cell><cell>1,433</cell><cell>7</cell></row><row><cell>Citeseer</cell><cell>3,327</cell><cell>120</cell><cell>4,732</cell><cell>3,703</cell><cell>6</cell></row><row><cell cols="2">PubMed 19,717</cell><cell>60</cell><cell>108,365</cell><cell>500</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Node classification performances (accuracy; unit: %) when incorporating three self-supervision tasks (Node Clustering, Graph Partitioning, and Graph Completion) into GCNs through various schemes: pretraining &amp; finetuning (abbr. P&amp;T), selftraining M3S<ref type="bibr" target="#b32">(Sun et al., 2019)</ref>), and multi-task learning (abbr. MTL). Red numbers indicate the best two performances with the mean improvement at least 0.8 (where 0.8 is comparable or less than observed standard deviations). In the case of GCN without self-supervision, gray numbers indicate the published results.</figDesc><table><row><cell></cell><cell>Cora</cell><cell>Citeseer</cell><cell>PubMed</cell></row><row><cell>GCN</cell><cell>81.00 ± 0.67 81.5</cell><cell>70.85 ± 0.70 70.3</cell><cell>79.10 ± 0.21 79.0</cell></row><row><cell>P&amp;F-Clu</cell><cell>81.83 ± 0.53</cell><cell>71.06 ± 0.59</cell><cell>79.20 ± 0.22</cell></row><row><cell>P&amp;F-Par</cell><cell>81.42 ± 0.51</cell><cell>70.68 ± 0.81</cell><cell>79.19 ± 0.21</cell></row><row><cell>P&amp;F-Comp</cell><cell>81.25 ± 0.65</cell><cell>71.06 ± 0.55</cell><cell>79.19 ± 0.39</cell></row><row><cell>M3S</cell><cell>81.60 ± 0.51</cell><cell>71.94 ± 0.83</cell><cell>79.28 ± 0.30</cell></row><row><cell>MTL-Clu</cell><cell>81.57 ± 0.59</cell><cell>70.73 ± 0.84</cell><cell>78.79 ± 0.36</cell></row><row><cell>MTL-Par</cell><cell>81.83 ± 0.65</cell><cell>71.34 ± 0.69</cell><cell>80.00 ± 0.74</cell></row><row><cell>MTL-Comp</cell><cell>81.03 ± 0.68</cell><cell>71.66 ± 0.48</cell><cell>79.14 ± 0.28</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Experiments on SOTAs (GCN, GAT, GIN, GMNN, and GraphMix) with multi-task self-supervision. Red numbers indicate the best two performances for each SOTA.</figDesc><table><row><cell>Datasets</cell><cell>Cora</cell><cell>Citeseer</cell><cell>PubMed</cell></row><row><cell>GCN</cell><cell>81.00 ± 0.67</cell><cell>70.85 ± 0.70</cell><cell>79.10 ± 0.21</cell></row><row><cell>GCN+Clu</cell><cell>81.57 ± 0.59</cell><cell>70.73 ± 0.84</cell><cell>78.79 ± 0.36</cell></row><row><cell>GCN+Par</cell><cell>81.83 ± 0.65</cell><cell>71.34 ± 0.69</cell><cell>80.00 ± 0.74</cell></row><row><cell>GCN+Comp</cell><cell>81.03 ± 0.68</cell><cell>71.66 ± 0.48</cell><cell>79.14 ± 0.28</cell></row><row><cell>GAT</cell><cell>77.66 ± 1.08</cell><cell>68.90 ± 1.07</cell><cell>78.05 ± 0.46</cell></row><row><cell>GAT+Clu</cell><cell>79.40 ± 0.73</cell><cell>69.88 ± 1.13</cell><cell>77.80 ± 0.28</cell></row><row><cell>GAT+Par</cell><cell>80.11 ± 0.84</cell><cell>69.76 ± 0.81</cell><cell>80.11 ± 0.34</cell></row><row><cell>GAT+Comp</cell><cell>80.47 ± 1.22</cell><cell>70.62 ± 1.26</cell><cell>77.10 ± 0.67</cell></row><row><cell>GIN</cell><cell>77.27 ± 0.52</cell><cell>68.83 ± 0.40</cell><cell>77.38 ± 0.59</cell></row><row><cell>GIN+Clu</cell><cell>78.43 ± 0.80</cell><cell>68.86 ± 0.91</cell><cell>76.71 ± 0.36</cell></row><row><cell>GIN+Par</cell><cell>81.83 ± 0.58</cell><cell>71.50 ± 0.44</cell><cell>80.28 ± 1.34</cell></row><row><cell>GIN+Comp</cell><cell>76.62 ± 1.17</cell><cell>68.71 ± 1.01</cell><cell>78.70 ± 0.69</cell></row><row><cell>GMNN</cell><cell>83.28 ± 0.81</cell><cell>72.83 ± 0.72</cell><cell>81.34 ± 0.59</cell></row><row><cell>GMNN+Clu</cell><cell>83.49 ± 0.65</cell><cell>73.13 ± 0.72</cell><cell>79.45 ± 0.76</cell></row><row><cell>GMNN+Par</cell><cell>83.51 ± 0.50</cell><cell>73.62 ± 0.65</cell><cell>80.92 ± 0.77</cell></row><row><cell>GMNN+Comp</cell><cell>83.31 ± 0.81</cell><cell>72.93 ± 0.79</cell><cell>81.33 ± 0.59</cell></row><row><cell>GraphMix</cell><cell>83.91 ± 0.63</cell><cell>74.33 ± 0.65</cell><cell>80.68 ± 0.57</cell></row><row><cell>GraphMix+Clu</cell><cell>83.87 ± 0.56</cell><cell>75.16 ± 0.52</cell><cell>79.99 ± 0.82</cell></row><row><cell>GraphMix+Par</cell><cell>84.04 ± 0.57</cell><cell>74.93 ± 0.43</cell><cell>81.36 ± 0.33</cell></row><row><cell>GraphMix+Comp</cell><cell>83.76 ± 0.64</cell><cell>74.43 ± 0.72</cell><cell>80.82 ± 0.54</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Adversarial defense performances on Cora using adversarial training (abbr. AdvT) without or with graph self-supervision. Attacks include those on links, features (abbr. Feats), and both. Red numbers indicate the best two performances in each attack scenario (node classification accuracy; unit: %).</figDesc><table><row><cell>Attacks</cell><cell>None</cell><cell>Links</cell><cell>Feats</cell><cell>Links &amp; Feats</cell></row><row><cell>GCN</cell><cell>80.61 ± 0.21</cell><cell>28.72 ± 0.63</cell><cell>44.06 ± 1.23</cell><cell>8.18 ± 0.27</cell></row><row><cell>AdvT</cell><cell>80.24 ± 0.74</cell><cell>54.58 ± 2.57</cell><cell>75.25 ± 1.26</cell><cell>39.08 ± 3.05</cell></row><row><cell>AdvT+Clu</cell><cell>80.26 ± 0.99</cell><cell>55.54 ± 3.19</cell><cell>76.24 ± 0.99</cell><cell>41.84 ± 3.48</cell></row><row><cell>AdvT+Par</cell><cell>80.42 ± 0.76</cell><cell>56.36 ± 2.57</cell><cell>75.88 ± 0.72</cell><cell>41.57 ± 3.47</cell></row><row><cell>AdvT+Comp</cell><cell>79.64 ± 0.99</cell><cell>59.05 ± 3.29</cell><cell>76.04 ± 0.68</cell><cell>47.14 ± 3.01</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Adversarial defense performances on Citeseer using adversarial training without or with graph self-supervision. and link attacks, respectively. Graph completion, with (joint) perturbation priors on both features and links, boost the robustness consistently and sometimes drastically for the most damaging feature &amp; link attacks.</figDesc><table><row><cell>Attacks</cell><cell>None</cell><cell>Links</cell><cell>Feats</cell><cell>Links &amp; Feats</cell></row><row><cell>GCN</cell><cell>71.05 ± 0.56</cell><cell>13.68 ± 1.09</cell><cell>22.08 ± 0.73</cell><cell>3.08 ± 0.17</cell></row><row><cell>AdvT</cell><cell>69.98 ± 1.03</cell><cell>39.32 ± 2.39</cell><cell>63.12 ± 0.62</cell><cell>26.20 ± 2.09</cell></row><row><cell>AdvT+Clu</cell><cell>70.13 ± 0.81</cell><cell>40.32 ± 1.73</cell><cell>63.67 ± 0.45</cell><cell>27.02 ± 1.29</cell></row><row><cell>AdvT+Par</cell><cell>69.96 ± 0.77</cell><cell>41.05 ± 1.91</cell><cell>64.06 ± 0.24</cell><cell>28.70 ± 1.60</cell></row><row><cell>AdvT+Comp</cell><cell>69.98 ± 0.82</cell><cell>40.42 ± 2.09</cell><cell>63.50 ± 0.31</cell><cell>27.16 ± 1.69</cell></row></table><note>attacks</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank anonymous reviewers for useful comments that help improve the paper during revision. This study was in part supported by the National Institute of General Medical Sciences of the National Institutes of Health [R35GM124952 to Y.S.], and a US Army Research Office Young Investigator Award [W911NF2010240 to Z.W.].</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The rendezvous algorithm: Multiclass semisupervised learning with markov random walks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Azran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
				<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Seeing the bigger picture: How nodes can learn their place within a complex ad hoc network topology</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bertrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="71" to="82" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning from labeled and unlabeled data using graph mincuts</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Carnegie Mellon University</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using randomized mincuts</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Rwebangira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Reddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-first international conference on Machine learning</title>
				<meeting>the twenty-first international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bias-variance tradeoff of graph laplacian regularizer</title>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1118" to="1122" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adversarial robustness: From selfsupervised pre-training to fine-tuning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="699" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02371</idno>
		<title level="m">Adversarial attack on graph structured data</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-task self-supervised visual learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2051" to="2060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="766" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning probabilistic relational models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pfeffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1300" to="1309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semi-supervised learning-a statistical physics approach</title>
		<author>
			<persName><forename type="first">G</forename><surname>Getz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shental</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Domany</surname></persName>
		</author>
		<idno>arXiv preprint cs/0604011</idno>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Scaling and benchmarking self-supervised visual representation learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.01235</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Using self-supervised learning can improve model robustness and uncertainty</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15637" to="15648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Explainable deep relational networks for predicting compound-protein affinities and contacts</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12553</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multilevel graph partitioning schemes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPP</title>
				<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="113" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Revisiting selfsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09005</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Probabilistic frame-based systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pfeffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI/IAAI</title>
				<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995. 1995</date>
			<biblScope unit="volume">3361</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A tour of modern image filtering: New insights and methods, both practical and theoretical. IEEE signal processing magazine</title>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="106" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Selfsupervised learning for generalizable out-of-distribution detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mohseni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pitale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yadawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><surname>Gmnn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06214</idno>
		<title level="m">Graph markov neural networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cross-domain self-supervised multitask feature learning using synthetic imagery</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="762" to="771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Big data analysis with signal processing on graphs: Representation and processing of massive data sets with irregular structure</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sandryhaila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="80" to="90" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">When does selfsupervision improve few-shot learning?</title>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03560</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Multi-stage self-supervised learning for graph convolutional networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.11038</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Partially labeled classification with markov random walks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="945" to="952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Selfsupervised pretraining for image embedding</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Selfie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02940</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Graphmix: Regularized training of graph neural networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11715</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03679</idno>
		<title level="m">Adversarial defense framework for graph neural network</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><surname>Graphdefense</surname></persName>
		</author>
		<title level="m">Towards robust graph convolutional networks</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Adversarial examples on graph data: Deep insights into attack and defense</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tyshetskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Docherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Estimating the trace of the matrix inverse by interpolating from the diagonal of an approximate inverse</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laeuchli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kalantzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stathopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gallopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">326</biblScope>
			<biblScope unit="page" from="828" to="844" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
		<title level="m">A comprehensive survey on graph neural networks</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks?</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">L2-gcn: Layerwise and learned efficient training of graph convolutional networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2127" to="2135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Generative image inpainting with contextual attention</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5505" to="5514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Towards semi-supervised classification with Markov random fields</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Introduction to semisupervised learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis lectures on artificial intelligence and machine learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="130" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Adversarial attacks on neural networks for graph data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akbarnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2847" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
