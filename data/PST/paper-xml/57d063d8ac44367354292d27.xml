<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">High-Frequency Trading Strategy Based on Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Andrés</forename><surname>Arévalo</surname></persName>
							<email>ararevalom@unal.edu.co</email>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Nacional de Colombia</orgName>
								<address>
									<settlement>Bogotá</settlement>
									<country key="CO">Colombia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jaime</forename><surname>Niño</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Nacional de Colombia</orgName>
								<address>
									<settlement>Bogotá</settlement>
									<country key="CO">Colombia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">German</forename><surname>Hernández</surname></persName>
							<email>gjhernandezp@unal.edu.co</email>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Nacional de Colombia</orgName>
								<address>
									<settlement>Bogotá</settlement>
									<country key="CO">Colombia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Javier</forename><surname>Sandoval</surname></persName>
							<email>javier.sandoval@uexternado.edu.co</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Algocodex Research Institute</orgName>
								<orgName type="institution" key="instit2">Universidad Externado</orgName>
								<address>
									<settlement>Bogotá</settlement>
									<country key="CO">Colombia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">High-Frequency Trading Strategy Based on Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E24FC8C5066F6D553B8340BEAB227E09</idno>
					<idno type="DOI">10.1007/978-3-319-42297-8_40</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Computational finance</term>
					<term>High-frequency trading</term>
					<term>Deep neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a high-frequency strategy based on Deep Neural Networks (DNNs). The DNN was trained on current time (hour and minute), and n-lagged one-minute pseudo-returns, price standard deviations and trend indicators in order to forecast the next one-minute average price. The DNN predictions are used to build a high-frequency trading strategy that buys (sells) when the next predicted average price is above (below) the last closing price. The data used for training and testing are the AAPL tick-by-tick transactions from September to November of 2008. The best-found DNN has a 66 % of directional accuracy. This strategy yields an 81 % successful trades during testing period.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Financial Markets modelling has caught a lot of attention during the recent years due to the growth of financial markets and the large number of investors around the world in pursuit of profits. However, modelling and predicting prices of Financial Assets is not an easy work, due to the complexity and chaotic dynamics of the markets, and the many non-decidable, non-stationary stochastic variables involved <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">9]</ref>. Many researchers from different areas have studied historical patterns of financial time series and they have proposed various models to predict the next value of time series with a limited precision and accuracy <ref type="bibr" target="#b6">[8]</ref>.</p><p>Since late 1980s, neural networks are a popular theme in data analysis. Artificial Neural Networks (ANNs) are inspired by brain structure; they are composed of many neurons that have connections with each other. Each neuron is a processor unit that performs a weighted aggregation of multiple input signals, and propagates a new output signal depending on its internal configuration. ANNs have the ability to extract essential features and learn complex information patterns in high dimensional spaces. Those features have proven useful for forecasting financial time series. Although neural network models have existed for long time and they have been used in many disciplines, only since early 1990s they are used in the field of finance <ref type="bibr" target="#b5">[7]</ref>; The first known application for forecasting financial time series was described in <ref type="bibr" target="#b14">[16]</ref>.</p><p>A Deep Neural Network (DNN) is an ANN with multiple hidden layers between the input and the output layer, such that data inputs are transformed from low-level to high-level features. The input layer is characterized by having many inputs. At each hidden layer, the data are encoded in features of less dimensions by non-linear transformations; then, the next layers refine the learned patterns in high-level features of less dimensions, and so on until it is capable of learning complex patterns, which are of interest in this work. This type of neural networks can learn high-level abstractions from large quantities raw data through intermediate processing and refinement that occurs in each hidden layer <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">14]</ref>.</p><p>Traditionally, neural networks are trained with the back-propagation algorithm, which consists in initializing the weights matrices of the model with random values. Then the error between network output and desired output is evaluated. In order to identify the neurons that contributed to the error, the error is propagated backwards from the output layer to all neurons in the hidden layers that contributed to it. This process is repeated layer by layer, until all neurons in the network have received an error signal describing their relative contribution to the total error. Later, the weights are updated in order to try to reduce the error. Then the error is calculated again and this process is repeated until a tolerable error or maximal number of iterations is reached <ref type="bibr" target="#b11">[13]</ref>.</p><p>A serious problem of back-propagation algorithm is that the error is diluted exponentially as it passes through hidden layers on their way to the network beginning. In a DNN that has many hidden layers, only the last layers are trained, while the first ones have barely changed. Although DNNs exist long time ago, they were useless because the challenge of training networks with many layers had remained unsolved. This challenge was solved in 2006 by <ref type="bibr" target="#b4">[5]</ref>, who successfully included paradigms of Deep Learning in Computer Science.</p><p>In recent years Deep Learning (DL) has emerged as a very robust machine learning technique, improving limitations of ANN. Models based on DL have begun to arouse the interest of the general public, because they are able to learn useful representations from raw data and they have shown high performance in complex data, such as text, images and even video. However, applications of DNNs in computational finance are limited <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b17">19]</ref>.</p><p>The paper is organized as follows: Sect. 2 presents some important definitions of key concepts in this work. Section 3 describes the dataset used for the experiment. Section 4 presents the DNN modelling for forecasting the next one-minute average price of Apple, Inc. within financial crisis of 2008, when a high volatility behaviour was evidenced. Section 5 describes the proposed trading strategy algorithm. Section 6 presents the strategy performance. Moreover, Sect. 7 presents some conclusions and recommendations for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Definitions</head><p>Bellow some important definitions are presented: Definition 1. Log-return. It is a term commonly used in finance. Let p t be the current trade or close price and p tÀ1 the previous trade or close price.</p><formula xml:id="formula_0">R ¼ ln p t p tÀ1 ¼ ln p t ð Þ À ln p tÀ1 ð Þ<label>ð1Þ</label></formula><p>From a log-return R, the original price p t can be reconstructed easily:</p><formula xml:id="formula_1">p t ¼ p tÀ1 e R<label>ð2Þ</label></formula><p>Definition 2. Pseudo-log-return. It is defined as a logarithmic difference (log of quotient) of between average prices on consecutive minutes. On the other hand, the typical log-return is a logarithmic difference of between closing prices on consecutive minutes. A small value, close to zero, means that in the next minute, the price is going to remain stable. A positive value means that the price is going to rise. A negative value means that the price is going to fall. Change is proportional to distance value compared to zero; if distance is too high, the price will rise or fall sharply.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset Description</head><p>From the TAQ database of the NYSE [6], all trade prices for Apple ordinary stock (ticker: AAPL) were downloaded from the September 2 nd to November 7 th of the year 2008. Figure <ref type="figure" target="#fig_1">1</ref> shows price behaviour in the selected period.</p><p>The selected period covers stock crash due to the financial crisis of 2008. During this crash, the AAPL price suffered a dramatic fall from 172 to 98 dollars. This period was chosen intentionally to demonstrate the performance of proposed strategy under high volatility conditions. During a financial crisis, market behaviour is strongly impacted by external factors to the system, such as news, rumours, anxiety of traders, among others. If a DNN can identify and learn patterns under these difficult conditions, it can yield equal or even better with other time series without financial crisis.</p><p>As it is shown on Fig. <ref type="figure" target="#fig_2">2</ref>, the distribution of Tick-by-Tick log-returns is some symmetric with mean À3:802453e À8 , zero in practical terms. The dataset is composed by 14,839,394 observations, has a maximum value on 0.09429531 and a minimum one on -0.09433554.</p><p>Reviewed literature suggests that any stock log-returns follows approximately a normal distribution with mean zero <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b15">17]</ref>. For this reason, the best variables that describe the behaviour of the market within a minute are mean price and standard deviation of prices.</p><p>First, the data consistency was verified. All dates were in working days (not holidays and not weekends). All times were during normal hours of trading operation (between 9:30:00 am and 3:59:59 pm EST). All prices and volumes were positive. Therefore, it was not necessary to delete records.</p><p>All data are summarized with a one-minute detailed level. Three time series were constructed from trading prices: Average Price, Standard Deviation of Prices and Trend Indicator. Each series has 19110 records (49 trading days Â 390 min per day).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Deep Neural Network Modelling 4.1 Features Selection</head><p>In total four inputs-groups were chosen: Current Time, last n pseudo-log-returns, last n standard deviations of prices and last n trend indicators, where n is the window size. The current time group is composed of two inputs: Current hour and current minute. The others groups are composed of n inputs for each one. In total the number of DNN inputs I is 3n þ 2. The following paragraphs describe each input group: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Current Time:</head><p>The literature reviewed did not include time as an input. However, the hour and minute as integer values were chosen as two additional inputs, due to financial markets are affected by regimes that occurs repeatedly in certain minutes or hours during the trading day. This behaviour may be explained by the fact that both human and automatized (machines) traders have strategies that are running in synchronized periods.</p><p>To illustrate this affirmation, Fig. <ref type="figure" target="#fig_0">3</ref> shows the price variations that occurred at the first, third and sixth day. Approximately, in the minute 170, the stock was traded at the same opening price of corresponding day. Approximately, in the minute 250, the stock price fell 3 dollars relative to the opening price in these days. As these patterns, many more are repeated at certain time of day. In order to identify and to differentiate better these patterns, the current hour and current minute of day were added as additional inputs of DNN. These variables have 7 and 60 possible values ranging from 0 to 6 and from 0 to 59 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Last n pseudo-log-returns:</head><p>It is common to see works of neural networks used to forecast time series whose inputs are composed principally by the last untransformed observations. This is fine for several types of time series, but it is not advisable in financial time series forecasting. In any dataset and particularly in the one used in this work, if the nominal prices are used, it will be useless because a neural network will train with special conditions (prices fluctuates between 120 and 170 dollars) and then it will be tested against different market conditions (prices fluctuates between 90 and 120 dollars).</p><p>In other words, the neural network learns to identify many static patterns that will be not appearing at all. For example, a pattern like when the price is over 150 dollars, raises to 150.25 dollars and falls to 149.75 dollars, then it will change to 150.50 dollars, could be found, but this pattern never will occur because in the closest future the prices fluctuates between 90 and 120 dollars. However, if prices are transformed into differences or logarithmic returns, not only the data variance is stabilized, but also the time series acquire temporal independence. For example at the beginning of the selected period, a pattern, like when the price rises 25 cents and it falls 50 cents, then it will raise 75 cents, could be found this pattern is more likely to occur in the future. Therefore, the last n one-minute pseudo-log-returns are inputs of DNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Last n standard deviations of prices:</head><p>The last n one-minute standard deviations of prices are DNN inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Last n trend indicators:</head><p>The last n one-minute trend indicators are DNN inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Output Selection of the Deep Neural Network</head><p>The DNN forecasts the next one-minute pseudo-log-return. As it is shown on Fig. <ref type="figure" target="#fig_4">4</ref>, the average price (black line) is the variable that best describes market behaviour. The highest or lowest prices (blue lines) usually are found within a confidence range of average price, therefore the next highest and lowest prices can be estimated from a predicted average price. The closing price (red line) can be any value close to the average price; it sometimes coincides with the highest or lowest price. Unlike the average price, the highest, lowest and closing ones are exposed largely to noise or external market dynamics, for example, some traders listen a false rumour about bad news that will cause a sudden fall in the price, in order to reduce losses. As a result, they decide to sell at a lower price than the one traded before. This operation could be done at a certain second and it could affect numerically the highest, lowest or closing prices on the minute.</p><p>Since the objective of this work is to learn the dynamics of the market to take advantage of it eventually, the average price forecasts could be more profitable than the closing price forecast. With a good average price forecast, it is known that the stock is going to trade to that predicted value at any moment within the next minute. A real automated trading strategy should wait until the right time (for example, stock price reaches to price forecast) to open or to close their positions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Deep Neural Network Architecture</head><p>The architecture was arbitrarily. It has one input layer, five hidden layers and one output layer. The number of neurons in each layer depends on the number of inputs I. Each layer has I, I, b4I=5c, b3I=5c, b2I=5c, bI=5c and 1 neurons respectively. All neurons use a tanh activation function except the output neuron that uses a linear activation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Deep Neural Network Training</head><p>The final dataset is made up of 19109 À n records. Each record contains 3n þ 3 numerical values (3n þ 2 inputs and 1 output). It should be noted that to construct each record, only information from the past is required. Therefore, there is not look-ahead bias and this DNN could be used for a real trading strategy.</p><p>As shown on Fig. <ref type="figure" target="#fig_1">1</ref>, the dataset has two regimes: one bearish regime (first 50 % samples) and a no-trending one (last 50 % samples). The final dataset was divided into two parts: In-sample data (first 85 % samples in bearish regime and first 85 % samples in no-trending regime) and out-sample data (last 15 % samples in bearish regime and last 15 % samples in no-trending regime). The Fig. <ref type="figure" target="#fig_5">5</ref> shows the splitting.</p><p>For this work, H 2 O, an open-source software for big-data analysis <ref type="bibr" target="#b10">[12]</ref>, was used. It implements algorithms at scale, such as deep learning <ref type="bibr" target="#b0">[1]</ref>, as well as featuring automatic versions of advanced optimization for DNN training. Additionally, it implements an adaptive learning rate algorithm, called ADADELTA <ref type="bibr" target="#b0">[1]</ref>, which is described in <ref type="bibr" target="#b18">[20]</ref>. It was chosen in order to improve the learning process, due:</p><p>• It is a per-dimension adaptive learning rate method for gradient descent.</p><p>• It is not necessary to search parameters for gradient descent manually.</p><p>• It is robust to large gradients and noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Deep Neural Network Assessment</head><p>In order to assess the DNN performance, two statistics were chosen. Let E as the expected series and F as the series forecast: • Mean Squared Error:</p><formula xml:id="formula_2">MSE ¼ 1 P n t¼1 t À F t ð Þ 2</formula><p>• Directional Accuracy: Percent of predicted directions that matches with the ideal differences time series. This measure is unaffected by outliers or variables scales.</p><formula xml:id="formula_3">DA ¼ 100 n P n t¼1 E t Á F t [ 0 ð Þ .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Proposed Strategy</head><p>The DNN predictions are used by the following high-frequency trading strategy: For each trading minute, it always buys (sells) a stock when the next predicted average price is above (below) the last closing price. When the price yields the predicted average price, it sells (buys) the stock in order to ensure the profit. If the price never yields the expected price, it sells (buys) the stock with the closing price of the minute, in order to close the position and potentially stop losing positions. Figure <ref type="figure" target="#fig_6">6</ref> shows the strategy flowchart. Below the algorithm is formally presented in pseudo-code: The DNN was trained only with the in-sample data during 50 epochs. The chosen ADADELTA parameters were q ¼ 0:9999 and 2¼ 10 À10 . On the other hand, the DNN was tested only with the out-sample date. Table <ref type="table" target="#tab_0">1</ref> illustrates DNN performance using different windows sizes and the same network architecture. Ten different networks were trained for each parameter. The best results are obtained with small window sizes such as three, four and five. Overall, the networks achieved between 63 % and 66 % directional accuracy. Depending on training results, DNN performance may be better, but all networks converge with very similar and homogeneous results. The DNN is able to predict these sudden rises or falls in price. This information may be useful for any trading strategy.</p><p>Figures 7, 8 and 9 show the strategy performance during a trading simulation over the testing data. The simulation did not consider transaction costs and it was performed with the best-found DNN (66.15492 % of DA). Buying and selling only one stock, the strategy accumulated 72.3036 dollars at the end of the period. It made 2333 successful trades and 520 unsuccessful ones, approximately 81.77 % successful trades.</p><p>During the training data period (from 0 % to 42.5 % and from 50 % to 92.5 % in the time series), the strategy did not perform trades, and then it did not yields profits and losses on those minutes. For this reason, Figs. 8 and 9 have a horizontal line during these periods.  Although the strategy turns out to be interesting and yields a good performance, it must be refined in order to implement in a real environment, for example, it could analyse whether it closes its position in the next minute or it keeps it open in order to decrease transaction costs.</p><p>Traders collectively repeat the behaviour of the traders that preceded them <ref type="bibr" target="#b9">[11]</ref>. Those patterns can be learned by a DNN. The proposed strategy replicates the concept of predicting prices for short periods. Furthermore, adding time as a DNN input allows it to differentiate atypical events and repetitive patterns in market dynamics. Moreover, small data windows sizes are able to explain future prices in a simpler way.</p><p>Overall, the DNNs can learn the market dynamic with a reasonable precision and accuracy. Within the deep learning arena, the DNN is the simplest model, as a result, a possible research opportunity could be to evaluate the performance of the strategy using other DL model such as Deep Recurrent Neural Networks, Deep Belief Networks, Convolutional Deep Belief Networks, Deep Coding Networks, among others.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Definition 3 .</head><label>3</label><figDesc>Trend Indicator. It is a new statistical indicator created for this work. All trades within each minute are taken, then a linear model (y ¼ ax þ b) is fitted. The Trend indicator is equal to the parameter a.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Apple stock price.</figDesc><graphic coords="3,42.80,384.73,339.60,123.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Distribution of Tick-by-Tick log-returns.</figDesc><graphic coords="4,57.47,197.10,338.65,165.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The price variations that occurred at the first (blue line), third (red line) and sixth (green line) day. (Color figure online)</figDesc><graphic coords="5,42.80,167.81,339.60,74.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. First 60 one-minute Apple Stock Prices at September 2nd, 2008. Blue: High and Low. Red: Close. Black: Average. (Color figure online)</figDesc><graphic coords="6,56.97,357.47,339.60,124.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Data splitting.</figDesc><graphic coords="7,66.59,306.43,292.08,50.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Strategy flowchart.</figDesc><graphic coords="8,56.97,300.47,339.60,234.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig. 7. Profit histogram of the trading strategy</figDesc><graphic coords="11,42.80,185.39,339.60,79.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Cumulated profit of the trading strategy</figDesc><graphic coords="11,42.80,433.70,339.60,84.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>DNN performance.</figDesc><table><row><cell>Size Window</cell><cell>DNN Architecture</cell><cell>Maximum</cell><cell>Minimum</cell><cell>Mean</cell></row><row><cell>2</cell><cell>8 8:6:4:3:1 1</cell><cell cols="3">0.07832 65.71328 0.06768 61.63236 0.07042 64.47506 0.00294 1.30354</cell></row><row><cell>3</cell><cell>11 11:8:6:4:2 1</cell><cell cols="3">0.07678 66.15492 0.06823 63.67759 0.07125 65.17794 0.00233 0.71620</cell></row><row><cell>4</cell><cell>14 14:11:8:5:2 1</cell><cell cols="3">0.09158 65.71328 0.07197 63.30659 0.07576 65.07847 0.00579 0.70564</cell></row><row><cell>5</cell><cell>17 17:13:10:6:3 1</cell><cell cols="3">0.10132 66.05024 0.07569 64.30565 0.08561 64.83949 0.00729 0.52326</cell></row><row><cell>6</cell><cell>20 20:16:12:7:3 1</cell><cell cols="3">0.10512 65.74816 0.07574 62.99267 0.08514 64.91105 0.00874 0.83872</cell></row><row><cell>7</cell><cell>23 23:18:13:9:4 1</cell><cell cols="3">0.10383 65.63154 0.08251 63.22400 0.08929 64.43475 0.00634 0.73761</cell></row><row><cell>8</cell><cell cols="4">26 26:20:15:10:5 1 0.09873 65.60865 0.07813 63.41123 0.08754 64.40879 0.00648 0.68195</cell></row><row><cell>9</cell><cell cols="4">29 29:23:17:11:5 1 0.09020 65.49197 0.07628 63.78227 0.08437 64.59874 0.00475 0.45499</cell></row><row><cell>10</cell><cell cols="4">32 32:25:19:12:6 1 0.10250 65.50401 0.07400 63.09731 0.08476 64.55877 0.00829 0.76088</cell></row><row><cell>11</cell><cell cols="4">35 35:28:21:13:6 1 0.10565 65.24773 0.07702 62.90997 0.08537 64.25680 0.00892 0.83953</cell></row><row><cell>12</cell><cell cols="4">38 38:30:22:15:7 1 0.09440 65.32961 0.07746 63.69026 0.08698 64.46110 0.00562 0.62078</cell></row><row><cell>13</cell><cell cols="4">41 41:32:24:16:8 1 0.09442 64.61967 0.07437 61.89811 0.08491 63.79972 0.00618 0.96908</cell></row><row><cell>14</cell><cell cols="4">44 44:35:26:17:8 1 0.09833 65.32961 0.07849 62.39972 0.08781 64.25531 0.00635 0.94794</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>High-Frequency Trading Strategy Based on Deep Neural Networks</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>A. Arévalo et al.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="47" xml:id="foot_2"><p>47:37:28:18:9 1 0.09871 64.86392 0.08201 61.86322 0.08916 63.64270 0.005311.13385   </p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deep Learning with H</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning deep architectures for AI</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends® Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">25 years of time series forecasting</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>De Gooijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Hyndman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Forecast</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="443" to="473" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Härdle</surname></persName>
		</author>
		<title level="m">Applied Quantitative Finance: Theory and Computational Tools</title>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Designing a neural network for forecasting financial and economic time series</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kaastra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="215" to="236" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Enhancing quantitative intra-day stock return prediction by integrating both market news and stock prices information</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">142</biblScope>
			<biblScope unit="page" from="228" to="238" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Modeling and forecasting financial time series with ordered fuzzy candlesticks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Burczyński</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci. (Ny)</title>
		<imprint>
			<biblScope unit="volume">273</biblScope>
			<biblScope unit="page" from="144" to="155" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The econometric modelling of financial time series</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Markellos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Murphy</surname></persName>
		</author>
		<title level="m">Technical Analysis of the Financial Markets: A Comprehensive Guide to Trading Methods and Applications</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Penguin</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Nusca</surname></persName>
		</author>
		<title level="m">Arno Candel, physicist and hacker, 0xdata. Meet Fortune&apos;s 2014 Big Data All-Stars</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A direct adaptive method for faster backpropagation learning: the RPROP algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Braun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="586" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Applying Deep Learning to Enhance Momentum Trading Strategies in Stocks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Takeuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>cs229.stanford.edu</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Trippi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Turban</surname></persName>
		</author>
		<title level="m">Neural Networks in Finance and Investing: Using Artificial Intelligence to Improve Real World Performance</title>
		<meeting><address><addrLine>Chicago</addrLine></address></meeting>
		<imprint>
			<publisher>Probus Publishing Company</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Tsay</surname></persName>
		</author>
		<title level="m">Analysis of Financial Time Series</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Yeh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Corporate Default Prediction via Deep Learning</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
	</analytic>
	<monogr>
		<title level="m">PRICAI 2014</title>
		<editor>
			<persName><forename type="first">D.-N</forename><surname>Pham</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S.-B</forename><surname>Park</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8862</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">ADADELTA: An Adaptive Learning Rate Method</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
