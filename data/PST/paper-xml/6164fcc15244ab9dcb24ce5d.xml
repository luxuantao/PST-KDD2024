<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KG-FiD: Infusing Knowledge Graph in Fusion-in-Decoder for Open-Domain Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Donghan</forename><surname>Yu</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
							<email>chezhu@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Yuwei</forename><surname>Fang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Notre Dame</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yichong</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Southern California 1</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Carnegie</forename><forename type="middle">Mellon</forename><surname>University</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Microsoft</forename><forename type="middle">Cognitive</forename><surname>Services</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Research</forename><surname>Group</surname></persName>
						</author>
						<title level="a" type="main">KG-FiD: Infusing Knowledge Graph in Fusion-in-Decoder for Open-Domain Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current Open-Domain Question Answering (ODQA) models typically include a retrieving module and a reading module, where the retriever selects potentially relevant passages from open-source documents for a given question, and the reader produces an answer based on the retrieved passages. The recently proposed Fusion-in-Decoder (FiD) framework is a representative example, which is built on top of a dense passage retriever and a generative reader, achieving the state-of-the-art performance. In this paper we further improve the FiD approach by introducing a knowledgeenhanced version, namely KG-FiD. Our new model uses a knowledge graph to establish the structural relationship among the retrieved passages, and a graph neural network (GNN) to re-rank the passages and select only a top few for further processing. Our experiments on common ODQA benchmark datasets (Natural Questions and TriviaQA) demonstrate that KG-FiD can achieve comparable or better performance in answer prediction than FiD, with less than 40% of the computation cost.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Open-Domain Question Answering (ODQA) is the task of answering natural language questions in open domains. A successful ODQA model relies on effective acquisition of world knowledge. A popular line of work treats a large collection of open-domain documents (such as Wikipedia articles) as the knowledge source, and design a ODQA system that consists of a retrieving module and a reading module. The retriever pulls out a small set of potentially relevant passages from the opensource documents for a given question, and the reader produces an answer based on the retrieved passages <ref type="bibr" target="#b11">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b5">Guu et al., 2020;</ref><ref type="bibr" target="#b7">Izacard and Grave, 2020)</ref>. An earlier example of this kind is DrQA <ref type="bibr" target="#b3">(Chen et al., 2017)</ref>, which used an traditional search engine based on the bag of words (BoW) document representation with TF-IDF term weighting, and a neural reader for extracting candidate answers for each query based on the dense embedding of the retrieved passages. With the successful development of Pre-trained Language Models (PLMs) in neural network research, dense embedding based passage retrieval (DPR) models <ref type="bibr" target="#b11">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b19">Qu et al., 2021)</ref> have shown superior performance over BoW/TF-IDF based retrieval models due to utilization of contextualized word embedding in DPR, and generative QA readers <ref type="bibr">(Lewis et al., 2020;</ref><ref type="bibr" target="#b22">Roberts et al., 2020</ref>) usually outperform extraction based readers <ref type="bibr" target="#b4">(Devlin et al., 2019;</ref><ref type="bibr" target="#b5">Guu et al., 2020)</ref> due to the capability of the former in capturing lexical variants with a richer flexibility.</p><p>The recently proposed Fusion-in-Decoder (FiD) model <ref type="bibr" target="#b8">(Izacard and Grave, 2021)</ref> is representative of those methods with a DPR retriever and a generative reader, achieving the state-of-the-art results on ODQA evaluation benchmarks. FiD also significantly improved the scalability of the system over previous generative methods by encoding the retrieved passages independently instead of encoding the concatenation of all retrieved passages (which was typical in previous methods).</p><p>Inspired by the success of FiD, this paper aims further improvements of the state of the art of ODQA in the paradigm with a DPR retriever and a generative reader. Specifically, we point out two potential weaknesses or limitations of FiD as the rooms for improvements, and we propose a novel solution namely KG-FiD to address these issues with FiD. The two issues are:</p><p>Issue 1. The independent assumption among passages is not justified. Notice that both the DPR retriever and the generative reader of FiD perform independent encoding of the retrieved passages, which means that they cannot leverage the semantic relationship among passages for passage embedding and answer generation even if such relational knowledge is available. But we know that rich semantic connections between passages often provide clues for better answering questions <ref type="bibr" target="#b16">(Min et al., 2019)</ref>.</p><p>Issue 2. Efficiency Bottleneck. For each input question, the FiD generative reader receives about 100 passages from the DPR module, with a relatively high computational cost. For example, the inference per question takes more than 6 trillion floating-point operations. Simply reducing the number of retrieved passages sent to the reader will not be a good solution as it will significantly decrease the model performance <ref type="bibr" target="#b8">(Izacard and Grave, 2021)</ref>. How to overcome such inefficient computation issue is a challenging question for the success of FiD in realistic ODQA settings.</p><p>We propose to address both of the above issues with FiD by leveraging an existing knowledge graph (KG) to establish relational dependencies among retrieved passages, and employing Graph Neural Networks (GNNs) to re-rank and prune retrieved passages for each query. We name our new approach as KG-FiD. Specifically, KG-FiD employs a two-stage passage reranking by applying GNN to model structural and semantic information of passages. Both stages rerank the input passages and only a few top-reranked passages are fed into subsequent modules. The first stage reranks passages returned by the retriever, where we use the passage embeddings generated by DPR as the initial GNN node representation. This allows reranking a much larger set of initial candidate passages to enhance coverage of answers. The second stage performs joint passage reranking and answer generation, where the node embeddings are initialized by the embeddings of passage-question pairs output from the reader encoder. This stage operates on a smaller candidate set but aims for more accurate reranking and passage pruning.</p><p>To improve the efficiency, in the second-stage reranking, our GNN model adopts representations from the intermediate layer in the reader encoder instead of the final layer to initiate passage node embeddings. Then only a few top reranked passages will be passed into the higher layers of encoder and the decoder for answer generation, while other passages will not be further processed. This is coupled with a joint training of passage reranking and answer generation. As shown in Section 4.3, these strategies significantly reduce the computation cost while still maintaining a good QA performance.</p><p>Our experiments on ODQA benchmark datasets Natural Questions and TriviaQA demonstrate that KG-FiD can achieve comparable or better performance in answer prediction than FiD, with only 40% of the computation cost of FiD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>ODQA with text corpus ODQA usually assumes that a large external knowledge source is accessible and can be leveraged to help answer prediction. For example, previous works <ref type="bibr" target="#b3">(Chen et al., 2017;</ref><ref type="bibr" target="#b11">Karpukhin et al., 2020;</ref><ref type="bibr" target="#b8">Izacard and Grave, 2021)</ref> mainly use Wikipedia as knowledge source which contains millions of text passages. In this case, current ODQA models mainly contains a retriever to select related passages and a reader to generate the answer. Thus, the follow-up works mainly aim to: (1) Improve the retriever: from sparse retrieval based on TF-IDF or BM25 <ref type="bibr" target="#b3">(Chen et al., 2017;</ref><ref type="bibr">Yang et al., 2019)</ref> to dense retrieval <ref type="bibr" target="#b11">(Karpukhin et al., 2020)</ref> based on contextualized embeddings generated by pre-trained language models (PLMs). Moreover, some further improvement are also proposed such as better training strategy <ref type="bibr" target="#b19">(Qu et al., 2021)</ref>, reranking based on retrieved passages <ref type="bibr" target="#b29">(Wang et al., 2018;</ref><ref type="bibr" target="#b17">Nogueira and Cho, 2019;</ref><ref type="bibr" target="#b15">Mao et al., 2021)</ref>, and knowledge distillation from reader to retriever <ref type="bibr" target="#b7">(Izacard and Grave, 2020)</ref>; (2) Improve the reader: changing from Recurrent Neural Network <ref type="bibr" target="#b3">(Chen et al., 2017)</ref> to PLMs such as extractive reader BERT <ref type="bibr" target="#b11">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b6">Iyer et al., 2021;</ref><ref type="bibr" target="#b5">Guu et al., 2020)</ref> and generative reader BART and T5 <ref type="bibr" target="#b8">(Izacard and Grave, 2021;</ref><ref type="bibr">Lewis et al., 2020)</ref>. Besides, some works <ref type="bibr" target="#b5">(Guu et al., 2020;</ref><ref type="bibr">Lewis et al., 2020;</ref><ref type="bibr" target="#b23">Sachan et al., 2021)</ref> have shown that additional unsupervised pre-training on retrieval-related language modeling tasks can further improve ODQA performance. However, none of these methods modeled the relationships among different passages.</p><p>ODQA with knowledge graph Besides the unstructured text corpus, world knowledge also exists in knowledge graphs (KGs), which represent entities and relations in a structural way and have been used in a variety of NLP tasks <ref type="bibr">(Xu et al., 2021b;</ref><ref type="bibr">Yu et al., 2020;</ref><ref type="bibr">Xu et al., 2021a)</ref>. Some works <ref type="bibr" target="#b1">(Berant et al., 2013;</ref><ref type="bibr" target="#b25">Sun et al., 2018</ref><ref type="bibr" target="#b24">Sun et al., , 2019;;</ref><ref type="bibr">Xiong et al., 2019</ref>) restrict the answer to be entities in the knowledge graph, while our work focus on more general ODQA setting where the answer can be any words or phrases. Under this setting, some recent efforts have been made to leverage knowledge graphs for ODQA <ref type="bibr" target="#b16">(Min et al., 2019;</ref><ref type="bibr" target="#b0">Asai et al., 2020;</ref><ref type="bibr">Zhou et al., 2020)</ref>. For example, UniK-QA <ref type="bibr" target="#b18">(Oguz et al., 2020)</ref> transforms KG triplets into text sentences and combine them into text corpus, which loses structure information of KG. Other works use KG to build relationship among passages similar to ours. <ref type="bibr">KAQA (Zhou et al., 2020)</ref> use passage graph to propagate passage retrieve scores and answer span scores. Graph-Retriever <ref type="bibr" target="#b16">(Min et al., 2019)</ref> iteratively retrieve passages based on the relationship between passages, and also use passage graph to improve passage selection in an extractive reader. However, applying KG to improve the recent advanced FiD framework remains unstudied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In the following sections, we first introduce how to apply KG to build a graph structure among the retrieved passages (Section 3.1). Then we show how we adopt the graph-based stage-1 reranking with DPR retriever to improve passage retrieval (Section 3.2). Next we introduce joint stage-2 reranking and answer generation in the reading module (Section 3.3). Finally we illustrate the improvement of efficiency by using intermediate layer representation for stage-2 reranking (Section 3.4). The overview of our framework is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Construct Passage Graph using KG</head><p>The intuition behind using KG is that there exists the structural relationship among the retrieved passages which can be captured by the KG. Similar to <ref type="bibr" target="#b16">(Min et al., 2019)</ref>, we construct the passage graph where vertices are passages of text and the edges represent the relationships that are derived from the external KGs as KG = {(e h , r, e t )}, where e h , r, e t are the head entity, relation and tail entity of a triplet respectively.</p><p>First, we formalize the definition of a passage. Following previous works <ref type="bibr" target="#b30">(Wang et al., 2019;</ref><ref type="bibr" target="#b11">Karpukhin et al., 2020)</ref>, each article in the text corpus is split into multiple disjoint text blocks of 100 words called passages, which serve as the basic retrieval units. We assume there is a oneone mapping between the KG entities and articles in the text corpus. Specifically, we use English Wikipedia as the text corpus and English Wikidata <ref type="bibr" target="#b28">(Vrandečić and Krötzsch, 2014)</ref> as the knowl-edge graph, since there exists an alignment between the two resources<ref type="foot" target="#foot_0">1</ref> . For example, for the article titled with "New York Yankees", it contains passages such as "The New York Yankees are an American professional baseball team ...". The article also corresponds to a KG entity with the same name as "New York Yankees".</p><p>Then we define the mapping function e = f (p), where the KG entity e corresponds to the article which p belongs to. Note that one passage can only be mapped to one entity, but multiple passages could be mapped to the same entity. The final passage graph is defined as G = {(p i , p j )}, where passages p i and p j are connected if and only if their mapped entities are directly connected in the KG, i.e., (f (p i ), r, f (p j )) ∈ KG.</p><p>Since the total number of passages is very large, e.g., more than 20M in Wikipedia, constructing and maintaining a graph over all the passages is inefficient and memory-consuming. Thus, we build a passage graph on the fly for each question, based on the retrieved passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Passage Retrieving &amp; Stage-1 Reranking</head><p>DPR Retriever: Our framework applies DPR <ref type="bibr" target="#b11">(Karpukhin et al., 2020)</ref> as the retriever, which uses a BERT based passage encoder to encode all the N passages in the text corpus {p 1 , p 2 , • • • , p N }. Suppose all the passage embeddings are fixed and stored in memory as M ∈ R N ×D where D is the hidden dimension:</p><formula xml:id="formula_0">M i = BERT(p i ) for i ∈ {1, 2, • • • N } (1)</formula><p>For an input question q, DPR applies another BERTbased question encoder to obtain its representation Q, then it builds on FAISS <ref type="bibr" target="#b9">(Johnson et al., 2019)</ref> to conduct fast dot-product similarity search between Q and M , and returns N 1 (N 1 ≪ N ) passages with the highest similarity scores.</p><p>Stage-1 Reranking: We see that the DPR retriever returns N 1 passages which are independently retrieved based on the similarity between the question and each passage, without considering inter-passage relationship. Thus instead of directly retrieving N 1 passages for the reader, we propose to first retrieve N 0 (N 0 &gt; N 1 ) passages, then rerank them and output top-N 1 reranked passages into the reader.</p><p>Following Section 3.1, we construct a graph among the N 0 retrieved passages denoted as G 0 . We aim to rerank the retrieved passages based on both the structural information and the textual semantic information of them.</p><p>To represent the semantic information of passages, one can use another pre-trained language model to encode the passage texts, but this will not only include lots of additional model parameters, but also incur heavy computational cost as N 0 can be large. To avoid both additional memory and computation cost, we propose to reuse the offline passage embeddings M generated from the DPR retriever in Equation 1 as the initial node representation:</p><formula xml:id="formula_1">E (0) i = M r i where {r i |i ∈ {1, 2, • • • , N 0 }} is the set of retrieved passage indices.</formula><p>Then we employ a graph attention network (GAT) <ref type="bibr" target="#b26">(Velickovic et al., 2018)</ref> with L g layers as GNN model to update representations for each node based on the passage graph and initial representation. The l-th layer of the GNN model updates the embedding of node i as follows:</p><formula xml:id="formula_2">E (l) i = h(E (l−1) i , {E (l−1) j } (i,j)∈G 0 ) (2)</formula><p>where h is usually a non-linear learnable function which aggregates the embeddings of the node itself and its neighbor nodes. The reranking score for each passage p r i is calculated by</p><formula xml:id="formula_3">s stage-1 i = Q T E (Lg) i</formula><p>, where Q is the question embedding also generated by the DPR retriever. Then we sort the retrieved passages by the reranking scores, and input the top-N 1 passages into the reader. The training loss of passage ranking for each question is:</p><formula xml:id="formula_4">L stage-1 r = − N 0 i=1 y i log exp(s stage-1 i ) N 0 j=1 exp(s stage-1 j )<label>(3)</label></formula><p>where y i = 1 if p r i is the gold passage<ref type="foot" target="#foot_1">2</ref> that contains the answer, and 0 otherwise. As we only add a lightweight graph neural network and reuse the pre-computed and static DPR passage embeddings, our reranking module can process a large number of candidate passages efficiently for each question. In experiments, we set N 0 = 1000 and N 1 = 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Joint Stage-2 Reranking and Answer Generation</head><p>In this section, we briefly introduce the vanilla FiD reading module before illustrating our joint reranking method. We suppose the reader takes</p><formula xml:id="formula_5">N 1 retrieved passages {p a 1 , p a 2 , • • • , p a N 1 } as input.</formula><p>Vanilla FiD Reading Module: We denote the hidden dimension as H and number of encoder layers and decoder layers as L, FiD reader first separately encodes each passage p a i concatenated with question q:</p><formula xml:id="formula_6">P (0) i = T5-Embed(q + p a i ) ∈ R Tp×H ,<label>(4)</label></formula><formula xml:id="formula_7">P (l) i = T5-Encoder l (P (l−1) i ) ∈ R Tp×H ,<label>(5)</label></formula><p>where T p is the sequence length of a passage concatenated with the question. T5-Embed(•) is the initial embedding layer of T5 model <ref type="bibr" target="#b20">(Raffel et al., 2019)</ref> and T5-Encoder l (•) is the l-th layer of its encoder module. Then the token embeddings of all passages output from the last layer of the encoder are concatenated and sent to the decoder to generate the answer tokens A:</p><formula xml:id="formula_8">A = T5-Decoder [P (L) 1 ; P (L) 2 ; • • • ; P (L) N 1 ]<label>(6)</label></formula><p>Stage-2 Reranking: Note that vanilla FiD reader neglect the cross information among passages, and the joint modeling in the decoding process makes it vulnerable to the noisy irrelevant passages. Thus, we propose to leverage the passage graph to rerank the input N 1 passages during the encoding and only select top-N 2 (N 2 &lt; N 1 ) reranked passages into the decoder, which is named as stage-2 reranking.</p><p>Similar to stage-1 reranking, the reranking model is based on both the structural information and the textual semantic information of passages. We denote the passage graph as G 1 , which is a subgraph of G 0 . To avoid additional computation and memory cost, we propose to reuse the encodergenerated question-aware passage representation from FiD reader for passage reranking as it is already computed in Equation <ref type="formula" target="#formula_7">5</ref>. Specifically, the initial node embeddings Z (0) i for passage p a i comes from the first token embedding of the final layer in the FiD-Encoder, i.e., Z</p><formula xml:id="formula_9">(0) i = P (L) i (0) ∈ R D .</formula><p>Then same as stage-1 reranking, we also employ a GAT <ref type="bibr" target="#b26">(Velickovic et al., 2018)</ref> with L g layers as the graph neural network (GNN) model to update representations for each node based on the passage graph, similar to Equation <ref type="formula">2</ref>:</p><formula xml:id="formula_10">Z (Lg) = GAT(Z (0) , G ′ 1 ). The reranking score of passage p a i is calculated by s stage-2 i = W T Z (Lg) i</formula><p>where W is a trainable model parameter. After reranking, only the final top-N 2 (N 2 &lt; N 1 ) passages are sent for decoding. Suppose their indices are {g 1 , g 2 , • • • , g N 2 }, the decoding process in Equation 6 becomes:</p><formula xml:id="formula_11">A = T5-Decoder [P (L) g 1 ; P (L) g 2 ; • • • ; P (L) g N 2 ] (7)</formula><p>where A is the generated answer. Similar to stage-1 reranking, the training loss of passage ranking for each question is:</p><formula xml:id="formula_12">L stage-2 r = − N 1 i=1 y i log exp(s stage-2 i ) N 1 j=1 exp(s stage-2 j )<label>(8)</label></formula><p>where y i = 1 if p a i is the gold passage that contains the answer, and 0 otherwise. The passage reranking and answer generation are jointly trained. We denote the answer generation loss for each question is L a , then the final training loss of our reader module is</p><formula xml:id="formula_13">L = L a + λL stage-2 r</formula><p>, where λ is a hyper-parameter which controls the weight of reranking task in the total loss.</p><p>Note that the first stage reranking is based on DPR embeddings, which are are high-level (one vector per passage) and not further trained. While the second stage is based on reader-generated passage-question embeddings, which are semanticlevel and trainable as part of the model output. Thus the second stage can better capture semantic information of passages and aims for more accurate reranking over a smaller candidate set. In the experiment, we set N 1 = 100 and N 2 = 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Improving Efficiency via Intermediate Representation in Stage-2 Reranking</head><p>Recall that in the stage-2 reranking, we take the passage representation from the last layer of reader encoder for passage reranking. In this section, we propose to further reduce the computation cost by taking the intermediate layer representation rather than the last layer. The intuition is that answer generation task is more difficult than passage reranking which only needs to predict whether the passage contains the answer or not. Thus we may not need the representation from the whole encoder module for passage reranking. Suppose we take the representation from the L 1th layer (1 ≤ L 1 &lt; L), i.e., Z</p><formula xml:id="formula_14">(0) i = P (L 1 ) i (0) for i ∈ {1, 2, • • • , N 1 },</formula><p>and the reranking method remains the same. Then only the top N 2 (N 2 &lt; N 1 ) reranked passages will go through the rest layers of FiD-encoder. Suppose their indices are</p><formula xml:id="formula_15">I g = {g 1 , g 2 , • • • , g N 2 }, for l ≥ L 1 + 1: P (l) i = T5-Encoder l (P (l−1) i ) if i ∈ I g Stop-Computing else<label>(9)</label></formula><p>Then P (L)</p><formula xml:id="formula_16">g 1 , P (L) g 2 , • • • , P (L)</formula><p>g N 2 are sent into the decoder for answer generation as in Equation <ref type="formula">7</ref>. In Section 4.3, we demonstrate this can reduce 60% computation cost than the original FiD while keeping the on-par performance on two benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Analysis on Computational Complexity</head><p>Here we analyze the theoretical time complexity of our proposed KG-FiD compared to vanilla FiD. More practical computation cost comparison is shown in Appendix A.5. Because both the computations of DPR retrieving and stage-1 reranking are negligible compared to the reading part, we only analyze the reading module here.</p><p>Suppose the length of answer sequence A is denoted as T a and the average length of the passage (concatenated with question) is T p . For vanilla FiD reader, the time complexity of the encoder module is</p><formula xml:id="formula_17">O(L • N 1 • T 2 p )</formula><p>, where L, N 1 denote the number of encoder layers and the number of passages for reading. The square comes from the self-attention mechanism. The decoder time com-</p><formula xml:id="formula_18">plexity is O(L•(N 1 •T p •T a +T 2 a ))</formula><p>, where N 1 •T p •T a comes from the cross-attention mechanism. For our reading module, all the N 1 candidate passages are processed by the first L 1 layers of encoder. But only N 2 passages are processed by the remaining L − L 1 encoder layers and sent into the decoder. Thus, the encoder computation complexity becomes O((</p><formula xml:id="formula_19">L 1 • N 1 + (L − L 1 ) • N 2 ) • T 2 p ), and the decoder computation takes O(L•(N 2 •T p •T a +T 2 a )). Because L 1 &lt; L, N 2 &lt; N 1 ,</formula><p>both the encoding and decoding of our method is more efficient than vanilla FiD.</p><p>Furthermore, the answer is usually much shorter than the passage (which is the case in our experiments), i.e., T a ≪ T p . Then the decoding computation can be negligible compared to the encoding. In this case, the approximated ratio of saved computation cost brought by our proposed method is:</p><formula xml:id="formula_20">S = 1 − (L 1 • N 1 + (L − L 1 ) • N 2 ) • T 2 p L • N 1 • T 2 p = (1 − L 1 L )(1 − N 2 N 1 )</formula><p>This shows that we can reduce more computation cost by decreasing L 1 or N 2 . For example, if setting L 1 = L/4, N 2 = N 1 /5, we can reduce about 60% of computation cost. More empirical results and discussions will be presented in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>In this section, we conduct extensive experiments on two most commonly-used ODQA benchmark datasets: Natural Questions (NQ) <ref type="bibr" target="#b13">(Kwiatkowski et al., 2019)</ref> which is based on Google Search Queries, and TriviaQA <ref type="bibr" target="#b10">(Joshi et al., 2017)</ref> which contains questions from trivia and quiz-league websites. We follow the same setting as <ref type="bibr" target="#b8">(Izacard and Grave, 2021)</ref> to preprocess these datasets, which is introduced in Appendix A.1. All our experiments are conducted on 8 Tesla A100 40GB GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>Knowledge Source: Following <ref type="bibr" target="#b11">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b8">Izacard and Grave, 2021)</ref>, we use the English Wikipedia as the text corpus, and apply the same preprocessing to divide them into disjoint passages with 100 words, which produces 21M passages in total. For the knowledge graph, we use English Wikidata. The number of aligned entities, relations and triplets among these entities are 2.7M, 974 and 14M respectively. Model Details: For the retrieving module, we use the DPR retriever <ref type="bibr" target="#b11">(Karpukhin et al., 2020)</ref> which contains two BERT (base) models for encoding question and passage separately. For the GNN reranking models, we adopt 3-layer Graph Attention Networks (GAT) <ref type="bibr" target="#b26">(Velickovic et al., 2018)</ref>. For the reading module, same as <ref type="bibr" target="#b8">(Izacard and Grave, 2021)</ref>, we initialize it with the pretrained T5-base and T5-large models <ref type="bibr" target="#b20">(Raffel et al., 2019)</ref>, and we name the former one as KG-FiD (base) and the latter one as KG-FiD (large). Our implementation is based on the HuggingFace Transformers library <ref type="bibr">(Wolf et al., 2019)</ref>. For number of passages, we set N 0 = 1000, N 1 = 100, N 2 = 20. The training process of our method is introduced in Appendix A.3. More results about model design and hyper-parameter search is in Appendix A.4.</p><p>Evaluation: We follow the standard evaluation metric of answer prediction in ODQA, which is the exact match score (EM) <ref type="bibr" target="#b21">(Rajpurkar et al., 2016)</ref>. A generated answer is considered correct if it matches any answer in the list of acceptable answers after normalization<ref type="foot" target="#foot_2">3</ref> . For all the experiments, we conduct 5 runs with different random seeds and report the averaged scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Methods</head><p>We mainly compare KG-FiD with the baseline model FiD <ref type="bibr" target="#b8">(Izacard and Grave, 2021)</ref>. For other baselines, we compare with representative methods from each category: (1) not using external knowledge source: T5 <ref type="bibr" target="#b22">(Roberts et al., 2020)</ref> and <ref type="bibr">GPT-3 (Brown et al., 2020)</ref>; (2) reranking-based methods: RIDER <ref type="bibr" target="#b15">(Mao et al., 2021)</ref> and RECON-SIDER <ref type="bibr" target="#b6">(Iyer et al., 2021)</ref>; (3) leveraging knowledge graphs or graph information between passages: Graph-Retriever <ref type="bibr" target="#b16">(Min et al., 2019)</ref>, Path-Retriever <ref type="bibr" target="#b0">(Asai et al., 2020)</ref>, <ref type="bibr">KAQA (Zhou et al., 2020)</ref>, and UniK-QA <ref type="bibr" target="#b18">(Oguz et al., 2020)</ref>. We also compare with methods (4) with additional largescale pre-training: REALM <ref type="bibr" target="#b5">(Guu et al., 2020</ref><ref type="bibr">), RAG (Lewis et al., 2020)</ref> and Joint Top-K <ref type="bibr" target="#b23">(Sachan et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results</head><p>Comparison with Baselines: Table <ref type="table" target="#tab_1">1</ref> shows the results of our method and all baselines. We see that our proposed model KG-FiD consistently and significantly improves FiD on both NQ and TriviaQA datasets over both base and large model. Specifically, for large model, KG-FiD improves FiD by 1.5% and 1.1% on two datasets respectively, which has larger improvement compared to base model. We think the reason is that more expressive reader will also benefit the stage-2 reranking since the initial passage embeddings are generated by the reader encoder module. We also see that our proposed method outperforms all the baseline methods except UniK-QA <ref type="bibr" target="#b18">(Oguz et al., 2020)</ref>. However, UniK-QA uses additional knowledge source Wikipedia-Table for retrieval, which is highly related with the NQ dataset and makes it unfair to directly compare with our method.</p><p>Efficiency &amp; Accuracy: that (1) for KG-FiD, decreasing L 1 can improve the computation efficiency as analyzed in Section 3.4, while increasing L 1 can improve the model performance. We think the performance improvement comes from the noise reduction of passage filtering. For a larger L 1 , the passage embeddings for reranking will have a better quality so that the gold passages are less likely to be filtered out.</p><p>(2) Simply reducing the number of passages N 1 into vanilla FiD reader can reduce computation cost, but the performance will also drop significantly (from 51.9 to 50.3 on NQ dataset).</p><p>(3) Our model can achieve the performance on par with FiD with only 38% of computation cost. When consuming the same amount of computations (L 1 = 24), our model significantly outperforms FiD on both NQ and TriviaQA datasets. These experiments demonstrate that our model is very flexible and can improve both the efficiency and effectiveness by changing L 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>Effect of Each Reranking Stage: Since our proposed graph-based reranking method are applied in both retrieving stage (Section 3.2) and reading stage (Section 3.3). We conduct ablation study Model #FLOPs NQ TriviaQA EM Latency (s) EM Latency (s) FiD (N 1 =40) 0.40x 50.3 0.74 (0.45x) 67.5 0.73 (0.44x) FiD (N 1 =100) 1.00x 51.9 1.65 (1.00x) 68.7 1.66 (1.00x) KG-FiD (N 1 =100, L 1 =6) 0.38x 52.0 0.70 (0.42x) 68.9 0.68 (0.41x) KG-FiD (N 1 =100, L 1 =12) 0.55x 52.3 0.96 (0.58x) 69.2 0.94 (0.57x) KG-FiD (N 1 =100, L 1 =18) 0.72x 52.6 1.22 (0.74x) 69.8 1.22 (0.73x) KG-FiD (N 1 =100, L 1 =24) 0.90x 53.4 1.49 (0.90x) 69.8 1.48 (0.89x) to validate the effectiveness of each one. Table <ref type="table" target="#tab_3">3</ref> shows the experiment results by removing each module. We see the performance of KG-FiD drops when removing any of the two reranking modules, demonstrating both of them can improve model performance. Another thing we observe is that stage-1 reranking is more effective in base model while stage-2 reranking is more effective in large model. This is reasonable since stage-2 reranking relies on the effectiveness of reader encoder module, where the large model is usually better than the base model.</p><p>Passage Ranking Results: We additionally show that our proposed GNN reranking method can improve the passage retrieval results. This is demonstrated in Figure <ref type="figure" target="#fig_2">2</ref>, where we report Hits@K metric over NQ test set, measuring the percentage of top-K retrieved passages that contain the gold passages (passages that contain the answer). We see that DPR+stage-1 reranking consistently outperforms DPR for all the K ∈ {10, 20, 50, 100}. With two stages of reranking, the retrieval results are further improved for K ∈ {10, 20} (We only cares about K ≤ 20 for stage-2 reranking since N 2 = 20). This shows that such reranking can increase the rank of gold passages which are previ-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This work tackles the task of Open-Domain Question Answering. We focus on the current best performed framework FiD and propose a novel KG-based reranking method to enhance the crossmodeling between passages and improve computation efficiency. Our two-stage reranking methods reuses the passage representation generated by DPR retriver and the reader encoder and apply graph neural networks to compute reranking scores. We further propose to use the intermediate layer of encoder to reduce computation cost while still maintaining good performance. Experiments on Natural Questions and TriviaQA show that our model can significantly improve original FiD by 1.5% exact match score and achieve on-par performance with FiD but reducing over 60% of computation cost. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Dataset</head><p>The datasets we use are Natural Questions (NQ) and TriviaQA. The open-domain version of NQ is obtained by discarding answers with more than 5 tokens. For TriviaQA, its unfiltered version is used for ODQA. We also convert all letters of answers in lowercase except the first letter of each word on TriviaQA. When training on NQ, we sample the answer target among the given list of answers, while for TriviaQA, we use the unique human-generated answer as generation target. For both datasets, we use the original validation data as test data, and keep 10% of the training set for validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Preliminary Analysis</head><p>We conduct preliminary analysis on the graph constructed among passages. Note that for each question, we first apply the retriever to retrieve a few candidate passages, then build edge connection only among the retrieved passages, which means that the passage graph is question-specific. Since the passage graph depends on the retrieved passages, before further utilizing the graph, we need avoid two trivia situations: (1) all the retrieved passages come from the same article; (2) The number of graph edges is very small. Thus we conduct statistics of the passage graphs on two ODQA benchmark datasets, which is shown in Figure <ref type="figure" target="#fig_3">3</ref>. For each question, the number of retrieved passages is 100. We see that the two trivia situations only happen for a small portion of questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Training Process</head><p>For training our framework, we adopt the separatetraining strategy to avoid out-of-memory issue: we first train the DPR model following its original paper, then freeze the DPR model to train the stage-1 reranking module, and finally jointly train stage-2 reranking and reader part. For the training of stage-1 reranking, the optimizer is AdamW (Loshchilov and Hutter, 2019) with learning rate as 1e-3 and linear-decay scheduler. The weight decay rate is 0.01. Batch size is set as 64. The number of total training steps is 15k, and the model is evaluated every 500 steps and the model with best validation results is saved as the final model. For the training of reading part, we adopt the same training setting except that the learning rate is 1e-4 for the base model and 5e-5 for the large model. We also adopt learning rate warm up with 1000 steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Additional Experiment Results</head><p>We show additional experiment results in this section, which includes the efficiency and performance comparison between FiD (base) and KG-FiD (base) shown in Table <ref type="table" target="#tab_6">4</ref>, and hyper-parameter search results listed below:   GNN Model Design: We conduct tuning on the model type and number of layers of our GNN based reranking model. For efficiency, we rerank 100 passages returned by DPR retriever and search them based on the passage retrieval results. Table <ref type="table" target="#tab_7">5</ref> shows the Hits scores for different choices. We see that GAT outperforms vanilla GCN model <ref type="bibr" target="#b12">(Kipf and Welling, 2017)</ref> which is reasonable since GAT leverage attention to reweight neighbor passages by their embeddings. The best choice for the number of GNN layers is 3. Note that other GNN models such as <ref type="bibr">GIN (Xu et al., 2019)</ref>, DGI <ref type="bibr" target="#b27">(Velickovic et al., 2019)</ref> can also be applied here and we leave the further exploration of GNN models as future work.</p><p>N 2 and λ. For the stage-2 reranking part in Section 3.3, we also conduct hyper-parameter search on the number of passages after filtering: N 2 ∈ {10, 20, 30} and the weight of reranking loss when training the reading module: λ ∈ {0.01, 0.1, 1.0}. As shown in Table <ref type="table" target="#tab_9">6</ref>, N 2 = 20 achieves better results than N 2 = 10, but further increasing N 2 does not bring performance gain while decreasing the efficiency of model since the number of passages to be processed by the decoder is increased. Thus we choose N 2 = 20. For the loss weight λ, we found that with its increment, the performance first increases then significantly drops. This shows that it's important to balance the weight of two training losses, as we want the model to learn better passage reranking while not overwhelming the training signal of answer generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 FLOPs Computation</head><p>In this section we compute the FLOPs of each module<ref type="foot" target="#foot_3">4</ref> . The results are shown in base model and large model respectively. Before the computation, we first show some basic statistics on two benchmark datasets: the average question length is 20, and the average answer length is 5.</p><p>For the reading part, the length of concatenated passage question pair is 250, number of input passages is N 1 = 100.</p><p>We first calculate the number of FLOPs of vanilla FiD model. For the retrieving part, it contains both question encoding and passage similarity search. We only consider the former part as the latter part depends on the corpus size and search methods and is usually very efficient. The question encoding flops by BERT-based model is about 4.4 Gigaflops (GFLOPs). For the reading part, the encoding of each question passage pair takes about 57/174 GFLOPs for base/large model, and the encoding of 100 passages takes 5772/17483 GFLOPs. The decoder part only costs 714.2/2534.5 GFLOPs for base/large model since the average length of answer is very small. In summary, vanilla FiD base/large model costs 6491.0/20022.0 GFLOPs.</p><p>For our model, the computation cost of retrieving part is the same as vanilla FiD. Since we set N 0 = 1000 and N 1 = 100, the GAT <ref type="bibr" target="#b26">(Velickovic et al., 2018)</ref> computation in stage-1 reranking takes about 3.5 GFLOPs, and the stage-2 reranking takes only 0.4/0.6 GFLOPs for base/large model. For the reader encoding part, the computation cost depends on L 1 and N 2 , which is analyzed in Section 3.5. For the reader decoding part, where cross attention takes most of the computation, KG-FiD only takes about N 2 /N 1 = 1/5 cost of vanilla FiD, which is 143.9/510.0 for base/large model respectively. The detailed flops are shown in Table <ref type="table" target="#tab_8">7 and 8.</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overall Model Framework. Pi indicates the node of the passage originally ranked the i-th by the DPR retriever, with the article title below it. The left part shows passage retrieval by DPR, passage graph construction based on KG (Section 3.1) and stage-1 reranking (Section 3.2). The right part shows joint stage-2 reranking and answer generation in the reading module (Section 3.3 and 3.4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Passage ranking results over NQ test set of DPR retriever and our proposed two-stage rerankings over base model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Preliminary Analysis on the retrieved passages by DPR.</figDesc><graphic url="image-7.png" coords="12,300.31,273.69,187.10,142.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>show the de-</cell></row><row><cell>tailed comparison between our method and FiD</cell></row><row><cell>in the large model version. The results of base</cell></row><row><cell>model version is shown in Appendix A.4. Be-</cell></row><row><cell>sides EM score, we also report the ratio of compu-</cell></row><row><cell>tation flops (#FLOPs) and inference latency (per</cell></row><row><cell>question). The detailed calculation of #FLOPs is</cell></row><row><cell>shown in Appendix A.5. From table 2, we see</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Exact match score of different models over the test sets of NQ and TriviaQA datasets. ⋆ means that additional knowledge source Wikipedia-Tables is used in this method.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Inference #FLOPs, Latency (second) and Exact match score of FiD (large) and KG-FiD (large). N 1 is the number of passages into the reader and L 1 is the number of intermediate layers used for stage-2 reranking as introduced in Section 3.4. The details of flop computation is introduced in Appendix A.5.</figDesc><table><row><cell>Model</cell><cell>NQ</cell><cell>TriviaQA</cell></row><row><cell></cell><cell cols="2">base large base large</cell></row><row><cell>FiD</cell><cell cols="2">48.8 51.9 66.2 68.7</cell></row><row><cell>KG-FiD</cell><cell cols="2">49.6 53.4 66.7 69.8</cell></row><row><cell cols="3">w/o Stage-1 49.3 53.1 66.2 69.5</cell></row><row><cell cols="3">w/o Stage-2 49.4 52.3 66.5 69.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of our graph-based reranking method in two stages. EM scores are reported over NQ and Trivia datasets with both base and large model version.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5878-5882, Hong Kong, China. Association for Computational Linguistics.</figDesc><table><row><cell cols="3">Thomas Wolf, Lysandre Debut, Victor Sanh, Julien</cell></row><row><cell cols="3">Chaumond, Clement Delangue, Anthony Moi, Pier-</cell></row><row><cell cols="3">ric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,</cell></row><row><cell cols="3">et al. 2019. Huggingface's transformers: State-of-</cell></row><row><cell cols="3">the-art natural language processing. arXiv preprint</cell></row><row><cell>arXiv:1910.03771.</cell><cell></cell></row><row><cell cols="3">Wenhan Xiong, Mo Yu, Shiyu Chang, Xiaoxiao Guo,</cell></row><row><cell cols="3">and William Yang Wang. 2019. Improving question</cell></row><row><cell cols="3">answering over incomplete KBs with knowledge-</cell></row><row><cell cols="3">aware reader. In Proceedings of the 57th Annual</cell></row><row><cell cols="3">Meeting of the Association for Computational Lin-</cell></row><row><cell cols="3">guistics, pages 4258-4264, Florence, Italy. Associa-</cell></row><row><cell cols="2">tion for Computational Linguistics.</cell></row><row><cell cols="3">Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie</cell></row><row><cell cols="3">Jegelka. 2019. How powerful are graph neural net-</cell></row><row><cell cols="3">works? In 7th International Conference on Learning</cell></row><row><cell cols="3">Representations, ICLR 2019, New Orleans, LA, USA,</cell></row><row><cell cols="2">May 6-9, 2019. OpenReview.net.</cell></row><row><cell cols="3">Ruochen Xu, Yuwei Fang, Chenguang Zhu, and Michael</cell></row><row><cell cols="3">Zeng. 2021a. Does knowledge help general nlu? an</cell></row><row><cell cols="3">empirical study. arXiv preprint arXiv:2109.00563.</cell></row><row><cell cols="3">Yichong Xu, Chenguang Zhu, Ruochen Xu, Yang Liu,</cell></row><row><cell cols="3">Michael Zeng, and Xuedong Huang. 2021b. Fus-</cell></row><row><cell cols="3">ing context into knowledge graph for commonsense</cell></row><row><cell cols="3">question answering. In Findings of the Association</cell></row><row><cell cols="3">for Computational Linguistics: ACL-IJCNLP 2021,</cell></row><row><cell cols="3">pages 1201-1207, Online. Association for Computa-</cell></row><row><cell>tional Linguistics.</cell><cell></cell></row><row><cell cols="3">Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen</cell></row><row><cell cols="3">Tan, Kun Xiong, Ming Li, and Jimmy Lin. 2019.</cell></row><row><cell cols="3">End-to-end open-domain question answering with</cell></row><row><cell cols="3">BERTserini. In Proceedings of the 2019 Confer-</cell></row><row><cell cols="3">ence of the North American Chapter of the Associa-</cell></row><row><cell cols="3">tion for Computational Linguistics (Demonstrations),</cell></row><row><cell cols="3">pages 72-77, Minneapolis, Minnesota. Association</cell></row><row><cell cols="2">for Computational Linguistics.</cell></row><row><cell cols="3">Donghan Yu, Chenguang Zhu, Yiming Yang, and</cell></row><row><cell cols="3">Michael Zeng. 2020. Jaket: Joint pre-training of</cell></row><row><cell cols="3">knowledge graph and language understanding. arXiv</cell></row><row><cell cols="2">preprint arXiv:2010.00796.</cell></row><row><cell cols="3">Mantong Zhou, Zhouxing Shi, Minlie Huang, and</cell></row><row><cell>Xiaoyan Zhu. 2020.</cell><cell cols="2">Knowledge-aided open-</cell></row><row><cell cols="2">domain question answering.</cell><cell>arXiv preprint</cell></row><row><cell>arXiv:2006.05244.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Inference #FLOPs, Latency (second) and Exact match score of FiD (base) and KG-FiD (base). N 1 is the number of passages into the reader and L 1 is the number of intermediate layers used for stage-2 reranking as introduced in Section 3.4. The details of flop computation is introduced in Appendix A.5.</figDesc><table><row><cell cols="4">Model H@1 H@5 H@10 H@20</cell></row><row><cell>GCN</cell><cell>49.1 69.7</cell><cell>75.7</cell><cell>79.9</cell></row><row><cell>GAT</cell><cell>50.1 70.1</cell><cell>76.1</cell><cell>80.2</cell></row><row><cell>#Layers</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>49.0 69.7</cell><cell>75.8</cell><cell>79.8</cell></row><row><cell>2</cell><cell>49.6 70.0</cell><cell>76.0</cell><cell>80.2</cell></row><row><cell>3</cell><cell>50.1 70.1</cell><cell>76.1</cell><cell>80.2</cell></row><row><cell>4</cell><cell>49.5 69.9</cell><cell>76.1</cell><cell>80.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Passage Retrieval Results on NQ dev data of our model under different GNN types and number of layers.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>and 8 for</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>EM scores on NQ dev data of our model under different choices of filtered passage numbers and weights of reranking loss.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Entity recognition and linking can be used if there is no such alignment.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We follow<ref type="bibr" target="#b11">Karpukhin et al. (2020)</ref> on the definition of gold passages.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">The normalization includes lowercasing and removing articles, punctuation and duplicated whitespace.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">Our computation is based on https://github.com/googleresearch/electra/blob/master/flops_computation.py</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head><p>We thank all the reviewers for their valuable comments. We also thank Woojeong Jin, Dong-Ho Lee, and Aaron Chan for useful discussions. Donghan Yu and Yiming Yang are supported in part by the United States Department of Energy via the Brookhaven National Laboratory under Contract No. 384608.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to retrieve reasoning paths over wikipedia graph for question answering</title>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Alec Radford, Ilya Sutskever, and Dario Amodei</title>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish; NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
	<note>Language models are few-shot learners</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reading Wikipedia to answer opendomain questions</title>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1171</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08909</idno>
		<title level="m">Realm: Retrievalaugmented language model pre-training</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">RECONSIDER: Improved re-ranking using span-focused cross-attention for open domain question answering</title>
		<author>
			<persName><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.100</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
				<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1280" to="1287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Distilling knowledge from reader to retriever for question answering</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04584</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Leveraging passage retrieval with generative models for open domain question answering</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
				<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="874" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with gpus</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1147</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1601" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for opendomain question answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.550</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. 2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Natural questions: A benchmark for question answering research</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00276</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="452" to="466" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive NLP tasks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><surname>Kiela</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2020. 2020. 2020. December 6-12, 2020. 2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
	<note>7th International Conference on Learning Representations. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reader-guided passage reranking for opendomain question answering</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.29</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
				<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="344" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Knowledge guided text retrieval and reading for open domain question answering</title>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03868</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<title level="m">Passage re-ranking with bert</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Unified open-domain question answering with structured and unstructured knowledge</title>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><surname>Peshterliev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Yih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.14610</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">RocketQA: An optimized training approach to dense passage retrieval for opendomain question answering</title>
		<author>
			<persName><forename type="first">Yingqi</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.466</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5835" to="5847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">How much knowledge can you pack into the parameters of a language model?</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.437</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5418" to="5426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-end training of neural retrievers for open-domain question answering</title>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Kant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.519</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6648" to="6662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">PullNet: Open domain question answering with iterative retrieval on knowledge bases and text</title>
		<author>
			<persName><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tania</forename><surname>Bedrax-Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1242</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2380" to="2390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Open domain question answering using early fusion of knowledge bases and text</title>
		<author>
			<persName><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1455</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4231" to="4242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30">2018. April 30 -May 3, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
		<ptr target="OpenRe-view.net" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Vrandečić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Krötzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">R 3: Reinforced ranker-reader for open-domain question answering</title>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerry</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-passage BERT: A globally normalized BERT model for open-domain question answering</title>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1599</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
