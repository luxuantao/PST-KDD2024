<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Key-Value Memory Networks for Knowledge Tracing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
							<email>jnzhang@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Shenzhen Key Laboratory of Rich Media Big Data Analytics and Application</orgName>
								<orgName type="institution" key="instit1">Shenzhen Research Institute</orgName>
								<orgName type="institution" key="instit2">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shatin, Hong Kong</settlement>
									<region>N.T</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
							<email>king@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Shenzhen Key Laboratory of Rich Media Big Data Analytics and Application</orgName>
								<orgName type="institution" key="instit1">Shenzhen Research Institute</orgName>
								<orgName type="institution" key="instit2">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shatin, Hong Kong</settlement>
									<region>N.T</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
							<email>dyyeung@cse.ust.hk</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Key-Value Memory Networks for Knowledge Tracing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4C78A4C4CBFB73FAC123BE96CBC2AC23</idno>
					<idno type="DOI">10.1145/3038912.3052580</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Massive Open Online Courses</term>
					<term>Knowledge Tracing</term>
					<term>Concept Discovery</term>
					<term>Deep Learning</term>
					<term>Dynamic Key-Value Memory Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge Tracing (KT) is a task of tracing evolving knowledge state of students with respect to one or more concepts as they engage in a sequence of learning activities. One important purpose of KT is to personalize the practice sequence to help students learn knowledge concepts efficiently. However, existing methods such as Bayesian Knowledge Tracing and Deep Knowledge Tracing either model knowledge state for each predefined concept separately or fail to pinpoint exactly which concepts a student is good at or unfamiliar with. To solve these problems, this work introduces a new model called Dynamic Key-Value Memory Networks (DKVMN) that can exploit the relationships between underlying concepts and directly output a student's mastery level of each concept. Unlike standard memory-augmented neural networks that facilitate a single memory matrix or two static memory matrices, our model has one static matrix called key, which stores the knowledge concepts and the other dynamic matrix called value, which stores and updates the mastery levels of corresponding concepts. Experiments show that our model consistently outperforms the state-ofthe-art model in a range of KT datasets. Moreover, the DKVMN model can automatically discover underlying concepts of exercises typically performed by human annotations and depict the changing knowledge state of a student.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>With the advent of massive open online courses and intelligent tutoring systems in the web, students can get appropriate guidance and acquire relevant knowledge in the process of solving exercises. When an exercise is posted, a student must apply one or more concepts to solve the exercise. For example, when a student attempts to solve the exercise "1+2", then he or she should apply the concept of "integer addition"; when a student attempts to solve "1+2+3.4", then he or she should apply the concepts of "integer addition" and "decimal addition". The probability that a student can answer the exercise correctly is based on the student's knowledge state, which stands for the depth and robustness of the underlying concepts the student has mastered.</p><p>The goal of knowledge tracing (KT) is to trace the knowledge state of students based on their past exercise performance. KT is an essential task in online learning platforms. Tutors can give proper hints and tailor the sequence of practice exercises based on the personal strengths and weaknesses of students. Students can be made aware of their learning progress and may devote more energy to lessfamiliar concepts to learn more efficiently.</p><p>Although effectively modeling the knowledge of students has high educational impact, using numerical simulations to represent the human learning process is inherently difficult <ref type="bibr" target="#b22">[22]</ref>. Usually, KT is formulated as a supervised sequence learning problem: given a student's past exercise interactions X = {x1,x2,...,xt-1}, predict the probability that the student will answer a new exercise correctly, i.e., p(rt = 1|qt, X ). Input xt = (qt, rt) is a tuple containing the exercise qt, which student attempts at the timestamp t, and the correctness of the student's answer rt. We model X as observed variables and a student's knowledge state S = {s1,s2,...,st-1} of N underlying concepts C = {c 1 , c 2 , ..., c N } as a hidden process.</p><p>Existing methods such as Bayesian Knowledge Tracing (BKT) <ref type="bibr" target="#b3">[3]</ref> and Deep Knowledge Tracing (DKT) <ref type="bibr" target="#b22">[22]</ref> model the knowledge state of students either in a concept specific manner or in one summarized hidden vector, as shown in Figure <ref type="figure" target="#fig_0">1</ref>. In BKT, a student's knowledge state st is analyzed into different concept states {s i t } and BKT models each con- BKT is concept specific. DKT uses a summarized hidden vector to model the knowledge state. Our model maintains the concept state for each concept simultaneously and all concept states constitute the knowledge state of a student. cept state separately. BKT assumes the concept state as a binary latent variable, known and unknown, and uses a Hidden Markov model to update the posterior distribution of the binary concept state. Therefore, BKT cannot capture the relationship between different concepts. Moreover, to keep the Bayesian inference tractable, BKT uses discrete random variables and simple transition models to describe the evolvement of each concept state. As a result, although BKT can output the student's mastery level of some predefined concepts, it lacks the ability to extract undefined concepts and model complex concept state transitions.</p><note type="other">BKT Our model DKT</note><p>Besides solving the problem from the Bayesian perspective, a deep learning method named DKT <ref type="bibr" target="#b22">[22]</ref> exploits a variant of recurrent neural networks (RNNs) called long short-term memory (LSTM) <ref type="bibr" target="#b9">[9]</ref>. LSTM assumes a highdimensional and continuous representation of the underlying knowledge state S. The nonlinear input-to-state and stateto-state transitions of DKT have stronger representational power than those of BKT. No human-labeled annotation is required. However, DKT summarizes a student's knowledge state of all concepts in one hidden state, which makes it difficult to trace how much a student has mastered a certain concept and pinpoint which concepts a student is good at or unfamiliar with <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b31">31]</ref>.</p><p>The present work introduces a new model called Dynamic Key-Value Memory Networks (DKVMN) that combines the best of two worlds: the ability to exploit the relationship between concepts and the ability to trace each concept state. Our DKVMN model can automatically learn the correlation between input exercises and underlying concepts and maintain a concept state for each concept. At each timestamp, only related concept states will be updated. For instance, in Figure <ref type="figure" target="#fig_0">1</ref>, when a new exercise qt comes, the model finds that qt requires the application of concept c j and c k . Then we read the corresponding concept states s j t-1 and s k t-1 to predict whether the student will answer the exercise correctly. After the student completes the exercise, our model will update these two concept states. All concept states constitute the knowledge state S of a student.</p><p>In addition, unlike standard memory-augmented neural networks (MANNs) that facilitate a single memory matrix <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b30">30]</ref> or a variation with two static memory matrices <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b27">27]</ref>, our model has one static matrix called key, which stores the concept representations and the other dynamic matrix called value, which stores and updates the student's understanding (concept state) of each concept. The terms static and dynamic matrices are respectively analogous to immutable and mutable objects as keys and values in the dictionary data structure (e.g., Python's dictionary). Meanwhile, our training process is analogous to object creation. After the keys are created, they will be fixed (i.e., immutable) during testing.</p><p>The network with two static memory matrices is not suitable for solving the KT task because learning is not a static process. Learning builds upon and is shaped by previous knowledge in human memory <ref type="bibr">[8]</ref>. The model with a single dynamic matrix maps the exercise with the correct answer and the exercise with the incorrect answer to different concept states, which does not match our cognition. Experiments show that our DKVMN model outperforms the MANN model with a single memory matrix and the stateof-the-art model.</p><p>Our main contributions are summarized as follows:</p><p>1. The utility of MANNs is exploited to better simulate the learning process of students.</p><p>2. A novel DKVMN model with one static key matrix and one dynamic value matrix is proposed.</p><p>3. Our model can automatically discover concepts, a task that is typically performed by human experts, and depict the evolving knowledge state of students.</p><p>4. Our end-to-end trainable model consistently outperforms BKT and DKT on one synthetic and three realworld datasets respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Knowledge Tracing</head><p>The KT task evaluates the knowledge state of a student based simply on the correctness or incorrectness rt of a student's answers in the process of solving exercises qt. In this study qt is an exercise tag and rt ∈ {0, 1} is a binary response (1 is correct and 0 is incorrect). No secondary data are incorporated <ref type="bibr" target="#b11">[11]</ref>.</p><p>BKT <ref type="bibr" target="#b3">[3]</ref> is a highly constrained and structured model <ref type="bibr" target="#b11">[11]</ref> because it models concept-specific performance, i.e., an individual instantiation of BKT is made for each concept, and BKT assumes knowledge state as a binary variable. Many following variations were raised by integrating personalization study <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b33">33]</ref>, exercise diversity <ref type="bibr" target="#b20">[20]</ref>, and other information <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b23">23]</ref> into the Bayesian framework.</p><p>DKT <ref type="bibr" target="#b22">[22]</ref> exploits the utility of LSTM <ref type="bibr" target="#b9">[9]</ref> to break the restriction of skill separation and binary state assumption. LSTM uses hidden states as a kind of summary of the past sequence of inputs, and the same parameters are shared over different time steps. Experiments in <ref type="bibr" target="#b22">[22]</ref> showed that DKT outperforms previous Bayesian models by a large margin in terms of prediction accuracy. This study was the first attempt to integrate deep learning models <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b25">25]</ref>, which have achieved significant success in other areas, including computer vision <ref type="bibr" target="#b13">[13]</ref> and natural language processing <ref type="bibr" target="#b16">[16]</ref> into KT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Memory-Augmented Neural Networks</head><p>Inspired by computer architecture, a particular neural network module called external memory was proposed to enhance the ability of a network to capture long-term dependencies and solve algorithmic problems <ref type="bibr" target="#b7">[7]</ref>. MANN have led the progress in various areas, such as question answering <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b1">1,</ref><ref type="bibr" target="#b17">17]</ref>, natural language transduction <ref type="bibr">[8]</ref>, algorithm inference <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b10">10]</ref>, and one-shot learning <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b28">28]</ref>.</p><p>The typical external memory module contains two parts, a memory matrix that stores the information and a controller that communicates with the environment and reads or writes to the memory. The reading and writing operations are achieved through additional attention mechanisms. Most previous works <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b24">24]</ref> use a similar way to compute the read weight. For an input kt, a cosine similarity or an inner product K[kt, Mt(i)] of the input and each memory slot Mt(i) is computed, which then goes through a softmax with a positive key strength βt to obtain a read weight w r t : w r t (i) = Softmax(βtK[kt, Mt(i)]), where Softmax(zi) = e z i / j e z j . For the write process, an attention mechanism of focusing both by content and by location is proposed in <ref type="bibr" target="#b6">[6]</ref> to facilitate all the locations of the memory. In addition, a pure content-based memory writer named least recently used access (LRUA) module is raised in <ref type="bibr" target="#b24">[24]</ref> to write the key either to the least recently used memory location or to the most recently used memory location.</p><p>Owing to the recurrence introduced in the read and write operations, MANN is a special kind of RNN as well. However, MANN is different from conventional RNNs like the LSTM used in DKT in three aspects. First, traditional RNN models use a single hidden state vector to encode the temporal information, whereas, MANN uses an external memory matrix that can increase storage capacity <ref type="bibr" target="#b30">[30]</ref>. Second, the state-to-state transition of traditional RNNs is unstructured and global, whereas MANN uses read and write operations to encourage local state transitions <ref type="bibr" target="#b6">[6]</ref>. Third, the number of parameters in traditional RNNs is tied to the size of hidden states <ref type="bibr" target="#b24">[24]</ref>. For MANN, increasing the number of memory slots will not increase the number of parameters, an outcome that is more computationally efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MODEL</head><p>In this section, we first introduce the way to exploit the existing MANN model to solve the KT problem. We then show the deficiencies of MANN and describe our DKVMN model. In our description below, we denote vectors with bold small letters and matrices with bold capital letters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Memory-Augmented Neural Network for Knowledge Tracing</head><p>To solve the KT problem, the external memory matrix of MANN is treated as the knowledge state of a student. The overall structure of the model is shown in Figure <ref type="figure" target="#fig_2">2a</ref>. The memory, denoted as Mt, is an N × M matrix, where N is the number of memory locations, and M is the vector size at each location. At each timestamp t, the input for MANN is a joint embedding vt of (qt, rt), where each qt comes from a set of Q distinct exercise tags and rt is a binary value indicating whether the student answered the exercise correctly. The embedding vector vt is used to compute the read weight w r t and the write weight w w t . In our implementation, we choose the cosine similarity attention mechanism to compute w r t and the LRUA mechanism <ref type="bibr" target="#b24">[24]</ref> to compute w w t . Details of these two attention mechanisms are shown in the appendix. The intuition of the MANN is that when a student answers the exercise that has been stored in the memory with the same response, vt will be written to the previously used memory locations and when a new exercise arrives or the student gets a different re-sponse, vt will be written to the least recently used memory locations.</p><p>In the read process, the read content rt is obtained by the weighted sum of all memory slots with the read weight w r t :</p><formula xml:id="formula_0">rt = N i=1 w r t (i)Mt(i).<label>(1)</label></formula><p>The output p t ∈ R Q , which is computed from rt, indicates the probability that the student can answer each exercise correctly in the next timestamp.</p><p>In the write process, we first erase unnecessary contents in the memory using the erase signal et and the write weight w r t , and then add vt into the memory using the add signal at <ref type="bibr" target="#b6">[6]</ref>. For further details, see Section 3.2.3.</p><p>MANN uses N memory slots to encode the knowledge state of a student and has a larger capacity than LSTM, which only encodes knowledge state in a single hidden vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dynamic Key-Value Memory Networks</head><p>Despite being more powerful than LSTM in storing the past performance of students, MANN still has deficiencies when applied to the KT task. In MANN, the content we read lies in the same space as the content we write. However, for tasks like KT, the input and the prediction, which are the exercises the student receives and the correctness of the student's answer have different types. Therefore, the way to embed the exercise and the response jointly as the attention key does not make sense. Furthermore, MANN cannot explicitly model the underlying concepts for input exercises. Knowledge state of a particular concept is dispersed and cannot be traced.</p><p>To solve these problems, our DKVMN model uses keyvalue pairs rather than a single matrix for the memory structure. Instead of attending, reading, and writing to the same memory matrix in MANN, our DKVMN model attends input to the key component, which is immutable, and reads and writes to the corresponding value component.</p><p>Unlike MANN, at each timestamp, DKVMN takes a discrete exercise tag qt, outputs the probability of response p(rt|qt), and then updates the memory with exercise and response tuple (qt, rt). Here, qt also comes from a set with Q distinct exercise tags and rt is a binary value. We further assume there are N latent concepts {c 1 , c 2 , ..., c N } underlying the exercises. These concepts are stored in the key matrix M k (of size N × d k ) and the student's mastery levels of each concept, i.e., concept states {s 1 t , s 2 t , ..., s N t } are stored in the value matrix M v t (of size N × dv), which changes over time. DKVMN traces the knowledge of a student by reading and writing to the value matrix using the correlation weight computed from the input exercise and the key matrix. The model details are elaborated in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Correlation Weight</head><p>The input exercise qt is first multiplied by an embedding matrix A (of size Q × d k ) to get a continuous embedding vector kt of dimension d k . The correlation weight is further computed by taking the softmax activation of the inner product between kt and each key slot M k (i):  where Softmax(zi) = e z i / j e z j and is differentiable. Both the read and write processes will use this weight vector wt, which represents the correlation between exercise and each latent concept.</p><formula xml:id="formula_1">wt(i) = Softmax(k T t M k (i)),<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Read process</head><p>When an exercise qt comes, the read content rt is retrieved by the weighted sum of all memory slots in the value matrix using wt:</p><formula xml:id="formula_2">rt = N i=1 wt(i)M v t (i).<label>(3)</label></formula><p>The calculated read content rt is treated as a summary of the student's mastery level of this exercise. Given that each exercise has its own difficulty, we concatenate the read content rt and the input exercise embedding kt and then pass it through a fully connected layer with a Tanh activation to get a summary vector ft, which contains both the student's mastery level and the prior difficulty of the exercise:</p><formula xml:id="formula_3">ft = Tanh(W T 1 [rt, kt] + b1),<label>(4)</label></formula><p>where Tanh(zi) = (e z i -e -z i )/(e z i + e -z i ). Finally, ft is passed through another fully connected layer with a Sigmoid activation to predict the performance of the student:</p><formula xml:id="formula_4">pt = Sigmoid(W T 2 ft + b2),<label>(5)</label></formula><p>where Sigmoid(zi) = 1/(1 + e -z i ), and pt is a scalar that represents the probability of answering qt correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Write process</head><p>After the student answers the question qt, the model will update the value matrix according to the correctness of the student's answer. A joint embedding of (qt, rt) will be writ-ten to the value part of the memory with the same correlation weight wt used in the read process.</p><p>The tuple (qt, rt) is embedded with an embedding matrix B of size 2Q × dv to obtain the knowledge growth vt of the student after working on this exercise. When writing the student's knowledge growth into the value component, the memory is erased first before new information is added <ref type="bibr" target="#b6">[6]</ref>, a step inspired by the input and forget gates in LSTMs.</p><p>Given a write weight (which is the correlation weight wt in our model), an erase vector et is computed from vt:</p><formula xml:id="formula_5">et = Sigmoid(E T vt + be),<label>(6)</label></formula><p>where the transformation matrix E is of shape dv × dv, et is a column vector with dv elements that all lie in the range (0, 1). The memory vectors of value component M v t-1 (i) from the previous timestamp are modified as follows:</p><formula xml:id="formula_6">Mv t (i) = M v t-1 (i)[1 -wt(i)et],<label>(7)</label></formula><p>where 1 is a row-vector of all 1-s. Therefore, the elements of a memory location are reset to zero only if both the weight at the location and the erase element are one. The memory vector is left unchanged if either the weight or the erase signal is zero. After erasing, a length dv add vector at is used to update each memory slot:</p><formula xml:id="formula_7">at = Tanh(D T vt + ba) T ,<label>(8)</label></formula><p>where the transformation matrix D is of shape dv × dv and at is a row vector. The value memory is updated at each time t by</p><formula xml:id="formula_8">M v t (i) = Mv t-1 (i) + wt(i)at.<label>(9)</label></formula><p>This erase-followed-by-add mechanism allows forgetting and strengthening concept states in the learning process of a student. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Training</head><p>The overall model architecture is shown in Figure <ref type="figure" target="#fig_2">2b</ref>. During training, both embedding matrices A and B, as well as other parameters and the initial value of M k and M v are jointly learned by minimizing a standard cross entropy loss between pt and the true label rt.</p><formula xml:id="formula_9">L = - t (rtlogpt + (1 -rt)log(1 -pt)). (<label>10</label></formula><formula xml:id="formula_10">)</formula><p>Our DKVMN model is fully differentiable and can be trained efficiently with stochastic gradient descent (see Section 4.2 for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>The prediction accuracy is first evaluated by comparing our DKVMN model with other methods on four datasets, namely one synthetic dataset and three real-world datasets, collected from online learning platforms. Then, comparative experiments of different dimensions of states are performed on DKVMN and DKT for further model exploration. Finally, the ability of our model is verified to discover concepts automatically and depict the knowledge state of students.</p><p>The experiment results lead to the following findings:</p><p>• DKVMN outperforms the standard MANN and the state-of-the-art method on four datasets.</p><p>• DKVMN can produce better results with fewer parameters than DKT.</p><p>• DKVMN does not suffer from overfitting, which is a big issue for DKT.</p><p>• DKVMN can discover underlying concepts for input exercises precisely.</p><p>• DKVMN can depict students' concept states of distinct concepts over time.</p><p>We implement the models using MXNet <ref type="bibr" target="#b2">[2]</ref> on a computer with a single NVIDIA K40 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>To evaluate performance, we test KT models on four datasets: Synthetic-5, ASSISTments2009, ASSISTments2015, and Stat-ics2011.</p><p>Synthetic-5: This dataset<ref type="foot" target="#foot_0">1</ref> simulates 2000 virtual students answering 50 exercises in both the training and testing dataset. Each exercise is drawn from one of five hidden concepts and has different levels of difficulty. We have no access to the underlying concept labels in the training process and simply use them as the ground truth to evaluate the discovered concept results using our DKVMN model.</p><p>ASSISTments2009: This dataset <ref type="bibr" target="#b5">[5]</ref> is gathered from the ASSISTments online tutoring platform. Owing to duplicated record issues <ref type="bibr" target="#b32">[32]</ref>, an updated version is released and all previous results on the old dataset are no longer reliable. The experiments in our paper are conducted using the updated "skill-builder" dataset<ref type="foot" target="#foot_1">2</ref> . Records without skill names are discarded in the preprocessing. Thus, the number of records in our experiments is smaller than that in <ref type="bibr" target="#b32">[32]</ref>. A total of 4,151 students answer 325,637 exercises along with 110 distinct exercise tags.</p><p>ASSISTments2015: ASSISTments2015<ref type="foot" target="#foot_2">3</ref> only contains student responses on 100 skills. After preprocessing (removing the value of correct / ∈ {0, 1}), 683,801 effective records from 19,840 students are remained in this dataset. Each problem set in this dataset has one associated skill. Although this dataset has the largest number of records, the average records for each student are the lowest.</p><p>Statics2011: Statics<ref type="foot" target="#foot_3">4</ref> is from a college-level engineering statics course with 189,297 trials, 333 students and 1,223 exercises tags <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b12">12]</ref>. In our experiments, a concatenation of problem name and step name is used as an exercise tag; thus it has the maximum number of exercise tags and the maximum number of average records per student.</p><p>The complete statistical information for all datasets can be found in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>The input exercise data are presented to neural networks using "one-hot" input vectors. Specifically, if Q different exercises exist in total, then the exercise tag qt for the key memory part is a length Q vector whose entries are all zero except for the q th t entry, which is one. Similarly, the combined input xt = (qt, rt) for the value matrix component is a length 2Q vector, where entry xt = qt + rt * Q is one.</p><p>We learn the initial value of both the key and the value matrix in the training process. Each slot of the key memory is the concept embedding and is fixed in the testing process. Meanwhile, the initial value of the value memory is the initial state of each concept, which represents the initial difficulty of each concept.</p><p>Table <ref type="table" target="#tab_3">2</ref>: Comparison of DKVMN with DKT on four datasets with different numbers of state dimensions and memory size N . "s. dim", "m. size" and "p. num" represent the state dimension, memory size (i.e., the number of concepts N ), and the number of parameters, respectively. We choose the state dimensions of 10, 50, 100, and 200 for both DKT and DKVMN. Then for DKVMN, we change memory size for 1, 2, 5, 10, 20, 50, and 100 for each state dimension and report the best test AUC with the corresponding memory size. We likewise compare the number of parameters for both models.  Of all the datasets, 30% of the sequences were held out as a testing set, except for the synthetic dataset where training and testing datasets had the same size. A total 20% of the training set was split to form a validation set, which was used to select the optimal model architecture and hyperparameters and perform early stopping <ref type="bibr" target="#b18">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>The parameters were initialized randomly from a Gaussian distribution with zero mean and standard deviation σ. The initial learning rate was case by case because the number of students, exercise tags, and total answers per dataset varied, but the learning rate γ annealed every 20 epochs by γ/1.5 until the 100th epoch was reached.</p><p>We used LSTM for DKT in our implementation. The standard MANN was implemented using the cosine similarity reading attention mechanism and the LRUA writing attention mechanism. Stochastic gradient descent with momentum and norm clipping <ref type="bibr" target="#b21">[21]</ref> were used to train DKT, MANN, and our DKVMN in all the experiments. We consistently set the momentum to be 0.9 and the norm clipping threshold to be 50.0. Given that input sequences are of different lengths, all sequences were set to be a length of 200 (for synthetic with a length of 50) and a null symbol was used to pad short sequence to a fixed size of 200.</p><p>In all cases, hyperparameters were tuned using the fivefold cross validation. The test area under the curve (AUC) was computed using the model with the highest validation AUC among the 100 epochs. We repeated each training five times with different initializations σ and reported the average test AUC along with the standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Student Performance Prediction</head><p>The AUC is measured to evaluate the prediction accuracy on each dataset. An AUC of 50% represents the score achievable by random guessing. A high AUC score accounts for a high prediction performance. Results of the test AUC on all datasets are shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>We compare the DKVMN model with the MANN baseline, the state-of-the-art DKT, the standard BKT model, and, when possible, optimal variations of BKT (BKT+). An interesting observation is that our implemented LSTM achieves better AUC than those in the original papers <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b32">32]</ref>. The reason may be that our implementations use norm clipping and early stopping, both of which improve the overfitting problem of LSTM. The results of BKT are directly obtained from recent works <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b32">32]</ref>.</p><p>On the Synthetic-5 dataset, the DKVMN model achieves the average test AUC of 82.7%. In our simulation, each exercise is treated as having a distinct skill label. MANN produces an average AUC of 81.0%. DKT produces an AUC value of 80.3%, which is better than the 75% reported in the original paper <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b11">11]</ref>. BKT and its variant model achieve the AUC of 62% and 80% respectively <ref type="bibr" target="#b11">[11]</ref>. The prediction results of DKVMN from the ASSISTments2009 achieve improvement over MANN, DKT, and BKT with 81.6% over 79.7%, 80.5%, and 63% respectively <ref type="bibr" target="#b32">[32]</ref>. As this dataset is preprocessed differently from that in <ref type="bibr" target="#b32">[32]</ref>, their results are not comparable. On the ASSISTments2015 dataset, the test AUC of DKVMN is 72.7%, which is better than 72.3% for MANN, 72.5% for DKT (originally 70% in <ref type="bibr" target="#b32">[32]</ref>), and 64% for Figure <ref type="figure">4</ref>: Concept discovery results on the synthetic-5 dataset when the memory size N is set to be 5. In the left heat map, the x-axis represents each exercise and the y-axis represents the correlation weight between the exercise and five latent concepts generated from our DKVMN model. The ground-true concept is labeled on the top of each exercise. In the right exercise clustering graph, each node number represents an exercise. Exercises from the same ground-truth concept are clustered together. (Best viewed in color.)  (Best viewed in color.)</p><p>classic BKT <ref type="bibr" target="#b32">[32]</ref>. With regard to Statics2011, which has the maximum number of exercise tags and the minimum number of answers, classical BKT gains the AUC of 73% and BKT cooperating with forgetting, skill discovery, and latent abilities obtains an AUC of 75% <ref type="bibr" target="#b11">[11]</ref>. Our implemented DKT leads to an AUC of 80.2%, which is better than the 76% from <ref type="bibr" target="#b11">[11]</ref>. MANN only produces the average AUC of 77.6%. However, our DKVMN model achieves an AUC of 82.8%, outperforming all previous models. In summary, DKVMN performs better than other methods across all the datasets, particularly on the Statics2011 dataset whose number of distinct exercises is large. This result demonstrates that our DKVMN can model student's knowledge well when the number of exercises is very large.</p><p>DKVMN can achieve better prediction accuracy over student exercise performance and also requires considerably fewer parameters than the DKT model because of its large external memory capacity.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Concept Discovery</head><p>Our DKVMN model has the power to discover underlying patterns or concepts for exercises using the correlation weight w, which is traditionally annotated by experts. The correlation weight between the exercise and the concept implies the strength of their inner relationship. Compared with the conditional influence approach in <ref type="bibr" target="#b22">[22]</ref> which computes the dependencies between exercises and then defines a threshold to cluster the exercises, our model directly assigns exercises to concepts. No predefined threshold is required. As a result, our model can discover the concepts of exercises in an end-to-end manner.</p><p>Each exercise is usually associated with a single concept. In this case, we assign the exercise to the concept with the largest correlation weight value. From the experiments, we find that our model can intelligently learn sparse weight among concepts, and the discovered concepts reveal a compelling result.</p><p>On the Synthetic-5 dataset, each exercise is drawn from a concept c k , where k ∈ 1...5, such that the ground truth concept can be accessed for all exercises, as shown on the top x-axis of the heat map in Figure <ref type="figure">4</ref>. Exercises from the same concept are labeled with squares in the same color. The left heat map in Figure <ref type="figure">4</ref> shows the correlation weight between 50 distinct exercises and 5 latent concepts (generated from DKVMN when the memory size is five). Each Moreover, when the memory size N is set to be larger than the ground truth 5, e.g., 50, our model can also end up with 5 exercise clusters and find the appropriate concept for each exercise. Additional results are described in the appendix.</p><p>On the ASSISTments2009 dataset, no ground truth concept is used for each exercise. However, the name for each exercise tag can be obtained, as shown in the right part of Figure <ref type="figure" target="#fig_4">5</ref>. Each exercise tag is followed by a name. The resulting cluster graph in Figure <ref type="figure" target="#fig_4">5</ref> is drawn using t-SNE <ref type="bibr" target="#b15">[15]</ref> by projecting the multi-dimensional correlation weights to the 2-D points. All exercises are grouped into 10 clusters, where the exercises from the same cluster (concept) are labeled in the same color. The clustering graph reveals many reasonable results. Some related exercises are close to one another in the cluster. For example, in the first cluster, 30 Ordering Fractions, 33 Ordering Integers, 37 Ordering Positive Decimals, and 62 Ordering Real Numbers are clustered together, which exposes the concept of elementary arithmetic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Knowledge State Depiction</head><p>Our DKVMN can also be used to depict the changing knowledge state of students. Depicting the knowledge state, especially each concept state, is helpful for the users on online learning platforms. If students possess their concept states of all concepts, which pinpoint their strengths and weaknesses, they will be more motivated to fill in the learning gaps independently. A student's changing knowledge state can be obtained in the read process using the following steps.</p><p>First, the content in the value component is directly used as the read content rt in Eq.( <ref type="formula" target="#formula_2">3</ref>), which can be accessed by setting the correlation weight wt to be [0, .., wi, ..0], where wi of concept c i is equal to 1.</p><p>Then, we mask the weight of the input content embedding in Eq.( <ref type="formula" target="#formula_3">4</ref>) to ignore the information of exercises:</p><formula xml:id="formula_11">ft = Tanh([W r 1 , 0] T [rt, mt] + b1),<label>(11)</label></formula><p>where W1 is split into two parts W r 1 and W m 1 , and let W m 1 = 0.</p><p>Finally, we compute the scalar p as in Eq.( <ref type="formula" target="#formula_4">5</ref>) to be the predictive mastery level of a concept (concept state). Figure <ref type="figure" target="#fig_6">6</ref> shows an example of depicting a student's five changing concept states. The first column represents the initial state of each concept before the student answers any exercise, such state differs from concept to concept. Owing to our model's ability to discover concepts for each exercise, each time the student answers an exercise, the concept state of the discovered concept will increase or decrease. For example, when the student answers the first three exercises correctly, concept states of the second and fifth concepts increase; when the student answers the fourth exercise incorrectly, the concept state of the third concept decreases. After answering 50 exercises, the student is shown to have mastered the second, third, and fourth concepts but failed to understand the fifth concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS AND FUTURE WORK</head><p>This work proposes a new sequence learning model called DKVMN to tackle the KT problem. The model can be implemented in online learning platforms to improve the study efficiency of students. DKVMN not only outperforms the state-of-the-art DKT but can also trace a student's understanding of each concept over time, which is the main drawback of DKT. Compared with standard MANNs, the keyvalue pair allows DKVMN to discover underlying concepts for each input exercise and trace a student's knowledge state of all concepts.</p><p>For future work, we will incorporate content information into the exercise and concept embeddings to further improve the representations. We will also investigate a hierarchical key-value memory networks structure which can encode the hierarchical relationship between concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGEMENT</head><p>The work described in this paper was partially supported by the Research Grants Council of the Hong Kong Special Administrative Region, China (No. CUHK 14208815 of the General Research Fund) and Ministry of Education of China (D.01.16.00101).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Concept Discovery</head><p>When memory size N is set to 50 for the synthetic-5 dataset, the exercises can still be clustered into five categories. The heat map in Figure <ref type="figure" target="#fig_7">7</ref> describes all correlation weight vectors, which only fall into several concepts. After clustering each exercise to the concept with the maximum weight value, the adjusted mutual information of our clustering result and the ground truth is 0.879. Additionally, if we cluster using t-SNE (in Figure <ref type="figure" target="#fig_7">7</ref>), then the adjusted mutual information will be 1.0, which reveals a perfect result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Read Attention Mechanism of MANN</head><p>For each input key kt, we compute the cosine similarity of the key and memory:</p><formula xml:id="formula_12">K[kt, Mt(i)] = kt • Mt(i) kt • Mt(i) ,<label>(12)</label></formula><p>which is then used to compute the read weight w r through a softmax with a positive key strength βt: w r t (i) = exp(βtK[kt, Mt(i)]) j exp(βtK[kt, Mt(j)])</p><formula xml:id="formula_13">. (<label>13</label></formula><formula xml:id="formula_14">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Write Attention Mechanism of MANN</head><p>The LRUA model <ref type="bibr" target="#b24">[24]</ref> writes the keys either to the least used memory location or the most recently used memory location.</p><p>First, a usage weight vector w u t is used to record the usage frequency of all memories: The usage weights are updated at each time-step by decaying the previous usage weights and adding the current reading and writing weights:</p><formula xml:id="formula_15">w u t = γw u t-1 + w r t + w w t , (<label>14</label></formula><formula xml:id="formula_16">)</formula><p>where γ is a decay parameter. γ is fixed to be 0.9 in our implementation. Then the least-used weight w lu t is defined to record the least-used memories using a notation m(v, n), which denotes n th smallest element of the vector v,</p><formula xml:id="formula_17">w lu t (i) = 0 if w u t (i) &gt; m(w u t , n) 1 if w u t (i) ≤ m(w u t , n),<label>(15)</label></formula><p>where n is set to equal the number of reads to memory. Now the write weight w w t is the convex combination of the previous read weights and previous least-used weights:</p><formula xml:id="formula_18">w w t = σ(α)w r t-1 + (1 -σ(α))w lu t-1 ,<label>(16)</label></formula><p>where σ(•) is a sigmoid function, and α is a scalar gate parameter to interpolate between two weights.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Model differences among BKT, DKT, and our model. BKT is concept specific. DKT uses a summarized hidden vector to model the knowledge state. Our model maintains the concept state for each concept simultaneously and all concept states constitute the knowledge state of a student.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Architecture for Memory-Augmented Neural Networks. Architecture for Dynamic Key-Value Memory Networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: In both architecture, the model is only drawn at the timestamp t, where the purple components describe the read process and the green components describe the write process. The blue components in the DKVMN model denote the attention process to compute the corresponding weight. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Validation AUC and training AUC of DKVMN and DKT on all datasets. The blue line represents the DKT model, and the red line represents our DKVMN model. The dotted line represents the training AUC and the line with upper triangles represents the validation AUC. (Best viewed in color.)</figDesc><graphic coords="6,59.24,249.34,120.51,101.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Concept discovery results on the ASSSISTments2009 dataset. 110 exercises are clustered into ten concepts. Exercises under the same concept are labeled in the same color in the left picture and also are put in the same block in the right table.(Best viewed in color.)</figDesc><graphic coords="7,55.20,182.22,145.61,144.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>For instance, on the Statics2011 dataset, DKT reaches the maximum test AUC of 80.20% when the dimension of states equals 200 using 1 million parameters. Meanwhile, DKVMN can achieve the test AUC of 82.84% only with 50 state dimensions using 197 thousand parameters. Moreover, the DKT model suffers severe overfitting, whereas our DKVMN model does not confront such a problem. As indicated in Figure 3, no huge gap exists between the training AUC and the validation AUC of DKVMN, and the val-idation AUC of DKVMN increases smoothly. However, as the epoch proceeds, the training AUC of DKT increases continuously, and the validation AUC of DKT only increases in the first several epochs and begins to decrease.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: An example of a student's changing knowledge state on 5 concepts. Concepts are marked in different colors on the left side. After answering 50 exercises, the student masters the second, third, and fourth concepts but fails to understand the fifth concept. (Best viewed in color.)</figDesc><graphic coords="8,78.91,53.80,451.91,78.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Concept discovery results on the synthetic-5 dataset when the memory size N is set to 50. In the heat map, the x-axis represents each exercise and the y-axis represents the correlation weight between the exercise and five latent concepts generated from our DKVMN model. In the below exercise clustering graph using t-SNE, each node number represents an exercise. Exercises from the same groundtruth concept are clustered together. (Best viewed in color.)</figDesc><graphic coords="9,53.80,359.89,241.02,216.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Test AUC results for all datasets. BKT is the standard BKT. BKT+ is the best-reported result with BKT variations. DKT is the result using LSTM. MANN is the baseline using a single memory matrix. DKVMN is our model.</figDesc><table><row><cell>Datasets</cell><cell cols="2">Overview Students Exercise Tags Records</cell><cell cols="2">BKT BKT+</cell><cell>Test AUC (%) DKT MANN</cell><cell>DKVMN</cell></row><row><cell>Synthetic-5</cell><cell>4,000</cell><cell>50 200,000</cell><cell>62</cell><cell cols="2">80 80.3±0.1 81.0±0.1 82.7±0.1</cell></row><row><cell>ASSISTments2009</cell><cell>4,151</cell><cell>110 325,637</cell><cell>63</cell><cell cols="2">-80.5±0.2 79.7±0.1 81.6±0.1</cell></row><row><cell>ASSISTments2015</cell><cell>19,840</cell><cell>100 683,801</cell><cell>64</cell><cell cols="2">-72.5±0.1 72.3±0.2 72.7±0.1</cell></row><row><cell>Statics2011</cell><cell>333</cell><cell>1,223 189,297</cell><cell>73</cell><cell cols="2">75 80.2±0.2 77.6±0.1 82.8±0.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Ruler or Scale 71 Angles on Parallel Lines Cut by a Transversal 93 Reflection 107 Parts of a Polyomial, Coefficient, Exponent 66 Write Linear Equation from Graph 95 Midpoint 72 Write Linear Equation from Ordered Pairs</figDesc><table><row><cell>17 Scatter Plot</cell><cell>1 Area Trapezoid</cell><cell>3 Probability of Two Distinct Events</cell><cell>24 Addition and Subtraction Fractions</cell></row><row><cell>21 Multiplication and Division Integers</cell><cell>11 Histogram as Table or Graph</cell><cell>4 Table</cell><cell>28 Calculations with Similar Figures</cell></row><row><cell>23 Absolute Value</cell><cell>47 Percent Discount</cell><cell>5 Median</cell><cell>53 Interior Angles Figures with More than 3 Sides</cell></row><row><cell>25 Subtraction Whole Numbers</cell><cell>54 Interior Angles Triangle</cell><cell>6 Stem and Leaf Plot</cell><cell>58 Solving for a variable</cell></row><row><cell>30 Ordering Fractions</cell><cell>64 Surface Area Rectangular Prism</cell><cell>7 Mode</cell><cell>77 Number Line</cell></row><row><cell>33 Ordering Integers</cell><cell>67 Percents</cell><cell>8 Mean</cell><cell>79 Solving Inequalities</cell></row><row><cell>37 Ordering Positive Decimals</cell><cell>68 Area Circle</cell><cell>9 Range</cell><cell>84 Effect of Changing Dimensions of a Shape</cell></row><row><cell>42 Pattern Finding</cell><cell>69 Least Common Multiple</cell><cell>10 Venn Diagram</cell><cell>105 Finding Slope from Ordered Pairs</cell></row><row><cell>43 Write Linear Equation from Situation</cell><cell>73 Prime Number</cell><cell>12 Circle Graph</cell><cell>2 Area Irregular Figure</cell></row><row><cell>44 Square Root</cell><cell>76 Computation with Real Numbers</cell><cell>13 Equivalent Fractions</cell><cell>48 Nets of 3D Figures</cell></row><row><cell>46 Algebraic Solving</cell><cell>78 Rate</cell><cell>14 Proportion</cell><cell>65 Scientific Notation</cell></row><row><cell>52 Congruence</cell><cell>85 Surface Area Cylinder</cell><cell>15 Fraction Of</cell><cell>83 Area Parallelogram</cell></row><row><cell>61 Estimation</cell><cell>88 Solving Systems of Linear Equations</cell><cell>16 Probability of a Single Event</cell><cell>86 Volume Cylinder</cell></row><row><cell>62 Ordering Real Numbers</cell><cell>89 Solving Systems of Linear Equations by Graphing</cell><cell>22 Addition Whole Numbers</cell><cell>92 Rotations</cell></row><row><cell cols="2">74 Multiplication and Division Positive Decimals 96 Interpreting Coordinate Graphs</cell><cell>29 Counting Methods</cell><cell>94 Translations</cell></row><row><cell>81 Area Rectangle</cell><cell>108 Recognize Quadratic Pattern</cell><cell>32 Box and Whisker</cell><cell>101 Angles -Obtuse, Acute, and Right</cell></row><row><cell cols="2">97 Choose an Equation from Given Information 110 Quadratic Formula to Solve Quadratic Equation</cell><cell>35 Percent Of</cell><cell>103 Recognize Linear Pattern</cell></row><row><cell>98 Intercept</cell><cell>18 Addition and Subtraction Positive Decimals</cell><cell>39 Volume Rectangular Prism</cell><cell>31 Circumference</cell></row><row><cell>99 Linear Equations</cell><cell>26 Equation Solving Two or Fewer Steps</cell><cell>41 Finding Percents</cell><cell>70 Equation Solving More Than Two Steps</cell></row><row><cell>100 Slope</cell><cell>36 Unit Rate</cell><cell>51 Understanding Concept Of Probabilities</cell><cell>75 Volume Sphere</cell></row><row><cell>102 Distributive Property</cell><cell>40 Order of Operations All</cell><cell>19 Multiplication Fractions</cell><cell>82 Area Triangle</cell></row><row><cell>109 Finding Slope From Equation</cell><cell>49 Complementary and Supplementary Angles</cell><cell cols="2">27 Order of Operations +,-,/,* () positive reals 91 Polynomial Factors</cell></row><row><cell>20 Addition and Subtraction Integers</cell><cell>59 Exponents</cell><cell>38 Rounding</cell><cell>104 Simplifying Expressions positive exponents</cell></row><row><cell>34 Conversion of Fraction Decimals Percents</cell><cell>80 Unit Conversion Within a System</cell><cell>60 Division Fractions</cell><cell>45 Algebraic Simplification</cell></row><row><cell>50 Pythagorean Theorem</cell><cell>87 Greatest Common Factor</cell><cell>63 Scale Factor</cell><cell>55 Divisibility Rules</cell></row><row><cell>57 Perimeter of a Polygon</cell><cell>90 Multiplication Whole Numbers</cell><cell>106 Finding Slope From Situation</cell><cell>56 Reading a</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>compares the DKVMN model with the DKT model using LSTM by traversing different hyperparameters. The table reveals that DKVMN with low state dimensions can achieve better prediction accuracy than DKT with high state dimensions.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Synthetic-5:https://github.com/chrispiech/ DeepKnowledgeTracing/tree/master/data/synthetic</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>ASSISTments2009:https://sites.google.com/site/ assistmentsdata/home/assistment-2009-2010-data/ skill-builder-data-2009-2010</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>ASSISTments2015:https://sites. google.com/site/assistmentsdata/home/ 2015-assistments-skill-builder-data</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Statics2011:https://pslcdatashop.web.cmu.edu/ DatasetInfo?datasetId=507</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Large-scale simple question answering with memory networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02075</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems, Workshop on Machine Learning Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Knowledge tracing: Modeling the acquisition of procedural knowledge</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Corbett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="253" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">More accurate student modeling through contextual estimation of slip and guess probabilities in bayesian knowledge tracing</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Corbett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Aleven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Tutoring Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="406" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Addressing the assessment challenge with an online system that tutors as it assesses</title>
		<author>
			<persName><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heffernan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Koedinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">User Modeling and User-Adapted Interaction</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="266" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grabska-Barwińska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Agapiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7626</biblScope>
			<biblScope unit="page" from="471" to="476" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to transduce with unbounded memory</title>
		<author>
			<persName><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1828" to="1836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inferring algorithmic patterns with stack-augmented recurrent nets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="190" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">How deep is knowledge tracing?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Khajah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Lindsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Mozer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Educational Data Mining</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A data repository for the edm community: The pslc datashop. Handbook of educational data mining</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Koedinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Skogsholm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stamper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">43</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-H</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03126</idno>
		<title level="m">Key-value memory networks for directly reading documents</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Neural networks: tricks of the trade</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling individualization in a bayesian networks implementation of knowledge tracing</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">A</forename><surname>Pardos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Heffernan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">User Modeling, Adaptation, and Personalization</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="255" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kt-idem: Introducing item difficulty to the knowledge tracing model</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">A</forename><surname>Pardos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Heffernan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">User Modeling, Adaption and Personalization</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="243" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 30th International Conference on Machine Learning</title>
		<meeting>The 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep knowledge tracing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Piech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="505" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Student modelling based on belief networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Reye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Artificial Intelligence in Education</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="96" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Oli engineering statics -fall</title>
		<author>
			<persName><forename type="first">P</forename><surname>Steif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-02">2011. Feb. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Alignment by maximization of mutual information</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<title level="m">Memory networks. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Estimating student proficiency: Deep learning is not the panacea</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khajah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Lindsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Karklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Van Inwegen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ekanadham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Beck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems, Workshop on Machine Learning for Education</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Going deeper with deep knowledge tracing</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Van Inwegen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Beck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Educational Data Mining</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Individualized bayesian knowledge tracing models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Yudelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Koedinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence in education</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
