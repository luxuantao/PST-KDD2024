<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Attention Based Spatial-Temporal Graph-to-Sequence Learning for Grounded Video Description</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kai</forename><surname>Shen</surname></persName>
							<email>shenkai@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fangli</forename><surname>Xu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Squirrel AI Learning</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siliang</forename><surname>Tang</surname></persName>
							<email>siliang@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Xiao</surname></persName>
							<email>junx@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
							<email>yzhuang@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Attention Based Spatial-Temporal Graph-to-Sequence Learning for Grounded Video Description</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The task of Grounded Video Description (GVD) is to generate sentences whose objects can be grounded with the bounding boxes in the video frames. Existing works often fail to exploit structural information both in modeling the relationships among the region proposals and in attending them for text generation. To address these issues, we cast the GVD task as a spatial-temporal Graph-to-Sequence learning problem, where we model video frames as spatial-temporal sequence graph in order to better capture implicit structural relationships. In particular, we exploit two ways to construct a sequence graph that captures spatial-temporal correlations among different objects in each frame and further present a novel graph topology refinement technique to discover optimal underlying graph structure. In addition, we also present hierarchical attention mechanism to attend sequence graph in different resolution levels for better generating the sentences. Our extensive experiments demonstrate the effectiveness of our proposed method compared to state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of Grounded video description (GVD) <ref type="bibr" target="#b8">[Zhou et al., 2019]</ref> aims to generate more grounded and accurate descriptions by linking the generated words with the regions in video frames. Compared to conventional video description task that generates a human-like sentence to describe the video contents <ref type="bibr" target="#b7">[Zhou et al., 2018]</ref>, GVD has advantages of modelling the video by objects and associating the generated text with them to describe the video in a high-quality and grounded way.</p><p>However, current state-of-the-art GVD methods often fail to exploit structural information both in two aspects: i) modeling the relationships among the region proposals; and ii) attending them for text generation. On one hand, existing works either encode region proposals independently or using selfattention-based mechanisms <ref type="bibr" target="#b8">[Zhou et al., 2019]</ref>. Therefore, it either fails to consider implicit structural information among the region proposals or needs to handle noisy or fake relationships among objects. In addition, the explicit structural features of objects (eg. spatial, temporal, semantic) which are potentially important to discover the true correlations among the objects, are overlooked using self-attention only.</p><p>On the other hand, when generating sentences, most previous works adopted top-down attention (means the objects are attended equally and individually) to focus on the relevant objects directly, regardless whether the video frames that these objects are located are semantically related in a high level. Although it can reduce the loss of grounding, the structural correlations of the video frames are completely ignored. However, for a specific word generation step, it is more reasonable to focus on a certain segment of the video frames first and then focus on the objects in these frames.</p><p>More recently, the graph-based method for video understanding started attracting more attentions in some close related fields such as image caption <ref type="bibr" target="#b4">[Li et al., 2019b]</ref>. However, due to the complexity of video understanding, there still remains significant challenges to adapt these graph-based approaches into the GVD task. The first challenge comes from the unique properties of video -how to model the spatialtemporal correlations using a graph. So far, the existing way of building a graph for visual contents, such as the scene graph, only focuses on the single static image <ref type="bibr">[Yang et al., 2019]</ref>. The technology that can effectively construct a graph for image sequences like a video is still unclear and worth exploring. Even we can model a video with a graph, another challenge still remains. Due to the temporal redundancy in video frames, similar objects staying in many frames. As a result, the constructed graph can be very noisy since there are many useless edges in the graph. This will mislead the model, and it may learn less discriminative features for downstream tasks such as generating the description. Therefore, the constructed graph structure should be refined according to the downstream tasks.</p><p>To address the aforementioned issues, we cast the GVD task as a graph-to-sequence learning problem and propose Hierarchical Attention based Spatial-Temporal Graphto-Sequence Learning framework (HAST-Graph2Seq) for Grounded Video Description. Specifically, we introduce spatial-temporal sequence graph A to capture the implicit correlations among region proposals, whose topology is ini-tially obtained in pre-processing with or without external knowledge. Furthermore, we train a similarity metric to construct a semantically implicit graph A implicit to refine the noisy initial graph A init through an end-to-end training for learning node (object) embeddings via graph neural networks. For the decoding procedure, we introduce hierarchical graph attention on the refined sequence-graph for description generation by first finding the regions of frames by attending a certain segment of the given video frames and then finding the regions of objects located in these related frames.</p><p>In summary, we highlight our main contributions below:</p><p>• We cast the GVD task as a spatial-temporal Graphto-Sequence learning problem, where we model video frames as sequence graph to better capture implicit spatial-temporal structural relationships. To the best of our knowledge, this is the first time a spatial-temporal Graph-to-Sequence model is presented for GVD task.</p><p>• In particular, we exploit two ways to construct a sequence graph that captures spatial-temporal correlations among different objects in each frames and further present a novel graph topology refinement techniques to discover optimal underlying graph structure.</p><p>• We also present hierarchical attention mechanism to attend sequence graph in different resolution levels for better generating the sentences. The results demonstrate the effectiveness of our proposed method.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visual Description</head><p>With the rapid development of deep learning in CV and NLP, video description begins to generate the description of a video using the attention-based encoder-decoder like architectures <ref type="bibr" target="#b5">[Venugopalan et al., 2015;</ref><ref type="bibr" target="#b5">Xu et al., 2018b;</ref><ref type="bibr" target="#b4">Liu et al., 2016]</ref>. These methods are effective but they overlook the fine-grained object clues that separated in frames.</p><p>Borrowing the ideas of spatial-attention in image caption domain <ref type="bibr" target="#b0">[Anderson et al., 2018;</ref><ref type="bibr" target="#b4">Liu et al., 2018;</ref><ref type="bibr" target="#b3">Li et al., 2019a]</ref>, many works model the video in both global video features and regional object features. In <ref type="bibr" target="#b8">[Zhou et al., 2019]</ref>, they encode the objects with transformer <ref type="bibr">[Vaswani et al., 2017]</ref> and then link the words of generated descriptions with the clues in certain regions in the video to generate descriptions more grounded. However, since not all objects are key for generating and not all objects have much to do with others, the methods like self-attention may confuse the model. Therefore, graph-based methods which model the regions with abundant semantic relations are introduced to this area. <ref type="bibr" target="#b6">[Yao et al., 2018]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph-to-sequence Learning</head><p>Graph-to-sequence learning has been surge of interests recently in the NLP domain. The main goal for graph-tosequence learning is to generate sequential content from graph structured data, which learns a mapping between graph inputs to sequence outputs through attention-based mechanisms <ref type="bibr" target="#b5">[Xu et al., 2018a;</ref><ref type="bibr" target="#b2">Chen et al., 2020;</ref><ref type="bibr" target="#b2">Gao et al., 2019]</ref>. However, since there is no explicit graph structure for video, it is hard to adapt these methods directly. Unlike these previous methods, we propose a novel Hierarchical Attention based Spatial-Temporal Graph-to-Sequence Learning framework considering both the modeling and the usage of regions in encoder and decoder, including initial graph construction, noisy initial topology refinement and attending on the graph hierarchically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HAST-Graph2Seq Framework for GVD</head><p>The GVD task aims to generate a text description S gt from a video segment denoted as V. In training stage, we will uniformly sample F frames from each video segment as V sample = {v 1 , v 2 , ..., v F }, and provide N gt object regions in V sample which are corresponding to words in S gt . But object regions will not be given in the inference stage.</p><p>To make the statement clear, we will give the mathematical notations of the concept mentioned. The video segment is denoted as</p><formula xml:id="formula_0">v = {v i } n i=1 ∈ V. The target sentence is s = {s i } m i=1</formula><p>∈ S, m is the length of the sentence. And we define N f object regions of each sampled frames in V sample which are denoted as R = {R 1 , R 2 , ..., R F } = {r 1 , r 2 , ..., r N } ∈ R d×N , where d is the dimension of the proposals and N = F f =1 N f is the amounts of the proposals. As Figure <ref type="figure" target="#fig_0">1</ref> illustrates, we encode the video in two streams. Firstly, we encode the global video features in the Video Global Encoder (Figure <ref type="figure" target="#fig_0">1 a</ref>). Then we encode the regions by spatial-temporal sequence graph whose topology will be refined in Graph with Refinement Encoder (Figure <ref type="figure" target="#fig_0">1 b</ref>). Finally, we adapt top-down attention by applying temporal attention to global video features and hierarchical graph attention on the spatial-temporal sequence graph in the Language Decoder (Figure <ref type="figure" target="#fig_0">1 c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Video Global Encoder</head><p>We model the video's global level feature by a Bi-directional LSTM network like most works <ref type="bibr" target="#b8">[Zhou et al., 2019]</ref> given by: h = BiLST M (v) = {h 1 , h 2 , ..., h m } where v ∈ R n×d is the global feature extracted by a pre-trained 3D-ConvNet <ref type="bibr" target="#b4">[Tran et al., 2015]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph with Refinement Encoder</head><p>In this section, we propose a novel visual representation method from the perspective of regions. First of all, inspired by <ref type="bibr" target="#b8">[Zhou et al., 2019]</ref>, we enhance the proposal features by adding the position and class features. As for proposal modeling, we propose a novel spatial-temporal sequence graph data structure, whose initial topology is obtained before training and refined in end-to-end manner. Notably, the initial topology can be obtained with or without prior knowledge considering the generality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Enhancement</head><p>In this part, we follow <ref type="bibr" target="#b8">[Zhou et al., 2019]</ref>'s work, which fusing the spatial-temporal and class features with the original features to enrich them.</p><p>(1) For each proposal, we define its' spatial and temporal information as a 5-D list, 4 values for normalized spatial location and 1 value for the normalized frame index. Then we project it to a d sp dimension space. So, the spatial-temporal features of region proposals are denoted as M sp .</p><p>(2) We assume that each region proposal r i has a class label c i ∈ {c 1 , c 2 , ..., c k }. We transfer detection model's weight which is pre-trained on VG dataset to initialize the class embedding denoted as W c ∈ R d×k and B c ∈ R 1×k , where d is the embedding dimension. Then we use a attention method to assign each region proposal a class representation:</p><formula xml:id="formula_1">M r (r i ) = Sof tmax(W T c r i + B c I T )</formula><p>To sum up, the region feature will be given by:</p><formula xml:id="formula_2">R = W p [R|M sp |M r ]</formula><p>(1) where [ | ] denotes row-wise concatenation and W p ∈ R l×(d+k+dsp) is the embedding weight. Then we will apply feature aggregation on the enhanced feature R.</p><p>We adopt the same classification loss just as <ref type="bibr" target="#b8">[Zhou et al., 2019]</ref> do denoted as L cls .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial-temporal Sequence-Graph Data Structure</head><p>Here we will define the data structure for region proposals. We will view each proposal (r i ) as a node (r i ) in the video graph. To make the problem easier, we will assume that the graph holds the following principles.</p><p>(1) Instead of modeling it as a fully connected graph, we assume that the graph hold the locality: each node r i in sampled frame v f will only have connection (if exist) with nodes in v f −1 , v f , v f +1 . Through this operation, we capture the local spatial relations in single frames and the local temporal relations between frames. What's more, we define the nodes in one single frame as a sub-graph, which consists of the whole graph through temporal edges.</p><p>(2) For simplification, we assume the final graph topology is undirected and weighted. However, since we introduce the spatial-temporal information into the node feature space, this assumption will not cause excessive loss of the key position and temporal characteristics. And it is weighted because we want the edges between nodes to be more meaningful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initial Graph Topology</head><p>By the constraints above, there are several potential methods to form a graph. Since they are formed during pre-processing, they may contain noise.</p><p>(1) Without external knowledge: KNN. If we have no prior knowledge of the given regions, a way is to find the correlations in feature space. For each node r i ∈ v f , we will find</p><formula xml:id="formula_3">p nodes R p = {r 1 , r 2 , ..., r p } ∈ {R f −1 , R f , R f +1 } by KNN</formula><p>Algorithm and add edges between r i and R p .</p><p>(2) With external knowledge method: Relation Graph. Since the region features are extracted by a pre-trained model trained on VG <ref type="bibr" target="#b2">[Krishna et al., 2017]</ref> dataset, we can train a semantic relation classifier <ref type="bibr" target="#b4">[Li et al., 2019b]</ref> on it. We adopt almost the same operation except replacing the KNN step by the classifier to find the related nodes set R p .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Refinement Procedure</head><p>The graph A init obtained above is noisy but can be refined during the training process.</p><p>Empirically, a powerful metric of relation should be learned from specific task. Inspired by <ref type="bibr">[Chen et al., 2019;</ref><ref type="bibr">Vaswani et al., 2017]</ref>, we design a multi-head weighted cosine similarity metric function:</p><formula xml:id="formula_4">A implicit[i,j] = 1 m m k=1 cos(w k ri , w k rj ) (2)</formula><p>where denotes the Hadamard product, w ∈ R m is the learnable weights, m is the heads number, ri ∈ R and A implicit is the implicit graph. We assume that by highlighting some specific dimensions of the region features, we can find the implicit relations beneficial to the task.</p><p>Here we adopt the same principles as the initial graph. So we drop the connections if they are against principles (1).</p><p>After that, we prune the implicit graph by a threshold , which means selecting the useful relations and drop the unimportant to make the graph sparse.</p><formula xml:id="formula_5">A i,j = A(i, j) * I(A(i, j) &gt; ) (<label>3</label></formula><formula xml:id="formula_6">)</formula><p>where I is the indicator function. Then we fuse the initial graph with the implicit graph as follows:</p><formula xml:id="formula_7">A dir = λ Âinitial + (1 − λ) Âimplicit (4)</formula><p>where λ is the hyper-parameter to balance the trade-off between the initial graph and the learned implicit graph. The Âinitial and the Âimplicit are normalized adjacency matrix of the initial graph and implicit graph. The normalization is defined as: Â = D −1/2 AD −1/2 and D is the degree matrix.</p><p>To make the graph undirected, the final adjacency matrix is given by: A = ( A dir + A T dir )/2 Feature Aggregation We adapt the classic spectral graph convolutional network to aggregate the features of the nodes modeled by topology A.</p><p>Inspired by resnet architecture <ref type="bibr" target="#b2">[He et al., 2016]</ref>, we propose the basic module of our architecture as follows (the layer normalization and dropout operations are omitted):</p><formula xml:id="formula_8">X out = (σ( AX in W) + X in )/ √ 2 (5)</formula><p>where the X in is the input ( R in Eq.1), A denotes the normalized adjacency matrix, W is the trainable weights and σ is the non-linear activation function. And we will stack k basic modules to explore deep correlations of the graph. We denote the regions after aggregation as R for further illustration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Language Decoder</head><p>In this section, we adapt the top-down attention language model for description generation. The attention LSTM is used to encode the visual features and the language LSTM is used to generate words. Between these two LSTMs, we attend on the global video features on temporal level and apply hierarchical graph attention on spatial-temporal sequence graph to capture the visual object clues in different grains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention LSTM</head><p>At time step t, we will fuse the hidden state in t − 1 which denoted as h t−1 with the pooled frame features v pool to generate a new hidden state h 1 t ∈ R r .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal Attention</head><p>Firstly, we will attend the global frame features in a coarsegrained way. When generating a new word, we should pay different weights on different frames. We denote the results as h f rame ∈ R r .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hierarchical Graph Attention</head><p>When handling the proposal regions, instead of attending each region equally, we propose hierarchical attention on the sequence-graph to hold the graph structure.</p><p>Firstly, we will attend on the sub-graph to capture the general area of the video. The sub-graph R i can be represented by</p><formula xml:id="formula_9">R = { R 1 , R 2 , ..., R F } = { r 1 , r 2 , ..., r N }.</formula><p>And then we apply mean-pooling to get the vector representation of each sub-graph given by: R i = M eanP ooling( r k:l ). r k:l denotes regions from r k to r l belong to sub-graph R i . Thus R ∈ R F ×l . Then we execute graph-level attention:</p><formula xml:id="formula_10">M( R i , h 1 t ) = W T tanh(W 1 R i + W 2 h 1 t ) (6)</formula><p>where</p><formula xml:id="formula_11">W 1 ∈ R o×l , W 2 ∈ R o×r . And W ∈ R o is a row vector.</formula><p>Then we apply softmax on M, given by:</p><formula xml:id="formula_12">α i = exp(M( R i , h 1 t )) F j=1 exp(M( R j , h 1 t ))<label>(7)</label></formula><p>Therefore, we can get the results denoted as α = {α 1 , α 2 , ..., α F } ∈ R F Secondly, we apply attention on each sub-graph parallelly. For each sub-graph R f ∈ { R 1 , R 2 , ..., R F }, we apply the same operation as in Eq.6 and Eq.7. For each R f , we can get a attention score β f ∈ R N f . So for all frames, the score are represented as</p><formula xml:id="formula_13">β = {β 1 , β 2 , ..., β F } ∈ R F ×N f</formula><p>Finally, we fuse the regions given by:</p><formula xml:id="formula_14">h attention = F i=1 α i Ni j=1 β i,j R i,j<label>(8)</label></formula><p>where h attention ∈ R l . Then we apply linear projection to project it to r dimension space. We adapt the same attention supervision here on both node (region) level and sub-graph (frame) level. Firstly, we define the region is positive if it has over 0.5 IOU (intersection over union) with any ground-truth bounding box. Then we apply cross-entropy loss on β. Besides focusing the correct regions, we also want the visual-groundable word to focus on the correct frames. So, we define a frame is positive if it has at least one positive region. Then we apply the same cross-entropy loss on α too.</p><formula xml:id="formula_15">L a attn = − F i=1 N f j=1 I i,j logβ i,j , L b attn = − F i=1 J i logα i , (<label>9</label></formula><formula xml:id="formula_16">)</formula><p>where I i,j = 1 only if this region is positive. J i = 1 only if this sub-graph is positive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language LSTM</head><p>The language LSTM is adopted to generate the words while absorbing the visual clues given by: h t = LST M (h<ref type="foot" target="#foot_1">1</ref> t , h f rame + h attention ). h t is used to generate descriptions. We adopt the same MLE loss as <ref type="bibr" target="#b8">[Zhou et al., 2019]</ref> which denoted by L sent .</p><p>Finally, the overall loss function consists of four parts:</p><formula xml:id="formula_17">L = L sent + λ a L a attn + λ b L b attn + λ c L cls<label>(10)</label></formula><p>4 Experiment</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We conduct our experiments on the Grounded ActivityNet-Entities Dataset <ref type="bibr" target="#b8">[Zhou et al., 2019]</ref> for evaluation. It contains 15k video with 158k spatially annotated bounding boxes from 52k video segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>In this section, we introduce some implementation details of our HAST-Graph2Seq method. Data processing. For a fair comparison, the data processing procedure is the same to <ref type="bibr" target="#b8">[Zhou et al., 2019]</ref>. For each video segment in the dataset, we uniformly sample 10 frames. And for each frame, we use a Faster R-CNN <ref type="bibr" target="#b4">[Ren et al., 2015]</ref> detector with ResNeXt-101 backbone to detect 100 region proposals and extract the feature. The detector is pre-trained on Visual Genome <ref type="bibr" target="#b2">[Krishna et al., 2017]</ref>. Finally, for the video feature, the temporal feature map is a stack of frame-wise appearance and motion features. Hyperparameter settings. We set the threshold value in Eq.3 to 0.4, λ a to 0.04, λ b to 0.08, λ c to 0.5. and number of heads m in Eq.2 to 5. The KNN hyper-parameter p ∈ {5, 10, 20, 30, 40} vary in the experiments as a results of model validation. The region proposal feature's original dimension d is 2048, the region proposals' embedding dimension l is 1024, the word embedding size is 512, rnn hidden size r is 1024 and GCN's layer k is 3. The λ in Eq.4 is 0.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Criteria</head><p>To measure the performance of our HAST-Graph2Seq model and other baselines, we consider two categories of evaluation criteria from the description generation quality and grounding accuracy respectively. Description generation quality. We use 4 widely used metrics to evaluate the description generation quality. They are BLEU@4, METEOR, CIDEr and SPICE. These scores are calculated by the official evaluation scripts 1 . Grounding accuracy. Grounding accuracy is another metric to measure if a model can correctly predict both object words and their locations in video frames. It is measured by F1 all and F1 loc. The F1 all score measures the object words if they are correctly predicted and localized. And the F1 loc score only measures the correctly predicted object words. We also use the official evaluation scripts<ref type="foot" target="#foot_2">2</ref> <ref type="foot" target="#foot_3">3</ref> to measure all of these scores. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance Comparisons</head><p>We compare HAST-Graph2Seq with the SOTA models, i.e., Masked Transformer <ref type="bibr" target="#b7">[Zhou et al., 2018]</ref>, BiM-STM+TempoAtnn <ref type="bibr" target="#b7">[Zhou et al., 2018]</ref> and ZhouGVD <ref type="bibr" target="#b8">[Zhou et al., 2019]</ref> on Grounded ActivityNet Captions Dataset to verify the effectiveness of our method. Moreover, since the initial graph of the HAST-Graph2Seq can be constructed in two different ways, we also create two variants of HAST-Graph2Seq, i.e., KNN-HAST (the KNN initial graph with p is set to 10) and RG-HAST (the relational initial graph) to further investigate the relation between the different graph initializations and the final performance. For a fair comparison, we use the same C-3D video feature and the same region proposals extracted by Faster-RCNN pre-trained on Visual Genome (VG). For all these methods we performed the same experiments 3 times, and we reported their average scores.</p><p>As shown in Table <ref type="table">1</ref>, our methods outperform all the state-of-the-art models on all metrics, especially on CIDEr, BLEU@4, and SPICE, which highlights the importance of modeling relationships among the region proposal and using these relations for video description generation. Moreover, we can observe that the RG initialization outperforms the KNN initialization on most metrics. This suggests that an initial graph with external commonsense knowledge is beneficial to better modeling the relations among the regions and further improve the generation performance.</p><p>We further investigate the effect of KNN initialization with different number of neighbors by varying the KNN parameter p in 5, 10, 20, 30, 40 on the validate set (the test set is not released so conducting on it is time-consuming). Table <ref type="table" target="#tab_1">2</ref> shows how different KNN initialization effects on the final performance. From Table <ref type="table" target="#tab_1">2</ref>, we note that the HAST-Graph2Seq achieves the best performance when p lies around 5 − 20. This suggests that for KNN initialization a proper p is crucial. When the p is too small, the initial graph may contain less useful relations, while when the p is too large, the graph</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initial graph</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Refined graph</head><p>Our method: A man is seen decorating a Christmas tree with a man beside him. ZhouGVD: A man is decorating a Christmas tree. GT: They are going around the tree adding lights to it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main sub-graph</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B A B</head><p>The edge weight decrease</p><p>The missing edge arise</p><p>The edge weight increase  may contain too much noise to be refined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>Next we conduct ablation studies to show how initial graph construction, graph refinement, and hierarchical attention contribute to the proposed method on the validate set. Without loss of generality, we consider KNN with p = 10 for initial graph construction. More concretely, we will discard one component at a time to generate ablation models as follows:</p><p>(1) w/o. initial graph (abbr: -init.). We remove the initial graph and use implicit graph generated by learned metrics.</p><p>(2) w/o. refinement (abbr: -refine). We remove the graph refinement component and use the KNN with p = 10 as the initial graph individually.</p><p>(3) w/o. hierarchical attention (abbr: -hie. attn.). We remove the hierarchical attention and replace it with the coarsegrain proposal attention proposed by <ref type="bibr" target="#b8">[Zhou et al., 2019]</ref>.</p><p>Table <ref type="table" target="#tab_2">3</ref> gives all ablation results on the validation set. As shown in Table <ref type="table" target="#tab_2">3</ref>, the HAST outperforms all ablation models on all metrics, which demonstrates that the initial graph construction, graph refinement, and hierarchical graph attention are all useful components for GVD.</p><p>Finally, by comparing among the ablation models, we find that a model without the initial graph performs the worst. This indicates that an good initial graph plays important role when exploiting spatial-temporal correlations in the video frames. The experiments also suggest that through refinement, the noise contains in the initial graph can be further reduced and the better graph topology can be discovered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Qualitative Analysis</head><p>To qualitatively validate the effectiveness of our proposed HAST-Graph2Seq network, we present one typical example. Figure <ref type="figure" target="#fig_1">2</ref> shows the description results of our method, the best baseline ZhouGVD and the ground-truth on the Grounded ActivityNet Caption dataset, respectively. We can find that the baseline method misses the man B beside the man A, while our method can find the correct relation between them. The reason lies in two aspects. Firstly, when generating the first man, our model focused on the second sub-graph and then focused on the objects in it with the help of hierarchical attention. Thus, the model can find the semantic relation with the second man. Secondly, we visualize the initial KNN sequence graph and the refined sequence graph (we just show the second sub-graph as the key role and the main nodes and edges related to it for reasons of brevity). Through refinement, we can see that the weights of the key edges (eg. the man A with the man B) increase while the unimportant ones decrease. Thus, through our graph refinement techniques, we can discover optimal underlying graph structure that is important for video understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel spatial-temporal sequence graph topology refinement with hierarchical attention for grounded video description task, which model the regions with spatial-temporal sequence graph. Specifically, we propose several methods to build the initial topology and refine it through end-to-end training. In addition, during decoding we apply hierarchical attention on the graph to focus on the regions in different gains. The extensive experiments demonstrate the effectiveness of our proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overall framework of our HAST-Graph2Seq: (a) The video encoder. (b) The graph refinement module. (c) The language module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Qualitative differences between ZhouGVD and our proposed HAST with visualization of the discovered implicit graph structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on Grounded ActivityNet-Entities val set.</figDesc><table><row><cell>Method</cell><cell>B@4</cell><cell>C</cell><cell>M</cell><cell>S</cell><cell cols="2">F1 all F1 loc</cell></row><row><cell>M. Trans</cell><cell>2.41</cell><cell cols="3">46.1 10.6 13.7</cell><cell>-</cell><cell>-</cell></row><row><cell>Temp-Attn</cell><cell>2.17</cell><cell cols="3">42.2 10.2 11.8</cell><cell>-</cell><cell>-</cell></row><row><cell>ZhouGVD</cell><cell>2.35</cell><cell cols="3">45.5 11.0 14.7</cell><cell>7.59</cell><cell>25.0</cell></row><row><cell>KNN-HAST</cell><cell>2.61</cell><cell cols="3">48.5 11.3 15.1</cell><cell>7.64</cell><cell>26.5</cell></row><row><cell>RG-HAST</cell><cell>2.65</cell><cell cols="3">49.3 11.2 15.2</cell><cell>7.66</cell><cell>26.1</cell></row><row><cell cols="7">Table 1: Results on Grounded ActivityNet-Entities test set. Nota-</cell></row><row><cell cols="7">tions: B@4-BLEU@4, C-CIDEr, M-METEOR, S-SPICE, M.Trans-</cell></row><row><cell cols="7">Masked Transformer, TempAttn-BiLSTM+TempoAttn. All accura-</cell></row><row><cell>cies are in %.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">p (KNN) B@4</cell><cell>C</cell><cell>M</cell><cell>S</cell><cell cols="2">F1 all F1 loc</cell></row><row><cell>5</cell><cell>2.70</cell><cell cols="3">49.4 11.2 15.2</cell><cell>7.04</cell><cell>23.5</cell></row><row><cell>10</cell><cell>2.80</cell><cell cols="3">49.6 11.3 15.3</cell><cell>7.22</cell><cell>24.9</cell></row><row><cell>20</cell><cell>2.76</cell><cell cols="3">49.4 11.3 15.2</cell><cell>6.91</cell><cell>23.4</cell></row><row><cell>30</cell><cell>2.71</cell><cell cols="3">49.3 11.2 14.9</cell><cell>6.89</cell><cell>23.5</cell></row><row><cell>40</cell><cell>2.68</cell><cell cols="3">48.9 11.1 15.1</cell><cell>6.70</cell><cell>23.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results on Ablation Model on ActivityNet val set.</figDesc><table><row><cell>Method</cell><cell>B@4</cell><cell>C</cell><cell>M</cell><cell>S</cell><cell cols="2">F1 all F1 loc</cell></row><row><cell>ZhouGVD</cell><cell>2.59</cell><cell cols="3">47.5 11.2 15.1</cell><cell>7.11</cell><cell>24.1</cell></row><row><cell>KNN-HAST</cell><cell>2.80</cell><cell cols="3">49.4 11.3 15.3</cell><cell>7.22</cell><cell>24.9</cell></row><row><cell>-init.</cell><cell>2.60</cell><cell cols="3">47.4 11.0 14.7</cell><cell>6.65</cell><cell>22.4</cell></row><row><cell>-refine</cell><cell>2.70</cell><cell cols="3">48.1 11.1 14.8</cell><cell>6.83</cell><cell>23.5</cell></row><row><cell>-hie. attn.</cell><cell>2.70</cell><cell cols="3">48.8 11.2 15.0</cell><cell>6.91</cell><cell>23.7</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">https://github.com/ranjaykrishna/densevid eval</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">https://github.com/facebookresearch/ActivityNet-Entities</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">https://competitions.codalab.org/competitions/20537</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been supported in part by National Key Research and Development Program of China (2018AAA010010), NSFC (U1611461, U19B2043, 61751209, 61976185), Zhejiang Natural Science Foundation (LR19F020002, LZ17F020001), University-Tongdun Technology Joint Laboratory of Artificial Intelligence, Zhejiang University iFLYTEK Joint Research Center, Chinese Knowledge Center of Engineering Science and Technology (CKCEST), Engineering Research Center of Digital Library, Ministry of Education, the Fundamental Research Funds for the Central Universities.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName><forename type="first">Anderson</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1912.07832</idno>
		<title level="m">Deep iterative and adaptive learning for graph neural networks</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dyngraph2seq: Dynamicgraph-to-sequence interpretable learning for health stage prediction in online health forums</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2020. 2020. 2019. 2019. 2016. 2016. 2017. 2017</date>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="32" to="73" />
		</imprint>
		<respStmt>
			<orgName>International Journal of Computer Vision</orgName>
		</respStmt>
	</monogr>
	<note>Visual genome: Connecting language and vision using crowdsourced dense image annotations</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unsupervised reinforcement learning of transferable meta-skills for embodied navigation</title>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07450</idno>
		<imprint>
			<date type="published" when="2019">2019a. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12314</idno>
		<idno>arXiv:1912.04316</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<editor>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</editor>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2019b. 2019. 2016. 2016. 2018. 2018. 2015. 2015. 2019. 2019. 2015. 2015. 2017. 2017</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph2seq: Graph to sequence learning with attentionbased neural networks</title>
		<author>
			<persName><surname>Venugopalan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00823</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<editor>
			<persName><forename type="first">Jianfei</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><surname>Cai</surname></persName>
		</editor>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>Xu Yang, Kaihua Tang</publisher>
			<date type="published" when="2015">2015. 2015. 2018. 2018. 2018a. 2018. 2018b. 2018. 2019. 2019</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="10685" to="10694" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the IEEE international conference on computer vision</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Object-aware aggregation with bidirectional temporal graph for video captioning</title>
		<author>
			<persName><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>Zhang and Peng</publisher>
			<date type="published" when="2018">2018. 2018. 2019. 2019</date>
			<biblScope unit="page" from="8327" to="8336" />
		</imprint>
	</monogr>
	<note>Proceedings of the European Conference on Computer Vision (ECCV)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">End-to-end dense video captioning with masked transformer</title>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="8739" to="8748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Grounded video description</title>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="6578" to="6587" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
