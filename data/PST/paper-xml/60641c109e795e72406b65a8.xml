<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Diversification-Aware Learning to Rank using Distributed Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Le</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Qin</surname></persName>
							<email>zhenqin@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rama</forename><forename type="middle">Kumar</forename><surname>Pasumarthi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
							<email>xuanhui@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Ben</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Diversification-Aware Learning to Rank using Distributed Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3442381.3449831</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Information systems â†’ Information retrieval</term>
					<term>search result diversification</term>
					<term>diversification-aware loss</term>
					<term>learning-torank</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing work on search result diversification typically falls into the "next document" paradigm, that is, selecting the next document based on the ones already chosen. A sequential process of selecting documents one-by-one is naturally modeled in learning-based approaches. However, such a process makes the learning difficult because there are an exponential number of ranking lists to consider. Sampling is usually used to reduce the computational complexity but this makes the learning less effective. In this paper, we propose a soft version of the "next document" paradigm in which we associate each document with an approximate rank, and thus the subtopics covered prior to a document can also be estimated. We show that we can derive differentiable diversification-aware losses, which are smooth approximation of diversity metrics like ğ›¼-NDCG, based on these estimates. We further propose to optimize the losses in the learning-to-rank setting using neural distributed representations of queries and documents. Experiments are conducted on the public benchmark TREC datasets. By comparing with an extensive list of baseline methods, we show that our Diversification-Aware LEarning-TO-Rank (DALETOR) approaches outperform them by a large margin, while being much simpler during learning and inference.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Search result diversification is critical for the utility of search engines due to the diverse information needs of users and the ambiguity of short queries. For example, a query about "Eiffel Tower" may seek for its history information or its visiting address. A list of results covering different subtopics is more desired in this case. Indeed, diversification has been a long-standing research topic in the Information Retrieval community, with the seminal work of Maximal Marginal Relevance (MMR) dating back to year 1998 <ref type="bibr" target="#b9">[10]</ref>.</p><p>Since users often do not examine all the returned results thoroughly but only look at a few top ones, the goal of search result diversification is to present relevant but diverse results at the top of a ranked list. These notions are taken into account by commonly used diversity evaluation metrics, including ğ›¼-NDCG <ref type="bibr" target="#b12">[13]</ref>, ERR-IA <ref type="bibr" target="#b10">[11]</ref> (Intent-Aware metrics <ref type="bibr" target="#b0">[1]</ref>), and S-recall <ref type="bibr" target="#b44">[45]</ref>. All of them consider both the ranks of relevant documents and how well the subtopics of a given query are covered by the top ranked documents. The contribution of a subtopic in a lower-ranked document is down-weighted if it has been covered well by top-ranked ones.</p><p>While traditional methods for diversification are mainly manually crafted <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b33">34]</ref>, recent research in this area shifts to supervised learning methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46]</ref> and shows superior performance with respect to the diversity evaluation metrics. However, different from the standard learning-to-rank setting <ref type="bibr" target="#b26">[27]</ref>, the design of learning approaches for diversification is non-trivial due to the inter-dependency among documents. Almost all of them fall into the so-called "next document" paradigm, that is, selecting the next document among the remaining ones to maximize an objective with respect to the ones already chosen. Such a paradigm is intuitively appealing and fits the diversification task naturally. However, the main challenge is that learning is inherently less effective because there is an exponentially large number of ranking lists to consider. Different methods are proposed to alleviate this problem. For example, R-LTR <ref type="bibr" target="#b45">[46]</ref> and SVM-DIV <ref type="bibr" target="#b43">[44]</ref> mainly focus on the ideal diversified ranking lists. Reinforcement learning (RL) based approaches <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b41">42]</ref> try to maximize the expected rewards over sampled lists from a distribution. Recently proposed PAMM <ref type="bibr" target="#b39">[40]</ref> and DVGAN <ref type="bibr" target="#b25">[26]</ref> maximize the margin between sampled positive and negative lists for training and show better performance. However, the huge number of candidates poses challenges for high-quality sampling <ref type="bibr" target="#b25">[26]</ref>.</p><p>The main difficulty of the existing learning-based approaches lies in the hard setting in the "next document" paradigm -the next document is evaluated based on the materialized previously selected documents. The key idea of this paper is to use a soft version where we do not need to materialize ranking lists. Specifically, we compute a differentiable approximate rank and associate it with each document. Given such approximate ranks, we can estimate the subtopic coverage of the documents prior to each document, and then use these estimates for diversification. In particular, we show that we can translate a diversity evaluation metric (e.g., ğ›¼-NDCG) Table <ref type="table">1</ref>: An example illustrating how distributed representation helps optimize diversification-aware ranking losses. ğ‘‘ 1 ğ‘‘ 2 and ğ‘‘ 3 are relevant to subtopic #1 in ğ‘“ 1 . ğ‘‘ 4 is relevant to subtopic #2 in ğ‘“ 2 . The third dimension ğ‘“ 3 is a diversity-useful dimension and the last, ğ‘“ 4 , is an independent dimension.</p><formula xml:id="formula_0">subtopics #1 #2 - distributed dims ğ‘“ 1 ğ‘“ 2 ğ‘“ 3 ğ‘“ 4 ğ‘ 1 1 0 0 ğ‘‘ 1 1 0 1 1 ğ‘‘ 2 1 0 0 1 ğ‘‘ 3 1 0 0 1 ğ‘‘ 4 0 1 0 1</formula><p>into a differentiable loss function based on these estimates. Such a loss function can be trained effectively by gradient descent in an end-to-end fashion.</p><p>With such a diversification-aware loss function, we formulate our problem in a learning-to-rank setting where a ranking function is learned to score and sort documents. To make the learning more effective, instead of using aggregated score features like BM25 or TF-IDF (commonly used in the traditional learning-to-rank setting <ref type="bibr" target="#b26">[27]</ref>), we resort to the distributed representations of queries and documents, where subtopic-relevant dimensions and diversityuseful dimensions can be learned.</p><p>A simple example in Table <ref type="table">1</ref> illustrates this. Suppose there are 2 subtopics encoded in a 4-dimension latent space. Without loss of generality, we define the coordinate with the first two axes ğ‘“ 1 and ğ‘“ 2 aligning with the 2 subtopic vectors. It is then easy to pick out the query and subtopic-relevant documents by looking at the alignments in these dimensions. Suppose we find 4 relevant documents: ğ‘‘ 1 , ğ‘‘ 2 , and ğ‘‘ 3 match subtopic #1; ğ‘‘ 4 matches subtopic #2. An ideal diversified ranking must have ğ‘‘ 4 ranked at either the first or the second position, which can be achieved by assigning different scores to ğ‘‘ 1 , ğ‘‘ 2 , and ğ‘‘ 3 in the "score-and-sort" setting. However, it is not feasible to distinguish ğ‘‘ 1 , ğ‘‘ 2 , and ğ‘‘ 3 by just looking at the alignments with the subtopics. In such case, the remaining dimensions of distributed representations become useful in generating the diversified ranking. In this example, suppose the four documents distributed non-trivially along axis ğ‘“ 3 , a diversification-aware loss function can facilitate learning such distributed representations and output a list [ğ‘‘ 1 , ğ‘‘ 4 , ğ‘‘ 2 , ğ‘‘ 3 ] by favoring a ranking function such as ğ‘“ 1 + 1.5ğ‘“ 2 + ğ‘“ 3 , while a standard loss function may not be able to pick up ğ‘“ 3 , as it has no effect on document relevance.</p><p>Note that once the ranking function has been learned, our method does not require the subtopics of a query to be given during inference and thus belongs to the implicit category. This is different from the explicit approaches (e.g., DVGAN <ref type="bibr" target="#b25">[26]</ref> or DSSA <ref type="bibr" target="#b22">[23]</ref>), which assume the subtopics to be available at inference time -not a realistic assumption for most search engines.</p><p>One obvious problem of the score-and-sort approach is the duplicate documents where two documents have the exact same feature representation and thus would always have the same scores. The benchmark dataset used in our experiments does have many documents covering the same subtopics but they are not duplicates. Without any pre-processing, our methods perform very well on this dataset, confirming that the diversification-aware loss can effectively leverage the difference among documents that are relevant to the same subtopics and yield a diversification-aware ranking function. In reality, this problem can be easily fixed by a pre-processing step where we make sure that all documents for a query have a minimum difference (e.g., using a near-deduplication technique <ref type="bibr" target="#b5">[6]</ref>).</p><p>In summary, we make the following contributions in this paper:</p><p>â€¢ We propose a novel method that can translate a diversity evaluation metric to a differentiable diversification-aware loss.</p><p>â€¢ We show that such a loss function can be effective in learning with the distributed representation using deep neural networks that are efficient and easily extendable. â€¢ We conduct experiments on a public benchmark dataset and show that our proposed method can significantly outperform strong recent baselines.</p><p>The rest of the paper is organized as follows. We review related work in Section 2. Our diversification-aware loss is described in Section 3 and a neural network based learning approach is described in Section 4. In Section 5, we give a theoretical analysis about our diversification-aware loss and distributed representation. We present our experiments in Section 6, our discussion in Section 7 and conclude this paper in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Search Result Diversification</head><p>Diversification approaches can be broadly classified into two approaches: implicit and explicit. Implicit approaches promote novel documents compared to other documents based on inter-document similarity and do not require subtopics given during the evaluation time. Explicit approaches promote documents that improve coverage over specified subtopics.</p><p>Most implicit approaches are inspired by the seminal work of Maximal Marginal Relevance (MMR) method <ref type="bibr" target="#b9">[10]</ref>, which iteratively picks relevant documents that are novel to the document set so far, based on user defined functions for inter-document similarity. Supervised machine learning methods for implicit diversification <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46]</ref> learn a scoring function that optimize diversification using remote proxies of evaluation metrics. SVM-DIV <ref type="bibr" target="#b43">[44]</ref> utilizes structural SVMs for scoring, whereas R-LTR <ref type="bibr" target="#b45">[46]</ref> proposes a relational learning to rank framework, to model document relations of an ideally diversified ranking. Based on R-LTR, PAMM <ref type="bibr" target="#b39">[40]</ref> improves the scoring function by considering difference between positive and negative rankings. NTN-DIV <ref type="bibr" target="#b40">[41]</ref> uses a neural tensor network to learn document similarity automatically instead of using handcrafted features or functions. Reinforcement learning methods have been proposed to improve the greedy nature of sequential document selection: MDP-DIV <ref type="bibr" target="#b41">[42]</ref> uses Markov Decision Processed (MDP) to optimize expected reward over sampled lists; M 2 DIV improves over MDP using Recurrent Neural Networks (RNNs) to model the document sequence and Monte Carlo Tree Search (MCTS). An alternate approach tries to directly optimize user's utility instead of any proxies of diversity. Determinantal Point Processes <ref type="bibr" target="#b38">[39]</ref> and Multi-Armed Bandits <ref type="bibr" target="#b35">[36]</ref> are used to generate diverse rankings to optimize click-through rate.</p><p>Explicit diversification approaches, e.g., PM-2 <ref type="bibr" target="#b13">[14]</ref> or xQuAD <ref type="bibr" target="#b33">[34]</ref> improve the coverage over subtopics relevant to a query based on the sub-queries given during evaluation. Most recent methods in this category employ various neural architectures. Diverse Search with Subtopic Attention (DSSA) <ref type="bibr" target="#b22">[23]</ref> uses RNNs with attention mechanism for a sequential procedure to greedily select relevant documents which are diverse to current selected set. As a follow-up to DSSA, DVGAN <ref type="bibr" target="#b25">[26]</ref> uses Generative Adversarial Networks to frame the diversification problem as a minimax game between a generator and a discriminator, where the generator models document similarity and the discriminator uses subtopic information, and DESA <ref type="bibr" target="#b31">[32]</ref> adds self-attention encoder and decoder to replace RNNs to model interactions of all documents in the list.</p><p>While our proposed method shares some commonalities with the above methods in terms of neural architecture (e.g., similarly to DESA <ref type="bibr" target="#b31">[32]</ref> we use self-attention), there are two crucial differences. First, we estimate latent subtopics from the distributed representation of the query, rather than requiring providing them explicitly. Second, our end-to-end learning-to-rank framework directly optimizes a smooth approximation of the diversity metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning To Rank</head><p>Learning to rank has traditionally focused <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33]</ref> on relevance. A scoring function per query-document pair (referred to as univariate scoring <ref type="bibr" target="#b2">[3]</ref>) is learned to minimize the loss using learning algorithms such as Gradient Boosted Decision Trees (GBDT) <ref type="bibr" target="#b23">[24]</ref> and neural networks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b28">29]</ref>. For learning the univariate scoring function, Deep Structured Semantic Matching models (DSSM) <ref type="bibr" target="#b21">[22]</ref> learn a low dimensional embedding for queries and documents and use a dot-product as the score. In the domain of neural network scoring functions, multivariate scoring functions which capture cross document interactions as listwise context have been proposed in such as Deep Listwise Context Model <ref type="bibr" target="#b1">[2]</ref>, Groupwise Scoring Functions <ref type="bibr" target="#b2">[3]</ref>, Document Interaction Network <ref type="bibr" target="#b29">[30]</ref>, and SetRank <ref type="bibr" target="#b27">[28]</ref>. We also leverage deep listwise context via self-attention, but our goal is diversify results, but not just on improving relevance as in existing learning to rank work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Approximation of Ranking Metrics</head><p>Neural networks are amenable for end-to-end learning to directly optimize ranking metrics <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> by creating a differentiable approximation. ApproxNDCG <ref type="bibr" target="#b7">[8]</ref> revisits the idea proposed in <ref type="bibr" target="#b30">[31]</ref> of replacing indicator functions in rank computation with sigmoid functions for a differentiable surrogate. Sampling scores from Gumbel distribution to compute the expectation of the approximated metric (over induced permutations) is shown to gain additional robustness <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref>. Based on these, we introduce differentiable approximations of diversity evaluation metrics for the first time.</p><p>Another common technique to make differentiable approximations of ranking metrics is LambdaRank <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b37">38]</ref>, which uses the metric delta when two documents are swapped in the loss. Compared to the approximation techniques above, it is nontrivial to apply LambdaRank to non-additive diversification metrics. A recent work by Yigit-Sert et al. <ref type="bibr" target="#b42">[43]</ref> treats diversification as a fusion task of rankings obtained by sorting the documents with respect to individual subtopics. The LambdaRank is used as the standard LTR to obtain the ranking for each subtopic, but not to optimize diversification metrics directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DIVERSIFICATION-AWARE LOSSES 3.1 Diversity Evaluation Metrics</head><p>To introduce our differentiable losses that are close approximations of the actual diversity evaluation metrics, we first review two commonly used diversity evaluation metrics: ğ›¼-NDCG and ERR-IA.</p><p>3.1.1 ğ›¼-NDCG. Consider ğ‘› documents associated with a query and each document may cover 0 to ğ‘š subtopics, which is indicated by subtopic labels ğ‘¦ ğ‘–ğ‘™ : ğ‘¦ ğ‘–ğ‘™ = 1 if document ğ‘– covers subtopic ğ‘™ and ğ‘¦ ğ‘–ğ‘™ = 0 otherwise. The ğ›¼ discounted cumulative gain (ğ›¼-DCG) <ref type="bibr" target="#b12">[13]</ref> is then defined as,</p><formula xml:id="formula_1">ğ›¼-DCG = ğ‘› ğ‘–=1 ğ‘š ğ‘™=1 ğ‘¦ ğ‘–ğ‘™ (1 âˆ’ ğ›¼) ğ‘ ğ‘™ğ‘– log 2 (1 + ğ‘Ÿ ğ‘– ) , (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where ğ›¼ is a parameter between 0 and 1 quantifying the probability a reader got the information about a given subtopic from a relevant document, ğ‘Ÿ ğ‘– is the rank of the document ğ‘–, and ğ‘ ğ‘™ğ‘– is the number of times the subtopic ğ‘™ being covered by documents prior to rank ğ‘Ÿ ğ‘– :</p><formula xml:id="formula_3">ğ‘ ğ‘™ğ‘– = ğ‘—:ğ‘Ÿ ğ‘— â‰¤ğ‘Ÿ ğ‘– ğ‘¦ ğ‘—ğ‘™ .<label>(2)</label></formula><p>We can normalize this measure into the range [0, 1] by dividing the optimal ğ›¼-DCG given the document list:</p><formula xml:id="formula_4">ğ›¼-NDCG = ğ›¼-DCG ğ›¼-DCG opt .<label>(3)</label></formula><p>ğ›¼-NDCG@ğ‘˜ are metrics commonly used, which are obtained by summing over only the top ğ‘˜ ranked documents in the list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">ERR-IA.</head><p>For the same setting as above, the ERR-IA <ref type="bibr" target="#b11">[12]</ref> is defined as</p><formula xml:id="formula_5">ERR-IA â‰¡ ğ‘› ğ‘–=1 1 ğ‘Ÿ ğ‘– ğ‘š ğ‘™=1 1 ğ‘š ğ‘—:ğ‘Ÿ ğ‘— &lt;ğ‘Ÿ ğ‘– (1 âˆ’ 2 ğ‘¦ ğ‘—ğ‘™ âˆ’ 1 2 ğ‘¦ max ğ‘™ ) 2 ğ‘¦ ğ‘–ğ‘™ âˆ’ 1 2 ğ‘¦ max ğ‘™ .<label>(4)</label></formula><p>For binary labels ğ‘¦ ğ‘–ğ‘™ = 0 or 1 and ğ‘¦ max ğ‘™ = 1, this definition can be easily rewritten in terms of rank ğ‘Ÿ ğ‘– and subtopic coverage ğ‘ ğ‘™ğ‘– ,</p><formula xml:id="formula_6">ERR-IA = ğ‘› ğ‘–=1 1 ğ‘Ÿ ğ‘– ğ‘š ğ‘™=1 1 ğ‘š ğ‘¦ ğ‘–ğ‘™ 2 ğ‘ ğ‘™ğ‘– +1<label>(5)</label></formula><p>In practice, ERR-IA is further normalized by a constant ğ‘› ğ‘Ÿ =1</p><p>1 ğ‘Ÿ 2 âˆ’ğ‘Ÿ . Similar to ğ›¼-NDCG@ğ‘˜, ERR-IA@ğ‘˜ are commonly used metrics with a summation over the top ğ‘˜ documents for both ERR-IA and the normalization factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Differentiable Approximate Losses</head><p>In both ğ›¼-DCG and ERR-IA metrics, ğ‘Ÿ ğ‘– and ğ‘ ğ‘™ğ‘– are ranking-dependent. These make them non-differentiable. The first contribution of this paper is a smooth approximation of the diversity metrics through soft versions of the rank ğ‘Ÿ ğ‘– and subtopic coverage ğ‘ ğ‘™ğ‘– . This is achieved by re-expressing them using the scores assigned to each document. In the following, we use ğ›¼-DCG as an example. All the derivations can be applied to the ERR-IA metric without any difficulty. Let ğ‘  ğ‘– be the score for document ğ‘–, ğ‘Ÿ ğ‘– and ğ‘ ğ‘™ğ‘– can then be formulated as:</p><formula xml:id="formula_7">ğ‘Ÿ ğ‘– = 1 + ğ‘— I ğ‘  ğ‘— &gt;ğ‘  ğ‘– , ğ‘ ğ‘™ğ‘– = ğ‘— ğ‘¦ ğ‘—ğ‘™ I ğ‘  ğ‘— &gt;ğ‘  ğ‘– ,<label>(6)</label></formula><p>where I is the indicator function. The indicator function is not differentiable, but can be approximated by the sigmoid function</p><formula xml:id="formula_8">sigmoid(ğ‘¥) = 1 1 + exp(âˆ’ğ‘¥/ğ‘‡ )</formula><p>where ğ‘‡ is a positive smoothness parameter. Applying this approximation, to the explicit formulations of rank and subtopic coverage in Eq.( <ref type="formula" target="#formula_7">6</ref>), we get the differentiable smooth approximations to rank ğ‘Ÿ ğ‘– and subtopic coverage ğ‘ ğ‘™ğ‘– with a single parameter ğ‘‡ .</p><formula xml:id="formula_9">ğ‘… ğ‘– = 1 + ğ‘—â‰ ğ‘– sigmoid ğ‘  ğ‘— âˆ’ ğ‘  ğ‘– ğ‘‡ = 1 2 + ğ‘— 1 1 + exp ğ‘  ğ‘– âˆ’ğ‘  ğ‘— ğ‘‡ , ğ¶ ğ‘™ğ‘– = ğ‘—â‰ ğ‘– ğ‘¦ ğ‘—ğ‘™ â€¢ sigmoid ğ‘  ğ‘— âˆ’ ğ‘  ğ‘– ğ‘‡ = ğ‘— ğ‘¦ ğ‘—ğ‘™ 1 + exp ğ‘  ğ‘– âˆ’ğ‘  ğ‘— ğ‘‡ âˆ’ ğ‘¦ ğ‘–ğ‘™ 2<label>(7)</label></formula><p>The approximations are strictly equal but not differentiable as ğ‘‡ â†’ 0, and the larger is the parameter ğ‘‡ , the more smooth are the differentiable approximations. These soft versions of ranks and subtopic coverage become the estimates to capture the soft version of next document.</p><p>By inserting these approximations to the ğ›¼-DCG metric definition in Eq.( <ref type="formula" target="#formula_1">1</ref>), we then obtain a directly differentiable diversificationaware ğ›¼-DCG loss,</p><formula xml:id="formula_10">L ğ›¼-DCG ({ğ‘  ğ‘ ğ‘– }) = âˆ’ 1 |Q| ğ‘ âˆˆ Q ğ‘› ğ‘–=1 ğ‘š ğ‘™=1 ğ‘¦ ğ‘ ğ‘–ğ‘™ (1 âˆ’ ğ›¼) ğ¶ ğ‘ ğ‘™ğ‘– log 2 (1 + ğ‘… ğ‘ ğ‘– ) ,<label>(8)</label></formula><p>where we add back superscript ğ‘ to define the loss over a set of queries Q. L ğ›¼-NDCG can be similarly derived with a constant normalization weight for each query. Finally, another useful variation to this diversity ranking loss is to add a stochastic treatment <ref type="bibr" target="#b6">[7]</ref>.</p><formula xml:id="formula_11">L Gumbel-ğ›¼-DCG ({ğ‘  ğ‘ ğ‘– }) = E g [L ğ›¼-DCG ({ğ›½ (ğ‘  ğ‘ ğ‘– + g ğ‘– )})],<label>(9)</label></formula><p>where parameter ğ›½ is a noise-level parameter and Gumbel noise g ğ‘– is sampled from Gumbel distribution</p><formula xml:id="formula_12">g ğ‘– = âˆ’ log(âˆ’ log(ğ‘ˆ ğ‘– )) with ğ‘ˆ ğ‘– uniformly distributed in [0, 1].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">NEURAL LEARNING</head><p>Now that we formulated the diversification-aware loss, we next describe how to leverage the neural networks to optimize this loss in the learning-to-rank setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Distributed Representation</head><p>Instead of the heuristic aggregated ranking features, our approach relies on distributed representations of the text-based query ğ‘ and candidate document list {ğ‘‘ ğ‘– }, which can be generated by a trainable neural encoder. Popular choices are BERT <ref type="bibr" target="#b14">[15]</ref> or doc2vec <ref type="bibr" target="#b24">[25]</ref>. Essentially, they encode the text-based queries and documents with various lengths into dense normalized vectors e ğ‘ âˆˆ R ğ¸ and e ğ‘– âˆˆ R ğ¸ of a fixed embedding dimension ğ¸, so that one can determine the similarity of different documents (and queries) in this latent space.</p><p>The latent space allows us to apply recent neural interaction methods to better capture the relationship between query-document pairs. In this work, we use a simple algorithm, latent cross <ref type="bibr" target="#b4">[5]</ref>, which can effectively generate high-order interaction features: for each pair of query and document representations e ğ‘ and e ğ‘– , we define a query-document cross feature c ğ‘– âˆˆ R ğ¸ by an element-wise multiplication,</p><formula xml:id="formula_13">c ğ‘– = e ğ‘– â€¢ e ğ‘ . (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>The representation of each query can be thought as a mix of the subtopics and the element-wise product between a query and a document can then be easily de-mixed to compute the matching between subtopics and the document.</p><p>Note that the latent representations for query and document can be extracted from pre-trained models, or jointly tuned with the ranker end-to-end in our neural framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Listwise Context Embedding</head><p>Intuitively, information from the entire document list is helpful for the diversification task. Thus, we enrich the distributed representation of a document by considering representations of the entire list based on pairwise document similarity. Specifically, we incorporate into our framework the Document Interaction Network (DIN) proposed in <ref type="bibr" target="#b29">[30]</ref>, a similar idea also implemented in <ref type="bibr" target="#b31">[32]</ref>. Note that while we use this to enhance our scoring function, the objective of this work is to improve diversification of ranking, whereas <ref type="bibr" target="#b29">[30]</ref> focuses on improving ranking measures.</p><p>DIN generates an embedding of the candidate list, a ğ‘– , for each document ğ‘–, using the multi-head self-attention (MHSA) mechanism, introduced in Transformers <ref type="bibr" target="#b36">[37]</ref>. DIN uses pairwise dotproduct attention to capture document similarity between document ğ‘– and every document in the list. For the multi-head selfattention mechanism, we concatenate the features for all documents in the list to input ğ· âˆˆ R ğ‘›Ã—ğ‘˜ , where ğ‘˜ is the feature dimension corresponding to one document. We project ğ· into a query<ref type="foot" target="#foot_0">1</ref> matrix ğ‘„ = ğ·ğ‘Š ğ‘„ , a key matrix ğ¾ = ğ·ğ‘Š ğ¾ , and a value matrix ğ‘‰ = ğ·ğ‘Š ğ‘‰ with trainable projection matrices ğ‘Š ğ‘„ , ğ‘Š ğ¾ , and ğ‘Š ğ‘‰ âˆˆ R ğ‘˜Ã—ğ‘§ , where ğ‘§ is the attention head size. Then a self-attention (SA) head computes the weighted sum of the transformed values ğ‘‰ as,</p><formula xml:id="formula_15">SA(ğ·) = Softmax(ğ‘† (ğ·))ğ‘‰ ,<label>(11)</label></formula><p>where similarity matrix between ğ‘„ and ğ¾ is defined as ğ‘† (ğ·) = ğ‘„ğ¾ ğ‘‡ âˆš ğ‘§ . For each layer, the results from the ğ» heads are concatenated to form the output of multi-head self-attention by</p><formula xml:id="formula_16">MHSA(ğ·) = concat â„ âˆˆ [ğ» ] [SA â„ (ğ·)]ğ‘Š out + ğ‘ out ,<label>(12)</label></formula><p>where ğ‘Š out âˆˆ R ğ»ğ‘§Ã—ğ‘§ and ğ‘ out âˆˆ R ğ‘›Ã—ğ‘§ are trainable parameters. To compute the listwise context embedding, we apply ğ¿ â‰¥ 1 layers of multi-head self-attention over the input documents ğ·. Similar to Transformer <ref type="bibr" target="#b36">[37]</ref>, we also apply residual connections <ref type="bibr" target="#b20">[21]</ref> and layer normalization <ref type="bibr" target="#b3">[4]</ref> to each layer. We augment the features of the query-document scoring function with the listwise context embedding from the final output of the ğ¿-th self-attention layer. Since this embedding contains information from the whole candidate</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Listwise Context Embedding</head><p>Encoder list, it serves as complementary features to distinguish documents covering the same subtopics. </p><formula xml:id="formula_17">o q e q o o â€¦ Latent Cross d 1 e 1 d i e i d n e n c 1 c i c n a 1 a i â€¦ a n â€¦ â€¦ s 1 ... s i â€¦ s n Concat FC-BN-ReLU a i e i c i Diversity Loss y 1 â€¦ y i â€¦ y n 1 â€¦ 0 â€¦ 1 0 â€¦ 1 â€¦ 0 1 â€¦ 0 â€¦ 0 Subtopic Labels e q s i Ã— N</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">A THEORETICAL ANALYSIS</head><p>In this section, we elaborate in-depth how diversification loss and distributed representation work to output a diversified ranking list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Relevance and Diversity in the Differential</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ğ›¼-DCG Loss</head><p>As the ğ›¼-DCG loss captures both the relevance through rank discount 1/log 2 (1 + ğ‘…) and the diversity through coverage discount</p><p>(1 âˆ’ ğ›¼) ğ¶ , optimizing ğ›¼-DCG loss trains the model to capture both sides of diversified ranking.</p><p>To show that, consider two documents 0 and 1 with close but different initial scores ğ‘  0 â‰ˆ ğ‘  1 , while scores of all other documents are fairly far from these two. We want to ask how this difference ğ›¿ğ‘  â‰¡ ğ‘  0 âˆ’ ğ‘  1 changes over the training course, especially, the sign of</p><formula xml:id="formula_18">ğ‘‘ğ›¿ğ‘  (ğ‘¡ ) ğ‘‘ğ‘¡</formula><p>and the sign of the coefficient if</p><formula xml:id="formula_19">ğ‘‘ğ›¿ğ‘  (ğ‘¡ ) ğ‘‘ğ‘¡</formula><p>is proportional to ğ›¿ğ‘ . Scores tend to go downward along the gradient,</p><formula xml:id="formula_20">ğ‘  ğ‘– (ğ‘¡ + 1) âˆ’ ğ‘  ğ‘– (ğ‘¡) âˆ âˆ’ ğœ•L ğœ•ğ‘  ğ‘– .</formula><p>As a result, the relative score changes as</p><formula xml:id="formula_21">ğ‘‘ğ›¿ğ‘  (ğ‘¡) ğ‘‘ğ‘¡ âˆ ğ›¿ğ‘  (ğ‘¡ + 1) âˆ’ ğ›¿ğ‘  (ğ‘¡) âˆ ğœ•L ğœ•ğ‘  1 âˆ’ ğœ•L ğœ•ğ‘  0 .</formula><p>For small score difference |ğ›¿ğ‘  | â‰ª ğ‘‡ , rewriting ğ‘  0 and ğ‘  1 in ğ›¿ğ‘  and s â‰¡ ğ‘  0 +ğ‘  1 2 as ğ‘  0 = s + ğ›¿ğ‘  2 and ğ‘  1 = s âˆ’ ğ›¿ğ‘  2 , we can expand around s,</p><formula xml:id="formula_22">ğ‘‘ğ›¿ğ‘  (ğ‘¡) ğ‘‘ğ‘¡ âˆ ğœ•L ğœ•ğ‘  1 âˆ’ ğœ•L ğœ•ğ‘  0 ğ‘  0 ,ğ‘  1 =s âˆ’ ğ›¿ğ‘ <label>2</label></formula><formula xml:id="formula_23">ğœ• 2 L ğœ•ğ‘  2 0 + ğœ• 2 L ğœ•ğ‘  2 1 âˆ’ 2 ğœ• 2 L ğœ•ğ‘  0 ğœ•ğ‘  1 ğ‘  0 ,ğ‘  1 =s + ğ‘œ (ğ›¿ğ‘  2 ). (<label>14</label></formula><formula xml:id="formula_24">)</formula><p>So the gradients ğœ•L/ğœ•ğ‘  and the hessians ğœ• 2 L/ğœ•ğ‘  2 are important for this analysis. We compute the explicit expressions of gradients and hessians for ğ›¼-DCG loss and softmax loss (not shown in the main text) and keep the dominant order in analysis below.</p><p>To show the ğ›¼-DCG loss promotes the relevant documents, we consider two documents: document 0 is not relevant to any of the subtopics ğ‘¦ 0ğ‘™ = 0, while document 1 covers some subtopic, âˆƒğ‘™ s. the sigmoid function sigmoid â€² ğ‘  ğ‘— âˆ’ğ‘  0,1 ğ‘‡ â‰ˆ 0 for âˆ€ğ‘— â‰  0, 1. So the relative score change between 0 and 1 will be dominated by the gradient ğœ• L ğœ•ğ‘  1 term in Eq.( <ref type="formula" target="#formula_23">14</ref>) as</p><formula xml:id="formula_25">ğ‘‘ğ›¿ğ‘  ğ‘‘ğ‘¡ âˆ âˆ’ 1 ğ‘‡ ğ‘¦ 11 1 + R (1 âˆ’ ğ›¼) C ln 2 (ln(1 + R)) 2 &lt; 0.</formula><p>The ranker learns to rank the relevant document 1 higher relative to document 0.</p><p>To demonstrate the loss promotes the diversity through learning features differentiating documents with the same labels, we now suppose the two documents cover the same subtopic of the query, say topic 1, ğ‘¦ 01 = ğ‘¦ 11 = 1 and 0 for ğ‘™ â‰  1. Then it is easy to show the gradient terms in Eq.( <ref type="formula" target="#formula_23">14</ref>) vanish and the rate of relative score becomes dominated by the hessian terms. Ignoring the higher order contributions of 1/ R from the derivatives of rank, we have</p><formula xml:id="formula_26">ğ‘‘ğ›¿ğ‘  (ğ‘¡) ğ‘‘ğ‘¡ âˆ ï£± ï£´ ï£´ ï£² ï£´ ï£´ ï£³ ln 1 1âˆ’ğ›¼ ğ‘‡ 2 (1 âˆ’ ğ›¼) Cğ‘¡ log 2 (1 + R) + ğ‘œ 1 1 + R ï£¼ ï£´ ï£´ ï£½ ï£´ ï£´ ï£¾ ğ›¿ğ‘ ,</formula><p>where the coefficient of the ğ›¿ğ‘  term on the right hand side is positive. So indicated by the ğ›¼-DCG loss, the score difference of the two documents with the same label tends to diverge over the training. In contrast, in other typical relevance ranking losses, softmax loss for example, one can show that the same coefficient in front of ğ›¿ğ‘  is negative so that the scorer trained with the softmax loss always tends to score the documents with the same label similarly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Distributed Representation Facilitates Diversity</head><p>Distributed representation boosts diversity learning through the following three aspects: (i) Neural networks can learn the representations of subtopics in the latent space; (ii) The alignment between a subtopic and a candidate document can be learned from a transformation of the latent cross between the corresponding query and document; (iii) A score function that discriminates the documents aligning with the same subtopics can be learned from the query and subtopic independent dimensions with nontrivial distributions of candidate documents. Consider distributed representations in a latent space, the number of dimensions ğ¸ is much greater than the number of subtopics ğ‘š. All subtopics and query can be represented as vectors in the space: say e ğ‘ ğ‘™ as the vector representation of subtopic ğ‘™ of query ğ‘. It can be decomposed into a component aligning with the query and a part that is perpendicular, e ğ‘ ğ‘™ = ğœ† ğ‘™ e ğ‘ + e ğ‘™ , with the latter as the additional information in subtopics that clarifies different modalities of the query. Proposition (i) becomes to show whether a neural network is able to infer this subtopic component e ğ‘™ from the training data query e ğ‘ and document representations {e ğ‘– }.</p><p>In general, the subtopic vector can be further decomposed into a query-specific component and a general component,</p><formula xml:id="formula_27">e ğ‘™ = f ğ‘™ (e ğ‘ ) + e 0 ğ‘™ .<label>(15)</label></formula><p>Apparently, the general modality e 0 ğ‘™ can be learned from the training documents and encoded as constant biases in the neural network. While how to encode the query dependent part is less obvious: linear transform functions will fail to generate features that are perpendicular to their variable. Fortunately, the neural networks are nonlinear in nature and are thus capable to learn nonlinear functions f ğ‘™ for subtopic representations. A simple example of such nonlinear functions is the projection operation used in Table <ref type="table">1</ref>: f ğ‘™ (e ğ‘ ) = P ğ‘™ â€¢ e ğ‘ . In the coordinate system with axes aligned with subtopic vectors as in the example in Table <ref type="table">1</ref>, the projection matrix P ğ‘™ equals to 1 at diagonal indices ğ‘™ and 0 otherwise, and is linearly transformed from such a diagonal matrix in general.</p><p>In addition, such projection operations also allow the neural networks to demonstrate (ii): learn to retrieve the alignment between a document and a subtopic e ğ‘™ â€¢ e ğ‘– from the query document latent cross c ğ‘– :</p><formula xml:id="formula_28">e ğ‘™ â€¢ e ğ‘– = (f ğ‘™ (e ğ‘ ) + e 0 ) â€¢ e ğ‘– = e ğ‘ â€¢ P ğ‘™ â€¢ e ğ‘– + e 0 â€¢ e ğ‘– = ğ‘— ğ‘“ ğ‘™,ğ‘— (c ğ‘– ) + e 0 â€¢ e ğ‘– ,</formula><p>which can be used as a feature in determining the relevance between the document and subtopic. In general, there exist nonlinear transformations in the representation space for neural networks to learn to encode implicitly subtopics and alignments between subtopic and documents -useful features in the diversification task.</p><p>Finally, to Proposition (iii), in the latent space, as ğ¸ â‰« ğ‘š, there are many query and subtopic independent, in other words, relevanceneutral directions where the candidate documents are spreading over. The nontrivial distributions of the document lists in these dimensions are useless for relevance ranking but can be utilized to diversify the output list. For example, giving higher scores to documents with less neighbors along these dimensions will end up with a more diverse subtopic-coverage in top ranks. See the example in Table <ref type="table">1</ref>. Our ğ›¼-DCG loss naturally leads the neural network to learn such a scoring rule by exploiting the nontrivial distributions in the query-independent dimensions.</p><p>Suppose ğ‘› ğ‘™ documents equally relevant to subtopic ğ‘™ are distributed over a subtopic independent dimension e div as ğœŒ ğ‘™ (ğ‘¥) with ğ‘¥ ğ‘– â‰¡ e div â€¢ e ğ‘– a measure on how a document ğ‘– aligns with this dimension. Then the score function training dynamics of a document of alignment ğ‘¥ can be obtained by integrating Eq. 14 over the distribution ğœŒ ğ‘™ (ğ‘¥),</p><formula xml:id="formula_29">ğ‘‘ğ‘  (ğ‘¥) ğ‘‘ğ‘¡ âˆ’ ğ‘‘ s ğ‘‘ğ‘¡ âˆ 1 ğ‘› ğ‘™ âˆ« ğ‘‘ğœ‰ğœŒ ğ‘™ (ğœ‰)ğ‘[ğ‘  (ğ‘¥) âˆ’ ğ‘  (ğœ‰)],</formula><p>where s = 1</p><formula xml:id="formula_30">ğ‘› ğ‘™ âˆ« ğ‘‘ğœ‰ğœŒ ğ‘™ (ğœ‰)ğ‘  (ğœ‰)</formula><p>is the average score of these ğ‘› ğ‘™ documents, ğ‘ is the positive coefficient computed form the hessian terms of ğ›¼-DCG loss. Knowing that ğ‘ is finite for |ğ‘  (ğœ‰) âˆ’ ğ‘  (ğ‘¥)| â‰ª ğ‘‡ and decays rapidly to zero when |ğ‘  (ğ‘§) âˆ’ ğ‘  (ğ‘¥)| â‰³ ğ‘‡ due to the derivative of the sigmoid funtion, we can expand functions of ğœ‰ around ğ‘¥ and get approximately,</p><formula xml:id="formula_31">ğ‘‘ğ‘  (ğ‘¥) ğ‘‘ğ‘¡ âˆ’ ğ‘‘ s ğ‘‘ğ‘¡ âˆ âˆ’ Äğ‘‡ 2 ğ‘› ğ‘™ ğœŒ â€² ğ‘™ (ğ‘¥) ğ‘  â€² (ğ‘¥) , (<label>16</label></formula><formula xml:id="formula_32">)</formula><p>where ğœŒ â€² ğ‘™ (ğ‘¥) and ğ‘  â€² (ğ‘¥) are the gradients of distribution and score function on alignment ğ‘¥. At the high score end ğ‘  (ğ‘¥) &gt; s, when distribution decays with ğ‘¥, ğœŒ â€² ğ‘™ (ğ‘¥) &lt; 0, score converges to increase with ğ‘¥, ğ‘  â€² (ğ‘¥) &gt; 0, and vice versa when ğœŒ â€² ğ‘™ (ğ‘¥) &gt; 0. As a result, scorer learns to give high scores to documents at low document density ğœŒ ğ‘™ along the subtopic independent dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>To verify the effectiveness of DALETOR, we experiment on the diversity task benchmarks of TREC 2009 -2012 Web Track datasets<ref type="foot" target="#foot_1">2</ref> , which are derived from ClueWeb09 and commonly tested by existing diversification models. The combined dataset of the four TREC datasets includes 198 queries in total (out of 200, 2 queries with no subtopic judgment are dropped). Each query covers 3 to 8 subtopics and the corresponding candidate documents are labeled in binary at the subtopic level, identified by the TREC assessors. We focus on the TREC official diversity evaluation metrics of ğ›¼-NDCG@ğ‘˜ <ref type="bibr" target="#b12">[13]</ref> and ERR-IA <ref type="bibr" target="#b10">[11]</ref>@ğ‘˜ with ğ‘˜ = 5 and 10, where the parameter ğ›¼ is set to 0.5 as the default settings in official TREC evaluation program.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Settings</head><p>For a fair comparison with recent methods, we specifically work on a public, pre-processed version of the dataset<ref type="foot" target="#foot_2">3</ref> by Feng et al. <ref type="bibr" target="#b17">[18]</ref>, which is based on the official TREC judgements on the ClueWeb09 Category B data collection. There are 42,245 (40,537 unique) labeled candidate documents associated with the 198 queries. Among them, about one third <ref type="bibr" target="#b12">(13,</ref><ref type="bibr">279)</ref> documents contain at least one subtopic. The query and document representation vectors were generated by doc2vec and the dimension of vector representations ğ¸ was set to 100. Please refer to <ref type="bibr" target="#b17">[18]</ref> for more details of the dataset.</p><p>We conduct 5-fold cross-validation experiments on the combined dataset with the same subset split as in <ref type="bibr" target="#b17">[18]</ref>. At each fold, three subsets were used for training, one was used for hyper-parameter tuning, and one was used for testing. The results reported are the average over the five trials on the testing set in each fold.</p><p>Through cross-validation, we choose the following optimizer and model configurations: the optimizer is "Adagrad" <ref type="bibr" target="#b16">[17]</ref> with learning rate ğœ‚ = 0.01. The univariate neural scorer contains three hidden layers with dimensions equal to 256, 128, and 64, followed by a one-dimension dense output layer to compute scores. When applicable, the listwise context embedding is composed of ğ¿ = 2 self-attention layers with ğ» = 2 attention heads in each layer with head size ğ‘§ = 256. Finally, the smoothness parameter of the ğ›¼-DCG loss and its variants is set to ğ‘‡ = 0.1.</p><p>We compare with the following baseline methods, including several recent state-of-the-art ones: MMR <ref type="bibr" target="#b9">[10]</ref>: a heuristic approach with the documents selected sequentially according to maximal marginal relevance; xQuAD <ref type="bibr" target="#b33">[34]</ref>: a representative method which models subtopics of the original query with sub-queries; PM-2 <ref type="bibr" target="#b13">[14]</ref>: a heuristic method of optimizing proportionality for search result diversification; SVM-DIV <ref type="bibr" target="#b43">[44]</ref>: a learning approach which utilizes structural SVMs to optimize subtopic coverage; R-LTR <ref type="bibr" target="#b45">[46]</ref>: a learning approach developed in the relational learning to rank framework; PAMM <ref type="bibr" target="#b39">[40]</ref>: a learning approach that optimizes ğ›¼-NDCG using structured Perceptron;</p><p>All methods above are taking classical aggregated features as input, while methods below are taking the distributed representations as input. NTN-DIV <ref type="bibr" target="#b40">[41]</ref>: a learning approach which learns novelty features based on neural tensor networks, PAMM-NTN in specific to directly optimize ğ›¼-NDCG@10; MDP-DIV <ref type="bibr" target="#b41">[42]</ref>: a state-of-theart reinforcement learning approach which uses a Markov Decision The variant of M 2 DIV, with latent cross features fed in as an input to the LSTM model <ref type="foot" target="#foot_3">4</ref> .</p><p>We investigate the following models in our DALETOR framework: DNN (softmax): a deep univariate scoring model with no latent cross or listwise context embedding, trained with the listwise softmax loss using number of covered subtopics as labels, which serves as a baseline that is not diversification-aware. DNN (R-LTR): a deep univariate scoring model with no latent cross or listwise context embedding, trained with the ListMLE loss using scores from a greedy solution optimizing ğ›¼-NDCG as labels, which serves as a diversification-aware baseline. DNN (ğ›¼-DCG) and DNN-LC (ğ›¼-DCG): a deep univariate scoring model without listwise context embedding trained with the ğ›¼-DCG loss, without and with latent cross. DIN (ğ›¼-DCG) and DIN-LC (ğ›¼-DCG): a document interaction network model with listwise context trained with the ğ›¼-DCG loss, without and with latent cross. We also test the other option to incorporate the listwise context with groupwise scoring functions (GSF) <ref type="bibr" target="#b2">[3]</ref> and report the best among group sizes tuned over 4, 16, and 64. The results of GSF (ğ›¼-DCG) and GSF-LC (ğ›¼-DCG) are without and with latent cross respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experimental results</head><p>The performance of different methods is reported in Tables <ref type="table" target="#tab_1">2, 3</ref>, and 4, where boldface indicates the highest scores among all methods in each metric. All statistical significance tests are under the double tailed t-test with p-value&lt; 0.01 indicated by superscripts ' * ' and ' â€ '. We can make several main observations from the results in these tables:</p><p>â€¢ Our complete model, DIN-LC(ğ›¼-DCG), outperforms all the baseline models by a large margin, as shown in Table <ref type="table" target="#tab_0">2</ref>. The improvements are statistically significant in terms of both ğ›¼-NDCG and ERR-IA metrics when compared with the stateof-the-art implicit method M 2 DIV. â€¢ The benefits of our diversification-aware losses are shown in Table <ref type="table" target="#tab_1">3</ref>. We compare our ğ›¼-DCG loss and the R-LTR loss under both DNN and DNN-LC in this table. We can see that DNN(ğ›¼-DCG) improves upon the baseline DNN(R-LTR) by more than 10%, so as DNN-LC(ğ›¼-DCG) against the baseline DNN-LC(R-LTR). â€¢ Our methods can effectively leverage the distributed representation. On one hand, using distributed representations of query and documents as input features directly improves the results by comparing DNN(R-LTR) and R-LTR, seen in Table <ref type="table" target="#tab_0">2</ref>. On the other hand, incorporating the latent cross features based on the distributed representation adds up another 8% increase in terms of ğ›¼-NDCG and ERR-IA metrics consistently over DNN and DIN as in Table <ref type="table" target="#tab_2">4</ref>. â€¢ Worth noting in Table <ref type="table" target="#tab_2">4</ref>, though the latent cross features also slightly help M 2 DIV, they appear to bring much larger gain within our DALETOR framework. â€¢ While slightly better, DIN-LC(ğ›¼-DCG) does not show statistical significance when compared with DNN-LC(ğ›¼-DCG). On the TREC dataset, the distributions of the candidate lists are consistent over the training and testing sets, so DNN itself is sufficient to encode information needed for the diversification task. In Sec. 6.2.3 we will show that the context-aware DIN models are more robust to the training-testing skew with perturbed testing sets.</p><p>6.2.1 Variants of ğ›¼-DCG loss. We investigate different variants of ğ›¼-DCG loss to show its extendability and robustness in terms of hyper-parameters. All experiments in this section are built on the DNN-LC base. We report ğ›¼-DCG loss with different smoothness parameters ğ‘‡ and Gumbel ğ›¼-DCG loss (ğ›½ = 10) defined in Eq.( <ref type="formula" target="#formula_11">9</ref>). The results are summarized in Table <ref type="table" target="#tab_3">5</ref>.</p><p>From variants of approximation smoothness ğ‘‡ , we find that the performance is robust in general and still outperforms existing methods. However, there exists an optimal smoothness with ğ‘‡ around 0.1. Either too smooth ğ‘‡ = 1.0 or too sharp ğ‘‡ = 0.01 of the approximation makes the learning less efficient. This is intuitive since very sharp approximation of the metric (i.e., ğ‘‡ â†’ 0) makes the gradient of the loss almost zero everywhere so that the learning becomes impossible. On the other end, very smooth approximation (i.e. ğ‘‡ â†’ âˆ) makes the loss deviate too far from the metric.</p><p>We also find some marginal but statistically insignificant improvement with stochastic treatment of the ğ›¼-DCG loss. It shows our framework is easily extendable, and such modifications might be more significant in other datasets.   </p><formula xml:id="formula_33">= ğ» = 2.</formula><p>In Table <ref type="table" target="#tab_4">6</ref>, we find that the performance of DIN model with ğ¿ = 2 is consistently better than the DIN model with ğ¿ = 1. However, when the model gets larger with ğ¿ &gt; 2, it becomes more difficult to train self-attention layers with the relatively small TREC dataset. The effect of the number of heads is less obvious. The performance tends to increase with the number of heads, but is not very consistent. The trend is more consistent with increasing head size: DIN models with larger head size perform better. But this increase becomes marginal when the head size is comparable to the embedding dimension of the representations. <ref type="table" target="#tab_0">2</ref>, when comparing DIN-LC with DNN-LC, we do not have statistically significant difference in performance by incorporating the listwise context embedding on the TREC dataset. The GSF results reported in Table <ref type="table" target="#tab_2">4</ref> also show similar observations, despite some minor deficiency of GSF in utilizing the latent cross features. This result is likely due to the fact that DNN is already sufficient to capture the training distribution, which is consistent with the testing distribution, for the diversification task. However, if there is a noticeable difference between training and testing sets, DIN may better model the distribution change captured by the listwise context. In this section, we explore the robustness of the listwise context embedding by artificially introducing duplication into the testing set. Note that the goal of this section is to rigorously show the robustness of DIN. In practice, it is likely that the training and testing distributions are similar. Also, document near-deduplication techniques are recommended when there is a concern of duplicated documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Effects of listwise context. As we have seen in Table</head><p>We randomly duplicate fraction ğ‘ of positive candidate documents by ğ‘› times with ğ‘› uniformly distributed in [0, 20]. We then apply the trained models on these perturbed testing sets.</p><p>As shown in Fig. <ref type="figure" target="#fig_4">2</ref>, increasing the probability of duplication ğ‘, the testing set deviates from the original distribution of the training set and both DNN and DIN model performances reduce. But the reduction of DIN performance is clearly slower than the DNN model. In terms of percentage change in ğ›¼-NDCG@10, DNN model performance is reduced by 4.7% and DIN model is reduced by 3.0% when about 20% of the positive documents are duplicated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION</head><p>Here we discuss the differences between the traditional "nextdocument" diversification methods and the proposed "score-andsort" setting with a "soft" version of "next document" to give an intuition on how it is able to promotes diversification. The key difference is that in the "score-and-sort" setting, the scores are predicted simultaneously rather than sequentially as in the "next-document" setting. We try to answer the following two successive questions: how can a scorer properly score the documents without knowing the context? And even if the scorer knows the scoring context, how To the first question, the naive solution is to take the context information as part of input to the scorer, which is what we did by incorporating listwise context using DIN. But we found in Table <ref type="table" target="#tab_0">2</ref>, the performance of DIN models is not significantly better than that of simple univariate models. This indicates that our models were able to learn subtopic distributions over the documents and reasonably diversify the results based on the listise diversificationaware losses.</p><p>The second question is special to the "score-and-sort" setting: in the "next-document" setting, we can output a diversified ranking as long as the relevance to each subtopics are inferred, but in the "score-and-sort" setting, we have to consult to features that are "perpendicular" to the relevance measure encoded in the distributed representations as shown in the Section 5. The intuition we got from the relevance-neutral features is that as long as we can score the documents with the same subtopic relevance differently, we will be able to rank the top ones in a diverse way. This is exactly we found from the scorers trained with diversification-aware loss: Table <ref type="table">7</ref> shows several candidate document examples that have the same subtopic label and they are scored by a scorer trained with softmax loss -Score(softmax) and a scorer trained with ğ›¼-DCG loss -Score(ğ›¼-DCG). When the model is trained with softmax loss, which is not diversification-aware, it tends to score documents with the same label similarly. However, when the model is trained with the diversification-aware ğ›¼-DCG loss, it can capture their differences in the latent space and score one document much higher than others to improve diversification.</p><p>Another advantage of the "score-and-sort" framework is it allows ğ‘‚ (ğ‘›) inference complexity, which can be done in parallel and significantly reduces serving latency in real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this work, we introduced a new perspective for learning-based search result diversification. To enable the diversification-aware learning, we introduced a differentiable loss from a close approximation of a commonly used diversity metric, ğ›¼-DCG in particular. With this differentiable loss, we could then train a neural network end-to-end from distributed representations of queries and documents. We further leveraged the latent cross of the distributed representations and the listwise context, resulting in a deep ranker that performs significantly better than state-of-the-art methods on the TREC dataset in terms of various diversity evaluation metrics. We further elaborated that how our model learns subtopics implicitly from distributed representations, how the approximate ğ›¼-DCG loss promotes diversity by learning the distribution of the candidate list from subtopic-independent features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>WWW ' 21 ,</head><label>21</label><figDesc>April 19-23, 2021, Ljubljana, Slovenia Yan et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of the diversification-aware learning to rank (DALETOR) architecture. The inputs {e ğ‘ , e ğ‘– , c ğ‘– , a ğ‘– } on the right are corresponding outputs from the left. FC stands for fully-connected layer, BN stands for batch normalization and ReLU stands for the nonlinear activation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1</head><label>1</label><figDesc>Figure1summarizes our end-to-end framework of Diversification-Aware LEarning TO Rank (DALETOR). From a text-based query and candidate document list, we first obtain their distributed representations, e ğ‘– , with a trainable document encoder. In the case when the latent cross (in the blue box) is applied, we generate the query-document cross representations c ğ‘– by an element-wise multiplication of query and document representations. If DIN (in the purple box) is incorporated, we pass all representations from query e ğ‘ , documents e ğ‘– , and query-document cross c ğ‘– (when applicable) to a self-attention layer to get the listwise context representation a ğ‘– for each document. Finally, all generated representations, query e ğ‘ , document e ğ‘– , latent cross c ğ‘– , and listwise context embedding a ğ‘– , are concatenated and passed through several full connected layers to compute the final ranking score for each document associated with the query. The output scores are then fed into a diversificationaware loss during training and used for sorting during inference. For a univariate neural scorer ğ‘  (.), the scoring function for the full DALETOR architecture is as follows:a ğ‘– = DIN ğ‘– ({concat(e ğ‘ , e ğ‘— , c ğ‘— )})ğ‘  DALETOR (ğ‘, {ğ‘‘ ğ‘– }) = {ğ‘  (concat(e ğ‘ , e ğ‘– , c ğ‘– , a ğ‘– ))}.<ref type="bibr" target="#b12">(13)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>t. ğ‘¦ 1ğ‘™ = 1. Without loss of generality, assume document 1 covers subtopic ğ‘™ = 1. As scores of document 0 and 1 are close to each other but far from the others |ğ‘  0 âˆ’ ğ‘  1 | â‰ª ğ‘‡ and |ğ‘  0,1 âˆ’ ğ‘  ğ‘— | â‰« ğ‘‡ for âˆ€ğ‘— â‰  0, 1, we have ğ‘… 0 â‰ˆ ğ‘… 1 , ğ¶ ğ‘™0 â‰ˆ ğ¶ ğ‘™1 , and the derivative of WWW '21, April 19-23, 2021, Ljubljana, Slovenia Yan et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Model performance on perturbed dataset. Left: ğ›¼-NDCG@10 metric. Right: Percentage change in ğ›¼-NDCG@10 metric.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Table 7 :</head><label>7</label><figDesc>Candidate documents with the same subtopic label of Query 78.Subtopic label Score(softmax) Score(ğ›¼-DCG) ğ‘‘ 1 documents with the same relevance to certain subtopics?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison with baselines. ' * ' indicates statistically significant improvement over M 2 DIV. The best results are bolded.</figDesc><table><row><cell>Method</cell><cell cols="4">ğ›¼-NDCG@5 ğ›¼-NDCG@10 ERR-IA@5 ERR-IA@10</cell></row><row><cell>MMR</cell><cell>0.2753</cell><cell>0.2979</cell><cell>0.2005</cell><cell>0.2309</cell></row><row><cell>xQuAD</cell><cell>0.3165</cell><cell>0.3941</cell><cell>0.2314</cell><cell>0.2890</cell></row><row><cell>PM-2</cell><cell>0.3047</cell><cell>0.3730</cell><cell>0.2298</cell><cell>0.2814</cell></row><row><cell>SVM-DIV</cell><cell>0.3030</cell><cell>0.3699</cell><cell>0.2268</cell><cell>0.2726</cell></row><row><cell>R-LTR</cell><cell>0.3498</cell><cell>0.4132</cell><cell>0.2521</cell><cell>0.3011</cell></row><row><cell>PAMM</cell><cell>0.3712</cell><cell>0.4327</cell><cell>0.2619</cell><cell>0.3029</cell></row><row><cell>NTN-DIV</cell><cell>0.3962</cell><cell>0.4577</cell><cell>0.2773</cell><cell>0.3285</cell></row><row><cell>MDP-DIV</cell><cell>0.4189</cell><cell>0.4762</cell><cell>0.2988</cell><cell>0.3494</cell></row><row><cell>M 2 DIV</cell><cell>0.4429</cell><cell>0.4839</cell><cell>0.3445</cell><cell>0.3658</cell></row><row><cell>DNN(softmax)</cell><cell>0.4280</cell><cell>0.4676</cell><cell>0.3293</cell><cell>0.3496</cell></row><row><cell>DNN(R-LTR)</cell><cell>0.4149</cell><cell>0.4517</cell><cell>0.3265</cell><cell>0.3454</cell></row><row><cell>DNN-LC(ğ›¼-DCG)</cell><cell>0.4968  *</cell><cell>0.5322  *</cell><cell>0.3868  *</cell><cell>0.4068  *</cell></row><row><cell>DIN-LC(ğ›¼-DCG)</cell><cell>0.5009  *</cell><cell>0.5294  *</cell><cell>0.3942  *</cell><cell>0.4119  *</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Benefits of the ğ›¼-DCG loss, by comparing with R-LTR loss. ' * ' indicates statistically significant improvement over DNN(R-LTR). ' â€  ' indicates statistically significant improvement over DNN-LC(R-LTR).</figDesc><table><row><cell>Method</cell><cell cols="4">ğ›¼-NDCG@5 ğ›¼-NDCG@10 ERR-IA@5 ERR-IA@10</cell></row><row><cell>DNN(R-LTR)</cell><cell>0.4149</cell><cell>0.4517</cell><cell>0.3265</cell><cell>0.3454</cell></row><row><cell>DNN(ğ›¼-DCG)</cell><cell>0.4614  *</cell><cell>0.5005  *</cell><cell>0.3633</cell><cell>0.3838  *</cell></row><row><cell>DNN-LC(R-LTR)</cell><cell>0.4451</cell><cell>0.4842</cell><cell>0.3483</cell><cell>0.3690</cell></row><row><cell>DNN-LC(ğ›¼-DCG)</cell><cell>0.4968  â€ </cell><cell>0.5322  â€ </cell><cell>0.3868  â€ </cell><cell>0.4068</cell></row></table><note>â€ Process (MDP) to model the diverse ranking process; M 2 DIV [18]: a state-of-the-art reinforcement learning approach which incorporates Monte Carlo Tree Search (MCTS) to enhance the MDP. M 2 DIV-LC:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Performance of the latent cross features and the groupwise scoring functions. ' * ' indicates statistically significant improvement over DNN(ğ›¼-DCG). ' â€  ' indicates statistically significant improvement over DIN(ğ›¼-DCG).</figDesc><table><row><cell>Method</cell><cell cols="4">ğ›¼-NDCG@5 ğ›¼-NDCG@10 ERR-IA@5 ERR-IA@10</cell></row><row><cell>M 2 DIV</cell><cell>0.4429</cell><cell>0.4839</cell><cell>0.3445</cell><cell>0.3658</cell></row><row><cell>M 2 DIV-LC</cell><cell>0.4551</cell><cell>0.4971</cell><cell>0.3509</cell><cell>0.3735</cell></row><row><cell>DNN(ğ›¼-DCG)</cell><cell>0.4614</cell><cell>0.5005</cell><cell>0.3633</cell><cell>0.3838</cell></row><row><cell>DNN-LC(ğ›¼-DCG)</cell><cell>0.4968  *</cell><cell>0.5322  *</cell><cell>0.3868</cell><cell>0.4068</cell></row><row><cell>DIN(ğ›¼-DCG)</cell><cell>0.4615</cell><cell>0.5041</cell><cell>0.3582</cell><cell>0.3808</cell></row><row><cell>DIN-LC(ğ›¼-DCG)</cell><cell>0.5009  â€ </cell><cell>0.5294</cell><cell>0.3942  â€ </cell><cell>0.4119</cell></row><row><cell>GSF(ğ›¼-DCG)</cell><cell>0.4568</cell><cell>0.5023</cell><cell>0.3569</cell><cell>0.3802</cell></row><row><cell>GSF-LC(ğ›¼-DCG)</cell><cell>0.4865</cell><cell>0.5219</cell><cell>0.3786</cell><cell>0.4003</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison of variants of ğ›¼-DCG loss</figDesc><table><row><cell>Method</cell><cell cols="4">ğ›¼-NDCG@5 ğ›¼-NDCG@10 ERR-IA@5 ERR-IA@10</cell></row><row><cell>ğ›¼-DCG (T=0.1)</cell><cell>0.4968</cell><cell>0.5322</cell><cell>0.3868</cell><cell>0.4068</cell></row><row><cell>ğ›¼-DCG (T=1.0)</cell><cell>0.4811</cell><cell>0.5184</cell><cell>0.3703</cell><cell>0.3912</cell></row><row><cell>ğ›¼-DCG (T=0.01)</cell><cell>0.4715</cell><cell>0.4978</cell><cell>0.3633</cell><cell>0.3799</cell></row><row><cell>Gumbel-ğ›¼-DCG</cell><cell>0.4970</cell><cell>0.5339</cell><cell>0.3855</cell><cell>0.4066</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Performance comparison of variants of self attention layers</figDesc><table><row><cell cols="2">(ğ¿, ğ» , ğ‘§)</cell><cell cols="9">ğ›¼-NDCG@5 ğ›¼-NDCG@10 ERR-IA@5 ERR-IA@10</cell></row><row><cell cols="2">(1, 1, 256)</cell><cell cols="2">0.4724</cell><cell></cell><cell>0.5182</cell><cell></cell><cell cols="2">0.3679</cell><cell></cell><cell>0.3915</cell></row><row><cell cols="2">(1, 2, 256)</cell><cell cols="2">0.4761</cell><cell></cell><cell>0.5139</cell><cell></cell><cell cols="2">0.3706</cell><cell></cell><cell>0.3908</cell></row><row><cell cols="2">(1, 3, 256)</cell><cell cols="2">0.4893</cell><cell></cell><cell>0.5224</cell><cell></cell><cell cols="2">0.3801</cell><cell></cell><cell>0.3993</cell></row><row><cell cols="2">(1, 4, 256)</cell><cell cols="2">0.4895</cell><cell></cell><cell>0.5252</cell><cell></cell><cell cols="2">0.3810</cell><cell></cell><cell>0.4010</cell></row><row><cell cols="2">(2, 1, 256)</cell><cell cols="2">0.4918</cell><cell></cell><cell>0.5299</cell><cell></cell><cell cols="2">0.3842</cell><cell></cell><cell>0.4052</cell></row><row><cell cols="2">(2, 2, 256)</cell><cell cols="2">0.5009</cell><cell></cell><cell>0.5294</cell><cell></cell><cell cols="2">0.3942</cell><cell></cell><cell>0.4119</cell></row><row><cell cols="2">(2, 3, 256)</cell><cell cols="2">0.4902</cell><cell></cell><cell>0.5224</cell><cell></cell><cell cols="2">0.3800</cell><cell></cell><cell>0.3991</cell></row><row><cell cols="2">(2, 4, 256)</cell><cell cols="2">0.5066</cell><cell></cell><cell>0.5344</cell><cell></cell><cell cols="2">0.3950</cell><cell></cell><cell>0.4122</cell></row><row><cell cols="2">(2, 2, 128)</cell><cell cols="2">0.4931</cell><cell></cell><cell>0.5295</cell><cell></cell><cell cols="2">0.3842</cell><cell></cell><cell>0.4048</cell></row><row><cell cols="2">(2, 2, 64)</cell><cell cols="2">0.4908</cell><cell></cell><cell>0.5245</cell><cell></cell><cell cols="2">0.3796</cell><cell></cell><cell>0.3993</cell></row><row><cell></cell><cell>0.530</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.525</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Î±-NDCG@10</cell><cell>0.515 0.520</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.510</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0.05</cell><cell>0.10</cell><cell>0.15</cell><cell>0.20</cell><cell>0</cell><cell>0.05</cell><cell>0.10</cell><cell>0.15</cell><cell>0.20</cell></row><row><cell></cell><cell></cell><cell cols="3">probability of duplication</cell><cell></cell><cell></cell><cell cols="3">probability of duplication</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>6.2.2 Variants of self-attention layers.We study the variants of listwise context embedding by tuning the self-attention layer hyperparameters in terms of number of attention layers ğ¿, number of attention heads in each layer ğ» , and the size of each attention head ğ‘§. In Table6, we have performances of DIN models with number of layers ğ¿ = 1, 2 and number of heads ğ» = 1, 2, 3, 4 given head size ğ‘§ = 256 and DIN models with head size ğ‘§ = 64, 128, and 256 given ğ¿</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Please note that this query is different from the search query.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://plg.uwaterloo.ca/ trecweb/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://github.com/sweetalyssum/M2DIV/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">We reproduced M 2 DIV and M 2 DIV-LC of the MCTS methods, but recalled their public results on other baselines in<ref type="bibr" target="#b17">[18]</ref>.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diversifying Search Results</title>
		<author>
			<persName><forename type="first">Rakesh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sreenivas</forename><surname>Gollapudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Halverson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Ieong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Web Search and Data Mining (WSDM)</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="5" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning a deep listwise context model for ranking refinement</title>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keping</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Research and Development in Information Retrieval (SIGIR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning Groupwise Multivariate Scoring Functions Using Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Bruch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Golbandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
		<idno>ICTIR. 85-92</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent Cross: Making Use of Context in Recurrent Recommender Systems</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sagar</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vince</forename><surname>Gatto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Web Search and Data Mining (WSDM)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="46" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Identifying and Filtering Near-Duplicate Documents</title>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">Z</forename><surname>Broder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Symposium on Combinatorial Pattern Matching</title>
				<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Stochastic Treatment of Learning to Rank Scoring Functions</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Bruch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Web Search and Data Mining (WSDM)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="61" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Revisiting Approximate Metric Optimization in the Age of Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nima Bruch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masrour</forename><surname>Zoghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Research and Development in Information Retrieval (SIGIR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1241" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning to Rank with Nonsmooth Cost Functions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Ragno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Viet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>In NeurIPS. 193-200</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The use of MMR, diversity-based reranking for reordering documents and producing summaries</title>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<biblScope unit="issue">SIGIR</biblScope>
			<biblScope unit="page" from="335" to="336" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Expected Reciprocal Rank for Graded Relevance</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metlzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Grinspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Information and Knowledge Management (CIKM)</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="621" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Expected Reciprocal Rank for Graded Relevance</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Grinspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Information and Knowledge Management (CIKM)</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="621" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Novelty and diversity in information retrieval evaluation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maheedhar</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azin</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Ashkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>BÃ¼ttcher</surname></persName>
		</author>
		<author>
			<persName><surname>Mackinnon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Research and Development in Information Retrieval (SIGIR)</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="659" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Diversity by proportionality: an electionbased approach to search result diversification</title>
		<author>
			<persName><forename type="first">Van</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Research and Development in Information Retrieval (SIGIR)</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter</title>
				<imprint>
			<publisher>Human Language Technologies (NAACL-HLT</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the local optimality of lambdarank</title>
		<author>
			<persName><forename type="first">Pinar</forename><surname>Donmez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krysta</forename><forename type="middle">M</forename><surname>Svore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval</title>
				<meeting>the 32nd international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="460" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07">2011. July 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">From Greedy Selection to Exploratory Decision-Making: Diverse Ranking with Policy-Value Networks</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Research and Development in Information Retrieval (SIGIR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="125" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.08850</idno>
		<title level="m">Stochastic optimization of sorting networks via continuous relaxations</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning-to-Rank with BERT in TF-Ranking</title>
		<author>
			<persName><forename type="first">Shuguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08476</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Information and Knowledge Management (CIKM)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to Diversify Search Results via Subtopic Attention</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Research and Development in Information Retrieval (SIGIR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="545" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">LightGBM: A highly efficient gradient boosting decision tree</title>
		<author>
			<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distributed Representations of Sentences and Documents</title>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">DV-GAN: A Minimax Game for Search Result Diversification Combining Explicit and Implicit Features</title>
		<author>
			<persName><forename type="first">Jiongnan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuqi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Research and Development in Information Retrieval (SIGIR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="479" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to rank for information retrieval</title>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="225" to="331" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jirong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.05891</idno>
		<title level="m">SetRank: Learning a Permutation-Invariant Ranking Model for Information Retrieval</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">TF-Ranking: Scalable tensorflow library for learning-to-rank</title>
		<author>
			<persName><forename type="first">Rama</forename><surname>Kumar Pasumarthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Bruch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Pfeifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Golbandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on KnowledgeDiscovery and Data Mining (KDD)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2970" to="2978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Permutation Equivariant Document Interaction Network for Neural Learning to Rank</title>
		<author>
			<persName><forename type="first">Rama</forename><surname>Kumar Pasumarthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglei</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 ACM SIGIR on International Conference on Theory of Information Retrieval</title>
				<meeting>the 2020 ACM SIGIR on International Conference on Theory of Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="145" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A general approximation framework for direct optimization of information retrieval measures</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="375" to="397" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Diversifying Search Results using Self-Attention Network</title>
		<author>
			<persName><forename type="first">Xubo</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information and Knowledge Management</title>
				<meeting>the 29th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Are Neural Rankers still Outperformed by Gradient Boosted Decision Trees</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglei</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rama</forename><surname>Kumar Pasumarthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the The Ninth International Conference on Learning Representations (ICLR)</title>
				<meeting>the The Ninth International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Exploiting query reformulations for web search result diversification</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Rodrygo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iadh</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference (WWW)</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="881" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning Optimally Diverse Rankings over Large Document Collections</title>
		<author>
			<persName><forename type="first">Aleksandrs</forename><surname>Slivkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sreenivas</forename><surname>Gollapudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on International Conference on Machine Learning (ICML&apos;10</title>
				<meeting>the 27th International Conference on International Conference on Machine Learning (ICML&apos;10</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="983" to="990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning optimally diverse rankings over large document collections</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Slivkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sreenivas</forename><surname>Gollapudi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Åukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The LambdaLoss Framework for Ranking Metric Optimization</title>
		<author>
			<persName><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Golbandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Information and Knowledge Management (CIKM)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1313" to="1322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Practical diversified recommendations on youtube with determinantal point processes</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajith</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Bonomo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sagar</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Gillenwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
				<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2165" to="2173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning maximal marginal relevance model via directly optimizing diversity evaluation measures</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Research and Development in Information Retrieval (SIGIR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="113" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Modeling document novelty with neural tensor network for search result diversification</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Research and Development in Information Retrieval (SIGIR)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="395" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adapting Markov Decision Process for Search Result Diversification</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Research and Development in Information Retrieval (SIGIR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="535" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Supervised approaches for explicit search result diversification</title>
		<author>
			<persName><forename type="first">Sevgi</forename><surname>Yigit-Sert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ismail</forename><surname>Sengor Altingovde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page">102356</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note>Iadh Ounis, and Ã–zgÃ¼r Ulusoy</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Predicting diverse subsets using structural SVMs</title>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on Machine learning (ICML)</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1224" to="1231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Beyond independent relevance: methods and evaluation metrics for subtopic retrieval</title>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Research and Development in Information Retrieval (SIGIR</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning for search result diversification</title>
		<author>
			<persName><forename type="first">Yadong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuzi</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Research and Development in Information Retrieval (SIGIR)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="293" to="302" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
