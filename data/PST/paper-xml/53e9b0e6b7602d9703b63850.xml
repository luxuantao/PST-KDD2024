<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamically Managed Multithreaded Reconfigurable Architectures for Chip Multiprocessors</title>
				<funder>
					<orgName type="full">Intel</orgName>
				</funder>
				<funder ref="#_hf6vCN6 #_j8pcFjy #_dpT4R3g #_C284MyN">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Matthew</forename><forename type="middle">A</forename><surname>Watkins</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">David</forename><forename type="middle">H</forename><surname>Albonesi</surname></persName>
							<email>albonesi@csl.cornell.edu</email>
						</author>
						<author>
							<persName><forename type="first">Spl</forename><surname>Core0</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Conf</forename><surname>Output</surname></persName>
						</author>
						<author>
							<persName><surname>Id</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Spl</forename><surname>Core3</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Systems Laboratory</orgName>
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Computer Systems Laboratory</orgName>
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamically Managed Multithreaded Reconfigurable Architectures for Chip Multiprocessors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>C.5.3 [Computer System Implementation]: Microcomputers-Microprocessors; C.1.3 [Processor Architectures]: Other Architecture Styles-Adaptable Architectures Design</term>
					<term>Performance Shared Resource Management</term>
					<term>Reconfigurable Architecture</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prior work has demonstrated that reconfigurable logic can significantly benefit certain applications. However, reconfigurable architectures have traditionally suffered from high area overhead and limited application coverage. We present a dynamically managed multithreaded reconfigurable architecture consisting of multiple clusters of shared reconfigurable fabrics that greatly reduces the area overhead of reconfigurability while still offering the same power efficiency and performance benefits. Like other shared SMT and CMP resources, the dynamic partitioning of the reconfigurable resource among sharing threads, along with the co-scheduling of threads among different reconfigurable clusters, must be intelligently managed for the full benefits of the shared fabrics to be realized.</p><p>We propose a number of sophisticated dynamic management approaches, including the application of machine learning, multithreaded phase-based management, and stability detection. Overall, we show that, with our dynamic management policies, multithreaded reconfigurable fabrics can achieve better energy?delay 2 , at far less area and power, than providing each core with a much larger private fabric. Moreover, our approach achieves dramatically higher performance and energy-efficiency for particular workloads compared to what can be ideally achieved by allocating the fabric area to additional cores.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Reconfigurable logic has been proposed as one possible way to improve the performance and power efficiency of microprocessors <ref type="bibr" target="#b24">[23,</ref><ref type="bibr" target="#b42">41]</ref>. Researchers have proposed specialized fabrics that are specifically designed for more efficient integration with general purpose processors than conventional FPGAs <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b23">22,</ref><ref type="bibr" target="#b45">44]</ref>. Despite these advances in fabric architecture, reconfigurable logic still incurs non-trivial power and area costs relative to the fixed hardware functionality of commercial microprocessors <ref type="bibr" target="#b29">[28]</ref>. These costs are especially important given the disparity in benefit that different applications can expect to receive from reconfigurable fabrics, from orders of magnitude benefit to no benefit at all.</p><p>The ability to integrate multiple cores and reconfigurable logic on a single die afforded by the billion transistor era provides a new opportunity to address these issues. This paper proposes dynamically managed multithreaded reconfigurable fabrics that, similar to shared SMT resources and last level caches, are shared among several cores of a CMP in order to save area and increase fabric utilization. Since the degree of fabric sharing must necessarily be limited, multiple clusters of cores sharing a common Specialized Programmable Logic (SPL) fabric may be implemented, depending on the expected percentage of applications that can be accelerated by the SPL. Like other shared resources in a CMP of SMT cores, where the partitioning of resources among the competing threads on a given SMT core and the co-scheduling of threads to multiple SMT cores significantly impact performance, the control of multiple multithreaded SPL clusters must be intelligently managed for good performance to be achieved. Specifically, such a manager must make two interrelated decisions: (1) determine the best match of threads to the multiple clusters of SPL, considering the interplay between different threads; and (2) decide when and how best to spatially partition each fabric on-the-fly in order to reduce contention among the threads, at the potential cost of degraded throughput.</p><p>In this paper, we explore a number of approaches to this complex management problem that range in sophistication from simple interval-based heuristic approaches to more advanced techniques that apply machine learning, multithreaded phase optimization, and stability analysis. Our algorithms permit the use of very compact SPL fabrics that are performance competitive (on multiple mixed sequential and parallel workloads with high SPL demand) with large private SPL attached to each core, while consuming several times less die area and energy. Moreover, we show that replacing the SPL with additional cores degrades performance by 62-143% for our workloads, demonstrating the benefit of dynamically managed clusters of multithreaded SPL.</p><p>In the next section, we describe the architecture of a CMP with embedded SPL, followed by our management policies in Section 3. Our evaluation methodology is presented in Section 4. Section 5 evaluates our approach. We describe related work in Section 6 and conclude in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MULTITHREADED RECONFIGURABLE ARCHITECTURES</head><p>Figure <ref type="figure" target="#fig_0">1</ref>(a) shows an overall depiction of a hypothetical 18 core CMP with three clusters 1 , with the external interface not shown for simplicity. Each of the two clusters on the left hand side consists of a multithreaded SPL fabric shared by four single issue out-of-order processor cores. In our previous work <ref type="bibr" target="#b44">[43]</ref>, we evaluated the use of SPL with a range of in-order and out-of-order core types and found that a simple out-of-order core coupled with SPL provided the best areaequivalent performance and power efficiency. Moreover, similar to SMT processors where adding additional contexts provides limited benefit beyond a certain point <ref type="bibr" target="#b43">[42]</ref>, sharing an SPL among four cores was shown to be the best trade-off between SPL fabric utilization and contention among com- 1 Although relative sizes of the cores and SPL are accurate, this is not intended to represent an actual floorplan. peting threads. To confirm that this result holds true for our set of workloads, we evaluated systems with both two 4way and one 8-way shared SPL and found that, in all cases, two 4-way shared SPL clusters outperformed a single 8-way shared SPL. While the techniques presented in this paper apply to any degree of sharing, we assume a 4-way shared SPL in the remainder of this paper.</p><p>In the "conventional" cluster on the right hand side of Figure <ref type="figure" target="#fig_0">1</ref>(a), each SPL has been replaced by one additional core, giving 10 cores in total. Applications that do not benefit from the SPL run on this conventional cluster, while those that can exploit the SPL run on one of the two left clusters. Of course, different mixes of SPL and conventional clusters (as well as other cluster types) are possible, but this consideration is beyond the scope of this paper.</p><p>Each SPL, adopted from <ref type="bibr" target="#b44">[43]</ref> and shown in more detail in Figures <ref type="figure" target="#fig_1">1(b</ref>) and (c), is a highly pipelined row-based <ref type="bibr" target="#b23">[22,</ref><ref type="bibr" target="#b45">44]</ref> programmable fabric that is temporally shared among the four cores. Each of the two clusters incorporates hardware monitors that capture cycle-level event counts relevant to application characteristics and SPL usage. As is shown in Figure <ref type="figure" target="#fig_0">1</ref>(d), the SPL Cluster Manager periodically reads the monitored information in order to assign threads to clusters, and to spatially partition each SPL as appropriate to optimize performance and power efficiency. In the remainder of Section 2, we provide an overview of the SPL microarchitecture. A more complete treatment is available in <ref type="bibr" target="#b44">[43]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rows/ SPL</head><note type="other">Total</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SPL Hardware Microarchitecture</head><p>We adopt the row-based fabric of our earlier work <ref type="bibr" target="#b44">[43]</ref> in which the space of shared SPL configurations was explored using validated SPL delay, power, and area models. The SPL is tightly integrated with the processor core as a reconfigurable functional unit, and is interfaced to the memory system via a queue-based decoupled architecture (Figure <ref type="figure" target="#fig_0">1</ref>(b)). The input queue matches the SPL row input width (512 bits) and special SPL load instructions place values into the queue at a particular data alignment. Likewise, the SPL writes to an output queue and the head entry is written out to the Store Queue using special SPL store instructions. Since the normal LSQ/cache datapath is used for data transfer, no additional steps are needed to handle memory dependencies with the processor core.</p><p>The SPL itself is composed of 8-bit wide computation cells. The same operation is performed on all 8 bits within a cell. Sixteen of these 8-bit cells are arranged in a row to form a 128-bit wide row. Each cell in a row can perform a different operation on its set of inputs and n of these rows are grouped together to form the overall SPL fabric. Figure <ref type="figure" target="#fig_0">1</ref>(c) shows the row and cell design. Feedback within a single row is allowed. For our benchmarks, 12 rows of private (per-core) SPL permits all but one of the configurations, the major loop within crypt, to achieve maximum performance <ref type="foot" target="#foot_0">2</ref> . In 65 nm technology, the SPL can be clocked at 500 MHz, one-fourth that of the processor core frequency of 2GHz (the same as the Pentium Core2 Duo <ref type="bibr" target="#b25">[24]</ref> and the AMD X2 Dual-Core <ref type="bibr" target="#b1">[1]</ref>, both of which are implemented in 65 nm). This latency permits each row to complete the longest possible computation in a single cycle. The SPL includes integrated on-chip storage for 12 configurations to allow for fast switching between different configurations. For our workloads, this permits all configurations for any phase to reside on-chip. Thus, reconfiguration latency is not an issue as all configurations are immediately available after the initial configuration overhead is paid.</p><p>Using our analytical models <ref type="bibr" target="#b44">[43]</ref>, we arrive at the area and power results for eight single issue out-of-order cores, eight private 12-row SPLs, and two four-way shared SPLs shown in Table <ref type="table" target="#tab_0">1</ref>. Although the area is prohibitive, the 12row private SPL serves as the baseline for comparison with the dynamically managed multithreaded SPL architecture. The latter is much more compact, requiring 4X less area than the private SPLs, and is much more power-efficient as well. While one might consider shrinking the private SPL even further, our previous work <ref type="bibr" target="#b44">[43]</ref> has shown that that this yields poor performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">SPL Virtualization</head><p>Virtualizing reconfigurable hardware was proposed by <ref type="bibr" target="#b3">[3]</ref> to allow a fabric to execute a configuration that requires more resources (i.e., rows) than are physically available. Although throughput is reduced when the design must be virtualized, virtualization permits the designer to trade performance for area. As more area becomes available (or for higher-end chips) larger fabrics can be created without requiring any change to the application mappings.</p><p>Virtualization is accomplished by using the same physical row to execute multiple virtual rows. For example, when executing a configuration requiring six rows on a fabric with only three physical rows, virtual rows 1 and 4 execute in physical row 1, virtual rows 2 and 5 in physical row 2, etc. In this example, this leads to a maximum 50% reduction in throughput relative to the unvirtualized case as new data can be inserted only half as often.</p><p>For shared fabrics the number of rows available to a function is not known at application design time even for a particular fabric implementation as the function may not be allocated the entire SPL. As such, virtualization is especially useful for shared fabrics as it allows all SPL functions to be executed, albeit with possibly different throughput, regardless of the number of rows that are allocated at runtime. shows how the SPL design permits temporal sharing among the threads executing on the four processor cores, as well as spatial partitioning to permit private or semi-private operation <ref type="bibr" target="#b44">[43]</ref>. Spatial partitioning is enabled by inserting additional multiplexers at each point where the SPL pool might be partitioned. To keep the hardware overhead reasonable, each of the two SPLs can only be divided into two halves, four quarters, or one half and two quarters, requiring a total of only four sets of input multiplexers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Temporal Sharing and Spatial Partitioning</head><p>With temporal sharing, all rows of the temporally shared fabric (which may be the whole SPL or a subset, depending on whether spatial partitioning is being used) are available to the sharing threads in a time multiplexed fashion. The control for temporal sharing is implemented in hardware on a fine-grain, cycle-level basis to avoid thread starvation for the shared SPL resource. Each SPL cycle, a round-robin scheduler selects an instruction from one of the queues to issue to the shared SPL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SPL Function Mapping</head><p>The SPL is used to accelerate a wide range of operations. We show one such example from the SPEC 2006 application 456.hmmer. We accelerate the P7Viterbi function, which accounts for 85% of the program execution time. The core loop of the function is shown in Figure <ref type="figure">2(a)</ref>. Figure <ref type="figure">2(b)</ref> shows how the portion of the code that calculates mc is mapped to the SPL. In the optimized code, the core first loads the input values needed to compute mc into the fabric, the SPL computes the value of mc, and finally the core receives the result. After receiving mc, the core computes the values of dc and ic and repeats the loop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DYNAMIC MANAGEMENT OF MULTI-THREADED FABRICS</head><p>Having provided background on the fabric architecture, we now present our dynamic runtime management policies  for multiple multithreaded fabrics. The objective of these policies is to achieve approximately the same performance as private (per-core) fabrics with the dramatically lower area and power consumption afforded by multithreaded fabrics. When multiple sequential and parallel applications that are compiled to use the SPL are simultaneously executing, the SPL Cluster Manager (Figure <ref type="figure" target="#fig_0">1</ref>(d)) optimizes overall performance through two inter-dependent mechanisms: (1) thread assignment among the clusters, and (2) spatial partitioning and recombination of the SPL within each cluster.</p><formula xml:id="formula_0">M; k++) { mc[k] = mpp[k-1] + tpmm[k-1]; if ((sc = ip[k-1] + tpim[k-1]) &gt; mc[k]) mc[k] = sc; if ((sc = dpp[k-1] + tpdm[k-1]) &gt; mc[k]) mc[k] = sc; if ((sc = xmb + bp[k]) &gt; mc[k]) mc[k] = sc; mc[k] += ms[k]; if (mc[k] &lt; -INFTY) mc[k] = -INFTY; dc[k] = dc[k-1] + tpdd[k-1]; if ((sc = mc[k-1] + tpmd[k-1]) &gt; dc[k]) dc[k] = sc; if (dc[k] &lt; -INFTY) dc[k] = -INFTY; if (k &lt; M) { ic[k] = mpp[k] + tpmi[k]; if ((sc = ip[k] + tpii[k]) &gt; ic[k]) ic[k] = sc; ic[k] += is[k]; if (ic[k] &lt; -INFTY) ic[k] = -INFTY; } } (a)</formula><p>There are a number of different factors that contribute to the SPL usage characteristics of an application, including the frequency of SPL accesses and the number of rows needed by each optimized function. The applications that are the biggest concern are those that either make frequent accesses to the SPL or require a large number of rows and therefore incur frequent virtualization. Applications that make frequent SPL accesses can be substantially impacted by poor scheduling, whereas applications with significant virtualization can substantially degrade the performance of other applications sharing the same cluster. We implemented and evaluated a wide range of management policies given the considerations listed in Table <ref type="table" target="#tab_1">2</ref>. We limit our discussion to four representative dynamic management algorithms.</p><p>Threads can also be assigned to a particular cluster on a CMP statically through the OS scheduler (in fact, we assume some initial static assignment for our dynamic policies). As we show later, the performance of static thread scheduling varies greatly, with slowdowns ranging from less than 1% to 1028% compared to a 12-row private SPL. Moreover, static scheduling requires dependable a priori knowledge about the threads and their potential interactions. Finally, many programs go through different phases during execution and their SPL usage can differ substantially in each phase. Static scheduling cannot exploit this dynamic phase behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Per Interval Thread Assignment Policies</head><p>We first investigated a number of policies that determine an assignment of threads to SPL clusters every interval based solely on the performance of the previous interval and make no use of the spatial partitioning capability of the SPL. We found that, although all applications are impacted by poor scheduling choices, certain applications are impacted more than others. In particular, the largest performance losses occur when threads that require large amounts of virtualization share an SPL cluster with those that rely heavily on the SPL. Based on this insight, the best interval-based thread assignment policy that we devised is Average Row Assignment. This policy uses the average number of rows used by the functions of a particular thread as an indicator of its degree of virtualization. Functions that require a large number of rows on average will experience more virtualization, assuming the amount exceeds the number of physical rows available. Thread assignment based on the number of rows alone, however, is insufficient; the SPL access frequency should also be taken into account as an indication of how much each thread relies on the SPL. Threads that heavily utilize the SPL are more likely to be degraded by increased wait time to access the fabric.</p><p>Average Row Assignment allocates threads to clusters based on the ratio of the average number of rows used by the thread to SPL accesses. Threads with high access rates and low row usage will have small values while threads with infrequent accesses and high row usage will have high values.</p><p>To assign threads to clusters we use a split assignment policy which aims to schedule threads with high and low values on different clusters. The threads are sorted based on the given metric, the first n/c threads are assigned to the first cluster, the next n/c threads to the second, and so on, where n is the number of threads and c is the number of clusters.</p><p>In order to compute the overall metric, each core maintains counters for instructions issued in the last interval. The core also tracks the number of rows required by each SPL instruction. This latter information is stored in the configuration information for each SPL function and is therefore available to the SPL Cluster Manager.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Composite Thread Assignment / Spatial Partitioning Policies</head><p>Average Row Assignment only considers thread assignment. As described previously, each SPL can also be spatially partitioned. This can be useful if SPL instructions are queued for a long time due to virtualization or due to high SPL usage from the number of threads sharing the SPL. Spatial partitioning can reduce stalls due to either of these cases as it reduces the number of threads that share the same SPL partition.</p><p>When considering both thread assignment and spatial partitioning, the number of clusters is effectively dynamic, as each SPL can be divided in half, in quarters, or in one half and two quarters, and thread assignment must account for this cluster size variability. As before, per-core metrics are gathered to determine how to assign threads to however many clusters currently exist. Moreover, the SPL Cluster Manager must determine when to split and merge SPL partitions in each cluster.</p><p>To determine thread-to-cluster assignments, this policy, henceforth referred to as Composite, uses the same SPL access to average row ratio with split assignment used by the Average Row scheduler. To determine when to split an SPL cluster, each core tracks the number of cycles an SPL instruction is stalled in the SPL queue and the number of its SPL instructions that are issued. If any thread spends too long on average waiting to issue an SPL instruction, i.e., the average wait time exceeds a threshold, then the SPL is split. Similarly, to determine when to merge, each cluster tracks the number of threads whose average wait time is less than a second threshold. If the sum of this value for the two clusters is greater than the current number of cores sharing a single cluster, then the two clusters are merged. Neither a split nor a merge will occur if both split and merge requests are received for the same cluster in a given interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning-Inspired SPL Cluster Management</head><p>The policies discussed thus far create their mappings of threads to clusters based solely on the relative ranking of some statistics for each thread during the last interval. Although this generally leads to good mappings, it may not produce the best possible mapping. In an attempt to achieve the best -or at least a better -mapping, we apply machine learning techniques to our cluster mapping problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Hill Climbing</head><p>Since we desire a fast, purely online approach, we focus on hill climbing. Previous work by Choi and Yeung <ref type="bibr" target="#b10">[10]</ref> investigated hill climbing for SMT resource allocation among concurrently running threads. Our scheduling problem is significantly different, and arguably harder, as we have to deal with both resource partitioning and determining which threads should share those partitions. At each scheduling interval the manager may perform one of the following actions: (a) swap two threads from different SPL clusters; (b) split or merge an SPL cluster that is not already at its minimum or maximum size; (c) create a random number of partitions as well as a random mapping of threads to those partitions. The last option adds an element of stochasticity which aims to escape local minima. Each of these options is selected with a predefined probability. We investigate a variety of different restrictions on thread swapping, from allowing only threads using the SPL to be swapped, to allowing any threads to be swapped, to allowing a single thread to be swapped into a cluster with an unused core.</p><p>The new assignment is run for the next interval. At the end, the performance of the interval is compared to the performance of the best interval to date for the current phase. If the new mapping achieves better performance, then it is set as the new best mapping; otherwise, the mapping reverts to the previous best mapping. In either case, a new local search step is applied to the current best mapping. After some number of consecutive unbeneficial steps, the best schedule is assumed to have been found and the phase is declared stable. After this point all future intervals in this phase use this stable mapping.</p><p>To identify phases, we developed a multi-threaded/multiprogrammed workload phase tracker. We use the phase tracker of Sherwood et al. <ref type="bibr" target="#b34">[33]</ref> to identify phases for each thread. The phase tracker reports the current phase for each running thread based on the mix of instructions executed during the last phase interval. This phase information is combined to index into a global management history table, which contains the best mapping executed so far for the given set of phases. In order to create a reasonably sized index to access the history table, we developed a hash function to map the application and phase IDs of all currently running threads to a reasonable number of bits. This function takes three byte groups of phase and application IDs (where each phase or application ID is one byte) and XORs them together as shown in Figure <ref type="figure" target="#fig_3">3</ref>. The IDs are ordered by application ID. The IDs within every other group are rotated by four bytes to increase diversity. This hashing scheme produces less than a 3% average false match rate for our workloads, and more importantly, degrades performance over a perfect hashing scheme by less than 0.1%.</p><p>To determine the relative performance of different mappings, the manager tracks the peak number of instructions graduated by each thread on a per phase basis and calculates the performance degradation for the current phase relative to this peak performance. The performance degradation of all threads are averaged to produce the overall degradation for the interval. The peak instruction count is determined by averaging the five highest observed graduation rates in that phase.</p><p>As will be shown in Section 5, our best hill climbing algorithm is able to match the performance of the Composite manager, but rarely exceeds it. This is due to the large performance degradation that can occur during some of the exploration intervals, and so any slight improvement in mapping over that found by the Composite algorithm is offset by the degradation incurred during the exploration period. Unlike typical pipeline resource allocation, small changes in the thread to SPL assignment can substantially change performance. This effect not only makes finding the optimal mapping difficult, it also significantly degrades throughput (by up to an order of magnitude) during intervals with poor mappings. If too many of these poor mappings are explored, performance degrades severely. Due to these large jumps, the exploration space is not necessarily nicely hill shaped; it is not only quite "bumpy" but there are likely to be numerous local minima that may be difficult to escape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Hybrid Heuristic-Hill Climbing Manager</head><p>Based on our experience with the previous techniques, and additional experimentation, we devised a hybrid manager that addresses three main sources of performance degradation of the prior approaches. The first two sources are present in the heuristic techniques and the last appears with Hill Climbing. First, most programs experience different phases in their execution, during which their use of the shared SPL may vary significantly. As such, the best mapping for one phase may be suboptimal for another, and reaching a new stable mapping may take multiple intervals using the aforementioned interval-based policies. Second, even within a phase there can be a small amount of variability in the performance of a thread. This variation can lead to a ping-ponging effect where threads are constantly being swapped between two clusters. This is especially true for multithreaded workloads where multiple threads can be performing the same task and performance can vary slightly depending on interactions with memory and other threads. This constant swapping can degrade performance due to the overhead for context switching threads. Finally, as mentioned previously, excessive exploration of the assignment space can degrade performance due to the significant performance degradation experienced in certain assignments. To address these issues, we devise a new algorithm that we call Hybrid Heuristic-Hill Climbing (H3C) that combines elements of the previous two approaches and incorporates a stability threshold such that further changes are not made if the performance is within some margin of "optimal." H3C evaluates performance and maintains current assignments using the same phase-based approach as Hill Climbing. Unlike Hill Climbing, no change is made to the assignment for the next interval if the previous interval is determined to be stable. An interval is considered stable if the average performance degradation (as indicated by the graduation rate relative to peak, same as Hill Climbing) for all threads is less than some threshold.</p><p>When not stable, the assignment for the next interval is determined by one of two methods. During the first x intervals of a particular multithreaded/multiprogrammed phase the threads are assigned using the Composite algorithm from Section 3.2. The goal of this step is to create a generally  good mapping that can be fine tuned in the next step. During the next y intervals a learning-based local search like that described in Section 3.3.1 is used to try to improve upon the mapping produced by the Composite algorithm. After this step it is assumed that the "best" mapping has been found and no further exploration is performed for this phase, even if the average degradation is not less than the stable threshold in some future intervals. The complete set of H3C state transitions are shown in Figure <ref type="figure" target="#fig_4">4</ref>. At each interval, the management table keeps track of the best mapping found so far and H3C reverts back to that mapping as a starting point for the next management interval if the previous interval did not improve upon the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EVALUATION METHODOLOGY</head><p>We use a highly modified version of SESC <ref type="bibr" target="#b33">[32]</ref> to evaluate our proposed multithreaded SPL cluster management policies. We assume processors implemented in 65 nm technology running at 2.0 GHz with a 1.1V supply voltage. The major architectural parameters are shown in Table <ref type="table" target="#tab_3">3</ref>. We use Wattch, Cacti, and HotLeakage to model power.</p><p>To model the overhead associated with performing thread management, instruction fetch for all cores is stopped for 1000 cycles at the end of each interval. This value was determined by executing code approximating the scheduling algorithms on our simulator to get an accurate cycle estimate. After this period, the instructions for any threads being migrated are drained and execution is stopped for an additional 500 cycles (again determined by running the requisite code in the simulator) to model the time necessary to context switch all state -including internal SPL stateto the new core. Finally, the threads are started on their new cores, where warm-up of caches and TLBs is modeled. The processor undergoes a similar sequence when the SPL is spatially split or merged by the manager, although in this case the context switch and cache and TLB warm-up are not needed as threads continue to execute on the same core. We experimentally determined the best parameters for the dynamic policies, which are shown in Table <ref type="table" target="#tab_4">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Phase Tracking</head><p>We use the same parameters for our phase tracker as Sherwood et al. <ref type="bibr" target="#b34">[33]</ref> with the exception of the phase interval length. We use a smaller 1 million instruction interval due to the shorter phases of some of our applications. Given these parameters, we estimate that the tracker would require around 1 kB of storage per core. Actual phase changes in the program as detected by the phase tracker may not exactly </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Benchmarks</head><p>We create four workload mixes to evaluate the performance and power efficiency of our approach. Each workload consists of a combination of parallel and sequential benchmarks. These mixes reflect the type of workloads systems are apt to see in the future as different applications are likely to be parallelized to different degrees. We choose three single threaded benchmarks from SPEC2006 <ref type="bibr" target="#b38">[37]</ref>, one from SPEC2000 <ref type="bibr" target="#b37">[36]</ref>, and one from MediaBench <ref type="bibr" target="#b30">[29]</ref>. Our multithreaded workloads consist of two benchmarks from ALPBench <ref type="bibr" target="#b31">[30]</ref> and a version of the JavaGrande <ref type="bibr" target="#b35">[34]</ref> crypt benchmark ported to C++. We run the ALPBench version of MPGdec with two different command line parameters (-o0 and -o3) as they produce different execution characteristics. Specifically, the o3 version enables additional processing which makes use of the SPL, leading to increased overall SPL usage. A complete list of the benchmarks as well as their SPL usage characteristics can be found in Table <ref type="table" target="#tab_5">5</ref>. Table <ref type="table" target="#tab_6">6</ref> lists the benchmarks in each workload mix.</p><p>In order to create SPL mappings, we profile each benchmark to determine which functions consume the largest portion of total execution time. Following this, we examine each of the functions in order to determine which ones can be efficiently mapped to the SPL. Configurations are then created for those functions by hand, although previous work has shown that compilers can produce good mappings for reconfigurable architectures <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b46">45]</ref>. Since dynamic thread scheduling is most useful when applications experience phase changes, we need to run the benchmarks long enough to witness these phase changes. The best option is to run benchmarks to completion. Due to the long running time of SPEC benchmarks with reference inputs, however, we are only able to run our non-SPEC benchmarks to completion. For our SPEC benchmarks we use Early SimPoints <ref type="bibr" target="#b32">[31]</ref> to select two 250 million instruction SimPoints from the original source code (i.e., code not utilizing the SPL). Since using the SPL changes the number of instructions executed, we determine where each of the two SimPoints begin and end and augment the code to fast forward through all but these two intervals. In this way both the original and SPL versions of the code execute functionally equivalent amounts of code. We select relatively long  intervals to capture phase changes within an interval. In order to make our comparison fair, we continuously respawn threads that finish early so that longer running threads still experience contention for the SPL due to the shorter running threads. We stop the simulation when the longest running benchmark completes and report the execution time for each benchmark averaged over all completed runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS</head><p>We first motivate the need for dynamic thread assignment and spatial partitioning by showing the varied performance achieved with static thread assignment relative to large private SPLs. We then compare dynamic management to the best, worst, and median-case static assignments. We also compare our approach with the performance and energy?delay 2 that would be ideally achieved by replacing the SPL with additional cores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Static Assignment Performance</head><p>The OS scheduler could statically assign threads to clusters (i.e., maintain the schedule throughout execution) using information regarding expected SPL usage gleaned from the compiler. For each workload, we simulate all 35 possible static assignments, and extract the best, worst, and median static assignments based on the mean relative execution time of all benchmarks. This information tells us what an oracle static scheduler could achieve, the worst performance that could occur if SPL usage is not taken into account by the scheduler at all, and the margin for error, i.e., whether most schedules are closer to the best or the worst schedule.</p><p>Figure <ref type="figure" target="#fig_5">5</ref> shows the performance of each benchmark for one of the workloads for all static assignments (results for the other three workloads show similar overall trends). The labels on the x-axis indicates the cluster, 1 or 2, to which each thread is assigned. A label of 12112212, for example, indicates the first spawned thread is assigned to cluster 1, the second thread to cluster 2, the third thread to cluster 1, etc. The performance is highly variable, varying by as much as 1028% between the best and worst static schedules for some benchmarks. The mean performance for the best, worst, and median static schedules for each workload relative to the 12-row private SPL baseline is shown in Figure <ref type="figure" target="#fig_6">6</ref> (second, third, and fourth bars). Individual benchmark per-</p><formula xml:id="formula_1">0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 Mix A Mix B Mix C Mix D</formula><p>Relative Exec Time  formance for each workload is shown in Figure <ref type="figure">7</ref> <ref type="foot" target="#foot_2">3</ref> . While in some cases the best (oracle) static scheduler performs reasonably well, the results for the worst schedule indicate that a static scheduler that is oblivious to SPL usage may perform poorly relative to the private 12-row SPL organization. Moreover, Figure <ref type="figure" target="#fig_6">6</ref> shows that the median schedule is much closer to the worst case schedule than the best case schedule. Thus, there is little margin for error in static scheduling; such errors could easily arise due to the lack of static information regarding the fabric contention among applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance of Dynamic SPL Cluster Management</head><p>The individual benchmark and average workload performance of the four representative dynamic management policies presented in Section 3 relative to the performance of private 12-row SPL is shown in Figures <ref type="figure">7</ref> and<ref type="figure" target="#fig_6">6</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Average Row and Composite Policies</head><p>Overall, the Average Row and Composite policies outperform the best possible static assignment by 21.9% and 23.6%, respectively. The benefits of permitting the manager to control spatial partitioning as done in the Composite policy are demonstrated by comparing the overall results for both policies (Figure <ref type="figure" target="#fig_6">6</ref>). Compared with the much higher overhead private 12-row SPL organization, the Average Row approach experiences a 7.0% slowdown and the Composite policy experiences a 5.3% slowdown.</p><p>For a few of the benchmarks, the dynamic algorithms occasionally improve performance relative to the 12-row private SPL. This occurs because we simulate private L2 caches. When scheduled on several different cores, threads may make use of multiple L2 caches. Thus, on an L2 cache miss, the data might be sourced from another L2 cache rather than the slower main memory. To ensure that this effect is not the primary reason for the improvement of our policies, we ran tests where all L2 misses are forced to access main memory. We found that the cache "sharing" effect on performance was negligible in comparison to the effect of the Cluster Manager.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">H3C Policy</head><p>As mentioned previously, and shown in the results, the Hill Climbing manager typically does not outperform the simpler Composite approach due to the performance loss in-0.8 0.9  curred during exploration. The H3C manager achieves the best all around performance, outperforming all other options in all but one case. In the one exception, Mix C under the Composite manager, the performance with H3C is less than 1% worse than the Composite scheduler. Overall the H3C policy achieves 25.3% better performance than the best static schedule. Compared to the 12-row private SPL, the H3C management approach experiences only a 3.6% slowdown while consuming 4X less area. When energy?delay 2 (ED 2 ) is considered (Figure <ref type="figure" target="#fig_7">8</ref>), the benefits of multithreaded SPLs incorporating both scheduling and spatial partitioning are further accentuated. The H3C manager achieves 5.4% better ED 2 than the 12-row private baseline on average (again with a 4X lower area cost due to the shared fabrics). By contrast, the best static schedule experiences an average 179% worse ED 2 than the 12-row private SPL. H3C is the only approach that provides better ED 2 than the 12-row baseline for all four workloads.</p><p>Another benefit of the dynamic policies is fairness. For most of the best static schedules, some of the threads achieve near optimal performance while others experience significant slowdown. With the dynamic policies, the performance impact is quite uniform across the threads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H3C Component Analysis</head><p>In Section 3.3.2 we detailed a number of factors that limit the performance of the Composite and Hill Climbing managers and how the H3C manager incorporates techniques to address these issues. To assess the importance of each, and also confirm that the proposed solutions achieve their stated goals, we run simulations where one or more of H3C features are modified. In particular we look at cases where the stability threshold is eliminated (No Stability), where hill climbing is eliminated (No Hill), and where additional hill climbing is performed (Extra Hill). We also look at a case where Composite scheduling is performed at every interval (essentially adding phase information to the base Composite manager) (Composite+Phase).</p><p>Figure <ref type="figure">9</ref> shows the performance loss of each case relative to the H3C manager. The H3C manager outperforms all of these alternatives in every instance. This confirms that hill climbing, stability detection, and phase analysis are all important and that eliminating any one of them degrades the quality of the manager. The most important factor is the stability threshold, without which performance degrades by </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phase Analysis</head><p>One of the key features of our management schemes is their ability to dynamically adapt to different application phases. Figure <ref type="figure" target="#fig_10">10</ref> shows an example of the thread scheduling and cluster partitioning for a section of Mix D with H3C management. The graph shows the thread-to-core assignment for the four main threads along with the SPL access patterns of the two threads that change phases during the given period. The horizontal dotted lines in the graph show which cores share a SPL partition and the vertical line indicates when one of the clusters is partitioned.</p><p>At the start of the example, four threads are actively using the SPL. The two crypt threads share one cluster and the two single threaded applications share the other in order to minimize conflicts. Around 134M cycles, MPGenc starts a section that uses the SPL. The H3C manager monitors SPL usage and determines how to schedule threads and partition the fabric to adapt to this change. In particular, one of the clusters is divided so that crypt still has its own partition, and the assignment of threads to clusters is rearranged based on current usage statistics.</p><p>The figure also shows how the manager can adapt to the changing access pattern of cjpeg. During phases when its access rate increases, cjpeg is rescheduled on the larger cluster to achieve better performance. Unlike the Composite manager, however, which always makes the same change, we can see in the last two access peaks for cjpeg that the H3C manager explores other options in an attempt to find an even better mapping.  <ref type="figure" target="#fig_1">1(a)</ref>. These results were obtained by simulating a given workload using the original benchmarks (no SPL code) with eight cores (one per thread), and then ideally scaling the results to 10 cores. This ideal scaling is achieved by linearly reducing the execution time by 1/5, but optimistically increasing the energy by only 12.5% (even though there are now 25% more cores).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Replacing the SPL with Additional Cores</head><p>A comparison of the Ideal 10 Core and all of the dynamic scheduling results in these graphs substantiates previous work that demonstrated significant benefits with SPL on  particular applications. Performance degrades by 62-143% when the workloads are run on the 10-core cluster rather than the two with multithreaded SPL, and the ED 2 differences are even more pronounced: up to 34X worse ED 2 for Ideal 10 Core. We emphasize again that on a largescale CMP, those applications that are not compiled to use the SPL can be scheduled on a non-SPL cluster. For those threads that are compiled to use the SPL, effective assignment of threads to multithreaded SPL clusters, coupled with judicious dynamic spatial partitioning of the SPLs, is crucial to achieving good performance, power efficiency, and area efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Reconfigurable Processors</head><p>Several excellent survey papers <ref type="bibr" target="#b24">[23,</ref><ref type="bibr" target="#b42">41]</ref> provide an overview of the contributions of prior reconfigurable computing projects. The bulk of these efforts focus on a single processor core with an attached reconfigurable fabric. There is a dearth of prior work in addressing how reconfigurable logic can best benefit future CMPs. In this section, we focus on those ideas most related to our proposed thread assignment and spatial partitioning policies.</p><p>Caspi et al. <ref type="bibr" target="#b6">[6]</ref> propose SCORE, a reconfigurable system which uses a stream programming model. Their design incorporates a single CPU and multiple reconfigurable blocks. Configurations can map to these blocks both spatially and temporally. A number of research efforts <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b14">13,</ref><ref type="bibr" target="#b21">20]</ref> have investigated the high level integration of a reconfigurable fabric on-chip. All of these, however, only investigate the integration with a single core, although Garcia and Compton <ref type="bibr" target="#b21">[20]</ref> state that their technique could be extended to a multicore system.</p><p>In <ref type="bibr" target="#b22">[21]</ref>, configuration data for a reconfigurable coprocessor is shared among multiple cores in order to increase fabric utilization by allowing a larger number of configurations to coexist in the fabric. Chen et al. <ref type="bibr">[8]</ref> investigate the benefits of including reconfigurable ISA support in a multicore processor and find that combining program parallelization with custom ISA support provides larger speedups than the product of the two techniques applied in isolation.</p><p>Our previous work <ref type="bibr" target="#b44">[43]</ref> identifies a number of characteristics of past reconfigurable proposals that are found to be highly amenable to incorporating reconfigurable fabrics in CMPs. We designed a shared SPL based on these fea-tures, and analyzed the impact of incorporating the fabric with processors of different complexity. While the emphasis of <ref type="bibr" target="#b44">[43]</ref> is on the fabric design, this paper proposes a complete hardware/software approach to managing multithreaded SPL clusters in future CMPs, including spatial partitioning and thread scheduling policies.</p><p>Reconfigurable computing has recently been gaining increased attention from industry. Both Intel and AMD permit tighter integration of FGPAs with general purpose processors through HyperTransport, QuickPath, and licensing of front side bus technology <ref type="bibr" target="#b19">[18,</ref><ref type="bibr" target="#b20">19]</ref>. Convey Computer's HC-1 pairs an Intel processor with a reconfigurable coprocessor and allows different instruction sets to be loaded into the coprocessor <ref type="bibr" target="#b13">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Thread Scheduling and Dynamic Resource Sharing</head><p>The benefits of dynamic thread scheduling in small scale CMP/SMT systems has been explored for a number of purposes, including cache-aware scheduling <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b41">40]</ref>, thermal management <ref type="bibr" target="#b12">[11]</ref>, and SMT resource-aware scheduling <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b36">35]</ref>. Most of these efforts deal with temporally scheduling threads between time slices where the number of threads is greater than the number of processor contexts. They aim to minimize contention or maximize sharing between threads scheduled in the same interval. Our work is different in a number of aspects, including that we perform spatial partitioning with all threads running at the same time.</p><p>Numerous SMT resource management techniques exist that aim to either directly <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b10">10]</ref> or indirectly <ref type="bibr" target="#b16">[15,</ref><ref type="bibr" target="#b43">42]</ref> control the amount of processor resources that any thread consumes. These techniques aim to maximize the benefit each thread realizes from its share of resources, particularly by limiting threads with outstanding misses from hogging resources. With SPL resource management, on the other hand, threads that are "hogging" the SPL are actually the ones receiving the most benefit from the fabric, and so SPL management is not as simple as just limiting threads that use a lot of the SPL. Also, unlike front-end resources, which can be reallocated quickly at a fine granularity, there is nontrivial overhead involved in both supporting and dynamically switching between different SPL sharing degrees, which impacts the techniques that can be employed.</p><p>Previous research has proposed sharing other architectural components among multiple cores. Several efforts investigate how to best allocate shared L2 cache space among multiple threads <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b26">25,</ref><ref type="bibr" target="#b39">38</ref>]. Sun's UltraSPARC T1 <ref type="bibr" target="#b40">[39]</ref> shares a single floating point unit between its eight SMT cores. Kumar et al. <ref type="bibr" target="#b28">[27]</ref> investigate sharing FP units, crossbar ports, and L1 instruction and data caches between two cores. Their work focuses on temporal sharing, and does not consider dynamic spatial techniques such as splitting a cache in half if inter-thread conflicts are too high. We also propose more advance policies that combine machine learning, phased-based analysis, and stability control. Prior work in optimizing resource allocation during different phases <ref type="bibr" target="#b15">[14,</ref><ref type="bibr" target="#b34">33]</ref> only address single applications. In our multithreaded environment, each thread has its own current phase and we must deal with optimizing thread assignment and resource allocation as phases change across multiple applications.</p><p>Our situation is more difficult than any of this previous work as we must concurrently manage both cluster thread assignment and fabric partitioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>We propose dynamically managed multithreaded reconfigurable fabrics for future CMPs. We examine a range of dynamic management policies that vary in their approach mapping threads to clusters, as well as how they exploit the ability to spatially partition each SPL to mitigate interthread conflicts.</p><p>Of the four representative approaches that we present, our best policy judiciously combines elements of machine learning, phase-based analysis, and stability detection to assign threads to SPL clusters and spatially partition the SPLs on-the-fly. This approach outperforms an oracle static scheduler, and experiences only a small slowdown compared with much larger private SPLs dedicated to each core. We also demonstrate dramatic improvements over allocating the SPL area to additional cores. Overall, we demonstrate that sharing reconfigurable fabrics and managing their resources on-the-fly are key to making reconfigurable fabrics an attractive, cost-effective, option for future CMPs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of SPL integration in a CMP. (a) Depiction of overall chip, with two SPL clusters and one conventional cluster, and blow-up of one SPL cluster, (b) four-way multithreaded SPL, (c) design of SPL cell (unless otherwise indicated all data paths in SPL are 8 bits wide), and (d) SPL Cluster Manager.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 (</head><label>1</label><figDesc>Figure1(b)  shows how the SPL design permits temporal sharing among the threads executing on the four processor cores, as well as spatial partitioning to permit private or semi-private operation<ref type="bibr" target="#b44">[43]</ref>. Spatial partitioning is enabled by inserting additional multiplexers at each point where the SPL pool might be partitioned. To keep the hardware overhead reasonable, each of the two SPLs can only be divided into two halves, four quarters, or one half and two quarters, requiring a total of only four sets of input multiplexers.With temporal sharing, all rows of the temporally shared fabric (which may be the whole SPL or a subset, depending on whether spatial partitioning is being used) are available to the sharing threads in a time multiplexed fashion. The control for temporal sharing is implemented in hardware on a fine-grain, cycle-level basis to avoid thread starvation for the shared SPL resource. Each SPL cycle, a round-robin scheduler selects an instruction from one of the queues to issue to the shared SPL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Hash function for phase IDs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: H3C Cluster Manager.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance of Mix A for multithreaded SPL clusters with all possible static schedules relative to private 12-row SPL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Average execution time for each workload relative to 12-row private SPL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Average energy?delay 2 for each workload relative to 12-row private SPL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figures 6 and 8</head><label>8</label><figDesc>Figures 6 and 8 also show results for each workload in which each SPL is replaced by one additional single issue core; in other words, the workloads are run on the conventional cluster on the right side of the chip diagram of Figure1(a). These results were obtained by simulating a given workload using the original benchmarks (no SPL code) with eight cores (one per thread), and then ideally scaling the results to 10 cores. This ideal scaling is achieved by linearly reducing the execution time by 1/5, but optimistically increasing the energy by only 12.5% (even though there are now 25% more cores).A comparison of the Ideal 10 Core and all of the dynamic scheduling results in these graphs substantiates previous work that demonstrated significant benefits with SPL on</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Thread-to-core assignment and SPL accesses for Mix D with H3C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Relative area and power of eight singleissue out-of-order cores, eight private SPLs, and two four-way shared SPLs.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Peak</cell><cell>Total</cell></row><row><cell></cell><cell></cell><cell>Area</cell><cell>Dynamic Power</cell><cell>Leakage Power</cell></row><row><cell>Eight Cores</cell><cell>N/A</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell>Eight Private SPL</cell><cell>12</cell><cell>0.97</cell><cell>0.29</cell><cell>1.32</cell></row><row><cell>Two 4-way Shared SPL</cell><cell>12</cell><cell>0.29</cell><cell>0.07</cell><cell>0.34</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Management policy considerations.</figDesc><table><row><cell></cell><cell>Source code</cell><cell>(b) Calculation of mc in SPL</cell></row><row><cell></cell><cell cols="2">Figure 2: Mapping of SPEC2006 456.hmmer P7Viterbi to SPL.</cell></row><row><cell>Consideration</cell><cell>Alternatives</cell></row><row><cell>Metrics</cell><cell>SPL Accesses, SPL Wait Time, Avg. Rows, Grad. Insts.</cell></row><row><cell>Spatial Partitioning</cell><cell>Yes, No</cell></row><row><cell>Granularity</cell><cell>Various fixed intervals, phase change</cell></row><row><cell>Algorithm</cell><cell>Split Assignment, Equalize Assignment, Hill Climbing, Hybrid</cell></row><row><cell></cell><cell>None, after n non-useful changes,</cell></row><row><cell>Stability</cell><cell>when average degradation &lt; threshold,</cell></row><row><cell></cell><cell>after n random intervals</cell></row><row><cell>Randomness</cell><cell>Swap SPL threads, swap any threads, swap with thread or empty core</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Architecture parameters.</figDesc><table><row><cell></cell><cell></cell><cell>Policy</cell><cell>Parameter</cell><cell>Value</cell></row><row><cell></cell><cell></cell><cell>Composite</cell><cell>Split threshold</cell><cell>16?avg. rows used by core</cell></row><row><cell>Branch Predictor BTB Size RAS Entries Fetch/Rename Width Issue/Retire Width Integer Registers FP Registers Retire Width Integer Queue Entries FP Queue Entries</cell><cell>gshare + bimodal 512B 32 2 1 64 64 1 32 16</cell><cell cols="2">Composite Hill Climbing Probability of splitting SPL Merge threshold Hill Climbing Probability of merging SPL Hill Climbing Probability of random mapping Hill Climbing Threads to consider swapping H3C Intervals of Composite Scheduling H3C Intervals of Hill Climbing H3C Stability threshold All Interval granularity</cell><cell>2?avg. rows used by core 20% 20% 10% All 5 5 Avg. Deg. &lt; 4% 100k cycles</cell></row><row><cell>ROB Entries</cell><cell>64</cell><cell></cell><cell></cell></row><row><cell>Int ALUs</cell><cell>1</cell><cell></cell><cell></cell></row><row><cell>Branch Units</cell><cell>1</cell><cell></cell><cell></cell></row><row><cell>Int Mult/Div Units</cell><cell>1</cell><cell></cell><cell></cell></row><row><cell>FP ALU Units</cell><cell>1</cell><cell></cell><cell></cell></row><row><cell>LD/ST Units</cell><cell>1</cell><cell></cell><cell></cell></row><row><cell>L1 Inst Cache</cell><cell>8kB 2-way</cell><cell></cell><cell></cell></row><row><cell>L1 Data Cache</cell><cell>8kB 2-way</cell><cell></cell><cell></cell></row><row><cell>L1 Access Latency</cell><cell>2 cycles</cell><cell></cell><cell></cell></row><row><cell>L2 Cache</cell><cell>1MB per core</cell><cell></cell><cell></cell></row><row><cell>L2 Access Latency</cell><cell>10 cycles</cell><cell></cell><cell></cell></row><row><cell>Coherence Protocol</cell><cell>MESI</cell><cell></cell><cell></cell></row><row><cell>Main Memory Access Time</cell><cell>100 ns</cell><cell></cell><cell></cell></row><row><cell>Phase History Entries</cell><cell>256</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Parameters for dynamic management policies.</figDesc><table><row><cell></cell><cell>SPL</cell><cell>Max</cell><cell>% Optimized</cell><cell>% Dyn.</cell><cell>SPL</cell></row><row><cell></cell><cell>Functions</cell><cell>Rows</cell><cell>Exec Time</cell><cell>SPL Insts</cell><cell>Usage</cell></row><row><cell>300.twolf</cell><cell>1</cell><cell>21</cell><cell>32.7%</cell><cell>0.10%</cell><cell>3.8%</cell></row><row><cell>456.hmmer</cell><cell>1</cell><cell>10</cell><cell>85.0%</cell><cell>1.15%</cell><cell>40.2%</cell></row><row><cell>462.libquantum</cell><cell>1</cell><cell>11</cell><cell>40.1%</cell><cell>2.19%</cell><cell>13.5%</cell></row><row><cell>473.astar</cell><cell>1</cell><cell>2</cell><cell>33.7%</cell><cell>0.79%</cell><cell>2.7%</cell></row><row><cell>cjpeg</cell><cell>5</cell><cell>21</cell><cell>49.9%</cell><cell>1.22%</cell><cell>20.6%</cell></row><row><cell>MPGenc</cell><cell>4</cell><cell>16</cell><cell>69.1%</cell><cell>0.72%</cell><cell>17.2%</cell></row><row><cell>MPGdec-o0</cell><cell>5</cell><cell>20</cell><cell>44.8%</cell><cell>0.35%</cell><cell>15.3%</cell></row><row><cell>MPGdec-o3</cell><cell>12</cell><cell>20</cell><cell>47.8%</cell><cell>0.57%</cell><cell>19.3%</cell></row><row><cell>crypt</cell><cell>1</cell><cell>298</cell><cell>97.9%</cell><cell>4.48%</cell><cell>99.9%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Benchmark description, number of SPL functions, maximum number of rows used by SPL functions, percentage of execution time of SPL optimized regions, percentage of SPL instructions executed relative to total committed instructions, and percentage of time with at least one SPL instruction in flight. The global SPL Cluster Management history table contains 256 entries. Each entry contains the phase IDs for each thread, the mapping of threads to clusters, and the size of each cluster. We estimate this table would require 4 kB worth of storage.</figDesc><table /><note><p>coincide with management interval boundaries as management intervals are based on cycles whereas phase tracking is based on instructions.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Workload composition. For parallel workloads the number after the name indicates the number of threads spawned during the run.</figDesc><table><row><cell>Name</cell><cell>Benchmarks</cell></row><row><cell>Mix A</cell><cell>MPGenc-2, MPGdec-o0-2, crypt-2, 456.hmmer, 473.astar</cell></row><row><cell>Mix B</cell><cell>MPGdec-o3-2, crypt-2, 456.hmmer, 300.twolf, 473.astar, 462.libquantum</cell></row><row><cell>Mix C</cell><cell>MPGdec-o3-4, crypt-2, 300.twolf, 462.libquantum</cell></row><row><cell>Mix D</cell><cell>MPGenc-4, crypt-2, cjpeg, 462.libquantum</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Figure 7: Performance of multithreaded SPL clusters with dynamic scheduling algorithms and best, worst, and median static schedules, relative to private 12-row SPL.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>1.5</cell><cell cols="2">3.0 3.2 2.8 2.8 2.8</cell><cell cols="2">11.0 8.3</cell><cell>2.1 2.1</cell><cell></cell><cell>1.5</cell><cell>1.6 8.4 8.2</cell><cell>10.3 8.4</cell><cell>2.1 2.1 3.3 3.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.4</cell></row><row><cell></cell><cell></cell><cell>Relative Exec Time</cell><cell>1 1.1 1.2 1.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Best Static Worst Static Median Static AvgRow Composite Hill Climbing H3C</cell><cell>Relative Exec Time</cell><cell>1 1.1 1.2 1.3</cell><cell>Best Static Worst Static Median Static AvgRow Composite Hill Climbing H3C</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>MPGenc2</cell><cell>MPGdec2o0</cell><cell>crypt2</cell><cell>hmmer</cell><cell>astar</cell><cell></cell><cell></cell><cell>MPGdec2o3</cell><cell>crypt2</cell><cell>hmmer</cell><cell>twolf</cell><cell>astar</cell><cell>libquantum</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(a) Mix A</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b) Mix B</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1.5</cell><cell>10.4 9.1</cell><cell></cell><cell></cell><cell cols="2">3.6 3.5</cell><cell></cell><cell>1.5</cell><cell>3.2 3.3 3.2</cell><cell>3.5 3.5</cell><cell>3.7 3.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.4</cell></row><row><cell></cell><cell></cell><cell>Relative Exec Time</cell><cell>1 1.1 1.2 1.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Best Static Worst Static Median Static AvgRow Composite Hill Climbing H3C</cell><cell>Relative Exec Time</cell><cell>1 1.1 1.2 1.3</cell><cell>Best Static Worst Static Median Static AvgRow Composite Hill Climbing H3C</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>MPGdec4o3</cell><cell>crypt2</cell><cell>twolf</cell><cell></cell><cell>libquantum</cell><cell></cell><cell></cell><cell>MPGenc4</cell><cell>crypt2</cell><cell>cjpeg</cell><cell>libquantum</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(c) Mix C</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(d) Mix D</cell></row><row><cell></cell><cell>1.5</cell><cell cols="6">27 3.3 124 61 18 143 102 26 179 125 33 19 18</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2 Relative Energy?Delay</cell><cell>1 1.1 1.2 1.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ideal 10 Core Best Static Worst Static Median Static AvgRow Composite Hill Climbing H3C</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Mix A</cell><cell>Mix B</cell><cell>Mix C</cell><cell cols="2">Mix D</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>The major loop in crypt requires nearly</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="300" xml:id="foot_1"><p>rows and so achieves less than optimal speedup for any reasonably sized fabric.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Note that the best schedule is based on mean performance, and as such might not be best for an individual benchmark.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This research was supported by an <rs type="funder">NSF</rs> <rs type="grantName">Graduate Research Fellowship</rs>; <rs type="funder">NSF</rs> grants <rs type="grantNumber">CCF-0916821</rs>, <rs type="grantNumber">CCF-0811729</rs>, and <rs type="grantNumber">CNS-0708788</rs>; and equipment grants from <rs type="funder">Intel</rs>.</p></div>
			</div>
			<div type="funding">
<div><p>Cluster 2 Cluster 2 Cluster 1 Cluster 1 8-bit Cell 8-bit Cell 8-bit Cell 8-bit Cell 8-bit Cell 8-bit Cell 8-bit Cell 8-bit Cell 8-bit Cell 8-bit Cell 32?32 Full Crossbar 32?32 Full Crossbar</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_hf6vCN6">
					<orgName type="grant-name">Graduate Research Fellowship</orgName>
				</org>
				<org type="funding" xml:id="_j8pcFjy">
					<idno type="grant-number">CCF-0916821</idno>
				</org>
				<org type="funding" xml:id="_dpT4R3g">
					<idno type="grant-number">CCF-0811729</idno>
				</org>
				<org type="funding" xml:id="_C284MyN">
					<idno type="grant-number">CNS-0708788</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="http://www.amdcompare.com/us-en/desktop/details.aspx?opn=ADH2350IAA5DD" />
		<title level="m">Advanced Micro Devices. AMD Athlon X2 Dual-Core Details</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast Compilation for Pipelined Reconfigurable Fabrics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Budiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1999 ACM/SIGDA 7th Int&apos;l Symposium on Field Programmable Gate Arrays</title>
		<meeting>1999 ACM/SIGDA 7th Int&apos;l Symposium on Field Programmable Gate Arrays</meeting>
		<imprint>
			<date type="published" when="1999-02">Feb. 1999</date>
			<biblScope unit="page" from="195" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Managing Pipeline-Reconfigurable FPGAs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cadambi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schmit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1998 ACM/SIGDA 6th Int&apos;l Symposium on Field Programmable Gate Arrays</title>
		<meeting>1998 ACM/SIGDA 6th Int&apos;l Symposium on Field Programmable Gate Arrays</meeting>
		<imprint>
			<date type="published" when="1998-02">Feb. 1998</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Garp Architecture and C Compiler</title>
		<author>
			<persName><forename type="first">T</forename><surname>Callahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wawrzynek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="62" to="69" />
			<date type="published" when="2000-04">Apr. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Effect of Reconfigurable Units in Superscalar Processors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carrillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2001 ACM/SIGDA 9th Int&apos;l Symposium on Field Programmable Gate Arrays</title>
		<meeting>2001 ACM/SIGDA 9th Int&apos;l Symposium on Field Programmable Gate Arrays</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stream Computations Organized for Reconfigurable Execution (SCORE)</title>
		<author>
			<persName><forename type="first">E</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wawrzynek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dehon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Int&apos;l Workshop on Field-Programmable Logic and Applications</title>
		<meeting>the 10th Int&apos;l Workshop on Field-Programmable Logic and Applications</meeting>
		<imprint>
			<date type="published" when="2000-08">Aug. 2000</date>
			<biblScope unit="page" from="605" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dynamically Controlled Resource Allocation in SMT Processors</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="171" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Combining Multicore and Reconfigurable Instruction Set Extensions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Pittman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Forin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th ACM/SIGDA Int&apos;l Symposium on Field Programmable Gate Arrays</title>
		<meeting>18th ACM/SIGDA Int&apos;l Symposium on Field Programmable Gate Arrays</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="33" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Managing Distributed, Shared L2 Caches through OS-Level Page Allocation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2006-12">Dec. 2006</date>
			<biblScope unit="page" from="455" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning-Based SMT Processor Resource Distribution via Hill-Climbing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 33rd</title>
		<meeting>33rd</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Int&apos;l Symposium on Computer Architecture</title>
		<imprint>
			<biblScope unit="page" from="239" to="251" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Performance Implications of Single Thread Migration on a Chip Multi-Core</title>
		<author>
			<persName><forename type="first">T</forename><surname>Constantinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sazeides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Michaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fetis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="80" to="91" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Convey HC-1 Computer</title>
	</analytic>
	<monogr>
		<title level="m">Convey Computer</title>
		<imprint>
			<publisher>White Paper</publisher>
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Managing a Reconfigurable Processor in a General Purpose Workstation Environment</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Design, Automation, and Test in Europe Converence and Exhibition</title>
		<meeting>of the Design, Automation, and Test in Europe Converence and Exhibition</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="980" to="985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Managing Multi-Configuration Hardware via Dynamic Working Set Analysis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Dhodapkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="233" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Front-End Policies for Improved Issue Efficiency in SMT Processors</title>
		<author>
			<persName><forename type="first">A</forename><surname>El-Moursy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Albonesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th IEEE Symposium on High Performance Computer Architecture</title>
		<meeting>9th IEEE Symposium on High Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Compatible Phase Co-Scheduling on a CMP of Multi-threaded Processors</title>
		<author>
			<persName><forename type="first">A</forename><surname>El-Moursy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Albonesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dwarkadas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 20th Int&apos;l Parallel and Distributed Processing Symposium</title>
		<meeting>of the 20th Int&apos;l Parallel and Distributed essing Symposium</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Performance of multithreaded chip multiprocessors and implications for operating system design</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fedorova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Small</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nussbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX 2005 Annual Technical Conference</title>
		<meeting><address><addrLine>Berkeley, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="395" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">FPGA Acceleration Gets a Boost from HP, Intel. HPCWire</title>
		<author>
			<persName><forename type="first">M</forename><surname>Feldman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-09">Sept. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Reconfigurable Computing Prospects on the Rise. HPCWire</title>
		<author>
			<persName><forename type="first">M</forename><surname>Feldman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-12">Dec. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Reconfigurable Hardware Interface for a Modern Computing System</title>
		<author>
			<persName><forename type="first">P</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Compton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2007 IEEE Symposium on Field-Programmable Custom Computing Machines</title>
		<meeting>2007 IEEE Symposium on Field-Programmable Custom Computing Machines</meeting>
		<imprint>
			<date type="published" when="2007-04">April 2007</date>
			<biblScope unit="page" from="73" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kernel Sharing on Reconfigurable Multiprocessor Systems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Compton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Conference on Field Programmable Technology</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="225" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">PipeRench: A Coprocessor for Streaming Multimedia Acceleration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schmit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Budiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cadambi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th</title>
		<meeting>26th</meeting>
		<imprint>
			<date type="published" when="1999-05">May 1999</date>
			<biblScope unit="page" from="28" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Decade of Reconfigurable Computing: A Visionary Retrospective</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hartenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Design, Automation, and Test in Europe</title>
		<meeting>of the Conference on Design, Automation, and Test in Europe</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="642" to="649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Intel Core 2 Extreme Processor X6800 and Intel Core 2 Duo Desktop Processor E6000 and E4000 Sequences</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="313278" to="313282" />
		</imprint>
	</monogr>
	<note>Intel Datasheet</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fair Cache Sharing and Partitioning in a Chip Multiprocessor Architecture</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th IEEE/ACM Int&apos;l Conference on Parallel Architectures and Compilation Techniques</title>
		<meeting>13th IEEE/ACM Int&apos;l Conference on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="111" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Opportunities for Cache Friendly Process Scheduling</title>
		<author>
			<persName><forename type="first">P</forename><surname>Koka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Lipasti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Interaction Between Operating Systems and Computer Architecture</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Conjoined-core Chip Multiprocessing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tullsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="195" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Measuring the Gap Between FPGAs and ASICs</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kuon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="215" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">MediaBench: A Tool for Evaluation and Synthesizing Multimedia and Communications Systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Potkonjak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Mangione-Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="330" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The ALPBench Benchmark Suite for Complex Multimedia Applications</title>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sasanka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Adve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Deves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int&apos;l Symposium on Workload Characterization</title>
		<meeting>of the IEEE Int&apos;l Symposium on Workload Characterization</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="34" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Picking Statistically Valid and Early Simulation Points</title>
		<author>
			<persName><forename type="first">E</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th IEEE/ACM Int&apos;l Conference on Parallel Architectures and Compilation Techniques</title>
		<meeting>12th IEEE/ACM Int&apos;l Conference on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<date type="published" when="2003-09">Sept 2003</date>
			<biblScope unit="page" from="244" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<ptr target="http://sourceforge.net/projects/sesc" />
		<title level="m">SESC Architectural Simulator</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Phase Tracking and Prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 30th</title>
		<meeting>30th</meeting>
		<imprint>
			<date type="published" when="2003-06">June 2003</date>
			<biblScope unit="page" from="336" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Parallel Java Grande Benchmark Suite</title>
		<author>
			<persName><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Obdrz?lek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supercomputing &apos;01: Proc. of the 2001 ACM/IEEE conference on Supercomputing (CDROM)</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="8" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Symbiotic Jobscheduling for a Simultaneous Multithreaded Processor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th ACM Symposium on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>9th ACM Symposium on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="234" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Standard Performance Evaluation Corporation. SPEC CPU Benchmark Suite</title>
		<ptr target="http://www.specbench.org/cpu" />
		<imprint>
			<date type="published" when="2000">2000/, 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Standard Performance Evaluation Corporation. SPEC CPU Benchmark Suite</title>
		<ptr target="http://www.specbench.org/cpu" />
		<imprint>
			<date type="published" when="2006">2006/, 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dynamic Partitioning of Shared Cache Memory</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Devadas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Supercomputing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="26" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Inc. UltraSPARC T1 Supplement to the UltraSPARC Architecture</title>
		<imprint>
			<date type="published" when="2005-03">2005. Mar. 2006</date>
		</imprint>
		<respStmt>
			<orgName>Sun Microsystems</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Thread Clustering: Sharing-Aware Scheduling on SMP-CMP-SMT multiprocessors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stumm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Conference on EuroSys</title>
		<meeting>the 2007 Conference on EuroSys</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="47" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Todman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Constantinides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mencer</surname></persName>
		</author>
		<title level="m">Reconfigurable Computing: Architectures and Design Methods. IEE Proc. -Computers and Digital Techniques</title>
		<imprint>
			<date type="published" when="2005-03">March 2005</date>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="193" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Exploiting Choice: Instruction Fetch and Issue on an Implementable Simultaneous Multithreading Processor</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Eggers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Stamm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="191" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Shared Reconfigurable Architectures for CMPs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cianchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Albonesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 18th Int&apos;l Conference on Field-Programmable Logic and Applications</title>
		<meeting>of the 18th Int&apos;l Conference on Field-Programmable Logic and Applications</meeting>
		<imprint>
			<date type="published" when="2008-09">Sept. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A Dynamic Instruction Set Computer</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wirthlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hutchins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1995 IEEE Symposium on Field-Programmable Custom Computing Machines</title>
		<meeting>1995 IEEE Symposium on Field-Programmable Custom Computing Machines</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="99" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A C Compiler for a Processor with a Reconfigurable Functional Unit</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">A</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2000 ACM/SIGDA 8th Int&apos;l Symposium on Field Programmable Gate Arrays</title>
		<meeting>2000 ACM/SIGDA 8th Int&apos;l Symposium on Field Programmable Gate Arrays</meeting>
		<imprint>
			<date type="published" when="2000-02">Feb. 2000</date>
			<biblScope unit="page" from="95" to="100" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
