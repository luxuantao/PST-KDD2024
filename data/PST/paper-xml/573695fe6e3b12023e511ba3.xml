<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stacked Attention Networks for Image Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
							<email>zichaoy@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<postCode>98052</postCode>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<email>jfgao@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<postCode>98052</postCode>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
							<email>deng@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<postCode>98052</postCode>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Stacked Attention Networks for Image Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents stacked attention networks (SANs)   that learn to answer natural language questions from images. SANs use semantic representation of a question as query to search for the regions in an image that are related to the answer. We argue that image question answering (QA) often requires multiple steps of reasoning. Thus, we develop a multiple-layer SAN in which we query an image multiple times to infer the answer progressively. Experiments conducted on four image QA data sets demonstrate that the proposed SANs significantly outperform previous state-of-the-art approaches. The visualization of the attention layers illustrates the progress that the SAN locates the relevant visual clues that lead to the answer of the question layer-by-layer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the recent advancement in computer vision and in natural language processing (NLP), image question answering (QA) becomes one of the most active research areas <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b18">19]</ref>. Unlike pure language based QA systems that have been studied extensively in the NLP community <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b31">32]</ref>, image QA systems are designed to automatically answer natural language questions according to the content of a reference image.</p><p>Most of the recently proposed image QA models are based on neural networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b18">19]</ref>. A commonly used approach was to extract a global image feature vector using a convolution neural network (CNN) <ref type="bibr" target="#b14">[15]</ref> and encode the corresponding question as a feature vector using a long short-term memory network (LSTM) <ref type="bibr" target="#b8">[9]</ref> and then combine them to infer the answer. Though impressive results have been reported, these models often fail to give precise answers when such answers are related to a set of fine-grained regions in an image.</p><p>By examining the image QA data sets, we find that it is often that case that answering a question from an image requires multi-step reasoning. Take the question and image in Fig. <ref type="figure" target="#fig_1">1</ref> as an example. There are several objects in the image: bicycles, window, street, baskets and  dogs. To answer the question what are sitting in the basket on a bicycle, we need to first locate those objects (e.g. basket, bicycle) and concepts (e.g., sitting in) referred in the question, then gradually rule out irrelevant objects, and finally pinpoint to the region that are most indicative to infer the answer (i.e., dogs in the example).</p><p>In this paper, we propose stacked attention networks (SANs) that allow multi-step reasoning for image QA. SANs can be viewed as an extension of the attention mechanism that has been successfully applied in image captioning <ref type="bibr" target="#b29">[30]</ref> and machine translation <ref type="bibr" target="#b1">[2]</ref>. The overall architecture of SAN is illustrated in Fig. <ref type="figure" target="#fig_1">1a</ref>. The SAN consists of three major components: (1) the image model, which uses a CNN to extract high level image representations, e.g. one vector for each region of the image; (2) the question model, which uses a CNN or a LSTM to extract a semantic vector of the question and (3) the stacked attention model, which locates, via multi-step reasoning, the image regions that are relevant to the question for answer prediction. As illustrated in Fig. <ref type="figure" target="#fig_1">1a</ref>, the SAN first uses the question vector to query the image vectors in the first visual attention layer, then combine the question vector and the retrieved image vectors to form a refined query vector to query the image vectors again in the second attention layer. The higher-level attention layer gives a sharper attention distribution focusing on the regions that are more relevant to the answer. Finally, we combine the image features from the highest attention layer with the last query vector to predict the answer.</p><p>The main contributions of our work are three-fold. First, we propose a stacked attention network for image QA tasks. Second, we perform comprehensive evaluations on four image QA benchmarks, demonstrating that the proposed multiple-layer SAN outperforms previous state-of-the-art approaches by a substantial margin. Third, we perform a detailed analysis where we visualize the outputs of different attention layers of the SAN and demonstrate the process that the SAN takes multiple steps to progressively focus the attention on the relevant visual clues that lead to the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Image QA is closely related to image captioning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20]</ref>. In <ref type="bibr" target="#b26">[27]</ref>, the system first extracted a high level image feature vector from GoogleNet and then fed it into a LSTM to generate captions. The method proposed in <ref type="bibr" target="#b29">[30]</ref> went one step further to use an attention mechanism in the caption generation process. Different from <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b26">27]</ref>, the approach proposed in <ref type="bibr" target="#b5">[6]</ref> first used a CNN to detect words given the images, then used a maximum entropy language model to generate a list of caption candidates, and finally used a deep multimodal similarity model (DMSM) to rerank the candidates. Instead of using a RNN or a LSTM, the DMSM uses a CNN to model the semantics of captions.</p><p>Unlike image captioning, in image QA, the question is given and the task is to learn the relevant visual and text representation to infer the answer. In order to facilitate the research of image QA, several data sets have been constructed in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b0">1]</ref> either through automatic generation based on image caption data or by human labeling of questions and answers given images. Among them, the image QA data set in <ref type="bibr" target="#b20">[21]</ref> is generated based on the COCO caption data set. Given a sentence that describes an image, the authors first used a parser to parse the sentence, then replaced the key word in the sentence using question words and the key word became the answer. <ref type="bibr" target="#b6">[7]</ref> created an image QA data set through human labeling. The initial version was in Chinese and then was translated to English. <ref type="bibr" target="#b0">[1]</ref> also created an image QA data set through human labeling. They collected questions and answers not only for real images, but also for abstract scenes.</p><p>Several image QA models were proposed in the literature. <ref type="bibr" target="#b17">[18]</ref> used semantic parsers and image segmentation methods to predict answers based on images and questions. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b6">7]</ref> both used encoder-decoder framework to generate answers given images and questions. They first used a LSTM to encoder the images and questions and then used another LSTM to decode the answers. They both fed the image feature to every LSTM cell. <ref type="bibr" target="#b20">[21]</ref> proposed several neural network based models, including the encoderdecoder based models that use single direction LSTMs and bi-direction LSTMs, respectively. However, the authors found the concatenation of image features and bag of words features worked the best. <ref type="bibr" target="#b0">[1]</ref> first encoded questions with LSTMs and then combined question vectors with image vectors by element wise multiplication. <ref type="bibr" target="#b16">[17]</ref> used a CNN for question modeling and used convolution operations to combine question vectors and image feature vectors. We compare the SAN with these models in Sec. 4.</p><p>To the best of our knowledge, the attention mechanism, which has been proved very successful in image captioning, has not been explored for image QA. The SAN adapt the attention mechanism to image QA, and can be viewed as a significant extension to previous models <ref type="bibr" target="#b29">[30]</ref> in that multiple attention layers are used to support multi-step reasoning for the image QA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Stacked Attention Networks (SANs)</head><p>The overall architecture of the SAN is shown in Fig. <ref type="figure" target="#fig_1">1a</ref>. We describe the three major components of SAN in this section: the image model, the question model, and the stacked attention model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Image Model</head><p>The image model uses a CNN <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref> to get the representation of images. Specifically, the VGGNet <ref type="bibr" target="#b22">[23]</ref> is used to extract the image feature map f I from a raw image I: </p><formula xml:id="formula_0">f I = CNN vgg (I).<label>(1)</label></formula><p>Unlike previous studies <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b6">7]</ref> that use features from the last inner product layer, we choose the features f I from the last pooling layer, which retains spatial information of the original images. We first rescale the images to be 448 × 448 pixels, and then take the features from the last pooling layer, which therefore have a dimension of 512×14×14, as shown in Fig. <ref type="figure" target="#fig_2">2</ref>. 14 × 14 is the number of regions in the image and 512 is the dimension of the feature vector for each region. Accordingly, each feature vector in f I corresponds to a 32× 32 pixel region of the input images. We denote by f i , i ∈ [0, 195] the feature vector of each image region.</p><p>Then for modeling convenience, we use a single layer perceptron to transform each feature vector to a new vector that has the same dimension as the question vector (described in Sec. 3.2):</p><formula xml:id="formula_1">v I = tanh(W I f I + b I ),<label>(2)</label></formula><p>where v I is a matrix and its i-th column v i is the visual feature vector for the region indexed by i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Question Model</head><p>As <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b5">6]</ref> show that LSTMs and CNNs are powerful to capture the semantic meaning of texts, we explore both models for question representations in this study. The essential structure of a LSTM unit is a memory cell c t which reserves the state of a sequence. At each step, the LSTM unit takes one input vector (word vector in our case) x t and updates the memory cell c t , then output a hidden state h t . The update process uses the gate mechanism. A forget gate f t controls how much information from past state c t−1 is preserved. An input gate i t controls how much the current input x t updates the memory cell. An output gate o t controls how much information of the memory is fed to the output as hidden state. The detailed update process is as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">LSTM based question model</head><formula xml:id="formula_2">i t =σ(W xi x t + W hi h t−1 + b i ),<label>(3)</label></formula><formula xml:id="formula_3">f t =σ(W xf x t + W hf h t−1 + b f ), (4) o t =σ(W xo x t + W ho h t−1 + b o ), (5) c t =f t c t−1 + i t tanh(W xc x t + W hc h t−1 + b c ), (6) h t =o t tanh(c t ),<label>(7)</label></formula><p>where i, f, o, c are input gate, forget gate, output gate and memory cell, respectively. The weight matrix and bias are parameters of the LSTM and are learned on training data.</p><p>Given the question q = [q 1 , ...q T ], where q t is the one hot vector representation of word at position t, we first embed the words to a vector space through an embedding matrix x t = W e q t . Then for every time step, we feed the embedding vector of words in the question to LSTM:</p><formula xml:id="formula_4">x t =W e q t , t ∈ {1, 2, ...T },<label>(8)</label></formula><formula xml:id="formula_5">h t =LSTM(x t ), t ∈ {1, 2, ...T }.<label>(9)</label></formula><p>As shown in Fig. <ref type="figure" target="#fig_3">3</ref>, the question what are sitting in the basket on a bicycle is fed into the LSTM. Then the final hidden layer is taken as the representation vector for the question, i.e., v Q = h T . In this study, we also explore to use a CNN similar to <ref type="bibr" target="#b10">[11]</ref> for question representation. Similar to the LSTMbased question model, we first embed words to vectors x t = W e q t and get the question vector by concatenating the word vectors:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">CNN based question model</head><formula xml:id="formula_6">x 1:T = [x 1 , x 2 , ..., x T ].<label>(10)</label></formula><p>Then we apply convolution operation on the word embedding vectors. We use three convolution filters, which have the size of one (unigram), two (bigram) and three (trigram) respectively. The t-th convolution output using window size c is given by:</p><formula xml:id="formula_7">h c,t = tanh(W c x t:t+c−1 + b c ). (<label>11</label></formula><formula xml:id="formula_8">)</formula><p>The filter is applied only to window t : t + c − 1 of size c. W c is the convolution weight and b c is the bias. The feature map of the filter with convolution size c is given by:</p><formula xml:id="formula_9">h c = [h c,1 , h c,2 , ..., h c,T −c+1 ].<label>(12)</label></formula><p>Then we apply max-pooling over the feature maps of the convolution size c and denote it as</p><formula xml:id="formula_10">hc = max t [h c,1 , h c,2 , ..., h c,T −c+1 ].<label>(13)</label></formula><p>The max-pooling over these vectors is a coordinate-wise max operation. For convolution feature maps of different sizes c = 1, 2, 3, we concatenate them to form the feature representation vector of the whole question sentence:</p><formula xml:id="formula_11">h = [ h1 , h2 , h3 ],<label>(14)</label></formula><p>hence v Q = h is the CNN based question vector.</p><p>The diagram of CNN model for question is shown in Fig. <ref type="figure" target="#fig_4">4</ref>. The convolutional and pooling layers for unigrams, bigrams and trigrams are drawn in red, blue and orange, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Stacked Attention Networks</head><p>Given the image feature matrix v I and the question feature vector v Q , SAN predicts the answer via multi-step reasoning.</p><p>In many cases, an answer only related to a small region of an image. For example, in Fig. <ref type="figure" target="#fig_1">1b</ref>, although there are multiple objects in the image: bicycles, baskets, window, street and dogs and the answer to the question only relates to dogs. Therefore, using the one global image feature vector to predict the answer could lead to suboptimal results due to the noises introduced from regions that are irrelevant to the potential answer. Instead, reasoning via multiple attention layers progressively, the SAN are able to gradually filter out noises and pinpoint the regions that are highly relevant to the answer.</p><p>Given the image feature matrix v I and the question vector v Q , we first feed them through a single layer neural network and then a softmax function to generate the attention distribution over the regions of the image:</p><formula xml:id="formula_12">h A = tanh(W I,A v I ⊕ (W Q,A v Q + b A )),<label>(15)</label></formula><formula xml:id="formula_13">p I =softmax(W P h A + b P ),<label>(16)</label></formula><p>where v I ∈ R d×m , d is the image representation dimension and m is the number of image regions,</p><formula xml:id="formula_14">v Q ∈ R d is a d dimensional vector. Suppose W I,A , W Q,A ∈ R k×d and W P ∈ R 1×k , then p I ∈ R m is an m dimensional vector,</formula><p>which corresponds to the attention probability of each image region given v Q . Note that we denote by ⊕ the addition of a matrix and a vector. Since W I,A v I ∈ R k×m and both</p><formula xml:id="formula_15">W Q,A v Q , b A ∈ R k are</formula><p>vectors, the addition between a matrix and a vector is performed by adding each column of the matrix by the vector.</p><p>Based on the attention distribution, we calculate the weighted sum of the image vectors, each from a region, ṽi as in Eq. 17. We then combine ṽi with the question vector v Q to form a refined query vector u as in Eq. 18. u is regarded as a refined query since it encodes both question information and the visual information that is relevant to the potential answer:</p><formula xml:id="formula_16">ṽI = ∑ i p i v i ,<label>(17)</label></formula><formula xml:id="formula_17">u =ṽ I + v Q . (<label>18</label></formula><formula xml:id="formula_18">)</formula><p>Compared to models that simply combine the question vector and the global image vector, attention models construct a more informative u since higher weights are put on the visual regions that are more relevant to the question. However, for complicated questions, a single attention layer is not sufficient to locate the correct region for answer prediction. For example, the question in Fig. <ref type="figure" target="#fig_1">1</ref> what are sitting in the basket on a bicycle refers to some subtle relationships among multiple objects in an image. Therefore, we iterate the above query-attention process using multiple attention layers, each extracting more fine-grained visual attention information for answer prediction. Formally, the SANs take the following formula: for the k-th attention layer, we compute:</p><formula xml:id="formula_19">h k A = tanh(W k I,A v I ⊕ (W k Q,A u k−1 + b k A )),<label>(19)</label></formula><formula xml:id="formula_20">p k I =softmax(W k P h k A + b k P ).<label>(20)</label></formula><p>where u 0 is initialized to be v Q . Then the aggregated image feature vector is added to the previous query vector to form a new query vector:</p><formula xml:id="formula_21">ṽk I = ∑ i p k i v i ,<label>(21)</label></formula><formula xml:id="formula_22">u k =ṽ k I + u k−1 .<label>(22)</label></formula><p>That is, in every layer, we use the combined question and image vector u k−1 as the query for the image. After the image region is picked, we update the new query vector as u k = ṽk I + u k−1 . We repeat this K times and then use the final u K to infer the answer:</p><formula xml:id="formula_23">p ans =softmax(W u u K + b u ).<label>(23)</label></formula><p>Fig. <ref type="figure" target="#fig_1">1b</ref> illustrates the reasoning process by an example. In the first attention layer, the model identifies roughly the area that are relevant to basket, bicycle, and sitting in. In the second attention layer, the model focuses more sharply on the region that corresponds to the answer dogs. More examples can be found in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data sets</head><p>We evaluate the SAN on four image QA data sets. DAQUAR-ALL is proposed in <ref type="bibr" target="#b17">[18]</ref>. There are 6, 795 training questions and 5, 673 test questions. These questions are generated on 795 and 654 images respectively. The images are mainly indoor scenes. The questions are categorized into three types including Object, Color and Number. Most of the answers are single words. Following the setting in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref>, we exclude data samples that have multiple words answers. The remaining data set covers 90% of the original data set.</p><p>DAQUAR-REDUCED is a reduced version of DAQUAR-ALL. There are 3, 876 training samples and 297 test samples. This data set is constrained to 37 object categories and uses only 25 test images. The single word answers data set covers 98% of the original data set.</p><p>COCO-QA is proposed in <ref type="bibr" target="#b20">[21]</ref>. Based on the Microsoft COCO data set, the authors first parse the caption of the image with an off-the-shelf parser, then replace the key components in the caption with question words for form questions. There are 78736 training samples and 38948 test samples in the data set. These questions are based on 8, 000 and 4, 000 images respectively. There are four types of questions including Object, Number, Color, and Location. Each type takes 70%, 7%, 17%, and 6% of the whole data set, respectively. All answers in this data set are single word.</p><p>VQA is created through human labeling <ref type="bibr" target="#b0">[1]</ref>. The data set uses images in the COCO image caption data set <ref type="bibr" target="#b15">[16]</ref>. Unlike the other data sets, for each image, there are three questions and for each question, there are ten answers labeled by human annotators. There are 248, 349 training questions and 121, 512 validation questions in the data set. Following <ref type="bibr" target="#b0">[1]</ref>, we use the top 1000 most frequent answer as possible outputs and this set of answers covers 82.67% of all answers. We first studied the performance of the proposed model on the validation set. Following <ref type="bibr" target="#b5">[6]</ref>, we split the validation data set into two halves, val1 and val2. We use training set and val1 to train and validate and val2 to test locally. The results on the val2 set are reported in Table. 6. We also evaluated the best model, SAN(2, CNN), on the standard test server as provided in <ref type="bibr" target="#b0">[1]</ref> and report the results in Table <ref type="table">.</ref> 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Baselines and evaluation methods</head><p>We compare our models with a set of baselines proposed recently <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b16">17]</ref> on image QA. Since the results of these baselines are reported on different data sets in different literature, we present the experimental results on different data sets in different tables.</p><p>For all four data sets, we formulate image QA as a classification problem since most of answers are single words. We evaluate the model using classification accuracy as reported in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b18">19]</ref>. The reference models also report the Wu-Palmer similarity (WUPS) measure <ref type="bibr" target="#b28">[29]</ref>. The WUPS measure calculates the similarity between two words based on their longest common subsequence in the taxonomy tree. We can set a threshold for WUPS, if the similarity is less than the threshold, then it is zeroed out. Following the refer-ence models, we use WUPS0.9 and WUPS0.0 as evaluation metrics besides the classification accuracy. The evaluation on the VQA data set is different from other three data sets, since for each question there are ten answer labels that may or may not be the same. We follow <ref type="bibr" target="#b0">[1]</ref> to use the following metric: min(# human labels that match that answer/3, 1), which basically gives full credit to the answer when three or more of the ten human labels match the answer and gives partial credit if there are less matches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Model configuration and training</head><p>For the image model, we use the VGGNet to extract features. When training the SAN, the parameter set of the CNN of the VGGNet is fixed. We take the output from the last pooling layer as our image feature which has a dimension of 512 × 14 × 14 .</p><p>For DAQUAR and COCO-QA, we set the word embedding dimension and LSTM's dimension to be 500 in the question model. For the CNN based question model, we set the unigram, bigram and trigram convolution filter size to be 128, 256, 256 respectively. The combination of these filters makes the question vector size to be 640. For VQA dataset, since it is larger than other data sets, we double the model size of the LSTM and the CNN to accommodate the large data set and the large number of classes. In evaluation, we experiment with SAN with one and two attention layers. We find that using three or more attention layers does not further improve the performance.</p><p>In our experiments<ref type="foot" target="#foot_0">1</ref> , all the models are trained using stochastic gradient descent with momentum 0.9. The batch size is fixed to be 100. The best learning rate is picked using grid search. Gradient clipping technique <ref type="bibr" target="#b7">[8]</ref> and dropout <ref type="bibr" target="#b23">[24]</ref> are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results and analysis</head><p>The experimental results on DAQUAR-ALL, DAQUAR-REDUCED, COCO-QA and VQA are presented in Table <ref type="table">.</ref> 1 to 6 respectively. Our model names explain their settings: SAN is short for the proposed stacked attention networks, the value 1 or 2 in the brackets refer to using one or two attention layers, respectively. The keyword LSTM or CNN refers to the question model that SANs use.</p><p>The experimental results in Table <ref type="table">.</ref> 1 to 6 show that the two-layer SAN gives the best results across all data sets and the two kinds of question models in the SAN, LSTM and CNN, give similar performance. For example, on DAQUAR-ALL (Table. 1), both of the proposed twolayer SANs outperform the two best baselines, the IMG-CNN in <ref type="bibr" target="#b16">[17]</ref> and the Ask-Your-Neuron in <ref type="bibr" target="#b18">[19]</ref>, by 5.9% and 7.6% absolute in accuracy, respectively. Similar range of improvements are observed in metrics of WUPS0.9 and Methods Accuracy WUPS0.9 WUPS0.0 Multi-World: <ref type="bibr" target="#b17">[18]</ref> Multi-World 7.9 11.9 38.8</p><p>Ask-Your-Neurons: <ref type="bibr">[</ref>  The SAN only improves the performance of this type of questions slightly. This could due to that the answer for a Yes/No question is very question dependent, so better modeling of the visual information does not provide much additional gains. This also confirms the similar observation reported in <ref type="bibr" target="#b0">[1]</ref>, e.g., using additional image information only slightly improves the performance in Yes/No, as shown in Table <ref type="table">.</ref> 5, Q+I vs Question, and LSTM Q+I vs LSTM Q.</p><p>Our results demonstrate clearly the positive impact of using multiple attention layers. In all four data sets, twolayer SANs always perform better than the one-layer SAN. Specifically, on COCO-QA, on average the two-layer SANs outperform the one-layer SANs by 2.2% in the type of Color, followed by 1.3% and 1.0% in the Location and Objects categories, and then 0.4% in Number. This aligns to the order of the improvements of the SAN over baselines. Similar trends are observed on VQA (Table. 6), e.g., the two-layer SAN improve over the one-layer SAN by 1.4% for the Other type of question, followed by 0.2% improvement for Number, and flat for Yes/No.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Visualization of attention layers</head><p>In this section, we present analysis to demonstrate that using multiple attention layers to perform multi-step reasoning leads to more fine-grained attention layer-by-layer in locating the regions that are relevant to the potential answers. We do so by visualizing the outputs of the attention layers of a sample set of images from the COCO-QA test set. Note the attention probability distribution is of size 14 × 14 and the original image is 448 × 448, we up-sample the attention probability distribution and apply a Gaussian filter to make it the same size as the original image.</p><p>Fig. <ref type="figure">5</ref> presents six examples. More examples are presented in the appendix. They cover types as broad as Object, Numbers, Color and Location. For each example, the three images from left to right are the original image, the output of the first attention layer and the output of the second attention layer, respectively. The bright part of the image is the detected attention. Across all those examples, we see that in the first attention layer, the attention is scattered on many objects in the image, largely corresponds to the objects and concepts referred in the question, whereas in the second layer, the attention is far more focused on the regions that lead to the correct answer. For example, consider the question what is the color of the horns, which asks the color of the horn on the woman's head in Fig. <ref type="figure">5</ref>(f). In the output of the first attention layer, the model first recognizes a woman in the image. In the output of the second attention layer, the attention is focused on the head of the woman, which leads to the answer of the question: the color of the horn is red.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Errors analysis</head><p>We randomly sample 100 images from the COCO-QA test set that the SAN make mistakes. We group the errors into four categories: (i) the SANs focus the attention on the wrong regions (22%), e.g., the example in Fig. <ref type="figure">6</ref>(a); (ii) the SANs focus on the right region but predict a wrong answer (42%), e.g., the examples in Fig. <ref type="figure">6</ref>(b)(c)(d); (iii) the answer is ambiguous, the SANs give answers that are different from labels, but might be acceptable (31%). E.g., in Fig. <ref type="figure">6(e</ref>), the answer label is pot, but out model predicts vase, which is also visually reasonable; (iv) the labels are clearly wrong (5%). E.g., in Fig. <ref type="figure">6</ref>(f), our model gives the correct answer trains while the label cars is wrong.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a new stacked attention network (SAN) for image QA. SAN uses a multiple-layer attention mechanism that queries an image multiple times to locate the relevant visual region and to infer the answer progressively. Experimental results demonstrate that the proposed SAN significantly outperforms previous state-of-theart approaches by a great margin on all four image QA data. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Visualization of the learned multiple attention layers. The stacked attention network first focuses on all referred concepts, e.g., bicycle, basket and objects in the basket (dogs) in the first attention layer and then further narrows down the focus in the second layer and finds out the answer dog.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Model architecture and visualization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: CNN based image model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: LSTM based question model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: CNN based question model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Visualization of two attention layers</figDesc><graphic url="image-4.png" coords="8,62.49,409.89,470.19,321.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>DAQUAR-ALL results, in percentage</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Methods</cell><cell cols="3">Accuracy WUPS0.9 WUPS0.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>VSE: [21]</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>GUESS</cell><cell>6.7</cell><cell>17.4</cell><cell>73.4</cell></row><row><cell>Language Language + IMG CNN: [17] IMG-CNN</cell><cell>19] 19.1 21.7 23.4</cell><cell>25.2 28.0 29.6</cell><cell>65.1 65.0 63.0</cell><cell>BOW LSTM IMG IMG+BOW VIS+LSTM 2-VIS+BLSTM</cell><cell>37.5 36.8 43.0 55.9 53.3 55.1</cell><cell>48.5 47.6 58.6 66.8 63.9 65.3</cell><cell>82.8 82.3 85.9 89.0 88.3 88.6</cell></row><row><cell>Ours: SAN(1, LSTM) SAN(1, CNN) SAN(2, LSTM)</cell><cell>28.9 29.2 29.3</cell><cell>34.7 35.1 34.9</cell><cell>68.5 67.8 68.1</cell><cell>CNN: [17] IMG-CNN CNN</cell><cell>55.0 32.7</cell><cell>65.4 44.3</cell><cell>88.6 80.9</cell></row><row><cell>SAN(2, CNN)</cell><cell>29.3</cell><cell>35.1</cell><cell>68.6</cell><cell>Ours:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Human :[18] Human</cell><cell>50.2</cell><cell>50.8</cell><cell>67.3</cell><cell>SAN(1, LSTM) SAN(1, CNN) SAN(2, LSTM)</cell><cell>59.6 60.7 61.0</cell><cell>69.6 70.6 71.0</cell><cell>90.1 90.5 90.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SAN(2, CNN)</cell><cell>61.6</cell><cell>71.6</cell><cell>90.9</cell></row><row><cell>Methods</cell><cell cols="3">Accuracy WUPS0.9 WUPS0.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Multi-World: [18]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Multi-World</cell><cell>12.7</cell><cell>18.2</cell><cell>51.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Ask-Your-Neurons: [19]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Language</cell><cell>31.7</cell><cell>38.4</cell><cell>80.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Language + IMG</cell><cell>34.7</cell><cell>40.8</cell><cell>79.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VSE: [21]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GUESS</cell><cell>18.2</cell><cell>29.7</cell><cell>77.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BOW</cell><cell>32.7</cell><cell>43.2</cell><cell>81.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LSTM</cell><cell>32.7</cell><cell>43.5</cell><cell>81.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IMG+BOW</cell><cell>34.2</cell><cell>45.0</cell><cell>81.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VIS+LSTM</cell><cell>34.4</cell><cell>46.1</cell><cell>82.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2-VIS+BLSTM</cell><cell>35.8</cell><cell>46.8</cell><cell>82.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CNN: [17]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IMG-CNN</cell><cell>39.7</cell><cell>44.9</cell><cell>83.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SAN(1, LSTM)</cell><cell>45.2</cell><cell>49.6</cell><cell>84.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SAN(1, CNN)</cell><cell>45.2</cell><cell>49.6</cell><cell>83.7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SAN(2, LSTM)</cell><cell>46.2</cell><cell>51.2</cell><cell>85.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SAN(2, CNN)</cell><cell>45.5</cell><cell>50.2</cell><cell>83.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Human :[18]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Human</cell><cell>60.3</cell><cell>61.0</cell><cell>79.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Table 2: DAQUAR-REDUCED results, in percentage</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">WUPS0.0. We also observe significant improvements on</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">DAQUAR-REDUCED (Table. 2), i.e., our SAN(2, LSTM)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">outperforms the IMG-CNN [17], the 2-VIS+BLSTM [21],</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">the Ask-Your-Neurons approach [19] and the Multi-World</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">[18] by 6.5%, 10.4%, 11.5% and 33.5% absolute in accu-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">racy, respectively. On the larger COCO-QA data set, the</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">proposed two-layer SANs significantly outperform the best</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>COCO-QA results, in percentage</figDesc><table><row><cell>Methods</cell><cell cols="4">Objects Number Color Location</cell></row><row><cell>VSE: [21]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GUESS</cell><cell>2.1</cell><cell>35.8</cell><cell>13.9</cell><cell>8.9</cell></row><row><cell>BOW</cell><cell>37.3</cell><cell>43.6</cell><cell>34.8</cell><cell>40.8</cell></row><row><cell>LSTM</cell><cell>35.9</cell><cell>45.3</cell><cell>36.3</cell><cell>38.4</cell></row><row><cell>IMG</cell><cell>40.4</cell><cell>29.3</cell><cell>42.7</cell><cell>44.2</cell></row><row><cell>IMG+BOW</cell><cell>58.7</cell><cell>44.1</cell><cell>52.0</cell><cell>49.4</cell></row><row><cell>VIS+LSTM</cell><cell>56.5</cell><cell>46.1</cell><cell>45.9</cell><cell>45.5</cell></row><row><cell>2-VIS+BLSTM</cell><cell>58.2</cell><cell>44.8</cell><cell>49.5</cell><cell>47.3</cell></row><row><cell>Ours:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SAN(1, LSTM)</cell><cell>62.5</cell><cell>49.0</cell><cell>54.8</cell><cell>51.6</cell></row><row><cell>SAN(1, CNN)</cell><cell>63.6</cell><cell>48.7</cell><cell>56.7</cell><cell>52.7</cell></row><row><cell>SAN(2, LSTM)</cell><cell>63.6</cell><cell>49.8</cell><cell>57.9</cell><cell>52.8</cell></row><row><cell>SAN(2, CNN)</cell><cell>64.5</cell><cell>48.6</cell><cell>57.9</cell><cell>54.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>COCO-QA accuracy per class, in percentage</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>test-dev</cell><cell></cell><cell>test-std</cell></row><row><cell>Methods</cell><cell>All</cell><cell cols="3">Yes/No Number Other</cell><cell>All</cell></row><row><cell>VQA: [1]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Question</cell><cell>48.1</cell><cell>75.7</cell><cell>36.7</cell><cell>27.1</cell><cell>-</cell></row><row><cell>Image</cell><cell>28.1</cell><cell>64.0</cell><cell>0.4</cell><cell>3.8</cell><cell>-</cell></row><row><cell>Q+I</cell><cell>52.6</cell><cell>75.6</cell><cell>33.7</cell><cell>37.4</cell><cell>-</cell></row><row><cell>LSTM Q</cell><cell>48.8</cell><cell>78.2</cell><cell>35.7</cell><cell>26.6</cell><cell>-</cell></row><row><cell>LSTM Q+I</cell><cell>53.7</cell><cell>78.9</cell><cell>35.2</cell><cell>36.4</cell><cell>54.1</cell></row><row><cell cols="2">SAN(2, CNN) 58.7</cell><cell>79.3</cell><cell>36.6</cell><cell>46.1</cell><cell>58.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>VQA results on the official server, in percentage</figDesc><table><row><cell>baselines from [17] (IMG-CNN) and [21] (IMG+BOW and</cell></row><row><cell>2-VIS+BLSTM) by 5.1% and 6.6% in accuracy (Table. 3).</cell></row><row><cell>Table. 5 summarizes the performance of various models on</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>VQA results on our partition, in percentage VQA, which is the largest among the four data sets. The overall results show that our best model, SAN(2, CNN), outperforms the LSTM Q+I model, the best baseline from<ref type="bibr" target="#b0">[1]</ref>, by 4.8% absolute. The superior performance of the SANs across all four benchmarks demonstrate the effectiveness of using multiple layers of attention.In order to study the strength and weakness of the SAN in detail, we report performance at question-type level on the two large data sets, COCO-QA and VQA, in Table.<ref type="bibr" target="#b3">4</ref> and 5, respectively. We observe that on COCO-QA, compared to the two best baselines, IMG+BOW and 2-VIS+BLSTM, out best model SAN(2, CNN) improves 7.2% in the question type of Color, followed by 6.1% in Objects, 5.7% in Location and 4.2% in Number. We observe similar trend of improvements on VQA. As shown in Table. 5, compared to the best baseline LSTM Q+I, the biggest improvement of SAN(2, CNN) is in the Other type, 9.7%, followed by the 1.4% improvement in Number and 0.4% improvement in Yes/No. Note that the Other type in VQA refers to questions that usually have the form of "what color, what kind, what are, what type, where" etc., which are similar to question types of Color, Objects and Location in COCO-QA. The VQA data set has a special Yes/No type of questions.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Our code is publicly available at https://github.com/ zcyang/imageqa-san</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00468</idno>
		<title level="m">Vqa: Visual question answering</title>
				<imprint>
			<date type="published" when="2007">2015. 1, 2, 5, 6, 7</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic parsing via paraphrasing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.3676</idno>
		<title level="m">Question answering with subgraph embeddings</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning a recurrent visual representation for image caption generation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.5654</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">From captions to visual concepts and back</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4952</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Are you talking to a machine? dataset and methods for multilingual image question answering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05612</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.2306</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<title level="m">Convolutional neural networks for sentence classification</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2539</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07285</idno>
		<title level="m">Ask me anything: Dynamic memory networks for natural language processing</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.00333</idno>
		<title level="m">Learning to answer questions from image using convolutional neural network</title>
				<imprint>
			<date type="published" when="2006">2015. 2, 5, 6</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A multi-world approach to question answering about real-world scenes based on uncertain input</title>
		<author>
			<persName><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2006">2014. 1, 2, 4, 5, 6</date>
			<biblScope unit="page" from="1682" to="1690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.01121</idno>
		<title level="m">Ask your neurons: A neural-based approach to answering questions about images</title>
				<imprint>
			<date type="published" when="2006">2015. 1, 2, 5, 6</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6632</idno>
		<title level="m">Deep captioning with multimodal recurrent neural networks (m-rnn)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.02074</idno>
		<imprint>
			<date type="published" when="2006">2015. 1, 2, 5, 6</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A latent semantic model with convolutional-pooling structure for information retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management</title>
				<meeting>the 23rd ACM International Conference on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<title level="m">Going deeper with convolutions</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4555</idno>
		<title level="m">Show and tell: A neural image caption generator</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3916</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">Memory networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Verbs semantics and lexical selection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd annual meeting on Association for Computational Linguistics</title>
				<meeting>the 32nd annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="133" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03044</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semantic parsing via staged query graph generation: Question answering with knowledge base</title>
		<author>
			<persName><forename type="first">W.-T</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natural Language Processing of the AFNLP</title>
				<meeting>the Joint Conference of the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semantic parsing for singlerelation question answering</title>
		<author>
			<persName><forename type="first">W.-T</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
