<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DFG-NAS: Deep and Flexible Graph Neural Architecture Search</title>
				<funder ref="#_syga5hu #_qkr6FSw">
					<orgName type="full">NSFC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-06-17">17 Jun 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of CS &amp; Key Laboratory of High Confidence Software Technologies</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zheyu</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of CS &amp; Key Laboratory of High Confidence Software Technologies</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of CS &amp; Key Laboratory of High Confidence Software Technologies</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhi</forename><surname>Yang</surname></persName>
							<email>&lt;yangzhi@pku.edu.cn&gt;</email>
							<affiliation key="aff0">
								<orgName type="department">School of CS &amp; Key Laboratory of High Confidence Software Technologies</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of CS &amp; Key Laboratory of High Confidence Software Technologies</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
							<email>&lt;bin.cui@pku.edu.cn&gt;.</email>
							<affiliation key="aff0">
								<orgName type="department">School of CS &amp; Key Laboratory of High Confidence Software Technologies</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computational Social Science</orgName>
								<orgName type="institution">Peking University (Qingdao)</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DFG-NAS: Deep and Flexible Graph Neural Architecture Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-06-17">17 Jun 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2206.08582v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNNs) have been intensively applied to various graph-based applications. Despite their success, manually designing the well-behaved GNNs requires immense human expertise. And thus it is inefficient to discover the potentially optimal data-specific GNN architecture. This paper proposes DFG-NAS, a new neural architecture search (NAS) method that enables the automatic search of very deep and flexible GNN architectures. Unlike most existing methods that focus on micro-architectures, DFG-NAS highlights another level of design: the search for macro-architectures on how atomic propagation (P) and transformation (T) operations are integrated and organized into a GNN. To this end, DFG-NAS proposes a novel search space for P-T permutations and combinations based on message-passing dis-aggregation, defines four custom-designed macro-architecture mutations, and employs the evolutionary algorithm to conduct an efficient and effective search. Empirical studies on four node classification tasks demonstrate that DFG-NAS outperforms state-of-the-art manual designs and NAS methods of GNNs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graph Neural Networks (GNNs) are a set of message passing algorithms, whose intuition is to smooth the node embedding across the edges of a graph. By staking multiple GNN layers, each node can enhance its node embedding with distant neighborhood nodes. Recently, GNNs have been applied in various domains such as social network analysis <ref type="bibr" target="#b37">(Zhang et al., 2020;</ref><ref type="bibr">Huang et al., 2021)</ref>, chemistry and biology <ref type="bibr" target="#b2">(Dai et al., 2019;</ref><ref type="bibr" target="#b0">Bradshaw et al., 2019)</ref>, rec-ommendation <ref type="bibr" target="#b13">(Jiang et al., 2022;</ref><ref type="bibr" target="#b33">Wu et al., 2020)</ref>, natural language processing <ref type="bibr" target="#b32">(Wu et al., 2021;</ref><ref type="bibr" target="#b27">Vashishth et al., 2020)</ref>, and computer vision <ref type="bibr" target="#b25">(Shi et al., 2019;</ref><ref type="bibr" target="#b24">Sarlin et al., 2020)</ref>. Despite a broad spectrum of applications, designing GNN architectures manually is a knowledge-intensive and laborintensive process. Existing literature <ref type="bibr" target="#b10">(Huan et al., 2021)</ref> suggests that handcrafted architectures (e.g., GCN <ref type="bibr" target="#b14">(Kipf &amp; Welling, 2017)</ref>, GraphSAGE <ref type="bibr" target="#b6">(Hamilton et al., 2017)</ref>, and GAT <ref type="bibr" target="#b29">(Velickovic et al., 2018)</ref>) can not behave well in all scenarios. Therefore, there is an ever-increasing demand for automated architecture exploration to obtain the optimal data-specific GNN architectures.</p><p>Motivated by the success of neural architecture search (NAS) in other established areas, e.g., computer vision, several recent graph neural architecture search (G-NAS) methods are proposed to effectively tackle the architecture challenge in GNNs, including GraphNAS <ref type="bibr">(Gao et al., 2020a)</ref>, Auto-GNN <ref type="bibr" target="#b41">(Zhou et al., 2019)</ref>, and GraphGym <ref type="bibr" target="#b35">(You et al., 2020)</ref>. These G-NAS methods assume GNNs consist of several repetitive message passing layers, and focus more on the intra-layer design such as aggregation function and nonlinear activation function. Despite their effectiveness, they suffer from two limitations: Fixed Pipeline Pattern. To generate GNNs, existing methods adopt a fixed message-passing pipeline to organize two types of atomic operations: propagating (P) representations of its neighbors and applying transformation (T) on the representations. In particular, most G-NAS methods adopt the tight entanglement of applying transformation after propagation in each layer (e.g., <ref type="figure">P-T-P-T</ref>). Several handcrafted architectures include a certain degree of entanglement by only retaining the first transformation or propagation, e.g., T-P-P-P <ref type="bibr" target="#b15">(Klicpera et al., 2019;</ref><ref type="bibr" target="#b20">Liu et al., 2020)</ref> or P-P-P-T <ref type="bibr" target="#b31">(Wu et al., 2019;</ref><ref type="bibr" target="#b23">Rossi et al., 2020;</ref><ref type="bibr">Zhang et al., 2021c)</ref>. However, these specific P-T permutations and combinations are still fixed pipeline designs, limiting the expressive power of macro-architecture search space. It remains to be seen whether more general and flexible pipelines can further improve the performance.</p><p>Restricted Pipeline Depth. A common practice to increase the expressive power is to directly stack multiple GNN layers <ref type="bibr" target="#b14">(Kipf &amp; Welling, 2017)</ref>. However, when the layers become deeper, the performance decreases as the node em-bedding becomes indistinguishable with too many P operations, which we refer to as the over-smoothing issue <ref type="bibr" target="#b17">(Li et al., 2018;</ref><ref type="bibr" target="#b22">Miao et al., 2021b)</ref>. Therefore, the existing G-NAS methods fix the number of layers to a small constant. For example, both AutoGNN and GraphNAS pre-define a very restricted GNN layer number (e.g., ? 3). How to increase the pipeline depth is another critical problem to conduct effective G-NAS. This paper proposes DFG-NAS, a new method for automatic search of deep and flexible GNN architectures. Our key insight is that propagation P and transformation T operation correspond to enforcing and mitigating the effect of smoothing. Inspired by this insight, we propose to search for the best P-T permutations and combinations (i.e., pipeline) to tune suitable smoothness level and thus obtain well-behaved data-specific GNN architectures. Instead of the microarchitecture in intra-layer design, we explore another level of design-pipeline search in the GNN macro-architecture. Specifically, DFG-NAS greatly increases the capabilities of GNN macro-architecture search in two dimensions: the pipeline pattern and depth of GNN generation. We accomplish this by dis-aggregate the P and T operations in our search space and define four effective mutation designs to explore P-T permutations and combinations. To solve the over-smoothing problem, we propose a gating mechanism between different P operations so that node-adaptive propagation can be achieved. Furthermore, the skip-connection mechanism is used to T operations to avoid model degradation <ref type="bibr" target="#b7">(He et al., 2016;</ref><ref type="bibr">Zhang et al., 2021a)</ref>. Besides the pipeline and mutation designs, an evolutionary algorithm is applied to search for well-behaved GNN architectures.</p><p>The contribution of the paper is summarized as follows: (1) By decoupling the P and T operations, DFG-NAS suggests a transition from studying specific fixed GNN pipelines to studying the GNN pipeline design space. (2) By further adding gating and skip-connection mechanisms, DFG-NAS could support both deep propagation and transformation, which has the ability to explore the best architecture design to push forward the GNN performance boundary. (3) We search for the flexible pipeline using a custom-designed genetic algorithm so that the final searched GNN architecture represents the result of joint optimization over the pattern and depth of pipelines. (4) Empirical results demonstrate that DFG-NAS achieves an accuracy improvement of up to 0.9% over state-of-the-art manual designs and brings up to 15.96x speedups over existing G-NAS methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Problem Formulation</head><p>Given a graph G = (V, E) with |V| = N nodes and |E| = M edges, feature matrix X = {x 1 , x 2 ..., x N } in which x i ? R d is the feature vector of node v i , the node set V is partitioned into training set V train (including both the labeled set V l and unlabeled set V u ), validation set V val and test set V test . Suppose c is the number of label classes, the one-hot vector y i ? R c is the ground-truth label for node v i , M is the performance evaluation metric of a design in any given graph analysis task, e.g., F1 score or accuracy in the node classification task. Specifically, let F be the search space (finite or infinite) of graph neural architecture. Graph neural architecture search (G-NAS) aims to find the optimal design f ? F, so that the model can be trained to achieve the best performance in terms of the evaluation metric M on the validation set. Formally, it can be defined as the following bi-level optimization problem:</p><formula xml:id="formula_0">arg max f ?F Ev i ?V val [M (yi, P ( ?i|f (? * )))] , s.t. : ? * = arg min ? (f (? * ), Vtrain),<label>(1)</label></formula><p>where P ( ?i |f (? * )) is the predicted label distribution of node v i , is the loss function and ? * is the optimized weights of model design f . For each design f , G-NAS first trains the corresponding model weight ? on the training set V train . Then, it evaluates the trained model f (? * ) on the validation set V val to obtain the final evaluation result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Graph Neural Networks</head><p>Based on the intuitive assumption that locally connected nodes are likely to have the same label, most GNNs iteratively propagate the information of each node to its adjacent nodes, and then transform the information with non-linear transformation. We refer to the propagation and transformation operations as P and T, respectively. At timestep t, a message vector m t v for node v ? V is computed with the representations of its neighbors N v using the P operation, which is m t v ? P h t-1 u |u ? N v . Then, m t v is then updated according to h t v ? T(m t v ) via the T operation, where T is usually a dense layer.</p><p>Take the vanilla GCN <ref type="bibr" target="#b14">(Kipf &amp; Welling, 2017)</ref> as an example, a message passing layer can be formulated as:</p><formula xml:id="formula_1">P : M t = ?H t-1 , T : H t = ?(M t W t ), (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where ? is the normalized adjacent matrix, W t is the training parameters of the t-th layer, M t and H t are matrices formed by m t v and h t v , respectively. By stacking k layers, each node in GCN can utilize the information from its khop neighborhood. Therefore, the model performance is expected to be improved when more distant neighborhood nodes get involved in the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Graph Neural Architecture Search</head><p>As the basic operations of GNNs, the search of P and T operations has been widely discussed. Specifically, the existing G-NAS methods can be classified into micro-architecture search and macro-architecture search.</p><p>Micro-architecture search. Micro-architecture corresponds to the design of graph convolution layers, especially the details for P and T operations. Specifically, the P operation determines how to exchange messages among different nodes in each graph convolution layer. Many GNNs <ref type="bibr" target="#b14">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b31">Wu et al., 2019;</ref><ref type="bibr" target="#b15">Klicpera et al., 2019)</ref> adopt the normalized adjacency matrix for neighbor propagation, and several attention mechanisms <ref type="bibr" target="#b29">(Velickovic et al., 2018;</ref><ref type="bibr" target="#b30">Wang et al., 2021)</ref> are also proposed for more effective propagation. In addition, how to combine the propagated node embeddings is also important in P (e.g., MEAN, MAX, etc). Unlike P, which is closely related to the graph structure, T focuses on the non-linear transformation in the neural network, including the choices of hidden size and activation function. Several G-NAS frameworks <ref type="bibr">(Gao et al., 2020b;</ref><ref type="bibr" target="#b41">Zhou et al., 2019;</ref><ref type="bibr" target="#b1">Cai et al., 2021;</ref><ref type="bibr">Li et al., 2021b)</ref> have been proposed to search for the best P and T operations from a pool of various implementations. Different from these works, we focus on macro-architecture.</p><p>Macro-architecture search. Different from the microarchitecture that highlights the details in each GNN layer, the macro-architecture search focuses on the interaction between different layers. Concretely, macro-architecture search involves the design of network topology, including the choices of layer depth and the inter-layer skip connections. Similar to residual connections and dense connections in CNNs <ref type="bibr" target="#b7">(He et al., 2016;</ref><ref type="bibr" target="#b12">Huang et al., 2017)</ref>, node representations in one GNN layer do not necessarily solely depend on the immediate previous layer <ref type="bibr" target="#b34">(Xu et al., 2018;</ref><ref type="bibr" target="#b16">Li et al., 2019;</ref><ref type="bibr">Miao et al., 2021a)</ref>. Take the representative Graph-Gym <ref type="bibr" target="#b35">(You et al., 2020)</ref> as an example, the search space for macro-architecture includes the choice of graph convolutional layer depth, the pre-processing and post-processing layer depth, and the skip-connections. The main difference between our method and previous work is that we open up new design opportunities for macro-architecture search in GNNs. Specifically, we disentangle the P and T operations in GNN layers, and thus allow exploring P-T permutations and combinations in designing GNN architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">How do P and T influence GNNs?</head><p>3.1. Entanglement of GNNs Entangled GNNs. The pattern of Entangled Propagation and Transformation is widely adopted by mainstream GNNs, e.g., GCN <ref type="bibr" target="#b14">(Kipf &amp; Welling, 2017)</ref>, GraphSAGE <ref type="bibr" target="#b6">(Hamilton et al., 2017)</ref>, GAT <ref type="bibr" target="#b29">(Velickovic et al., 2018)</ref> SAINT <ref type="bibr" target="#b36">(Zeng et al., 2020)</ref>. Similar to GCN shown in Figure <ref type="figure" target="#fig_0">1</ref>, entangled GNNs pass the input signals through a set of filters to propagate the information, which is further followed by a non-linear transformation. The propagation operation P and transformation operation T are intertwined and executed alternately in entangled GNNs. Therefore, the entangled GNNs share a strict restriction that D p = D t , where D p and D t are the number of propagation and transformation operations, respectively.</p><p>Disentangled GNNs. Recently, some researches show that the entanglement of P and T could compromise performance on a range of benchmark tasks <ref type="bibr" target="#b31">(Wu et al., 2019;</ref><ref type="bibr" target="#b8">He et al., 2020;</ref><ref type="bibr" target="#b20">Liu et al., 2020;</ref><ref type="bibr" target="#b3">Frasca et al., 2020;</ref><ref type="bibr" target="#b15">Klicpera et al., 2019)</ref>. They also argue that the true effectiveness of GNNs lies in the propagation operation P rather than the T operation inside the graph convolution. In this way, some disentangled GNNs are proposed to separate P and T. Following SGC <ref type="bibr" target="#b31">(Wu et al., 2019)</ref> as shown in Figure <ref type="figure" target="#fig_0">1</ref>, several methods <ref type="bibr" target="#b8">(He et al., 2020;</ref><ref type="bibr" target="#b3">Frasca et al., 2020)</ref> execute P operations in advance, and then feed the propagated features into multiple T operations. On the contrary, several methods first transform the node features and then propagate the node information to distant neighbors, e.g., APPNP <ref type="bibr" target="#b15">(Klicpera et al., 2019)</ref>, DAGNN <ref type="bibr" target="#b20">(Liu et al., 2020)</ref>, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pipeline Pattern and Depth</head><p>The number of P and T operations plays a very important role in GNN learning.. As introduced in previous studies <ref type="bibr" target="#b31">(Wu et al., 2019;</ref><ref type="bibr" target="#b20">Liu et al., 2020;</ref><ref type="bibr">Miao et al., 2021a)</ref>, more P operations are required to enhance the information propagation when the edges, labels or features are sparse.</p><p>In addition, it is widely recognized that more training parameters and T operations should be used to increase the expressive power of neural networks when the size of dataset (i.e., the number of nodes in a graph) is large.</p><p>Besides the number of P and T operations, the pipeline pattern (i.e., permutations and combinations) of P and T operations also matters. To verify this, we fix the depths of P and T operations to 2 and only change the pipeline pattern. We evaluate the three architectures in Figure <ref type="figure" target="#fig_0">1</ref>, and report their test accuracy on three citation networks. The  <ref type="table" target="#tab_0">1</ref> show that the test accuracy varies a lot in the pipeline pattern. Concretely, the TTPP architecture achieves the best performance on PubMed, while it is less competitive than the PPTT architecture on Cora and Citeseer. Since the pipeline pattern highly influences the architecture performance and the optimal pipeline pattern varies across different graph datasets, it is necessary to consider the pipeline pattern given a specific dataset/task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Influence on smoothness</head><p>It is widely recognized that the propagation operation in GNN is a Laplacian smoothing, which may lead to indistinguishable node representations <ref type="bibr" target="#b17">(Li et al., 2018)</ref>. In other words, the representations for all nodes will converge to the same value when GNNs go deeper, which is also called over-smoothing. In fact, most entangled GNNs (e.g., GCN and GAT) face the over-smoothing problem and suffer from performance degradation when stacking too many GNN layers. To better measure the influence of P and T operations on smoothness, we analyze the change of smoothness when adding a single P or T operation each time.</p><p>We measure the smoothness by calculating the average similarity <ref type="bibr" target="#b20">(Liu et al., 2020)</ref> between two different node embeddings after a P or T operation. Concretely, the node smoothness is defined as follows,</p><formula xml:id="formula_3">S t i = 1 - 1 2N -2 j?V,j =i e t i ||e t i || - e t j ||e t j || ,<label>(3)</label></formula><p>where e t i is the i-th node embedding of the t-th GNN layer, and S t i measures the similarity of e t i to the entire graph. Larger e t i means node i faces higher risk of over-smoothing issue, and S t i ranges in [0, 1] since we adopt the normalized node embedding to compute their Euclidean distance. Based on the node smoothness S t i , we further measure the average similarities between all the node pairs and define the smoothness of the whole graph G as:</p><formula xml:id="formula_4">S t = 1 N i?V S t i ,<label>(4)</label></formula><p>where S t is the graph smoothness of the t-th GNN layer.</p><p>Unlike previous G-NAS methods that treat the combination of P T as one GNN layer, we treat each P or T here as an individual layer. We change different permutations and combinations of P and T, and report the corresponding output smoothness of each layer in SGC and GCN on Cora. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, the results show that 1) the smoothness increases, i.e., the node embedding becomes similar, by applying the P operation; 2) the smoothness decreases by applying the T operation, which implies that the T operation has the ability to alleviate the over-smoothing issue. While over-smoothing leads to indistinguishable node embedding, under-smoothing cannot unleash the full potential of the graph structure. Therefore, how to carefully control the smoothness with P or T in GNNs is also an open question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The Proposed Method</head><p>Based on the message passing dis-aggregation, we propose a general design space for GNN pipeline search, which includes the crucial aspects of the number, and the permutations and combinations of P and T operations. Then we provides a custom-designed genetic algorithm to search the space of pipeline pattern and depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Pipeline Search Space</head><p>Our general search space of GNNs pipeline includes P-T permutations and combinations, and the number of P-T operations. Besides, we further add connections among each type of operation to enable deep propagation and transformation in DFG-NAS. Suppose a single P or T operation is one GNN layer in DFG-NAS, o</p><p>v is the output of node v in the l-th layer, L P and L T are two sets that include the layer index of all P operations and T operations, respectively. The layer connections between different layers is as follows.</p><p>Propagation Connection. As introduced in Section 3.3, GNNs may suffer from the over-smoothing or undersmoothing issue if we execute too many or too few propagation operations. In addition, a previous study <ref type="bibr" target="#b22">(Zhang et al., 2021b)</ref> shows that the nodes with different local structures have different smoothing speeds. Therefore, how to control the smoothness of different nodes in a node-adaptive way is essential in GNN architecture design.</p><p>To allow deep propagation and provide suitable smoothness for different nodes, we adopt a gating mechanism upon P operations. The output of the l-th P operation is the propagated node embedding of o (l-1) if its next operation is P. As shown in Figure <ref type="figure" target="#fig_2">3</ref>, if the next operation is T, we assign a node-adaptive combination weight for the node embeddings propagated by all previous P operations. The above process can be formulated as,</p><formula xml:id="formula_6">z (l) v = P(o (l-1) v ), o (l) v = ? ? ? ? ? z (l) v ,</formula><p>Followed by</p><formula xml:id="formula_7">P i?L P ,i?l Softmax(?i)z (i) v , Followed by T ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_8">? i = ?(s ? o i v )</formula><p>is the weight for i-th layer output of node v. s is the trainable vector shared by all nodes , and ? denotes the Sigmoid function We adopt the Softmax function to scale the sum of gating scores to 1.</p><p>Transformation Connection. GNNs require more training parameters and non-linear transformation operations to enhance their expressive power on larger graphs. However, it is widely acknowledged that too many transformation operations will lead to the model degradation issue <ref type="bibr" target="#b7">(He et al., 2016)</ref>, i.e., both the training and test accuracies decrease along with the increased transformation steps.</p><p>To allow deeper transformation and meanwhile alleviate the model degradation issue, we introduce the skip-connection mechanism into T operations. As shown in Figure <ref type="figure" target="#fig_2">3</ref>, the input of each T operation is the sum of the output of the last layer and the outputs of all previous T operations before the last layer. Concretely, the input and output of the l-th T operation can be defined as follows:</p><formula xml:id="formula_9">z (l) v = o (l-1) v + i?L T ,i&lt;m(l) o (i) v , o (l) v = ? z (l) v W (l) ,<label>(6)</label></formula><p>where m(l) is the index of the last T operation before the l-th layer, and W (l) is the learnable parameter in the l-th T operation. Specifically, the first layer in our search space is a T operation, and z (1) refers to the original node features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Search via Genetic Algorithm</head><p>Genetic algorithm (GA) aims to evolve and improve an entire population of individuals via nature-inspired mech-anisms such as mutations. In DFG-NAS, an individual's chromosome represents a GNN pipeline. To create a new generation of individuals, we perform the following mutations on the chromosomes of the current generation.</p><p>Mutation Designs. Evolutionary algorithms are a class of optimization algorithms inspired by biological evolution. Specifically, they apply mutations on a population of designs, i.e., the set of different GNN architectures. In DFG-NAS, each GNN architecture is encoded as a sequence consisting of the P and T operations. As shown in Figure <ref type="figure" target="#fig_4">4</ref>, we design four mutations, which are as follows,</p><p>Case 1 + P: Add a propagation operation.</p><p>Case 2 + T: Add a transformation operation.</p><p>Case 3 P ? T: Replace a propagation operation by a transformation operation.</p><p>Case 4 T ? P: Replace a transformation operation by a propagation operation.</p><p>The above four mutations take place at a random position of the sequence. For example, we can add a propagation operation after or before any other operations. These four specific mutations are proposed for their similarity to the actions that a human designer may take when improving the GNN architecture.</p><p>Evolutionary Algorithm. To perform an efficient and effective search on our search space, we adopt the evolutionary algorithm as the searching method, which is a class of optimization algorithms inspired by biological evolution. The concrete searching pipeline is summarized in Algorithm 1.</p><p>We randomly generate k different GNN architectures as initial individuals in a population set Q (line 1), and then evaluate these GNNs on the validation set (line 2). Next, we randomly sample m (m &lt; k) individuals from the population (line 4) and select the architecture with the best validation performance as parent A (line 5). After that, the child GNN architecture B is generated by randomly picking one of the four mutations (i.e., + P, + T, P ? T, or T ? P) introduced in Section 4.2. At last, B is evaluated and added to the population (line 7), and then the oldest individual in Q is removed (line 8). The above process is repeated for T generations and finally returns the architecture with the best observed evaluation performance (line 9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Relationship with Existing Literature</head><p>The proposed DFG-NAS differs from existing G-NAS methods in both the pipeline pattern and pipeline depth of the searched architectures. GNN layer, which leads to a fixed pipeline pattern. Instead, DFG-NAS disentangles the propagation and transformation operations and thus enjoys better flexibility.</p><p>Shallow vs. Deep. The over-smoothing issue in deep GNNs has not been well studied and considered in the search space of the existing methods. For example, both GraphNAS and AutoGNN adopt a search space that applies a fixed threelayer GNN as the macro-architecture. DFG-NAS enables both deep P and T with the gating mechanism and skipconnection mechanism, respectively.</p><p>Note that DFG-NAS mainly focuses on GNN macroarchitecture search, so it is orthogonal to and compatible with the GNN micro-architecture search space (e.g, aggregation/activation functions, batch normalization, etc.) used by existing G-NAS methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and Results</head><p>To evaluate DFG-NAS, we apply it on four public graph datasets. Compared with the state-of-the-art baselines, we list three main insights that we will investigate as follows,</p><p>? DFG-NAS generates more powerful architectures than state-of-the-art manual designs.</p><p>? DFG-NAS works more efficiently than other NAS methods for GNN. It reaches similar performance to other methods while spending less search time.</p><p>? DFG-NAS is able to tackle the two limitations as mentioned in Section 1. Concretely, the Gate operation helps avoid over-smoothing while the disentanglement of P and T increases architecture flexibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setup</head><p>Baselines. We compare our proposed method DFG-NAS with nine manual GNN architectures and three NAS methods for GNN. The manual designs include the following three types of GNNs according to their pipeline pattern of propagation (P) and transformation (T) operations.</p><p>? Alternate P and T: GCN <ref type="bibr" target="#b14">(Kipf &amp; Welling, 2017)</ref>, GraphSAGE <ref type="bibr" target="#b6">(Hamilton et al., 2017)</ref>, and GAT <ref type="bibr" target="#b28">(Veli?kovi? et al., 2017)</ref>. The propagation operation P and transformation operation T are intertwined and executed alternately in these entangled GNNs.</p><p>? T before P: APPNP <ref type="bibr" target="#b15">(Klicpera et al., 2019)</ref>, AP-GCN <ref type="bibr" target="#b26">(Spinelli et al., 2021)</ref>, and DAGNN <ref type="bibr" target="#b20">(Liu et al., 2020)</ref>. These disentangled GNNs firstly transform the node features with T operations, and then propagate the outputs with multiple P operations.</p><p>? P before T: SGC <ref type="bibr" target="#b31">(Wu et al., 2019)</ref> , SIGN <ref type="bibr" target="#b23">(Rossi et al., 2020)</ref>, and S 2 GC <ref type="bibr" target="#b42">(Zhu &amp; Koniusz, 2021)</ref>. On contrary to APPNP, these disentangled GNNs execute multiple P operations in advance, and then transform propagated features with T operations.</p><p>More details of the above manual methods are provided in Appendix A.3. Besides, the compared G-NAS methods include: (1) Auto-GNN <ref type="bibr" target="#b41">(Zhou et al., 2019)</ref>: a reinforced conservative search strategy by adopting both RNNs and evolutionary algorithms in the controller; (2) Graph-NAS <ref type="bibr">(Gao et al., 2020a</ref>): a reinforcement learning-based method that uses an RNN controller to sample from the multiple architectures sequentially; (3) GraphGym <ref type="bibr" target="#b35">(You et al., 2020)</ref>: a variant of random search on a general GNN search space that considers intra-layer design, inter-layer design, and training configurations.</p><p>Datasets. We conduct the experiments on four public graph datasets: three citation graphs (Cora, Citeseer and PubMed) <ref type="bibr" target="#b14">(Kipf &amp; Welling, 2017)</ref>, and one large OGB graph (ogbn-arxiv) <ref type="bibr" target="#b9">(Hu et al., 2020)</ref>. We follow the public training/validation/test split for three citation networks and adopt the official split in the OGB graph. The statistics of these datasets are summarized in Appendix A.1.</p><p>Experimental Settings. For a fair end-to-end comparison on each task, we run DFG-NAS for 500 iterations, and set the time cost as the budget for other G-NAS baselines.</p><p>Then we report the average best observed test accuracy of the searched architecture. Specifically, we re-training each searched architecture ten times to avoid randomness. These baselines are implemented based on their open-sourced version. Since AutoGNN is not publicly available, we only report its performance on citation graphs following its paper. The details of hyperparameters and reproduction instructions are provided in Appendix A.2 and A.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with Existing GNNs</head><p>We first compare the architectures obtained by DFG-NAS with state-of-the-art manual designs on four datasets in Table <ref type="table" target="#tab_1">2</ref>. The structures of the searched architectures are provided in Appendix A.6. Among the manual designs, DAGNN achieves stable and competitive performance on four datasets. The reason is that it applies a single Gate operation to combine all propagation outputs, and in this way, it allows a deeper propagation than other manual designs. However, employs the straight-forward MLP for transformation, which may lead to the model degradation issue when transformation goes deeper. In addition, as all the compared manual designs follow a specific pipeline pattern of P and T, their performance can not be further improved due to their restricted flexibility. As a result, DFG-NAS obtains the deepest architectures with high expressive power and achieves the best performance on all four datasets. Remarkably, the architecture searched by DFG-NAS on the large-scale ogbn-arxiv dataset contains 27 P and 16 T operations, and it outperforms the best manual design DAGNN by a margin of 0.3% on test accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with NAS methods</head><p>Figure <ref type="figure">5</ref> and Table <ref type="table" target="#tab_1">2</ref> demonstrate the performance of DFG-NAS compared with existing NAS methods for GNN. Since AutoGNN is not open-sourced, we only report the results on three datasets from its paper. From Figure <ref type="figure">5</ref>, we observe that DFG-NAS consistently outperforms the compared NAS methods. The main reason is that the existing NAS methods only consider a fixed pipeline pattern of propagation and transformation operations in their architecture design, which may not be the optimal order as introduced in Section 3.2. When the search budget exhausts, DFG-NAS outperforms the second-best baseline by a margin of 0.3-1.5% on test accuracy. Compared with the second-best baseline GraphNAS, DFG-NAS achieves 6.97-15.96x speedups when achieving the same test accuracy on four datasets, which indicates the superior efficiency of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Study</head><p>In this part, we present an ablation study to show the effectiveness of three designs in DFG-NAS, which are 1) the Gate operation; 2) the disentanglement of architectures and 3) the skip-connection operation.</p><p>In Table <ref type="table" target="#tab_2">3</ref>, we compare DFG-NAS with two baselines: 1) A w/o Gate: the resulting architecture with the Gate operation disabled and 2) S w/o Gate: the searched results over a reduced space without the Gate operation in architecture design. We observe that the accuracy declines on all datasets. Remarkably, the accuracy drops by 3.0-3.8% for architectures without Gate. The reason is that without the Gate operation, the transformation steps only take the last output of the propagation steps as inputs, which may suffer from a risk of over-smoothing. The Gate operation dynamically aggregates the information from all propagation steps and thus controls the smoothness of different nodes in a node-adaptive way. In addition, when the Gate operation is removed from the architecture space, the accuracy drops by 0.8-1.8%. In this setting, the searched architectures tend to have shallow propagation (i.e., only 3-5 steps) on all datasets, which is not enough to capture sufficient neighborhood information for each node. These ablation results demonstrate the importance of the gating mechanism on GNN architecture design. To show the flexibility of the search space in DFG-NAS, we consider the following search spaces inspired by existing designs of propagation (P) and transformation (T): (1) P before T, (2) T before P, and ( <ref type="formula" target="#formula_3">3</ref>) alternate P and T. The depths of P and T range from 1 to 10, which leads to 100 unique architectures in (1) and ( <ref type="formula" target="#formula_1">2</ref>), and ten unique architectures in (3). We exhaustively evaluate all possible architectures in the baseline search space, and Table <ref type="table" target="#tab_3">4</ref> shows the best observed performance over each search space. Among the baselines, alternate P and T has the worst performance, which is consistent with the observations in previous studies <ref type="bibr" target="#b20">(Liu et al., 2020)</ref>. In addition, none of the three baselines dominate the others on all datasets, which indicates the necessity of a more flexible search on macro-architecture. Since the search space of DFG-NAS does not restrict the pipeline pattern of P and T, the space of DFG-NAS covers the compared baselines, and the architectures are more flexible than those from the baselines. As a result, DFG-NAS achieves an increase of 1.1% to 2.3% on all datasets.</p><p>To show the effectiveness of skip-connection. In Table <ref type="table" target="#tab_4">5</ref>, we compare DFG-NAS with two baselines: 1) A w/o Skip: the resulting architecture with the Skip-connection disabled and 2) S w/o Skip: the searched results over a reduced space without the skip-connection in architecture design. We observe that the accuracy declines on all datasets. And we also find when the skip-connection is removed from the search space, the architectures searched by our method will have less T operations.</p><p>To summarize, the searched architecture performance is improved by introducing the Gate operation in propagation steps, disentangling P and T in architecture designs, and the skip-connection in T operations. All of them may help unleash the full potential of GNNs and inspire more powerful manual designs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Interpretability</head><p>In Figure <ref type="figure">6</ref>, we plot the average smoothness of the last layer outputs of architectures in the population during each iteration on two datasets. The definition of smoothness is provided in Equation <ref type="formula" target="#formula_4">4</ref>. Though graph datasets differ in the requirement of smoothness, both two curves are generally saturating. In the first half of the search process, the smoothness quickly increases due to the frequent elimination of bad initial individuals. Then, the smoothness fluctuates around a certain level (0.46 for Cora and 0.90 for Pubmed), which implies that the search algorithm in DFG-NAS can ensure sufficient smoothness of inputs in the searched architectures, and meanwhile avoid over-smoothing.</p><p>Finally, we study how the sparsity and size of datasets influence the number of P and T in the searched architectures, respectively. To simulate datasets with different sparsity, we randomly delete some of the edges in PubMed. Figure <ref type="figure">7</ref>(a) shows that the average number of P in top-10 architectures increases when the dataset grows sparser. The reason is that, when the graph is sparse, the architecture should contain more propagation steps so that each node is able to capture sufficient neighborhood information for classification. On the other hand, when the size of datasets grows larger, the architecture should include more transformation steps to ensure the expressive power. As shown in Figure <ref type="figure">7</ref>(b), the average number of T is similar in Core and Citeseer, which includes about 3k samples. The number further increases to 4.5 and 13.0 on larger datasets PubMed and ogbn-arxiv with 20k and 169k samples, respectively. The trend of P and T discovered by DFG-NAS may help guide the manual design of flexible GNN architectures, i.e., more propagation steps are required when the graph is sparse, while more transformation steps are needed for the large graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we proposed DFG-NAS, a new method that allows both flexible and deep graph neural architecture search.</p><p>The key idea of DFG-NAS is to dis-aggregate the design pipeline of GNN generation, allowing a flexible permutations and combinations of the two basic operations in GNNs: propagation and transformation. In addition, we also analyzed how both operations influence the smoothness. Based on the observation, we adopted the gating mechanism and skip-connection mechanisms to support very deep GNN pipelines. We provided the genetic algorithm to search for a good permutations and combinations in GNNs. The experimental results on four different graph datasets showed that DFG-NAS achieves an accuracy improvement of up to 0.9% over state-of-the-art manual designs and brings up to 15.96x speedups over existing G-NAS methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Entanglement of GNNs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Smoothness of different PT orders.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. GNN pipeline example in the search space of DFG-NAS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fixed68</head><label></label><figDesc>vs. Flexible. The existing G-NAS methods treat the combination of propagation and transformation as one Algorithm 1 Searching method. Input: The population set Q, the maximum generation times T , the generation number t = 1. Output: The best GNN architecture. 1 Initialize the population set Q with |Q| = k; 2 Evaluating k different GNN architectures in Q; 3 for 1 ? t ? T do 4 Randomly sample m individuals from Q; 5 Select the parent A with best evaluation performance from the m individuals; Randomly mutate A with the mutation designs, and get the mutated individual B; 7 Evaluate B and adding it to Q;Remove the oldest individual in Q; 9 return the individual with best evaluation performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Overview of four different mutations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Figure 6. Average smoothness over iterations on two datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Test accuracy of GNNs with different PT orders.</figDesc><table><row><cell>Methods</cell><cell>Cora</cell><cell>Citeseer PubMed</cell></row><row><cell>PPTT</cell><cell cols="2">83.4?0.3 72.2?0.4 78.5?0.5</cell></row><row><cell>TTPP</cell><cell cols="2">82.8?0.2 71.8?0.3 79.8?0.3</cell></row><row><cell>PTPT</cell><cell cols="2">81.2?0.6 71.2?0.4 79.1?0.2</cell></row><row><cell cols="2">experimental results in Table</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Test accuracy on the node classification task.</figDesc><table><row><cell>Methods</cell><cell>Cora</cell><cell cols="2">Citeseer PubMed ogbn-arxiv</cell></row><row><cell></cell><cell cols="2">Alternate P and T</cell></row><row><cell>GCN</cell><cell cols="2">81.3?0.6 71.1?0.1 78.8?0.4</cell><cell>71.7?0.3</cell></row><row><cell>GAT</cell><cell cols="2">82.9?0.2 70.8?0.5 79.1?0.1</cell><cell>71.9?0.2</cell></row><row><cell cols="3">GraphSAGE 79.2?0.6 71.6?0.5 77.4?0.5</cell><cell>71.5?0.3</cell></row><row><cell></cell><cell></cell><cell>T before P</cell></row><row><cell>APPNP</cell><cell cols="2">83.1?0.5 71.8?0.4 80.1?0.2</cell><cell>72.0?0.1</cell></row><row><cell>AP-GCN</cell><cell cols="2">83.4?0.3 71.3?0.5 79.7?0.3</cell><cell>71.9?0.2</cell></row><row><cell>DAGNN</cell><cell cols="2">84.3?0.2 73.3?0.6 80.5?0.5</cell><cell>72.0?0.3</cell></row><row><cell></cell><cell></cell><cell>P before T</cell></row><row><cell>SGC</cell><cell cols="2">81.7?0.2 71.3?0.2 78.8?0.1</cell><cell>71.6?0.3</cell></row><row><cell>SIGN</cell><cell cols="2">82.1?0.3 72.4?0.8 79.5?0.5</cell><cell>71.9?0.1</cell></row><row><cell>S 2 GC</cell><cell cols="2">82.7?0.3 73.0?0.2 79.9?0.3</cell><cell>71.8?0.3</cell></row><row><cell></cell><cell></cell><cell>G-NAS Methods</cell></row><row><cell>GraphNAS</cell><cell cols="2">83.7?0.4 73.5?0.3 80.5?0.3</cell><cell>71.7?0.2</cell></row><row><cell>AutoGNN</cell><cell cols="2">83.6?0.3 73.8?0.7 79.7?0.4</cell><cell>/</cell></row><row><cell cols="3">GraphGym 83.5?0.2 73.4?0.3 80.3?0.2</cell><cell>71.6?0.3</cell></row><row><cell>DFG-NAS</cell><cell cols="2">85.2?0.2 74.1?0.4 81.1?0.3</cell><cell>72.3?0.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation study on Gate operations.</figDesc><table><row><cell>Methods</cell><cell>Cora</cell><cell cols="2">Citeseer PubMed ogbn-arxiv</cell></row><row><cell cols="3">A w/o Gate 82.0?0.3 70.8?0.5 78.1?0.1</cell><cell>68.5?0.6</cell></row><row><cell cols="3">S w/o Gate 83.6?0.2 72.6?0.2 79.3?0.3</cell><cell>71.5?0.4</cell></row><row><cell cols="3">DFG-NAS 85.2?0.2 74.1?0.4 81.1?0.3</cell><cell>72.3?0.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation study on pipeline pattern.</figDesc><table><row><cell>Methods</cell><cell>Cora</cell><cell cols="2">Citeseer PubMed ogbn-arxiv</cell></row><row><cell>P before T</cell><cell cols="2">83.2?0.3 72.7?0.2 80.0?0.4</cell><cell>69.4?0.1</cell></row><row><cell>T before P</cell><cell cols="2">83.6?0.5 71.8?0.3 79.0?0.5</cell><cell>70.0?0.1</cell></row><row><cell cols="3">Alternate P and T 82.5?0.4 69.9?0.6 78.1?0.3</cell><cell>68.1?0.4</cell></row><row><cell>DFG-NAS</cell><cell cols="2">85.2?0.2 74.1?0.4 81.1?0.3</cell><cell>72.3?0.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Ablation study on skip-connection.</figDesc><table><row><cell>Best Test Accuacy</cell><cell>0.725 0.750 0.775 0.800 0.825 0.850</cell><cell>0</cell><cell>2000</cell><cell>4000</cell><cell>6000</cell><cell>8000</cell><cell>10000 GraphNAS 12000 GraphGym DFG-NAS</cell><cell>Best Test Accuacy</cell><cell>0.50 0.55 0.60 0.65 0.70 0.75</cell><cell>0</cell><cell>2000</cell><cell>4000</cell><cell>6000</cell><cell>GraphNAS GraphGym DFG-NAS</cell><cell>8000</cell><cell>Best Test Accuacy</cell><cell>0.72 0.82 0.74 0.76 0.78 0.80</cell><cell>0</cell><cell>4600</cell><cell>9200</cell><cell>13800</cell><cell>18400 GraphNAS 23000 GraphGym DFG-NAS</cell><cell>Best Test Accuacy</cell><cell>0.75 0.50 0.55 0.60 0.65 0.70</cell><cell>0</cell><cell>10000</cell><cell>20000</cell><cell>30000</cell><cell>40000 GraphNAS 50000 GraphGym DFG-NAS</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Search Time (s)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Search Time (s)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Search Time (s)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Search Time(s)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(a) Cora</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) Citeseer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(c) Pubmed</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(d) ogbn-arxiv</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="17">Figure 5. Test results during neural architecture search on four datasets.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Methods</cell><cell></cell><cell cols="2">Cora</cell><cell cols="7">Citeseer PubMed</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="14">A w/o Skip 83.7?0.5 71.6?0.5 81.0?0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="14">S w/o Skip 83.9?0.4 72.8?0.6 80.6?0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="14">DFG-NAS 85.2?0.2 74.1?0.4 81.1?0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work is supported by <rs type="funder">NSFC</rs> (No. <rs type="grantNumber">61832001</rs>, <rs type="grantNumber">61972004</rs>), and <rs type="person">Alibaba Group</rs> through <rs type="programName">Alibaba Innovative Research Program</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_syga5hu">
					<idno type="grant-number">61832001</idno>
				</org>
				<org type="funding" xml:id="_qkr6FSw">
					<idno type="grant-number">61972004</idno>
					<orgName type="program" subtype="full">Alibaba Innovative Research Program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Outline</head><p>The appendix is organized as follows:</p><p>A.1 Datasets Description.</p><p>A.2 Hyper-parameters Setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Compared Baselines.</head><p>A.4 Efficiency Analysis.</p><p>A.5 Reproduction Instructions.</p><p>A.6 The Best Architecture Searched by DFG-NAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Dataset Description</head><p>Cora, Citeseer, and Pubmed 1 are three popular citation network datasets, and we follow the public training/validation/test split in GCN <ref type="bibr" target="#b14">(Kipf &amp; Welling, 2017)</ref>. In these three networks, papers from different topics are considered as nodes, and the edges are citations among the papers. The node attributes are binary word vectors, and class labels are the topics papers belong to.</p><p>ogbn-arxiv is a directed graph, representing the citation network among all Computer Science (CS) arXiv papers indexed by MAG. The training/validation/test split in our experiment is the same as the public version. The public version provided by OGB 2 is used in our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Hyper-parameters Setting</head><p>For the architecture search, the number of the population set k and the maximum generation times T in Algorithm 1 are 20 and 500 for all datasets. For the training of GNN architectures, we follow the same hyper-parameter in their original paper and tune it with OpenBox <ref type="bibr">(Li et al., 2021a)</ref>. The training budget of each searched GNN architecture in DFG-NAS is 200 epochs for three citation networks and 500 epochs for the ogbn-arxiv dataset. Specifically, we train them using Adam optimizer with a learning rate of 0.02 for Cora, 0.03 for Citeseer, 0.1 for PubMed, and 0.001 for ogbn-arxiv. The regularization factor is 5e-4 for all datasets. We apply dropout to all feature vectors with rates of 0.5 for Cora and Citeseer, and 0.3 for PubMed and ogbn-arxiv. Besides, the dropout between different GNN layers is 0.8 for Cora and Citeseer, and 0.5 for PubMed and ogbn-arxiv. At last, the hidden size of each GNN layer is 128 for Cora and ogbn-arxiv, 256 for Citeseer, and 512 for ogbn-arxiv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Compared Baselines.</head><p>For existing manual GNNs, we summarize the existing baselines according to the pipeline pattern of propagation (P) 1 https://github.com/tkipf/gcn/tree/master/gcn/data 2 https://ogb.stanford.edu/docs/nodeprop/#ogbn-arxiv and transformation (T) operations. Specifically, they can be classified into the following three types.</p><p>(1) Alternate P and T:</p><p>? Graph Convolutional Network(GCN) <ref type="bibr" target="#b14">(Kipf &amp; Welling, 2017)</ref>: GCN adopts an efficient layer-wise propagation rule that is based on a first-order approx-imation of spectral convolutions on graphs.</p><p>? Graph Attention Networks(GAT) <ref type="bibr" target="#b28">(Veli?kovi? et al., 2017)</ref>: GAT leverages masked self-attention layers to specify different weights to different nodes in a neighborhood, thus better represent graph information.</p><p>? GraphSAGE <ref type="bibr" target="#b6">(Hamilton et al., 2017)</ref>: GraphSAGE is an inductive framework that leverages node attribute information to efficiently generate representations on previously unseen data.</p><p>(2) P before T:</p><p>? Simplified GCN (SGC) <ref type="bibr" target="#b31">(Wu et al., 2019)</ref>: SGC simplifies GCN by removing nonlinearities and collapsing weight matrices between consecutive layers.</p><p>? Scalable Inception Graph Neural Networks (SIGN) <ref type="bibr" target="#b23">(Rossi et al., 2020)</ref>: SIGN is an efficient and scalable graph embedding method that sidesteps graph sampling in GCN and uses different local graph operators to support different tasks.</p><p>? Simple Spectral Graph Convolution (S 2 GC) <ref type="bibr" target="#b42">(Zhu &amp; Koniusz, 2021)</ref>: S 2 GC is a trade-off of low-and highpass filter bands which capture the global and local contexts of each node</p><p>(3) T before P</p><p>? APPNP <ref type="bibr" target="#b15">(Klicpera et al., 2019)</ref>: APPNP has the ability to use the relationship between graph convolution networks (GCN) and PageRank to derive improved node representations.</p><p>? Adaptive Propagation Graph Convolution Network (AP-GCN) <ref type="bibr" target="#b26">(Spinelli et al., 2021)</ref>: AP-GCN uses a halting unit to decide a receptive range of a given node.</p><p>? Deep Adaptive Graph Neural Network (DAGNN): DAGNN decouples the propagation and transformation  <ref type="figure">P,</ref><ref type="figure">P,</ref><ref type="figure">P,</ref><ref type="figure">P,</ref><ref type="figure">P,</ref><ref type="figure">P,</ref><ref type="figure">P,</ref><ref type="figure">P,</ref><ref type="figure">P,</ref><ref type="figure">P,</ref><ref type="figure">T,</ref><ref type="figure">T,</ref><ref type="figure">P,</ref><ref type="figure">P,</ref><ref type="figure">P,</ref><ref type="figure">T,</ref><ref type="figure">P,</ref><ref type="figure">T,</ref><ref type="figure">P,</ref><ref type="figure">P,</ref><ref type="figure">T,</ref><ref type="figure">T,</ref><ref type="figure">P,</ref><ref type="figure">P,</ref><ref type="figure">T,</ref><ref type="figure">T,</ref><ref type="figure">T,</ref><ref type="figure">P,</ref><ref type="figure">T,</ref><ref type="figure">P,</ref><ref type="figure">P,</ref><ref type="figure">P,</ref><ref type="figure">T,</ref><ref type="figure">T,</ref><ref type="figure">P,</ref><ref type="figure">T,</ref><ref type="figure">P,</ref><ref type="figure">T,</ref><ref type="figure">P,</ref><ref type="figure">P,</ref><ref type="figure">T,</ref><ref type="figure">P,</ref><ref type="figure">T,</ref><ref type="figure">T</ref>] operations, and it can adaptively incorporate information from large receptive fields.</p><p>For G-NAS methods, the compared baselines are as follows:</p><p>? Auto-GNN <ref type="bibr" target="#b41">(Zhou et al., 2019)</ref>: Auto-GNN is a reinforced conservative search strategy by adopting both RNNs and evolutionary algorithms in the controller.</p><p>? GraphNAS <ref type="bibr">(Gao et al., 2020a)</ref>: GraphNAS is a reinforcement learning-based method that uses an RNN controller to sample from the multiple architectures sequentially.</p><p>? GraphGym <ref type="bibr" target="#b35">(You et al., 2020)</ref>: GraphGym is a variant of random search on a general GNN search space that considers intra-layer design, inter-layer design, and training configurations.</p><p>Note that we implement GraphNAS 3 and GraphGym 4 according to their open-sourced version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Efficiency Analysis</head><p>The time complexity of the EA method is O(m), where m is the population size. In other words, the method is independent of the number of evaluations, and runs as fast as random search. While P or T can be added infinitely, previous methods for finite spaces can not be directly applied to our design space. And thus we perform the end-to-end comparisons in our paper. Figure <ref type="figure">5</ref> shows that DFG-NAS achieves similar performance to other methods with less search time. As our searching algorithm is relatively simple, we attribute this gain to the design of our search space, i.e., the well-designed architecture in our search space outperforms the precious ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Reproduction Instructions</head><p>The experiments are conducted on a machine with Intel(R) Xeon(R) Gold 5120 CPU @ 2.20GHz, and a single NVIDIA TITAN RTX GPU with 24GB GPU memory. The operating system of the machine is Ubuntu 16.04. For software versions, we use Python 3.6, Pytorch 1.7.1, and CUDA 10.1. Our code is available in the anonymized repository https://github.com/PKU-DAIR/DFG-NAS.</p><p>3 https://github.com/GraphNAS/GraphNAS 4 https://github.com/snap-stanford/GraphGym A.6. The Best Architecture Searched by DFG-NAS</p><p>The best GNN architecture searched by DFG-NAS on different graph datasets is summarized in Table <ref type="table">7</ref>. Note that each GNN architecture will begin with a T operation for dimension reduction and end with a T operation for getting the softmax outputs.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A generative model for electron paths</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bradshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H S</forename><surname>Segler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hern?ndez-Lobato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rethinking graph neural architecture search from message-passing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6657" to="6666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Retrosynthesis prediction with conditional graph logic network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="8870" to="8880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sign: Scalable inception graph neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2020 Workshop on Graph Representation Learning and Beyond</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Graph neural architecture search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Bessiere</surname></persName>
		</editor>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="1403" to="1409" />
		</imprint>
	</monogr>
	<note>ijcai.org, 2020a</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph neural architecture search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1403" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simplifying and powering graph convolution network for recommendation</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Lightgcn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">July 25-30, 2020. 2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
	<note>SIGIR 2020, Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Search to aggregate neighborhood for graph neural network</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Quanming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weiwei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 37th International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="552" to="563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Knowledge-aware coupled graph neural network for social recommendation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><surname>Zoomer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.12596</idno>
		<title level="m">Boosting retrieval on web-scale graphs by regions of interest</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019. OpenReview.net, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00421</idno>
		<title level="m">A generalized black-box optimization service</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">One-shot graph neural architecture search with dynamic search space</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="8510" to="8517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards deeper graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="338" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Degnn: Improving graph neural networks with graph decomposition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>G?rel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1223" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Lasagne: A multi-layer graph convolutional network framework via node-aware deep architecture</title>
		<author>
			<persName><forename type="first">X</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11198</idno>
		<title level="m">Sign: Scalable inception graph neural networks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning feature matching with graph neural networks</title>
		<author>
			<persName><forename type="first">P.-E</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName><surname>Superglue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4938" to="4947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with directed graph neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7912" to="7921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adaptive propagation graph convolutional network</title>
		<author>
			<persName><forename type="first">I</forename><surname>Spinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Scardapane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Uncini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4755" to="4760" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graph-based deep learning in natural language processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yadati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoDS-COMAD 2020: 7th ACM IKDD CoDS and 25th COMAD</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Roy</surname></persName>
		</editor>
		<meeting><address><addrLine>Hyderabad India</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">January 5-7, 2020. 2020</date>
			<biblScope unit="page" from="371" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05-03">April 30 -May 3, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multihop attention graph neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</editor>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-08-27">19-27 August 2021. 2021</date>
			<biblScope unit="page" from="3089" to="3096" />
		</imprint>
	</monogr>
	<note>ijcai.org</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Graph neural networks for natural language processing: A survey</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Long</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.06090</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph neural networks in recommender systems: a survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5449" to="5458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Design space for graph neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Prasanna</surname></persName>
		</author>
		<author>
			<persName><surname>Graphsaint</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Reliable data distillation on graph convolutional network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ruas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2020 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1399" to="1414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.00955</idno>
		<title level="m">Evaluating deep graph neural networks</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Node dependent local smoothing for scalable graph learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Graph attention multi-layer perceptron</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10097</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Auto-gnn: Neural architecture search of graph neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03184</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Simple spectral graph convolution</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
