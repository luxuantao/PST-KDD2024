<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ACE: A Fast Multiscale Eigenvectors Computation for Drawing Huge Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
							<email>yehuda@wisdom.weizmann.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">The Weizmann Institute of Science</orgName>
								<address>
									<postCode>76100</postCode>
									<settlement>Rehovot</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liran</forename><surname>Carmel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Weizmann Institute of Science</orgName>
								<address>
									<postCode>76100</postCode>
									<settlement>Rehovot</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Harel</surname></persName>
							<email>harel@wisdom.weizmann.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">The Weizmann Institute of Science</orgName>
								<address>
									<postCode>76100</postCode>
									<settlement>Rehovot</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ACE: A Fast Multiscale Eigenvectors Computation for Drawing Huge Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0B36FD56276D3C3B1128EF610785621C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>algebraic multigrid</term>
					<term>graph drawing</term>
					<term>Hall</term>
					<term>generalized eigenvalue problem</term>
					<term>Fiedler vector</term>
					<term>force directed layout</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an extremely fast graph drawing algorithm for very large graphs, which we term ACE (for Algebraic multigrid Computation of Eigenvectors). ACE finds an optimal drawing by minimizing a quadratic energy function due to Hall, using a novel algebraic multigrid technique. The algorithm exhibits an improvement of something like two orders of magnitude over the fastest algorithms we are aware of; it draws graphs of a million nodes in less than a minute. Moreover, the algorithm can deal with more general entities, such as graphs with masses and negative weights (to be defined in the text), and it appears to be applicable outside of graph drawing too.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A graph is a structure G(V, E) representing a binary relation E over a set of entities V . In a very general sense, we expect the drawing of a graph to visually capture its inherent structure. Interpreting this vague desire into strict well-defined criterion for the purpose of assessing the quality of a drawing can be done in various ways, leading to many approaches to graph drawing <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>One of the most popular approaches is to define an energy function (or a force model), whose minimization determines the optimal drawing. Several such functions have been proposed, e.g. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b6">7]</ref>, each is characterized by a different set of properties. In this paper we concentrate on one particular form of an energy function, characterized by being simple and smooth, thus enabling rigorous analytical analysis and a straightforward implementation. This particular function was first applied to graph drawing by Hall <ref type="bibr" target="#b9">[10]</ref>, and we therefore term it Hall's energy. It was further studied in <ref type="bibr" target="#b21">[22]</ref>, where its results are shown to satisfy several desired aesthetic properties.</p><p>Most graph drawing methods suffer from lengthy computation times when applied to really large graphs. Several publications in the graph drawing conference of 2000 <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b18">19]</ref> present fast graph drawing algorithms, but even the fastest of them ( <ref type="bibr" target="#b23">[24]</ref>) requires about 10 minutes for a typical 10 5 -node graph. As far as we know, no faster algorithm has been presented since. In fact, a naive implementation of the minimization of Hall's energy would also encounter real difficulties on a 10 5 node graph.</p><p>In this paper we suggest an extremely fast algebraic multigrid (AMG) implementation for minimizing Hall's energy. It results in typical computation times of less than 1 minute for 10 6 -node graphs. Actually, we suggest not only an implementation, but rather a generalization of the original method, allowing for the drawing of a much larger family of graphs. Furthermore, the problem that we will be solving is of more fundamental nature, and our algorithm can be used in areas outside of graph drawing, such as clustering, partitioning, ordering, and image segmentation.</p><p>In section 2 we describe the original method proposed by <ref type="bibr">Hall.</ref> In section 3 we develop ACE, and investigate its features. Section 4 presents results of many tests that we have been running, demonstrating the capabilities of ACE. A discussion follows in section 5. Appendix A summarizes some results of linear algebra that we will be needing, and appendix B addresses related issues of graph-connectivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Eigenprojection Method</head><p>In this section we briefly introduce the original method of energy minimization derived by Hall <ref type="bibr" target="#b9">[10]</ref>, but put his results in somewhat different notations, to facilitate later derivations. We will call his method as the eigenprojection method, for reasons that will become clear shortly.</p><p>Let G(V, E) be a graph, with V a set of n nodes, and E a set of weighted edges, and with w ij ≥ 0 being the weight of the edge connecting nodes i and j. We assume that the weights satisfy three properties:</p><p>• w ij ≥ 0 ∀i, j.</p><p>• w ii = 0 ∀i (no self-edges).</p><p>• w ij = 0 for i, j non-adjacent pair.</p><p>Hall suggested to draw such a graph in one-dimension by minimizing the following energy: Definition 2.1 (Hall's Energy) Let x k be a one-dimensional coordinate associated with the k'th node. The energy of the one-dimensional drawing is defined as</p><formula xml:id="formula_0">E = 1 2 n i,j=1 w ij (x i -x j ) 2 .</formula><p>Clearly, E ≥ 0 for any drawing and any graph. Intuitively, the larger is the weight of the edge connecting the pair i, j , the closer should be x i and x j to keep the contribution to the energy small. Given this function, the energy minimization strategy suggests that the coordinates will be determined from:</p><formula xml:id="formula_1">min x1,...,xn 1 2 n i,j=1 w ij (x i -x j ) 2 .</formula><p>(</p><formula xml:id="formula_2">)<label>1</label></formula><p>This minimization problem can be written in a more compact form using the notion of Laplacian:</p><p>Definition 2.2 (Laplacian) Let G(V, E) be a graph. The Laplacian of the graph is the symmetric n × n matrix</p><formula xml:id="formula_3">L ij = n k=1 w ik i = j -w ij i = j i, j = 1, . . . , n.</formula><p>The energy can now be written in matrix notation.</p><p>Lemma 2.1 Denoting x = (x 1 , x 2 , . . . , x n ) T , Hall's energy is:</p><formula xml:id="formula_4">E = x T Lx = 1 2 n i,j=1</formula><p>w ij (x i -x j ) 2 .</p><p>Proof Expanding the sum:</p><formula xml:id="formula_5">E = 1 2 n i,j=1 w ij (x i -x j ) 2 = 1 2 n i,j=1 w ij x 2 i + 1 2 n i,j=1 w ij x 2 j - n i,j=1</formula><p>w ij x i x j .</p><p>Due to the symmetry of w ij (i.e., that w ij = w ji ) the first two terms are identical, thus</p><formula xml:id="formula_6">E = n i,j=1 w ij x 2 i - n i,j=1 w ij x i x j = n i=1   n j=1 w ij   x 2 i - n i,j=1</formula><p>w ij x i x j .</p><p>In the rightmost term, we can use the fact that ∀i w ii = 0 so that</p><formula xml:id="formula_7">E = n i=1   n j=1 w ij   x 2 i - n i=1 n j=1,j =i w ij x i x j = = n i=1 L ii x 2 i + n i=1 n j=1,j =i L ij x i x j = n i,j=1</formula><p>L ij x i x j = x T Lx.</p><p>The minimization problem <ref type="bibr" target="#b0">(1)</ref> can thus be written in the alternative form</p><formula xml:id="formula_8">min x x T Lx.<label>(2)</label></formula><p>The Laplacian will be proved to be a key feature of ACE, and in the next corollary we prove one of its more important properties: Corollary 2. <ref type="bibr" target="#b0">1</ref> The Laplacian is symmetric positive semi-definite <ref type="foot" target="#foot_0">1</ref> .</p><p>Proof By Definition 2.2 the Laplacian is symmetric. The positive semi-definiteness follows from Lemma 2.1, by the fact that E ≥ 0.</p><p>Formulating the graph drawing problem in either form (1) or ( <ref type="formula" target="#formula_8">2</ref>) is not enough. The reason is that E is minimized (and takes the value 0) by solutions of the form x 1 = x 2 = . . . = x n , as is clearly seen by looking at <ref type="bibr" target="#b0">(1)</ref>. To facilitate notations let us denote</p><formula xml:id="formula_9">1 n =      1 1 . . . 1              </formula><p>n times, so that x 1 = x 2 = . . . = x n = c is replaced by c • 1 n , with c any constant. These solutions are undesirable from the graph drawing point of view since they place all the nodes at the same point. Unfortunately, we claim that for connected graphs vectors of the form c • 1 n are the only minimizers of E. To see this we differentiate E = x T Lx with respect to x to get (see Theorem A.8) dE dx = 2Lx.</p><p>Equating this to zero gives Lx = 0. Thus, a minimizer of E is an eigenvector of L with zero eigenvalue. As is proved in appendix B (Lemma B.1), for connected graphs the only eigenvector of L with zero eigenvalue is c • 1 n . The way Hall skirted these undesirable solutions was by posing an overall scaling constraint in the form x T x = 1, thus replacing <ref type="bibr" target="#b1">(2)</ref> with</p><formula xml:id="formula_10">min x x T Lx (3) given x T x = 1.</formula><p>The choice of the value 1 in the constraint is arbitrary. We could equally choose a constraint of the form x T x = c, with c any positive scalar. The choice of c merely determines the length of x and the scale of E. For, if x 0 is a minimizer of (3) with energy E 0 = x T 0 Lx 0 , then √ cx 0 (with energy cE 0 ) will be a minimizer of the same problem but with the constraint</p><p>x T x = c. The geometrical meaning of this constraint is to force the solution to lie on the unit hyper-sphere in R n .</p><p>Formulating the problem in the form (3) still cannot be taken as the final form, since the undesirable solution (1/ √ n) • 1 n is clearly a minimizer of (3). To see how we can avoid this solution, we need to further process problem (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 2.2</head><p>The extrema of ( <ref type="formula">3</ref>) are obtained for those x that are the eigenvectors of L, and the value of E at these extrema is given by the corresponding eigenvalues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof</head><p>We solve (3) by means of Lagrange multipliers. The function to be minimized is x T Lx -λ(x T x -1) with λ the Lagrange multiplier. Differentiating it with respect to x (see Theorems A.8 and A.9) and equating to zero gives</p><formula xml:id="formula_11">Lx = λx,</formula><p>which is nothing but the eigenvalue problem of L. Moreover, let x be a normalized eigenvector (i.e., of length 1) of L with eigenvalue λ. Then</p><formula xml:id="formula_12">E(x) = x T Lx = λx T x = λ.</formula><p>The eigenvalues of L are all real and non-negative (see Theorem A.3). Also, one can always find a set of n real orthonormal eigenvectors (see Theorem A.2). Moreover, L has at least one zero eigenvalue corresponding to the "undesirable" eigenvector <ref type="foot" target="#foot_1">2</ref> Throughout the paper we will use the convention 0 = λ 1 ≤ λ 2 ≤ . . . ≤ λ n for the eigenvalues of L, and denote the corresponding real orthonormal eigenvectors by</p><formula xml:id="formula_13">(1/ √ n) • 1 n (since L • 1 n = 0).</formula><formula xml:id="formula_14">v 1 = (1/ √ n) • 1 n , v 2 , . . . , v n .</formula><p>To avoid v 1 , Hall's strategy is to take the next eigenvector, v 2 , as the vector of coordinates. As a matter of fact, v 2 is a vector of fundamental importance to many fields besides graph drawing, see, e.g., <ref type="bibr" target="#b16">[17]</ref>, so fundamental to be privileged in having a unique name -the Fiedler vector. Relying on the orthogonality of the eigenvectors, v 2 is the minimizer of (3) in the subspace orthogonal to v 1 . Therefore, Hall's strategy can be formulated as:</p><formula xml:id="formula_15">min x x T Lx (4) given x T x = 1 in the subspace x T • 1 n = 0.</formula><p>Appropriately, we henceforth dub it the eigenprojection problem. If it is desired to plot the graph in more dimensions, subsequent eigenvectors may be taken. Thus, a 2-D drawing is obtained by taking the x-coordinates of the nodes to be given by v 2 , and the y-coordinates to be given by v 3 .</p><p>Calculating the first few eigenvectors of L is a difficult task that presents a real problem for standard algorithms when n becomes around 10 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ACE Algorithm</head><p>In this section we describe our algorithm, ACE, for solving the eigenprojection problem (4). In fact, as we shall see, ACE has a broader spectrum of applicability, being capable of solving a more general problem, the generalized eigenprojection problem.</p><p>ACE employs a technique common to the so-called algebraic multigrid (AMG) algorithms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23]</ref>. These algorithms progressively express an originally high-dimensional problem in lower and lower dimensions, using a process called coarsening. On the coarsest scale the problem is solved exactly, and then starts a refinement process, during which the solution is progressively projected back into higher and higher dimensions, updated appropriately at each scale, until the original problem is reproduced. An AMG algorithm should be specifically designed for a given problem, so that the coarsening process keeps the essence of the problem unchanged, while the refinement process needs only fast updates to obtain an accurate solution at each scale.</p><p>As far as we know, this is the first time that a formal and rigorous AMG algorithm is developed for graph drawing. Several authors, including works from our own group, have designed "heuristic" multiscale/multilevel graph drawing algorithms, i.e., algorithms in which there is no formal relationship between the problems on the different scales; see <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b7">8]</ref>. Another important heuristic multiscale algorithm, from which we draw some of our inspiration, was developed by Barnard and Simon <ref type="bibr" target="#b0">[1]</ref>. It computes the Fiedler vector for use in the partitioning problem</p><p>In subsection 3.1 we derive the generalized eigenprojection problem. Our coarsening and refinement techniques are depicted in subsection 3.2 and 3.3 respectively. Subsection 3.4 is devoted to the interpolation matrix -a key magnitude of the algorithm to be introduced later on. A schematic summary of the algorithm is given in subsection 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Generalized Eigenprojection Problem</head><p>In contrast to what AMG algorithms strive for, it will become apparent in Subsection 3.2 that the eigenprojection problem, (4), is not preserved during coarsening. Hence, what we do is to define a different problem -the generalized eigenprojection problem -which is preserved during coarsening, and which contains the eigenprojection problem as a special case. ACE is designed to solve generalized eigenprojection problems, and thereof also eigenprojection problems.</p><p>Hence, we dedicate this first part to describe this generalized eigenprojection problem. One possible course of action would be to formulate it in a purely formal fashion, expressing it as a problem in linear algebra. However, we take a different course of action and show how the generalized problem is tightly connected to graph theory, and emerges naturally from trying to draw a more general entity, that we shall be calling a PSD graph.</p><formula xml:id="formula_16">Definition 3.1 (Positive Semi-Definite (PSD) Graph) A PSD graph is a structure G(V, E, M) in which: 1. V a set of n nodes.</formula><p>2. E a set of weighted edges, w ij being the weight of the edge connecting nodes i and j.</p><p>These weights should obey the following rules:</p><p>• The Laplacian of G (see Definition 2.2) is positive semi-definite.</p><p>• w ii = 0 ∀i (no self-edges).</p><p>• w ij = 0 for i, j non-adjacent pair.</p><p>3. M a set of n strictly positive masses, m i being the mass of the i'th node.</p><p>A PSD graph differs from classical graphs in two aspects. First, it involves an additional set of numbers -the masses. Second, we have fewer restrictions on the weights -instead of requiring them to be all positive (see section 2) we permit negative weights, as long as the Laplacian remains positive semi-definite. For the purpose of graph drawing positive weights are interpreted as measuring the similarity between pairs of nodes -the larger is w ij the more similar are nodes i and j. In analogy we might interpret negative weights as measuring dissimilarity -the larger is -w ij the more dissimilar are nodes i and j. Consequently, in the drawing we would expect nodes connected by large positive weights to be close to each other, and those connected by large negative weights to be distantly located. A zero weight w ij thus expresses indifference as to the relative locations of nodes i and j. With this interpretation in mind, minimization of Hall's energy</p><formula xml:id="formula_17">E = x T Lx = 1 2 n i,j=1 w ij (x i -x j ) 2</formula><p>still looks like a good drawing strategy. But why do we need to assure positive semidefiniteness of the Laplacian? Because otherwise we can find x such that the energy is negative, x T Lx &lt; 0. But then, stretching the coordinates by a constant factor, say by c &gt; 0, we will further decrease the energy, c 2 x T Lx &lt; x T Lx &lt; 0. Consequently, as x will be stretched farther and farther to infinity, the energy will be decrease to minus infinity. Physically, such graphs are not interesting, and we do not expect to find them in real problems. Therefore, we may safely rule them out. We are aware of the fact that negative weights might seem unnatural to some of the readers, and we hope that this work will demonstrate that graphs with negative weights are as natural and legitimate as those with only positive weights. Since we are going to frequently address the issue of negative versus positive weights, we find it convenient to give a special name for the special case of a PSD graph with no negative weights: Definition 3.2 (All-Positive (AP) graph) A PSD graph G(V, E, M) is called all-positive if all its weights are non-negative.</p><p>Due to the positive semi-definiteness of the Laplacian, negative weights will always be inferior with respect to positive ones. This is an important observation that we prove in the following definition and claim: Definition 3.3 (Degree) The degree of the i'th node, denoted d i , is defined as the sum of the weights of all the edges connected to it,</p><formula xml:id="formula_18">d i = k w ik .</formula><p>Claim 3.1 Let G(V, E, M) be a PSD graph. The degree of each of its nodes is non-negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof</head><p>The diagonal element L ii of the Laplacian is nothing but the degree d i (see Definition 2.2). Therefore, our claim is just a re-statement of a well known property of positive semidefinite matrices, saying that all the diagonal elements are non-negative.</p><p>To prove it let us assume that one node, say the first, has a negative degree, d 1 &lt; 0. Then, if we take x = (1, 0, 0, . . . , 0) T , we get E = 1 2 i,j w ij (x i -x j ) 2 = j w 1j = d 1 &lt; 0, and so L cannot be positive semi-definite.</p><p>So far, we saw that the function to be minimized, Hall's energy, did not change if we deal with PSD graphs. The deviation from Hall's formulation starts only with the scaling constraint. Instead of requiring that x be normalized, i.e., that x T x = 1, we now require x to be M -normalized, i.e., x T M x = 1 (see Definition A.6), where M is the diagonal mass matrix defined as follows:</p><formula xml:id="formula_19">Definition 3.4 (Mass Matrix) Let G(V, E, M) be an n-node PSD graph. The n × n diago- nal matrix M , defined by M ij = 0 i = j m i i = j, is called the mass matrix of G.</formula><p>Problem (3) is accordingly replaced by</p><formula xml:id="formula_20">min x x T Lx<label>(5)</label></formula><p>given x T M x = 1.</p><p>In the next section we will see that this form of the constraint is simply a natural outcome of the coarsening process, and for all practical reasons it suffices just to take it as such. Nonetheless, we try to give here some intuition regarding the question why ( <ref type="formula" target="#formula_20">5</ref>) is more appropriate for drawing a PSD graph than (3): The constraint x T x = 1 forces x to lie on an n-dimensional unit-radius hyper-sphere. The constraint x T M x = 1 though, forces x to lie on an n-dimensional ellipsoid centered at the origin of coordinates, whose principal axes are parallel to the coordinates' axes. Additionally, the length of the i'th principal axis is given by 2m</p><formula xml:id="formula_21">-1 2 i</formula><p>. Therefore, as one can see in Figure <ref type="figure">1</ref>, the heavier is the node (i.e., its mass is large) the shorter is its corresponding principal axis. So, while in the hyper-sphere case a certain value in the range [-1, 1] has equal probability to be occupied by any of the x i 's, this is not the case for an ellipsoid. There, heavier nodes will be associated with smaller (in absolute value) coordinate values, thus will be concentrated near the origin of coordinates, while lighter nodes have good chances of being far from the origin of coordinates. Aesthetically, this is convincing -as in the atomic system, heavier particles are at the center, while lighter ones surround them from a distance. Further than that, in some applications one desires that certain nodes will be centered, see, e.g., <ref type="bibr" target="#b5">[6]</ref> who was interested in pushing high-degree nodes to the center. This can be achieved with ACE by associating high masses with these nodes.</p><p>Since it is obvious from Definition 3.1 that both the Laplacian L and the mass matrix M completely define a PSD graph<ref type="foot" target="#foot_2">3</ref> , we will freely use both G(V, E, M) and G(L, M ) to symbolize a PSD graph. Figure <ref type="figure">1</ref>: A two-dimensional example of the relation between masses and the shape of the ellipsoids -the larger is the mass, the smaller is the principal axis in its direction.</p><p>Lemma 3.1 The extrema of ( <ref type="formula" target="#formula_20">5</ref>) are obtained for those x that are the solutions of the generalized eigenvalue problem Lx = µM x, and the value of E at these extrema is given by the corresponding generalized eigenvalues µ.</p><p>Proof Again, we use the Lagrange multipliers method. The function to be minimized is x T Lx -µ(x T M x -1) with µ the Lagrange multiplier. Since L and M are both symmetric, differentiating this function with respect to x (see Theorem A.8) and equating to zero gives</p><formula xml:id="formula_22">Lx = µM x,</formula><p>which is the generalized eigenvalue problem of L and M . Moreover, since</p><formula xml:id="formula_23">x is M -normalized then E(x) = x T Lx = µx T M x = µ.</formula><p>Similarly to the eigenprojection problem, also here we can say many things on the generalized eigenvalues and eigenvectors of L and M : the generalized eigenvalues are all real and non-negative (see Theorem A.6), and one can always find a set of n real M -orthonormal generalized eigenvectors (see Theorem A.7). Moreover, we are guaranteed to have at least one zero generalized eigenvalue, which corresponds to the generalized eigenvector α • 1 n where</p><formula xml:id="formula_24">α = 1 √ m 1 + m 2 + . . . + m n = 1 √</formula><p>TrM .</p><p>This last statement can be proved by simply substituting α • 1 n in <ref type="bibr" target="#b4">(5)</ref>. As λ 1 , . . . , λ n denote the eigenvalues of L and v 1 , . . . , v n denote their corresponding eigenvectors, we adopt a similar notation here, using 0 = µ 1 ≤ µ 2 ≤ . . . ≤ µ n to denote the generalized eigenvalues of L and M , and u 1 = α • 1 n , u 2 , . . . , u n to denote their corresponding real M -orthonormal generalized eigenvectors. Again, we are faced with having to avoid the undesirable solution u 1 . Pushing further the analogy to the original analysis of Hall, we choose to take u 2 as the vector of coordinates. Due to the M -orthogonality of the generalized eigenvectors, u 2 is the minimizer of ( <ref type="formula" target="#formula_20">5</ref>) in the subspace that is M -orthogonal to u 1 (i.e., the subspace of vectors x such that i m i x i = 0). We thus end up with the final form of the generalized eigenprojection problem:</p><formula xml:id="formula_25">min x x T Lx<label>(6)</label></formula><p>given</p><formula xml:id="formula_26">x T M x = 1 in the subspace x T M • 1 n = 0.</formula><p>If more dimensions are required in the drawing, we may take subsequent generalized eigenvectors, e.g., u 2 and u 3 as the x-coordinates and y-coordinates of the nodes in a two-dimensional plot.</p><p>Clearly, the eigenprojection problem is just a special case of the generalized eigenprojection problem, given the graph is an AP graph, and given that all the masses are 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The coarsening Processes</head><p>During the coarsening, we iteratively represent an initial PSD graph as smaller and smaller PSD graphs. Let G be such a PSD graph containing n nodes. A single coarsening step would be to replace G with another PSD graph G containing only m &lt; n nodes (typically, m ≈ 1 2 n). Of course, the structure of G should be strongly linked to that of G, such that both describe approximately the same entity, but on different scales. One can think of many clever ways to erect G given the structure of G, but we postpone discussion on this wide topic until subsection 3.4. Instead, we establish a general framework for the coarsening, into which later specific methods would be easily implanted.</p><p>A key concept we will use is that of an interpolation matrix, which is the tool that links G and G . This is an</p><formula xml:id="formula_27">n × m matrix A that interpolates m-dimensional vectors y ∈ R m into n-dimensional ones x = Ay (x ∈ R n ).</formula><p>If y is a solution of the generalized eigenprojection problem of G , a good interpolation matrix is one for which x = Ay is close enough to a solution of the generalized eigenprojection problem of G. Such interpolation matrices can be designed in various ways, to be discussed, as already mentioned, in subsection 3.4. In the meantime we just state the general definition of A:</p><formula xml:id="formula_28">Definition 3.5 (Interpolation Matrix) An interpolation matrix A is an n × m matrix (n &gt; m) such that 1. All elements are non-negative, A ij ≥ 0 ∀i, j.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The sum of each row is one,</head><formula xml:id="formula_29">m j=1 A ij = 1 ∀i. 3. A has a full column rank, rank(A) = m.</formula><p>Properties 1 and 2 follow naturally by interpreting the i'th row of A as a series of weights that shows how to determine coordinate x i given all the y's, namely, x i = m j=1 A ij y j . Property 3 prevents the possibility of two distinct m-dimensional vectors being interpolated to the same n-dimensional vectors. Since 1 n = A • 1 m (from property 2), we are thus assured that no "good" drawing of G will be interpolated to the undesirable drawing of G, α • 1 n . In appendix B we show that the fact that A is of full column rank also has a role in issues of graph connectivity.</p><p>The interpolation matrix defines a coarsening step completely: Given a fine n-node PSD graph G(L, M ) and an n × m interpolation matrix A, we can fully obtain the coarse m-node PSD graph G (L , M ). Putting differently, given L, M , and A, we calculate the Laplacian L , and the mass matrix M . In the rest of this subsection we show how we do it: In 3.2.1 we explain how M is found, and in 3.2.2 we explain how L is found. Finally, in 3.2.3 we discuss the relations between the generalized eigenprojection problem of G and that of G.</p><p>But before we start, let us introduce a simple example upon which we demonstrate the subsequent results. Let the fine graph G be the 5-node (1,2,3,4,5) PSD graph (actually, AP graph) shown in Figure <ref type="figure" target="#fig_2">2</ref>. Due to its structure, this graph will be henceforth dubbed the Eiffel tower graph. Let all the masses of G be 1, so that its Laplacian and mass matrix are given by:</p><formula xml:id="formula_30">L =       9 -5 0 -4 0 -5 17 -2 -7 -3 0 -2 4 -2 0 -4 -7 -2 19 -6 0 -3 0 -6 9       M =       1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1       .<label>(7)</label></formula><p>Let the interpolation matrix be the 5 × 3 matrix</p><formula xml:id="formula_31">A =      </formula><p>0.55 0 0.45 0.52 0 0.48 0.3 0.4 0.3 0.45 0 0.55 0.4 0 0.6</p><formula xml:id="formula_32">      . (<label>8</label></formula><formula xml:id="formula_33">)</formula><p>We would like to remind the reader that for the time being we assume that A is given in advance. Ways to build A given G(L, M ) are discussed only in subsection 3.4. Anyhow, we can easily be convinced that this A is reasonable, since it fixes the first and third nodes of the resulting 3-node PSD graph G (1' and 3') as the coarse version of the "tower base" quadruplet (1,2,4,5), and the second node 2' as the coarse version of the "tower top".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Calculating the mass matrix of G</head><p>The coarse masses are derived by:  In 3.2.3 we will understand why this is a natural definition. In the meantime, let us just say that this law is nothing but a statement of mass conservation if we interpret the nodes of G as physical masses formed by breaking and re-fusing (as dictated by the interpolation matrix) pieces of the physical masses from which G is built up. Applying the mass law to the Eiffel tower example we get:</p><formula xml:id="formula_34">M =   2.22 0 0 0 0.4 0 0 0 2.38   .<label>(9)</label></formula><p>Notice that the total mass is indeed conserved, TrM = TrM = 5. Next, we prove two useful equalities (which are basically just two variations on the same equality) that stem from the mass law, and will serve us later: Lemma 3.2 Let A be an n × m interpolation matrix, M be the n × n mass matrix of G, and M the corresponding m × m mass matrix of G . Then</p><formula xml:id="formula_35">M • 1 m = A T M • 1 n . Proof M • 1 n is just (m 1 , m 2 , . . . , m n ) T , and M • 1 m is just (m 1 , m 2 , . . . , m m ) T . Thus, (M • 1 m ) = A T (M • 1 n</formula><p>) is merely a matrix form of the mass law.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 3.3 Let</head><p>A be an n × m interpolation matrix, M be the n × n mass matrix of G, and M the corresponding m × m mass matrix of G . Then, the sum of the i'th row (or column) of</p><formula xml:id="formula_36">A T M A is just m i , or M • 1 m = A T M A • 1 m .</formula><p>Proof The rows of A all sum up to one, thus</p><formula xml:id="formula_37">A • 1 m = 1 n . Replacing 1 n by A • 1 m in Lemma 3.</formula><p>2 completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Calculating the Laplacian of G</head><p>The Hall energy of G(L, M ) is E = x T Lx, where x ∈ R n . Substituting x = Ay, we can express this energy in terms of the m-dimensional vector y, E = y T A T LAy. It would then be quite natural to define the Laplacian of G to be</p><formula xml:id="formula_38">L = A T LA,<label>(10)</label></formula><p>so that the Hall energy of G (L , M ) is E = y T L y. The next claim shows that this definition is consistent with our previous definitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Claim 3.2 L is Laplacian of a PSD graph.</head><p>Proof We have to show that L is symmetric positive semi-definite, and that the sum of each of its rows is 1. The symmetry and positive semi-definiteness are imposed only by the fact that L is such:</p><formula xml:id="formula_39">a. Symmetry: Notice that L T = (A T LA) T = A T L T A = A T LA = L .</formula><p>b. Positive semi-definiteness: Given any vector y ∈ R m , let us denote by z ∈ R n the vector Ay. Then, y T L y = y T A T LAy = z T Lz, which is always non-negative due to the positive semi-definiteness of L.</p><p>Proving that the rows of L sum up to 1 requires the use of the special features of A. The</p><formula xml:id="formula_40">statement j A ij = 1 is equivalent of writing A • 1 m = 1 n . Then L • 1 m = A T LA • 1 m = A T L • 1 n = 0,</formula><p>where the last step is due to the fact that L • 1 n = 0.</p><p>Having found both L and M , G is completely defined and the coarsening step comes to its end. Our definition of coarsening clarifies why we needed the concept of PSD graphs; negative weights might occur in G even if G is an AP graph. Such a case happens in our example, since we get</p><formula xml:id="formula_41">L = A T LA =  </formula><p>0.2788 -0.296 0.0172 -0.296 0.64 -0.344 0.0172 -0.344 0.3268</p><formula xml:id="formula_42">  .</formula><p>This Laplacian, together with the mass matrix <ref type="bibr" target="#b8">(9)</ref>, defines the 3-node PSD graph G shown in Figure <ref type="figure" target="#fig_3">3</ref>, with a negative weight connecting nodes 1' and 3'.</p><p>The reason for the emergence of negative weights is delicate. Our definition of coarse Laplacian, L = A T LA, has a tendency to diminish the value of a weight which connects two highly correlated coarse nodes, i.e., two coarse nodes that during the interpolation often contribute simultaneously to the same fine nodes. To see this, let us examine explicitly the expression for the coarse weights. From <ref type="bibr" target="#b9">(10)</ref> </p><formula xml:id="formula_43">L ij = p,q A pi A qj L pq .</formula><p>Considering the definition of L (Definition 2.2), and separating the cases q = p and q = p, we get</p><formula xml:id="formula_44">L ij = - p,q A pi A qj w pq + p A pi A pj q w pq .</formula><p>The coarse weights are obtained from the off-diagonal elements of L . Using the notion of degree, see Definition 3.3, we get The first term collects the contributions of all pairs p, q of fine nodes such that one is interpolated from i and the other from j. For an AP graph this term is always positive. The second term collects the contributions of those fine nodes that both i and j contribute to their interpolation. But now, for an AP graph this term is always negative, thus having the tendency to decrease w ij . In the Eiffel tower example, nodes 1' and 3' are highly correlated, as can be seen by looking at the first and the third columns of A in (8), thus the negative weight of the edge connecting them.</p><formula xml:id="formula_45">w ij = -L ij = p,q A pi A qj w pq - p d p A pi A pj , i = j.<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">The eigenprojection solutions of G and G</head><p>Given the coarse graph G we have defined, its corresponding generalized eigenprojection problem is</p><formula xml:id="formula_46">min y y T L y<label>(12)</label></formula><p>given</p><formula xml:id="formula_47">y T M y = 1 in the subspace y T M • 1 m = 0,</formula><p>to be hereinafter dubbed the coarse generalized eigenprojection problem. The issue that needs to be addressed now is the relationship between this problem and the original generalized eigenprojection problem (6) of the fine graph G. To bring the two problems to the same dimension we substitute x = Ay in (6), obtaining the form</p><formula xml:id="formula_48">min y y T A T LAy (13) given y T A T M Ay = 1 in the subspace y T A T M • 1 n = 0,</formula><p>to be referred hereinafter as the restricted generalized eigenprojection problem. In general, the coarse generalized eigenprojection problem <ref type="bibr" target="#b11">(12)</ref> and the restricted generalized eigenprojection problem ( <ref type="formula">13</ref>) are different problems. Had these two problems been identical, then the multiscale method would be optimal in the sense that the structure of the problem is preserved during coarsening. As we will show in subsection 3.4, one can adopt strategies of choosing A such that this would indeed be the case. Yet other strategies, that may yield more powerful interpolation matrices, do not posses this property. Nevertheless, we will show that in these cases the coarse generalized eigenprojection problem ( <ref type="formula" target="#formula_46">12</ref>) is a reasonable approximation of the restricted generalized eigenprojection problem <ref type="bibr" target="#b12">(13)</ref>, and consequently the solutions of both problems share much resemblance.</p><p>To start with, let us compare in more details the two problems. Due to the equality L = A T LA, the function to be minimized is the same in both forms. Moreover, since M •</p><formula xml:id="formula_49">1 m = A T M • 1 n (see Lemma 3.</formula><p>2), we are looking for solutions to both of these in the same subspace (i.e., in both we avoid the same undesirable solution). The only difference between them is in the scaling constraint since in general M = A T M A. One can force equality by adopting certain strategies for calculating A. Yet other strategies, that may yield more powerful interpolation matrices, do not, indeed, obey an equality. In fact, we can formulate a simple criterion to decide, given A, whether M = A T M A or not:</p><formula xml:id="formula_50">Claim 3.3 Let G(L, M</formula><p>) be an n-node PSD graph, and let A be an n × m interpolation matrix. The coarse generalized eigenprojection problem <ref type="bibr" target="#b11">(12)</ref> identifies with the restricted generalized eigenprojection problem <ref type="bibr" target="#b12">(13)</ref> </p><formula xml:id="formula_51">if A T M A is diagonal 4 . Proof The two problems are identical if M = A T M A. By Lemma 3.3, the sum of the i'th row (or column) of A T M A is just m i , so obviously M = A T M A if A T M A is diagonal.</formula><p>In the case that M = A T M A we would rather solve the coarse generalized eigenprojection problem than the restricted generalized eigenprojection problem, i.e., we would prefer working with M over working with A T M A. The reason is that ACE requires taking the square root of this matrix (see subsection 3.3). Had A T M A been diagonal, this would be a simple operation. But, if A T M A is not diagonal we are forced to compute the Cholesky decomposition <ref type="bibr" target="#b24">[25]</ref> of A T M A, which might be time consuming. So we work only with M , and in the next lines we justify it by showing that the coarse generalized eigenprojection problem <ref type="bibr" target="#b11">(12)</ref> stems naturally as an approximation of the restricted generalized eigenprojection problem <ref type="bibr" target="#b12">(13)</ref>.</p><p>Let us start with the restricted generalized eigenprojection problem. As explained, the offdiagonal elements of A T M A slow down the computation, and therefore we have an interest in approximating the problem such as to conceal them. These elements are responsible for the fact that in the constraint y T A T M Ay = 1 we are evidence to the appearance of "mixed terms", i.e., terms that involve the multiplication y i • y j for i = j. Expanding this constraint explicitly</p><formula xml:id="formula_52">y T A T M Ay = i,j,p m p A pi A pj y i y j ,<label>(14)</label></formula><p>we notice that for the mixed terms to be non-zero, A pi and A pj should be non-zero simultaneously, implying a correlation between y i and y j . For any reasonable interpolation matrix, this means that y i and y j are not too distantly located. A good approximation, therefore, would be to replace each y i and y j in <ref type="bibr" target="#b13">(14)</ref> with their average 1 2 (y i + y j ). Therefore,</p><formula xml:id="formula_53">y i y j ≈ 1 4 (y i + y j ) 2 = 1 4 y 2 i + 1 4 y 2 j + 1 2 y i y j ,</formula><p>or</p><formula xml:id="formula_54">y i y j ≈ 1 2 (y 2 i + y 2 j</formula><p>). Substituting this in <ref type="bibr" target="#b13">(14)</ref> gives</p><formula xml:id="formula_55">y T A T M Ay ≈ 1 2 i,j,p m p A pi A pj (y 2 i + y 2 j ) = = 1 2 i,j,p m p A pi A pj y 2 i + 1 2 i,j,p m p A pi A pj y 2 j .</formula><p>The two last terms are actually the same, thus</p><formula xml:id="formula_56">y T A T M Ay ≈ i,j,p m p A pi A pj y 2 i = i,p m p A pi y 2 i   j A pj   = i,p m p A pi y 2 i ,</formula><p>where the last step is due to the fact that the sum of each row of any interpolation matrix is 1.</p><p>Using further the mass law, we get</p><formula xml:id="formula_57">y T A T M Ay ≈ i y 2 i p m p A pi = i m i y 2 i = y T M y,</formula><p>which is just the result that we wanted to prove.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Refinement Process</head><p>We keep coarsening further and further until we obtain a coarse PSD graph that is sufficiently small. Typically, we would stop the process when we have less than 100 nodes (the speed of the algorithm does not depend on the exact value of the threshold). The two-dimensional drawing of G(L, M ), the coarsest graph, is obtained from the second and third generalized eigenvectors of Lu = µM u, denoted u 2 and u 3 , respectively. Since M is obviously positive definite, we can use Theorem A.5 to write the problem as the eigenvalue problem</p><formula xml:id="formula_58">Bv = µv<label>(15)</label></formula><p>where</p><formula xml:id="formula_59">B = M -1 2 LM -1 2 and v = M 1 2</formula><p>u, which calls for finding the second and third eigenvectors of B. This is a classical problem that can be handled by numerous algorithms, such as QR and Lanczos <ref type="bibr" target="#b24">[25]</ref>. Due to the small size of B, finding its eigenvectors takes a negligible fraction of the total running time, which makes the choice of the algorithm a marginal issue. We chose to use the Lanczos method, which is well-known for giving fast and reliable results when we are interested only in the extreme eigenvectors (i.e., those associated with either the smallest or the largest eigenvalues). Given the second and third eigenvectors of B, to be denoted v 2 and v 3 , we are able to calculate the generalized eigenvectors u 2 and u 3 from</p><formula xml:id="formula_60">u 2 = M -1 2 v 2 and u 3 = M -1 2 v 3 .</formula><p>Having solved the generalized eigenprojection problem of the coarsest PSD graph directly, we use the solution to find the corresponding solution of the second coarsest PSD graph without the need to solve it directly. We then proceed iteratively until the solution of the original problem is obtained.</p><p>How is this 'inductive step', from the coarser to the finer graph, to be carried out? Well, let G (L , M ) be a coarse m-node PSD graph, and let u 2 , u 3 , . . . u p be the first few solutions of its generalized eigenprojection problem. For two-dimensional drawing u 2 and u 3 are all that we need (thus p = 3), but we do not specify p so as to keep the algorithm general. Let the previous n-node PSD graph in the sequence of coarsenings be G(L, M ), and let u 2 , u 3 , . . . , u p be the first few solutions of its generalized eigenprojection problem. Let also A be the n × m interpolation matrix connecting G and G . The refinement step, outlined in Figure <ref type="figure" target="#fig_4">4</ref>, uses u 2 , u 3 , . . . , u p to calculate u 2 , u 3 , . . . , u p without solving directly the generalized eigenprojection problem of G. To see how it is done, let us again use Theorem A.5 to write the generalized eigenprojection problem of G in the form of an eigenvalue problem Bv = µv where B = M -1 2 LM -1 2 and v = M 1 2 u. Consequently, we now seek for the eigenvectors v 2 , v 3 , . . . , v p . The basic idea of the refinement is that for a reasonable interpolation matrix, Au i is a good approximation of u i , or alternatively M 1 2 Au i is a good approximation of v i . Then, we have to apply an iterative algorithm whose input is the initial guess v 0 i = M 1 2 Au i and whose output is the closest eigenvector v i . Such an excellent iterative algorithm is the Rayleigh quotient iteration (RQI) technique, depicted in Figure <ref type="figure" target="#fig_5">5</ref>, and described in details in <ref type="bibr" target="#b24">[25]</ref>. In RQI we find v i as the asymptotic direction of (B -</p><formula xml:id="formula_61">θ 1 I) -1 v 0 i , (B -θ 2 I) -1 (B -θ 1 I) -1 v 0 i , .</formula><p>. ., where θ 1 , θ 2 , . . . are better and better estimates of µ i . In practice, as is seen in Figure <ref type="figure" target="#fig_5">5</ref>, we do not invert B -θI, but rather solve for x a problem of the form (B -θI)x = y. Due to the sparse nature of B, it is economic to solve this problem iteratively. The well-accepted method of conjugate gradient will not work here since B -θI is indefinite. Therefore, RQI implementations frequently use either the SYMMLQ method <ref type="bibr" target="#b17">[18]</ref> or the MINRES method <ref type="bibr" target="#b17">[18]</ref>. We follow <ref type="bibr" target="#b0">[1]</ref> and use SYMMLQ. We explicitly use the fact that B -θI is symmetric and thus has orthogonal eigenvectors, by compelling v i in each iteration to be orthogonal to the previously found eigenvectors v 2 , . . . , v i-1 .</p><p>We would like to dwell upon the convergence criterion in our RQI implementation, see Figure <ref type="figure" target="#fig_5">5</ref>. The desire is to measure in each scale how similar are the two vectors Bv i and θv i . In other algorithms, see e.g. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12]</ref>, the convergence criterion was absolute, e.g., it measured directly the un-normalized difference Bv i -θv i . A disadvantage of an absolute criterion is that the magnitude of the difference depends on the type of graph, thus the tolerance should be adapted specifically for every new graph and new scale. We found the relative convergence criterion</p><formula xml:id="formula_62">Bv i -θv i √ nθ ≤ 0<label>(16)</label></formula><p>much more appropriate for our application, being very consistent and uniform in range. Here, 0 is a user-defined tolerance and n is the number of nodes in the graph. We can justify ( <ref type="formula" target="#formula_62">16</ref>) by the following argumentation: θv i is an n-dimensional vector of length θ, thus its typical coordinate has an absolute value of θ/ √ n. Accordingly,</p><formula xml:id="formula_63">n j=1 (Bv i -θv i ) 2 /(θ/ √ n) 2</formula><p>measures the sum of squared deviations of all the coordinates of the difference Bv i -θv i normalized with this typical value. Taking the square root of this magnitude and averaging, we immediately retrieve <ref type="bibr" target="#b15">(16)</ref>. This criterion was experimentally proved as an incredibly sound one, 0 = 10 -2 suffices for good convergence at all scales in all the graphs that we have checked. When comparing with other algorithm, one should keep in mind that our tolerance 0 is equivalent to an absolute tolerance abs = √ nθ 0 . In our graphs √ nθ was at most 10 -1 , thus 0 = 10 -2 is equivalent to a tolerance of at most 10 -3 in other algorithms. Occasionally, √ nθ is much smaller, resulting in equivalent absolute tolerance even as low as 10 -7 . The minimum of Hall's energy tends to increase as we coarsen the graph. Therefore θ, being an estimate of this energy, increases during coarsening. On the other hand √ n obviously decreases during coarsening, and the net effect of √ nθ keeps the convergence criterion tight in all scales.</p><p>The RQI technique is an extremely fast one, and in the examples that we have been running the number of iterations in each scale needed for calculating either v 2 or v 3 typically does not exceed 3 (for tolerance 0 = 10 -2 ). To even further decrease the average number of RQI iterations, we perform a small number (typically 3-4) of Gauss-Seidel iterations just before applying the RQI. These does not take too much time, but tend to improve the initial guess in the RQI, thus reducing the total number of RQI iterations and accelerating the computation. Indeed, the average number of RQI iterations when we precede them with a few Gauss-Seidel iterations is typically lower than 1.  Remark: In a more recent version of this work, we have replaced the RQI algorithm with a variant of the much simpler to implement power iteration algorithm (PI). A single iteration of RQI requires many computations (the solution of a system of linear equations), but on the other hand the convergence rate is extremely fast, and an accurate solution is obtained within a few iterations. In contrast, a single iteration of PI is very fast, but much more iterations are required. In general, the performance of both algorithms was quite alike. As a matter of fact, the PI can be used in itself, or as a preceding stage before the RQI, much like the Gauss-Seidel iterations. Another advantage of using the PI, is that it can serve as a solver of the coarsest problem, and thus replace the currently used Lanczos algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Function refine ({u</head><formula xml:id="formula_64">2 , u 3 , . . . , u p }, A, L, M ) B ← M -1 2 LM -1 2 for i = 2, . . . , p v 0 i ← M 1 2 Au i Orthogonalize v 0 i with respect to v 2 , . . . , v i-1 v i ← rqi(v 0 i , B) u i ← M -1 2 v i end for return u 2 , u 3 , . . . , u p</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Function rqi (v</head><formula xml:id="formula_65">0 i , B, 0 ) n ← dimension(B) v i ← v 0 i / v 0 i θ ← v T i Bv i do solve (B -θI)x = v i v i ← x/ x θ ← v T i Bv i ← Bv i -θv i /( √ nθ) until ≤ 0 return v i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The Interpolation Matrix</head><p>At this point, the multiscale scheme is completely defined, and the only thing left unexplained is how we construct a specific interpolation matrix. More precisely, the question that we address here is: given an n-node PSD graph G(L, M ) that we would like to represent by a coarser, m-node, PSD graph G , what is an appropriate n × m interpolation matrix A? As already mentioned, there is no unique recipe for this, and we can come up with many feasible methods. However, for the interpolation matrix to successfully fulfill its role, some guidelines should be followed:</p><p>1. The interpolation matrix should faithfully retain the structure of G; i.e., similar fine nodes should be similarly interpolated. This is the most important property of the interpolation matrix, since it determines how close will the initial guess Au i be to the desired solution u i .</p><p>2. The interpolation matrix should be fast to calculate, much faster than solving directly the generalized eigenprojection problem of the fine graph. Otherwise, we have not done anything in speeding up the calculation.</p><p>3. The sparser the interpolation matrix the better, since matrix manipulations will be faster. In a single coarsening step the coarse Laplacian L is determined from the fine one, L, by L = A T LA. Therefore, to preserve sparsity of the Laplacian we need a sparse interpolation matrix.</p><p>Generally speaking, these guidelines are contradicting. Improving the preservation of the structure of a graph requires more accurate interpolation, and hence a denser and more complex matrix A. A good interpolation matrix thus reflects a reasonable compromise regarding the tradeoff between accuracy and simplicity.</p><p>We now describe the two methods we have been using to construct A. To be able to compare between these algorithms, we apply them on the Eiffel tower graph, and then compare the interpolated vectors Au i to the exact ones u i . The u i 's were calculated by directly solving the generalized eigenprojection problem of the Eiffel tower graph. We bring here explicitly the first two:</p><formula xml:id="formula_66">u 2 =       0.2947 0.1354 -0.8835 0.1513 0.3021       u 3 =       -0.6961 -0.0968 0.0080 0.0777 0.7071       ,<label>(17)</label></formula><p>which give rise to the drawing plotted in Figure <ref type="figure" target="#fig_6">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Edge Contraction Interpolation</head><p>In this method we interpolate each node of the finer graph from exactly one coarse node. To find an appropriate interpolation matrix we first divide the nodes of the fine graph into small disjoint connected subsets, and then associate the members of each subset with a single coarse node. Consequently, the rows of all the members of the same subset in the interpolation matrix will be identical, having a single non-zero element (which is, of course, 1). In the Eiffel tower example, reasonable subsets would be {1, 2}, 3, and {4, 5}, giving rise to the interpolation matrix</p><formula xml:id="formula_67">A con =       1 0 0 1 0 0 0 1 0 0 0 1 0 0 1      </formula><p>, where the subscript con stands for contraction. A well known method to efficiently create such disjoint subsets is by contracting edges that participate in max-matching (see, e.g., <ref type="bibr" target="#b23">[24]</ref>).</p><p>Using the interpolation matrix A con , we obtain a coarse PSD graph G characterized by the Laplacian and mass matrix</p><formula xml:id="formula_68">L con =   16 -2 -14 -2 4 -2 -14 -2 16   M con =   2 0 0 0 1 0 0 0 2   .</formula><p>Solving directly the generalized eigenvalue problem for L con and M con , we get</p><formula xml:id="formula_69">u 2 =   0.2236 -0.8944 0.2236   u 3 =   -0.5 0 0.5   .</formula><p>Interpolating back gives the following approximations for the exact u 2 and u 3 :</p><formula xml:id="formula_70">u 0 2 = A con u 2 =       0.2236 0.2236 -0.8944 0.2236 0.2236       u 0 3 = A con u 3 =       -0.5 -0.5 0 0.5 0.5       .</formula><p>To evaluate how good are these approximations, we calculated the angles between them and the exact generalized eigenvectors (of which only u 2 and u 3 are given here explicitly in ( <ref type="formula" target="#formula_66">17</ref>)):</p><formula xml:id="formula_71">u 0 2 • u 1 = 90 • u 0 3 • u 1 = 90 • u 0 2 • u 2 = 8.9473 • u 0 3 • u 2 = 89.3327 • u 0 2 • u 3 = 89.4852 • u 0 3 • u 3 = 37.9197 • u 0 2 • u 4 = 81.1724 • u 0 3 • u 4 = 83.0270 • u 0 2 • u 5 = 88.6478 • u 0 3 • u 5 = 52.9628 • Clearly, u 0</formula><p>2 is close to u 2 , and is almost orthogonal to all others. Also, the closest vector to u 0 3 is u 3 , but in a less significant way. u 0 3 almost lies on the plane defined by (u 3 , u 5 ), with a smaller angle with the u 3 -axis than with the u 5 -axis.</p><p>The edge contraction method evidently prefers simplicity over accuracy. On the one hand, A is very sparse (each row contains exactly one non-zero element) and is easy to compute, but on the other hand, the interpolation is crude, taking into consideration only the strongest connection of each node. Indeed, we will see in section 4 that this method is characterized by very fast coarsening, but less accurate interpolations during the refinement.</p><p>The simple form of the resulting A gives rise to two elegant properties of the edge contraction algorithm, that we are about to prove: it preserves the structure of the problem, and it preserves all-positiveness of the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Claim 3.4 Let G(L, M</head><p>) be an n-node PSD graph, and let A be an n×m interpolation matrix derived by the edge contraction algorithm. Then, the coarse generalized eigenprojection problem <ref type="bibr" target="#b11">(12)</ref> identifies with the restricted generalized eigenprojection problem <ref type="bibr" target="#b12">(13)</ref>.</p><p>Proof By Claim 3.3 all that we have to show is that A T M A is diagonal for any M . The ij'th</p><formula xml:id="formula_72">element of A T M A is (A T M A) ij = p m p A pi A pj .</formula><p>But A has exactly one non-zero element in each row, therefore A pi A pj is zero whenever i = j. So, (A T M A) ij = 0 for i = j. Indeed, it is easy to see that in the Eiffel tower example M = A T M A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Claim 3.5 Let G(L, M</head><p>) be an n-node AP graph, and let A be an n × m interpolation matrix derived by edge contraction. Then, the coarse graph G will also be an AP graph.</p><p>Proof Recall expression <ref type="bibr" target="#b10">(11)</ref> for w ij , the weights of G ,</p><formula xml:id="formula_73">w ij = p,q A pi A qj w pq - p d p A pi A pj , i = j.</formula><p>The second term vanishes, as we have showed in the previous proof that A pi A pj = 0 for i = j. Therefore, if ∀p, q w pq ≥ 0, then ∀i, j w ij ≥ 0. Indeed, the coarse graph G of the Eiffel tower is an AP graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Weighted Interpolation</head><p>In this method, inspired by <ref type="bibr" target="#b19">[20]</ref>, we interpolate each node of the fine graph from possibly several coarse nodes. The first stage is to choose a subset of m nodes out of the n in G, such that they will be the nodes of G . Hereinafter, the elements of this subset will be called representatives. We pick the representatives using a technique that we have developed especially for this purpose. In order to properly explain this technique, we need further notations, to be defined in the next lines. Let R by the set of representatives, and let V -R be the set of non-representatives (remember that we used V to denote the set of all nodes). Since a representative i is uniquely associated with a coarse node, we denote this coarse node by <ref type="bibr">[i]</ref>. For each non-representative we define the following two closely related magnitudes: Definition 3.7 (Partial Degree) The partial degree of a non-representative node i is</p><formula xml:id="formula_74">d i = k∈R w ik .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3.8 (Relative Connectivity)</head><p>The relative connectivity of a non-representative node i is</p><formula xml:id="formula_75">r i = d i d i ,</formula><p>where d i is its partial degree and d i its degree.</p><p>The partial degree of a non-representative is its degree with respect to the representatives only. The relative connectivity is an important magnitude which will also be used later on.</p><p>For an AP graph 0 ≤ r i ≤ 1 ∀i, but for a general PSD graph r i may take any value.</p><p>Function Find Representatives (V, t 0 , t inc ) % V -the set of all nodes % t 0 -the initial threshold value % t inc -the amount by which we increase the threshold in each new sweep  Equipped with these definitions, we can now describe the technique by which we pick the representatives, see Figure <ref type="figure" target="#fig_7">7</ref>. The idea is to randomly order the nodes, and then to conduct a predetermined number of sweeps (typically 2-3) along them, choosing as representatives those whose relative connectivity to the already chosen representatives is below a certain threshold. Naturally, the value of this threshold must be increased before a new sweep. Typically, we start with a threshold of 0.05, and then increase it in each sweep by 0.05.</p><formula xml:id="formula_76">Build v i1 , v i2 , . . . ,</formula><p>Having chosen the representatives we are now ready to construct the interpolation matrix. We fix the coordinates of the representatives by their values as given from the coarse problem. But then we adopt a different strategy to the interpolation of the non-representatives. Their coordinates are determined such that the total energy is minimized. From (1) this energy can be written as:</p><formula xml:id="formula_77">E = 1 2 i,j∈V w ij (x i -x j ) 2 = 1 2 i,j∈R w ij (x i -x j ) 2 + + i∈R j∈V -R w ij (x i -x j ) 2 + 1 2 i,j∈V -R w ij (x i -x j ) 2 . Let k ∈ V -R. Differentiating E with respect to x k gives ∂E ∂x k = -2 i∈R w ik (x i -x k ) -2 i∈V -R w ik (x i -x k ) k ∈ V -R. (<label>18</label></formula><formula xml:id="formula_78">)</formula><p>Equating this to zero and isolating x k we get</p><formula xml:id="formula_79">x k = i∈R w ik x i + i∈V -R w ik x i i∈R w ik + i∈V -R w ik k ∈ V -R. (<label>19</label></formula><formula xml:id="formula_80">)</formula><p>This equation is satisfied by each of the non-representatives, and therefore ( <ref type="formula" target="#formula_79">19</ref>) actually defines a set of n -m coupled equations. What we would really like to do, though, is to express the coordinates of each non-representative by those of the representatives. Indeed, it is possible in theory to work out this set of n -m equations such as to isolate the n -m coordinates of the non-representatives as functions of the m coordinates of the representatives. Yet in practice we would avoid it for two reasons: First, this is expensive in computation time. Second, it will give a dense interpolation matrix, since each x k (k ∈ V -R) will depend in potentially many representatives. Therefore, to avoid lengthy computations and to save sparsity we would like to approximate <ref type="bibr" target="#b18">(19)</ref> by a simpler expression. Such an approximation might be achieved if we ignore, when calculating x k , all other non-representatives, i.e., taking them all as if they were located at x k . Due to the local nature of the interpolation, this is a reasonable approximation. For, if we look at the location of a particular non-representative x k , then all the other nonrepresentatives will be, on the average, equally distributed around it. This assumption is equivalent to taking the second term in <ref type="bibr" target="#b17">(18)</ref> as zero. Taking large enough relative connectivity threshold in the process of selecting representatives, we are assured that the term that we do not neglect (the first term in 18) is indeed significant. Substituting</p><formula xml:id="formula_81">x i = x k for i ∈ V -R in (19) gives x k = i∈R w ik x i i∈R w ik k ∈ V -R.<label>(20)</label></formula><p>Now, the elements of the interpolation matrix are just</p><formula xml:id="formula_82">A i[j] = w ij k∈R w ik i ∈ V -R, j ∈ R<label>(21)</label></formula><formula xml:id="formula_83">A i[j] = 1 ifj = i 0 ifj = i i ∈ R. (<label>22</label></formula><formula xml:id="formula_84">)</formula><p>This expression is much simpler and is fast to compute. It also results in a sparser interpolation matrix than we would have get from <ref type="bibr" target="#b18">(19)</ref>. If still the result is not sparse enough we can always make it even sparser by interpolating x k only from the first few most dominant representatives. That is, for each k ∈ V -R we define a set R k ⊆ R that includes at most r (a certain small number) representatives from which x k is interpolated. Then ( <ref type="formula" target="#formula_81">20</ref>) and ( <ref type="formula" target="#formula_82">21</ref>) become</p><formula xml:id="formula_85">x k = i∈R k w ik x i i∈R k w ik k ∈ V -R,<label>(23)</label></formula><p>and</p><formula xml:id="formula_86">A i[j] = w ij k∈R k w ik i ∈ V -R, j ∈ R i , R i ⊆ R.<label>(24)</label></formula><p>Obviously, if ∀k R k = R then ( <ref type="formula" target="#formula_81">20</ref>) and ( <ref type="formula" target="#formula_82">21</ref>) are reconstructed. Now we have guaranteed both sparsity and speed of computation, but we are facing a new problem. Equations ( <ref type="formula" target="#formula_82">21</ref>) or ( <ref type="formula" target="#formula_86">24</ref>) might violate the requirements from an interpolation matrix. If negative weights are present, A ij is no longer bounded to the region [0, 1], but may take negative values (and thus also values greater than 1), what might give rise to negative masses. To assure a proper interpolation matrix, we must take further precautions: Let P i be the set of all representatives that are connected to fine node i by positive weights, and let p i be the sum of these weights. Similarly, let N i be the set of all representatives that are connected to fine node i by negative weights, and let n i be their sum (obviously,</p><formula xml:id="formula_87">p i + n i = k∈R w ik = d i ).</formula><p>Then, if we choose R i in <ref type="bibr" target="#b23">(24)</ref> as P i if the positive weights are dominant (p i ≥ -n i ) and as N i if the negative weights are dominant (p i &lt; -n i ) we obtain a proper interpolation matrix:</p><formula xml:id="formula_88">A i[j] =    w ij /p i j ∈ P i and p i ≥ -n i w ij /n i j ∈ N i and p i &lt; -n i 0 otherwise i ∈ V -R.<label>(25)</label></formula><p>Of course, if either P k or N k is empty, (25) identifies with <ref type="bibr" target="#b20">(21)</ref>. Moreover, A will still be proper if we choose</p><formula xml:id="formula_89">R i ⊆ P i if p i ≥ -n i and R i ⊆ N i if p i &lt; -n i .</formula><p>Actually, we have seen that if we start with an AP graph then the positive weights are almost always dominant during the entire coarsening process -the negative weights that do appear are significantly inferior in number and in magnitude. We cannot prove this observation directly, but we can support it with two facts: First, recall that the degree of each node is non-negative (see Claim 3.1), and therefore the total sum of positive weights of a node is never lower than the sum of the absolute value of its negative weights. Second, we can prove a weak version of the all-positiveness preservation: Claim 3.6 Let G be an n-node AP graph, let A be an n × m interpolation matrix derived by the weighted interpolation <ref type="bibr" target="#b20">(21)</ref>, and let r i be the relative connectivity of the i'th node. Then, if ∀i ∈ V -R r i ≥ 1  2 the coarse graph G will also be an AP graph <ref type="foot" target="#foot_4">5</ref> . Proof Recall expression <ref type="bibr" target="#b10">(11)</ref> for w [i][j] , the weights of G ,</p><formula xml:id="formula_90">w [i][j] = p,q∈V A p[i] A q[j] w pq - p∈V d p A p[i] A p[j] , i = j.</formula><p>This equation contains two terms, that we denote T 1 and T 2 . We start by examining the second term, T 2 . Dividing the sum into sum over representatives and sum over non-representatives</p><formula xml:id="formula_91">we get T 2 = -p∈V -R d p A p[i] A p[j] -p∈R d p A p[i] A p[j]</formula><p>. Keeping in mind that each representative is interpolated only from itself, we know that if p ∈ R then A p[i] A p[j] = 0 for i = j. Therefore, the summation over representatives vanishes and we are left with</p><formula xml:id="formula_92">T 2 = -p∈V -R d p A p[i] A p[j]</formula><p>. Similar considerations for T 1 gives</p><formula xml:id="formula_93">T 1 = w ij + p,q∈V -R A p[i] A q[j] w pq + p∈V -R A p[j] w pi + p∈V -R A p[i] w pj .</formula><p>We now use <ref type="bibr" target="#b20">(21)</ref> to write the overall result</p><formula xml:id="formula_94">w [i][j] = T 1 + T 2 = w ij + p,q∈V -R w pi w qj w pq d p d q + p∈V -R w pj w pi d p + + p∈V -R w pi w pj d p - p∈V -R d p w pi w pj d p d p = = w ij + p,q∈V -R w pi w qj w pq d p d q + p∈V -R w pj w pi d p 2 - d p d p .</formula><p>The first two terms are obviously non-negative. The third will be non-negative if ∀p 2 -</p><formula xml:id="formula_95">d p /d p ≥ 0, which is just r p ≥ 1 2 .</formula><p>From the proof it is clearly seen that 1  2 is a high value for the bound. Due to the first two positive terms of w ij we expect in most of the cases that G will be an AP graph even for lower values of the relative connectivity. This observation is important for practical reasons since if we keep r i ≥ 1  2 for all non-representatives, we might be needing a large number of representatives that would make A very dense.</p><p>Clearly, in comparison with the edge contraction algorithm, this one prefers accuracy over simplicity. A is less sparse, more expensive to compute, but far more accurate. Expectedly, we will see in section 4 that the weighted interpolation algorithm gives longer coarsening times but faster refinement. For large enough graphs, this algorithm evidently outperforms the edge contraction algorithm, as will become apparent in section 4.</p><p>All these properties are nicely seen when we apply the weighted interpolation algorithm on the Eiffel tower graph. Taking the nodes 1,3, and 5 as representatives, such that [1] = 1, [3] = 2, and [5] = 3, the interpolation matrix that we get is</p><formula xml:id="formula_96">A wi =       1 0 0 0.5 0.2 0.3 0 1 0 0.333 0.167 0.5 0 0 1      </formula><p>, where the subscript wi stands for weighted interpolation. Using this matrix, we obtain a coarse PSD graph G characterized by the Laplacian and mass matrix</p><formula xml:id="formula_97">L wi =   5.3611 -1.6278 -3.7333 -1.6278 3.2744 1.6467 -3.7333 -1.6467 5.38   M wi =   1.8333 0 0 0 1.3667 0 0 0 1.8   .</formula><p>Solving directly the generalized eigenvalue problem for L wi and M wi , we get</p><formula xml:id="formula_98">u 2 =   0.3869 -1 0.3652   u 3 =   -0.9665 -0.0205 1   .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparing Edge Contraction to Weighted Interpolation</head><p>In Subsection 3.4 we introduced two techniques for generating an interpolation matrix, edge contraction and weighted interpolation. We expect edge contraction to have a shorter coarsening time, but a longer RQI time with respect to the weighted interpolation. For relatively loose tolerance (large 0 in RQI), not much work is required during the refinement and we can expect the coarsening time to be the dominant portion of the total running time, resulting in faster performance of the edge contraction. However, as we decrease 0 , the RQI time becomes the dominant part of the running time, yielding faster performance of the weighted interpolation. A demonstration of the above is brought in Figure <ref type="figure">9</ref>, where the total computa- Figure <ref type="figure">9</ref>: A comparison between the total computation time of ACE using either the edge contraction method or the weighted interpolation method. The comparison is made as a function of the tolerance 0 for two graphs: Grid400 is simple 400 × 400 rectangular grid containing 160,00 nodes and 319,200 edges. Sierpinski10 is a 10-layer Sierpinski graph containing 88,575 nodes and 177,147 edges. The two graphs behave similarly -at high 0 (low accuracy) the edge contraction has a slight advantage, while for lower 0 the weighted interpolation gains more and more advantage over the edge contraction. The coarsening times for Grid400 are 2.3 seconds and 0.5 seconds for the weighted interpolation and the edge contraction, respectively. For Sierpinski10 the coarsening times are 0.7 seconds and 0.3 seconds for the weighted interpolation and the edge contraction, respectively.</p><p>tion time for both methods is plotted versus 0 for two types of graphs. In this Figure, every point is the average of 10 identical runs of ACE. In each run we asked ACE to compute the generalized eigenvectors u 2 and u 3 so as to produce the standard two-dimensional drawing.</p><p>Grid400 is a graph of a simple 400 × 400 rectangular mesh, containing 160,000 nodes and 319,200 edges. The coarsening time is independent of the tolerance, being 2.3 seconds for the weighted interpolation and 0.5 seconds for the edge contraction. For high tolerance of 0 = 10 0 and 0 = 10 -1 , the RQI time is less than 0.5 seconds, and the total computation time is dominated by the coarsening time, making the edge contraction faster. However, as the tolerance decreases the RQI time becomes more and more dominant, giving an increasing gap in computation time in favor of the weighted interpolation. Sierpinski10 is a 10-layer Sierpinski graph, constructed of 88,575 nodes and 177,147 edges. The coarsening times are 0.7 seconds and 0.3 seconds for weighted interpolation and edge contraction, respectively.</p><p>In this example the coarsening times of the two methods are much closer, and only when 0 = 10 0 the RQI time is negligible enough to give an advantage to the edge contraction. Just like for Grid400, as the tolerance decreases the weighted interpolation becomes more and more favorable.</p><p>But what is the tolerance that we should work with? For both graphs in the above example, a convergence to the correct solution occurred in all tolerances. However, as the tolerance decreases the global layout of the drawing becomes sharper and more accurate. For a reasonably nice global drawing we recommend using a tolerance of 0 = 10 -2 , for which the edge contraction method is slightly faster. For the purpose of presentation, or whenever sharp and accurate drawings are needed, then a tolerance of 0 = 10 -3 will generally do the work. In these cases, the weighted interpolation is almost always the preferred method.</p><p>Sometimes, one is interested in computing more than two generalized eigenvectors. This might be the case when carrying out three-dimensional drawings, or in two-dimensional drawings where the coordinates are associated with generalized eigenvectors other than u 2 and u 3 . As an example, we will see later in this section a graph whose drawing is "prettier" when the coordinates are associated with u 4 and u 5 . In any case, when more than two generalized eigenvectors are requested, the expected RQI time of ACE increases while the coarsening time does not change. Consequently, the advantages of the weighted interpolation become more salient, suggesting its preferring in such cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Speed of computation for Selected Graphs</head><p>Table <ref type="table" target="#tab_1">1</ref> shows the results of applying ACE to a number of large graphs, each with more than 10 5 nodes, and gives a pretty good feeling for the speed of the algorithm. We used the edge contraction method, and a tolerance of 0 = 10 -2 , asking ACE to compute the two generalized eigenvectors u 2 and u 3 .</p><p>In the table, the magnitude and complexity of the graphs are characterized by the number of nodes V , the number of edges E, and the minimal, average, and maximal degree of the graph. Then the total computation time of ACE is shown, preceded by the RQI time and the coarsening time. The sum of these two is less than the total time, chiefly due to a few Gauss-Seidel relaxations that we perform in each scale before applying RQI. The last column of the table gives the average number of RQI iterations that ACE carries out within a single scale. This average is no larger than 1, which is characteristic of the use of a tolerance of</p><formula xml:id="formula_99">0 = 10 -2 .</formula><p>Graphs of around 10 5 nodes are drawn in a few seconds; with 10 6 nodes the algorithm takes less than a minute. The largest graph consists of 7.5 • 10 6 nodes, and the running time is about two and a half minutes. Thus, ACE exhibits a truly significant improvement in computation time for drawing large graphs. Moreover, one can use it to draw huge graphs of 10 6 -10 7 node, which we have not seen dealt with appropriately in the literature, in quite a reasonable amount of time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Drawings of Selected Graphs</head><p>Unfortunately, limitations of file size and printing resolution prevent us from bringing here full drawings of really huge graphs. Yet, for the reader to obtain a visual impression of the kind of drawings produced by ACE, we bring here a collection of drawings of smaller graphs.</p><p>Before discussing specific examples, here are some general comments about the nature of drawings produced by minimizing Hall's energy. On the one hand, we are assured to be in a global minimum of the energy, thus we might expect the global layout of the drawing to faithfully represent the structure of the graph. On the other hand, there is nothing in Hall's energy that prevents nodes from being very close. Hence, the drawing might show dense local arrangement.</p><p>These general claims are nicely demonstrated in the examples drawn in Figure <ref type="figure" target="#fig_9">10</ref>. Figure <ref type="figure" target="#fig_9">10a</ref> shows a folded grid with 10, 00 nodes and over 18, 000 edges, obtained by taking a rectangular grid, removing the horizontal edges in its central region, and connecting opposing corners. This graph has high level of symmetry, which is well reflected in the drawing. Figure <ref type="figure" target="#fig_9">10b</ref> shows another highly symmetric graph, the 4970 graph. Besides the excellent preservation of symmetry in the two-dimensional layout, this graph shows how ACE handles a graph in which many different distance scales are embedded. Figure <ref type="figure" target="#fig_9">10c</ref>, the 4elt graph, is yet another example of a graph with inherent diversity of distance scales. Its drawing resembles the one shown in the technical report version of <ref type="bibr" target="#b23">[24]</ref>, which was obtained using a different graph drawing approach. The graph has over 15, 000 nodes and almost 46, 000 edges. Figure <ref type="figure" target="#fig_9">10d</ref> is the Sierpinski graph with 9-layer depth. This is an example of a fractal structure with again high level of symmetry, with almost 30, 000 nodes and 59, 000 edges. As to be expected from the previous discussion, these drawings indeed exhibit rather impressive global layout, but also have locally dense regions.</p><p>For the vast majority of graphs, associating the coordinates with u 2 and u 3 gives satisfactory results. For example, in Figure <ref type="figure" target="#fig_9">10</ref> all the drawing were obtained so. In some cases, though, dense local regions might become dominant enough to significantly flaw the visual perception. An example for such a scenario is shown in Figure <ref type="figure">11a</ref>. Although arguably globally nice, the extremely dense local arrangement makes this drawing uninteresting. Asking ACE to calculate additional generalized eigenvectors, we found the drawing obtained from u 4 and u 5 to be much "prettier", see Figure <ref type="figure">11b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>It appears that the time performance of the ACE algorithm is sufficiently good to finally enable graph drawing tools to be used to visualize huge systems such as the World Wide Web or networks of protein-protein interactions. This remains to be seen.</p><p>Obviously, the quality of the algorithm's results depends on the appropriateness of Hall's energy to the problem of graph drawing. In comparison with many of the other energy functions used in graph drawing, such as the Kamada-Kawai energy <ref type="bibr" target="#b14">[15]</ref>, Hall's energy is distinguished by its simple form. The tractability of the mathematical analysis that this brings with it yields the vastly improved computation speed and a guaranteed convergence to a global minimum. All this lends ACE stability and causes its results to be "globally aesthetic". On the other hand, local details of the graph might be aesthetically inferior with respect to the results of other, more complicated, energy functions. For certain applications it might be possible to combine the advantages of different graph drawing algorithms, obtaining the global layout of a huge graph with ACE, and then use slower methods to beautify particular regions of interest.</p><p>Having ACE quickly determine the coordinates of the nodes of huge graphs poses new challenges for their actual display. Obviously, graphs with millions of nodes cannot be beneficially displayed or printed as is, and new display tools would be required. A promising direction is to display only a portion of a graph at any given time, using various smooth navigation tools. A survey of such methods can be found in, e.g., <ref type="bibr" target="#b12">[13]</ref>. Another interesting approach is to use the coarse graphs produced by ACE during the coarsening process as abstractions of the original graph for various display purposes. Moreover, in line with the previous paragraph, we could use ACE to produce the global layout and its abstractions, and then use other algorithms for the instantaneous drawing of zoomed portions.</p><p>In developing the ACE algorithm we restricted ourselves to the problem of graph drawing. However, what we have been actually doing is to devise an algorithm that quickly finds the first few generalized eigenvectors of any problem of the form Lx = µM x with L a Laplacian and M a real diagonal positive definite matrix. L being a Laplacian does not really limit the algorithm, since it can also deal with the case of L being any real positive semi-definite matrix. The fact that M is diagonal, however, is important to the speed of the algorithm, since if it were non-diagonal positive definite, we would be forced to add an additional calculation of its root, e.g., by Cholesky factorization.</p><p>Problems of the form Lx = µM x, with L a Laplacian and M real diagonal positive definite, appear frequently also outside graph drawing; for example, in image segmentation <ref type="bibr" target="#b20">[21]</ref>, partitioning <ref type="bibr" target="#b0">[1]</ref>, and linear ordering <ref type="bibr" target="#b13">[14]</ref>, and we hope that ACE may be found useful by researchers in these and other fields.</p><p>Finally, we would like to emphasize that the two algorithms we proposed for calculating an interpolation matrix should be taken as suggestions only. Since there is no strict criterion for the evaluation of an interpolation matrix, those that we were using are not necessarily "optimal", and in fact we have a number of additional ideas about this issue that require further inspection. Theorem A. <ref type="bibr" target="#b4">5</ref> Let S and Q be two n × n real matrices, and let Q also be symmetric positive definite. Let µ and x be generalized eigenvalues and eigenvectors of S and Q. Then, the eigenvalues and eigenvectors of the n × n real matrix Q -1 2 SQ -1 2 are µ and Q</p><formula xml:id="formula_100">1 2 x.</formula><p>Proof From theorem A.4 there exists a real symmetric positive definite (and thus non-singular) matrix Q</p><formula xml:id="formula_101">1 2 such that Q = Q 1 2 • Q 1 2</formula><p>. Substituting this in the generalized eigenvalue problem gives Sx = µQ</p><formula xml:id="formula_102">1 2 • Q 1 2</formula><p>x. Multiplying from the left by Q -1 2 and inserting</p><formula xml:id="formula_103">Q -1 2 • Q 1 2 = I we get Q -1 2 SQ -1 2 (Q 1 2 x) = µ(Q 1 2 x).</formula><p>Theorem A. <ref type="bibr" target="#b5">6</ref> If S is n×n real positive (semi-)definite and Q is n×n real symmetric positive definite, then the generalized eigenvalues of S and Q are all real and positive (non-negative).</p><p>Proof From Theorem A.5 we know that the µ's are the eigenvalues of Q -1 2 SQ -1 2 . S and Q -1 2 are both real and symmetric, thus Q -1 2 SQ -1 2 is real and symmetric. Theorem A.1 then dictates that the µ's are all real. Definition A.5 (Q-Orthogonal Vectors) Let Q be an n × n real symmetric matrix. The two vectors x, y ∈ R n are called Q-orthogonal if x T Qy = y T Qx = 0. Definition A.6 (Q-Normalized Vector) Let Q be an n×n real symmetric matrix. The vector</p><formula xml:id="formula_104">x ∈ R n is called Q-normalized if x T Qx = 1.</formula><p>Definition A.7 (Q-Orthonormal Set) Let Q be an n × n real symmetric matrix. The set of vectors x 1 , x 2 , . . . , x n ∈ R n is called Q-orthonormal if (x i ) T Qx j = δ ij . <ref type="foot" target="#foot_5">6</ref>Theorem A. <ref type="bibr" target="#b6">7</ref> Let S and Q be n × n real symmetric matrices, and let Q also be positive definite. Then, one can always find a set of n real Q-orthonormal generalized eigenvectors of S and Q.</p><p>Proof From Theorem A.5 we know that the n vectors Q 1 2 x i (i = 1, . . . , n) are the eigenvectors of Q -1 2 SQ -1 2 . But, S and Q -1 2 are both real and symmetric, and consequently so is Q -1 2 SQ -1 2 . Therefore, by Theorem A.2 one can find a set of real orthonormal set Q 1 2 x i (i = 1, . . . , n), and so (Q</p><formula xml:id="formula_105">1 2 x i ) T • Q 1 2 x j = x T i Q 1 2 Q 1 2 x j = x T i Qx j = δ ij .</formula><p>Definition A.8 (Gradient) Let f (x) be a scalar-valued function the vector x ∈ R n . The gradient of f , written ∇f or df /dx is defined as the n-dimensional vector df dx i = ∂f ∂x i .</p><p>Theorem A.8 Let S be an n×n real symmetric matrix and let f (x) = x T Sx. Then df /dx = 2Sx.</p><p>Proof Let us differentiate f (x) with respect to x i :</p><formula xml:id="formula_106">df dx i = ∂f ∂x i = ∂ ∂x i n r,p=1</formula><p>S rp x r x p = 2 n p=1 S ip x p = 2(Sx) i .</p><p>Theorem A.9 Let f (x) = x T x. Then df /dx = 2x.</p><p>Proof This is merely a special case of Theorem A.8 with S = I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Graph Connectivity</head><p>Along the paper we deliberately did not discuss the issue of graph connectivity, mainly because it would have distracted the flow of the presentation. Nevertheless, we believe that many readers will find interest in the question whether ACE preserves connectivity, i.e., starting with a connected graph, can we obtain disconnected ones along the coarsening. Superficial observation tells that no matter what is the original graph, negative weights might appear in the coarsening, thus there is a theoretical, if not practical, possibility that weights will be concealed (i.e., weights of an edge might be summed up to zero) such that the graph will become disconnected. We would like to prove that, at least when we start with an AP graph, this can never happen! We start by revealing the relations between the Laplacian and the connectivity of an AP graph. Hall <ref type="bibr" target="#b9">[10]</ref> has shown that if G is an AP connected graph, its Laplacian has one and only one zero eigenvalue. We extend his proof by proving that the opposite is also true.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma B.1 Let G(L, M ) be an AP graph. G is connected if and only if its Laplacian has exactly one zero eigenvalue.</head><p>Proof Since the sum of each row of the Laplacian L is zero, the vector v 1 = 1 n is definitely an eigenvector with zero eigenvalue, L•1 n = 0. So, L must have at least one zero eigenvalue. Let us assume that it has a second zero eigenvalue, corresponding to the vector v 2 which is orthogonal to v 1 . Then, Hall's energy for v 2 must vanish, E = v T 2 Lv 2 = 0. Writing this energy in the form E = 1 2 i,j w ij (x i -x j ) 2 , we see that it can vanish only if x i = x j for all adjacent pairs i, j . Since v 2 is orthogonal to v 1 , there must exist a non-adjacent pair, say i, j, whose coordinates are not the same, x i = x j . But, if G is connected there must be a path connecting i and j, and somewhere along this path we must encounter an adjacent pair with non-identical coordinates. Therefore, if G is connected v 2 cannot exist and L must have one and only one eigenvalue.</p><p>Let us now prove the opposite direction by showing that if G is disconnected, L must have at least two zero eigenvalues. Let G be an n-node AP graph, composed of disconnected subgraphs. Let us decompose G into two disconnected subgraphs containing n 1 and n 2 nodes (obviously, n 1 + n 2 = n), and let us also sort the nodes such that x 1 , . . . , x n1 belong to the first subgraph, while x n1+1 , . . . , x n belong to the second. Let v 2 be an n-dimensional vector whose n 1 first elements are 1 -n/n 1 , and the rest are 1. v 2 is orthogonal to v 1 since</p><formula xml:id="formula_107">v T 1 • v 2 = n 1 1 - n n 1 + n 2 = -n + n 1 + n 2 = 0.</formula><p>Moreover, it is an eigenvector of L with zero eigenvalue because the coordinates of each connected pair of nodes are identical. 7   Unfortunately, this lemma does not hold for PSD graphs with negative weights. Positive and negative terms might exactly sum up to zero Hall's energy, enabling additional zero eigenvalues. A special case of a connected PSD graph with two zero eigenvalues is shown in Figure <ref type="figure" target="#fig_2">12</ref>. The Laplacian is 7 One can even extend this proof to show that the number of zero eigenvalues of the Laplacian of an AP graph G equals to the number of disconnected subgraphs composing it, see e.g., <ref type="bibr" target="#b16">[17]</ref>. </p><formula xml:id="formula_108">L =   1 -2 1 -2 4 -2 1 -2 1   ,</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Definition 3 . 6 (A</head><label>36</label><figDesc>Mass law) Let G be a fine n-node PSD graph with masses m 1 , m 2 , . . . , m n , and let A be an n × m interpolation matrix. The masses of the coarse PSD graph G m 1 , m 2 , . . . , m m are given by m j = n i=1 ij m i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Eiffel tower -fine graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Eiffel tower -coarse graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A single refinement step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Rayleigh quotient iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The drawing of the Eiffel tower example obtained by direct solution of the generalized eigenprojection problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The technique to choose the representatives for the weighted interpolation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Examples of ACE drawings, obtained from the generalized eigenvectors u2 and u3. a) A folded-grid, based on a 100 × 100 rectangular grid. Its size is |V | = 10, 000 and |E| = 18, 713. b) The 4970 graph, taken from [24]. Here, |V | = 4970 and |E| = 7400. c) The 4elt graph, taken from Francois Pellegrini's Scotch graph collection, at: http://www.labri.u-bordeaux.fr/ Equipe/PARADIS/Member/pelegrin/graph. Here, |V | = 15, 606 and |E| = 45, 878. d) A Sierpinski graph with 9-layer depth. |V | = 29, 526 and |E| = 59, 049.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Running times of the various components of ACE, and the average number of RQI iterations per scale. The graphs in the table are all very large ones, consisting of at least 10 5 nodes. Most of them were taken from internet resources, as indicated in the table. grid1000 and grid1415, which are simple rectangular grids, were produced by us. Taken from George Karypis' collection at: ftp.cs.umn.edu/users/kumar/Graphs ‡ Taken from Francois Pellegrini's Scotch graph collection, at: http://www.labri.u-bordeaux.fr/Equipe/PARADIS/Member/pelegrin/graph § Taken from the University of Greenwich Graph Partitioning Archive, at: http://www.gre.ac.uk/˜c.walshaw/partition</figDesc><table><row><cell>Graph Name</cell><cell>V</cell><cell>Size</cell><cell>E</cell><cell>min</cell><cell>Degree avg.</cell><cell>max</cell><cell>RQI</cell><cell>Times [sec] coarse</cell><cell>total</cell><cell>Avg. RQI iterations</cell></row><row><cell>598a  †</cell><cell>110,971</cell><cell></cell><cell>741,934</cell><cell>5</cell><cell>13.37</cell><cell>26</cell><cell>0.6</cell><cell>0.8</cell><cell>3</cell><cell>0.55</cell></row><row><cell>ocean  ‡</cell><cell>143,437</cell><cell></cell><cell>409,593</cell><cell>1</cell><cell>5.71</cell><cell>6</cell><cell>0.6</cell><cell>0.6</cell><cell>2.2</cell><cell>0.5</cell></row><row><cell>144  †</cell><cell>144,649</cell><cell cols="2">1,074,393</cell><cell>4</cell><cell>14.86</cell><cell>26</cell><cell>0.7</cell><cell>1</cell><cell>3.8</cell><cell>0.36</cell></row><row><cell>wave  §</cell><cell>156,317</cell><cell cols="2">1,059,331</cell><cell>3</cell><cell>13.55</cell><cell>44</cell><cell>0.6</cell><cell>0.8</cell><cell>2.9</cell><cell>0.5</cell></row><row><cell>m14b  †</cell><cell>214,765</cell><cell cols="2">1,679,018</cell><cell>4</cell><cell>15.64</cell><cell>40</cell><cell>1.1</cell><cell>1.6</cell><cell>5.7</cell><cell>0.58</cell></row><row><cell>mrngA  †</cell><cell>257,000</cell><cell></cell><cell>505,048</cell><cell>2</cell><cell>3.93</cell><cell>4</cell><cell>1</cell><cell>1.4</cell><cell>4.5</cell><cell>0.46</cell></row><row><cell>auto  †</cell><cell>448,695</cell><cell cols="2">3,314,611</cell><cell>4</cell><cell>14.77</cell><cell>37</cell><cell>2.5</cell><cell>3.8</cell><cell>14.1</cell><cell>0.42</cell></row><row><cell cols="2">grid1000 1,000,000</cell><cell cols="2">1,998,000</cell><cell>2</cell><cell>4.00</cell><cell>4</cell><cell>44.4</cell><cell>3.5</cell><cell>53.5</cell><cell>1.00</cell></row><row><cell>mrngB  †</cell><cell>1,017,253</cell><cell cols="2">2,015,714</cell><cell>2</cell><cell>3.96</cell><cell>4</cell><cell>4.2</cell><cell>6.6</cell><cell>20.8</cell><cell>0.37</cell></row><row><cell cols="2">grid1415 2,002,225</cell><cell cols="2">4,001,620</cell><cell>2</cell><cell>4.00</cell><cell>4</cell><cell>109.6</cell><cell>7.0</cell><cell>125.9</cell><cell>1.00</cell></row><row><cell>mrngC  †</cell><cell>4,039,160</cell><cell cols="2">8,016,848</cell><cell>2</cell><cell>3.97</cell><cell>4</cell><cell>14.6</cell><cell>22.1</cell><cell>69</cell><cell>0.44</cell></row><row><cell>mrngD  †</cell><cell cols="3">7,533,224 14,991,280</cell><cell>2</cell><cell>3.98</cell><cell>4</cell><cell>28</cell><cell>47</cell><cell>153.7</cell><cell>0.39</cell></row><row><cell>†</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>see Definition A.3</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Actually, for connected graphs L has one and only one zero eigenvalue, see Lemma B.1</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Actually, this is true as long as we are given a definite order 1, . . . , n of the nodes.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>As a matter of fact, one can show that A T M A will be diagonal if and only if A has exactly one non-zero element in each row.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>A similar claim can be proved when the interpolation matrix is derived from<ref type="bibr" target="#b23">(24)</ref>, but the relative connectivity of node i should be taken as r i = k∈R i w ik /d i .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>δ ij is defined as 1 for i = j and as 0 otherwise</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Bruce Hendrickson and Robert Leland for giving their permission to use parts of their code, Chaco <ref type="bibr" target="#b11">[12]</ref>. In particular, we have been using their Lanczos and RQI implementation.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Interpolating back gives the following approximations for the exact u 2 and u 3 :</p><p>-0.9665 -0.1874 -0.0205 0.1744 1</p><p>.</p><p>Again, we calculate the angles between these approximations and the exact generalized eigenvectors:</p><p>over the edge contraction algorithm is undoubtedly observed, since both u 0 2 and u 0 3 are very close to their target values u 2 and u 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Summary of the Algorithm</head><p>In Figure <ref type="figure">8</ref> we bring an outline of the full ACE algorithm, in the form of the recursive function  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We have been tested our algorithm against a variety of graphs, taken from diverse sources.</p><p>Here we present some of our more interesting results, all obtained using Pentium III 1GHz computer with 1.5 GB RAM. In subsection 4.1 we show a comparison between our two methods for generating an interpolation matrix. In subsection 4.2 we show data on computation speed for selected graphs. Finally, in subsection 4.3 we bring examples of drawings of some interesting graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Useful Properties of Matrices</head><p>Here, fundamental results of matrix theory that we use in this paper are brought. In some cases, proofs are not supplied, and the reader is referred to a standard textbooks.</p><p>Theorem A. <ref type="bibr" target="#b0">1</ref> The eigenvalues of a real symmetric matrix are all real.</p><p>Proof See <ref type="bibr" target="#b24">[25]</ref>  and it has two zero eigenvalues corresponding to the orthogonal vectors v 1 = (1, 1, 1) T and v 2 = (1, 0, -1) T . Nonetheless, we can still say the following:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma B.2 Let G(L, M ) be a PSD graph. If its Laplacian L has only one zero eigenvalue, then G is connected.</head><p>Proof Similarly to the second part of the proof of Lemma B.1. There we show that if G is disconnected, L must have at least two zero eigenvalues. This part of the proof applies also to PSD graphs, and therefore if L has only one zero eigenvector G must be connected. Proof G is connected by Lemma B.2. All that is left to prove is that for any interpolation matrix A, the Laplacian L = A T LA has only one zero eigenvalue. Let v be any eigenvector of L with zero eigenvalue, so that v T L v = 0. Let z be the vector interpolated from v , z = Av . Then v T L v = z T Lz = 0. But L has only one zero eigenvalue, so that z must be of the form c • 1 n , and v satisfies the equation Av = c • 1 n . We already know that v = c • 1 m solves this equation, and indeed this is the already familiar eigenvector of L corresponding to zero eigenvalue. Another solution of Av = c • 1 n does not exist because A is of full column rank. Therefore, L has only one zero eigenvalue.</p><p>Obviously, this theorem can be used inductively to show that if we start with a PSD graph whose Laplacian possesses only one zero eigenvalue, then each of its successive coarse versions obtained during the coarsening process are connected.</p><p>The connectivity preservation of AP graphs stems directly from Theorem B.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Fast Multilevel Implementation of Recursive Spectral Bisection for Partitioning Unstructured Problems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency: Practice &amp; Experience</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="101" to="117" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Algebraic multigrid (AMG) for automatic multigrid solution with applications to geodetic computations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccormick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ruge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Report, Inst. for Computational Studies</title>
		<imprint>
			<biblScope unit="volume">1852</biblScope>
			<date type="published" when="1982">1982</date>
			<pubPlace>Fort Colins, Colorado</pubPlace>
		</imprint>
	</monogr>
	<note>POB</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Drawing Graphs Nicely Using Simulated Annealing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="301" to="331" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Algorithms for the Visualization of Graphs</title>
		<author>
			<persName><forename type="first">G</forename><surname>Di Battista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tamassia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Tollis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Prentice-Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Heuristic for Graph Drawing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Eades</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Congressus Numerantium</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="149" to="160" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Fast Adaptive Layout Algorithm for Undirected Graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Frick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mehldau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Graph Drawing</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>Graph Drawing</meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="1994">1994. 1995</date>
			<biblScope unit="volume">894</biblScope>
			<biblScope unit="page" from="389" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph Drawing by Force-Directed Placement</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M G</forename><surname>Fruchterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Reingold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Software-Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1129" to="1164" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Multi-dimensional Approach to Force-Directed Layouts of Large Graphs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gajer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Kobourov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Graph Drawing 2000</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>Graph Drawing 2000</meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1984</biblScope>
			<biblScope unit="page" from="211" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Multi-Scale Method for Drawing Graphs Nicely</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hadany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="3" to="21" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An r-dimensional Quadratic Placement Algorithm</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="219" to="229" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Fast Multi-scale Method for Drawing Large Graphs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">1984</biblScope>
			<biblScope unit="page" from="183" to="196" />
			<date type="published" when="2000">2000. 2000</date>
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
	<note>Proceedings of Graph Drawing</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The Chaco User&apos;s Guide: Version 2.0</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hendrickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leland</surname></persName>
		</author>
		<idno>SAND95-2344</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>Sandia National Laboratories</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Graph Visualisation and Navigation in Information Visualisation</title>
		<author>
			<persName><forename type="first">I</forename><surname>Herman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Melancon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Marshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="24" to="43" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Optimal linear labelings and eigenvalues of graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Juvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mohar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="153" to="168" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An Algorithm for Drawing General Undirected Graphs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kawai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="7" to="15" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Drawing GraphsMethods and Models</title>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Kaufmann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">2025</biblScope>
			<date type="published" when="2001">2001</date>
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Laplacian Spectrum of Graphs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mohar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graph Theory, Combinatorics, and Applications</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="871" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Solution of Sparse Indefinite Systems of Linear Equations</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Saunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="617" to="629" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">FADE: Graph Drawing, Clustering, and Visual Abstraction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Quigley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eades</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Graph Drawing 2000</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>Graph Drawing 2000</meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1984</biblScope>
			<biblScope unit="page" from="183" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast Multiscale Image Segmentation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="70" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Normalized Cuts and Image Segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="731" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Characterizing Graph Drawing with Eigenvectors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pisanski</surname></persName>
		</author>
		<idno>TW20 0EX</idno>
		<imprint>
			<pubPlace>Royal Holloway; Egham, Surrey; England</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of London, Department of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An Introduction to Algebraic Multigrid</title>
		<author>
			<persName><forename type="first">K</forename><surname>Stueben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Appendix A in Multigrid</title>
		<editor>
			<persName><forename type="first">U</forename><surname>Trottenberg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Oosterlee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Schuller</surname></persName>
		</editor>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Multilevel Algorithm for Force-Directed Graph Drawing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Walshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Graph Drawing</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>Graph Drawing</meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2000">2000. 2000</date>
			<biblScope unit="volume">1984</biblScope>
			<biblScope unit="page" from="171" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Watkins</surname></persName>
		</author>
		<title level="m">Fundamentals of Matrix Computations</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
