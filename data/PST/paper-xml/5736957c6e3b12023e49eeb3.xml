<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Extreme Learning Machines: Supervised Autoencoding Architecture for Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-08-05">August 5, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Migel</forename><forename type="middle">D</forename><surname>Tissera</surname></persName>
							<email>migel.tissera@mymail.unisa.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Deep Extreme Learning Machines: Supervised Autoencoding Architecture for Classification</orgName>
								<address>
									<settlement>Neurocomputing</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Telecommunications Research</orgName>
								<orgName type="laboratory">Computational and Theoretical Neuroscience Laboratory</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Information Technology and Mathematical Sciences</orgName>
								<orgName type="institution">University of South Australia</orgName>
								<address>
									<postCode>5095</postCode>
									<settlement>Mawson Lakes</settlement>
									<region>SA</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Mcdonnell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Deep Extreme Learning Machines: Supervised Autoencoding Architecture for Classification</orgName>
								<address>
									<settlement>Neurocomputing</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Telecommunications Research</orgName>
								<orgName type="laboratory">Computational and Theoretical Neuroscience Laboratory</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Information Technology and Mathematical Sciences</orgName>
								<orgName type="institution">University of South Australia</orgName>
								<address>
									<postCode>5095</postCode>
									<settlement>Mawson Lakes</settlement>
									<region>SA</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Extreme Learning Machines: Supervised Autoencoding Architecture for Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-08-05">August 5, 2015</date>
						</imprint>
					</monogr>
					<idno type="MD5">BFA4D2AF8B9B4F864E3467711688B8EE</idno>
					<idno type="DOI">10.1016/j.neucom.2015.03.110</idno>
					<note type="submission">Received date: 29 October 2014 Revised date: 17 March 2015 Accepted date: 17 March 2015 Preprint submitted to Neurocomputing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Extreme Learning Machine</term>
					<term>Supervised learning</term>
					<term>Autoencoder</term>
					<term>Classifier</term>
					<term>MNIST</term>
					<term>CIFAR</term>
					<term>SVHN</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a method for synthesising deep neural networks using Extreme Learning Machines (ELMs) as a stack of supervised autoencoders. We test the method using standard benchmark datasets for multi-class image classification (MNIST, CIFAR-10 and Google Streetview House Numbers (SVHN)), and show that the classification error rate can progressively improve with inclusion of additional autoencoding ELM modules in a stack. Moreover, we found that the method can correctly classify up to 99.19% of MNIST test images, which surpasses the best error rates reported for standard 3-layer ELMs or previous deep ELM approaches when applied to MNIST. The approach simultaneously offers a significantly faster training algorithm to achieve its best performance (in the order of five minutes on a four-core CPU for MNIST) relative to a single ELM with the same total number of hidden units as the deep ELM, hence offering the best of both worlds: lower error rates and fast implementation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years several hardware platforms optimised for neural network implementation have been developed. These implementations range from massively-parallel custom-built System-on-Chip (SoC) silicon microprocessor arrays (e.g. SpiNNaker <ref type="bibr" target="#b1">[1]</ref>), to analog VLSI processors directly emulating the ion channels of the neurons as leakage currents in CMOS subthreshold region (e.g. Neurogrid <ref type="bibr" target="#b2">[2]</ref>). The emergence of these platforms has been accompanied by a parallel effort to develop algorithms which mimic the computational capability of the human brain, particularly in developing synthesised (engineered) neural networks. These algorithms are now utilised for both investigating brain function in computational neuroscience (for example, models of controlling eye position and working memory <ref type="bibr" target="#b3">[3]</ref>), and for implementing computing systems in machine learning. In machine learning, an emerging algorithm is the Extreme Learning Machine (ELM) <ref type="bibr" target="#b4">[4,</ref><ref type="bibr">5,</ref><ref type="bibr" target="#b6">6]</ref>, which is known to be relatively fast to train in comparison with iterative training methods, and performs with similar accuracy to Support Vector Machines (SVMs) <ref type="bibr" target="#b7">[7]</ref>. This current paper is motivated by recent work that has aimed to produce neuromorphic implementations of ELM <ref type="bibr" target="#b8">[8]</ref> and related methods <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b10">10]</ref>, based on hardware that simulates 'spiking neurons'. See <ref type="bibr" target="#b11">[11]</ref> for further discussion of neuromorphic implementations. One potential limitation of hardware implementations, or implementations on resources-constrained platforms, is the number of hidden units available for concurrent activation. Our focus is on developing an ELM algorithm that enables the number of hidden units that need to be concurrently activated to be reduced, as well as offering even faster training times, whilst maintaining good performance.</p><p>The neural network architecture of standard existing ELM approaches is a three layer feedforward structure. The first layer is the input layer, the second-the hidden layer-is activated by weighted projections of the input to non-linear sigmoid neurons, and the third and final layer is the output, consisting of units with linear input-output characteristics (see Figure <ref type="figure">1</ref>). In ELM, the connection weights between the input and the hidden layer neurons are randomly specified and remain untrained <ref type="bibr" target="#b4">[4,</ref><ref type="bibr">5]</ref>. For example, the input layer connection weights may be uniformly distributed with values between -0.5 and +0.5. This is analogous with neurobiology, in the sense that a negative connection weight inhibits a neuron's activity, and a positive weight excites neuronal activity. After projecting the input to the hidden layer, each hidden-unit's non-linear sigmoid function generates responses. Then using training data, the connection weights between the hidden and the output layer is trained in a single pass by mathematical optimisation. Only this connection weight matrix is altered during training. It is calculated by a least squares regression method such as the Moore-Penrose pseudoinverse</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Randomly weighted</head><p>all-to-all connections Input Vector (N dimensional) Hidden Layer of non-linear neurons (M dimensional)</p><p>Output Vector (L dimensional)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Solved output weights</head><p>Figure <ref type="figure">1</ref>: The network layout of an Extreme Learning Machine (ELM). The first layer is the input layer, and the second-the hidden layer-is activated by weighted projections of the input to non-linear sigmoid units. The third and final layer is the output and consists of units with linear input-output characteristics. <ref type="bibr" target="#b12">[12]</ref>. The methodology used in the above approach can be summarised as follows:</p><p>1. Using random and fixed weights, project an input layer to a hidden layer of sigmoidal units. 2. Using training data, numerically solve for the output weights between the hidden layer and the output layer by calculating the pseudoinverse of the matrix product of the hidden layer values for all training data, and the corresponding desired output responses.</p><p>This class of methods has been referred to as Linear Solutions of Higher Dimensional Interlayer (LSHDI) networks <ref type="bibr" target="#b11">[11]</ref>. It is a significant deviation from classical artificial neural network training methods. In classical artificial neural networks, the input weights are iteratively trained, rather than computing the output weights only, in a single batch. This interesting property can significantly enhance the efficiency of training since the full and final solution is obtained by mathematical optimisation of a convex function, in one single step. LSHDI methods also can solve significant problems in computational neuroscience simulations of real neurobiological neurons <ref type="bibr" target="#b3">[3]</ref>. Although widely accepted and very capable models exist at the single neuron level to mimic neurobiology, until the emergence of LSHDI, there had been no widely applicable method to synthesise (train) a network to solve multiple tasks <ref type="bibr" target="#b13">[13]</ref>. This class of methods are now emerging as the core of a generic neural compiler for creating silicon neural systems <ref type="bibr" target="#b1">[1]</ref>.</p><p>One drawback of classical ELMs is the number of neurons in its single hidden layer are typically very large and hence training the network can be computationally impractical, given a large dataset (the algorithm order of complexity for solving for the output weights is O(KM 2 ), where K is the number of training points and M is the number of hidden units <ref type="bibr" target="#b14">[14]</ref>). It also makes use of batch training, meaning that the network is trained using the entire dataset at once, which usually requires large memory and processing power. In <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b16">16]</ref> on-line training methods (as opposed to batch training) have been proposed to overcome this, but due to the large number of neurons typically used in the single hidden layer, the training time still largely depends on the size of the network, retaining an O(M 2 ) implementation complexity.</p><p>In this paper we introduce a different way to address the problem of large hidden layer sizes. Our approach takes inspiration from biology and the recent advances in deep learning architectures <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b19">19]</ref>. We show that by constructing a deep ELM network as a stack of supervised autoencoder ELM modules, and training module by module, the network training time and memory usage can be significantly improved, whilst simultaneously boosting classification error-rate performance above what can be achieved using a single ELM with the same total number of hidden units.</p><p>There have been several previous approaches to multi-layered ELM networks. Two approaches result in a similar deep network architecture to ours: (i) <ref type="bibr" target="#b20">[20]</ref>, uses unsupervised autoencoding of hidden-layer activations as a method for constructing a deep network; (ii) <ref type="bibr" target="#b21">[21]</ref> introduces a 'random shifts and kernalization' method to define the input to each hidden layer in the network. Another relevant approach is that of <ref type="bibr" target="#b22">[22]</ref>, which splits the input variables amongst a cascade of multiple ELM modules, with modules beyond the first module also receiving responses from the previous module. In the Discussion (Section 4) we describe how our approach fundamentally differs from these networks.</p><p>The advances made by our algorithm are a result two key factors:</p><p>1. selection of the untrained input weights using our recently introduced weight shaping method known as constrained receptive field ELM (RF-C-ELM) <ref type="bibr" target="#b14">[14]</ref> (which builds upon the Constrained ELM (C-ELM) method of <ref type="bibr" target="#b23">[23]</ref>), rather than selecting these weights from a random distribution; 2. we train each ELM module in the stack to both autoencode its input and classify it, and then feed both the autoencoding and the classification vectors into the subsequent module.</p><p>As we shall show, training ELM modules in the stack using both the training data and the classification results of the previous module leads to an iteratively improved classification of test data with each subsequent module. This enhancement occurs simultaneously with a reduction in the training order of complexity for the same total number of hidden units. Thus our method offers the 'best of both worlds': enhanced classification rates and enhanced runtime complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methodology</head><p>In this section, we introduce the methods that we use to construct our deep ELM network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Deep ELM architecture and unsupervised training component</head><p>We start by describing the flow of an input vector through the network. The input layer takes as input a test or training vector, X ∈ R N ×1 . This input layer is connected to the first ELM module's hidden layer (of size M ) by an all-to-all weight matrix W p1 ∈ R M ×N . We write the input vector to this layer as H 1 = W p1 X ∈ R M ×1 . The output of the i-th hidden layer unit is given by the logistic sigmoid function,</p><formula xml:id="formula_0">f [H 1,i ] = 1 1 + exp (-H 1,i ) . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>An output weights matrix W o1 is then multiplied by the hidden-layer responses to produce an approximation of the input, X ∈ R N ×1 . In order to use K training vectors to train the output weights to perform this autoencoding, we form a matrix A ∈ R M ×K in which each column contains the output of the hidden layer f [H 1 ] at one training point. Then using the training data itself as another matrix Y ∈ R N ×K in which each column contains training vectors, we solve for W o1 ∈ R N ×M that minimises the mean square error between</p><formula xml:id="formula_2">Y predicted := W o1 A<label>(2)</label></formula><p>and the set of original training images, Y. Similar to supervised training of an ELM carried out elsewhere <ref type="bibr">[5,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b14">14]</ref>, we solve this problem numerically by finding the solution to the following set of NM linear equations in NM unknown variables comprised from the elements of W o1 ,</p><formula xml:id="formula_3">YA = W o1 (AA + cI), (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where c is a regularisation parameter that can be optimised as a hyperparameter, and I is the M × M identity matrix. This trained weight matrix W o1 converts an input instance, X, into a new vector X1 = W o1 f [H 1 ], which is the autoencoded version of the original data.</p><p>We next construct the second module of the deep network. Using a new projection weight matrix W p2 ∈ R Q×N , we connect X to the second hidden layer, whose input is H 2 ∈ R Q×1 , where Q is not necessary the same as the first hidden layer size, M . Then the output weights of the second module, W o2 , are trained in the same fashion as those of the first module to produce a new auto encoded response, X2 :</p><formula xml:id="formula_5">= W o2 f [H 2 ].</formula><p>The entire process can be repeated numerous times, to form a V -module deep ELM network.</p><p>The above explanation provides a procedural description of the sequence of steps required during training. After the entire network has been trained, however, it is natural to combine the two weight matrices W o1 and W p2 to form one single weight matrix W h12 ∈ R Q×M , which fully connects the first two hidden layers, i.e.</p><formula xml:id="formula_6">W h12 = W p2 W o1 ,<label>(4)</label></formula><p>and similar for subsequent modules. This is only possible because both the input and output layers of an ELM module are linear. The approach described above is illustrated in Figure <ref type="figure">2</ref>.</p><p>We next describe the supervised aspect of the autoencoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Vector (N dimensional)</head><p>Hidden Layer #1 of non-linear neurons (M dimensional)</p><p>Intermediate Vector (N dimensional)</p><formula xml:id="formula_7">W o1 W p1 Hidden Layer #2 of non-linear neurons (Q dimensional) W p2 X X Figure 2:</formula><p>The first two hidden-layers of our supervised deep extreme learning machine network are shown. From left-to-right in feedforward: an N dimensional input vector, X is projected to the M dimensional first hidden-layer using a random weight matrix W p1 . After transformation by the sigmoidal hidden units, the result is multiplied by the output weights to produce a vector X of the same dimensionality as the input. Then the vector X is projected to a Q dimensional second hidden-layer using a random weight matrix W p2 . The output weight matrices (e.g. W o1 ) are obtained using training data by solving Equation <ref type="bibr" target="#b3">(3)</ref>. We refer to each 3-layer ELM as a module; each module contains a single hidden layer, and thus the stack of modules is a deep neural network. Additional modules can be added indefinitely, with a readout of a classification available from each intermediate layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deep ELM supervised autoencoding: 'label pixels'</head><p>In the above description, no supervision or use of training data labels was mentioned, but we now describe how these aspects can be introduced using the same general method. The approach is inspired by that used in discriminative restricted Boltzmann machines <ref type="bibr" target="#b24">[24]</ref> (see also <ref type="bibr" target="#b25">[25]</ref>). Regular restricted Boltzmann machines are generative, and train both forward weights from an input layer to a hidden layer, and backward weights from the hidden layer to the input layer. It has been shown that providing additional elements in the input layer with values that correspond to training labels during training, but are initially zero during testing, enables 'generation' of predictions of the labels of test data in these additional elements following repeated propagation of inputs to the hidden layer and back to the input layer. This method has been shown to provide good classification results on standard benchmarks <ref type="bibr" target="#b24">[24]</ref>.</p><p>Inspired by this idea, one simple approach that we use that is suitable for classification of images is to embed the training labels as pixel values in the otherwise unused borders of training images. More generally if unused borders are not available, or the data is not images, it is trivial to instead simply expand the dimension of the input data by the number of classes, just as in discriminative restricted Boltzmann machines. For the example of the MNIST dataset <ref type="bibr" target="#b26">[26]</ref> we can take the first 10 pixels in the first row of the training images, and reset them to zero (if they are not already at zero, which most are). Then we use the label of the respective image, to set the corresponding pixel to 1. For example, for label '0', pixel 1 is set to one; for label 9, pixel 10 is set to one (note that we preprocess all image data in MNIST by rescaling pixel values to the interval [0, 1]). The addition of 'label pixels' is illustrated in Figure <ref type="figure" target="#fig_0">3</ref>.</p><p>We use the resulting combined "images and labels" training set as the target matrix, Y, used to train each autoencoding ELM module in our network. Note that the first module receives as input the images only, without the supervised 'label pixels.' Hence, the matrix Y contains the labelled training images, but the input to the network's first module, X, is the raw images. In the classification step (i.e. at the output layer of each module), we use the highest valued pixel from amongst the label pixels as the classified output. As in discriminative restricted Boltzmann machines, the values of these pixels tend to converge towards the actual labels of the test data with repeated generation; the architectural difference is that we use a feedforward network to achieve this. For the more general case, we simply use the elements of the prediction vector corresponding to the labels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">RF-C-ELM input-weight shaping method</head><p>Our Deep ELM method both significantly improves the network training time relative to a single ELM with the same total number of hidden units (as shown in Section 3 below), and can improve the classification accuracy on well known datasets relative to any other single-module ELM method. Achieving the latter relies on using a method introduced recently elsewhere [14] that extends the approach of <ref type="bibr" target="#b23">[23]</ref>. We now briefly overview this method.</p><p>In the standard ELM approach, the input weights are initialised randomly, for example uniformly distributed between -0.5 and +0.5. This works reasonably well, but in <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b23">23]</ref>, it was shown to be advantageous to explicitly compute the input weights based on supervised use of the training data.</p><p>Additionally, it was shown in <ref type="bibr" target="#b14">[14]</ref> that classification performance when using ELMs can be significantly enhanced by restricting the number of input pixels or features that provide input to an ELM hidden layer. In other words, it is advantageous to ensure the input weights vector is sparse. For images, we have found that limiting the non-zero weights to each hidden unit to randomly sized and located rectangular patches, analogous to receptive fields in the mammalian visual system, gives better performance than random selection of which weights are set to zero.</p><p>Finally, we have shown in <ref type="bibr" target="#b14">[14]</ref> that combining the Constrained ELM (C-ELM) method of <ref type="bibr" target="#b23">[23]</ref> with the receptive-fields ELM (RF-ELM) method introduced in <ref type="bibr" target="#b14">[14]</ref>, is superior to either method alone. This RF-C-ELM method is readily carried out by first calculating the input C-ELM weights according to the algorithm of <ref type="bibr" target="#b23">[23]</ref>, and then applying receptive field masks to those weights as described in <ref type="bibr" target="#b14">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Input weights for the label pixels</head><p>We found that the best classification performance resulted when we reserved a fraction of the hidden units in each ELM module to receive weighted input only from the label pixels, and all other hidden units to receive weighted input only from the remaining pixels. Since there are only 10 label pixels, we did not use the RF-C-ELM method to choose the weights from label pixels to the hidden units associated with label pixels. Instead, we simply used zero-mean bipolar random weights for these weights and RF-C-ELM for all remaining weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>We firstly describe three image classification tasks that we tested our method on, and then present results on each of these benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Classification Databases 3.1.1. MNIST</head><p>The main dataset we focus on is MNIST <ref type="bibr" target="#b26">[26]</ref>, which consists of a total of 70000 handwritten digits (60000 for training and 10000 for testing) that have been converted into 28 × 28 pixel images. Although originally binary images, the standard dataset is now greyscale due to resizing with interpolation. Hence, the input layer to our deep ELMs is of size N = 784. Since we perform autoencoding, the size of the output layer is the same as the input layer, i.e. N = 784 in all input and output layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">CIFAR-10</head><p>Like MNIST, the CIFAR-10 image classification task has 10 classes of image <ref type="bibr" target="#b28">[28]</ref>. Unlike MNIST, the images are in RGB format, and are of size 32×32 pixels. We treat each of the three channels as adding additional input pixels, and hence have N = 32 × 32 × 3 = 3072. There are 50000 training images and 10000 test images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">SVHN</head><p>SVHN is the Google Streetview House Number database <ref type="bibr" target="#b29">[29]</ref>. Like MNIST, the 10 classes correspond to ten numerical digits. Unlike MNIST and like CIFAR-10, each image is RGB and of size 32 × 32 pixels. We converted each RGB image to greyscale and thus have N = 1024. Although the full database has 26032 test images and over 600000 training images, we used only the smaller 'hard' set of 73257 training images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Results</head><p>To begin, Figure <ref type="figure" target="#fig_1">4</ref> shows results for classification error rates for MNIST. Figure <ref type="figure" target="#fig_1">4</ref>(a) shows the error rate as the number of modules increases. The most important aspects that be observed are that (i) the error rate generally decreases as the number off modules increases from the single ELM case; (ii) adding more modules for a given value of M can readily surpass the performance of a single ELM with larger M . However, after some point adding more modules leads to diminished returns in the error rate. Increasing the number of modules above the values shown in Figure <ref type="figure" target="#fig_1">4</ref>(a) does not enhance performance significantly. This is consistent with data in Figure <ref type="figure" target="#fig_1">4(b)</ref>, which shows the error rate when the data used for training is classified by the trained deep ELM-note that the error rate for the training data significantly surpasses that on the test data for M = 12800, as well as for smaller M as the number of modules increases.</p><p>Figure <ref type="figure" target="#fig_2">5</ref> shows the same data from Figure <ref type="figure" target="#fig_1">4</ref>(a), but with a rescaling where the error rate is plotted against the total number of hidden units (summed over all modules) rather than than the number of modules. Although the   <ref type="figure" target="#fig_1">4</ref>, but the X-axis is scaled by the total number of hidden units in each deep ELM. Each trace is for a different number of hidden units in each module, M .</p><p>best performance is achieved with fewer modules with more hidden units per module, it is clear there is a very small performance loss in comparison with using more modules and fewer hidden units per module. In fact, we found that the best error rate can actually be achieved with fewer M per module and more modules. This is evidenced in Table <ref type="table" target="#tab_0">1</ref>, which summarises the best results we obtained for the data shown in Figs <ref type="figure" target="#fig_1">4</ref> and<ref type="figure" target="#fig_2">5</ref>. The best classification performance we obtained was 99.19% correct on the MNIST test data, which occurred for M = 6400 with 8 modules. However, this corresponds to only one error fewer than for M = 3200 with 16 modules and M = 12800 with 3 modules, and therefore given the random nature of the input layer weights, it can be concluded that smaller M and more modules can achieve best performance.</p><p>Given the O(M 2 ) algorithm for training each layer, these results suggest the deep ELM is advantageous for situations where training time is considered to be an important factor.</p><p>To evaluate this quantitatively, Figure <ref type="figure" target="#fig_4">6</ref>(a) shows how the mean time required to train a deep ELM varies with the number of hidden units per module. When the total runtime is divided by the number of layers, the result increases approximately linearly on the log-log axes as the number of hidden units per module, M , increases. This is due to the O(M 2 ) algorithm required to find the output weights for each module. This subfigure also shows that the total runtime required for deep ELMs with the same total number of hidden units (12800) increases as the number of hidden units per module, M , increases, despite needing to calculate weights for fewer modules as M increases. This result illustrates the O(V M 2 ) run-time dependency of a V -layer deep ELM. Figure <ref type="figure" target="#fig_4">6</ref>(b) further reinforces that good close to optimal performance for the method can be obtained for a deep ELM with smaller M at a significantly save on run-time.</p><p>As a comparison to illustrate that deep ELMs can be advantageous on a range of data sets, Figure <ref type="figure" target="#fig_6">7</ref> shows results for the error rate against the total number of hidden units for deep ELMs for the CIFAR-10 and SVHN datasets. In each case, adding more layers generally decreases the error rate up to a point where more layers offers diminishing returns.</p><p>Finally, in order to demonstrate that the supervision component of the deep ELM is essential for improved performance with increasing numbers of layers, we show in Figure <ref type="figure" target="#fig_7">8</ref> the error rate against increasing number of modules, when the label pixels are set to zero in all modules for training. In contrast with the data shown in Figure <ref type="figure" target="#fig_4">6</ref>, performance does not improve with added modules.   <ref type="figure" target="#fig_1">4</ref>. The total training time is defined as the total time to train a total of 12800 hidden units using between 1 and 64 layers of size from 12800 to 200. We also show in subfigure (a) the total runtime divided by the number of modules. Subfigure (b) illustrates that the runtime required for near optimal error rates can be much smaller when using multiple layers of smaller size.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>In previous work, a deep ELM structure that exploits autoencoding was proposed <ref type="bibr" target="#b20">[20]</ref>. In that method, the initial step is to train an ELM using the training data as the target, without using any labels. Then, the transpose of these trained autoencoding output weights replace the input weights in the ELM. Then, the hidden-layer activations are trained in a similar autoencoding fashion multiple times, before finally projecting into a larger hidden layer to train as the classifier output.</p><p>This process is quite different from our autoencoding deep network in three significant ways:</p><p>1. we retain untrained input weights in multiple separate ELM modules that combine to form the deep ELM; 2. we autoencode the input to each module's input layer, rather than autoencoding a hidden layer response; 3. we use the label data to train each module's output weights, thus providing supervision to the training.</p><p>The deep ELM approach of <ref type="bibr" target="#b21">[21]</ref> is similar to ours in that it combines the classification output from one ELM module with a representation of the input data. However, in <ref type="bibr" target="#b21">[21]</ref>, the original input data is used, whereas we use an autoencoding. Moreover, the method of combining is different; in <ref type="bibr" target="#b21">[21]</ref>, the output of each ELM module is randomly projected to the dimension of the input data, and then added to it. In contrast, we randomly project the 'label pixel' outputs from an ELM module to an independent set of hidden units in the next ELM module. Moreover, we use the RF-C-ELM method to specify the remaining input layer weights from input representation to hidden layer in each module.</p><p>We have found (see Section 3) that the method we propose achieves up to 99.19% correct classification on MNIST test data, which compares favourably with the best results reported for previous deep ELM methods (i.e. 99.03% in <ref type="bibr" target="#b20">[20]</ref> and 92.01% in <ref type="bibr" target="#b21">[21]</ref>), and the best result for a single ELM without use of data augmentation (99.10% reported in <ref type="bibr" target="#b14">[14]</ref>). The performance we achieved in this paper for CIFAR-10 is about 56% correct classification, which although better than the ∼43% in <ref type="bibr" target="#b21">[21]</ref>, remains significantly lower than non-ELM methods that make use of convolutions (amongst other differences) to achieve correct classification rates greater than 90% <ref type="bibr" target="#b30">[30]</ref>. Thus, it seems likely that although a deep ELM approach can achieve close to state-of-the-art performance on a relatively easy to classify database like MNIST, substantial innovations that expand the basic principles will be needed to begin to close the gap to state-of-the-art non-ELM methods. One step in this direction has been made by adding a convolutional stage prior to ELM classification <ref type="bibr" target="#b31">[31]</ref>, which enables correct classification rates on MNIST of greater than 99.5%, on CIFAR-10 of greater than 75% and on SVHN of greater than 96%. Similarly, preprocessing of CIFAR-10 prior to ELM classification has led to reported results in excess of 65% correct classification <ref type="bibr" target="#b32">[32]</ref>. In future work, it will be interesting to combine such convolutional and preprocessing approaches with the deep ELM approach described in this paper.</p><p>In other previous work, a deep ELM structure was proposed where the input variables are split into disjoint sets, and fed forward into multiple ELM modules/layers <ref type="bibr" target="#b22">[22]</ref>. The modules are combined sequentially by feeding the output of module i as an additional input to module i + 1. While we also feedforward the response of module i in our own network, we do not split the data; instead, the first layer received all input variables, and subsequent layers receive the autoencoding response of the previous layer, along with its classification response.</p><p>Our approach enables a classification to be made for every module of the deep network, with generally improved classification for subsequent modules. There is scope for future work to exploit this feature. For example, the output classification vector for each stage can be assessed in terms of confidence in whether the correct prediction is made. If, for example, one of the prediction vectors shows one class predictions as having a value close to its target value (unity in our approach) and the rest as close to zero, then confidence in a correct prediction will be high, and propagation of data through remaining layers may be unnecessary. There is therefore potential for faster test data classification than would be the case if propagation through all L modules is carried out.</p><p>In summary, using our method, we have reduced the network training time compared with a single hidden-layer network of equivalent hidden neurons. This becomes apparent as the number of hidden-layer neurons increases. This method of constructing deep neural networks is potentially favourable for hardware or online implementations, due to the reduced number of hidden units per layer, which could be a benefit to IC designs of limited size. The expected loss of performance could potentially be offset through the use of time-multiplexing that reuses the same circuitry repeatedly to compute the responses of each layer in our deep ELM.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of one way to introduce supervised 'label pixels' to training data images. Left hand side: Examples of the original MNIST Training images. Right hand side: Combined "image and label" training data. Top Right: Note the 6 th pixel has been set to 1 to indicate the label 5, and similar for the other examples. During testing, the autoencoded test images progress from having all the label pixels equal to zero initially to having label pixels generated that represent classifications of the image itself, similar to discriminative restricted Boltzmann machines [24].</figDesc><graphic coords="10,236.18,353.63,59.26,59.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Classification error rates on MNIST for deep ELMs for increasing network depth. All data was calculated as the average over 10 repeats for each condition. Subfigure (a) are results from the 10000 test images. Subfigure (b) are results from applying the trained deep ELM to the 60000 images used for the training. Each trace is for a different number of hidden units in each module, M (each module contains a single hidden layer).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Classification error rates on MNIST for deep ELMs for increasing total number of hidden units. The figure shows the same data as in Figure4, but the X-axis is scaled by the total number of hidden units in each deep ELM. Each trace is for a different number of hidden units in each module, M .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>best result each M (s) (b) Runtime-Error rate tradeoff</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Indicative run times for deep ELM. Both subfigures show the training time required prior to producing the data in Figure4. The total training time is defined as the total time to train a total of 12800 hidden units using between 1 and 64 layers of size from 12800 to 200. We also show in subfigure (a) the total runtime divided by the number of modules. Subfigure (b) illustrates that the runtime required for near optimal error rates can be much smaller when using multiple layers of smaller size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Deep ELM classification error rates on CIFAR-10 and SVHN test data as the total number of hidden units per module varies. All data was calculated as the average over 3 repeats. Each trace is for a different number of hidden units in each module, M , and each data point is the error rate after 1, 2, . . . modules. In each case the percentage error shown is calculated after each individual module in a deep ELM composed from multiple modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Classification error rates on MNIST for unsupervised deep ELM, as the number of layers increases. The figure shows that the removal of supervision from the deep ELM by setting all label pixels to zero does not lead to improved performance as the number of modules increases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of best classification performance rates (percent of correctly classified test images) on MNIST, for our deep ELM. Data was obtained from a maximum of 32 modules for M ≤ 1600 or a maximum of 51200 total hidden units for M ≥ 3200. The best result was obtained from ten repeated generations of all RF-C-ELM input layer weights in each deep ELM.</figDesc><table><row><cell>M</cell><cell cols="3">Best Result Num modules for best Total hidden units for best</cell></row><row><cell>200</cell><cell>96.56 %</cell><cell>30</cell><cell>6000</cell></row><row><cell>400</cell><cell>98.08 %</cell><cell>31</cell><cell>12400</cell></row><row><cell>800</cell><cell>98.80 %</cell><cell>31</cell><cell>24800</cell></row><row><cell>1600</cell><cell>99.05 %</cell><cell>25</cell><cell>40000</cell></row><row><cell>3200</cell><cell>99.18 %</cell><cell>16</cell><cell>51200</cell></row><row><cell>6400</cell><cell>99.19 %</cell><cell>8</cell><cell>51200</cell></row><row><cell>12800</cell><cell>99.18 %</cell><cell>3</cell><cell>38400</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Mark D. McDonnell's contribution was supported by the Australian Research Council under ARC grant DP1093425 (including an Australian Research Fellowship).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The SpiNNaker Project</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Furber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Galluppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Temple</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Plana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="652" to="665" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neurogrid: A mixed analog-digital multi chip system for large-scale neural simulations</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mcquinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Bussat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alvarez-Icaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Merolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Boahen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="699" to="716" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Eliasmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Anderson</surname></persName>
		</author>
		<title level="m">Neural Engineering: Computation, Representation, and Dynamics in Neurobiological Systems</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Extreme Learning Machine: Theory and applications</title>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Siew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="489" to="501" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Extreme Learning Machines: A Survey</title>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Machine Learning and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="107" to="122" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extreme learning machines</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="30" to="31" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An insight into extreme learning machines: Random neurons, random features and kernels</title>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="376" to="390" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Silicon spiking neurons for hardware implementation of extreme learning machines</title>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="125" to="134" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Real time on-chip implementation of dynamical systems with spiking neurons</title>
		<author>
			<persName><forename type="first">F</forename><surname>Galluppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Furber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Eliasmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Joint Conference on Neural Networks IJCNN</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Silicon neurons that compute</title>
		<author>
			<persName><forename type="first">S</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sloan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neckar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Trautmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Eliasmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Boahen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Artificial Neural Networks, Lecture Notes in Computer Science (LNCS)</title>
		<meeting>International Conference on Artificial Neural Networks, Lecture Notes in Computer Science (LNCS)<address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7552</biblScope>
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning the pseudoinverse solution to network weights</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tapson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Schaik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="94" to="100" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A generalized inverse for matrices</title>
		<author>
			<persName><forename type="first">R</forename><surname>Penrose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Mathematical Proceedings of the Cambidge Philosophical Society</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="406" to="413" />
			<date type="published" when="1955">1955</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Large-Scale Model of the Functioning Brain</title>
		<author>
			<persName><forename type="first">C</forename><surname>Eliasmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bekolay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dewolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">338</biblScope>
			<biblScope unit="page" from="1202" to="1205" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fast, simple and accurate handwritten digit classification using extreme learning machines with shaped input-weights</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Mcdonnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Tissera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Schaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tapson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.8307</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A fast and accurate online sequential learning algorithm for feedforward networks</title>
		<author>
			<persName><forename type="first">N.-Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Saratchandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1411" to="1423" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The No-Prop algorithm: A new learning algorithm for multilayer neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Widrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Greenblatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="182" to="188" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning deep architectures for ai</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations and trends in Machine Learning</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Fast Learning Algorithm for Deep Belief Nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Representational Learning with ELMs for Big Data</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L C</forename><surname>Kasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="31" to="34" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning deep representations via extreme learning machines</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="page" from="308" to="315" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical extreme learning machine for feedforward neural network</title>
		<author>
			<persName><forename type="first">H.-G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="128" to="135" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Constrained extreme learning machine: a novel highly discriminative random feedforward neural network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Joint Conference on Neural Networks (IJCNN)</title>
		<meeting>International Joint Conference on Neural Networks (IJCNN)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-07">July. 2014</date>
			<biblScope unit="page" from="800" to="807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Classification using discriminative restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning -ICML &apos;08</title>
		<meeting>the 25th international conference on Machine learning -ICML &apos;08<address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="536" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning algorithms for the classification restricted Boltzmann machine</title>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="643" to="669" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The MNIST database of handwritten digits</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="2014-08">August 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Explicit computation of input weights in Extreme Learning Machines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tapson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>De Chazal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Schaik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2889</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ELM2014, Accepted</title>
		<meeting>ELM2014, Accepted</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/kriz/cifar.html" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>Dept of CS, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation, Masters Thesis</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="http://ufldl.stanford.edu/housenumbers" />
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deeplysupervised nets</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>18th International Conference on Artificial Intelligence and Statistics (AISTATS)<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Enhanced image classification with a fast-learning shallow convolutional neural network</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Mcdonnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vladusich</surname></persName>
		</author>
		<idno>arxiv.org:1503.04596</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Constrained extreme learning machines: A study on classification cases</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.06115</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
