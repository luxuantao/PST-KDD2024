<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Provenance-based Adaptive Scheduling Heuristic for Parallel Scientific Workflows in Clouds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012-08-25">25 August 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daniel</forename><surname>De Oliveira</surname></persName>
							<email>danielc@cos.ufrj.br</email>
						</author>
						<author>
							<persName><forename type="first">Kary</forename><forename type="middle">A C S</forename><surname>Ocaña</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fernanda</forename><surname>Baião</surname></persName>
							<email>fernanda.baiao@uniriotec.br</email>
						</author>
						<author>
							<persName><forename type="first">Marta</forename><surname>Mattoso</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">COPPE/UFRJ</orgName>
								<orgName type="institution">Federal University of Rio de Janeiro</orgName>
								<address>
									<postBox>P.O. Box 68511</postBox>
									<postCode>21941-972</postCode>
									<settlement>Rio de Janeiro</settlement>
									<region>RJ</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Federal University of the State of Rio de Janeiro -UNIRIO</orgName>
								<address>
									<settlement>Rio de Janeiro</settlement>
									<region>RJ</region>
									<country>Brazil F. Baião</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Provenance-based Adaptive Scheduling Heuristic for Parallel Scientific Workflows in Clouds</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2012-08-25">25 August 2012</date>
						</imprint>
					</monogr>
					<idno type="MD5">04DEA849F503CEFF9E77FF2B14571BF7</idno>
					<idno type="DOI">10.1007/s10723-012-9227-2</idno>
					<note type="submission">Received: 30 September 2011 / Accepted: 9 August 2012 /</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cloud computing</term>
					<term>Scientific workflow</term>
					<term>Scientific experiment</term>
					<term>Provenance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the last years, scientific workflows have emerged as a fundamental abstraction for structuring and executing scientific experiments in computational environments. Scientific workflows are becoming increasingly complex and more demanding in terms of computational resources, thus requiring the usage of parallel techniques and high performance computing (HPC) environments. Meanwhile, clouds have emerged as a new paradigm where resources are virtualized and provided on demand. By using clouds, scientists have expanded beyond single parallel</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>computers to hundreds or even thousands of virtual machines. Although the initial focus of clouds was to provide high throughput computing, clouds are already being used to provide an HPC environment where elastic resources can be instantiated on demand during the course of a scientific workflow. However, this model also raises many open, yet important, challenges such as scheduling workflow activities. Scheduling parallel scientific workflows in the cloud is a very complex task since we have to take into account many different criteria and to explore the elasticity characteristic for optimizing workflow execution. In this paper, we introduce an adaptive scheduling heuristic for parallel execution of scientific workflows in the cloud that is based on three criteria: total execution time (makespan), reliability and financial cost. Besides scheduling workflow activities based on a 3-objective cost model, this approach also scales resources up and down according to the restrictions imposed by scientists before workflow execution. This tuning is based on provenance data captured and queried at runtime. We conducted a thorough validation of our approach using a real bioinformatics workflow. The experiments were performed in SciCumulus, a cloud workflow engine for managing scientific workflow execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cloud computing <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> is a recently proposed and evolving paradigm that provides computing as a service that can be purchased on demand by different types of clients following a pay-per-use model <ref type="bibr" target="#b1">[2]</ref>. Differently from Grid <ref type="bibr" target="#b2">[3]</ref> and clusters users, cloud users benefit from the concept of elasticity, as they can easily scale resources up and down according to their processing demand, in a virtually unlimited form <ref type="bibr" target="#b0">[1]</ref>. Clouds are strongly based on the concept of virtualization in which Virtual Machines (VM) play a fundamental role. This allocation of resources, on demand, provides a new dimension for High Performance Computing (HPC). Although, at a first analysis, clouds were considered not suitable for HPC applications <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, other studies, such as He et al. <ref type="bibr" target="#b5">[6]</ref> provide concrete evidences that new developments in cloud infrastructure are significantly improving the scalability and performance of cloud-based HPC systems.</p><p>Due to its elastic capability, cloud environments are beginning to be adopted as a platform for running high-performance and distributed applications, such as, large-scale scientific experiments. Clouds, however, have mostly been used for business applications, thus there is a gap between the cloud environment and the management of scientific experiments, especially ones modelled as data-intensive scientific workflows. This new scenario presents several open issues, three of them discussed as follows.</p><p>Scientific experiments are based on complex computer simulations that consume and produce large datasets and allocate huge amounts of computational resources <ref type="bibr" target="#b6">[7]</ref>. To help scientists in managing resources involved in large-scale scientific simulations, scientific workflows are gaining much interest <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>. A scientific workflow is an abstraction that structures the steps of a scientific experiment as a graph of activities, in which nodes correspond to data processing activities and edges represent the dataflow between them <ref type="bibr" target="#b7">[8]</ref>. Scientific Workflow Management Systems (SWfMS) allow for defining, executing and monitoring workflow execution. Additionally, a scientific experiment is only considered "scientific" if it can be reproducible. To obtain the same results, scientists have to analyze data from previous executions. This data is called provenance <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>As the complexity of the scientific workflow grows (e.g. exploration of thousands of parameters and in many cases using repetition structures over dozens of complex activities), running these workflows demands parallelism and powerful HPC capabilities that can be provided by existing clouds. There are several scientific workflows that were designed to be executed in parallel (exploring parameter sweep <ref type="bibr" target="#b11">[12]</ref> or data fragmentation <ref type="bibr" target="#b12">[13]</ref>) that demand HPC capabilities, such as Montage, a workflow for astronomical analysis <ref type="bibr" target="#b13">[14]</ref>, EdgeCFD, a computational fluid dynamics workflow <ref type="bibr" target="#b14">[15]</ref>, X-Ray crystallography analysis <ref type="bibr" target="#b15">[16]</ref>, orthologous detection <ref type="bibr" target="#b16">[17]</ref>, phylogenomic analysis <ref type="bibr" target="#b17">[18]</ref>, evolutionary analysis <ref type="bibr" target="#b18">[19]</ref> and phylogenetic analysis <ref type="bibr" target="#b19">[20]</ref>. Each one of these workflow executions is composed by hundreds or thousands of parallel tasks that consume several hundreds of files and produce thousands of other files. For each execution in the cloud, scientists should inform some restrictions such as the deadline (the maximum time to wait for workflow completion) and the limit budget (the maximum amount of money to be paid). According to this information the workflow tasks have to be scheduled and executed in parallel on the VMs of the cloud.</p><p>Task scheduling is a well-known NP-complete problem even in simple scenarios <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref>. Therefore, many heuristics have been proposed to address the scheduling problem in different platforms (clusters, Grids). In the last few years many static heuristics have been proposed <ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref> for clusters and Grids. These heuristics try to generate an optimal (or sub-optimal) scheduling plan, i.e. to allocate activities of a workflow onto a set of available machines prior to the workflow execution.</p><p>However, scheduling and executing parallel scientific workflows in clouds is still a challenge due to several reasons. Firstly, cloud environments have a unique pricing model that have to be considered to fit in the budget informed by scientists. In clusters and Grids, there is an initial investment (sometimes huge) and a small operational cost paid over time (to pay for energy consumption or maintenance team, for instance). On the other hand, cloud providers allow for acquiring resources on demand, based on pay-per-use pricing model, e.g. users pay per quantum of time used (e.g. minutes, hours, days, etc.).</p><p>Secondly, clouds are changing environments and they may be susceptible to performance variations during the execution course of the workflow, thus requiring adaptive scheduling solutions. Elastic scaling of resources (hardware and software capabilities) is a key characteristic of clouds. To provide this feature, resources are allocated and reallocated as needed by the provider and the users are not aware of those changes. For example, if scientists execute their workflows using Amazon EC2's spot instances <ref type="bibr" target="#b27">[28]</ref>, VMs can be unallocated or moved as the provider needs more CPU capacity. In the case of Amazon, the demand varies during the years, which means that they may need more resources to handle their Christmas rush than they do the rest of the year, for example. These moves and instabilities can produce negative impacts on the scheduling of workflow tasks.</p><p>Thirdly, in clouds, VM failures are no longer an exception but rather a characteristic of these environments. Although the cloud provider tries to guarantee the reliability of the environment, the workflow scheduling has to consider reliability issues when distributing parallel tasks to execute on several distributed VMs.</p><p>In this paper, we present an approach based on a heuristic for adaptive scheduling of parallel scientific workflows in cloud environments. This approach is based on a 3-objective weighted cost model that considers execution time, reliability and financial cost simultaneously. This cost model was based on the ideas of Boeres et al. <ref type="bibr" target="#b24">[25]</ref>. By using this cost model, an adaptive greedy scheduling algorithm schedules workflows tasks into the many VMs and a load balancing algorithm instantiates and destroys VMs to meet the deadline and to fit in the budget informed by scientists.</p><p>The proposed cost model and the greedy scheduling algorithm are highly based on the provenance data of experiments captured and queried at runtime by the cloud workflow engine. Besides providing information that allows for reproducing experiments, the provenance repository also provides the necessary information from previous executions to estimate the execution time of tasks, to analyze volume of data produced by a task or to discover which tasks share input files. This type of information is fundamental for the proposed cost model and scheduling algorithm and allows for a fine tuning.</p><p>In order to be evaluated, this approach is implemented within SciCumulus<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>. SciCumulus is a cloud workflow engine that supports parallel execution of scientific workflows in clouds with coupled provenance support. Differently from the current mainstream, SciCumulus automatically adapts the workflow execution according to the current state of the environment, thus benefiting from elasticity. It automatically creates VMs and configures virtual clusters based on these VMs following restrictions defined by scientists. In addition, SciCumulus generates runtime provenance data, i.e. registers information about the workflow being executed and makes this data available for queries at runtime. This allows for the proposed approach to adapt more efficiently. In this way, the main contributions of this paper are:</p><p>1. A 3-objective weighted cost model for scheduling scientific workflows on the cloud considering total execution time, financial cost and reliability. 2. An adaptive greedy scheduling algorithm to explore the space of alternative schedules considering changes in the cloud environment and the proposed cost model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A thorough experimental evaluation based on</head><p>the implementation of the proposed approach in SciCumulus cloud workflow engine. We use SciPhy <ref type="bibr" target="#b19">[20]</ref>, a real scientific workflow for phylogenetic analysis on a 128-core virtual cluster in Amazon EC2 environment that shows the performance benefits of the proposed approach.</p><p>The paper is organized as follows. Section 2 brings related work. Section 3 discusses the formalism for scientific workflows used in this paper. Section 4 introduces the 3-objective cost model that takes into account execution time, financial cost and reliability. Section 5 presents the adaptive greedy scheduling algorithm. Section 6 details SciCumulus architecture used in our experiments, Section 7</p><p>presents the bioinformatics scientific workflow used as case study. In Section 8, results of the experimental evaluation are analyzed; comparing the proposed approach with existing solutions and finally, Section 9 concludes the paper and points out some future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The adaptive approach based on a cost model presented in this paper was strongly inspired by wellestablished adaptive query processing techniques <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref>. In adaptive query processing the focus is on using runtime feedback from database systems (DBMS) to modify query processing in a way that provides better response time and more efficient CPU utilization. In the context of this paper, the query is equivalent to the execution of a task that consumes some data and a combination of parameter values. Since we did not find scheduling algorithms that considered cloud elasticity issues and scientific workflows computations we have structured the related work section in two parts.</p><p>In the first one we discuss existing scheduling algorithms based on multi-objective cost models, not necessarily for clouds. In the second part we present existing approaches for executing scientific workflows in parallel on the cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multi-objective Scheduling</head><p>In the literature, there are some works that focus on multi-objective scheduling algorithms, more specifically in bi-objective scheduling algorithms.</p><p>In general, bi-objective scheduling considers execution time and reliability of the system. However, no work was found that considers financial costs. Assayad et al. <ref type="bibr" target="#b26">[27]</ref> introduce a bi-objective scheduling heuristic for DAG of activities according to: the minimization of the schedule length (execution time), and second the maximization of the system reliability. It also uses the active replication of activities to improve reliability. Since they focus on clusters, they do not take into account the financial cost to be minimized. Qin et al. <ref type="bibr" target="#b25">[26]</ref> present a dynamic scheduling heuristic for parallel real-time jobs on heterogeneous clusters. It is assumed that jobs are modeled by DAGs and that they arrive at the system following a Poisson process <ref type="bibr" target="#b33">[34]</ref>. The algorithm considers the reliability measure and there is an admission control so that a parallel real-time job whose deadline cannot be guaranteed is rejected. Although this approach focuses on meeting deadlines (restriction) it does not take into account the financial cost or limit budget informed by scientists as well.</p><p>Boeres et al. <ref type="bibr" target="#b24">[25]</ref> introduce a cost function developed for the Makespan and Reliability Cost Driven (MRCD) algorithm that integrates reliability and performance (execution time) objectives simultaneously in the same formula. This article was the inspiration for the cost model proposed in our approach. However, Boeres et al. do not consider financial costs involved in the execution and the variation of the resources since it was designed to execute in "static" environments such as computing clusters. This cost function was chosen to be extended in this paper because it allows for fine tuning the criteria based on parameter weights, which was not found in other related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Approaches for Executing Workflows in Clouds</head><p>The MapReduce model <ref type="bibr" target="#b34">[35]</ref> recently gained attention as a model for parallel programming. A range of applications can be parallelized by using this programming model. In several applications, embarrassingly parallel <ref type="bibr" target="#b35">[36]</ref> can greatly benefit from this model. However, MapReduce implementations are not focused on parallelizing scientific workflow activities. Thus, in workflows, MapReduce frameworks have to be coupled to existing SWfMS. There are several SWfMS that show coupled MapReduce implementations to their workflow engines such as Kepler + Hadoop <ref type="bibr" target="#b36">[37]</ref>, MapReduce VisTrails <ref type="bibr" target="#b37">[38]</ref> and View <ref type="bibr" target="#b38">[39]</ref>. Each one of these SWfMS provides components to implement Map and Reduce functions in its architecture. Despite being presented as a contribution in the articles, the use of cloud is not clear in the proposals. In all approaches the components to be connected to the cloud are not native of SGWfC. In the case of Kepler, the experiments were performed in a cluster using the word count activity as a case study <ref type="bibr" target="#b36">[37]</ref>, which does not necessarily reflect the scenarios found in scientific workflows.</p><p>One of the existing approaches is Nimrod/K <ref type="bibr" target="#b39">[40]</ref>. It proposes mechanisms to provide Many Task Computing (MTC) capabilities to scientific applications. Nimrod/K has a set of components and a run time machine for SWfMS Kepler <ref type="bibr" target="#b40">[41]</ref>. Nimrod/K is based on an architecture that uses tagged dataflow concepts for parallel machines. These concepts are implemented as a Kepler director that orchestrates the execution on clusters, Grids and clouds using MTC. Nimrod/K focused on parameter exploration, and differently from SciCumulus it is dependent of Kepler and relies on the pre-existence of a configured cloud environment, thus it does not configure and instantiate the VMs on demand. Different from SciCumulus, Nimrod/K does not adapt the execution of the parameter sweep according to the changes of the environment and to meet deadline and budget constraints.</p><p>Hoffa et al. <ref type="bibr" target="#b41">[42]</ref> propose the use of VM clusters with SWfMS Pegasus <ref type="bibr" target="#b42">[43]</ref> to evaluate the tradeoffs between running scientific workflows in a local environment and running in a virtualized environment. In this pioneer article, their perspective was not to analyze detailed performance metrics, but show that domain scientists can use different environments to perform their experiments. Although they coupled a virtualized environment to a SWfMS, the approach was specific for Pegasus. In addition, the experiments presented by Hoffa et al. do not concern about adaptive mechanisms, i.e. the scheduling of new activities is based on the status of the cloud environment in the beginning of the workflow execution. The lack of a detailed study and performance and scalability metrics of running parallel scientific workflows in cloud environments prevents the reuse of the approach.</p><p>Warneke and Cao <ref type="bibr" target="#b44">[44]</ref> propose Nephele, a data processing framework that explicitly exploits the dynamic resource allocation offered by clouds. Nephele schedules tasks of a specific job to different types of VMs and manages their instantiation and termination during the course of the execution of a specific job. However, Nephele is disconnected from the concept of scientific workflows and it is focused on parallel data processing, thus not providing parameter sweep features. In addition, the adaptive algorithm of Nephele only considers the number of VMs involved in the execution, and it does not try to adapt the size of the task to be executed either to meet deadlines or to fit into specified budgets.</p><p>There is some work in the literature that deals with adaptive scheduling algorithms for cloud environments even when they are not directly related to scientific workflows. Nuage, proposed by Lee et al. <ref type="bibr" target="#b45">[45]</ref> is one of these approaches and it uses evolutionary game theory to provide an adaptive and stable application deployment in cloud environments. Differently from SciCumulus, Nuage is focused on the deployment of business applications and it tries to adapt the volume of requests of each application according to the available VMs to use. However, the applications deployed using Nuage did not connect to the concept of scientific workflows and scientific experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Scientific Workflow Formalism</head><p>The scientific workflow formalism presented in this paper was inspired on Ogasawara et al. <ref type="bibr" target="#b46">[46]</ref>. In this paper, a scientific workflow is defined by a Directed Acyclic Graph (DAG) named W(A,Dep). Nodes (A) in W correspond to all workflow activities to be executed in parallel and the edges (Dep) are associated to data dependency between activities in A. This way, A = {a 1 , a 2 , . . ., a n }.</p><p>Given</p><formula xml:id="formula_0">a i |(1 ≤ i ≤ n), let I = {i 1 , i 2 , .</formula><p>. ., i m } be the input dataset for activity a i , then Input(a i ) ⊃ I. Also, let O be the output data produced by a i , then Output(a i ) ⊃ O. A specific activity in A is modeled as a(time) where time is the total execution time of a.</p><p>The dependency between two activities is modeled as dep a i , a j , ds ↔ ∃O k ∈ Input a j O k ∈ Output (a i ) and ds is the size of data transferred between these two activities. Given a workflow W, a set Ca = {ca 1 , ca 2 , . . ., ca k } of cloud activities <ref type="bibr" target="#b15">[16]</ref> (a synonym of task in the context of SciCumulus workflow engine) is created for its execution. Each cloud activity ca i is associated to a specific a i which is represented as Act(ca i ) = a i . A set of cloud activities of a workflow activity a i is denoted as Ca(a i ).</p><p>Each ca i consumes its own input data InputCa(ca i ) and produces output data OutputCa(ca i ). We establish the dependency between two cloud activities as DepCa(ca i , ca j , ds) ↔ ∃r ∈ Input(ca j ) r ∈ Output(ca i ) ∧ dep(Act(ca i ), Act(ca j ), ds). A specific cloud activity ca i is modeled as ca(time) where time is the total execution time of ca.</p><p>Data parallelism in scientific workflows is characterized by the simultaneous execution of various cloud activities of one activity a i of the workflow W, where each ca i processes a disjoint subset of the entire input data, which can be represented by a fragment set F = { f i 1 , f i 2 , . . ., f i k }. The set of fragmented output data fo k generated at each k-th execution have to be merged (following a specific merging criteria) in order to produce the final result (O i ).</p><p>Parameter sweep parallelism, which is presented in our experimental evaluation, is defined as simultaneous executions of cloud activities, where each ca i consumes a specific subset named Pv i which is the combination of the set of parameters' domain Pt of an activity a i to be processed. It can be achieved by generating several cloud activities of a specific activity and each ca i processes a specific subset of the possible parameters values Pv i . For example, suppose that two cloud activities ca i and ca l have parameters pt 1 and pt 2 . Suppose also that the domain of possible values for parameters pt 1 and pt 2 are Dpt 1 = {x, x } and Dpt 2 = {y, y }. Let us consider that ca i consumes values x and y for parameters pt 1 and pt 2 respectively, and ca l consumes values x and y . This way, we can define the combinations Pv 1 = {x, y} and Pv 2 = {x , y }. Consequently, the parameter sweep parallelism can be achieved by processing Pv 1 and Pv 2 in parallel. The several Pv i are formed by taking the parameter values from the domain in a given order. Thus, Pv 1 is formed by Dpt 1 <ref type="bibr" target="#b0">[1]</ref> and Dpt 2 <ref type="bibr" target="#b0">[1]</ref>. Pv 2 is formed by Dpt 1 <ref type="bibr" target="#b1">[2]</ref> and Dpt 2 <ref type="bibr" target="#b1">[2]</ref>, and successively. The set of output data O k generated by the k-th execution needs to be aggregated, since they refer to different executions of the same experiment. In this type of parallelism, Iis the same for each ca i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Proposed 3-Objective Cost Model</head><p>A cost model consists of a set of formulas that estimates the costs involved of performing a task in a particular computing environment <ref type="bibr" target="#b47">[47]</ref>, which in our case is the cloud environment. A cost model is usually implemented in a system that performs several calculations to choose a specific solution. In the context of this paper, this system is SciCumulus cloud workflow engine (detailed in Section 6) that needs the proposed cost model to schedule several cloud activities on a changing set of VMs. The proposed cost model was based on the ideas of Boeres et al. <ref type="bibr" target="#b24">[25]</ref>.</p><p>Let us consider VM = {VM 0 , VM 1 ,..,VM m-1 } as the set of m virtual machines that composes a virtual cluster and that are available for general use, and each VM j is associated to a specific computational slowdown index (csi) <ref type="bibr" target="#b24">[25]</ref>. The csi value is used in the model since cloud environments typically classify their VM according to processing power. To obtain this information we may query the provenance repository (that is detailed in Section 6) or we can execute a toy program (it can be a simple program to multiply two numbers, or to merge two different input files) to estimate the performance of the VMs. There may be different VMs with different hardware configurations, which produce different performance information related to the toy program. This metric is calculated to be inversely proportional to the computational power of VM j . Then, a specific virtual machine in VM is modeled as vm(network, csi, memory, maxGranfactor, part-GranFactor, avgTime, prevExecTime) where network is related to the network bandwidth of this VM, csi is the computational slowdown index, memory is the RAM capacity of this VM, max-GranFactor, partGranFactor, avgTime and pre-vExecTime are information about the previous executions of this VM and they are further explained as follows. The computational slowdown index of a VM can be also represented as csi(VM j ). Let P(ca i ,VM j )be the expected execution time of a cloud activity ca i in VM j in the virtual cluster, this way P(ca i ,VM j ) = ca i .time × csi(VM j ).</p><p>Let us also consider ϕ(W,VM) as a scheduling of all cloud activities of CA of W on VM. Formally, given a workflow W that includes a set of activities A = {a 1 , a 2 . . . , a n } and a set CA = {ca 1 , . . . , ca k } of cloud activities created for parallel workflow execution, let ϕ(W,VM) = {sched 1 , sched 2 , . . . , sched k } for CA. Let us also consider as sched(ca i , VM j , start, end) where start and end are the start and end time of a cloud activity ca i in VM j . Since the cloud environment is a changing environment (VMs are instantiated and destroyed during the course of the workflow) we cannot create an a priori scheduling plan. This way, the several sched have to be generated during the course of the experiment. We define ord(sched i ) to be the position of sched i in the sequence of all schedules. We say that sched i &lt; sched j ↔ ord(sched i ) &lt; ord(sched j ).</p><p>Since SciCumulus creates a virtual cluster to execute scientific workflows in parallel, let us consider |VM| the number of VMs in the virtual cluster. Then we may represent the cloud activity processing rate (i.e., the number of cloud activities processed per unit of time) of a virtual cluster as (upper bound):</p><formula xml:id="formula_1">β = |V M| × |C A| k i=0 P ca i , V M j (1)</formula><p>Equation 1 presents the ratio between all possible execution combinations of cloud activities and VMs and the total time needed to execute these cloud activities. Since (1) represents the cloud activity processing rate in a virtual cluster, let us define σ as the number of cloud activities in a wait queue. Then we may define the makespan of all k cloud activities in a scientific workflow by <ref type="bibr" target="#b24">[25]</ref>:</p><formula xml:id="formula_2">MS = σ β = k i=0 P cai i , V M j |V M| × |C A| (2)</formula><p>Before scheduling a scientific workflow in the cloud using this model we need to estimate the value of P(ca i , V M j ) before we can estimate the makespan MS. We are able to use the average value of P(ca i , V M j ) in all previous executions of workflows by querying this information on the provenance repository of SciCumulus or using an off-the-model estimation. This off-the-model estimation has to be used when there is no provenance data available. When we have available data of previous executions we can analyze the average execution time and use this information as input to the proposed cost model. Note that all information used in this cost model is available at the proposed provenance schema (as detailed in Section 6). VMs present performance fluctuations: they may fail and may be created or destroyed during workflow execution. In this model we assume that VMs can fail following a Poisson distribution F(VM j )∀ VM j ∈ VM with constant value. F(VM j ) expresses the probability of a given number of failures in VM j occurring in a fixed interval of time (during workflow execution) independently of the time since the last event (last VM failure). This information can also be gathered from the provenance repository if the VMs are the same of previous workflow executions. The reliability cost R(ca i ,VM j ) is defined as:</p><formula xml:id="formula_3">R ca i , V M j = F V M j × P ca i , V M j (3)</formula><p>It has to be minimized so the reliability gets near 1. This way, the total reliability is defined by the sum of the reliability of all cloud activities involved in the execution. Considering γ (VM j ) as the set of cloud activities already executed in VM j , the reliability cost associated to VM j is:</p><formula xml:id="formula_4">R p V M j = ∀X ∈γ (VMj) RC ca i , V M j (4)</formula><p>Another important subject to be considered in our cost model is the network cost. This cost is mainly associated to data transferring from and to the cloud. Since we are in the SciCumulus context, let us assume that the workflow W reads and writes information to a shared area <ref type="bibr" target="#b15">[16]</ref>, this way all possible schedules in ϕ(W,VM) read and write the same amount of data from/to the same shared area. So the cost of reading and writing is always the same for each schedule.</p><p>To model the cost of transferring data let us consider ca i to be a cloud activity associated to activity a i of (Act(ca i ) = a i ) that belongs to W defined as ca i (time) with sched(ca i ,VM k ). From ca j cloud activity associated to activity a j (Act(ca i ) = a j ) with DepCa(ca i , ca j , D cai → caj ) and sched(ca j ,VM w ). Considering VM k = VM w we have to insert a new data dependency in W to represent the data transferring between ca i and the shared area and from shared area to ca j . This way, we have to replace depCa(ca i , ca j , D cai → caj ) by depCa(ca i , dm w , D cai → caj ) and depCa(dm w , ca j , D cai → caj ) and define sched(dm w , VM k ) and sched(dm w , VM x ). The execution time is defined by:</p><formula xml:id="formula_5">dm w .time = Dcai → caj min (V M k .network, V M x .network) (5)</formula><p>This way, the total transfer time (TT) for all cloud activities of W is (assuming that there are (k + 1) transfers for k cloud activities):</p><formula xml:id="formula_6">TT = k i=0 dmw.time (6)</formula><p>The total monetary cost M(ϕ) depends on the pricing scheme of the cloud provider (such as Amazon EC2 or GoGrid). Most of the existing cloud providers calculate the payment based on quantum of time, i.e. you pay a specific value per quantum of time, and which varies is the granularity of the quantum. For example, Amazon EC2 uses one hour quantum while when using GoGrid prepaid plans where we pre-pay an amount and the provider discounts usage and overage rates per minute. Due to this reason in this model we were inspired in the ideas of Kllapi et al. <ref type="bibr" target="#b48">[48]</ref>, our total time as a sum of several quanta of time Qt which generated the set δ = {δ 1 , δ 2 , . . . , δ x ) of quanta. For each VM j involved in the execution we have to pay a value per quantum (hour, minute) that we named Vq. Let us consider ω(δ i ,VM j ) as a function to return if the VM j is executing a cloud activity in the quantum δ i .</p><formula xml:id="formula_7">ω δ i ,VM j 1, if VM j is running a cloud activity in δ i 0, elsewhere</formula><p>We have also to consider the time that the VMs need to transfer data. This way, our monetary cost is separated in two main parts. The first one is related to the cloud activities execution cost and the second one is related to data transfer cost. Thus, the total execution monetary cost is defined by the following equation:</p><formula xml:id="formula_8">M (ϕ) = Vq × |V M| j=1 MS / Qt j=1 ω δ i , V M j + Vq × k i=0 dmi.time<label>(7)</label></formula><p>The transfer financial cost Tr(ϕ) also depends on the pricing scheme of the cloud provider (such as Amazon EC2 or GoGrid). Let us consider Vt as the value the scientist has to pay for each unit of transferred data. This way, the total amount to be paid for data transferring is:</p><formula xml:id="formula_9">Tr (ϕ) = Vt × k-1 j=0 k-1 i=0</formula><p>depT.ds ca i , ca j <ref type="bibr" target="#b7">(8)</ref> The proposed cost model is then based on execution time, financial cost and reliability criteria.</p><p>To schedule based on these three objectives we use a weighted cost model where the scientist has to inform the weight of each criterion (it varies from scientist to scientist or from experiment to experiment). This way, for each VM j in VM that is idle and request for a cloud activity, it is then performed a search for the best ca i in the list of available cloud activities Ca (the ones ready to be executed) to execute in VM j following the 3objective cost model <ref type="bibr" target="#b8">(9)</ref>. Given VM j ∈ VM as the next VM to execute a cloud activity, we have to find ca i ∈ Ca which minimizes the following cost function:</p><formula xml:id="formula_10">f ca i , V M j = α 1 × P ca i , V M j + k j=0 depT.ds (cai, caj) V M j .network +α 2 × R ca i , V M j + α 3 × Vh × P ca i , V M j + k j=0 depT.ds (cai, caj) V M j .network +Vt × k j=0 depT.ds ca i , ca j (9)</formula><p>Equation 9 is composed by aforementioned formulas for calculating execution time, reliability cost and monetary cost. Note that the chosen ca i is the one that satisfies</p><formula xml:id="formula_11">F(ca i , V M j ) = min ∀ V M j ∈ V M{ f (ca i , V M j )}.</formula><p>The weight associated to each criterion is a variable in the form 0 ≤ α i ≤ 1 which represents the level of relevance for each criterion for the scientist (i = 1, 2, 3). By allowing scientists to inform α i , we provide a way to perform a fine tuning in the workflow execution parameters. The search space for this cost model is a 3-objective one since we try to find the best set of available cloud activities to execute for a specific VM following execution time, reliability and financial cost criteria. This cost model extends the one proposed by Boeres et al. <ref type="bibr" target="#b24">[25]</ref> by adding a new financial dimension that is fundamental when executing workflows in clouds.</p><p>This cost model has to be part of a scheduling algorithm and implemented in a computational system. In the following sections we present the adaptive greedy scheduling algorithm proposed in this paper and the cloud workflow engine where both cost model and the greedy scheduling algorithms are implemented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">The Adaptive Scheduling Algorithm</head><p>In clouds, the search space is three dimensional (execution time, reliability and financial cost). In fact, the scheduling solutions represent the tradeoffs between these three criteria. In this paper we focus on solving four different scenarios: (S1) to find the fastest scheduling plan; (S2) to find the plan with the less financial cost; (S3) to find the most secure scheduling plan where cloud activities present minimum fail and (S4) to find a balanced scheduling plan (each one of the criterion-total execution time, reliability and financial costhave equal weight). Actually, to implement each one of these scenarios we have to set different values of α i (0 ≤ α i ≤ 1) in order to focus on each specific criterion.</p><p>However, independently of the particular scenario that is being solved we execute the same scheduling algorithm, just varying α i in the cost model. The proposed adaptive approach is based on a greedy scheduling algorithm <ref type="bibr" target="#b49">[49]</ref> and a load balancing algorithm that scales up and down VMs involved in the execution to meet deadline and the limit budget informed by scientists. This adaptive approach is composed by four algorithms: (i) a greedy scheduling algorithm; (ii) a cloud activity grouping algorithm; (iii) a cloud activity perform algorithm and (iv) a load balancing algorithm. We explain each one of those algorithms following in this section.</p><p>Algorithm 1 is responsible for choosing the most suitable cloud activity to execute for a given idle VM based on the proposed cost model. Actually, this algorithm chooses the best group of cloud activities (that are encapsulated into a new cloud activity as presented in Algorithm 2) to execute in a specific VM. Algorithm 1 starts by loading the list of available cloud activities without dependency, i.e. ready to be executed, (line 3) and the list of available VMs (line 4). After that, the algorithm initializes the VMs with fundamental information to generate groups of cloud activities (lines 5-10).</p><p>When all VMs are initialized, the algorithm starts analyzing if there are available cloud activities to be executed (line 11). If there is at least one cloud activity to execute the algorithm searches for idle VMs to execute the available cloud activities (line 12). It is important to highlight that the algorithm orders the available VMs according to VM capacity and the scenario. If the scenario is S1, then the algorithm orders the VMs from the least powerful VM (smallest csi) to the most powerful VM (largest csi). On the other hand, if the considered scenario is S2 then the algorithm orders the list of available VM from the most powerful one (largest csi) to the least powerful one (smallest csi). If there is an idle VM, the algorithm analyzes if there is a granularity factor or a partial granularity factor associated to this VM. This factor is used to group available cloud activities to encapsulate in a new one (lines 13-20). Then, based on these new cloud activities (clusters) the algorithm calculates the most suitable group to execute in the VM by using the proposed cost model (lines 21-27). After that, the list of available cloud activities and available VMs are updated (lines 28-29). At the end of the algorithm the final schedule plan is provided (line 32).</p><p>The main focus of Algorithm 2 is to produce new cloud activities by encapsulating two or more cloud activities into a new one. The focus is to reduce communication and data transfer overhead. Every time a VM sends a message requesting for a new cloud activity we face a communication overhead. If we can group cloud activities that consume (or partially consume) the same input files we could reduce this overhead. By doing this, we avoid data transfers that are costly in cloud environments. For example, let us suppose that cloud activity ca 1 consumes files f 1 and f 2 and cloud activity ca 2 consumes f 1 , f 2 and f 3 . If we schedule ca 1 to VM 1 and ca 2 to VM 2 we have to transfer 2 files to VM 1 and 3 files to VM 2 while if we encapsulate ca 1 and ca 2 into a new cloud activity ca 3 we have to transfer 3 files to the chosen VM. However, one of the fundamental challenges is how to determine group sizes and which cloud activities have to be grouped.</p><p>This way we dynamically tune group size based on existing provenance data (to estimate execution time and to verify which files are consumed and produced by each activity). By analyzing provenance data related to previously executed experiments we are able to estimate the execution time of the new cloud activity (two or more cloud activities encapsulated into a new one). In addition, we aim at maintaining the same average execution time of cloud activities associated to the VM.</p><p>Algorithm 2 starts by verifying if the granularity factor is null and in this case the group is impossible to be set (line 2). If the number of available cloud activities to be grouped is inferior to the granularity factor, just one group is formed (line 6). On the other hand, if there are more cloud activities than the value of the granularity factor, the algorithm tries to generate several groups by choosing available cloud activities (that preferentially share some input files to avoid data transferring) to be grouped by maintaining an approximate average execution time (lines 11-27).</p><p>Algorithm 3 is responsible for setting up the granularity factor for each VM in the system. This information is a prerequisite for Algorithm 2. This algorithm starts by executing the cloud activity (or group of cloud activities) (line 1). After that it calculates the average execution time (line 3) and the average execution time of the previous execution (line 4) on the VM. If the new average time is inferior to the previous average time (line 5) the algorithm increases the granularity factor (line 9). On the other hand if the new average time is superior to the previous average time we set the maximum granularity factor (line 6).</p><p>Algorithm 4 focuses on allowing the system to adapt the amount of resources (VMs) to fit the deadline and the limit budget informed by scientists. This way, this algorithm allows scientists to quickly scale virtual cluster capacity, both up and down, as your computing requirements (deadline and budget) cannot be meet.</p><p>Algorithm 4 starts by verifying if the throughput (cloud activities finished per second) has varied (line 6). If not, nothing changes in the set of available VMs. If the throughput has reduced, the algorithm simulates the new makespan (MS') and the new financial cost (MC') (lines 7-8). If the new makespan is larger than the deadline and the financial cost is smaller than the informed budget, the number of VMs is scaled up in order to meet the deadline (lines 9-15). On the other hand, if the deadline is met but the financial cost is over budget, then the algorithm scales down the number of VMs involved in the execution (lines <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref>. This reduction in the number of VMs may cause some interruption in the cloud activities. This way, some cloud activities should be rescheduled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">SciCumulus Conceptual Architecture</head><p>This section briefly describes the architecture of SciCumulus cloud engine used as the computational infra-structure to perform the parallel execution of scientific workflows in clouds and the data provenance model used as basis of the cost model. Both cost model and the algorithms proposed in previous sections are implemented in SciCumulus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">SciCumulus Conceptual Architecture</head><p>SciCumulus provides support for two types of parallelism: parameter sweep <ref type="bibr" target="#b11">[12]</ref> and data parallelism <ref type="bibr" target="#b50">[50]</ref>. SciCumulus is designed to distribute, control and monitor parallel execution of scientific workflow activities (or even entire scientific workflows) started from a SWfMS into a cloud environment, such as Amazon EC2 <ref type="bibr" target="#b27">[28]</ref>. It is itself distributed and it is based on four-tier architecture: It is responsible for executing programs encapsulated in cloud activities and to store provenance data, iv Data Tier: This tier is responsible for storing input data and provenance data consumed and generated by the parallel execution of the workflow. In addition, this tier has information about the environment characteristics collected by an autonomous agent <ref type="bibr" target="#b52">[52]</ref>.</p><p>SciCumulus provides the minimal computational infra-structure to support workflow parallelism with provenance gathering. SciCumulus architecture is simple and may be deployed in any cloud environment (such as Amazon EC2) and connected to any existing SWfMS, diminishing effort from scientists.  The Distribution Tier manages the execution of parallel activities in cloud environments by creating and managing cloud activities that contain the program to be executed, its parallel strategy, parameters values and input data to be consumed. Specifically in this tier we have two components that have to be better explained. The first one is the encapsulator. The encapsulator generates all cloud activities to be executed following a specific fragmentation process or a parameter sweep as presented in Section 3. Based on these cloud activities the Scheduler defines which VMs receive a specific cloud activity to execute based on VM request for cloud activities. At this point, the scheduler executes the proposed greedy algorithm to determine the best cloud activity for a specific VM that is idle. Note that the VMs may be provided by any cloud provider. The scheduler has to take into account the available VMs, the permissions to access VMs and the computational power of each one of them (information that is retrieved from the provenance repository of the data tier). The scheduler in SciCumulus may work in two different modes: a static one and an adaptive one. In the static mode, the scheduler considers a fixed amount of available VMs in the beginning of the execution. On the other hand, the adaptive mode aims at analyzing the state of the cloud environment during the course of the execution in order to use more VMs and to adapt the number of cloud activities to be sent to a VM according to the available VMs.</p><p>The Execution Tier is responsible for invoking the programs associated to the activities of the workflow in many VMs available for use. The components of the execution tier may be deployed in more than one cloud provider. Finally, the Data Tier contains all repositories of data used by SciCumulus. It has the provenance repository that contains important provenance data <ref type="bibr" target="#b10">[11]</ref> collected during the course of the workflow. It also contains information about the types of available VMs, the instantiated VMs at the moment and locality characteristics. This information is captured by an autonomous agent that monitors the environment looking for changes in the environment (new available VMs or destroyed VMs).</p><p>The data tier also contains all input data consumed by several VMs in the parallel execution. To the best of authors' knowledge, none of the existing cloud environments offer native support to collect provenance and any other means to store provenance metadata produced by scientific experiments. However, there are some papers that highlight the importance of the subject as in Muniswamy Reddy et al. <ref type="bibr" target="#b53">[53]</ref>, where the authors discuss some alternatives to storage of provenance using cloud computing services offered by Amazon EC2 and using the PASS system <ref type="bibr" target="#b54">[54]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">SciCumulus Provenance Model</head><p>SciCumulus is designed and implemented for operating in the cloud as well as its provenance model. It represents all information about the experiment being executed and about the cloud environment itself. This provenance model is based on the proposed Open Provenance Model (OPM) recommendation <ref type="bibr" target="#b55">[55]</ref>. The OPM is a recommendation that is open from an inter-operability viewpoint but also with respect to the community of its contributors, reviewers and users. The main idea of OPM is to represent the causal relations between processes, agents, artifacts and roles involved in a parallel workflow execution. The OPM is not directly instantiable in a database, but it is a standard representation of provenance data for most SWfMS <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b56">56]</ref>.</p><p>Figure <ref type="figure" target="#fig_3">2</ref> presents the conceptual provenance schema designed for SciCumulus. All information related to previous executions of scientific workflows used in the cost model and the scheduling algorithm is captured from the provenance repository of SciCumulus. This model is represented as a Unified Modeling Language (UML) <ref type="bibr" target="#b57">[57]</ref> model and it was modeled based on requirements elicited with scientists. All information of the provenance model is captured by the components in the execution tier of SciCumulus and by the autonomous agent that captures the information about the cloud environment. This provenance model is composed by four main parts: (i) elements that represent the processes executed in the cloud; (ii) elements that represent the artifacts consumed and produced by the workflow execution, (iii) elements that represent the temporality Since this provenance schema is based on the OPM recommendation the classes InputValue, OutputValue, VirtualMachine correspond to the conceptual representation of an artifact since both classes have the same semantics, i.e. model the same digital structures in computational systems (parameters, database, files, and instances).</p><p>The classes Activity and Task are mapped as an OPM process. An OPM process represents one or more actions that consume or produce artifacts. The class Provider represents an OPM user. All the provenance data in SciCumulus is generated at runtime, i.e., during workflow execution. This runtime provenance allows SciCumulus to use provenance data for scheduling. In existing approaches such as VisTrails <ref type="bibr" target="#b51">[51]</ref>, provenance data is only generated at the end of the execution of the workflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">SciCumulus Implementation Details</head><p>For the experiments executed in this paper, we have deployed SciCumulus architecture on top of Amazon EC2. Deploying a workflow engine like SciCumulus into the Amazon EC2 environment is not a simple task to accomplish. Mainly because of the adaptations that have to be performed on the conceptual architecture due to environment restrictions. SciCumulus current version was developed using Java Version 6.27. The components were implemented based on MPJ <ref type="bibr" target="#b58">[58]</ref>. The provenance data is persisted using PostgreSQL relational database version 8.4.6 that was configured in a dedicated Amazon EC2 VM, as well. The client tier components were implemented to connect to the VMs in which the distribution tier components were deployed using Secure Shell (SSH), and send/receive the data files directly to a shared file system in the cloud. Although we use a secure connection to transfer data (via SSH), security issues are outside the scope of this paper. Security issues in scientific workflows are discussed by Gadelha and Mattoso <ref type="bibr" target="#b59">[59]</ref>.</p><p>SciCumulus uses a shared file system (or any equivalent shared area) to manipulate input and output files. In order to provide this shared file system, we have configured an Amazon Elastic Block Storage (EBS) volume <ref type="bibr" target="#b27">[28]</ref>. EBS volumes storages can be accessed by EC2 VMs and their lifetime is independent from the lifetime of the VMs used by SciCumulus execution tier. In other words, this volume is persistent. However, EBS restricts the data size to be stored, and this poses a serious limitation since typically large volumes of scientific data is produced. Following the approach proposed by Jackson et al. <ref type="bibr" target="#b4">[5]</ref> the input data is currently put in the local storage of the VM and all output data is stored in Amazon Elastic Store (S3) <ref type="bibr" target="#b27">[28]</ref>. To connect the VMs with Amazon S3 we used Subcloud <ref type="bibr" target="#b60">[60]</ref>. Subcloud is a shared enterprise file system built on top of Amazon S3. Using Subcloud, we are able to mount a directory on each VM and point to one single bucket in Amazon S3, creating a virtual Shared File System.</p><p>In SciCumulus architecture the Distribution tier is responsible for creating a virtual cluster in the cloud environment. Since we are using MPJ, we have to define the head node (which is a VM labeled with rank 0-a common number used to identify a specific process in a set of running processes) and the workers nodes (rank 1, 2 and so on). To setup a virtual cluster we used Amazon EC2 API Tools <ref type="bibr" target="#b27">[28]</ref>. A custom image (AMI) for the execution VMs was developed. The Distribution tier queries the provenance repository of types of images to be instantiated in VMs and using the Amazon API starts the necessary number of VMs on demand. It creates a list of IPs of instantiated VMs and this list is used to create the machines.conf file that informs the available VMs for MPJ framework to use. The only disadvantage of using this approach is that when a change is needed in any existing image, a new image should be created (a process that usually takes a long time to be performed) and the provenance schema has to be updated in order to use the new image.</p><p>In previous work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b61">61]</ref> we have evaluated SciCumulus by executing static and simulated dynamic experiments for parameter sweep scenarios, using a simplified scheduling algorithm that does not take the proposed cost model into account. The goal was to make a preliminary analysis of the viability of a parallel execution of a scientific workflow in the cloud, before starting the implementation of an adaptive mechanism such as the one proposed in this paper. The next section presents the case study used to evaluate the proposed cost model and the adaptive scheduling algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Data-intensive Phylogenetic Analysis Workflow</head><p>This section presents the workflow used as a case study in this paper. In the last few years, phylogenetic analysis experiments are evolving in a fast pace due to new technologies such as new DNAsequencing methods and scientific apparatus <ref type="bibr" target="#b62">[62]</ref>.</p><p>Comparative genomics is one of many bioinformatics fields that aim at computationally comparing hundreds of different genomes <ref type="bibr" target="#b63">[63]</ref>. Many types of bioinformatics applications associated to this field, such as Multiple Sequence Alignment (MSA), Homologues Detection and Phylogenetic Analysis are increasing in scale and complexity <ref type="bibr" target="#b64">[64]</ref>. Managing genomic experiments is far from trivial, since they are computationally intensive and process large amounts of data. As they are based on a pipeline of scientific programs, computational genomics experiments have been assisted by scientific workflows techniques. Especially in phylogenetic analysis workflows, scientists perform a specific set of activities to produce a set of phylogenetic trees, which are used to infer evolutionary relationships between homologous genes represented in the genomes of divergent species.</p><p>In order to model a phylogenetic analysis experiment as a scientific workflow it was proposed SciPhy <ref type="bibr" target="#b19">[20]</ref>. SciPhy workflow is a parameter sweep one where the same workflow is executed for each file in a given large input dataset. It is composed by five main activities. The first three activities are: multiple sequence alignment (MSA), search for the best evolutionary model, and construction of phylogenetic trees, and they respectively execute the following bioinformatics applications: MSA programs (MAFFT <ref type="bibr" target="#b65">[65,</ref><ref type="bibr" target="#b66">66]</ref>, Kalign <ref type="bibr" target="#b67">[67]</ref>, ClustalW <ref type="bibr" target="#b68">[68]</ref>, Muscle <ref type="bibr" target="#b25">[26]</ref> and Prob-Cons <ref type="bibr" target="#b69">[69]</ref>), ModelGenerator <ref type="bibr" target="#b70">[70]</ref>, and RAxML <ref type="bibr" target="#b71">[71]</ref>.</p><p>The last two activities represent the Phylogenomic Analysis: concatenation of MSA to obtain a "superalignment" <ref type="bibr" target="#b72">[72]</ref>, and construction of phylogenomic trees. The programs that respectively execute these activities are: a Perl script to concatenate each MSA (generated by different MSA programs-one concatenation per MSA program), and RAxML.</p><p>The general steps of SciPhy are presented in Fig. <ref type="figure">3</ref>. The first activity of SciPhy constructs individual MSA using five MSA programs-ClustalW, Kalign, MAFFT, Muscle, and ProbCons-with default parameters. Each MSA program receives a multifasta file as input, then producing a MSA as output. Each MSA is tested at the second activity to find the best evolutionary model using ModelGenerator, and both of them (individual MSA and evolutionary model) are used in the third activity to generate phylogenetic trees using RAxML with 100 bootstrap replicates <ref type="bibr" target="#b62">[62]</ref>. Consequently, we obtain several trees for each one of the MSA programs.</p><p>In the fourth activity, all individual MSA are used as input for a Perl script, in which they are concatenated thus obtaining a superalignment as output. Since we are exploring five different MSA programs, we can only obtain five superalignments. Each superalignment jointly with the specific pre-chosen evolutionary models (BLO-SUM62, CPREV, JTT, WAG, or RtREV) is used as input to construct supermatrix phylogenomic trees.</p><p>In phylogenomic analysis activity, there is the option to concatenate multiple gene sequences to construct phylogenetic trees on the genomic level, also known as "genome trees", "supermatrix trees" or also called "supermatrix phylogenomic trees". Trees that have more phylogenetic signals are less susceptible to stochastic errors than those built from a single gene. Nevertheless, phylogenomic trees have held the promise of minimizing anomalies by the sheer power of genome-scale data as they are based on the maximum quantity of genetic information. A phylogenomic tree should be the best reflection of the evolutionary history of the species.</p><p>Since we aim at executing a parameter sweep in SciPhy, each one of the first three activities is going to be executed for a different input file containing several sequences (multi-fasta file). Each one of these executions can be performed in parallel. However, the MSA concatenation is a nonparallel cloud activity and just one instance of it is allowed per MSA program. The evolutionary </p><formula xml:id="formula_12">… … … … … … … … … … … … … … …</formula><p>analysis and the construction of the tree activities can be parallelized for each one of the five available methods. A representation of the parallel execution of SciPhy can be viewed in Fig. <ref type="figure">4</ref>. Each one of the circles represents a different cloud activity to be executed in parallel. Note that the degree of parallelism changes during the execution course of the workflow. For example, if we process 200 multi-fasta files as input. The first part of the workflow (first three activities) is going to have 200 cloud activities for each MSA method. Then, in the second part (last two activities) we are going to have just five cloud activities (one for each chosen method).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Experimental Results</head><p>This section presents the configurations used to model and execute SciPhy using SciCumulus and experimental results achieved. The central idea of this section is to compare the proposed adaptive approach (cost model and adaptive greedy scheduling algorithm) with existing solutions that can be used to parallelize workflows that need HPC in clouds such as MapReduce <ref type="bibr" target="#b34">[35]</ref> implementations (e.g. Hadoop <ref type="bibr" target="#b73">[73]</ref>). To evaluate SciCumulus adaptive approach, we have modeled Sci-Phy workflow as presented in Section 6 on a distributed cloud environment based on CentOS 5.0 operating system machines and we compared the proposed approach with Hadoop. As we discussed at Section 2, we did not find scheduling algorithms that considered cloud elasticity issues and scientific workflow computations. This way, we have chosen to compare the proposed approach with Hadoop implementation, since many of the existing SWfMS parallel approaches are based on Hadoop. In the adaptive evaluation we compared our approach with ideal curves of scheduling and parallel performance measures of speedup, typically the ideal linear speedup curve.</p><p>First in this section we briefly describe Hadoop and MapReduce model. Then we describe the environment used for executing the case study and the experiment configuration. After that, we present and discuss about experimental results. Our experimental results are composed by three main parts. In the first one we compare SciCumulus performance with the proposed cost model and scheduling algorithm (without scaling VMs up and down) with Hadoop. In the second part we discuss about results achieved while scaling resources up and down (adaptive approach). In the third part we analyze financial costs of the executions and data transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">MapReduce and Hadoop</head><p>MapReduce <ref type="bibr" target="#b74">[74]</ref> is a programming model that aims at easing the parallelization of applications. MapReduce deals with large volumes of data to be processed in parallel. Using MapReduce, data can be partitioned into a set of intermediate keyvalue pairs by a map function, and these intermediate results are merged into the final result using the reduce function following a specific criteria. The main idea behind MapReduce is to transparently provide for users data partitioning, scheduling, load balancing, and fault tolerance mechanisms. To use MapReduce, two main functions have to be programmed: (i) a map function in the form map(in_key,in_val) → list(out_key,intermediate_val) and (ii) a reduce function in the form reduce(out_key, list(intermediate_val)) → list(out_val). Each one of these functions is domain-specific. In the context of scientific workflows, a new map and a reduce functions should be implemented for each different type of activity. MapReduce was initially used in business applications such as document indexing. In the last few years its use is being extrapolated for scientific domains <ref type="bibr" target="#b75">[75]</ref>.</p><p>One of the most famous and used implementations of MapReduce is Hadoop <ref type="bibr" target="#b76">[76]</ref>, a framework developed by Apache group that provides the MapReduce core component and the Hadoop Distributed File System (HDFS) natively. In the Hadoop architecture one central process acts as master and coordinates MapReduce tasks (i.e. parallel executions of activities) while all other processes act as workers on different nodes. Workers execute tasks that are generated and distributed by the master node. The same worker is able to execute several maps and reduces tasks at the same time. In addition, implementations of MapReduce such as Hadoop are being used in cloud <ref type="bibr" target="#b0">[1]</ref> environments such as Amazon EC2 <ref type="bibr" target="#b27">[28]</ref> to ease the access of large amounts of computing power to run data-intensive tasks, including scientific ones.</p><p>Although these MapReduce functions jointly with the Hadoop environment variables can be implemented and configured (respectively) by computer specialists, it may be a very complex task to be executed by scientists with little (or none) computational background. In addition, by executing some activities of an experiment using MapReduce decoupled from the SWfMS and without provenance support may produce undesirable results, such as data loss or, more importantly, the inability to reproduce the scientific experiment. These problems may occur as part of the produced data (the one that is produced in the cloud or in a cluster) is not stored in the provenance schema of the SWfMS, for example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Environment Setup</head><p>For the experiments executed in this article, we have deployed SciCumulus and Hadoop on top of Amazon EC2. Amazon EC2 is one of the most popular cloud computing environments, and many scientific and commercial applications are being deployed on it. Amazon EC2 provides several different types of VMs for scientists to instantiate and use. Each one of them has unique characteristics (CPU power, RAM and storage capacity).</p><p>There are several types of VMs, such as micro (EC2 ID: t1.micro-613 MB RAM, 1 core, EBS storage only ), large (EC2 ID: m1.large-7.5 GB RAM, 850 GB storage, 2 cores), extra-large (EC2 ID: m1.xlarge-15 GB RAM, 1,690 GB storage, 4 cores), high CPU extra-large instance (EC2 ID: c1.xlarge-7.5 GB RAM, 850 GB storage, 8 cores), and Quadruple Extra Large Instance (EC2 ID: cc1.4xlarge-23 GB RAM, 1,690 GB storage, 8 cores). In the experiments presented in this paper we have considered just Amazon's micro and large type.</p><p>Each one of the instances uses quad-core Intel Xeon processors @ 2.33 GHz. Each instantiated VM in the phylogenetic experiments presented in this paper uses Linux Cent OS 5 (64-bit), and it was configured with the necessary software and libraries like MPJ <ref type="bibr" target="#b77">[77]</ref> and the bioinformatics applications. All instances were configured to be accessed using SSH without password checking (although this is not recommended due to security issues). Additionally, these images (ami-e4c7368d and ami-ceb949a7) were stored in the cloud as well and SciCumulus creates the virtual cluster to execute the experiment based on these images. The same images were used to execute Hadoop. In terms of software, all instances, no matter its type, executes the same programs and configurations. According to Amazon, all VMs were instantiated in the US East-N. Virginia location and follow the pricing rules of that locality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Experiment Setup</head><p>To execute SciPhy, we have varied the MSA program and the evolutionary model used in the scientific workflow (see possible variability of MSA construction and Evolutionary Model Election activities in Fig. <ref type="figure">3</ref>). Our executions use as input a dataset of multi-fasta files of protein sequences extracted from RefSeq release 48 <ref type="bibr" target="#b78">[78]</ref>. This dataset is formed by 1,600 amino acid multifasta files and each multi-fasta file is constituted by an average of 10 sequences.</p><p>To perform SciPhy, once downloaded, each input multifasta file is aligned to obtain an alignment using the following MSA programs: ClustalW version 2.1, Kalign version 1.04, MAFFT version 6.857, Muscle version 3.8.31, and ProbCons version 1.12. Each alignment is used as input to the program ModelGenerator version 0.85 that produces an evolutionary model as output. Then, both of them, alignment and evolutionary model, are used as input to construct phylogenetic trees using RAxML-7.2.8-ALPHA. We also have varied the MSA program and the evolutionary model used in the scientific workflow (see possible variability of MSA concatenation and Evolutionary Model activities in Fig. <ref type="figure">3</ref>). All the resultant alignments (from each MSA program), obtained from Phylogenetic Analyses, are concatenated to produce a superalignment. Then, this superalignment is tested with five evolutionary models (BLOSUM62, CPREV, JTT, WAG, and RtREV) and both of them, superalignment and evolutionary models are used as input to construct phylogenomic trees using RAxML-7.2.8-ALPHA. SciPhy workflow was modeled using SciCumulus and Hadoop 0.21.0.</p><p>In order to implement each one of these activities in Hadoop we had to implement specific Map and Reduce functions. Since the input of the standalone MSA programs is a single multifasta file (or many multifasta files), meanwhile, in our case, the input format of the MSA activities in Hadoop is a set of multifasta files. We collect those uploaded input files from HDFS to create key-value pairs for the Mapper. The Mapper mainly creates a java process to call the specific program (ClustalW, Kalign, MAFFT, Muscle or ProbCons). The Map key is the filename, and the Map value includes the full HDFS path for each uploaded input files. Each map task downloads the assigned input file from HDFS, and it passes this input to run the associated program. The next activities in the workflow follow the same approach. For each program that is invoked, a new Map and Reduce function had to be implemented (for ModelGenerator, RAxML and for the superlignment script), but these new activities are going to consume the produced data from the previous activity. The reducer is responsible for collecting all intermediate outputs and groups all of them in one single output file to be transferred to the scientist's machine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Cloud Activity Execution Time Distribution</head><p>Analyzing the provenance repository of SciCumulus we could build the histogram of the execution time for all cloud activities of the SciPhy workflow, as presented in Fig. <ref type="figure" target="#fig_5">5</ref>. Based on the information obtained from this repository, it is possible to calculate the average (1,703.5 s) and standard deviation (108.3 s) of each cloud activity. Additionally, scientists can use the provenance repository to check which parameter generated the desired result or submit queries like "what is the average execution time of MAFFT cloud activities that executed in 16 cores?".</p><p>The main advantage of having such distribution of execution times is that we can schedule more computing intensive cloud activities to more powerful VMs if we are focusing on performance. On the other hand, we can schedule more computing intensive cloud activities to cheaper VMs if we are focusing on financial cost. Such distribution can benefit from the proposed cost model. In the next subsection we present performance results for the cost model and scheduling algorithms for each one of the scenarios aforementioned.  Firstly, we adjusted the cost model for focusing on performance (scenario S1) by setting α 1 = 0.9, α 2 = 0.05 and α 3 = 0.05. The execution time and speedup results are presented in Figs. <ref type="figure">6</ref> and<ref type="figure">7</ref>, respectively, where we can observe that from Figs. 6, and 7 the total execution time of SciPhy workflow using both SciCumulus and Hadoop decreases, as expected, when provided more VMs (and consequently more virtual cores). However SciCumulus outperformed Hadoop when using from 1 to 128 virtual cores. This is due to two reasons. The first one is that Hadoop presents serious overhead to generate This way, since the csi of large VMs is greater than the csi of micro VMs, the total execution time is reduced. For example, when we execute SciPhy with 64 cores, SciCumulus executed in 4. <ref type="bibr" target="#b65">65</ref>  In clouds there are many factors that can harm speedup. For example, in parallel computers the speedup value is impacted by serial portions of the code and communication between processors, whereas in the cloud, we have to consider other factors such as heterogeneity of the environment, performance fluctuations due to the virtualization and high communication latency <ref type="bibr" target="#b0">[1]</ref>.</p><p>Secondly, we adjusted the cost model for focusing on monetary cost (scenario S2) by setting α 1 = 0.05, α 2 = 0.05 and α 3 = 0.9. The execution time and speedup results are presented in Figs. <ref type="figure" target="#fig_8">8</ref> and<ref type="figure" target="#fig_9">9</ref>, respectively. Differently from the execution focused on performance, in this case we have chosen to schedule more intensive cloud activities to micro VMs (which are cheaper) while less intensive cloud activities were scheduled to large VMs (which are more expensive).</p><p>However, SciCumulus did not outperform Hadoop when using from 2 to 128 virtual cores, which is acceptable since the focus, is on financial cost instead of performance. SciCumulus only outperformed Hadoop using one single core since there is only a single option to schedule cloud activities. From 2 cores to 128 the performance using SciCumulus was degraded because the least powerful VMs (the ones with the smallest csi) were preferentially chosen (since they are cheaper) while in Hadoop the scheduling does not take into account the monetary costs involved in the execution. This approach is beneficial because if we are executing the experiment in a provider in which the quantum is 1 min we are going to spend less because intensive cloud activities are executed in cheaper VMs. On the other hand, if the quantum is 1 h, we are not going to spend less financial resources, however, since less intensive cloud activities are scheduled to the powerful VMs, if we are focusing on financial cost, when the load balancing algorithm is executed, it is going to destroy more expensive VMs, thus stopping just less intensive cloud activities (these cloud activities has to be rescheduled).</p><p>When we execute SciPhy focusing on financial cost with 64 cores, SciCumulus executed in 8.99 h while Hadoop executed in 7.49 h. It is a "negative" difference of 16.7 % on the overall performance. This execution led to a speedup of 27.36 (Hadoop considering 64 cores available) and 20.94 (SciCumulus considering 64 cores available). Analyzing the speedup in Fig. <ref type="figure" target="#fig_9">9</ref>, we can state that Hadoop reached a better speedup than SciCumulus, and both suffered a small degradation when using 64 up to 128 cores. However the difference of speedup is acceptable since the main focus here is on financial cost instead of performance. Thirdly, we adjusted the cost model for focusing on reliability (scenario S3) by setting α 1 = 0.05, α 2 = 0.9 and α 3 = 0.05. The execution time and speedup results are presented in Figs. <ref type="figure" target="#fig_2">10</ref> and<ref type="figure" target="#fig_2">11</ref>, respectively. Since we are focusing on reliability we have to calculate the probability of a given number of failures in the VMs involved in the execution. As we could note by querying provenance data, micro VMs degrade faster than large VMs. Consequently, the failure probability is higher in micro VMs.</p><p>This way, the cost model and the scheduling algorithms schedules intensive cloud activities to large VMs while less intensive activities are scheduled to micro VMs. In fact, we avoid re-executing intensive cloud activities because they are scheduled to be executed in more reliable VMs. This way, since large VMs are more reliable than micro VMs, the behavior of this scheduling is similar to the first scenario. Similarly to the first scenario (S1) SciCumulus outperformed Hadoop when using from 1 to 128 virtual cores.</p><p>Since the failure probability of large VMs is smaller than the failure probability of micro VMs, the total execution time is reduced. For example, when we execute SciPhy with 64 cores, SciCumulus executed in 5.43 h while Hadoop executed in 7.49 h. It is difference of 34.7 % on the overall performance. This execution led to a speedup of 27.36 (Hadoop considering 64 cores available) and 32.68 (SciCumulus considering 64 cores available). Sim-ilarly to the results achieved in scenario S1, there were performance gains up to 128 cores (approximately 1 h difference). Analyzing the speedup in Fig. <ref type="figure" target="#fig_2">11</ref>, we can state that SciCumulus reached a near linear speedup from 2 to 16 cores, and suffered a small degradation when using 32 up to 128 cores.</p><p>In the last scenario, we adjusted the cost model for providing equal focus for all criteria (scenario S4) by setting α 1 = 0.33, α 2 = 0.33 and α 3 = 0.33. The execution time and speedup results are presented in Figs. <ref type="figure" target="#fig_3">12</ref> and<ref type="figure" target="#fig_2">13</ref>, respectively. Since we have attributed equal weights for each criterion, performance reached was very similar to Hadoop as can be stated in Fig. <ref type="figure" target="#fig_3">12</ref>. The main difference in performance was due to the overhead imposed by In order to analyze the overhead imposed by SciCumulus when processing large volumes of data, we executed SciPhy, using 16 cores, varying the number of input files and the scenario (weights associated to α i in the cost model). This experiment consumed from 100 to 1,600 input files (multi-fasta files). The scalability results are presented in Fig. <ref type="figure" target="#fig_12">14</ref>. As we can observe, as expected, that for each one of the scenarios (S1, S2, S3, S4 and Hadoop) as the volume of data to be consumed increases the execution time also increases. However, we can state that the overhead imposed by Hadoop in this case also scales up. Scenarios S1, S3 and S4 presented a good scalability by reaching a near linear scalability. However, scenario S2 could not scale in the same rate since its focus is on monetary cost instead of performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.6">Financial Cost Analysis</head><p>In order to verify the financial cost involved in the execution of the five scenarios (Hadoop, S1, S2, S3 and S4), we analyzed generated provenance data and calculated the final cost using two payment forms. In the first form we set the quantum as 1 h (Amazon EC2 payment method) while in the second form we set the quantum as 1 min (GoGrid pre-paid payment form). We varied the type of used VM as explained in the last subsection. For each workflow execution, half of the instantiated VMs is large ones and the other half is micro VMs. The overall financial cost did not vary much when executing four of five scenarios and S4) because these executions are not focused on financial cost. However it is important to highlight that the achieved financial cost is not prohibitive, i.e. most of scientists are able to pay for those executions. We are more interested on analyzing the financial cost for the second scenario (S2) where we focused on reducing the financial cost. In this scenario the overall financial cost varies pretty much according with the chosen quantum as presented in Table <ref type="table" target="#tab_2">1</ref> and Fig. <ref type="figure" target="#fig_6">15</ref>. This difference of financial cost is found because if we are executing the experiment in a provider in which the quantum is set to one minute we are going to schedule intensive cloud activities to VMs that are cheaper. This way, most of the execution time of the workflow is spent in micro VMs and just less intensive activities are executed in large VMs. Since less intensive activities execute in some minutes, the small quantum benefits because there is little wasted time. In addition, we pay only when we are executing a cloud activity. On the other hand, if we are using a one hour quantum, consider that a less  When the quantum is one minute if we spend some seconds of the quantum, the financial cost is not going to increase much. In addition, when the budget is about to be reached, the scheduler stops scheduling activities to expensive VMs to reduce the overall financial cost. This explains the difference in total cost when executing with 64 and 128 virtual cores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.7">Adaptive Performance Analysis</head><p>To analyze the performance of the adaptive approach in SciCumulus, we conducted a series of real experiments. We are specifically interested in comparing system performance using the adaptive approach and using scheduling algorithm without scaling resources, measuring and analyzing the overhead imposed by the adaptive approach so as to verify its benefits. The executed experiments fixed the number of consumed input multi-fasta files in to 100 and used just the large VM type (m1.large).</p><p>The experiments presented in this sub-section investigate the performance of the proposed adaptive algorithm with the cost model and their implementation while compared with the ideal result. Apart from current MapReduce solutions, which are all static, we could not analyze the advantages of the proposed adaptive algorithm compared with the existing scheduling algorithms because none of them focus on scheduling scientific workflow activities in a cloud.</p><p>As an experiment to analyze the adaptive approach where the load balancing algorithm scales resources up and down, we executed SciPhy in several scenarios. One of these executions is presented in Fig. <ref type="figure" target="#fig_2">16</ref>. In this particular case we aim at analyzing the performance by setting up the deadline as 10 h (36,000 s) and the limit budget as US$ 150.00. The focus on performance is seen in the cost model (α 1 = 0.9, α 2 = 0.05 and α 3 = 0.05). In this execution the number of VMs increase and decrease over the time to meet the deadline and the budget. In addition, VMs can be destroyed without interference of SciCumulus. This scenario occurs when we use Amazon EC2 spot instances Fig. <ref type="bibr" target="#b14">15</ref> Monetary cost analysis Fig. <ref type="bibr" target="#b15">16</ref> Adaptive execution (that can be destroyed by the cloud provider without previous warning).</p><p>We executed three different cases in this experiment. The first one we called "ideal scenario". We know a priori that if we executed SciPhy using 16 cores we could meet the deadline and the limit budget with the best possible performance. This way, the "ideal scenario" starts and ends using 16 cores. On the other hand, the other 2 cases, the amount of VMs increases and decreases gradually. In the first one we considered that there is no available provenance data. This way, the load balancing algorithm has to wait for performance provenance data to be generated to estimate the necessary amount of resources. In fact, the load balancing algorithm takes some time so start increasing the amount of VMs.</p><p>In the second one we assumed that there is provenance data available at the repository to perform a better estimation. This way, the load balancing algorithm starts to increase the number of VMs earlier than when there is no provenance available. The load balancing algorithm scales gradually because the first two activities of Sci-Phy are less intensive activities. When we reach the third activity (a compute intensive one) the Fig. <ref type="figure" target="#fig_2">17</ref> Idleness analysis-adaptive execution Fig. <ref type="bibr" target="#b17">18</ref> Read and write throughput algorithm scales the number of VMs faster. We can observe in Fig. <ref type="figure" target="#fig_2">16</ref> that the total amount of VMs increases and decreases gradually when using adaptive approach of SciCumulus. However, in this case the "static scenario" execution overcomes adaptive scheduling by 3.2 % and 8.1 % in the two scenarios where provenance is available and when it is not, respectively. These results are very near the ideal scenario, which is not known a priori by scientists. In addition, the adaptive approach avoids under and overestimations of the amount of resources to use.</p><p>We have also analyzed the level of idleness of the VMs during the workflow execution (Fig. <ref type="figure" target="#fig_2">17</ref>). Since using the adaptive scheduling, several calculations have to be performed (cost model, estimation for scale number of VMs, etc.) SciCumulus takes more time to start a new cloud activity. This way, when using the adaptive approach, the level of idleness of the VMs tends to be greater than when using a "static scenario" approach. However, this difference is acceptable. The average idleness is 11.34 %, 12.60 % and 14.26 % when executing in a "static scenario", adaptive with provenance and adaptive without provenance, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.8">Data Transferring Analysis</head><p>In order to data performance using SciCumulus SciPhy workflow, we measured the throughput of SciCumulus components built on top of Amazon S3 framework. For this experiment we fixed the 200 input multi-fasta and large type of amazon VMs. Each VM reads and writes large volume of data from and to Amazon S3 environments (80 MB and 8.5 GB, respectively). The aggregated throughput is presented in Fig. <ref type="figure" target="#fig_8">18</ref>. In this experiment, to transfer the 80 MB compressed files, it takes up to 7 min using a slow connection (256 mbps). This latency when compared to the overall execution time of SciPhy is acceptable. However, this input data can be already placed in S3, reducing the overall transferring cost. In addition, new mechanisms for data staging are planned to be developed in SciCumulus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusions</head><p>Large scale scientific experiments usually present a long duration where several executions, using different parameters and input data, are necessary to draw conclusions. Scientists frequently use parallel techniques to improve performance. However, it is far from trivial to manage parallel executions of large scientific workflows, particularly in cloud environments, which typically provide an elastic set of computing resources (VMs). To increase the uptake of the cloud model for executing scientific workflows that demand HPC capabilities, new solutions have to be developed, especially for scheduling parallel cloud activities in cloud resources.</p><p>Cloud activity scheduling is a well-known NPcomplete problem even in its simplest form. Although there are several heuristic-based solutions to solve scheduling problems, most of them assume environments that do not change during the execution course of an experiment. These heuristics generate a priori scheduling plans (before starting executing the workflow). A priori scheduling is not an option when we execute scientific workflows in clouds. Since cloud resources are elastic, workflows should benefit from this elasticity. In addition, clouds are based on virtualization presenting fluctuations on the performance of VMs, thus impacting the overall workflow performance. This way, it is necessary to use an adaptive scheduling approach to consider those changes in the environment, including the number of VMs and the capacity of the VMs.</p><p>In previous work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b61">61]</ref> we have addressed workflow execution in clouds using a static scheduling algorithm and simulated dynamic executions. One of the available infrastructures to execute scientific workflows in cloud environments is SciCumulus, a workflow execution engine to support parallel execution in clouds. To achieve high performance while keeping track of the workflow execution, SciCumulus has specific data provenance components to capture cloud execution information and drive the adaptive scheduling decisions at runtime. This paper proposes a weighted cost function and an adaptive scheduling heuristic to explore the dynamicity of clouds while scheduling cloud activities to several VMs in a virtual cluster. Both the cost model and the scheduling algorithms were implemented in SciCumulus. Experimental results show that the proposed approach is able to find efficient solutions (depending on the chosen scenario-performance, financial cost, reliability and equal weight) when compared to an ideal solution and to other heuristics under evaluation (Hadoop approach).</p><p>Hadoop was the natural choice for a baseline, since we did not find scheduling algorithms that considered cloud elasticity issues and scientific workflow computations. This way, we have chosen to compare the static approach with Hadoop implementation, since many of the existing SWfMS parallel approaches are based on Hadoop. The adaptive approach was compared to the ideal solution, obtained artificially.</p><p>In the static analysis (where the number of VMs does not change during the course of the workflow) the overall performance results show that a near linear speedup was obtained when we executed SciPhy workflow processing 200 multifasta files and using from 2 up to 64 virtual cores. From 64 to 128 cores, the speedup presented some degradation. However, this degradation is acceptable. The static approach is useful when there is no (or a small) demand for HPC capabilities or when the environment is not susceptible to performance fluctuations, such as in a private (and controlled) cloud. The static approach is incompatible with environments that are elastic and whose processing capacity and the amount of resources vary over time.</p><p>The performance evaluation of the adaptive execution in SciCumulus showed results very close to the ideal solution. The evaluation also showed that the tradeoff between collecting provenance and performance prediction is very good. Only a small level of idleness was registered. Although the obtained results present a step forward, clouds are still beginning to be considered as a HPC scientific computation environment. There are only few approaches available and as clouds become more robust and more reliable, new techniques will be proposed. As future work, we plan to use the proposed approach in conjunction with a faulttolerance mechanism named SciMultaneous that is focused on recovering failured cloud activities and execute them with more reliability on distributed cloud environments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>i</head><label></label><figDesc>Client Tier: Its components are placed in the scientists' workstation. It dispatches workflow activities to be executed in the cloud environment using a local SWfMS such as VisTrails [51], ii Distribution Tier: It generates and manages the execution of cloud activities in one or more VMs instantiated in one or more cloud environments, iii Execution Tier: Its components are placed in the several VMs involved with the parallel execution of the cloud activities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 presents</head><label>1</label><figDesc>SciCumulus conceptual architecture and its four main tiers. The Client Tier is responsible for starting parallel execution of workflow activities in the cloud. The components of the client tier are deployed in an existing SWfMS such as VisTrails. The components installed in the SWfMS are responsible for transferring data to and from the cloud and to start the parallel execution in the cloud. The components of the client tier launch the execution process of cloud activities in the cloud</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 SciCumulus conceptual architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2 SciCumulus provenance schema</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 Fig. 4</head><label>34</label><figDesc>Fig. 3 Conceptual view of SciPhy</figDesc><graphic coords="16,192.75,458.04,169.96,196.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>8. 5</head><label>5</label><figDesc>Non-adaptive Performance and Scalability Analysis of SciCumulusIn order to perform a fair comparison of Sci-Cumulus scheduling approach with Hadoop we executed SciPhy in SciCumulus without varying the number of VMs involved in the execution (since Hadoop does not scale the number of VMs during execution) and consuming 200 input files (each one of the files contains about 10 sequences to be aligned). In this first part, we execute real</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5</head><label>5</label><figDesc>Fig. 5 Cloud activities execution time distribution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 Fig. 7</head><label>67</label><figDesc>Fig. 6 Execution time for scenario S1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8</head><label>8</label><figDesc>Fig. 8 Execution time for scenario S2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9</head><label>9</label><figDesc>Fig. 9 Speedup scenario S2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 Fig. 11</head><label>1011</label><figDesc>Fig. 10 Execution time for scenario S3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 Fig. 13</head><label>1213</label><figDesc>Fig. 12 Execution time for scenario S4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 14</head><label>14</label><figDesc>Fig. 14 Scalability according to the number of input files</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Hadoop to generate maps and reduces. When we execute SciPhy with 64 cores and equally weighted parameters for the cost model, SciCumulus executed in 5.91 h while Hadoop executed in 7.49 h. It is difference of 21.02 % on the overall performance. This execution led to a speedup of 27.36 (Hadoop considering 64 cores available) and 31.92 (SciCumulus considering 64 cores available). Both SciCumulus and Hadoop presented very similar results in this scenario (S4).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>Total monetary cost for scenario S2</figDesc><table><row><cell>Number</cell><cell>Price (US$)-</cell><cell>Price (US$)-</cell></row><row><cell>of VMs</cell><cell>1 h quantum</cell><cell>1 min quantum</cell></row><row><cell>1</cell><cell>66.15</cell><cell>65.15</cell></row><row><cell>2</cell><cell>82.79</cell><cell>37.61</cell></row><row><cell>4</cell><cell>107.23</cell><cell>48.72</cell></row><row><cell>8</cell><cell>125.65</cell><cell>57.09</cell></row><row><cell>16</cell><cell>135.18</cell><cell>61.44</cell></row><row><cell>32</cell><cell>185.80</cell><cell>84.66</cell></row><row><cell>64</cell><cell>201.47</cell><cell>91.58</cell></row><row><cell>128</cell><cell>256.07</cell><cell>116.35</cell></row><row><cell cols="3">intensive activity was scheduled to a large VM.</cell></row><row><cell cols="3">This less intensive activity lasts for 16 min, but</cell></row><row><cell cols="3">you are paying for one hour in this case, this</cell></row><row><cell cols="2">way, wasting time and money.</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://scicumulus.sourceforge.net/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank CNPq, CAPES and FAPERJ for partially funding this work. We also thank Eduardo Ogasawara for his valuable comments with the experiments and Pedro Cruz together with Ricardo Busquet for their effort in developing SciCumulus components.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A break in the clouds: towards a cloud definition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Vaquero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rodero-Merino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caceres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lindner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGCOMM Comput. Commun. Rev</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50" to="55" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards a Taxonomy for Cloud Computing from an e-Science Perspective</title>
		<author>
			<persName><forename type="first">D</forename><surname>De Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Baião</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-84996-241-4_3</idno>
	</analytic>
	<monogr>
		<title level="m">Cloud Computing</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Antonopoulos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Gillam</surname></persName>
		</editor>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="page" from="47" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The Grid: blueprint for a new computing infrastructure</title>
		<author>
			<persName><forename type="first">I</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kesselman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Mateo, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploring the Performance Fluctuations of HPC Workloads on Clouds</title>
		<author>
			<persName><forename type="first">Y</forename><surname>El-Khamra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Parashar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 IEEE Second International Conference on Cloud Computing Technology and Science</title>
		<meeting>the 2010 IEEE Second International Conference on Cloud Computing Technology and Science</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="383" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Performance Analysis of High Performance Computing Applications on the Amazon Web Services Cloud</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Muriki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cholia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shalf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Wasserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 IEEE Second International Conference on Cloud Computing Technology and Science</title>
		<meeting>the 2010 IEEE Second International Conference on Cloud Computing Technology and Science</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="159" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Case study for running HPC applications in public clouds</title>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kobler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duffy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mcglynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing</title>
		<meeting>the 19th ACM International Symposium on High Performance Distributed Computing</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="395" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards supporting the life cycle of large-scale scientific experiments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Travassos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Braganholo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Murta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ogasawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M S</forename><surname>Da Cruz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Martinho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJBPIM</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="92" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Deelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Gannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shields</surname></persName>
		</author>
		<title level="m">Workflows for e-Science: Scientific Workflows for Grids</title>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Workflows and e-Science: an overview of workflow system features and capabilities</title>
		<author>
			<persName><forename type="first">E</forename><surname>Deelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shields</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Gener. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="528" to="540" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Provenance and scientific workflows: challenges and opportunities</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD International Conference on Management of Data</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1345" to="1350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Provenance for computational tasks: a survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Freire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="11" to="21" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Challenges in executing large parameter sweep studies across widely distributed computing environments</title>
		<author>
			<persName><forename type="first">E</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Challenges of large applications in distributed environments</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data parallelism in bioinformatics workflows using Hydra</title>
		<author>
			<persName><forename type="first">F</forename><surname>Coutinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ogasawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>De Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Braganholo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A B</forename><surname>Lima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M R</forename><surname>Dávila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th ACM International Symposium on High Performance Distributed Computing</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="507" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Montage: a Grid portal and software toolkit for science-grade astronomical image mosaicking</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Berriman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Good</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Laity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Deelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kesselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCSE</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="73" to="87" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploring many task computing in scientific workflows</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ogasawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chirigati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Elias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Braganholo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coutinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Many-Task Computing on Grids and Supercomputers</title>
		<meeting>the 2nd Workshop on Many-Task Computing on Grids and Supercomputers</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A performance evaluation of X-ray crystallography scientific workflow using SciCumulus</title>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ocana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ogasawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Baiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Cloud Computing (CLOUD)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="708" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">OrthoSearch: a scientific workflow approach to detect distant homologies on protozoans</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M S</forename><surname>Da Cruz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M R</forename><surname>Dávila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vilela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L M</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cuadrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tschoeke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACM SAC</title>
		<meeting>of the ACM SAC</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1282" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cloud-based phylogenomic inference of evolutionary relationships: a performance study</title>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A C S</forename><surname>Ocaña</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ogasawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Goncalves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Workshop on Cloud Computing and Scientific Applications</title>
		<meeting>the 2nd International Workshop on Cloud Computing and Scientific Applications</meeting>
		<imprint>
			<publisher>CCSA</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploring molecular evolution reconstruction using a parallel cloud-based scientific workflow</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A C S</forename><surname>Ocaña</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>De Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Horta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ogasawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Brazilian Symposium on Bioinformatics</title>
		<meeting>the 2012 Brazilian Symposium on Bioinformatics</meeting>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SciPhy: a cloudbased workflow for phylogenetic analysis of drug targets in protozoan genomes</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A C S</forename><surname>Ocaña</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ogasawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M R</forename><surname>Dávila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A B</forename><surname>Lima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Bioinformatics and Computational Biology</title>
		<editor>
			<persName><forename type="first">Norberto</forename><surname>De Souza</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><surname>Telles</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Palakal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="66" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic scheduling for heterogeneous Desktop Grids</title>
		<author>
			<persName><forename type="first">I</forename><surname>Al-Azzoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Down</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 9th IEEE/ACM International Conference on Grid Computing</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="136" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Scheduling multiple parameter sweep workflow instances on the Grid</title>
		<author>
			<persName><forename type="first">S</forename><surname>Smanchat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Indrawan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Enticott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Abramson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="300" to="306" />
		</imprint>
	</monogr>
	<note>In: e-Science 2009-5th IEEE</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Buyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Siegel</surname></persName>
		</author>
		<title level="m">Scheduling parallel applications on utility Grids: time and cost trade-off management</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scheduling scientific workflow applications with deadline and budget constraints using genetic algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Buyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="217" to="230" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An efficient weighted bi-objective scheduling algorithm for heterogeneous systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Boeres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sardiña</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Comput</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="349" to="364" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A dynamic and reliability-driven scheduling algorithm for parallel real-time jobs executing on heterogeneous clusters</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Parallel Distrib. Comput</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="885" to="900" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A bi-criteria scheduling heuristic for distributed embedded systems under reliability and real-time constraints</title>
		<author>
			<persName><forename type="first">I</forename><surname>Assayad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Girault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kalla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2004 International Conference on Dependable Systems and Networks</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="347" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<ptr target="http://aws.amazon.com/ec2/" />
		<title level="m">Amazon EC2. Amazon Elastic Compute Cloud (Amazon EC2)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An adaptive approach for workflow activity execution in clouds</title>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ogasawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Baiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Workshop on Challenges in e-Science-SBAC</title>
		<imprint>
			<biblScope unit="page" from="9" to="16" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SciCumulus: a lightweight cloud middleware to explore many task computing paradigm in scientific workflows</title>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ogasawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Baião</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Cloud Computing</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="378" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adaptive virtual partitioning for OLAP query processing in a database cluster</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JIDM</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="88" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Parallel query processing for OLAP in Grids</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kotowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A B</forename><surname>Lima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pacitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CCPE</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="2039" to="2048" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">high-performance query processing of a real-world OLAP Database with ParGRES</title>
		<author>
			<persName><forename type="first">M</forename><surname>Paes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A B</forename><surname>Lima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computing for Computational Science (VEC-PAR)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="188" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pisani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Purves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<pubPlace>Norton, New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">MapReduce: a flexible data processing tool</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="72" to="77" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Designing and Building Parallel Programs: Concepts and Tools for Parallel Software Engineering</title>
		<author>
			<persName><forename type="first">I</forename><surname>Foster</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Addison Wesley</publisher>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Kepler + Hadoop: a general architecture facilitating data-intensive applications in scientific workflow systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Crawl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Altintas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th Workshop on Workflows in Support of Large-Scale Science</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Query-driven visualization in the cloud with mapreduce</title>
		<author>
			<persName><forename type="first">B</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Annual Workshop on Ultrascale Visualization</title>
		<meeting>the Fourth Annual Workshop on Ultrascale Visualization</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scheduling Scientific Workflows Elastically for Cloud Computing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Cloud Computing (CLOUD)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="746" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Nimrod/K: towards massively parallel dynamic Grid workflows</title>
		<author>
			<persName><forename type="first">D</forename><surname>Abramson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Enticott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Altinas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<meeting>of International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Kepler: an extensible system for design and execution of scientific workflows</title>
		<author>
			<persName><forename type="first">I</forename><surname>Altintas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berkley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ludascher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scientific and Statistical Database Management</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="423" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On the use of cloud computing for scientific workflows</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hoffa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Deelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keahey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Berriman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Good</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Fourth International Conference on eScience</title>
		<meeting><address><addrLine>Indianapolis, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Pegasus: Mapping Large-Scale Workflows to Distributed Resources</title>
		<author>
			<persName><forename type="first">E</forename><surname>Deelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vahi</surname></persName>
		</author>
		<imprint>
			<publisher>Workflows for e-Science</publisher>
			<biblScope unit="page" from="376" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<pubPlace>Berlin Heidelberg New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Nephele: efficient parallel data processing in the cloud</title>
		<author>
			<persName><forename type="first">D</forename><surname>Warneke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Many-Task Computing on Grids and Supercomputers</title>
		<meeting>the 2nd Workshop on Many-Task Computing on Grids and Supercomputers</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An evolutionary game theoretic approach to adaptive and stable application deployment in clouds</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vasilakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Oba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 2nd workshop on Bioinspired algorithms for distributed systems</title>
		<meeting>eeding of the 2nd workshop on Bioinspired algorithms for distributed systems</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An algebraic approach for data-centric scientific workflows</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ogasawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of VLDB Endowment</title>
		<meeting>of VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1328" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Principles of Distributed Database Systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Özsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">3</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>rd edn.</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Schedule optimization for data processing flows on the cloud</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kllapi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sitaridi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Tsangaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ioannidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page">289</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Cormen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
	<note>Introduction to Algorithms, 3rd edn</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Parallelism in Bioinformatics Workflows</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A V C</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Rössle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Bisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computing for Computational Science-VECPAR 2004</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="583" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">VisTrails: visualization meets data management</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Callahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Vo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD International Conference on Management of Data</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="745" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Towards a cost model for scheduling scientific workflows activities in cloud environments</title>
		<author>
			<persName><forename type="first">V</forename><surname>Viana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>De Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE World Congress on Services (SERVICES)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="216" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Making a cloud provenance-aware</title>
		<author>
			<persName><forename type="first">K.-K</forename><surname>Muniswamy-Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Macko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First workshop on on Theory and practice of provenance</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A framework for collecting provenance in data-centric scientific workflows</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Simmhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Plale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICWS</title>
		<imprint>
			<biblScope unit="page" from="427" to="436" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The open provenance model: an overview</title>
		<author>
			<persName><forename type="first">L</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Futrelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcgrath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Paulson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Provenance and Annotation of Data and Processes</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="323" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Greenwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Goble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Addis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Oinn</surname></persName>
		</author>
		<title level="m">Provenance of e-Science Experiments-Experience from Bioinformatics. UK OST e-Science second All Hands Meeting</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="223" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">UML Distilled: A Brief Guide to the Standard Object Modeling Language, 3rd edn</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fowler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Addison-Wesley Professional</publisher>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Nested parallelism for multi-core HPC systems using Java</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Parallel Distrib. Comput</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="532" to="545" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Kairos: An Architecture for Securing Authorship and Temporal Information of Provenance Data in Grid-Enabled Workflow Management Systems</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M R</forename><surname>Gadelha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Scientific Workflows and Business Workflow Standards in e-Science (SWBES 2008)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="597" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Shared Enterprise File System for Amazon S3 Cloud Storage | SubCloud</title>
		<author>
			<persName><surname>Subcloud</surname></persName>
		</author>
		<ptr target="http://www.subcloud.com/" />
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">An adaptive parallel execution strategy for cloud-based scientific workflows</title>
		<author>
			<persName><forename type="first">D</forename><surname>De Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ogasawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ocaña</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Baião</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
		<idno type="DOI">10.1002/cpe.1880</idno>
	</analytic>
	<monogr>
		<title level="j">Concurrency Computat.: Pract. Exper</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1531" to="1550" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Understanding Bioinformatics, 1 edn. Garland Science</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zvelebil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Baum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">W</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Makova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nekrutenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Hardison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comparative genomics. ARGHG</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="56" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Genomics of the evolutionary process</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Ecol. Evol</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="316" to="321" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Recent developments in the MAFFT multiple sequence alignment program</title>
		<author>
			<persName><forename type="first">K</forename><surname>Katoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Toh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brief. Bioinform</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="286" to="298" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Parallelization of the MAFFT multiple sequence alignment program</title>
		<author>
			<persName><forename type="first">K</forename><surname>Katoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Toh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="1899" to="1900" />
			<date type="published" when="2010">2010</date>
			<pubPlace>Oxford, England</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Kalign-an accurate and fast multiple sequence alignment algorithm</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lassmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L L</forename><surname>Sonnhammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">298</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">CLUSTAL W: improving the sensitivity of progressive multiple sequence alignment through sequence weighting, position-specific gap penalties and weight matrix choice</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="4673" to="4680" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">ProbCons: probabilistic consistencybased multiple sequence alignment</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S P</forename><surname>Mahabhashyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brudno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Batzoglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="330" to="340" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Assessment of methods for amino acid matrix selection and their use on empirical data shows that ad hoc assumptions for choice of matrix are not justified</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Keane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Creevey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Pentony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Naughton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Mclnerney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Evol. Biol</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">RAxML-VI-HPC: maximum likelihood-based phylogenetic analyses with thousands of taxa and mixed models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stamatakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="2688" to="2690" />
			<date type="published" when="2006">2006</date>
			<pubPlace>Oxford, England</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Assessment of phylogenomic and orthology approaches for phylogenetic inference</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Dutilh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Van Noort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T J M</forename><surname>Van Der Heijden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Boekhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Snel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Huynen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="815" to="824" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Apache Software Foundation. Hadoop. Internet Website</title>
		<imprint>
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">MapReduce: simplified data processing on large clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="113" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">X</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A MapReduce-Enabled Scientific Workflow Composition Framework. ICWS</title>
		<imprint>
			<biblScope unit="page" from="663" to="670" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Apache Hadoop Web page</title>
		<author>
			<persName><surname>Hadoop</surname></persName>
		</author>
		<ptr target="http://hadoop.apache.org/" />
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">MPJ: MPI-like message passing for Java</title>
		<author>
			<persName><forename type="first">B</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Getov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Skjellum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CCPE</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1019" to="1038" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">NCBI Reference Sequences: current status, policy and new initiatives</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Pruitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tatusova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Klimke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Maglott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="32" to="D36" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
