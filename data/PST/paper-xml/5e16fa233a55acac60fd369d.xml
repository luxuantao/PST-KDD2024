<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">iDLG: Improved Deep Leakage from Gradients</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-01-08">8 Jan 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bo</forename><surname>Zhao</surname></persName>
							<email>bo.zhao@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">The University of Edinburgh</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Konda</forename><forename type="middle">Reddy</forename><surname>Mopuri</surname></persName>
							<email>kmopuri@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">The University of Edinburgh</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
							<email>hbilen@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">The University of Edinburgh</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">iDLG: Improved Deep Leakage from Gradients</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-01-08">8 Jan 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2001.02610v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It is widely believed that sharing gradients will not leak private training data in distributed learning systems such as Collaborative Learning and Federated  Learning, etc. Recently, Zhu et al. [1]  presented an approach which shows the possibility to obtain private training data from the publicly shared gradients. In their Deep Leakage from Gradient (DLG) method, they synthesize the dummy data and corresponding labels with the supervision of shared gradients. However, DLG has difficulty in convergence and discovering the ground-truth labels consistently. In this paper, we find that sharing gradients definitely leaks the ground-truth labels. We propose a simple but reliable approach to extract accurate data from the gradients. Particularly, our approach can certainly extract the ground-truth labels as opposed to DLG, hence we name it Improved DLG (iDLG). Our approach is valid for any differentiable model trained with cross-entropy loss over one-hot labels. We mathematically illustrate how our method can extract ground-truth labels from the gradients and empirically demonstrate the advantages over DLG.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In multi-node distributed learning systems such as Collaborative Learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> and Federated Learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>, it is widely believed that sharing gradients between nodes will not leak the private training data. In the popular setup, all the individual participants aim to learn a shared model in a centralized or decentralized manner. They would share the individual gradients and update the model parameters with the aggregated gradients. In these frameworks, it is a common practice to share only the gradients in order protect the proprietary data. However, recent work by Zhu et al., "Deep Leakage from Gradient" (DLG) <ref type="bibr" target="#b0">[1]</ref> showed the possibility to steal the private training data from the shared gradients of other participants.</p><p>The main idea of DLG is to generate dummy data and corresponding labels via matching the dummy gradients to the shared gradients. Specifically, they start with randomly initialized the dummy data and labels. From there, they compute dummy gradients over the current shared model in the distributed setup. Via minimizing the difference between dummy gradients and the shared real gradients, they iteratively update the dummy data and labels simultaneously. Although DLG works, we find that it is not able to reliably extract the ground-truth labels or generate good quality dummy data.</p><p>In this paper, we propose a simple but definitely valid approach to extract the ground-truth labels from the shared gradients. By derivation, we demonstrate that the gradient of the classification (cross-entropy) loss w.r.t. the correct label activation (in the output layer) lies in (−1, 0), while those of other labels lie in (0, 1). Hence, the signs of gradients w.r.t. correct and wrong labels are opposite. When the gradients w.r.t. the outputs (logits) are not accessible, we show that the gradients w.r.t. the last-layer weights (between the output layer and the layer in front of it) also follow this rule. With this rule, we can identify the ground-truth labels based on the shared gradients. In other words, the ground-truth labels are definitely leaked by sharing gradients of a Neural Network (NN) trained with cross-entropy loss. This enables us to always extract the ground-truth labels and significantly simplify the objective of DLG <ref type="bibr" target="#b0">[1]</ref> in order to extract good-quality data. Hence, we name our approach, Improved DLG (iDLG). The main contributions of our work includes:</p><p>• By revealing the relationship between labels and signs of gradients, we present an analytical procedure to extract the ground-truth labels from the shared gradients with 100% accuracy, which facilitates the data extraction with better fidelity.</p><p>• We empirically demonstrate the advantages of iDLG over DLG <ref type="bibr" target="#b0">[1]</ref> via comparing the accuracy of extracted labels and the fidelity of extracted data on three datasets.</p><p>The rest of the paper is organised as follows: Section 2 presents the analytical procedure to extract the ground-truth labels from the shared gradients and the proposed iDLG method. Section 3 demonstrates the advantages of iDLG over DLG through the experimental evaluation, and Section 4 concludes the paper with discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Recent work by Zhu et al. <ref type="bibr" target="#b0">[1]</ref> presents an approach (DLG) to steal the proprietary data protected by the participants in distributed learning from the shared gradients. In their method, they attempt to generate the dummy data and corresponding labels via a gradient matching objective. However, in practice, it is observed that their method generates wrong labels frequently. In this work, we present an analytical approach to extract the ground-truth labels from the shared gradients, then we can extract the data more effectively based on correct labels. Hence, we name our approach, improved Deep Leakage from Gradients (iDLG). In this section, we first present the procedure to extract the ground-truth labels. Then, we show the iDLG method based on the extracted labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Extracting Ground-truth Labels</head><p>Let us consider the classification scenario, where the NN model is generally trained with cross-entropy loss over one-hot labels, which is defined as</p><formula xml:id="formula_0">l(x, c) = − log e yc Σ j e yj ,<label>(1)</label></formula><p>where x is the input datum, c is the corresponding ground-truth label. y = [y 1 , y 2 , ...] is the outputs (logits), and y i denotes the score (confidence) predicted for the i th class. Then, the gradients of the loss w.r.t. each of the outputs is</p><formula xml:id="formula_1">g i = ∂l(x, c) ∂y i = − ∂ log e yc − ∂ log Σ j e yj ∂y i =        −1+ e yi Σ j e yj , if i = c e yi Σ j e yj , else<label>(2)</label></formula><p>As the probability e y i Σj e y j ∈ (0, 1), we have g i ∈ (−1, 0) when i = c and g i ∈ (0, 1) when i = c. Hence, we can identify the ground-truth label as the index of the output that has the negative gradient. However, we may not be able to access the gradients w.r.t. the outputs y, as they are not included in the shared gradients ∇W which are the derivatives w.r.t. the weights of the model W. We find that the gradient vector ∇W i L w.r.t. the weights W i L connected to the i th logit in the output layer can be written as</p><formula xml:id="formula_2">∇W i L = ∂l(x, c) ∂W i L = ∂l(x, c) ∂y i • ∂y i ∂W i L = g i • ∂(W i L T a L−1 + b i L ) ∂W i L = g i • a L−1 ,<label>(3)</label></formula><p>where the network has L layers, y = a L is the output layer activations, b i L is the bias parameter, and</p><formula xml:id="formula_3">y i = W i L T a L−1 + b i L .</formula><p>As the activation vector a L−1 is independent of the class (logit) index i, we can easily identify the ground-truth label according to the sign of ∇W i L which is different from others. Therefore, the ground-truth label c is predicted as</p><formula xml:id="formula_4">c = i, s.t. ∇W i L T • ∇W j L ≤ 0, ∀j = i<label>(4)</label></formula><p>When the non-negative activation function, e.g. ReLU and Sigmoid, is used, the signs of ∇W i L and g i are the same. Hence, we can simply identify the ground-truth label whose corresponding ∇W i L is negative. With this rule, it is easy to identify the ground-truth label c of the private training datum x from shared gradients ∇W. Note that this rule is independent of the model architectures and parameters. In other words, this holds for any network at any training stage from any randomly initialized parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Improved DLG (iDLG)</head><p>Based on the extracted ground-truth labels, we propose the improved DLG (iDLG) which is more stable and efficient to optimize. The algorithm is illustrated in Algorithm 1. The iDLG procedure starts with the differentiable learning model F (x; W) with the model parameters W, and the gradients ∇W calculated based on private training datum (x, c). The first step is to extract the ground-truth label c from the shared gradients ∇W as in eq <ref type="bibr" target="#b3">(4)</ref>.</p><p>Then, we randomly initialize the dummy datum x ← − N (0, 1). We calculate the dummy gradients ∇W based on the dummy datum and the extracted label (x , c ). The training objective is to match the dummy gradients with the shared gradients, i.e., to minimize</p><formula xml:id="formula_5">L G = ∇W − ∇W 2 F (5)</formula><p>Based on this training objective, we update the dummy datum x by gradient descent</p><formula xml:id="formula_6">x ← x − η∇ x L G<label>(6)</label></formula><p>for N training iterations, where η is the learning rate. LG = ∇W − ∇W 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F</head><p>Calculate the loss (difference between gradients). 6:</p><p>x ← − x − η∇ x LG Update the dummy datum. 7: end for <ref type="bibr" target="#b2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we empirically demonstrate the advantages of our (iDLG) method over DLG <ref type="bibr" target="#b0">[1]</ref>. We perform experiments on the classification task over three datasets: MNIST <ref type="bibr" target="#b7">[8]</ref>, CIFAR-100 <ref type="bibr" target="#b8">[9]</ref>, and LFW <ref type="bibr" target="#b9">[10]</ref> with 10, 100, and 5749 categories respectively. Following the settings in <ref type="bibr" target="#b0">[1]</ref>, we use the randomly initialized LeNet for all experiments. L-BFGS <ref type="bibr" target="#b10">[11]</ref> with learning rate 1 is used as the optimizer. For fast training, we resize all images in LFW to 32 × 32.</p><p>For DLG <ref type="bibr" target="#b0">[1]</ref>, as described by the authors, we start the procedure with the randomly initialized dummy data and outputs (x , y ), then iteratively update them to minimize the gradient matching objective. For both two algorithms, we perform the optimization for 300 iterations, and evaluate the performance in terms of (i) the accuracy of the extracted labels c , and (ii) the fidelity of the extracted Dataset DLG iDLG MNIST 89.9% 100.0% CIFAR-100 83.3% 100.0% LFW 79.1% 100.0% Table <ref type="table">1</ref>: Accuracy of the extracted labels for DLG <ref type="bibr" target="#b0">[1]</ref> and iDLG. Note that iDLG always extracts the correct label as opposed to DLG which extracts wrong labels frequently. threshold of good fidelity. From left to right, the threshold decreases and the fidelity requirement improves. Obviously, the proposed iDLG consistently outperforms DLG in recovering data with significant margin on three tasks. The advantage of iDLG is remarkable on the hard task of LFW. data x . We run all experiments for 1000 times with randomly initialized networks and report the mean values. The code has been released on GitHub<ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Accuracy of Extracted Labels</head><p>Table <ref type="table">1</ref> shows the accuracy of the two methods to recover the ground-truth labels. It is clear that iDLG always extracts the correct label as opposed to DLG which extracts wrong labels many times. Specifically, the accuracy of DLG on MNIST, CIFAR-100 and LFW is 89.9%, 83.3% and 79.1% respectively, which shows that DLG suffers more on harder tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Fidelity of Extracted Data</head><p>In this subsection, we compare the fidelity of two data extraction methods (DLG and iDLG) by calculating the MSE (mean square error) between the dummy and original data. We vary the (MSE) threshold of good fidelity. Figure <ref type="figure" target="#fig_0">1</ref> shows the fidelity comparison of two methods under different thresholds over three datasets.</p><p>The plots show the percentage of extracting (or generating) data with good fidelity. The x-axis indicates the (MSE) threshold for good fidelity. For example, 0.001 means that we consider it good fidelity when the MSE between dummy and original data is less than 0.001. From left to right, the threshold decreases and the fidelity requirement improves. Obviously, the proposed iDLG consistently outperforms DLG in recovering data with significant margin on three tasks. The advantage of iDLG is remarkable on the hard task of LFW. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and Conclusion</head><p>In this paper, we present an effective approach to steal the data and the corresponding labels from the shared gradients in a distributed training scenario. Particularly, we analytically illustrate the relationship between the labels and the signs of corresponding gradients. Based on this, our approach can extract the ground-truth labels with 100% accuracy which facilitates the data extraction with increased fidelity. Currently, our method works with a simplified scenario of sharing gradients of every datum. In other words, iDLG can identify the ground-truth labels only if gradients w.r.t. every sample in a training batch are provided.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>Improved Deep Leakage from Gradients (iDLG) Require: F (x; W): Differentiable learning model, W: Model parameters, ∇W: Gradients produced by private training datum (x, c), N : maximum number of iterations. η: learning rate. Ensure: (x , c ): Dummy datum and label. 1: c ← − i s.t. ∇W i L T • ∇W j L ≤ 0, ∀j = i Extract the ground-truth label. 2: x ← − N (0, 1) Initialize the dummy datum. 3: for i ← − 1 to N do 4: ∇W ← − ∂l(F (x ; W), c )/∂W Calculate the dummy gradients. 5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FidelityFigure 1 :</head><label>1</label><figDesc>Figure 1: The Fidelity comparison of DLG [1] and iDLG on three datasets. The x-axis denotes the (MSE)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2</head><label>2</label><figDesc>Figure 2 gives an example of the training process of DLG (left) and iDLG (right) on LFW face dataset. The first image is the (original) private training image. The followings are the extracted images in different training iterations. It is clear that the training of iDLG is easier to converge. iDLG needs only 90 training iterations to get the similar performance which requires DLG to train for 200 iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of the training process of DLG (left) and iDLG (right) on LFW face dataset. The first image is the (original) private training image, and the followings are the extracted images at different training iterations. It is clear that the training of iDLG is easier to converge.</figDesc><graphic url="image-1.png" coords="5,108.00,72.00,194.04,76.25" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/PatrickZH/Improved-Deep-Leakage-from-Gradients</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep leakage from gradients</title>
		<author>
			<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Privacy-preserving deep learning</title>
		<author>
			<persName><forename type="first">Reza</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGSAC conference on computer and communications security</title>
				<meeting>the 22nd ACM SIGSAC conference on computer and communications security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1310" to="1321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Collaborative learning for deep neural networks</title>
		<author>
			<persName><forename type="first">Guocong</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1832" to="1841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Inference attacks against collaborative learning</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congzheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emiliano</forename><surname>De Cristofaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04049</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Konečnỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananda</forename><surname>Theertha Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Bacon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.05492</idno>
		<title level="m">Strategies for improving communication efficiency</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Federated learning: Collaborative machine learning without centralized training data</title>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Google Research Blog</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Tian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anit</forename><surname>Kumar Sahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07873</idno>
		<title level="m">Federated learning: Challenges, methods, and future directions</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database forstudying face recognition in unconstrained environments</title>
		<author>
			<persName><forename type="first">Marwan</forename><surname>Gary B Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><surname>Mattar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the limited memory bfgs method for large scale optimization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical programming</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="503" to="528" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
