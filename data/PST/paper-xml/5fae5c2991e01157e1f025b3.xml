<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Review of Uncertainty Quantification in Deep Learning: Techniques, Applications and Challenges</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Moloud</forename><surname>Abdar</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Farhad</forename><surname>Pourpanah</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sadiq</forename><surname>Hussain</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dana</forename><surname>Rezazadegan</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Li</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><surname>Ghavamzadeh</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Paul</forename><surname>Fieguth</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE, U</roleName><forename type="first">Abbas</forename><surname>Khosravi</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Rajendra</forename><surname>Acharya</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vladimir</forename><surname>Makarenkov</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Saeid</forename><surname>Nahavandi</surname></persName>
						</author>
						<title level="a" type="main">A Review of Uncertainty Quantification in Deep Learning: Techniques, Applications and Challenges</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Artificial intelligence</term>
					<term>Uncertainty quantification</term>
					<term>Deep learning</term>
					<term>Machine learning</term>
					<term>Bayesian statistics</term>
					<term>Ensemble learning</term>
					<term>Reinforcement learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Uncertainty quantification (UQ) plays a pivotal role in reduction of uncertainties during both optimization and decision making processes. It can be applied to solve a variety of real-world applications in science and engineering. Bayesian approximation and ensemble learning techniques are two most widely-used UQ methods in the literature. In this regard, researchers have proposed different UQ methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classification and segmentation), natural language processing (e.g., text classification, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ methods used in deep learning. Moreover, we also investigate the application of these methods in reinforcement learning (RL). Then, we outline a few important applications of UQ methods. Finally, we briefly highlight the fundamental research challenges faced by UQ methods and discuss the future research directions in this field.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>I N everyday scenarios, we deal with uncertainties in numerous fields, from invest opportunities and medical diagnosis to sporting games and weather forecast, with an objective to make decision based on collected observations and uncertain domain knowledge. Nowadays, we can rely on models developed using machine and deep learning techniques can quantify the uncertainties to accomplish statistical inference <ref type="bibr" target="#b0">[1]</ref>. It is very important to evaluate the • M. Abdar, A. Khosravi and S. Nahavandi are with the Institute for Intelligent Systems Research and Innovation (IISRI), Deakin University, Australia (e-mails: m.abdar1987@gmail.com, mabdar@deakin.edu.au, abbas.khosravi@deakin.edu.au &amp; saeid.nahavandi@deakin.edu.au). • F. Pourpanah is with the College of Mathematics and Statistics, Shenzhen University, Shenzhen, China (e-mail: farhad@szu.edu.cn). • S. Hussain is with the System Administrator, Dibrugarh University, Dibrugarh, India (e-mail: sadiq@dibru.ac.in). • D. Rezazadegan is with the Department of Computer Science and Software Engineering, Swinburne University of Technology, Melbourne, Australia (e-mail: drezazadegan@swin.edu.au). • L. Liu is with the Center for Machine Vision and Signal Analysis, University of Oulu, Oulu, Finland (e-mail: li.liu@oulu.fi). • M. Ghavamzadeh is with the Google research (e-mail: ghavamza@google.com). • P. Fieguth is with the Department of Systems Design Engineering, University of Waterloo, Waterloo, Canada (e-mail: pfieguth@uwaterloo.ca). • U. R. Acharya is with the Department of Electronics and Computer Engineering, Ngee Ann Polytechnic, Clementi, Singapore (e-mail: aru@np.edu.sg). • V. Makarenkov is with the Department of Computer Science, University of Quebec in Montreal, Montreal (QC), Canada (e-mail: makarenkov.vladimir@uqam.ca). • * Corresponding author: Moloud Abdar, m.abdar1987@gmail.com</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Epistemic Aleatoric</head><p>Fig. <ref type="figure" target="#fig_16">1:</ref> A schematic view of main differences between aleatoric and epistemic uncertainties. efficacy of artificial intelligence (AI) systems before its usage <ref type="bibr" target="#b1">[2]</ref>. The predictions made by such models are uncertain as they are prone to noises and wrong model inference besides the inductive assumptions that are inherent in case of uncertainty. Thus, it is highly desirable to represent uncertainty in a trustworthy manner in any AI-based systems. Such automated systems should be able to perform accurately by handling uncertainty effectively. Principle of uncertainty plays an important role in AI settings such as concrete learning algorithms <ref type="bibr" target="#b2">[3]</ref>, and active learning (AL) <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> data <ref type="bibr" target="#b5">[6]</ref>. Estimating knowledge uncertainty is more difficult compared to data uncertainty which naturally measures it as a result of maximum likelihood training. Sources of uncertainty in prediction are essential to tackle the uncertainty estimation problem <ref type="bibr" target="#b6">[7]</ref>. There are two main sources of uncertainty, conceptually called aleatoric and epistemic uncertainties <ref type="bibr" target="#b7">[8]</ref> (see Fig. <ref type="figure" target="#fig_2">1</ref>). Irreducible uncertainty in data giving rise to uncertainty in predictions is an aleatoric uncertainty (also known as data uncertainty). This type of uncertainty is not the property of the model, but rather is an inherent property of the data distribution; hence it is irreducible. Another type of uncertainty is epistemic uncertainty (also known as knowledge uncertainty) that occurs due to inadequate knowledge and data. One can define models to answer different human questions poised in model-based prediction. In the case of data-rich problem, there is a collection of massive data but it may be informatively poor <ref type="bibr" target="#b9">[10]</ref>. In such cases, AIbased methods can be used to define the efficient models which characterize the emergent features from the data. Very often these data are incomplete, noisy, discordant and multimodal <ref type="bibr" target="#b0">[1]</ref>. Uncertainty quantification (UQ) underpins many critical decisions today. Predictions made without UQ are usually not trustworthy and inaccurate. To understand the Deep Learning (DL) <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> process life cycle, we need to comprehend the role of UQ in DL. The DL models start with the collection of most comprehensive and potentially relevant datasets available for decision making process. The DL scenarios are designed to meet some performance goals to select the most appropriate DL architecture after training the model using the labeled data. The iterative training process optimizes different learning parameters, which will  <ref type="figure">3</ref>: A graphical representation of two different uncertainty-aware (UA) models, reproduced based on <ref type="bibr" target="#b13">[14]</ref>. be 'tweaked' until the network provides a satisfactory level of performance. There are several uncertainties that need to be quantified in the steps involved. The uncertainties that are obvious in these steps are the following: (i) selection and collection of training data, (ii) completeness and accuracy of training data, (iii) understanding the DL (or traditional machine learning) model with performance bounds and its limitations, and (iv) uncertainties corresponds to the performance of the model based on operational data <ref type="bibr" target="#b12">[13]</ref>. Data driven approaches such as DL associated with UQ poses at least four overlapping groups of challenges: (i) absence of theory, (ii) absence of casual models, (iii) sensitivity to imperfect data, and (iv) computational expenses. To mitigate such challenges, ad hoc solutions like the study of model variability and sensitivity analysis are sometimes employed. Uncertainty estimation and quantification have been extensively studied in DL and traditional machine learning. In the following, we provide a brief summary of some recent studies that examined the effectiveness of various methods to deal with uncertainties. A schematic comparison of the three different uncertainty models <ref type="bibr" target="#b8">[9]</ref> (MC dropout, Boostrap model and GMM is provided in Fig. <ref type="figure" target="#fig_0">2</ref>. In addition, two graphical representations of uncertainty-aware models (BNN) vs OoD classifier) is illustrated in Fig. <ref type="figure">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Research Objectives and Outline</head><p>In the era of big data, ML and DL, intelligent use of different raw data has an enormous potential to benefit wide variety of areas. However, UQ in different ML and DL methods can significantly increase the reliability of their results. Ning et al. <ref type="bibr" target="#b14">[15]</ref> summarized and classified the main contributions of the data-driven optimization paradigm under uncertainty. As can be observed, this paper reviewed the data-driven optimization only. In another study, Kabir et al. <ref type="bibr" target="#b15">[16]</ref> reviewed Neural Network-based UQ. The authors focused on probabilistic forecasting and prediction intervals (PIs) as they are among most widely used techniques in the literature for UQ. We have noticed that, from 2010 to 2020 (end of June), more than 2500 papers on UQ in AI have been published in various fields (e.g., computer vision, image processing, medical image analysis, signal processing, natural language processing, etc.). In one hand, we ignore large number of papers due to lack of adequate connection with the subject of our review. On the other hand, although many papers that we have reviewed have been published in related conferences and journals, many papers have been found on open-access repository as electronic preprints (i.e. arXiv) that we reviewed them due to their high quality and full relevance to the subject. We have tried our level to best to cover most of the related articles in this review paper. It is worth mentioning that this review can, therefore, serve as a comprehensive guide to the readers in order to steer this fast-growing research field. Unlike previous review papers in the field of UQ, this study reviewed most recent articles published in quantifying uncertainty in AI (ML and DL) using different approaches. In addition, we are keen to find how UQ can impact the real cases and solve uncertainty in AI can help to obtain reliable results. Meanwhile, finding important chats in existing methods is a great way to shed light on the path to the future research. In this regard, this review paper gives more inputs to future researchers who work on UQ in ML and DL. We investigated more recent studies in the domain of UQ applied in ML and DL methods. Therefore, we summarized few existing studies on UQ in ML and DL. It is worth mentioning that the main purpose of this study is not to compare the performance of different UQ methods proposed because these methods are introduced for different data and specific tasks. For this reason, we argue that comparing the performance of all methods is beyond the scope of this study. For this reason, this study mainly focuses on important areas including DL, ML and Reinforcement Learning (RL). Hence, the main contributions of this study are as follows:</p><p>• To the best of our knowledge, this is the first comprehensive review paper regarding UQ methods used in ML and DL methods which is worthwhile for researchers in this domain.</p><p>• A comprehensive review of newly proposed UQ methods is provided.</p><p>• Moreover, the main categories of important applications of UQ methods are also listed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The main research gaps of UQ methods are pointed out.</p><p>• Finally, few solid future directions are discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>In this section, we explained the structure of feed-forward neural network followed by Bayesian modeling to discuss the uncertainty in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Feed-forward neural network</head><p>In this section, the structure of a single-hidden layer neural network <ref type="bibr" target="#b16">[17]</ref> is explained, which can be extended to multiple layers. Suppose x is a D-dimensional input vector, we use a linear map W 1 and bias b to transform x into a row vector with Q elements, i.e., W 1 x + b. Next a non-linear transfer function σ(.), such as rectified linear (ReLU), can be applied to obtain the output of the hidden layer. Then another linear function W 2 can be used to map hidden layer to the output:</p><formula xml:id="formula_0">ŷ = σ(xW 1 + b)W 2<label>(1)</label></formula><p>For classification, to compute the probability of X belonging to a label c in the set {1, ..., C}, the normalized score is obtained by passing the model output ŷ through a softmax function pd = exp(ŷ d )/( d exp(ŷ d )). Then the softmax loss is used:</p><formula xml:id="formula_1">E W1,W2,b (X, Y ) = − 1 N N i=1 log(p i,ci )<label>(2)</label></formula><p>where X = (x 1 , ..., x N ) and Y = (y 1 , ..., y N ) are inputs and their corresponding outputs, respectively. For regression, the Euclidean loss can be used:</p><formula xml:id="formula_2">E W1,W2,b (X, Y ) = 1 2N N i=1 y i − ŷ 2<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Uncertainty Modeling</head><p>As mentioned above, there are two main uncertainties: epistemic (model uncertainty) and aleatoric (data uncertainty) <ref type="bibr" target="#b17">[18]</ref>. The aleatoric uncertainty has two types: homoscedastic and heteroscedastic <ref type="bibr" target="#b18">[19]</ref>.</p><p>The predictive uncertainty (PU) consists of two parts: (i) epistemic uncertainty (EU), and (ii) aleatoric uncertainty (AU), which can be written as sum of these two parts:</p><formula xml:id="formula_3">P U = EU + AU.<label>(4)</label></formula><p>Epistemic uncertainties can be formulated as probability distribution over model parameters. Let D tr = {X, Y } = {(x i , y i )} N i=1 denotes a training dataset with inputs x i ∈ D and their corresponding classes y i ∈ {1, ..., C}, where C represents the number of classes. The aim is to optimize the parameters, i.e., ω, of a function y = f ω (x) that can produce the desired output. To achieve this, the Bayesian approach defines a model likelihood, i.e., p(y|x, ω). For classification, the softmax likelihood can be used:</p><formula xml:id="formula_4">p(y = c|x, ω) = exp(f ω c (x)) c exp(f ω c (x)) .<label>(5)</label></formula><p>and the Gaussian likelihood can be assumed for regression:</p><formula xml:id="formula_5">p(y|x, ω) = N (y; f ω (x), τ −1 I),<label>(6)</label></formula><p>where τ represents the model precision.</p><p>The posterior distribution, i.e., p(ω|x, y), for a given dataset D tr over ω by applying Bayes' theorem can be written as follows:</p><formula xml:id="formula_6">p(ω|X, Y ) = p(Y |X, ω)p(ω)) p(Y |X) . (<label>7</label></formula><formula xml:id="formula_7">)</formula><p>For a given test sample x * , a class label with regard to the p(ω|X, Y ) can be predicted:</p><formula xml:id="formula_8">p(y * |x * , X, Y ) = p(y * |x * , ω)p(ω|X, Y )dω. (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>This process is called inference or marginalization. However, p(ω|X, Y ) cannot be computed analytically, but it can be approximated by variational parameters, i.e., q θ (ω).</p><p>The aim is to approximate a distribution that is close to the posterior distribution obtained by the model. As such, the Kullback-Leibler (KL) <ref type="bibr" target="#b19">[20]</ref> divergence is needed to be minimised with regard to θ. The level of similarity among two distributions can be measured as follows:</p><p>KL(q θ (ω) p(ω|X, Y )) = q θ (ω) log q θ (ω) p(ω|X, Y ) dω. <ref type="bibr" target="#b8">(9)</ref> The predictive distribution can be approximated by minimizing KL divergence, as follows:</p><p>p(y * |x * , X, Y ) ≈ p(y * |x * , ω)q * θ (ω)dω =: q * θ (y * , x * ), <ref type="bibr" target="#b9">(10)</ref> where q * θ (ω) indicates the optimized objective. KL divergence minimization can also be rearranged into the evidence lower bound (ELBO) maximization <ref type="bibr" target="#b20">[21]</ref>: L V I (θ) := q θ (ω) log p(Y |X, ω)dω − KL(q θ (ω) p(ω)), <ref type="bibr" target="#b10">(11)</ref> where q θ (ω) is able to describe the data well by maximizing the first term, and be as close as possible to the prior by minimizing the second term. This process is called variational inference (VI). Dropout VI is one of the most common approaches that has been widely used to approximate inference in complex models <ref type="bibr" target="#b21">[22]</ref>. The minimization objective is as follows <ref type="bibr" target="#b22">[23]</ref>:</p><formula xml:id="formula_10">L(θ, p) = − 1 N N i=1 log p(y i |x i , ω) + 1 − p 2N θ 2<label>(12)</label></formula><p>where N and P represent the number of samples and dropout probability, respectively.</p><p>To obtain data-dependent uncertainty, the precision τ in <ref type="bibr" target="#b5">(6)</ref> can be formulated as a function of data. One approach to obtain epistemic uncertainty is to mix two functions: predictive mean, i.e., f θ (x), and model precision, i.e., g θ (x), and the likelihood function can be written as y i = N (f θ (x), g θ (x) −1 ). A prior distribution is placed over the weights of the model, and then the amount of change in the weights for given data samples is computed. The Euclidian distance loss function (3) can be adapted as follows:</p><formula xml:id="formula_11">E W1,W2,b := 1 2 (y − f W1,W2,b (x))g W1,W2,b (x)(y − f W1,W2,b (x)) T − 1 2 log det g W1,W2,b + D 2 log 2π = − log N (f θ (x), g θ (x) −1 )<label>(13)</label></formula><p>The predictive variance can be obtained as follows:</p><p>V Gaussian Dropout (f) Spike-and-Slab Dropout Fig. <ref type="figure">4</ref>: A graphical representation of several different visualization of variational distributions on a simple NN which is reproduced based on <ref type="bibr" target="#b30">[31]</ref>.</p><p>this issue, BNNs/BDL <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> can be used to interpret the model parameters. BNNs/BDL are robust to overfitting problem and can be trained on both small and big datasets <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Monte Carlo (MC) dropout</head><p>As stated earlier, it is difficult to compute the exact posterior inference, but it can be approximated. In this regard, Monte Carlo (MC) <ref type="bibr" target="#b27">[28]</ref> is an effective method. Nonetheless, it is a slow and computationally expensive method when integrated into a deep architecture. To combat this, MC (MC) dropout has been introduced, which uses dropout <ref type="bibr" target="#b28">[29]</ref> as a regularization term to compute the prediction uncertainty <ref type="bibr" target="#b29">[30]</ref>. Dropout is an effective technique that has been widely used to solve over-fitting problem in DNNs. During the training process, dropout randomly drops some units of NN to avoid them from co-tuning too much. Assume a NN with L layers, which W l , b l and K l denote the weight matrices, bias vectors and dimensions of the lth layer, respectively. The output of NN and target class of the ith input x i (i = 1, ..., N ) are indicated by ŷi and y i , respectively. The objective function using L 2 regularization can be written as:</p><formula xml:id="formula_12">L dropout := 1 N N i=1 E(y i , ŷi ) + λ L l=1 ( W i 2 2 + b i<label>2</label></formula><p>2 ) (15)</p><p>Dropout samples binary variables for each input data and every network unit in each layer (except the output layer), with the probability p i for ith layer, if its value is 0, the unit i is dropped for a given input data. Same values are used in the backward pass to update parameters. Fig. <ref type="figure">4</ref> shows several visualization of variational distributions on a simple NN <ref type="bibr" target="#b30">[31]</ref>.</p><p>Several studies used MC dropout <ref type="bibr" target="#b31">[32]</ref> to estimate UQ. Wang et al. <ref type="bibr" target="#b32">[33]</ref> analyzed epistemic and aleatoric uncertainties for deep CNN-based medical image segmentation problems at both pixel and structure levels. They augmented the input image during test phase to estimate the transformation uncertainty. Specifically, the MC sampling was used to</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Teacher Model Student Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EMA (exponential moving average) Same</head><p>Arch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monte Carlo Dropout</head><p>Input (left atrium 3D MRI images) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Guide</head><p>Fig. <ref type="figure">5</ref>: A general view demonstrating the semi-supervised UA-MT framework applied to LA segmentation which is reproduced based on <ref type="bibr" target="#b35">[36]</ref>.</p><p>estimate the distribution of the output segmentation. Liu et al. <ref type="bibr" target="#b33">[34]</ref> proposed a unified model using SGD to approximate both epistemic and aleatoric uncertainties of CNNs in presence of universal adversarial perturbations. The epistemic uncertainty was estimated by applying MC dropout with Bernoulli distribution at the output of neurons. In addition, they introduced the texture bias to better approximate the aleatoric uncertainty. Nasir et al. <ref type="bibr" target="#b34">[35]</ref> conducted MC dropout to estimate four types of uncertainties, including variance of MC samples, predictive entropy, and Mutual Information (MI), in a 3D CNN to segment lesion from MRI sequences.</p><p>In <ref type="bibr" target="#b36">[37]</ref>, two dropout methods, i.e. element-wise Bernoulli dropout <ref type="bibr" target="#b28">[29]</ref> and spatial Bernoulli dropout <ref type="bibr" target="#b37">[38]</ref> are implemented to compute the model uncertainty in BNNs for the end-to-end autonomous vehicle control. McClure and Kriegeskorte <ref type="bibr" target="#b30">[31]</ref> expressed that sampling of weights using Bernoulli or Gaussian can lead to have a more accurate depiction of uncertainty in comparison to sampling of units. However, according to the outcomes obtained in <ref type="bibr" target="#b30">[31]</ref>, it can be argued that using either Bernoulli or Gaussian dropout can improve the classification accuracy of CNN. Based on these findings, they proposed a novel model (called spikeand-slab sampling) by combining Bernoulli or Gaussian dropout. Do et al. <ref type="bibr" target="#b38">[39]</ref> modified U-Net <ref type="bibr" target="#b39">[40]</ref>, which is a CNN-based deep model, to segment myocardial arterial spin labeling and estimate uncertainty. Specifically, batch normalization and dropout are added after each convolutional layer and resolution scale, respectively. Later, Teye et al. <ref type="bibr" target="#b40">[41]</ref> proposed MC batch normalization (MCBN) that can be used to estimate uncertainty of networks with batch normalization. They showed that batch normalization can be considered as an approximate Bayesian model. Yu et al. <ref type="bibr" target="#b35">[36]</ref> proposed a semi-supervised model to segment left atrium from 3D MR images. It consists of two modules including teacher and student, and used them in UA framework called UA self-ensembling mean teacher (UA-MT) model (see Fig. <ref type="figure">5</ref>).</p><p>As such, the student model learns from teacher model via minimizing the segmentation and consistency losses of the labeled samples and targets of the teacher model, respectively. In addition, UA framework based on MC dropout was designed to help student model to learn a better model by using uncertainty information obtained from teacher model. Table <ref type="table" target="#tab_2">1</ref> lists studies that directly applied MC dropout to approximate uncertainty along with their applications.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Markov chain Monte Carlo (MCMC)</head><p>Markov chain Monte Carlo (MCMC) <ref type="bibr" target="#b64">[65]</ref> is another effective method that has been used to approximate inference. It starts by taking random draw z 0 from distribution q(z 0 ) or q(z 0 |x). Then, it applies a stochastic transition to z 0 , as follows:</p><formula xml:id="formula_13">Z t ∼ q(z t |z t−1 , x). (<label>16</label></formula><formula xml:id="formula_14">)</formula><p>This transition operator is chosen and repeated for T times, and the outcome, which is a random variable, converges in distribution to the exact posterior. Salakhutdinov et al. <ref type="bibr" target="#b65">[66]</ref> used MCMC to approximate the predictive distribution rating values of the movies. Despite the success of the conventional MCMC, the sufficiant number of iteration is unknown. In addition, MCMC requires long time to converge to a desired distribution <ref type="bibr" target="#b27">[28]</ref>. Several studies have been conducted to overcome these shortcomings. For example, Salimans et al. <ref type="bibr" target="#b66">[67]</ref> expanded space into a set of auxiliary random variables and interpreted the stochastic Markov chain as a variational approximation. The stochastic gradient MCMC (SG-MCMC) <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b68">[69]</ref> was proposed to train DNNs. It only needs to estimate the gradient on small sets of mini-batches. In addition, SG-MCMC can be converged to the true posterior by decreasing the step sizes <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b70">[71]</ref>. Gong et al. <ref type="bibr" target="#b71">[72]</ref> combined amortized inference with SG-MCMC to increase the generalization ability of the model. Li et al. <ref type="bibr" target="#b41">[42]</ref> proposed an accelerating SG-MCMC to improve the speed of the conventional SG-MCMC (see Fig. <ref type="figure" target="#fig_3">6</ref> for implementation of different SG-MCMC models). However, in short time, SG-MCMC suffers from a bounded estimation error <ref type="bibr" target="#b72">[73]</ref> and it loses surface when applied to the multi-layer networks <ref type="bibr" target="#b73">[74]</ref>. In this regard, Zhang et al. <ref type="bibr" target="#b74">[75]</ref> developed a cyclical SG-MCMC (cSG-MCMC) to compute the posterior over the weights of neural networks. Specifically, a cyclical stepsize was used instead of the decreasing one. Large stepsize allows the sampler to take large moves, while small stepsize attempts the sampler to explore local mode.</p><p>Although SG-MCMC reduces the computational complexity by using a smaller subset, i.e. mini-batch, of dataset at each iteration to update the model parameters, those small subsets of data add noise into the model, and consequently increase the uncertainty of the system. To alleviate this, Luo et al. <ref type="bibr" target="#b75">[76]</ref> introduced a sampling method called the thermostat-assisted continuously tempered Hamiltonian Monte Carlo, which is an extended version of the conventional Hamiltonian MC (HMC) <ref type="bibr" target="#b76">[77]</ref>. Note that HMC is a MCMC method <ref type="bibr" target="#b77">[78]</ref>. Specifically, they used Nosé-Hoover thermostats <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b79">[80]</ref> to handle the noise generated by minibatch datasets. Later, dropout HMC (D-HMC) <ref type="bibr" target="#b77">[78]</ref> was proposed for uncertainty estimation, and compared with SG-MCMC <ref type="bibr" target="#b67">[68]</ref> and SGLD <ref type="bibr" target="#b80">[81]</ref>. Besides, MCMC was integrated into the generative based methods to approximate posterior. For example, in <ref type="bibr" target="#b81">[82]</ref>, MCMC was applied to the stochastic object models, which is learned by generative adversarial networks (GANs), to approximate the ideal observer. In <ref type="bibr" target="#b82">[83]</ref>, a visual tracking system based on a variational autoencoder (VAE) MCMC (VAE-MCMC) was proposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Variational Inference (VI)</head><p>The variational inference (VI) is an approximation method that learns the posterior distribution over BNN weights. VI- Fig. <ref type="figure">7</ref>: A summary of various VI methods for BDL which is reproduced based on <ref type="bibr" target="#b83">[84]</ref>. Note that W eight sharing (M ean − f ield assumption + dramatic reduction) is added based on the proposed method in <ref type="bibr" target="#b83">[84]</ref>.</p><p>based methods consider the Bayesian inference problem as an optimization problem which is used by the SGD to train DNNs. Fig. <ref type="figure">7</ref> summaries various VI methods for BNN <ref type="bibr" target="#b83">[84]</ref>.</p><p>For BNNs, VI-based methods aim to approximate posterior distributions over the weights of NN. To achieve this, the loss can be defined as follows:</p><formula xml:id="formula_15">L(Φ) ≈ 1 2|D| |D| i=1 L R (y (i) , x (i) ) + 1 |D| KL(q φ (w) p(w))<label>(17)</label></formula><p>where |D| indicates the number of samples, and</p><formula xml:id="formula_16">L R (y, x) = − log(τ x ) T 1 + √ τ x (y − μx ) 2 (18) μx = μ(x, w µ ); w ∼ q φ (w) (19) τx = τ (x, w r ).<label>(20)</label></formula><p>where and 1 represent the element-wise product and vector filled with ones, respectively. Eq. ( <ref type="formula" target="#formula_15">17</ref>) can be used to compute <ref type="bibr" target="#b9">(10)</ref>.</p><p>Posch et al. <ref type="bibr" target="#b84">[85]</ref> defined the variational distribution using a product of Gaussian distributions along with diagonal covariance matrices. For each network layer, a posterior uncertainty of the network parameter was represented. Later,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>x y</head><p>Full-covariance 3+ 'mean-field' layers Similarly expressive Fig. <ref type="figure">8</ref>: A general architecture of the deeper linear mean-field network with three mean-field weight layers or more which is reproduced based on <ref type="bibr" target="#b97">[98]</ref>.</p><p>in <ref type="bibr" target="#b85">[86]</ref>, they replaced the diagonal covariance matrices with the traditional ones to allow the network parameters to correlate with each other. Inspired from transfer learning and empirical Bayes (EB) <ref type="bibr" target="#b86">[87]</ref>, MOPED <ref type="bibr" target="#b87">[88]</ref> used a deterministic weights, which was derived from a pretrained DNNs with same architecture, to select meaningful prior distributions over the weight space. Later, in <ref type="bibr" target="#b88">[89]</ref>, they integrated an approach based on parametric EB into MOPED for mean field VI in Bayesian DNNs, and used fully factorized Gaussian distribution to model the weights. In addition, they used a real-world case study, i.e., diabetic retinopathy diagnosis, to evaluate their method. Subedar et al. <ref type="bibr" target="#b89">[90]</ref> proposed an uncertainty aware framework based on multi-modal Bayesian fusion for activity recognition. They scaled BDNN into deeper structure by combining deterministic and variational layers. Marino et al. <ref type="bibr" target="#b90">[91]</ref> proposed a stochastic modeling based approach to model uncertainty. Specifically, the DBNN was used to learn the stochastic learning of the system. Variational BNN <ref type="bibr" target="#b91">[92]</ref>, which is a generative-based model, was proposed to predict the superconducting transition temperature. Specifically, the VI was adapted to compute the distribution in the latent space for the model.</p><p>Louizos and Welling <ref type="bibr" target="#b92">[93]</ref> adopted a stochastic gradient VI <ref type="bibr" target="#b93">[94]</ref> to compute the posterior distributions over the weights of NNs. Hubin and Storvik <ref type="bibr" target="#b94">[95]</ref> proposed a stochastic VI method that jointly considers both model and parameter uncertainties in BNNs, and introduced a latent binary variables to include/exclude certain weights of the model. Liu et al. <ref type="bibr" target="#b95">[96]</ref> integrated the VI into a spatial-temporal NN to approximate the posterior parameter distribution of the network and estimate the probability of the prediction. Ryu et al. <ref type="bibr" target="#b96">[97]</ref> integrated the graph convolutional network (GCN) into the Bayesian framework to learn representations and predict the molecular properties. Swiatkowski et al. <ref type="bibr" target="#b83">[84]</ref> empirically studied the Gaussian mean-field VI. They decomposed the variational parameters into a lowrank factorization to make a more compact approximation, and improve the SNR ratio of the SG in estimating the lower bound of the variational. Franquhar et al. <ref type="bibr" target="#b97">[98]</ref> used the mean-field VI to better train deep models. They argued that a deeper linear mean-field network can provide an analogous distribution of function space like shallowly fullco-variance networks. A schematic view of the proposed approach is demonstrated in Fig. <ref type="figure">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Bayesian Active Learning (BAL)</head><p>Active learning (AL) methods aim to learn from unlabeled samples by querying an oracle <ref type="bibr" target="#b98">[99]</ref>. Defining the right acquisition function, i.e., the condition on which sample is most informative for the model, is the main challenge of AL-based methods. Although existing AL frameworks have shown promising results in variety of tasks, they lack of scalability to high-dimensional data <ref type="bibr" target="#b99">[100]</ref>. In this regard, the Baysian approaches can be integrated into DL structure to represent the uncertainty, and then combine with deep AL acquisition function to probe for the uncertain samples in the oracle. DBAL <ref type="bibr" target="#b100">[101]</ref>, i.e., deep Bayesian AL, combine an AL framework with Bayesian DL to deal with high-dimensional data problems, i.e., image data. DBAL used batch acquisition to select the top n samples with the highest Bayesian AL by disagreement (BALD) <ref type="bibr" target="#b101">[102]</ref> score. Model priors from empirical bayes (MOPED) <ref type="bibr" target="#b102">[103]</ref> used BALD to evaluate the uncertainty. In addition, MC dropout was applied to estimate the model uncertainty. Later, Krisch et al. <ref type="bibr" target="#b103">[104]</ref> proposed BatchBALD, which uses greedy algorithm to select a batch in linear time and reduce the run time. They modeled the uncertainty by leveraging the Bayesian AL (BAL) using Dropout-sampling. In <ref type="bibr" target="#b104">[105]</ref>, two types of uncertainty measures namely entropy and BALD <ref type="bibr" target="#b101">[102]</ref>, were compared.</p><p>ActiveHARNet <ref type="bibr" target="#b105">[106]</ref>, which is an AL-based framework for human action recognition, modeled the uncertainty by linking BNNs with GP using dropout. To achieve this, dropout was applied before each fully connected layer to estimate the mean and variance of BNN. DeepBASS <ref type="bibr" target="#b106">[107]</ref>, i.e., a deep AL semi-supervised learning, is an expectationmaximization <ref type="bibr" target="#b107">[108]</ref> -based technique paired with an AL component. It applied MC dropout to estimate the uncertainty.</p><p>Scandalea et al. <ref type="bibr" target="#b108">[109]</ref> proposed a framework based on U-Net structure for deep AL to segment biomedical images, and used uncertainty measure obtained by MC dropout, to suggest the sample to be annotated. Specifically, the uncertainty was defined based on the posterior probabilities' SD of the MC-samples. Zheng et al. <ref type="bibr" target="#b109">[110]</ref> varied the number of Bayesian layers and their positions to estimate uncertainty through AL on MNIST dataset. The outcome indicated that few Bayesian layers near the output layer are enough to fully estimate the uncertainty of the model.</p><p>Inspired from <ref type="bibr" target="#b110">[111]</ref>, the Bayesian batch AL <ref type="bibr" target="#b111">[112]</ref>, which selects a batch of samples at each AL iteration to perform posterior inference over the model parameters, was proposed for large-scale problems. Active user training <ref type="bibr" target="#b112">[113]</ref>, which is a BAL-based crowdsourcing model, was proposed to tackle high-dimensional and complex classification problems. In addition, the Bayesian inference proposed in <ref type="bibr" target="#b113">[114]</ref> was used to consider the uncertainty of the confusion matrix of the annotators.</p><p>Several generative-based AL frameworks have been introduced. In <ref type="bibr" target="#b114">[115]</ref>, a semi-supervised Bayesian AL model, which is a deep generative-based model that uses BNNs to give discriminative component, was developed. Tran et al. <ref type="bibr" target="#b115">[116]</ref> proposed a Bayesian-based generative deep AL (BGADL) (Fig. <ref type="figure">9</ref>) for image classification problems. They, firstly used the concept of DBAL to select the must in- </p><formula xml:id="formula_17">(𝐗 * , 𝐘 * ) (𝐗 ′ , 𝐘 * ) {(X, Y)} ∪ (𝐗 * , 𝐘 * ) ∪ (𝐗 ′ , 𝐘 * )</formula><p>Fig. <ref type="figure">9</ref>: Bayesian generative active deep learning (Note, ACGAN stands for the Auxiliary-classifier GAN which is reproduced based on <ref type="bibr" target="#b115">[116]</ref>.</p><p>formative samples and then VAE-ACGAN was applied to generate new samples based on the selected ones. Akbari et al. <ref type="bibr" target="#b116">[117]</ref> proposed a unified BDL framework to quantify both aleatoric and epistemic uncertainties for activity recognition.</p><p>They used an unsupervised DL model to extract features from the time series, and then their posterior distributions was learned through a VAE model. Finally, the Dropout <ref type="bibr" target="#b29">[30]</ref> was applied after each dense layer and test phase for randomness of the model weights and sample from the approximate posterior, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Bayes by Backprop (BBB)</head><p>The learning process of a probability distribution using the weights of neural networks plays significant role for having better predictions results. Blundell et al. <ref type="bibr" target="#b117">[118]</ref> proposed a novel yet efficient algorithm named Bayes by Backprop (BBB) to quantify uncertainty of these weights. The proposed BBB minimizes the compression cost which is known as the variational free energy (VFE) or the lower bound (expected) of the marginal likelihood. To do so, they defined a cost function as follows:</p><formula xml:id="formula_18">F (D, θ) = KL[q(w|θ) P (w)]−E q(w,θ) [log P (D|w)].<label>(21)</label></formula><p>The BBB algorithm uses unbiased gradient estimates of the cost function in 21 for learning distribution over the weights of neural networks. In another research, Fortunato et al. <ref type="bibr" target="#b118">[119]</ref> proposed a new Bayesian recurrent neural network (BRNNs) using BBB algorithm. In order to improve the BBB algorithm, they used a simple adaptation of truncated back-propagation throughout the time. The proposed Bayesian RNN (BRNN) model is shown in Fig. <ref type="figure" target="#fig_16">10</ref>.</p><p>Ebrahimi et al. <ref type="bibr" target="#b119">[120]</ref> proposed an uncertainty-guided continual approach with BNNs (named UCB which stands for Uncertainty-guided continual learning technique with BNNs). The continual learning leads to learn a variety of new tasks while impound the aforetime knowledge obtained learned ones. The proposed UCB exploits the predicted uncertainty of the posterior distribution in order to formulate the modification in "important" parameters both in setting a hard-threshold as well as in a soft way. Recognition of different actions in videos needs not only big data but also is a time consuming process. To deal with Fig. <ref type="figure" target="#fig_16">10</ref>: Bayesian RNNs (BRNNs) which is reproduced based on the proposed model by Fortunato et al. <ref type="bibr" target="#b118">[119]</ref>.</p><p>this issue, de la Riva and Mettes <ref type="bibr" target="#b120">[121]</ref> proposed a Bayesianbased deep learning method (named Bayesian 3D ConvNet) to analyze a small number of videos. In this regard, BBB was extended to be used by 3D CNNs and then employed to deal with uncertainty over the convolution weights in the proposed model. To do so, Gaussian distribution was applied to approximate the correct posterior in the proposed 3D Convolution layers using mean and STD (standard deviation) as follows:</p><formula xml:id="formula_19">     θ = (µ, α), σ 2 = α.µ 2 , q θ (w ijhwt |D) = N (µ ijhwt , α ijhwt µ 2 ijhwt ),<label>(22)</label></formula><p>where i represents the input, j is the output, h is the filter height, w is the filter width and t is the time dimension. In another research, Ng et al. <ref type="bibr" target="#b60">[61]</ref>  Fig. <ref type="figure" target="#fig_16">11</ref>: Pairwise Supervised Hashing-Bernoulli VAE (PSH-BVAE) which is reproduced based on <ref type="bibr" target="#b121">[122]</ref>.</p><p>An autoencoder is a variant of DL that consists of two components: (i) encoder, and (ii) decoder. Encoder aims to map high-dimensional input sample x to a low-dimensional latent variable z. While decoder reproduces the original sample x using latent variable z. The latent variables are compelled to conform a given prior distribution P (z). Variational Autoencoders (VAEs) <ref type="bibr" target="#b93">[94]</ref> are effective methods to model the posterior. They cast learning representations for high-dimensional distributions as a VI problem <ref type="bibr" target="#b122">[123]</ref>. A probabilistic model P θ (x) of sample x in data space with a latent variable z in latent space can be written as follows:</p><formula xml:id="formula_20">p θ (x) = z p θ (x|z)p(z),<label>(23)</label></formula><p>The VI can be used to model the evidence lower bound log p θ (x) as follows:</p><formula xml:id="formula_21">log p θ (x) = E q φ (z|x) [log p θ (x|z)] − D KL (q φ (z|x) p(x)),<label>(24)</label></formula><p>where q φ (z|x) and p θ (x|z) are the encoder and decoder models, respectively, and φ and θ indicate their parameters. Zamani et al. <ref type="bibr" target="#b121">[122]</ref> developed a discrete VAE framework with Bernoulli latent variables as binary hashing code (Fig. <ref type="figure" target="#fig_16">11</ref>). The stochastic gradient was exploited to learn the model. They proposed a pairwise supervised hashing (PSH) framework to derive better hashing codes. PSH maximizes the ELBO with weighted KL regularization to learn more informative binary codes, and adapts a pairwise loss function to reward within-class similarity and between-class dissimilarity to minimize the distance among the hashing codes of samples from same class and vice versa. Bohm et al. <ref type="bibr" target="#b123">[124]</ref> studied UQ for linear inverse problems using VAEs. Specifically, the vanilla VAE with mean-field Gaussian posterior was trained on uncorrupted samples under the ELBO. In addition, the EL 2 O method <ref type="bibr" target="#b124">[125]</ref> was adopted to approximate the posterior. Edupuganti et al. <ref type="bibr" target="#b125">[126]</ref> studied the UQ tasks in magnetic resonance image recovery (see Fig. <ref type="figure" target="#fig_16">12</ref>). As such, a VAE-GAN, which is a probabilistic recovery scheme, was developed to map the low quality images to high-quality ones. The VAE-GAN consists of VAE and multi-layer CNN as generator and discriminator, respectively. In addition, the Stein's unbiased risk estimator (SURE) was leveraged as a proxy to predict error and estimate the uncertainty of the model.</p><p>In <ref type="bibr" target="#b126">[127]</ref>, a framework based on variational U-Net <ref type="bibr" target="#b127">[128]</ref> architecture was proposed for UQ tasks in reservoir simulations. Both simple U-Net and variational U-Net (VUNet) are illustrated in Fig. <ref type="figure" target="#fig_16">13</ref>. Cosmo VAE <ref type="bibr" target="#b128">[129]</ref>, which is a DL, i.e., U-Net, based VAE, was proposed to restore the missing observations of the cosmic microwave background (CMB) map. As such, the variational Bayes approximation was used to determine the ELBO of likelihood of the reconstructed image. Mehrasa et al. <ref type="bibr" target="#b129">[130]</ref> proposed action point process VAE (APP VAE) for action sequences. APP VAE consists of two LSTM to estimate the prior and posterior distributions. Sato et al. <ref type="bibr" target="#b130">[131]</ref> proposed a VAE-based UA for anomaly detection. They used MC sampling to estimate posterior.</p><p>Since VAEs are not stochastic processes, they are limited to encode finite-dimensional priors. To alleviate this limitation, Mishra et al. <ref type="bibr" target="#b131">[132]</ref> developed the prior encoding VAE, i.e., πVAE. Inspired by the Gaussian process <ref type="bibr" target="#b132">[133]</ref>, πVAE is a stochastic process that learns the distribution over functions. To achieve this, πVAE encoder, firstly, transforms the locations to a high-dimensional space, and then, uses a linear mapping to link the feature space to outputs. While πVAE encoder aims to recreate linear mapping from the lower dimensional probabilistic embedding. Finally, the recreated mapping is used to get the reconstruction of the outputs. Guo et al. <ref type="bibr" target="#b133">[134]</ref> used VAE to deal with data uncertainty under a just-in-time learning framework. The Gaussian distribution was employed to describe latent space features as variable-wise, and then the KL-divergence was used to ensure that the selected samples are the most relevant to a new sample. Daxberger et al. <ref type="bibr" target="#b134">[135]</ref> tried to detect OoD samples during test phase. As such, the developed an unsupervised, probabilistic framework based on a Bayesian VAE. Besides, they estimated the posterior over the decoder parameters by applying SG-MCMC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OTHER METHODS</head><p>In this section, we discuss few other proposed UQ methods used in machine and deep learning algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Deep Gaussian processes</head><p>Deep Gaussian processes (DGPs) <ref type="bibr" target="#b135">[136]</ref>, <ref type="bibr" target="#b136">[137]</ref>, <ref type="bibr" target="#b137">[138]</ref>, <ref type="bibr" target="#b138">[139]</ref>, <ref type="bibr" target="#b139">[140]</ref>, <ref type="bibr" target="#b140">[141]</ref>, <ref type="bibr" target="#b141">[142]</ref> are effective multi-layer decision making models that can accurately model the uncertainty. They represent a multi-layer hierarchy to Gaussian processes (GPs) <ref type="bibr" target="#b142">[143]</ref>, <ref type="bibr" target="#b143">[144]</ref>. GPs is a non-parametric type of Bayesian model that encodes the similarity between samples using kernel function. It represents distributions over the latent variables with respect to the input samples as a Gaussian distribution f x ∼ GP(m(x), k(x, x )). Then, the output y is distributed based on a likelihood function y|f x ∼ h(f x ). However, the conventional GPs can not effectively scale the large datasets. To alleviate this issue, inducing samples can be used. As such, the following variational lower bound can be optimized. <ref type="bibr" target="#b24">(25)</ref> where Z and q(f x ) are the location of the inducing samples and the approximated variational to the distribution of f x , respectively.</p><formula xml:id="formula_22">log p(Y ) ≥ y,x∈Y,X E q(fx) [log p(y|f x )] − KL(q(f Z ) p(f Z )),</formula><p>Oh et al. <ref type="bibr" target="#b144">[145]</ref> proposed the hedged instance embedding (HIB), which hedges the position of each sample in the embedding space, to model the uncertainty when the input sample is ambiguous. As such, the probability of two samples matching was extended to stochastic embedding, and the MC sampling was used to approximate it. Specifically, the mixture of C Gaussians was used to represent the uncertainty. Havasi et al. <ref type="bibr" target="#b145">[146]</ref> applied SGHMC into DGPs to approximate the posterior distribution. They introduced a moving window MC expectation maximization to obtain the maximum likelihood to deal with the problem of optimizing large number of parameters in DGPs. Maddox et al. <ref type="bibr" target="#b146">[147]</ref> used stochastic weight averaging (SWA) <ref type="bibr" target="#b147">[148]</ref> to build a Gaussian-baed model to approximate the true posterior. Later, they proposed SWA-G <ref type="bibr" target="#b148">[149]</ref>, which is SWA-Gaussian, to model Bayesian averaging and estimate uncertainty. Most of the weight perturbation-based algorithms suffer from high variance of gradient estimation due to sharing same perturbation by all samples in a mini-batch. To alleviate this problem, flipout <ref type="bibr" target="#b149">[150]</ref> was proposed. Filipout samples the pseudo-independent weight perturbations for each input to decorrelate the gradient within the mini-batch. It is able to reduce variance and computational time in training NNs with multiplicative Gaussian perturbations.</p><p>Despite the success of DNNs in dealing with complex and high-dimensional image data, they are not robust to adversarial examples <ref type="bibr" target="#b150">[151]</ref>. Bradshaw et al. <ref type="bibr" target="#b151">[152]</ref> proposed a hybrid model of GP and DNNs (GPDNNs) to deal with uncertainty caused by adversarial examples (see Fig. <ref type="figure" target="#fig_5">14</ref>).</p><p>Choi et al. <ref type="bibr" target="#b152">[153]</ref> proposed a Gaussian-based model to predict the localization uncertainty in YOLOv3 <ref type="bibr" target="#b153">[154]</ref>. As such, they applied a single Gaussian model to the bbox co- ordinates of the detection layer. Specifically, the coordinates of each bbox is modeled as the mean (µ) and variance ( ) to predict the uncertainty of bbox.</p><p>Khan et al. <ref type="bibr" target="#b154">[155]</ref> proposed a natural gradient-based algorithm for Gaussian mean-field VI. The Gaussian distribution with diagonal covariances was used to estimate the probability. The proposed algorithm was implemented within the Adam optimizer. To achieve this, the network weights were perturbed during the gradient evaluation. In addition, they used a vector to adapt the learning rate to estimate uncertainty.</p><p>Sun et al. <ref type="bibr" target="#b155">[156]</ref> considered structural information of the model weights. They used the matrix variate Gaussian (MVG) <ref type="bibr" target="#b156">[157]</ref> distribution to model structured correlations within the weights of DNNs, and introduced a reparametrization for the MVG posterior to make the posterior inference feasible. The resulting MVG model was applied to a probabilistic BP framework to estimate posterior inference. Louizos and Welling <ref type="bibr" target="#b157">[158]</ref> used MVG distribution to estimate the weight posterior uncertainty. They treated the weight matrix as a whole rather than treating each component of weight matrix independently. As mentioned earlier, GPs were widely used for UQ in deep learning methods. Van der Wilk et al. <ref type="bibr" target="#b158">[159]</ref>, Blomqvist et al. <ref type="bibr" target="#b159">[160]</ref>, Tran et al. <ref type="bibr" target="#b160">[161]</ref>, Dutordoir et al. <ref type="bibr" target="#b161">[162]</ref> and Shi et al. <ref type="bibr" target="#b162">[163]</ref> introduced convolutional structure into GP. In another study, Corbière et al. <ref type="bibr" target="#b163">[164]</ref> expressed that the confidence of DDNs and predicting their failures is of key importance for the practical application of these methods. In this regard, they showed that the TCP (T rue Class P robability) is more suitable than the MCP (M aximum Class P robability) for failure prediction of such deep learning methods as follows:</p><formula xml:id="formula_23">T CP : R d × Y → R (x, y * ) → P (Y = y * |w, x),<label>(26)</label></formula><p>where x i ∈ R d represents a d-dimensional feature and y * i ∈ Y = {1, ..., K} is its correct class. Then, they introduced a new normalized type of the TCP confidence criterion:</p><formula xml:id="formula_24">T CP r (x, y * ) = P (Y = y * |w, x) P (Y = ŷ|w, x) .<label>(27)</label></formula><p>A general view of the proposed model in <ref type="bibr" target="#b163">[164]</ref> is illustrated by Fig. <ref type="figure" target="#fig_6">15</ref>.</p><p>In another research, Atanov et al. <ref type="bibr" target="#b164">[165]</ref> introduced a probabilistic model and showed that Batch Normalization (BN) approach can maximize the lower bound of its related marginalized log-likelihood. Since inference computationally was not efficient, they proposed the Stochastic BN (SBN) approach for approximation of appropriate inference procedure, as an uncertainty estimation method. Moreover, the induced noise is generally employed to capture the uncertainty, check overfitting and slightly improve the performance via test-time averaging whereas ordinary stochastic neural networks typically depend on the expected values of their weights to formulate predictions. Neklyudov et al. <ref type="bibr" target="#b165">[166]</ref> proposed a different kind of stochastic layer called variance layers. It is parameterized by its variance and each weight of a variance layer obeyed a zero-mean distribution. It implies that each object was denoted by a zero-mean distribution in the space of the activations. They demonstrated that these layers presented an upright defense against adversarial attacks and could serve as a crucial exploration tool in reinforcement learning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Laplace approximations</head><p>Laplace approximations (LAs) are other popular UQ methods which are used to estimate the Bayesian inference <ref type="bibr" target="#b166">[167]</ref>. They build a Gaussian distribution around true posterior using a Taylor expansion around the MAP, θ * , as follows:</p><formula xml:id="formula_25">p(θ|D) ≈ p(θ * ) exp{− 1 2 (θ − θ * ) H |θ * (θ − θ * )}<label>(28)</label></formula><p>where  introduced a scalable LA (SLA) approach for different NNs.</p><formula xml:id="formula_26">H</formula><p>The proposed the model, then compared with the other well-known methods such as Dropout and a diagonal LA for the uncertainty estimation of networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">UNCERTAINTY QUANTIFICATION IN REINFORCE-MENT LEARNING</head><p>In decision making process, uncertainty plays a key role in decision performance in various fields such as Reinforcement Learning (RL) <ref type="bibr" target="#b168">[169]</ref>. Different UQ methods in RL have been widely investigated in the literature <ref type="bibr" target="#b169">[170]</ref>. Lee et al. <ref type="bibr" target="#b170">[171]</ref> formulated the model uncertainty problem as Bayes-Adaptive Markov Decision Process (BAMDP). The general BAMDP defined by a tuple S, Φ, A, T, R, P 0 , γ , where where S shows the underlying MDP's observable state space, Φ indicates the latent space, A represents the action space, T is the parameterized transition and finally R is the reward functions, respectively. Lets b 0 be an initial belief, a Bayes filter updates the posterior as follows:</p><formula xml:id="formula_27">b (φ |s, b, a , s ) = η φ∈Φ b(φ)T (s, φ, a , s , φ )<label>(29)</label></formula><p>Then, Bayesian Policy Optimization (BPO) method (see Fig. <ref type="figure" target="#fig_7">16</ref>) is applied to POMDPs as a Bayes filter to compute the belief b of the hidden state as follows:</p><formula xml:id="formula_28">b (s ) = ψ(b, a , o ) = η s∈S b(s)T (s, a , s )Z(s, a , o ) (30)</formula><p>In another research, O'Donoghue et al. <ref type="bibr" target="#b171">[172]</ref> proposed the uncertainty Bellman equation (UBE) to quantify uncertainty. The authors used a Bellman-based which propagated the uncertainty (here variance) relationship of the posterior distribution of Bayesian. Kahn et al. a <ref type="bibr" target="#b172">[173]</ref> presented a new UA model for learning algorithm to control a mobile robot. A review of past studies in RL shows that different Bayesian approaches have been used for handling parameters uncertainty <ref type="bibr" target="#b173">[174]</ref>. Bayesian RL was significantly reviewed by Ghavamzadeh et al. <ref type="bibr" target="#b173">[174]</ref> in 2015. Due to page limitation, we do not discuss the application of UQ in RL; but we summarise some of the recent studies here. Kahn et al. a <ref type="bibr" target="#b172">[173]</ref> used both Bootstrapping and Dropout methods to estimate uncertainty in NNs and then used in UA collision prediction model. Besides Bayesian statistical methods, ensemble methods have been used to quantify uncertainty in RL <ref type="bibr" target="#b174">[175]</ref>. In this regard, Tschantz et al. <ref type="bibr" target="#b174">[175]</ref> applied an ensemble of different point-estimate parameters θ = {θ 0 , ..., θ B } when trained on various batches of a dataset D and then maintained and treated by the posterior distribution p(θ|D). The ensemble method helped to capture both aleatoric and epistemic uncertainty. There are more UQ techniques used in RL, however, we are not able to discuss all of them in details in this work due to various reasons, such as page restrictions and the breadth of articles. Table <ref type="table" target="#tab_8">2</ref> summarizes different UQ methods used in a variety of RL subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ENSEMBLE TECHNIQUES</head><p>Deep neural networks (DNNs) have been effectively employed in a wide variety of machine learning tasks and have achieved state-of-the-art performance in different domains such as bioinformatics, natural language processing (NLP), speech recognition and computer vision <ref type="bibr" target="#b186">[187]</ref>, <ref type="bibr" target="#b187">[188]</ref>. In supervised learning benchmarks, NNs yielded competitive ac curacies, yet poor predictive uncertainty quantification. Hence, it is inclined to generate overconfident predictions. Incorrect overconfident predictions can be harmful; hence it is important to handle UQ in a proper manner in real-world applications <ref type="bibr" target="#b188">[189]</ref>. As empirical evidence of uncertainty estimates are not available in general, quality of predictive uncertainty evaluation is a challenging task. Two evaluation measures called calibration and domain shift are applied which usually are inspired by the practical applications of NNs. Calibration measures the discrepancy between longrun frequencies and subjective forecasts. The second notion concerns generalization of the predictive uncertainty to domain shift that is estimating if the network knows what it knows. An ensemble of models enhances predictive performance. However, it is not evident why and when an ensemble of NNs can generate good uncertainty estimates. Bayesian model averaging (BMA) believes that the true model reclines within the hypothesis class of the prior and executes soft model selection to locate the single best model within the hypothesis class. On the contrary, ensembles combine models to discover more powerful model; ensembles can be anticipated to be better when the true model does not lie down within the hypothesis class. The authors in <ref type="bibr" target="#b189">[190]</ref> devised Maximize Overall Diversity (MOD) model to estimate ensemble-based uncertainty by taking into account diversity in ensemble predictions across future possible inputs. Gustafsson et al. <ref type="bibr" target="#b190">[191]</ref> presented an evaluation approach for measuring uncertainty estimation to investigate the robustness in computer vision domain. Researchers in <ref type="bibr" target="#b191">[192]</ref> proposed a deep ensemble echo state net-work model for spatio-temporal forecasting in uncertainty quantification. Chua et al. <ref type="bibr" target="#b192">[193]</ref> devised a novel method called probabilistic ensembles with trajectory sampling that integrated sampling-based uncertainty propagation with UA deep network dynamics approach. The authors in <ref type="bibr" target="#b186">[187]</ref> demonstrated that prevailing calibration error estimators were unreliable in small data regime and hence proposed kernel density-based estimator for calibration performance evaluation and proved its consistency and unbiasedness. Liu et al. <ref type="bibr" target="#b193">[194]</ref> presented a Bayesian nonparametric ensemble method which enhanced an ensemble model that augmented model's distribution functions using Bayesian nonparametric machinery and prediction mechanism. Hu et al. <ref type="bibr" target="#b194">[195]</ref> proposed a model called margin-based Pareto deep ensemble pruning utilizing deep ensemble network that yielded competitive uncertainty estimation with elevated confidence of prediction interval coverage probability and a small value of the prediction interval width. In another study, the researchers <ref type="bibr" target="#b195">[196]</ref> exploited the challenges associated with attaining uncertainty estimations for structured predictions job and presented baselines for sequence-level out-of-domain input detection, sequence-level prediction rejection and token-level error detection utilizing ensembles. Ensembles involve memory and computational cost which is not acceptable in many application <ref type="bibr" target="#b196">[197]</ref>. There has been noteworthy work done on the distillation of an ensemble into a single model. Such approaches achieved comparable accuracy using ensembles and mitigated the computational costs. In posterior distribution p(θ|D), the uncertainty of model is captured. Let us consider from the posterior sampled ensemble of models {P (y|x , θ (m) )} M m=1 as follows <ref type="bibr" target="#b196">[197]</ref>:</p><formula xml:id="formula_29">{P (y|x , θ (m) )} M m=1 → {P (y|π (m) )} M m=1 , π m = f (x ; θ (m) ), θ (m) ∼ p(θ|D) (31)</formula><p>where x * a test is input and π represents the parameters of a categorical distribution [P (y = w 1 ), ..., P (y = w k )] T . By taken into account the expectation with respect to the model posterior, predictive posterior or the expected predictive distribution, for a test input x * is acquired. And then we have:</p><formula xml:id="formula_30">P (y|x , D) = E p(θ|D) [P (y|x , θ)]<label>(32)</label></formula><p>Different estimate of data uncertainty are demonstrated by each of the models P (y|x , θ (m) ). The 'disagreement' or the level of spread of an ensemble sampled from the posterior is occurred due to the uncertainty in predictions as a result of model uncertainty. Let us consider an ensemble {P (y|x , θ (m) )} M m=1 that yields the expected set of behaviors, the entropy of expected distribution P (y|x , D) can be utilized as an estimate of total uncertainty in the prediction. Measures of spread or 'disagreement' of the ensemble such as MI can be used to assess uncertainty in predictions due to knowledge uncertainty as follows:</p><formula xml:id="formula_31">MI[y, θ|x , D] Knowledge U ncertainty = H[E p(θ|D) [P (y|x , θ)]] T otal U ncertainty − E p(θ|D) [H[P (y|x , θ)]]</formula><p>Expected Data U ncertainty <ref type="bibr" target="#b32">(33)</ref> 13 The total uncertainty can be decomposed into expected data uncertainty and knowledge uncertainty via MI formulation.</p><p>If the model is uncertain -both in out-of-domain and regions of severe class overlap, entropy of the total uncertainty or predictive posterior is high. If the models disagree, the difference of the expected entropy and entropy of predictive posterior of the individual models will be non-zero. For example, MI will be low and expected and predictive posterior entropy will be similar, and each member of the ensemble will demonstrate high entropy distribution in case of in regions of class overlap. In such scenario, data uncertainty dominates total uncertainty. The predictive posterior is near uniform while the expected entropy of each model may be low that yielded from diverse distributions over classes as a result of out-of-domain inputs on the other hand.In this region of input space, knowledge uncertainty is high because of the model's understanding of data is low. In ensemble distribution distillation, the aim is not only to capture its diversity but also the mean of the ensemble. An ensemble can be observed as a set of samples from an implicit distribution of output distributions: The authors in <ref type="bibr" target="#b197">[198]</ref> exploited in-domain uncertainty and examined its standards for its quantification and revealed pitfalls of prevailing matrices. They presented the deep ensemble equivalent score (DEE) and demonstrated how an ensemble of trained networks which is only few in number can be equivalent to many urbane ensembling methods with respect to test performance. For one ensemble, they proposed the test-time augmentation (TTA) in order to improve the performance of different ensemble learning techniques (see Fig. <ref type="figure" target="#fig_8">17</ref>).</p><formula xml:id="formula_32">{P (y|x , θ (m) )} M m=1 → {P (y|π (m) )} M m=1 , π (m) ∼ p(π|x , D).<label>(34)</label></formula><p>However, deep ensembles <ref type="bibr" target="#b198">[199]</ref> are a simple approach that presents independent samples from various modes of the loss setting. Under a fixed test-time computed budget, deep ensembles can be regarded as powerful baseline for the performance of other ensembling methods. It is a challenging task to compare the performance of ensembling methods. Different values of matrices are achieved by different models on different datasets. Interpretability is lacking in values of matrices as performance gain is compared with dataset and model specific baseline. Hence, Ashukha et al. <ref type="bibr" target="#b197">[198]</ref> proposed DDE with an aim to introduce interpretability and perspective that applies deep ensembles to compute the performance of other ensembling methods. DDE score tries to answer the question: what size of deep ensemble demonstrates the same performance as a specific ensembling technique? The DDE score is based on calibrated log-likelihood (CLL). DDE is defined for an ensembling technique (m) and lower and upper bounds are depicted as below <ref type="bibr" target="#b197">[198]</ref>:</p><formula xml:id="formula_33">DEE m (k) = min{l ∈ R, l ≥ 1|CLL mean DE (l) ≥ CLL mean m (k)}, (<label>35</label></formula><formula xml:id="formula_34">)</formula><formula xml:id="formula_35">DEE upper/lower m (k) = min{l ∈ R, l ≥ 1|CLL mean DE (l) ∓ CLL std DE (l) ≥ CLL mean m (k)},<label>(36)</label></formula><p>where the mean and standard deviation of the calibrated log-likelihood yielded by an ensembling technique m with l samples is dubbed as CLL  <ref type="bibr" target="#b193">[194]</ref>. Bayesian nonparametric machinery was utilized to augment distribution functions and prediction of a model by BNE. The BNE measure the uncertainty patterns in data distribution and decompose uncertainty into discrete components that are due to error and noise. The model yielded precise uncertainty estimates from observational noise and demonstrated its utility with respect to model's bias detection and uncertainty decomposition for an ensemble method used in prediction. The predictive mean of BNE can be expressed as below <ref type="bibr" target="#b193">[194]</ref>:</p><formula xml:id="formula_36">E(y|X, ω, δ, G) = K k=1 f k (X)ω k + δ(X) Due to δ + y∈ † Φ((y|X, µ) − G[Φ((y|X, µ] dy Due to G . (<label>37</label></formula><formula xml:id="formula_37">)</formula><p>The predictive mean for the full BNE is comprised of three sections:</p><p>1) The predictive mean of original ensemble To denote BNE's predictive uncertainty estimation, the term Φ ε,ω is used which is the predictive cumulative distribution function of the original ensemble (i.e. with variance σ 2 ε and mean k f k ω k ). The BNE's predictive interval is presented as <ref type="bibr" target="#b193">[194]</ref>:</p><formula xml:id="formula_38">U q (y|X, ω, δ, G) = Φ −1 ε,ω G −1 (1 − q 2 |X) + δ(x), Φ −1 ε,ω G −1 (1 + q 2 |X) + δ(x) . (<label>38</label></formula><formula xml:id="formula_39">)</formula><p>Comparing the above equation to the predictive interval</p><formula xml:id="formula_40">of original ensemble Φ −1 ε,ω G −1 (1 − q 2 |X) , Φ −1 ε,ω G −1 (1 + q 2 |X</formula><p>) , it can be observed that the residual process δ adjusts the locations of the BNE predictive interval endpoints while G calibrates the spread of the predictive interval.</p><p>As an important part of ensemble techniques, loss functions play a significant role of having a good performance by different ensemble techniques. In other words, choosing the appropriate loss function can dramatically improve results. Due to page limitation, we summarise the most important loss functions applied for UQ in Table <ref type="table" target="#tab_9">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Deep Ensemble</head><p>Deep ensemble, is another powerful method used to measure uncertainty and has been extensively applied in many real-world applications <ref type="bibr" target="#b194">[195]</ref>. To achieve good learning results, the data distributions in testing datasets should be as close as the training datasets. In many situations, the distributions of test datasets are unknown especially in case of uncertainty prediction problem. Hence, it is tricky for the traditional learning models to yield competitive performance. Some researchers applied MCMC and BNNs that relied on the prior distribution of datasets to work out the uncertainty prediction problems <ref type="bibr" target="#b189">[190]</ref>. When these approaches are employed into large size networks, it becomes computationally expensive. Model ensembling is an effective technique which can be used to enhance the predictive performance of supervised learners. Deep ensembles are applied to get better predictions on test data and also produce model uncertainty estimates when learns are provided with OoD data. The success of ensembles depends on the variance-reduction generated by combining predictions that are prone to several types of errors individually. Hence, the improvement in predictions is comprehended by utilizing a large ensemble with numerous base models and such ensembles also generate distributional estimates of model uncertainty. A deep ensemble echo state network (D-EESN) model with two versions of the model for spatio-temporal forecasting and associated uncertainty measurement presented in <ref type="bibr" target="#b191">[192]</ref>. The first framework applies a bootstrap ensemble approach and second one devised within a hierarchical Bayesian framework. Multiple levels of uncertainties and non-Gaussian data types were accommodated by general hierarchical Bayesian approach. The authors in <ref type="bibr" target="#b191">[192]</ref>   <ref type="bibr" target="#b200">[201]</ref> Image Neural Networks Diverse Information Bottleneck in Ensembles (DIBS)  </p><formula xml:id="formula_41">L OR (y, ŷ) = − 1 N K j=1 y j . log(ŷ j ) + (1 − y j ). log(1 − ŷj ) × Sinha et al.</formula><formula xml:id="formula_42">L G = E ẑ1 ∼q( z i |x),ẑ 2 ∼q( z j |x) [log D(ẑ 1 , ẑ2 )] + E ẑ1 ∼r( z),ẑ 2 ∼q( z i |x) [log(1 − D(ẑ 1 , ẑ2 ))] + E ẑ1 ∼q( z i |x),ẑ 2 ∼q( z i |x) [log 1 − D(ẑ 1 , ẑ2 ))] √ Zhang et al. [187] Image Neural Networks Mix-n-Match Cali- bration E z − y 2 2 (</formula><formula xml:id="formula_43">L(θ) = 1 N N i=1 (y i − μ(x i )) 2 σ 2 (x i ) + log σ 2 (x i ) + 1 N θ θ, Classification: L(θ) = − 1 N N i=1 C k=1 y i,k log ŝ(x i ) k + 1 2N θ θ √ Chua et</formula><formula xml:id="formula_44">Loss multi = W CV AE * Loss CV AE + W CRN N * Loss CRN N × Malinin et al. [197] Image Neural Networks Ensemble Distribution Distillation (EnD 2 ) L(φ, Dens) = − 1 N N i=1 ln Γ( α(i) 0 − K c=1 ln Γ( α(i) c + 1 M M m=1 K c=1 ( α(i) 0 − 1) ln π ( c im) √ Ashukha et al. [198] Image Neural Networks Deep ensemble equivalent score (DEE) L(w) = − 1 N N i=1 log p(y i |x i , w)+ λ 2 w 2 → min w √</formula><p>Pearce et al. <ref type="bibr" target="#b201">[202]</ref> Tabular data Neural Networks Quality-Driven Ensembles (QD-Ens)</p><formula xml:id="formula_45">Loss QD = M P IWcapt. + λ n α(1−α) max(0, (1 − α) − P ICP ) 2 √</formula><p>Ambrogioni et al. <ref type="bibr" target="#b202">[203]</ref> Tabular data Bayesian logistic regression</p><p>Wasserstein variational gradient descent (WVG)</p><formula xml:id="formula_46">L(z 1 ) = −E z∼p(z|x) [c(z j , z)] ×</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hu et al. [204] Image Neural Networks</head><p>Bias-variance decomposition</p><formula xml:id="formula_47">L = 1 2 exp(−s(x)) r yr (x)−ŷ(x) 2 R + 1 2 s(x) ×</formula><p>broadened some of the deep ESN technique constituents presented by Antonelo et al. <ref type="bibr" target="#b204">[205]</ref> and Ma et al. <ref type="bibr" target="#b205">[206]</ref> to fit in a spatio-temporal ensemble approach in the D-EESN model to contain such structure. As shown in previous section, in the following , we summarise few loss functions of deep ensembles in Table <ref type="table" target="#tab_12">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Deep Ensemble Bayesian</head><p>The expressive power of various ensemble techniques extensively shown in the literature. However, traditional learning techniques suffered from several drawbacks and limitations as listed in <ref type="bibr" target="#b209">[210]</ref>. To overcome these limitations, Fersini et al. <ref type="bibr" target="#b209">[210]</ref> utilized the ensemble learning approach to mitigate the noise sensitivity related to language ambiguity and more accurate prediction of polarity can be estimated. The proposed ensemble method employed Bayesian model averaging, where both reliability and uncertainty of each single model were considered.</p><p>Study <ref type="bibr" target="#b210">[211]</ref> presented one alteration to the prevailing approximate Bayesian inference by regularizing parameters about values derived from a distribution that could be set equal to the prior. The analysis of the process suggested that the recovered posterior was centered correctly but leaned to have an overestimated correlation and underestimated marginal variance. To obtain uncertainty estimates, one of the most promising frameworks is Deep BAL (DBAL) with MC dropout. Pop et al. <ref type="bibr" target="#b198">[199]</ref>  </p><formula xml:id="formula_48">L(y m i , ŷm,q i ) = 1 Q q∈Q max (q − 1)Hε y m i , ŷm,q i , qHε y m i , ŷm,q i × van Amersfoort et al. [209] Image Neural Networks Deterministic UQ (DUQ) L(x, y) = − c yc log(Kc)+(1−yc) log(1−Kc) ×</formula><p>In another research, Pearce et al. <ref type="bibr" target="#b212">[213]</ref> a new ensemble of NNs, approximately Bayesian ensembling approach, called "anchoredensembling". The proposed approach regularises the parameters regarding values attracted from a distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Uncertainty Quantification in Traditional Machine Learning domain using Ensemble Techniques</head><p>It is worthwhile noting that UQ in traditional machine learning algorithms have extensively been studied using different ensemble techniques and few more UQ methods (e.g. please see <ref type="bibr" target="#b213">[214]</ref>) in the literature. However, due to page limitation, we just summarized some of the ensemble techniques (as UQ methods) used in traditional machine learning domain.</p><p>For example, Tzelepis et al. <ref type="bibr" target="#b213">[214]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">FURTHER STUDIES OF UQ METHODS</head><p>In this section, we cover other methods used to estimate the uncertainty. In this regard, presented a summary of the proposed methods, but not the theoretical parts. Due to the page limitation and large number of references, we are not able to review all the details of the methods. For this reason, we recommend that readers check more details of each method in the reference if needed. The OoD is a common error appears in machine and deep learning systems when training data have different distribution. To address this issue, Ardywibowo et al.  Fig. <ref type="figure" target="#fig_0">20:</ref> A general view of the DUQ architecture which is reproduced based on <ref type="bibr" target="#b208">[209]</ref>, <ref type="bibr" target="#b221">[222]</ref>.</p><p>decision may lead to severe penalty in many domains such as autonomous driving, security and medical diagnosis. Traditional approaches are incapable of scaling complex large neural networks. Mobiny et al. <ref type="bibr" target="#b223">[224]</ref> proposed an approach by imposing a Bernoulli distribution on the model weights to approximate Bayesian inference for DNNs. Their framework dubbed as MC-DropConnect demonstrated model uncertainty by small alternation in the model structure or computed cost. They validated their technique on various datasets and architectures for semantic segmentation and classification tasks. They also introduced a novel uncertainty quantification metrics. Their experimental results showed considerable enhancements in uncertainty estimation and prediction accuracy compared to the prior approaches. Uncertainty measures are crucial estimating tools in machine learning domain, that can lead to evaluate the similarity and dependence between two feature subsets and can be utilized to verify the importance of features in clustering and classification algorithms. There are few uncertainty measure tools to estimate a feature subset including rough entropy, information entropy, roughness, and accuracy etc. in the classical rough sets. These measures are not proper for real-valued datasets and relevant to discretevalued information systems. Chen et al. <ref type="bibr" target="#b224">[225]</ref> proposed the neighborhood rough set model. In their approach, each object is related to a neighborhood subset, dubbed as a neighborhood granule. Different uncertainty measures of neighborhood granules were introduced, that were information granularity, neighborhood entropy, information quantity, and neighborhood accuracy. Further, they confirmed that these measures of uncertainty assured monotonicity, invariance and non-negativity. In the neighborhood systems, their experimental results and theoretical analysis demonstrated that information granularity, neighborhood entropy and information quantity performed superior to the neighborhood accuracy measure. On the other hand, reliable and accurate machine learning systems depends on techniques for reasoning under uncertainty. The UQ is provided by a framework using Bayesian methods. But Bayesian uncertainty estimations are often imprecise because of the use of approximate inference and model misspecification. Kuleshov et al. <ref type="bibr" target="#b225">[226]</ref> devised a simple method for calibrating any regression algorithm; it was guaranteed to provide calibrated uncertainty estimates having enough data when used to probabilistic and Bayesian models. They assessed their technique on recurrent, feedforward neural networks, and Bayesian linear regression and located outputs wellcalibrated credible intervals while enhancing performance on model-based RL and time series forecasting tasks. Gradient-based optimization techniques have showed its efficacy in learning overparameterized and complex neural networks from non-convex objectives. Nevertheless, generalization in DNNs, the induced training dynamics, and specific theoretical relationship between gradient-based optimization methods are still unclear. Rudner et al. <ref type="bibr" target="#b226">[227]</ref> examined training dynamics of overparameterized neural networks under natural gradient descent. They demonstrated that the discrepancy between the functions obtained from non-linearized and linearized natural gradient descent is smaller in comparison to standard gradient descent. They showed empirically that there was no need to formulate a limit argument about the width of the neural network layers as the discrepancy is small for overparameterized neural networks. Finally, they demonstrated that the discrepancy was small on a set of regression benchmark problems and their theoretical results were steady with empirical discrepancy between the functions obtained from non-linearized and linearized natural gradient descent.  an efficient technique, was proportional to their posterior and different types of uncertainties could be estimated. It is a challenging job to learn unbiased models on imbalanced datasets. The generalization of learned boundaries to novel test examples are hindered by concentrated representation in the classification space in rare classes. Khan et al. <ref type="bibr" target="#b230">[231]</ref> yielded that the difficulty level of individual samples and rarity of classes had direct correlation with Bayesian uncertainty estimates. They presented a new approach for uncertainty based class imbalance learning that exploited two-folded insights: 1. In rare (uncertain) classes, the classification boundaries should be broadened to evade overfitting and improved its generalization; 2. sample's uncertainty was defined by multivariate Gaussian distribution with a covariance matrix and a mean vector that modelled each sample. Individual samples and its distribution in the feature space should be taken care by the learned boundaries. Class and sample uncertainty information was used to obtain generalizable classification techniques and robust features. They formulated a loss function for max-margin learning based on Bayesian uncertainty measure. Their technique exhibited key performance enhancements on six benchmark databases for skin lesion detection, digit/object classification, attribute prediction and face verification. Neural networks do not measure uncertainty meaningfully as it leans to be overconfident on incorrectly labelled, noisy or unseen data. Variational approximations such as Multiplicative Normalising Flows or BBB are utilized by BDL to overcome this limitation. However, current methods have shortcomings regarding scalability and flexibility. Pawlowski et al. <ref type="bibr" target="#b231">[232]</ref> proposed a novel technique of variational approximation, termed as Bayes by Hypernet (BbH) that deduced hypernetworks as implicit distributions. It naturally scaled to deep learning architectures and utilized neural networks to model arbitrarily complex distributions. Their method was robust against adversarial attacks and yielded competitive accuracies. On the other hand, significant increase in prediction accuracy records in deep learning models, but it comes along with the enhancement in the cost of rendering predictions. Wang et al. <ref type="bibr" target="#b232">[233]</ref> speculated that for many of the real world inputs, deep learning models created recently, it tended to "over-think" on simple inputs. They proposed I Don't Know" (IDK) prediction cascades approach to create a set of pre-trained models systematically without a loss in prediction accuracy to speed up inference. They introduced two search based techniques for producing a new cost-aware objective as well as cascades. Their IDK cascade approach can be adopted in a model without further model retraining. They tested its efficacy on a variety of benchmarks. Yang et al. <ref type="bibr" target="#b233">[234]</ref> proposed a deep learning approach for propagating and quantifying uncertainty in models inspired by non-linear differential equations utilized by physicsinformed neural networks. Probabilistic representations for the system states were produced by latent variable models while physical laws described by partial differential equations were satisfied by constraining their predictions. It also forwards an adversarial inference method for training them on data. A regularization approach for efficiently training deep generative models was provided by such physics-informed constraints. Surrogates of physical models in which the training of datasets was usually small, and the cost of data acquisition was high. The outputs of physical systems were characterized by the framework due to noise in their observations or randomness in their inputs that bypassed the need of sampling costly experiments or numerical simulators. They proved efficacy of their method via a series of examples that demonstrated uncertainty propagation in non-linear conservation laws and detection of constitutive laws. For autonomous driving, 3D scene flow estimation techniques generate 3D motion of a scene and 3D geometry. Brickwedde et al. <ref type="bibr" target="#b234">[235]</ref> devised a new monocular 3D scene flow estimation technique dubbed as Mono-SF that assessed both motion of the scene and 3D structure by integrating single-view depth information and multiview geometry. A CNN algorithm termed as ProbDepthNet was devised for combining single-view depth in a statistical manner. The new recalibration technique, ProbDepth-Net, was presented for regression problems to guarantee wellcalibrated distributions. ProbDepthNet design and Mono-SF method proved its efficacy in comparison to the state-of-theart approaches. Mixup is a DNN training technique where extra samples are produced during training by convexly integrating random pairs of images and their labels. The method had demonstrated its effectiveness in improving the image classification performance. Thulasidasan et al. <ref type="bibr" target="#b235">[236]</ref> investigated the predictive uncertainty and calibration of models trained with mixup. They revealed that DNNs trained with mixup were notably better calibrated than trained in regular technique. They tested their technique in large datasets and observed that this technique was less likely to over-confident predictions using random-noise and OoD data. Label smoothing in mixup trained DNNs played a crucial role in enhancing calibration. They concluded that training with hard labels caused overconfidence observed in neural networks. The transparency, fairness and reliability of the methods can be improved by explaining black-box machine learning models. Model's robustness and users' trust raised concern as the explanation of these models exhibited considerable uncertainty. Zhang et al. <ref type="bibr" target="#b236">[237]</ref> illustrated the incidence of three sources of uncertainty, viz. variation in explained model credibility, variation with sampling proximity and randomness in the sampling procedure across different data points by concentrating on a specific local explanation technique called Local Interpretable Model-Agnostic Explanations (LIME). Even the black-box models with high accuracy yielded uncertainty. They tested the uncertainty in the LIME technique on two publicly available datasets and synthetic data.</p><p>In the incidence of even small adversarial perturbations, employment of DNNs in safety-critical environments is rigorously restricted. Sheikholeslami et al. <ref type="bibr" target="#b237">[238]</ref> devised a randomized approach to identify these perturbations that dealt with minimum uncertainty metrics by sampling at the hidden layers during the DNN inference period. Adversarial corrupted inputs were identified by the sampling probabilities. Any pre-trained DNN at no additional training could be exploited by new detector of adversaries. The output uncertainty of DNN from the BNNs perspectives could be quantified by choosing units to sample per hidden layer where layer-wise components denoted the overall uncertainty. Low-complexity approximate solvers were obtained by simplifying the objective function. These approximations associated state-of-the-art randomized adversarial detectors with the new approach in addition to delivering meaningful insights. Moreover, consistency loss between various predictions under random perturbations is the basis of one of the effective strategies in semi-supervised learning. In a successful student model, teachers' pseudo labels must possess good quality, otherwise learning process will suffer. But the prevailing models do not evaluate the quality of teachers' pseudo labels. Li et al. <ref type="bibr" target="#b238">[239]</ref> presented a new certainty-driven consistency loss (CCL) that employed predictive uncertainty information in the consistency loss to learn students from reliable targets dynamically. They devised two strategies i.e. Temperature CCL and Filtering CCL to either pay less attention on the uncertain ones or filter out uncertain predictions in the consistency regularization. They termed it FT-CCL by integrating the two strategies to enhance consistency learning approach. The FT-CCL demonstrated robustness to noisy labels and enhancement on a semi-supervised learning job. They presented a new mutual learning technique where one student was detached with its teacher and gained additional knowledge with another student's teacher. Englesson et al. <ref type="bibr" target="#b239">[240]</ref> introduced a modified knowledge distillation method to achieve computationally competent uncertainty estimates with deep networks. They tried to yield competitive uncertainty estimates both for out and in-of-distribution samples. Their major contributions were as follows: 1. adapting and demonstrating to distillation's regularization effect, 2. presenting a new target teacher distribution, 3. OoD uncertainty estimates were enhanced by a simple augmentation method, and 4. widespread set of experiments were executed to shed light on the distillation method. On the other hand, well calibrated uncertainty and accurate full predictive distributions are provided by Bayesian inference. High dimensionality of the parameter space limits the scaling of Bayesian inference methods to DNNs. Izmailov et al. <ref type="bibr" target="#b240">[241]</ref> designed low-dimensional subspaces of parameter space that comprised of diverse sets of high performing approaches. They applied variational inference and elliptical slice sampling in the subspaces. Their method yielded well-calibrated predictive uncertainty and accurate predictions for both image classification and regression by exploiting Bayesian model averaging over the induced posterior in the subspaces. Csáji et al. <ref type="bibr" target="#b241">[242]</ref> introduced a data-driven strategy for uncertainty quantification of models based on kernel techniques. The method needed few mild regularities in the computation of noise instead of distributional assumptions such as dealing with exponential families or GPs. The uncertainty about the model could be estimated by perturbing the residuals in the gradient of the objective function. They devised an algorithm to make it distributionfree, non-asymptotically guaranteed and exact confidence regions for noise-free and ideal depiction of function that they estimated. For the symmetric noises and usual convex quadratic problems, the regions were star convex centred on a specified small estimate, and ellipsoidal outer approximations were also efficiently executed. On the other hand, the uncertainty estimates can be measured while pretraining process. Hendrycks et al. <ref type="bibr" target="#b242">[243]</ref>  Trustworthy confidence estimates are required by high-risk domains from predictive models. Rigid variational distributions utilized for tractable inference that erred on the side of overconfidence suffered from deep latent variable models. Veeling et al. <ref type="bibr" target="#b243">[244]</ref> devised Stochastic Quantized Activation Distributions (SQUAD) that executed a tractable yet flexible distribution over discretized latent variables. The presented technique is sample efficient, self-normalizing and scalable. Their method yielded predictive uncertainty of high quality, learnt interesting non-linearities, fully used the flexible distribution. Multi-task learning (MTL) is another domain that the impact of the importance of uncertainty methods on it can be considered. For example, MTL demonstrated its efficacy for MR-only radiotherapy planning as it can jointly automate contour of organs-at-risk -a segmentation taskand simulate a synthetic CT (synCT) scan -a regression task from MRI scans. Bragman et al. <ref type="bibr" target="#b244">[245]</ref> suggested utilizing a probabilistic deep-learning technique to estimate the parameter and intrinsic uncertainty. Parameter uncertainty was estimated through a approximate Bayesian inference whilst intrinsic uncertainty was modelled using a heteroscedastic noise technique. This developed an approach for uncertainty measuring over prediction of the tasks and datadriven adaptation of task losses on a voxel-wise basis. They demonstrated competitive performance in the segmentation and regression of prostate cancer scans. More information can be found in Tables <ref type="table" target="#tab_22">5 and 6</ref>. As discussed earlier, GP is a powerful technique used for quantifying uncertainty. However, it is complex to form a Gaussian approximation to the posterior distribution even in the context of uncertainty estimation in huge deeplearning models. In such scenario, prevailing techniques generally route to a diagonal approximation of the covariance matrix in spite of executing low uncertainty estimates by these matrices. Mishkin et al. <ref type="bibr" target="#b367">[368]</ref> designed a novel stochastic, low-rank, approximate natural-gradient (SLANG) technique for VI in huge deep models to tackle this issue. Their technique computed a "diagonal plus lowrank" structure based on back-propagated gradients of the network log-likelihood. Their findings indicate that the proposed technique in forming Gaussian approximation to the posterior distribution. As a fact, the safety of the AI systems can be enhanced by estimating uncertainty in predictions. Such uncertainties arise due to distributional mismatch between the training and test data distributions, irreducible data uncertainty and uncertainty in model parameters. Malinin et al. <ref type="bibr" target="#b325">[326]</ref> devised a novel framework for predictive uncertainty dubbed as Prior Networks (PNs) that modelled distributional uncertainty explicitly. They achieved it by parameterizing a prior distribution over predictive distributions. Their work aimed at uncertainty for classification and scrutinized PNs on the tasks of recognizing OoD samples and identifying misclassification on the CIFAR-10 and MNIST datasets. Empirical results indicate that PNs, unlike non-Bayesian methods, could successfully discriminate between distributional and data uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Other UQ Techniques</head><p>In this sub-section, we aim to summary few other UQ techniques applied in the literature. We have selected the most relevant ones for this part of study. BNNs have been reinvigorating research interest by enhancing accurate predictions presented by neural networks with well-calibrated predictive uncertainties. But selection of model and number of nodes remain a challenge. Ghosh et al. <ref type="bibr" target="#b368">[369]</ref> exploited the horseshoe, continuous shrinkage priors and the regularized horseshoe distributions for selection of model in BNNs (see Fig. <ref type="figure" target="#fig_0">22</ref>). The strong shrinkage provided by the horseshoe was effective in turning of nodes that did not help explaining data when placed over node pre-activations and coupled with appropriate variational approximations. Their model selection technique over the number of nodes did not come at the expense of computational or predictive performance.</p><p>In another research, Hernandez-Lobato et al. <ref type="bibr" target="#b369">[370]</ref> introduced a novel approximate inference method based on the minimization of α-divergences termed as black-box alpha TABLE 5: More UQ methods in the main three categories proposed in the literature. Note that we provide in the row related to other methods the names of the proposed UQ methods for each reference. But, because of the importance of mentioning the proposed method, we also did the same in some other parts (General information).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UQ category</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bayesian</head><p>Balan et al. <ref type="bibr" target="#b245">[246]</ref> (BPE: Bayesian parameter estimation), Houthooft et al. <ref type="bibr" target="#b246">[247]</ref>   <ref type="bibr" target="#b274">[275]</ref> (MOPED: MOdel Priors with Empirical Bayes using DNN), Jesson et al. <ref type="bibr" target="#b275">[276]</ref>, Filos et al. <ref type="bibr" target="#b276">[277]</ref>, Huang et al. <ref type="bibr" target="#b277">[278]</ref>, Amit and Meir <ref type="bibr" target="#b278">[279]</ref>, Bhattacharyya et al. <ref type="bibr" target="#b279">[280]</ref>, Yao et al. <ref type="bibr" target="#b280">[281]</ref>, Laves et al. <ref type="bibr" target="#b281">[282]</ref> (UCE: uncertainty calibration error), Yang et al. <ref type="bibr" target="#b282">[283]</ref> (OC-BNN: Output-Constrained BNN), Thakur et al. <ref type="bibr" target="#b283">[284]</ref> (LUNA: Learned UA), Yacoby et al. <ref type="bibr" target="#b284">[285]</ref> (NCAI: Noise Constrained Approximate Inference), Masood and Doshi-Velez <ref type="bibr" target="#b285">[286]</ref> (PVI Particle-based VI), Abdolshah et al. <ref type="bibr" target="#b286">[287]</ref> (MOBO: Multi-objective Bayesian optimisation), White et al. <ref type="bibr" target="#b287">[288]</ref> (BO), Balandat et al. <ref type="bibr" target="#b288">[289]</ref> (BOTORCH), Galy-Fajou et al. <ref type="bibr" target="#b289">[290]</ref> (CMGGPC: Conjugate multi-class GP classification), Lee et al. <ref type="bibr" target="#b290">[291]</ref> (BTAML: Bayesian Task Adaptive Meta Learning), Vadera and Marlin <ref type="bibr" target="#b291">[292]</ref> (BDK: Bayesian Dark Knowledge), Siahkoohi et al. <ref type="bibr" target="#b292">[293]</ref> (SGLD: stochastic gradient Langevin dynamics), Sun et al. <ref type="bibr" target="#b293">[294]</ref>, Patacchiola et al. <ref type="bibr" target="#b294">[295]</ref>, Cheng et al. <ref type="bibr" target="#b295">[296]</ref>, Caldeira and Nord <ref type="bibr" target="#b296">[297]</ref>, Wandzik et al. <ref type="bibr" target="#b297">[298]</ref> (MCSD: Monte Carlo Stochastic Depth), Deng et al. <ref type="bibr" target="#b298">[299]</ref> (DBSN: DNN Structures), González-L ópez et al. <ref type="bibr" target="#b299">[300]</ref>, Foong et al. <ref type="bibr" target="#b300">[301]</ref> (ConvNP: Convolutional Neural Process), Yao et al. <ref type="bibr" target="#b301">[302]</ref> (SI: Stacked inference), Prijatelj et al. <ref type="bibr" target="#b302">[303]</ref>, Herzog et al. <ref type="bibr" target="#b303">[304]</ref>, Prokudin et al. <ref type="bibr" target="#b304">[305]</ref> (CVAE: conditional VAE), Tuo and Wang <ref type="bibr" target="#b305">[306]</ref>, Acerbi <ref type="bibr" target="#b306">[307]</ref> (VBMC+EIG (expected information gain)/VIQR (variational interquantile range)), Zhao et al. <ref type="bibr" target="#b307">[308]</ref> (GEP: generalized expectation propagation), Li et al. <ref type="bibr" target="#b308">[309]</ref> (DBGP: deep Bayesian GP), He et al. <ref type="bibr" target="#b309">[310]</ref> (NTK: Neural Tangent Kernel), Wang and Ročková <ref type="bibr" target="#b310">[311]</ref> (Gaussian approximability) Ensemble Zhang et al. <ref type="bibr" target="#b252">[253]</ref>, Chang et al. <ref type="bibr" target="#b259">[260]</ref>, He et al. <ref type="bibr" target="#b309">[310]</ref> (BDE: Bayesian Deep Ensembles), Schwab et al. <ref type="bibr" target="#b311">[312]</ref>, Smith et al. <ref type="bibr" target="#b312">[313]</ref>, Malinin and Gales <ref type="bibr" target="#b313">[314]</ref>, Jain et al. <ref type="bibr" target="#b314">[315]</ref>, Valdenegro-Toro <ref type="bibr" target="#b315">[316]</ref>, Juraska et al. <ref type="bibr" target="#b316">[317]</ref>, Oh et al. <ref type="bibr" target="#b317">[318]</ref>, Brown et al. <ref type="bibr" target="#b318">[319]</ref>, Salem et al. <ref type="bibr" target="#b319">[320]</ref>, Wen et al. <ref type="bibr" target="#b320">[321]</ref> Others Jiang et al. <ref type="bibr" target="#b1">[2]</ref> (Trust score), Qin et al. <ref type="bibr" target="#b321">[322]</ref> (infoCAM: informative class activation map), Wu et al. <ref type="bibr" target="#b322">[323]</ref> (Deep Dirichlet mixture networks), Qian et al. <ref type="bibr" target="#b323">[324]</ref> (Margin preserving metric learning), Gomez et al. <ref type="bibr" target="#b324">[325]</ref> (Targeted dropout), Malinin and Gales <ref type="bibr" target="#b325">[326]</ref> (Prior networks), Dunlop et al.et al. <ref type="bibr" target="#b326">[327]</ref> (DGP: deep GP), Hendrycks et al. <ref type="bibr" target="#b327">[328]</ref> (Self-supervision), Kumar et al. <ref type="bibr" target="#b328">[329]</ref> (Scaling-binning calibrator), <ref type="bibr" target="#b329">[330]</ref> (AugMix as a data processing approach), Mo żejko et al. <ref type="bibr" target="#b330">[331]</ref> (Softmax output), Boiarov et al. <ref type="bibr" target="#b331">[332]</ref> (SPSA: Simultaneous Perturbation Stochastic Approximation), Ye et al. <ref type="bibr" target="#b332">[333]</ref> (Lasso bootstrap), Monteiro et al. <ref type="bibr" target="#b333">[334]</ref> (SSNs: Stochastic Segmentation Networks), Maggi et al. <ref type="bibr" target="#b334">[335]</ref> (Superposition semantics), Amiri et al <ref type="bibr" target="#b335">[336]</ref> (LCORPP: learning-commonsense reasoning and probabilistic planning), Sensoy et al. <ref type="bibr" target="#b336">[337]</ref> (GEN: Generative Evidential Neural Network), Belakaria, et al. <ref type="bibr" target="#b337">[338]</ref> (USeMO: UA Search framework for optimizing Multiple Objectives), Liu et al. <ref type="bibr" target="#b338">[339]</ref> (UaGGP: UA Graph GP), Northcutt et al. <ref type="bibr" target="#b339">[340]</ref> (CL: Confident learning), Manders et al. <ref type="bibr" target="#b340">[341]</ref> (Class Prediction Uncertainty Alignment), Chun et al. <ref type="bibr" target="#b341">[342]</ref> (Regularization Method), Mehta et al. <ref type="bibr" target="#b342">[343]</ref> (Uncertainty metric), Liu et al. <ref type="bibr" target="#b343">[344]</ref> (SNGP: Spectral-normalized Neural GP), Scillitoe et al. <ref type="bibr" target="#b344">[345]</ref> (MF's: Mondrian forests), Ovadia et al. <ref type="bibr" target="#b345">[346]</ref> (Dataset shift), Biloš et al. <ref type="bibr" target="#b346">[347]</ref> (FD-Dir (Function Decomposition-Dirichlet) and WGP-LN (Weighted GP-Logistic-Normal)), Zheng and Yang <ref type="bibr" target="#b347">[348]</ref> (MR: memory regularization), Zelikman et al. <ref type="bibr" target="#b348">[349]</ref> (CRUDE: Calibrating Regression Uncertainty Distributions Empirically), Da Silva et al. <ref type="bibr" target="#b349">[350]</ref> (RCMP: Requesting Confidence-Moderated Policy advice), Thiagarajan et al. <ref type="bibr" target="#b350">[351]</ref> (Uncertainty matching), Zhou et al. <ref type="bibr" target="#b351">[352]</ref> (POMBU: Policy Optimization method with Model-Based Uncertainty), Standvoss et al. <ref type="bibr" target="#b352">[353]</ref> (RGNN: recurrent generative NN), Wang et al. <ref type="bibr" target="#b353">[354]</ref> (TransCal: Transferable Calibration), Grover and Ermon <ref type="bibr" target="#b354">[355]</ref> (UAE: uncertainty autoencoders), Cakir et al. <ref type="bibr" target="#b355">[356]</ref>, <ref type="bibr" target="#b356">[357]</ref> (MI), Yildiz et al. <ref type="bibr" target="#b357">[358]</ref> (ODE 2 V AE: Ordinary Differential Equation VAE), Titsias, Michalis et al. <ref type="bibr" target="#b358">[359]</ref> and Lee et al. <ref type="bibr" target="#b359">[360]</ref> (GP), Ravi and Beatson <ref type="bibr" target="#b360">[361]</ref> (AVI: Amortized VI), Lu et al. <ref type="bibr" target="#b361">[362]</ref> (DGPM: DGP with Moments), Wang et al. <ref type="bibr" target="#b362">[363]</ref> (NLE loss: negative log-likelihood error), Tai et al. <ref type="bibr" target="#b363">[364]</ref> (UIA: UA imitation learning), Selvan et al. <ref type="bibr" target="#b364">[365]</ref> (cFlow: conditional Normalizing Flow), Poggi et al. <ref type="bibr" target="#b365">[366]</ref> (Self-Teaching), Cui et al. <ref type="bibr" target="#b366">[367]</ref>  Fig. <ref type="figure" target="#fig_0">22</ref>: A summary of graphical models for the conditional dependencies of BNNs with Horseshoe priors which is reproduced based on <ref type="bibr" target="#b368">[369]</ref>. Note, the left part of the image is the centered parameterization and the right part is the on-centered parameterization.</p><p>(BB-α). It could be implemented using stochastic gradient descent as BB-α scaled to large datasets. Only the likelihood function and its gradients were required as input to im-plement BB-α in complex probabilistic models. Automatic differentiation could be applied to obtain these gradients.</p><p>Their method was able to interpolate between variational Bayes and an algorithm similar to expectation propagation by changing the divergence parameter α.</p><p>Patro et al. <ref type="bibr" target="#b370">[371]</ref> presented a probabilistic approach for solving the task of 'Visual Dialog'. Common sense knowledge to answer and understanding and reasoning of language modality, and visual modality are required to solve this task. They believed that the sources of uncertainty were critical in solving this task. Their framework helped a varied generation of answers and in estimating uncertainty. Their framework comprised of probabilistic representation module that provided representations for conversation history, question and image and an uncertainty representation module that selected the appropriate answer that minimized uncertainty. They achieved an enhanced visual dialog system that is also more explainable utilizing the presented probabilistic approach. Farquhar et al. <ref type="bibr" target="#b371">[372]</ref> introduced a variational approximate posterior for BNNs termed as Radial BNNs which scales well to large models.</p><p>Radial BNNs maintained full support: avoiding the a priori implausibility of discrete support and letting them acted as a prior for continual learning. Their technique evaded a sampling issue in mean-field variational inference (MFVI) occurred due to 'soap-bubble' pathology of multivariate Gaussians. They demonstrated that Radial BNNs are robust to hyperparameters unlike MFVI and proved its efficacy in real world tasks without needing intensive tuning and adhoc tweaks. Novak et al. <ref type="bibr" target="#b372">[373]</ref> proposed neural tangents; a library intended to facilitate research into infinite-width neural networks. It permitted a high-level API for denoting hierarchical and complex neural network architectures. These networks could be trained and estimated either in their infinitewidth or at finite-width limit as usual. Infinite-width networks can be trained analytically using gradient descent or using exact Bayesian inference via the Neural Tangent Kernel. All computations were distributed automatically over several accelerators with near-linear scaling in the different devices. Yıldız et al. <ref type="bibr" target="#b357">[358]</ref> proposed Ordinary Differential Equation Variational Auto-Encoder (ODE 2 V AE), a latent second order ODE model for high-dimensional sequential data. Their model could concurrently learn the embedding of high dimensional trajectories and deduce capriciously complex continuous-time latent dynamics. Their approach explicitly decomposed the latent space into position and momentum components and cracked a second order ODE system that was contrary to RNN based time series approaches and recently presented black-box ODE methods. They further presented probabilistic latent ODE dynamics parameterized by deep BNN for tackling uncertainty. They tested their method on bouncing balls, image rotation and motion capture datasets. Liu et al. <ref type="bibr" target="#b373">[374]</ref> proposed a novel technique to train a robust neural network against adversarial attacks. They observed that although fusing randomness could enhance the robustness of neural networks, incorporating noise blindly to all the layers was not the optimal way to add randomness. They formally learnt the posterior distribution of models in a scalable way by modelling randomness under the framework of BNN. They devised the mini-max problem in BNN to learn the best model distribution under adversarial attacks. Their method yielded state-of-the-art performance under strong attacks. As mentioned earlier, DNNs have yielded outstanding performances in several noteworthy domains including autonomous driving, security and medical diagnosis. In these domains, safety is very crucial, hence knowing what DNNs do not know is highly desirable. Most BNNs are trained by minimizing a suitable ELBO on a variational approximation or sampled through MC methods because of the intractability of the resulting optimization problem. Pomponi et al. <ref type="bibr" target="#b374">[375]</ref> devised a variant of former and replaced KL divergence in the ELBO term with a Maximum Mean Discrepancy (MMD) estimator. Their method based on the properties of the MMD exhibited numerous advantages including robustness to the choice of a prior over the weights, better calibrated and higher accuracy. They estimated the uncertainty as well as it performed in a robust manner against the injection of noise and adversarial attacks over their inputs. BNNs show promising results in enhancing UQ and robustness of modern DL methods.</p><p>However, they have problem with parameter efficiency and underfitting at scale. On the contrary, deep ensembles outperformed BNNs on certain problems, but they also have efficiency issues. The strengths of these two approaches need to be combined to remedy their common problems. Dusenberry et al. <ref type="bibr" target="#b272">[273]</ref> devised a rank-1 parameterization of BNNs and also utilized mixture approximate posteriors to capture multiple modes. Rank-1 BNNs demonstrated stateof-the-art performance across out of distribution variants, calibration on the test sets, accuracy and log-likelihood. Indeed, there are different types of uncertainties in machine and deep learning domains which are needed to be handled in different ways. Harang et al. <ref type="bibr" target="#b375">[376]</ref> investigated three types of uncertainties-open set uncertainty, intrinsic data uncertainty, and model capacity uncertainty and review methods to address each one. They proposed a unified hierarchical technique that integrated techniques from invertible latent density inference, Bayesian inference and discriminative classification in a single end-to-end DNN topology to demonstrate competent per-sample uncertainty estimation in a detection context. Their method could accommodate base/prior rates for binary detection and addressed all three uncertainties. In addition, it is critical for the safety of using an AI application to know the reliability of different classification accuracies. The standard procedure to access it is to use the confidence score of the classifier. Jiang et al. <ref type="bibr" target="#b1">[2]</ref> presented a novel score dubbed as trust score that estimated the agreement between a modified nearest-neighbor classifier and classifier on the testing example. They empirically demonstrated that high trust score exhibited high precision at recognizing correctly classified examples, outperforming the classifier's confidence score and other baselines as well. In another work, Rohekar et al. <ref type="bibr" target="#b376">[377]</ref> introduced a technique that covers both model selection and model averaging in the same framework. Their technique combined bootstrap with constraint-based learning to tackle prime limitation of constraint-based learning-sensitivity to errors in the independence tests. They formulated an algorithm for learning a tree, in which each node denoted a scored CPDAG for a subset of variables and the level of the node correspond to the maximal order of conditional independencies that were encoded in the graph. Greater computational efficiency was guaranteed by reusing stable low order independencies. Their algorithm learnt better MAP models, scaled well to hundreds of variables and more reliable causal relationships between variables. The Bayesian probabilistic framework presents an ethical way to perform model comparison and derive meaningful metrics for guiding decisions. However, many models are intractable with standard Bayesian methods, as their likelihood is computationally too expensive to evaluate or lack a closed-form likelihood function. Radev et al. <ref type="bibr" target="#b377">[378]</ref> presented a new approach for performing Bayesian model comparison using specialized deep learning architectures. They also introduced a new way to measure epistemic uncertainty in model comparison problems. They argued that their measure of epistemic uncertainty offers a distinctive proxy to quantify absolute evidence even in a framework which believed that the true data-generating model was within a finite set of candidate models. In another study, Belakaria et al. <ref type="bibr" target="#b337">[338]</ref> tackled the issue of multi-objective (MO) blackbox utilizing expensive function evaluations, where the goal was to approximate the true Pareto set of solutions while reducing the number of function evaluations. They introduced a new UA search framework called as USeMO to select efficiently the sequence of inputs for assessment to crack this issue. The selection process of USeMO comprised of cracking a cheap MO optimization problem via surrogate models of the true functions to recognize the most potential candidates and choosing the best candidate based on a measure of uncertainty. They also presented theoretical analysis to characterize the efficiency of their method. Extensive tuning of hyperparameters are required in many machine learning models to perform well. BO and variety of other methods are utilized to expedite and automate this process. As tuning usually requires repeatedly fully training models, hence it remains tremendously costly. Ariafar et al. <ref type="bibr" target="#b378">[379]</ref> introduced to hasten the Bayesian optimization method by applying the relative amount of information supplied by each training example. They leveraged importance sampling (IS) to do so. That enhanced the quality of the black-box function evaluations and their run-time and hence must be executed carefully. Indeed, NNs with binary weights are hardware-friendly and computation-efficient, but as it involves a discrete optimization problem, their training is challenging. Applying gradient-based methods, such as Straight-Through Estimator and ignoring the discrete nature of the problem surprisingly works well in practice. Meng et al. <ref type="bibr" target="#b379">[380]</ref> presented a principled approach that justified such methods applying Bayesian learning rule. The rule resulted in an algorithm when applied to compute a Bernoulli distribution over the binary weights. The algorithm enabled uncertainty estimation for continual learning to avoid catastrophic forgetting and also achieved state-ofthe-art performance. UQ methods have also been used in semi-supervised learning, zero-shot learning as well as meta learning. Semisupervised learning models, such as co-training, could present a powerful method to influence unlabeled data. Xia et al. <ref type="bibr" target="#b380">[381]</ref> introduced a new approach, UMCT (UA multiview co-training), to address semi-supervised learning on 3D data, such as volumetric data from medical imaging. Co-training was attained by exploring multi-viewpoint consistency of 3D data. They produced different views by permuting or rotating the 3D data and used asymmetrical 3D kernels to support diversified features in different sub-networks. Additionally, they presented an uncertaintyweighted label fusion technique to measure the reliability of each view's prediction with BDL. On the other hand, many models in generalized zero-shot learning depend on cross-modal mapping between the class embedding space and the image feature space. Sch önfeld et al. <ref type="bibr" target="#b381">[382]</ref> devised an approach where class embeddings and a shared latent space of image features were learned by modality-specific aligned VAE (named CADA-VAE). The key to their model is that they aligned the distributions learned from images and from side-information to produce latent features that contained the essential multi-modal information associated with unseen classes. They examined their learned latent features on several benchmark datasets and confirmed a novel state-of-the-art on generalized on few-shot or zeroshot learning. The general view and detailed CADA-VAE Meta-learning models <ref type="bibr" target="#b382">[383]</ref> are subject to overfitting when there are no enough training tasks for the metalearners to generalize. Tseng et al. <ref type="bibr" target="#b383">[384]</ref> proposed an effective and simple approach to mitigate the risk of overfitting for gradient-based meta-learning. They randomly dropped the gradient in the inner-loop optimization during the gradientbased adaptation stage such that the augmented gradients enhanced generalization to novel tasks. They proposed a general form of the presented gradient dropout regularization and demonstrated that this term could be sampled from either the Gaussian or Bernoulli distribution. They empirically yielded that the gradient dropout regularization alleviated the overfitting issue and enhanced the performance on different gradient-based meta-learning frameworks. There are some more UQ methods which have been proposed in the literature. For example, Variational Bayes (VB) is computationally efficient, theoretically grounded and generally applicable among methods to realize probabilistic inference in DNNs. Wu et al. <ref type="bibr" target="#b384">[385]</ref> devised two methods to turn VB into a robust inference tool for BNNs. First method presented a new deterministic method to approximate moments in neural networks, got rid of gradient variance. Second method proposed a hierarchical prior for parameters and a new Empirical Bayes technique for automated selection of prior variances. The resulting method combining these two methods is very robust and efficient. Another research direction is related to GP models for using in BNNs. A flexible and simple technique to generating expressive priors in GP models produces new kernels from a combination of basic kernels. Despite the link between BNNs and GPs, the BNN analogue of this has not yet been investigated. Pearce et al. <ref type="bibr" target="#b385">[386]</ref> explored BNN architectures mirroring such kernel combinations. They showed further how BNNs could generate periodic kernels that were often helpful in this context. Their empirical experiments demonstrated the practical value of these ideas in reinforcement and supervised settings. On the other hand, two prime obstacles in adoption of variational BNN are the high parameter overhead and difficulty of implementation that occurred due to "programming overhead". MC dropout tackles these obstacles well, but has limitation in model performance when applied in networks with batch normalization layers. Chang et al. <ref type="bibr" target="#b259">[260]</ref> designed a general variational family for ensemble-based BNN that included dropout as a special case. They further proposed two members of the family that worked well with batch normalization layers while preserving the advantages of low parameter and programming overhead. Bayesian inference facilitates a general framework for incorporating specific properties or prior knowledge into machine learning techniques through selecting a prior distribution carefully. Atanov et al. <ref type="bibr" target="#b386">[387]</ref> presented a novel type of prior distributions for CNN, deep weight prior (DWP), that examined generative models to persuade a certain structure of trained convolutional filters. They devised a technique for VI with implicit priors and denoted DWP in a form of an implicit distribution. The experimental results empirically showed that DWP enhanced the performance of BNN when training data is small and initialization of weights with samples from DWP hastened training of CNN. The catastrophic forgetting is an unavoidable issue in continual learning models for dynamic environments. Li et al. <ref type="bibr" target="#b387">[388]</ref> introduced a technique termed as Continual Bayesian Learning Networks (CBLN) to address this problem that facilitates the networks to distribute supplementary resources to acclimatize to new tasks without forgetting the formerly learned tasks. CBLN preserved a mixture of Gaussian posterior distributions that are combined with diverse tasks utilizing a BNN. The presented technique did not require accessing the past training data and could select proper weights to classify the data points during the test time automatically based on an uncertainty criterion. Along with all listed UQ methods, there are some other effective UQ methods which we would include them here as well. Alaa and van der Schaar <ref type="bibr" target="#b388">[389]</ref> developed the discriminative jackknife (DJ) procedure. The proposed DJ approach is flexible procedure which is usable to a wide range of DL methods. Shekhovtsov et al. <ref type="bibr" target="#b389">[390]</ref> exploited the cause of enhanced the generalization performance of deep networks due to Batch Normalization (BN). They argued that randomness of batch statistics was one of the prime reasons. The randomness emerged in the parameters rather than in activations and declared an explanation as a handy Bayesian learning. They utilized the idea to other deterministic normalization methods that were ignorant of the batch size. One of the prime drawbacks of NN-based UQ is the high memory requirement during training; which hinders their application to processing shallower architectures and/or smaller field-of-views (FOVs). Gantenbein et al. <ref type="bibr" target="#b390">[391]</ref> examined the efficacy of applying reversible blocks for constructing memory-efficient NN for quantification of segmentation uncertainty. The reversible architecture yielded memory saving by precisely computing the activations from the outputs of the successive layers during backpropagation instead of accumulating the activations for each layer. They incorporated the reversible blocks into an architecture termed as PHiSeg that was devised for UQ in medical image segmentation. The reversible architecture, RevPHiSeg, permitted training NNs for quantifying segmentation uncertainty on GPUs with restricted memory and processing larger FOVs. The authors in <ref type="bibr" target="#b9">[10]</ref> reengineered DeepLab-v3+, to produce its Bayesian counterpart applying Concrete dropout (CD) and MC dropout inference techniques. The UQ and surrogate modeling job for PDE systems are considered as supervised learning problems in most of the circumstances where output and input pairs are used for training <ref type="bibr" target="#b391">[392]</ref>. Yang et al. <ref type="bibr" target="#b392">[393]</ref> designed a framework for UQ for image registration using a low-rank Hessian approximation, a pathological image registration using an image synthesis deep network and predicted registration parameter with a patch-based deep learning approach by applying image appearances only. Their network predicted the initial momentum for the Deformation Diffeomorphic Metric Mapping (LDDMM) model for both multi-modal and uni-modal registration problems. The researchers in <ref type="bibr" target="#b393">[394]</ref> proposed a BDL model that integrated epistemic uncertainty with input-dependent aleatoric uncertainty. Their explicit uncertainty formulation produced a novel loss functions that could be interpreted as learned attenuation. Zhu et al. <ref type="bibr" target="#b391">[392]</ref> devised a physics-constrained deep learning model for high-dimensional UQ and surrogate modeling without labeled data. The ensembles of DNNs that belong to the mixture models class can be employed to quantify the prediction uncertainty <ref type="bibr" target="#b394">[395]</ref>. Vishnu et al. <ref type="bibr" target="#b199">[200]</ref> presented deep learning framework for prognostics with UQ that were helpful in conditions where (i) inherent noise was there in the sensor readings, (ii) operational conditions for future were not estimated, and (iii) labeled failure data was rare because of scarcity of failures. Constructing a Gaussian distribution over the weights, and sample it to produce a distribution over the categorical output are employed for approximation of distributions over the output of classification neural networks in BDL, however, the process is costly <ref type="bibr" target="#b395">[396]</ref>. Begoli et al. <ref type="bibr" target="#b12">[13]</ref> presented the challenges for the adoption of UQ due to the absence of sound underlying theory and new research opportunities for the advancement of theoretical methods and practical approaches of UQ in the area of AI assisted medical decision making. Kendall et al. <ref type="bibr" target="#b396">[397]</ref> proposed a real-time robust six-degree of freedom monocular visual relocalization system by applying a Bayesian convolutional neural network to single RGB images to regress the 6 degrees of freedom (6-DOF) camera pose. They obtained an estimation of relocalization uncertainty of their system and enhanced the state-of-theart localization accuracy on an outdoor database of large scale in nature. Semantic segmentation with Bayesian Deep Learning (BDL) has been revolutionary to attain uncertainty maps from deep models in semantic class prediction <ref type="bibr" target="#b9">[10]</ref>. To estimate whether a BDL model records an improved uncertainty estimates than another model, we need new metrics. The authors in <ref type="bibr" target="#b394">[395]</ref> utilized both variational Bayesian inference and maximum likelihood to train compound density networks, and demonstrated that they can obtain competitive uncertainty estimates on out-of-distribution (OoD) data which are robust in terms of adversarial examples. To guarantee high operational availability of equipment and condition-based maintenance, multi-sensor time series data extracted from Remaining Useful Life (RUL) or prognostics estimation are crucial. Hobbhahn et al. <ref type="bibr" target="#b395">[396]</ref> utilized the Dirichlet approximation to devise a lightweight uncertaintyaware output ranking for the setup of ImageNet. In this regard, they used the LaplaceBridge to map a Gaussian distribution onto a Dirichlet one. Bayesian neural networks (BNNs) do not scale well to computer vision jobs as it is tricky to train and show meager generalization under dataset-shift <ref type="bibr" target="#b200">[201]</ref>. This generates the need of effective ensembles that can generalize and produce trustworthy uncertainty estimates. The authors <ref type="bibr" target="#b200">[201]</ref> obtained diversity in the output predictions used for multi-modal data modeling by optimizing the diversity inducing adversarial loss for learning latent variables. The novel BDL tools make it possible to model epistemic uncertainty in computer vision <ref type="bibr" target="#b393">[394]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">APPLICATIONS</head><p>In this section, we discuss few most important applications of different UQ techniques used in machine and deep learning methods. In this regard, we first summarise the application of UQ techniques in image processing and computer vision followed by medical image analysis. Afterwards, we show how UQ has been applied to Natural Language Processing (NLP) and some more applications of UQ techniques in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Image Processing and Computer Vision</head><p>Nowadays, deep learning algorithms are being vastly used to map high dimensional data to output arrays, while these mappings can be inaccurate in many cases, e.g. the wrong identification of two African Americans as gorillas in an image classification system <ref type="bibr" target="#b397">[398]</ref>, has lead to racial discrimination <ref type="bibr" target="#b393">[394]</ref>. Therefore, it's important to take uncertainty into account, where the predictions made by deep learning based computer vision algorithms. To date, there have been number of studies addressed uncertainty in deep learning algorithms for various applications including but not limited to image/video retrieval <ref type="bibr" target="#b398">[399]</ref>, <ref type="bibr" target="#b399">[400]</ref>, depth estimation <ref type="bibr" target="#b400">[401]</ref>, <ref type="bibr" target="#b401">[402]</ref>, object detection <ref type="bibr" target="#b402">[403]</ref>, <ref type="bibr" target="#b403">[404]</ref>, <ref type="bibr" target="#b404">[405]</ref>, semantic segmentation and scene understanding <ref type="bibr" target="#b405">[406]</ref>, <ref type="bibr" target="#b406">[407]</ref>, <ref type="bibr" target="#b407">[408]</ref>, <ref type="bibr" target="#b408">[409]</ref>, <ref type="bibr" target="#b9">[10]</ref>, optical flow estimation and motion prediction <ref type="bibr" target="#b248">[249]</ref>, <ref type="bibr" target="#b409">[410]</ref>, <ref type="bibr" target="#b410">[411]</ref>, human pose estimation and pedestrian localization <ref type="bibr" target="#b411">[412]</ref>, <ref type="bibr" target="#b412">[413]</ref>, <ref type="bibr" target="#b304">[305]</ref>, person re-identification and face recognition <ref type="bibr" target="#b413">[414]</ref>, <ref type="bibr" target="#b414">[415]</ref>, <ref type="bibr" target="#b415">[416]</ref>, camera re-localization <ref type="bibr" target="#b396">[397]</ref>, avoiding adversarial attacks <ref type="bibr" target="#b416">[417]</ref>, <ref type="bibr" target="#b417">[418]</ref>, during the years 2016 to 2020. As a fact, most of research studies in deep learning applications are concentrating on prediction accuracy. Unlike those studies, untangling the complexity of various DNNs and addressing uncertainty for a variety of computer vision tasks has attracted significant interest <ref type="bibr" target="#b418">[419]</ref>. There still has been a good record of using Bayesian neural networks (BNNs) and Monte Carlo dropout (MC dropout) for uncertainty estimation on use of deep learning architectures. Nine studies have reported MC dropout as the most effective uncertainty quantity technique <ref type="bibr" target="#b396">[397]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b248">[249]</ref>, <ref type="bibr" target="#b419">[420]</ref>, <ref type="bibr" target="#b393">[394]</ref>, <ref type="bibr" target="#b412">[413]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b410">[411]</ref>, <ref type="bibr" target="#b417">[418]</ref>, applicable on various deep learning architectures. Kendall et al showed that uncertainty of their Bayesian convolutional neural networks model came from appearance and pose dissimilarity of images to the training set and could estimate the model's re-localization uncertainty, which improved localization accuracy on a large outdoor dataset <ref type="bibr" target="#b396">[397]</ref>. Same authors have developed a measure of model uncertainty by MC sampling with dropout and enhanced the semantic segmentation performance compared to the state-of-the-art methods in 2016 <ref type="bibr" target="#b42">[43]</ref>. Eldesokey et al. <ref type="bibr" target="#b420">[421]</ref> proposed an UA model for CNNs and tested on the KITTI dataset. The proposed model identified disturbed measurements of the input data after learning an input confidence estimator in a self-supervised procedure using the normalized CNNs (NCNNs). Indeed, epistemic uncertainty estimation is a challenging problem, and while several scalable techniques lately have appeared, no widespread assessment has been carried out in a real-world setting. Gustafsson et al. <ref type="bibr" target="#b421">[422]</ref> devised a comprehensive assessment framework for scalable epistemic uncertainty estimation techniques in deep learning. Their framework tested for the robustness needed in realworld computer vision applications. They also utilized their framework to compare conclusively and extensively two scalable techniques: MC-dropout and ensembling. Postels et al. <ref type="bibr" target="#b408">[409]</ref> proposed a sampling-free method for estimating the epistemic uncertainty of a neural network. Epistemic uncertainty is crucial in safety-critical applications, since it denotes the reliability of predictions using new data. Their prime contribution was the approximation of epistemic uncertainty estimated by these techniques which did not necessitate sampling, thus remarkably mitigating the computational overhead. They used their method to volumetric visual jobs to showcase the advantages of their techniques in terms of computational overhead as well as uncertainty estimates. Cai et al. <ref type="bibr" target="#b422">[423]</ref> worked on the hand segmentation generalization issue without using segmentation labels in the target domain. They designed a Bayesian CNN-based model adaptation approach for hand segmentation, which devised and considered two vital factors: 1) general information about hand shapes shared across domains and 2) prediction uncertainty when the model was used in a new domain. Accordingly, they introduced iterative selftraining strategy hand segmentation in the novel domain, which was directed by the model uncertainty approximated by a Bayesian CNN. But Bayesian techniques have not been exploited extensively for 3D modalities such as point clouds often utilized for autonomous systems and robots. Bhandary et al. <ref type="bibr" target="#b423">[424]</ref> examined three uncertainty quantification techniques viz. MC-DropConnect, MC dropout and deep ensemble on the DarkNet21Seg 3D semantic segmentation model and analyzed the impact of different parameters such as drop probability values on task performance, number of models in ensembles or forward passes and uncertainty estimate quality. They demonstrated that deep ensembles generated better results than other methods in terms of uncertainty metrics and performance. Weakly-supervised semantic segmentation using imagelevel labels is accomplished by acquiring object response maps. However, prevailing techniques depend on the classifier that can result in a response map only attending on discriminative object regions as the network does not require seeing the complete object for optimizing the classification loss. Chang et al. <ref type="bibr" target="#b424">[425]</ref> introduced a principled and end-toend trainable approach to let the network paying attention to other parts of the object, while generating a more uniform and complete response map. They proposed specifically Mixup data augmentation strategy into the classification network and devised two uncertainty regularization terms to better act together with the Mixup scheme. More information regarding different UQ methods applied in computer vision and image processing tasks is illustrated in Table <ref type="table" target="#tab_23">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Medical Applications</head><p>An automated analysis of medical image has come into existence as soon as it was possible load and scan medical images into the computer <ref type="bibr" target="#b425">[426]</ref>. At the outset, from the 1970s to the 1990s, medical image analysis was done with sequential application of low-level pixel processing (region growing, edge and line detector filters) and mathematical modeling (fitting lines, ellipses and circles) to build compound rulebased systems that solved particular tasks. It is analogous to expert systems with many if-then-else statements that were popular in artificial intelligence in the same period. At the end of the 1990s, supervised methods, where training samples are used to develop a system, is becoming gradually more popular in medical image analysis. Examples include active shape models, atlas, concept of feature extraction and use of statistical classifiers. This machine learning approach is still very popular and forms the foundation for various booming commercially available medical image analysis systems. Hence, there is a shift from systems that are entirely devised by humans to systems that are trained by computers utilizing example data from which feature vectors are derived. Computer algorithms establish the optimal decision boundary in the high-dimensional feature space. Both monetary and ethical costs of erroneous predictions can be noteworthy in medicine, and the complexity of the issue imposes progressively more complex models. Although DL methods have achieved outstanding performances in medical image analysis, most of them have not been employed in extremely automated disease monitoring systems due to lack of reliability of the model <ref type="bibr" target="#b426">[427]</ref>. For example, Dusenberry et al. <ref type="bibr" target="#b427">[428]</ref> studied the role of model uncertainty strategies in the medical domain. They demonstrated that population-level metrics, such as calibration error, loglikelihood, AUC-ROC and AUC-PR did not capture model uncertainty and was shown by applying RNN ensembles and different BRNNs. They showcased that RNNs with only Bayesian embeddings could be a competent way to tackle model uncertainty compared to ensembles. As we all know, medical well-annotated data is extremely expensive for conducting medical image segmentation. However, unlabeled data are very appropriate solution which can be used both in semi-supervised and unsupervised learning domains. As discussed earlier, Xia et al. <ref type="bibr" target="#b428">[429]</ref> introduced the UMCT model as a semi-supervised framework and tested it on various medical image datasets. They extended the Dice loss for ULF (uncertainty-weighted label fusion) as follows:</p><formula xml:id="formula_49">L Dice = 1 D Σ D d=0 2Σ N i=1 y d i ŷd i Σ N i=1 (y d i ) 2 + Σ N i=1 (ŷ d i ) 2 ,<label>(39)</label></formula><p>According to the obtained results, the proposed UMCT method outperformed the other applied methods to the same datasets. As a result, they concluded that having a proper uncertainty method can assist having a better medical image analysis performance. Blood oxygen saturation (sO 2 ) measurement by optical imaging oximetry offers insight into local tissue metabolism and functions. Traditional methods for quantifying sO 2 suffer from uncertainties due to variations in the experimental conditions, systemic spectral bias, light spectral bias, tissue geometry and biological variability. Liu et al. <ref type="bibr" target="#b429">[430]</ref> devised deep spectral learning (DSL), a novel data-driven approach to yield oximetry that was robust to experimental variations and also facilitated uncertainty quantification for each sO 2 prediction. Predictions calculated by DSL were highly adaptive to the depth-dependent backscattering spectra as well as to experimental variabilities. The DSL-predicted sO 2 demonstrated notably lower mean-square errors than those of the traditional least-squares fitting method. Inherent ambiguities cause many real-world vision problems.</p><p>It is difficult to access for example which region contains cancerous tissue from a CT scan in clinical applications. Kohl et al. <ref type="bibr" target="#b430">[431]</ref> devised a generative segmentation model based on the combination of a U-Net with a conditional VAE which is capable of generating large number of plausible hypotheses. They exhibited that on a Cityscapes segmentation task and a lung abnormalities segmentation task approach regenerated all the possible segmentation variants as well as the frequencies with which they outperformed the existing methods.</p><p>In another research, Ara újo et al. <ref type="bibr" target="#b431">[432]</ref> proposed an uncertainty-aware deep learning model (named DR-GRADUATE) for grading diabetic retinopathy (DR) using eye fundus images. In this regard, they introduced a new Gaussian-sampling technique on a Multiple Instance Learning (MIL) framework and used the proposed system as a second-opinion DR diagnostic system. UQ methods have also been used in prostate cancer domain. Karimi et al. <ref type="bibr" target="#b432">[433]</ref> studied the prostate cancer using ultrasound images. In this regard, they proposed a robust and accurate deep learning (CNN) segmentation model. Moreover, due to the importance of uncertainty in medical image analysis, they computed the uncertainty as follows:</p><formula xml:id="formula_50">Q = 1 − p2 − (1 − p) 2 , (<label>40</label></formula><formula xml:id="formula_51">)</formula><p>where p is the average of the applied probability maps. The obtained results confirmed that adding uncertainty resulted to having better prostate cancer segmentation outcomes. As discussed above, the MC dropout has demonstrated impressive performance for quantifying uncertainty in deep learning methods. Combalia et al. <ref type="bibr" target="#b433">[434]</ref> applied the MC dropout in DNNs for UQ of dermoscopic (skin lesion) image classification. Their results indicated that using different uncertainty metrics are appropriate solution to explore difficult and OoD samples. The cardiovascular disease detection by machine and deep learning is another research topic for application of UQ methods. 2D echocardiography is a widespread imaging modality for cardiovascular diseases. Deep learning techniques have widely been used in 2D echocardiography for structural and functional assessment and automated view classification. Most of the models do not estimate uncertainty in this regard which is very crucial. Dahal et al. <ref type="bibr" target="#b434">[435]</ref> compared three ensemble based uncertainty techniques utilizing four different metrics to achieve an insight of uncertainty modeling for left ventricular segmentation from Ultrasound (US) images. They further showed how uncertainty estimation could be utilized to reject inferior quality images and hence enhanced the segmentation results.</p><p>Registration is a basic task in medical image analysis which can be used in numerous tasks including motion analysis, multi-modal image alignment, intra-operative tracking and image segmentation. Zhu et al. <ref type="bibr" target="#b435">[436]</ref> proposed a neural registration framework (NeurReg) with a hybrid loss of displacement fields and data similarity, which considerably enhanced the existing state-of-the-art of registrations. They simulated different transformations by a registration simulator which created fixed image and displacement field ground truth for training. They devised three segmentation frameworks based on the proposed registration framework: 1) MTL with atlas-based segmentation as an intermediate feature, 2) joint learning of both registration and segmentation tasks, and 3) atlas-based segmentation. Different probable ailments can be detected by accurate and automatic segmentation of anatomical structures on medical images. Bian et al. <ref type="bibr" target="#b436">[437]</ref> introduced an uncertainty-aware domain alignment approach to tackle the domain shift issue in the cross-domain UDA (Unsupervised Domain Adaptation) task. Domain shift is an issue related to performance of the segmentation of various deep neural networks and segmentation task may deteriorate several devices or modalities due to the noteworthy dissimilarity across the domains. In this regard, they devised specifically an UESM (Uncertainty Estimation and Segmentation Module) to attain the uncertainty map estimation. Then, they proposed a new UCE (Uncertainty-aware Cross Entropy) loss to leverage the uncertainty information to enhance the performance of segmentation on extremely uncertain regions. The optimal target samples by uncertainty guidance were selected by an UST (Uncertainty-aware Self-Training) method to further boost the performance in the UDA task. Kohl et al. <ref type="bibr" target="#b437">[438]</ref> devised a segmentation network with a conditional variational auto-encoder (cVAE) termed it as Hierarchical Probabilistic U-Net that applied a hierarchical latent space decomposition. They demonstrated that their model formulation permitted reconstruction and sampling of segmentations with high fidelity while providing the flexibility to learn complex structured distributions across scales. Their model split automatically an inductive bias that they estimated useful in structured output prediction tasks beyond segmentation. In another research, Yin et al. <ref type="bibr" target="#b438">[439]</ref> stated that uncertainty related to FFR (fractional flow reserve) of coronary artery disease (CAD) in few properties such as anatomic and physiologic is common. For this reason, they proposed a predictive probabilistic model for FFR using the BO approach. The obtained outcomes clearly acknowledge the importance of dealing with uncertainty in the diagnosis of CAD. Li et al. <ref type="bibr" target="#b439">[440]</ref> exploited uncertainty calibration within an AL framework for medical image segmentation. Uncertainty estimation is specifically crucial in the data-driven AL setting where the goal is to attain definite accuracy with least labeling effort. The model learns to choose the most enlightening unlabeled samples for annotation derived from its estimated uncertainty. Different acquisition strategies and uncertainty estimation techniques were explored. They argued that choosing regions to annotate instead of full images led to more well-calibrated models. We provide further information about UQ methods applied in different medical application tasks in Table <ref type="table" target="#tab_28">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Natural Language Processing and Text Mining</head><p>Natural language processing (NLP) focuses on understanding, analysing and generating languages that humans utilize naturally <ref type="bibr" target="#b440">[441]</ref>. In recent years, significant and practical real-world problems have been addressed and large-scale systems are also deployed in this research domain. Novel machine and deep learning approaches such as continuous space methods and DNNs have inferred language patterns from the huge data of real world and make accurate predictions about the new data. One noteworthy challenge is to describe a language in a form that can be effectively processed by a learning system. NLP is an interdisciplinary field between linguistics and artificial intelligence <ref type="bibr" target="#b441">[442]</ref>. One of the most broadly studied areas of NLP is text mining (TM) that collects vital information from free (unstructured) texts. In this way, new knowledge can be extracted from a huge amount of texts. But the acquisition of reliable information from texts is not straightforward because of human linguistic ability of speaking about non-existing and non-realistic things or events. There are some propositions whose truth value cannot be unambiguously determined as these propositions are uncertain and they may be false true in some possible worlds but may be true in other ones. Uncertainty is a significant linguistic incident that is pertinent in many fields of language processing. In most general case, it can be termed as lack of information as the reader or listener is uncertain for a piece of information. Hence, uncertain propositions are those whose reliability or truth value cannot be determined due to lack of information. Distinguishing between uncertain and factual (i.e. true or false) propositions is of prime importance both in natural language processing and linguistics applications. It is essential to recognize linguistic cues of uncertainty since the target and source language may differ in their framework to express uncertainty in machine translation. In clinical document classification, medical reports can be grouped depending on whether the patient probability suffers, does not suffer or suffers from an ailment. There are several different NLP applications that try to investigate uncertainty in natural language texts in a couple of domains (e.g. news or biomedical texts). Most of these approaches use annotated databases for assessment. Various uncertainty corpora like the CoNLL-2010 Shared Task, FactBank, Genia and BioScope corpora has been produced in the recent years. Comparison of these corpora is not possible for the lack of unified annotation principles. The prevailing uncertainty detectors can hardly be applied across domains, and novel resource creation for each domain is costly and time consuming. Instead, a unified widespread approach is needed that can be adapted to a particular need of each domain without much effort and language independence of the model would also be preferred. But we reported a table which includes a summary of the most important UQ methods applied in NLP domain in Table <ref type="table">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Summary of some applied UQ methods in NLP</head><p>In this part of review, we briefly summarize some studies have been conducted on UQ in the domain of NLP. It should be noted that we do not disclose the details of the methods due to page limitations. For this reason, we strongly recommend that if the readers need more information of the proposed UQ methods, they can refer to the main sources.</p><p>A high-dimensional hidden layer and a large dictionary size make training of the RNN-language model (RNN-LM) as an ill-posed challenge. Chien et al. <ref type="bibr" target="#b440">[441]</ref> proposed a Bayesian approach to regularize the RNN-LM and utilized it for continuous speech recognition. The uncertainty of the estimated model parameters that was presented by a Gaussian prior was compensated by penalizing the too complicated RNN-LM. The Gaussian hyperparameter was estimated by maximizing the marginal likelihood and regularized parameters were computed with reference to maximum a posterior criterion was utilized to construct regularized model. A small set of salient outer-products were selected to devise the Bayesian RNN-LM (BRNN-LM) by developing a rapid approximation to a Hessian matrix.</p><p>As we know, clinical named entity recognition (NER) is one of the basic tasks for devising clinical NLP systems. Domain experts are required for annotating large amount of samples to achieve good performance by a machine learning (ML) system. This is an expensive exercise. A sample selection technique called active learning (AL) tries to mitigate the annotation cost. Chen et al. <ref type="bibr" target="#b442">[443]</ref> introduced and examined both novel and existing AL techniques for a clinical NER job to recognize medical treatments, problems and laboratory tests from the clinical notes. They simulated the AL experiments by applying different novel and prevailing algorithms in three categories including baseline sampling, diversity-based, and uncertainty-based techniques. Based on number of sentences vs. the learning curves of Fmeasure, uncertainty sampling performed superior to all its counterparts in terms of the area under the learning curve (ALC) score. Most diversity-based techniques yielded better performance than random sampling in ALC. In another research, Kong et al. <ref type="bibr" target="#b443">[444]</ref> introduced a novel theoretical perspective of data noising in RNN language models. They demonstrated that variants of data noising were instances of Bayesian RNN with a particular variational distribution. They presented natural extensions to data noising under the variational framework and a more principled method to apply at prediction time by utilizing this insight. They devised an element-wise variational smoothing technique and variational smoothing with tied input and output embedding matrices. Their model was empirically tested on two language modelling datasets and exhibited superior performance than the prevailing data noising techniques. As we know, factuality is a major concern in many domains especially in social media where informal texts are in abundance. The dependence of the existing methods in social media is on lexical cues where phrases are either omitted from the sentences or is expressed in substandard form. Han et al. <ref type="bibr" target="#b444">[445]</ref> introduced ANFU, an Attentionbased Neural Framework for Uncertainty identification on social media texts. ANFU incorporated CNN to capture the most vital semantics and attention-based Long Short-Term Memory (LSTM) networks to denote the semantics of words. The experiments were performed on four benchmark datasets (2 English + 2 Chinese). Their proposed ANFU method performed better than any state-of-the-art techniques in terms of F1 score using four social media datasets. Zhang et al. <ref type="bibr" target="#b445">[446]</ref> demonstrated that a huge deep learning model could utilize dropout variational inference to predict price movements from limit order books (LOBs), the data source with pricing and trading movements. To enhance profits by avoiding needless trades and position sizing, uncertainty information extracted from posterior predictive distributions could be applied. Their experimental results showcased that Bayesian techniques enhanced predictive performance as stochastic regularisers and uncertainty information could be utilised in trading. In another research, the authors in <ref type="bibr" target="#b446">[447]</ref> designed a measure of uncertainty for long sequences of discrete random variables related to the words in the output sentence. This measure took care of epistemic uncertainty similar to the MI that is applied for single discrete random variables such as in classification. Their uncertainty measures cracked a prime intractability in the raw application of prevailing methods on long sentences. They utilized Europarl and WMT 13 for German-English translation task to train a Transformer model with dropout.</p><p>As we know, machine translation is a hot topic in neural sequence-to-sequence models. A lack of diversity is observed in the final translations and performance degradation is reported with large beams. The study <ref type="bibr" target="#b447">[448]</ref> tried to uncover extrinsic uncertainty caused by noisy training data and related to some of the concerns associated to the inherent uncertainty of the task, due to the existence of numerous valid translations for a single source sentence. They proposed metrics and tools to examine how uncertainty in the data was recorded by the model distribution and the effects of searching techniques in translation. They also presented tools for examining model calibration and some limitations of the current models could be fixed by it. The authors in <ref type="bibr" target="#b115">[116]</ref> designed a module for rapid experimentation with neural network uncertainty and dubbed it as Bayesian Layers.</p><p>Neural network libraries with drop-in replacements for common layers were extended by it. These layers recorded activations ("stochastic output layers"), pre-activation units (dropout), uncertainty overweight (Bayesian neural nets) or the function itself (GP). They fused a 5-billion parameter "Bayesian Transformer" on 512 TPUv2 cores for uncertainty in a Bayesian dynamics and machine translation model for model-oriented planning. Bayesian Layers could be utilized for probabilistic programming with stochastic processes used within the Edward2 language. On the other hand, the complexity of machine learning models pose uncontrolled risks, the lack of control and knowledge of the internals of each component applied generate unavoidable effects, such as difficulty in auditability and lack of transparency. Mena et al. <ref type="bibr" target="#b448">[449]</ref> presented a wrapper that given a black-box model augmented its output prediction with an assessment of uncertainty. Decision rejection mechanism was employed to lessen the uncertainty or risk. They advocated for a rejection system based on the resulting uncertainty measure that discarded more uncertain predictions but selected more confident predictions; hence improved trustability of the system. They empirically showcased their method in simulated sentiment analysis framework for different domains.</p><p>As we know, reliable UQ is a prime step towards devising accountable, transparent, and explainable artificial intelligent systems and BDL plays a crucial role in such quantification. Xiao et al. <ref type="bibr" target="#b449">[450]</ref> presented new strategies to examine the data uncertainties and the benefits of characterizing model for NLP tasks. They utilized recurrent and CNN models to experiment empirically on language modelling, named entity recognition, and sentiment analysis to demonstrate that explicitly modelling uncertainties was not only improved model performances but also essential to compute output confidence levels in different NLP tasks. More studies have been conducted on the impact of Bayesian methods in improving the results of deep learning methods in NLP. The authors in <ref type="bibr" target="#b118">[119]</ref> investigated a variational Bayes scheme for RNN. At first, they demonstrated that good quality uncertainty estimates and superior regularisation could be adapted by using truncated backpropagation with an extra computational cost during training and also mitigating the number of parameters by 80%. Secondly, they illustrated that the performance of Bayesian RNNs could be enhanced further by employing a new kind of posterior approximation. The current batch statistics could be sharpened by incorporating local gradient information into the approximate posterior. This technique could be utilized broadly in training of BNNs. They empirically yielded that Bayesian RNNs performed better on an image captioning task and a language modelling benchmark than traditional RNNs. The authors in <ref type="bibr" target="#b450">[451]</ref> proposed an intelligent framework to enhance en-route flight safety by trajectory prediction where a Bayesian approach was utilized for model prediction uncertainty. Four steps were employed. In the first step, huge raw messages were processed with a distributed computing engine Apache Spark to derive trajectory information efficiently. Two deep learning models were then trained to predict the flight trajectory from different perspectives. The deep learning models were blended together to create multi-fidelity prediction in the third step. Then, the multi-fidelity technique was expanded to multiple flights to examine safety based on vertical and horizontal separation distance between two flights. The blended models showed promising results in en-route safety and flight trajectory prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5">Further Applications</head><p>In this section, we summarize more applications of various UQ methods. This section tries to cover few most important recent studies. As mentioned earlier, uncertainty estimates in RL tasks and large vision models can be obtained via dropout. A gridsearch over the dropout probabilities is essential -an impossible one with RL and a prohibitive operation with large models. Gal et al. <ref type="bibr" target="#b464">[465]</ref> devised a novel dropout variant that provided better performance and improved calibrated uncertainties. They used a continuous relaxation of dropout's discrete masks depending on the recent advancements in BDL. They analysed their variant on several tasks and provided insights into usual practice in the area where larger dropout probabilities are often utilized in deeper model layers.</p><p>Mobile robots for indoor use depend on 2D laser scanners for navigation, localization and mapping. These sensors are unable to measure the full occupancy of complex objects and cannot detect transparent surfaces. These estimates are prone to uncertainty and thus make the evaluation of confidence a significant issue for autonomous navigation and mapping. Verdoja et al. <ref type="bibr" target="#b465">[466]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">LITERATURE GAPS AND OPEN ISSUES</head><p>We reviewed most of UQ works in machine and deep learning methods and briefly discussed them in the previous sections. Nevertheless, there are several important literature gaps and open issues. In the following, we list the most important gaps and open issues that should be addressed in the near future. Furthermore, we have listed few future research directions for further studies. The most important gaps, open issues and future directions are discussed in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>This review shows that most of the proposed UQ methods are presented in the supervised, followed by unsupervised learning methods. However, we realized that there are fewer studies on the semisupervised learning methods. We believe that this is an important gap in the domain of UQ which could be filled in the future.</p><p>• Our findings reveal that most of UQ methods have been proposed overwhelmingly for different types of NNs, especially DL methods. However, there are many other methods in the field of ML that uncertainty has either not been investigated or superficially discussed. The probable reason for this possibility is that DL methods have been able to be almost the best (SOTA: the State Of The Art) methods in various fields (e.g. computer vision, medical image analysis, NLP, time series analysis, signal processing, etc.). But as a matter of fact, it can be claimed that different types of traditional ML methods have a significant performance on the analysis of small data whereas DL is almost incapable.</p><p>• Fusion-based methods (e.g., multi-modal Bayesian fusion <ref type="bibr" target="#b89">[90]</ref>, multi-level fusion <ref type="bibr" target="#b470">[471]</ref>, image fusion <ref type="bibr" target="#b471">[472]</ref>, data fusion <ref type="bibr" target="#b472">[473]</ref>, etc.) have shown great ability to optimize different machine and deep learning methods. This led us to investigate the effects of a variety of fusion-based methods to deal with uncertainty in machine and deep learning methods. We realized that fusion-based methods have significant potential to address the uncertainty of models. Therefore, we would suggest that more fusion-based methods can be used in future work for quantifying uncertainty.</p><p>•</p><p>The results of our research show that although a variety of ensemble methods <ref type="bibr" target="#b473">[474]</ref>, <ref type="bibr" target="#b474">[475]</ref>, <ref type="bibr" target="#b475">[476]</ref>, <ref type="bibr" target="#b476">[477]</ref>, <ref type="bibr" target="#b477">[478]</ref>, <ref type="bibr" target="#b478">[479]</ref>, <ref type="bibr" target="#b479">[480]</ref> also have a high ability to deal with uncertainty along with good performance and optimizing the performance of other methods, these high capabilities of these methods have not been used more significantly. In other words, we noticed that these methods have performed remarkably well in few studies. But we realized that the ensemble methods are less commonly used in recent studies. Therefore, we strongly recommend further studies on ensemble methods and their substantial impact on quantifying uncertainty in machine and deep learning domain. For example, Caldeira and Nord <ref type="bibr" target="#b296">[297]</ref> presented a comparative study to compare the performance of the three UQ important methods: BNN, CD and DE. Based on the obtained outcomes, they recommend DE for further investigations and applications as it achieved the best or comparable results in the study.</p><p>• Decision-making is a cognitive process which results in choosing the best possible action/belief among all other alternative options. There are few wellknown theories such as three-way decisions <ref type="bibr" target="#b480">[481]</ref> and Info-Gap (IG) decision <ref type="bibr" target="#b481">[482]</ref>, <ref type="bibr" target="#b482">[483]</ref> which can be used as UQ methods. Our findings reveal that these theories have been able to significantly help in dealing with uncertainty. For this reason, we think using various decision-making theories can be used during the decision-making process of machine and deep learning methods. Fig. <ref type="figure" target="#fig_0">24:</ref> A schematic view of the VAAL model which is reproduced based on <ref type="bibr" target="#b487">[488]</ref>.</p><p>reviews; however, reveal that although uncertainty in this area is quite important, but very few studies have been done in this subject. For example, Sinha et al. <ref type="bibr" target="#b487">[488]</ref> proposed a new Al in an adversarial manner which is called VAAL (Variational Adversarial AL).</p><p>The proposed model trained a latent space by using a VAE and an adversarial network trained to discriminate between labeled and unlabeled data (see Fig. <ref type="figure" target="#fig_0">24</ref>). They showed the dramatic impact of this type of method on UQ. For this reason, we believe that researchers can fill this gap with further studies in order to improve the data labeling quality by having far more certainty than previous studies. • Self supervised learning (SSL) <ref type="bibr" target="#b242">[243]</ref>, <ref type="bibr" target="#b489">[490]</ref> is an important subset of unsupervised learning for generating output labels from different data objects by finding a relationship between various parts of the object, or a variety of views of the given object. We think that the SSL methods have several uncertainties and therefore further investigation of these methods has a high potential as an important research gap of UQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The attention mechanism <ref type="bibr" target="#b490">[491]</ref>, <ref type="bibr" target="#b491">[492]</ref> is a powerful strategy in NNs and DL methods to concentrate on adequate parts of the input data more than the unnecessary (irrelevant) parts while conducting a prediction task. However, we found that selecting relevant and irrelevant parts of data is accompanied by uncertainty. Our reviews show that fewer studies of UQ have been conducted in this area. For this reason, we also list this area as a research gap for further investigations.</p><p>•</p><p>The OoD <ref type="bibr" target="#b492">[493]</ref>, <ref type="bibr" target="#b493">[494]</ref>, <ref type="bibr" target="#b494">[495]</ref>, <ref type="bibr" target="#b495">[496]</ref>, <ref type="bibr" target="#b496">[497]</ref> inputs (samples) can improve different models robustness as well as uncertainty. For example, Lee et al. <ref type="bibr" target="#b290">[291]</ref> a new model called BTAML for dealing with imbalanced data and also detection OoD samples. According to previous studies, we can see that detection of OoD can help for outstanding performance of different NN and DL methods. In the following, we aim to list few most important future directions in these domains. Theoretical analysis and a more resilient inference methodology of various UQ methods should be investigated in future studies. For example, integration of semi-supervised and AL can be developed for acquiring new samples. In addition, data labelling is a time consuming and costly process in all domains not only for computer vision and image processing. Therefore, we recommend further studies on automated data labelling techniques and investigating the impact of UQ methods. Applications of the cascade structures have proven to be a powerful mechanism for improving various machine and deep learning methods. However, we think that simplifying these methods and their integration with UQ methods for different computer vision and image processing tasks is valuable. Moreover, integration of dynamic/multi-modal image restoration issues with some advanced inversion approaches (e.g. different plug-and-play type schemes) for applying UQ methods for revealing relevant point estimates, is another interesting future direction. In addition, the review outcomes reveal that ensemble methods are still among the best approaches especially for detecting epistemic uncertainties (OoD issue). For this reason, application of new ensemble methods is another interesting research direction in computer vision and image processing. Integration of UQ methods with different human pose architectures and then use the estimated uncertainty in future frame prediction and motion tracking issues, is another engaging open research direction. Also, although we mentioned above some UQ methods for BAL, but we noticed that better uncertainty estimates in BAL as well as more complex methods should be proposed in this domain. In addition, we found that detecting adversarial samples is an interesting challenge for BNNs <ref type="bibr" target="#b497">[498]</ref>. Thus, we highly recommend further studies to develop more effective UQ methods for detecting adversarial samples. Sampling-free learning methods (e.g. Bayesian quantized networks (BQNs) <ref type="bibr" target="#b498">[499]</ref>, Sampling-free VI <ref type="bibr" target="#b499">[500]</ref>) are powerful techniques for learning an appropriate posterior distribution over their discrete parameters with truly-calibrated uncertainty. Furthermore, embedding techniques have obtained outstanding performance in different computer vision and image processing tasks. However, we found that there are very few studies on probabilistic embedding strategies <ref type="bibr" target="#b500">[501]</ref>, <ref type="bibr" target="#b501">[502]</ref> to quantify uncertainties. We also noticed that even though Bayesian methods (i.e., Variational Bayes <ref type="bibr" target="#b502">[503]</ref> have been used for UQ in capsule routing methods, but calibrated uncertainty estimates of predictions by using different capsule networks is an open future work direction. Online applications of different BNNs is an open issue for future investigations due to various reasons such as the limitations of variational techniques and having risks for selecting appropriate approximations of batch posteriors <ref type="bibr" target="#b503">[504]</ref>. Uncertainty of continual learning (CL) <ref type="bibr" target="#b251">[252]</ref> is another open research direction in computer vision and image processing. For example, Nguyen et al. <ref type="bibr" target="#b504">[505]</ref>, <ref type="bibr" target="#b505">[506]</ref> proposed variational CL (VCL) to deal with uncertainty and showed the effectiveness of such a approach. Medical image analysis: One possible research direction that could be considered in the future is a closer collaboration between medical and artificial intelligence researchers. Due to the high sensitivity in this field of science, we strongly recommend collecting larger medical data in the domain. This can be very helpful in resolving uncertainty, as a result, the proposed machine and deep learning methods can perform better in predicting various diseases and cancers. As we know, ground truth data for medical image segmentation plays a critical role in the correctness of the obtained results. For this reason, closer cooperation between the two groups can provide platforms for optimizing existing machine and deep learning models. Furthermore, referral of incorrect predicted data to specialists has a great role in dealing with uncertainty. Hence, there is a need for close collaboration between medical and computer researchers in the field of medical image segmentation. We also noticed that various fusion methods have good potential for segmentation uncertainty predictions of medical images. Moreover, we found out that in most of previous studies standard losses for medical segmentation have been used whereas there are some new, yet effective, losses which could be used. On the other hand, combining both visualization of uncertainty and medical image segmentation can be used within AL computer assisted diagnostic (CAD) systems to improve the segmentation output. Another important future direction can be concentrated on big medical data collection. As we know, having a bigger data can dramatically improve the performance of various deep and machine learning methods. Our comprehensive review reveals that the problem of having enough medical data is still open. But if this is not possible, transfer learning techniques can be an ideal solution for improving the training process. Using this technique we can properly tune the applied DL methods; however, we know there are few uncertainties. As mentioned above, this can be considered as an open gap for future researchers. The development of different semi-supervised-based methods for medical image segmentation is another promising approach for dealing with medical data shortages. We found out that UQ methods can have great impact in semisupervised-based medical image segmentation which is another interesting research direction for future investigations. Along with all of these open directions in medical data analysis, MTL have also showed promising performance for medical data analysis. But as Nguyen et al. <ref type="bibr" target="#b506">[507]</ref> showed adding UQ methods to MTL can significantly quantify uncertainty for prediction of clinical risks. However, as Gast and Roth <ref type="bibr" target="#b409">[410]</ref> stated although probabilistic approaches have widely been used for several years; however, probabilistic approaches have not been comprehensively applied in practice since sampling techniques are frequently too slow. To solve this issue, development of proper probabilistic approaches <ref type="bibr" target="#b409">[410]</ref> can be used in the real medical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Natural language processing (NLP):</head><p>There are few NLPbased subjects such as neural machine translation <ref type="bibr" target="#b507">[508]</ref> and some other interdisciplinary subjects such as image captioning <ref type="bibr" target="#b508">[509]</ref>, <ref type="bibr" target="#b509">[510]</ref>, <ref type="bibr" target="#b510">[511]</ref>, <ref type="bibr" target="#b511">[512]</ref>, visual question answering (VQA) <ref type="bibr" target="#b512">[513]</ref>, <ref type="bibr" target="#b513">[514]</ref>, <ref type="bibr" target="#b514">[515]</ref>, <ref type="bibr" target="#b515">[516]</ref>, <ref type="bibr" target="#b516">[517]</ref> which are closely associated with NLP. Finding the right caption for an image has always been a challenge in computer science. Especially since this issue can be accompanied by some uncertainties due to the merging of two important disciplines (i.e., image processing and NLP). On the other hand, medical VQA is a very important task for health organisations to find out the most appropriate answers to the given questions. Indeed, this topic handles both image processing and NLP tasks at the same time. We believe that because of the essence of the matter, adding methods to deal with uncertainties can greatly contribute to the productivity of this branch. In addition, classification of the data stream text environments is an interesting domain for application of UQ methods for finding the uncertain sentences. Further directions: In this phase, we discuss few research directions in various subjects such as RL, numerical data analysis, signal processing, toy and synthetic data, etc. For instance, application of meta RL (MRL) is effective and efficient in optimizing decision performance. However, decision making process is always come with uncertainty. Hence, we suggest adding UQ methods with various MRL models can yield better decisions with more certainty. The proposed natural-gradient approach <ref type="bibr" target="#b154">[155]</ref> can be generalized with some other approximation types (e.g., exponential-family distributions) and apply it on RL and stochastic optimization. Moreover, proper understanding of the interaction between choosing the inference algorithm and approximating family is another future research direction for synthetic data (regression task). Developing various stochastic regularization methods is another open direction for researchers. We also noticed that leveraging proper weights of the Frank-Wolfe optimization algorithm <ref type="bibr" target="#b111">[112]</ref> and finding how this technique interacts with some alternative procedures approximate inference can be interesting avenues for further investigations.</p><p>Moreover, the digital healthcare is a very important research area which can help to make medicine more precise and personalized. Quantifying uncertainty of the digital healthcare and deploy them in some real-world clinical settings is another open research path. Approximate Bayesian inference in the continual learning and sequential decision making applications should be used as an inner procedure of a larger method. And this procedure needs robust version of BNNs. Hence, application of Deterministic VI <ref type="bibr" target="#b384">[385]</ref> with different BNNs is an ideal approach. Accordingly, learning of different BNNs <ref type="bibr" target="#b517">[518]</ref>, <ref type="bibr" target="#b155">[156]</ref> can be optimized using various Assumed Density Filtering (ADF) techniques <ref type="bibr" target="#b518">[519]</ref> and apply them for different machine and deep learning tasks (e.g. NLP, computer vision and image processing, signal processing and many more). In addition, ensemblebased sampling <ref type="bibr" target="#b519">[520]</ref>, <ref type="bibr" target="#b520">[521]</ref> methods have shown the capability to approximate sampling techniques (i.e., Thompson sampling) and properly deal with uncertainty in complex methods such as different NNs. Finally, quantifying uncertainties for multi-agent systems <ref type="bibr" target="#b521">[522]</ref>, <ref type="bibr" target="#b522">[523]</ref> is also another important future direction since an individual agent cannot solve problems as are impossible or difficult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Lack of Data and Code Availability</head><p>As we know, availability of data and codes play significant role for improving prior methods. In other words, having data and codes will assist researchers to go through proposed methods and find main gaps. Our comprehensive review shows that most of previous studies (especially medical case studies) do not share their data and codes with others. We understand that because of few cases the authors may not be able to make the data/code public, but we believe sharing both data and code will be very helpful to improve the quality and performance of different machine and deep learning methods. In other words, having a code and data of a research paper will accelerate the optimization of the code in the study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">CONCLUSION</head><p>Uncertainty quantification is one of the key parts of decision making process. The UQ methods are becoming popular to evaluate the uncertainty in various real-life applications. Nowadays, uncertainty has become an inseparable part of traditional machine and deep leering methods. This study has comprehensively reviewed the most important UQ methods which have been applied in traditional machine learning and deep learning. Hence, we provided a comprehensive description and comparative analysis of current state-of-the-art UQ approaches. Also, in this study we reviewed many important UQ perspectives, including common UQ applications, in-depth discussions, main research gaps, ending by presenting several solid future research directions in the area. We believe that this review paper focusing on the use of the UQ methods in artificial intelligence will benefit researchers in a variety of fields, and may be considered as a guideline for the use of UQ in deep learning-based applications.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Schematic view of three different uncertainty models with the related network architectures, reproduced based on [9].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig.3: A graphical representation of two different uncertainty-aware (UA) models, reproduced based on<ref type="bibr" target="#b13">[14]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>g 1</head><label>1</label><figDesc>ωt (x)I + f ωt (x * ) T f ωt (x * ) − E[y * ] T E[y * ] −→ T →∞ V ar q * θ (y * |x * ) [y * ] (14) Bayesian Deep Learning/Bayesian Neural NetworksDespite the success of standard DL methods in solving various real-word problems, they cannot provide information about the reliability of their predictions. To alleviate</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Fig.6: A graphical implementations of different SG-MCMC models which is reproduced based on<ref type="bibr" target="#b41">[42]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 12 :Fig. 13 :</head><label>1213</label><figDesc>Fig. 12: A schematic view of the proposed VAE model by Edupuganti et al. which is reproduced based on [126].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 14 :</head><label>14</label><figDesc>Fig. 14: A general Gaussian-based DNN model proposed by Bradshaw et al. [152] which is reproduced based on the same reference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 15 :</head><label>15</label><figDesc>Fig. 15: A schematic view of the TCP model which is reproduced based on the same reference. [164].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 16 :</head><label>16</label><figDesc>Fig.16: A general view of BPO which is reproduced based on<ref type="bibr" target="#b170">[171]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 17 :</head><label>17</label><figDesc>Fig.17:A schematic view of TTA for ensembling techniques which is reproduced based on<ref type="bibr" target="#b197">[198]</ref>.the Prior Network p(π|x , ∅) represents a distribution over categorical output distributions. Ensembling performance is measured by uncertainty estimation. Deep learning ensembles produces benchmark results in uncertainty estimation. The authors in<ref type="bibr" target="#b197">[198]</ref> exploited in-domain uncertainty and examined its standards for its quantification and revealed pitfalls of prevailing matrices. They presented the deep ensemble equivalent score (DEE) and demonstrated how an ensemble of trained networks which is only few in number can be equivalent to many urbane ensembling methods with respect to test performance. For one ensemble, they proposed the test-time augmentation (TTA) in order to improve the performance of different ensemble learning techniques (see Fig. 17).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>mean/std m (l). They measured CLL std DE (l) and CLL mean DE (l) for natural numbers l ∈ N &gt;0 and linear interpolation is applied to define them for real values l ≥ 1. They depict DDE m (k) for different number of samples k for different methods m with upper and lower bounds DEE upper m (k) and DEE lower m (k). Different sources of model uncertainty can be taken care by incorporating a presented ensemble technique to propose a Bayesian nonparametric ensemble (BNE) model devised by Liu et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>K</head><label></label><figDesc>k=1 f k (X)ω k ; 2) BNE's direct correction to the prediction function is represented by the term δ; and 3) BNE's indirect correction on prediction derived from the relaxation of the Gaussian assumption in the model cumulative distribution function is represented by the term Φ((y|X, µ) − G[Φ((y|X, µ] dy. In addition, two error correction terms D δ (y|X) and D G (y|X) are also presented.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc><ref type="bibr" target="#b217">[218]</ref> introduced a new UA architecture called N eural Architecture Distribution Search (N ADS). The proposed NADS finds an appropriate distribution of different architectures which accomplish significantly good on a specified task. A single block diagram for searching space in the architecture is presented by Fig.19. Unlike previous designing architecture methods, NADS allows to recognize common blocks amongst the entire UA architectures. On the other hand, the cost functions for the uncertainty oriented neural network (NN) are not always converging. Moreover, an optimized prediction interval (PI) is not always generated by the converged NNs. The convergence of training is uncertain and they are not customizable in the case of such cost functions. To construct optimal PIs, Kabir et al.<ref type="bibr" target="#b218">[219]</ref> presented a smooth customizable cost function to develop optimal PIs to construct NNs. The PI coverage probability (PICP), PI-failure distances and optimized average width of PIs were computed to lessen the variation in the quality of PIs, enhance convergence probability and speed up the training. They tested their method on electricity demand and wind power generation data. In the case of non-Bayesian deep neural classification, uncertainty estimation methods introduced biased estimates for instances whose predictions are highly accurate. They argued that this limitation occurred because of the dynamics of training with SGD-like optimizers and possessed similar characteristics such as overfitting. Geifman et al.<ref type="bibr" target="#b219">[220]</ref> proposed an uncertainty estimation method that computed the uncertainty of highly confident points by utilizing snapshots of the trained model before their approximations were jittered. The proposed algorithm outperformed all well-known techniques. In another research, Tagasovska et al.<ref type="bibr" target="#b220">[221]</ref> proposed single-model estimates for DNNs of epistemic and aleatoric uncertainty. They suggested a loss function called Simultaneous Quantile Regression (SQR) to discover the conditional quantiles of a target variable to assess aleatoric uncertainty. Well-calibrated prediction intervals could be derived by using these quantiles. They devised Orthonormal Certificates (OCs), a collection of nonconstant functions that mapped training samples to zero to estimate epistemic uncertainty. The OoD examples were mapped by these certificates to non-zero values. van Amersfoort et al.<ref type="bibr" target="#b208">[209]</ref>,<ref type="bibr" target="#b221">[222]</ref> presented a method to find and reject distribution data points for training a deterministic deep model with a single forward pass at test time. They exploited the ideas of RBF networks to devise deterministic UQ (DUQ) which is presented in Fig.20. They scaled training in this with a centroid updating scheme and new loss function. Their method could detect out of distribution data consistently by utilizing a gradient penalty to track changes in the input. Their method is able to enhance deep ensembles and scaled well to huge databases. Tagasovska et al.<ref type="bibr" target="#b222">[223]</ref> demonstrated frequentist estimates of epistemic and aleatoric uncertainty for DNNs. They proposed a loss function, simultaneous quantile regression to estimate all the conditional quantiles of a given target variable in case of aleatoric uncertainty. Well-calibrated prediction intervals could be measured by using these quantiles. They proposed a collection of non-trivial diverse functions that map all training samples to zero and dubbed as training certificates for the estimation of epistemic uncertainty. The certificates signalled high epistemic uncertainty by mapping OoD examples to non-zero values. By using Bayesian deep networks, it is possible to know what the DNNs do not know in the domains where safety is a major concern. Flawed</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 21 :</head><label>21</label><figDesc>Fig. 21: A causal view demonstrating the main assumptions taken by Rohekar et al. [230] (this figure is reproduced based on the reference).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>demonstrated that pre-training enhanced the uncertainty estimates and model robustness although it might not improve the classification metrics. They showed the key gains from pre-training by performing empirical experiments on confidence calibration, OoD detection, class imbalance, label corruption and adversarial examples. Their adversarial pre-training method demonstrated approximately10% enhancement over existing methods in adversarial robustness. Pre-training without task-specific techniques highlighted the need for pretraining, surpassed the state-of-the-art when examining the future techniques on uncertainty and robustness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>Fig. 23: A schematic view of CADA-VAE (Cross-and Distribution Aligned VAE) which is reproduced based on [382].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>9. 1</head><label>1</label><figDesc>Future DirectionsUQ methods have been achieving highly regarded achievement and obtaining distinguished performance when applied to different machine and deep learning methods. However, there are some open challenges that should be addressed. We provide several future directions of UQ methods for the three main research topics: computer vision and image processing, medical image processing and NLP.Computer vision and image processing:As discussed earlier, computer vision and image processing are two main research domains for the application of different UQ methods. Although various studies have been conducted in these areas, but there are certainly still many open research directions which should be considered in the future researches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1 :</head><label>1</label><figDesc>A summary of studies that applied the original MC dropout to approximate uncertainty along with their applications (Sorted by year).</figDesc><table><row><cell>Study</cell><cell>Year</cell><cell>Method</cell><cell>Application</cell><cell>Code</cell></row><row><cell>Kendal et al. [43]</cell><cell>2015</cell><cell>SegNet [44]</cell><cell>semantic segmentation</cell><cell>√</cell></row><row><cell>Leibig et al. [45]</cell><cell>2017</cell><cell>CNN</cell><cell>diabetic retinopathy</cell><cell>√</cell></row><row><cell>Choi et al. [46]</cell><cell cols="2">2017 mixture density network (MDN) [47]</cell><cell>regression</cell><cell>×</cell></row><row><cell>Jung et al. [48]</cell><cell>2018</cell><cell>full-resolution ResNet [49]</cell><cell>brain tumor segmentation</cell><cell>×</cell></row><row><cell>Wickstrom et al. [50]</cell><cell>2018</cell><cell>FCN [51] and SehNet [44]</cell><cell>polyps segmentation</cell><cell>×</cell></row><row><cell>Jungo et al. [52]</cell><cell>2018</cell><cell>FCN</cell><cell>brain tumor segmentation</cell><cell>×</cell></row><row><cell>Vandal et al. [53]</cell><cell>2018</cell><cell>Variational LSTM</cell><cell>predict flight delays</cell><cell>×</cell></row><row><cell>Devries and Taylor [54]</cell><cell>2018</cell><cell>CNN</cell><cell>medical image segmentation</cell><cell>×</cell></row><row><cell>Tousignant et al. [55]</cell><cell>2019</cell><cell>CNN</cell><cell>MRI images</cell><cell>×</cell></row><row><cell>Norouzi et al. [56]</cell><cell>2019</cell><cell>FCN</cell><cell>MRI images segmentation</cell><cell>×</cell></row><row><cell>Roy et al. [57]</cell><cell>2019</cell><cell>Bayesian FCNN</cell><cell>brain images (MRI) segmentation</cell><cell>√</cell></row><row><cell>Filos et al. [58]</cell><cell>2019</cell><cell>CNN</cell><cell>diabetic retinopathy</cell><cell>√</cell></row><row><cell cols="2">Harper and Southern [59] 2020</cell><cell>RNN and CNN</cell><cell>emotion prediction</cell><cell>×</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>3.7 Variational Autoencoders ℓ ′′ (𝒛 𝟏 , 𝒚 𝟏 , 𝒛 𝟐 , 𝒚 𝟐 )</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>compared the performance</cell></row><row><cell cols="3">of two well-known uncertainty methods (MC dropout and</cell></row><row><cell cols="3">BBB) in medical image segmentation (cardiac MRI) on a U-</cell></row><row><cell cols="3">Net model. The obtained results showed that MC dropout</cell></row><row><cell cols="3">and BBB demonstrated almost similar performances in med-</cell></row><row><cell cols="2">ical image segmentation task.</cell><cell></cell></row><row><cell></cell><cell>ෝ 𝒚 𝟏</cell><cell></cell></row><row><cell></cell><cell>𝒛 𝟏</cell><cell></cell></row><row><cell>𝒙 𝟏</cell><cell>𝒒 𝝓 (</cell><cell>𝒙 𝟏</cell></row><row><cell></cell><cell>𝒚 𝟏</cell><cell>𝒚 𝟐</cell></row><row><cell>𝒙 𝟐</cell><cell></cell><cell>𝒙 𝟐</cell></row><row><cell></cell><cell>𝒛 𝟐</cell><cell></cell></row><row><cell></cell><cell>ෝ 𝒚 𝟐</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>z|x) 𝒒 𝝓 (z|x) 𝒑 𝜽 (x|z) 𝒑 𝜽 (x|z)</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 2 :</head><label>2</label><figDesc>Further information of some UQ methods used in RL.</figDesc><table><row><cell>Study</cell><cell></cell><cell>Application</cell><cell></cell><cell></cell><cell>Goal/Objective</cell><cell>UQ method</cell><cell></cell><cell></cell><cell>Code</cell></row><row><cell>Tegho et al. [176]</cell><cell></cell><cell cols="3">Dialogue management con-</cell><cell>Dialogue policy optimisation</cell><cell cols="3">BBB propagation deep Q-networks</cell><cell>×</cell></row><row><cell></cell><cell></cell><cell>text</cell><cell></cell><cell></cell><cell></cell><cell>(BBQN)</cell><cell></cell><cell></cell></row><row><cell>Janz et al. [177]</cell><cell></cell><cell cols="3">Temporal difference learn-</cell><cell>Posterior sampling for RL</cell><cell cols="2">Successor Uncertainties (SU)</cell><cell></cell><cell>√</cell></row><row><cell></cell><cell></cell><cell>ing</cell><cell></cell><cell></cell><cell>(PSRL)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Shen and How [178]</cell><cell></cell><cell>Discriminating</cell><cell cols="2">potential</cell><cell>Stochastic belief space policy</cell><cell>Soft-Q learning</cell><cell></cell><cell></cell><cell>×</cell></row><row><cell></cell><cell></cell><cell>threats</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Benatan and Pyzer-</cell><cell>Safe RL (SRL)</cell><cell></cell><cell></cell><cell>The weights in RNN using</cell><cell cols="3">Probabilistic Backpropagation (PBP)</cell><cell>×</cell></row><row><cell>Knapp [179]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>mean and variance weights</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Kalweit and Boedecker</cell><cell>Continuous</cell><cell>Deep</cell><cell>RL</cell><cell>Minimizing real-world inter-</cell><cell>Model-assisted</cell><cell>Bootstrapped</cell><cell>Deep</cell><cell>×</cell></row><row><cell>[180]</cell><cell></cell><cell>(CDRL)</cell><cell></cell><cell></cell><cell>action</cell><cell cols="3">Deterministic Policy Gradient (MA-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>BDDPG)</cell><cell></cell><cell></cell></row><row><cell>Riquelme et al. [181]</cell><cell></cell><cell cols="3">Approximating the poste-</cell><cell>Balancing both exploration</cell><cell cols="3">Deep Bayesian Bandits Showdown using</cell><cell>√</cell></row><row><cell></cell><cell></cell><cell>rior sampling</cell><cell></cell><cell></cell><cell>and exploitation in different</cell><cell cols="2">Thompson sampling</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>complex domains</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Huang et al. [182]</cell><cell></cell><cell cols="2">Model-based RL (MRL)</cell><cell></cell><cell>Better decision and improve</cell><cell cols="3">Bootstrapped model-based RL (BMRL)</cell><cell>×</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>performance</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Eriksson</cell><cell>and</cell><cell cols="3">Risk measures and leverag-</cell><cell>Risk-Sensitive RL (RSRL)</cell><cell cols="3">Epistemic Risk Sensitive Policy Gradient</cell></row><row><cell>Dimitrakakis [183]</cell><cell></cell><cell>ing preferences</cell><cell></cell><cell></cell><cell></cell><cell>(EPPG)</cell><cell></cell><cell></cell></row></table><note>× L ötjens et al. [184] SRL UA navigation Ensemble of MC dropout (EMCD) and Bootstrapping × Clements et al. [185] Designing risk-sensitive algorithm Disentangling aleatoric and epistemic uncertainties Combination of distributional RL (DRL) and Approximate Bayesian computation (ABC) methods with NNs √ D'Eramo et al. [186] Drive exploration Multi-Armed Bandit (MAB) Bootstrapped deep Q-network with TS (BDQNTS) ×</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 3 :</head><label>3</label><figDesc>Main loss functions used by ensemble techniques for UQ.</figDesc><table><row><cell>Study</cell><cell>Dataset type</cell><cell>Base</cell><cell>Method's name</cell><cell>Loss equation</cell><cell>Code</cell></row><row><cell></cell><cell></cell><cell>classifier(s)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TV et al. [200]</cell><cell>Sensor data</cell><cell>Neural</cell><cell>Ordinal</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Networks</cell><cell>Regression (OR)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(LSTM)</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>the standard square loss) ×</figDesc><table><row><cell>Lakshminarayanan</cell><cell>Image</cell><cell>Neural</cell><cell>Deep Ensembles</cell><cell>L(θ) = −S(p θ , q)</cell><cell>×</cell></row><row><cell>et al. [189]</cell><cell></cell><cell>Networks</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Jain et al. [190]</cell><cell>Image and Protein</cell><cell>Deep</cell><cell>Maximize Overall</cell><cell></cell><cell></cell></row><row><cell></cell><cell>DNA binding</cell><cell>Ensembles</cell><cell>Diversity (MOD)</cell><cell></cell><cell></cell></row><row><cell>Gustafsson et al.</cell><cell>Video</cell><cell>Neural</cell><cell>Scalable BDL</cell><cell>Regression:</cell><cell></cell></row><row><cell>[191]</cell><cell></cell><cell>Networks</cell><cell></cell><cell></cell><cell></cell></row></table><note>L(θm; xn, yn) = − log p θm (yn|xm) ×</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 4 :</head><label>4</label><figDesc>Main loss functions used by deep ensemble techniques for UQ. = H(F ensemble (X t−T :t−1 ), one hot(Xt)) ×</figDesc><table><row><cell>argued that in variational</cell></row></table><note>methods. They devised Deep Ensemble BAL that addressed the mode collapse issue and improved the MC dropout method. In another study, Pop et al.<ref type="bibr" target="#b211">[212]</ref> proposed a novel AL technique especially for DNNs. The statistical properties and expressive power of model ensembles were employed to enhance the state-of-the-art deep BAL technique that suffered from the mode collapse problem. L</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Uncertainty modelling in DNNs is an open problem despite advancements in the area. BNNs, where the prior over network weights is a design choice, is a powerful solution. Frequently normal or other distribution supports sparsity. The prior is agnostic to the generative process of the input data. This may direct to unwarranted generalization for outof-distribution tested data. Rohekar et al.<ref type="bibr" target="#b229">[230]</ref> suggested a confounder for the relation between the discriminative function and the input data given the target label.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>They proposed</cell></row><row><cell cols="4">for modelling the confounder by sharing neural connec-</cell></row><row><cell cols="4">tivity patterns between the discriminative and generative</cell></row><row><cell cols="4">networks. Hence, a novel deep architecture was framed</cell></row><row><cell cols="4">where networks were coupled into a compact hierarchy and</cell></row><row><cell cols="4">sampled from the posterior of local causal structures (see</cell></row><row><cell>Fig. 21).</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">They showed that sampling networks from the hierarchy,</cell></row><row><cell></cell><cell>Generative function</cell><cell></cell><cell></cell></row><row><cell></cell><cell>parameters</cell><cell></cell><cell></cell></row><row><cell></cell><cell>𝜃</cell><cell></cell><cell></cell></row><row><cell>Input data (features)</cell><cell>𝑿</cell><cell>∅</cell><cell>Discriminative function parameters</cell></row><row><cell></cell><cell>𝑌</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Class</cell><cell></cell><cell></cell></row><row><cell></cell><cell>(label)</cell><cell></cell><cell></cell></row><row><cell>Independent normal prior in weight space leads to weak</cell><cell></cell><cell></cell><cell></cell></row><row><cell>constraints on the function posterior, permit it to generalize</cell><cell></cell><cell></cell><cell></cell></row><row><cell>in unanticipated ways on inputs outside of the training</cell><cell></cell><cell></cell><cell></cell></row><row><cell>distribution. Hafner et al. [14] presented noise contrastive</cell><cell></cell><cell></cell><cell></cell></row><row><cell>priors (NCPs) to estimate consistent uncertainty. The prime</cell><cell></cell><cell></cell><cell></cell></row><row><cell>initiative was to train the model for data points outside</cell><cell></cell><cell></cell><cell></cell></row><row><cell>of the training distribution to output elevated uncertainty.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>The NCPs relied on input prior, that included noise to the</cell><cell></cell><cell></cell><cell></cell></row><row><cell>inputs of the current mini batch, and an output prior, that</cell><cell></cell><cell></cell><cell></cell></row><row><cell>was an extensive distribution set by these inputs. The NCPs</cell><cell></cell><cell></cell><cell></cell></row><row><cell>restricted overfitting outside of the training distribution</cell><cell></cell><cell></cell><cell></cell></row><row><cell>and produced handy uncertainty estimates for AL. BNNs</cell><cell></cell><cell></cell><cell></cell></row><row><cell>with latent variables are flexible and scalable probabilistic</cell><cell></cell><cell></cell><cell></cell></row><row><cell>models. They can record complex noise patterns in the</cell><cell></cell><cell></cell><cell></cell></row><row><cell>data by using latent variables and uncertainty is accounted</cell><cell></cell><cell></cell><cell></cell></row></table><note>Patro et  al.<ref type="bibr" target="#b227">[228]</ref> devised gradient-based certainty estimates with visual attention maps. They resolved visual question answering job. The gradients for the estimates were enhanced by incorporating probabilistic deep learning techniques. There are two key advantages: 1. enhancement in getting the certainty estimates correlated better with misclassified samples and 2. state-of-the-art results obtained by improving attention maps correlated with human attention regions. The enhanced attention maps consistently improved different techniques for visual question answering. Improved certainty estimates and explanation of deep learning techniques could be achieved through the presented method. They provided empirical results on all benchmarks for the visual question answering job and compared it with standard techniques. BNNs have been used as a solution for neural network predictions, but it is still an open challenge to specify their prior. by network weights. Depeweg et al.<ref type="bibr" target="#b228">[229]</ref> exhibited the decomposition and derived uncertainty into aleatoric and epistemic for decision support systems. That empowered them to detect informative points for AL of functions with bimodal and heteroscedastic noises. They further described a new risk-sensitive condition to recognize policies for RL that balanced noise aversion, model-bias and expected cost by applying decomposition.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>(VIME: VI Maximizing Exploration), Springenberg et al. [248], Ilg et al. [249], Heo et al. [250], Henderson et al. [251], Ahn et al. [252], Zhang et al. [253], Sensoy et al. [254], Khan et al. [155], Acerbi [255] (VBMC: Variational Bayesian Monte Carlo), Haußmann et al. [256], Gong et al. [72], De Ath et al. [257], Foong et al. [258], Hasanzadeh et al. [259], Chang et al. [260], Stoean et al. [261], Xiao et al. [262], Repetti et al. [263], T óthová et al. [264], Moss et al. [265], Dutordoir et al. [266], Luo et al. [267], Gafni et al. [268], Jin et al. [269],Han et al. [270], Stoean et al. [271], Oh et al. [272], Dusenberry et al. [273], Havasi et al. [274], Krishnan et al.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>(MMD: Maximum Mean Discrepancy)</figDesc><table><row><cell></cell><cell>𝒃 𝟎</cell><cell></cell><cell>𝒃 𝒈</cell><cell>𝒃 𝟎</cell><cell>𝒃 𝒈</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>𝕀</cell></row><row><cell></cell><cell cols="2">𝝀 𝒌𝒍</cell><cell>𝓥 𝒍</cell><cell>𝝀 𝒌𝒍</cell><cell>𝓥 𝒍</cell><cell>𝒙 𝒏</cell><cell></cell><cell>𝜷</cell></row><row><cell>𝒙 𝒏</cell><cell>𝝉 𝒌𝒍</cell><cell></cell><cell>𝝊 𝒍</cell><cell>𝝉 𝒌𝒍</cell><cell>𝝊 𝒍</cell><cell></cell><cell></cell><cell></cell></row><row><cell>𝒚 𝒏</cell><cell>𝑁</cell><cell>𝑲 𝒍 𝓦</cell><cell>𝑳 − 𝟏</cell><cell>𝑲 𝒍 𝑳 − 𝟏</cell><cell></cell><cell>𝒚 𝒏</cell><cell>𝑁</cell><cell>𝓴</cell><cell>𝒃 𝒌 𝝆</cell></row><row><cell></cell><cell></cell><cell>𝓴</cell><cell>𝝆</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>𝒃 𝒌</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>As mentioned in the previous sections, BDL has been dealing with both epistemic and aleatoric uncertainties in predictions and successful in different domains such as climate Sampling from the posterior distribution was applied in the Bayesian approach that estimates uncertainty in a statistically principled way. Semi-supervised learning untangled regression and representation learning allowing the model to start AL from a small training data and keeping uncertainty estimates accurate in the low data limit. Their method highlighted the promise of BDL for chemistry. According to the literature, it is obvious that machine learning has the prospective to present valuable assistance in clinical decision making especially in the Intensive Care Unit (ICU) of a hospital. Traditional machine learning models do not take into account uncertainty in predictions. Ruhe et al.<ref type="bibr" target="#b462">[463]</ref> exhibited how the predictive uncertainty and Bayesian modelling could be utilized to recognize out-ofdomain examples and reduce the risk of faulty predictions. They utilized BNN to predict risk of mortality of ICU patients. Their empirical results show that uncertainty could detect out-of-domain patients and avert probable errors. Many machine learning techniques need human supervision to yield optimal performance. The quality of manual annotations essentially limited in tasks such as DensePose. Neverova et al.<ref type="bibr" target="#b463">[464]</ref> addressed the issue by augmenting neural network predictors with the ability to output a distribution over labels, thus introspectively and explicitly capturing the aleatoric uncertainty in the annotations. A new state-of-the-art accuracy in the benchmark could be achieved by understanding uncertainty better and hence solving the original DensePose task more accurately. The uncertainty estimates produced by multiple models could be used in fusing predictions in a better way to model ensembling that could enhance the accuracy further.</figDesc><table><row><cell>On the other hand, RNN language models (RNNLMs) have</cell><cell>change. Vandal et al. [452] devised a discrete-continuous</cell></row><row><cell>proved its superiority in several different tasks including</cell><cell>BDL technique with lognormal and Gaussian likelihoods</cell></row><row><cell>speech recognition. Learning appropriate representation of</cell><cell>for uncertainty quantification. They presented a super-</cell></row><row><cell>contexts for word prediction can be achieved through the</cell><cell>resolution based DL model dubbed as "DeepSD" for Statis-</cell></row><row><cell>hidden layers of RNNLMs. Fixed hidden vectors and deter-</cell><cell>tical Downscaling (SD) in climate utilized in precipitation</cell></row><row><cell>ministic model parameters in conventional RNNLMs have</cell><cell>that followed highly skewed distribution. Their discrete-</cell></row><row><cell>limitation in modelling the uncertainty over hidden repre-</cell><cell>continuous models performed superior to Gaussian distri-</cell></row><row><cell>sentations. Yu et al. [457] presented a comparative study</cell><cell>bution with respect to uncertainty calibration and predictive</cell></row><row><cell>of hidden and parametric representation uncertainty mod-</cell><cell>accuracy. As a fact, traditional ANNs lack the capability</cell></row><row><cell>elling techniques based variational RNNLMs and Bayesian</cell><cell>to model uncertainty and hence not suitable for long-term</cell></row><row><cell>gates respectively was examined on gated recurrent units</cell><cell>planning tasks. ANN long-term estimations are deviated</cell></row><row><cell>(GRU) and LSTM language models. Performance improve-</cell><cell>from the real behaviour of the system due to approxima-</cell></row><row><cell>ments were observed over conventional RNNLMs by their</cell><cell>tion errors and process noise. In another research, Nalis-</cell></row><row><cell>model in terms of word error rate and perplexity.</cell><cell>nick et al. [453] presented two structured priors-automatic</cell></row><row><cell>Predictive accuracy in black-box turbulence models is en-</cell><cell>depth determination (ADD) and joint automatic relevance</cell></row><row><cell>hanced by tuning Reynolds-Averaged Stokes (RANS) simu-</cell><cell>determination (ARD)-ADD-to permit Bayesian reasoning</cell></row><row><cell>lations and applying machine learning algorithms. Geneva</cell><cell>about a neural network's depth. The implementation led to</cell></row><row><cell>et al. [458] presented a new data-driven approach to pro-</cell><cell>runtime costs or little extra memory to BBB. Future work</cell></row><row><cell>vide probabilistic bounds for fluid quantities and enhanced</cell><cell>includes the use of structured variational approximations,</cell></row><row><cell>RANS predictions. The anisotropic tensor component of</cell><cell>comparison against other variational inference strategies</cell></row><row><cell>Reynolds stress was predicted by using an invariant BDNN.</cell><cell>and experiments on larger datasets. Dusenberry et al. [428]</cell></row><row><cell>Stein variational gradient decent algorithm was applied to</cell><cell>examined the role of model uncertainty techniques in the</cell></row><row><cell>train the model. Based on the proposed method, the as-</cell><cell>medical domain. They demonstrated that population-level</cell></row><row><cell>sociated probabilistic bounds and prediction enhancement</cell><cell>metrics, such as calibration error, log-likelihood, AUC-ROC</cell></row><row><cell>of the data-driven model were addressed. Following the</cell><cell>and AUC-PR did not capture model uncertainty by applying</cell></row><row><cell>research for dealing with uncertainty, we came across the</cell><cell>different BRNNs and RNN ensembles. The need for estimat-</cell></row><row><cell>study of Feng et al. [459] which proposed a novel extreme</cell><cell>ing model uncertainty was motivated by considerable vari-</cell></row><row><cell>learning machine (ELM) termed as rough ELM (RELM).</cell><cell>ability in patient-specific optimal decisions and predictions.</cell></row><row><cell>RELM utilized rough set to divide data into lower approx-</cell><cell>They further demonstrated that RNNs with only Bayesian</cell></row><row><cell>imation set and upper approximation set, and they were</cell><cell>embeddings yielded better results in model uncertainty</cell></row><row><cell>used to train lower approximation neurons and upper ap-</cell><cell>compared to ensembles.</cell></row><row><cell>proximation neurons. RELM showed a comparable accuracy</cell><cell>As we know, new challenges come up in prevailing pixel-</cell></row><row><cell>and repeatability in most classification tasks. In another</cell><cell>based prediction techniques with the advancement of re-</cell></row><row><cell>study, Walmsley et al. [460] applied Bayesian CNN and a</cell><cell>mote sensing imagery. Although deep learning methods</cell></row><row><cell>new generative model of Galaxy Zoo volunteer responses to</cell><cell>achieved a breakthrough in semantic segmentation of high-</cell></row><row><cell>infer posteriors for the visual morphology of galaxies. The</cell><cell>resolution images, most of the methods yielded predic-</cell></row><row><cell>probability of each possible label can be predicted by using</cell><cell>tions with poor boundaries. Bischke et al. [454] proposed</cell></row><row><cell>Bayesian CNN to learn from galaxy images with uncertain</cell><cell>a novel cascaded multi-task loss for preserving semantic</cell></row><row><cell>labels. Their posteriors were reliable for practical use as</cell><cell>segmentation boundaries in satellite imagery. Their method</cell></row><row><cell>they were well-calibrated. They utilized BALD AL strategy</cell><cell>outperformed the state-of-the-art techniques by 8.3% with-</cell></row><row><cell>applying their posteriors to request volunteer responses for</cell><cell>out an extra post-processing step. However, in autonomous</cell></row><row><cell>the subset of galaxies. They demonstrated that training their</cell><cell>driving, object detection plays a crucial role. Localizing the</cell></row><row><cell>Bayesian CNNs utilizing AL needed up to 35-60% fewer</cell><cell>objects and recognize objects perfectly is infeasible due to</cell></row><row><cell>labelled galaxies relying on the morphological features.</cell><cell>incomplete data and sensor noise. Hence, the uncertainty</cell></row><row><cell>The distribution of states at execution time may differ from</cell><cell>associated with the predictions should be computed by the</cell></row><row><cell>the distribution observed during training makes learning a</cell><cell>detector. Meyer et al. [455] devised a method that enhanced</cell></row><row><cell>policy utilizing only observational data a challenging task.</cell><cell>the learning of probability distribution by taking into ac-</cell></row><row><cell>Henaff et al. [461] introduced to train a policy by unrolling a</cell><cell>count potential noise in the ground-truth labeled data. Their</cell></row><row><cell>learned model of environment dynamics over multiple time</cell><cell>method enhanced not only the object detection performance</cell></row><row><cell>steps while explicitly penalizing two costs. The original cost</cell><cell>but also the accuracy of the learned distribution. RNNs have</cell></row><row><cell>the policy sought to optimize, and an uncertainty cost that</cell><cell>been applied to forecast increasingly complicated systems.</cell></row><row><cell>represented its divergence from the states it was trained on.</cell><cell>Although the RNN literature is highly developed and ex-</cell></row><row><cell>They examined their strategy utilizing huge observational</cell><cell>pansive, UQ is often not taken into account. If considered,</cell></row><row><cell>dataset of driving behaviour recorded from traffic cameras.</cell><cell>then also the uncertainty is usually quantified without the</cell></row><row><cell>In drug discovery, as another application of UQ methods, it</cell><cell>utilization of a rigorous approach. McDermott et al. [456]</cell></row><row><cell>is a challenge to predict physical properties and bioactivity</cell><cell>proposed a Bayesian RNN model for nonlinear spatio-</cell></row><row><cell>of small molecules. Zhang et al. [462] used Bayesian semi-</cell><cell>temporal forecasting while quantifying uncertainty in a</cell></row><row><cell>supervised graph convolutional neural networks to achieve</cell><cell>more formal framework. Unique nature of nonlinear spatio-</cell></row><row><cell>UQ and AL.</cell><cell>temporal data was accommodated by modifying the basic</cell></row><row><cell></cell><cell>RNN. They tested their model with two nonlinear spatio-</cell></row><row><cell></cell><cell>temporal forecasting frameworks and a Lorenz simulation.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>TABLE 6 :</head><label>6</label><figDesc>More information regarding additional UQ techniques proposed in the literature (Detailed information, sorted by year).</figDesc><table><row><cell>Study</cell><cell></cell><cell>Year Subject</cell><cell># datasets</cell><cell>Uncertainty method's name</cell><cell>Code</cell></row><row><cell>Balan et al. [246]</cell><cell></cell><cell>2015 Image processing,</cell><cell>4</cell><cell>PBE</cell><cell>×</cell></row><row><cell></cell><cell></cell><cell>toy and numerical</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Houthooft et al. [247]</cell><cell>data and 2016 Toy data (regres-</cell><cell>1</cell><cell>VIME</cell><cell>√</cell></row><row><cell>Springenberg</cell><cell>et</cell><cell>sion) 2016 Numerical data (re-</cell><cell>4</cell><cell>BNN</cell><cell>√</cell></row><row><cell>al. [248] Zhang et al. [253] Khan et al. [155] Malinin et al. [326]</cell><cell></cell><cell>gression) 2017 Image processing 2018 Numerical data 2018 Image processing</cell><cell>7 8 2</cell><cell>UCF (uncertain convolutional features) Vadam PN</cell><cell>√ √ √</cell></row><row><cell cols="2">Ilg et al. [249] Heo et al. [250] Sensoy et al. [254] Prokudin et al. [305] Smith et al. [313]</cell><cell>2018 Computer vision 2018 Medical signal 2018 Image processing 2018 Image processing 2018 Image processing</cell><cell>1 3 2 3 2</cell><cell>FlowNetH-Pred-Merged UA EDL CVAE MI</cell><cell>× √ √ √ √</cell></row><row><cell>Qian et al. [324]</cell><cell></cell><cell>2018 Image processing</cell><cell>4</cell><cell>MaPML</cell><cell>×</cell></row><row><cell>Dunlop et al. [327]</cell><cell></cell><cell>2018 Synthetic data</cell><cell>2</cell><cell>DGP</cell><cell>×</cell></row><row><cell cols="2">Manders et al. [341] Lee et al. [360] Acerbi [255]</cell><cell>2018 Image processing 2018 Image processing 2018 N/A</cell><cell>2 2 2</cell><cell>CPUA GP VBMC</cell><cell>× √ √</cell></row><row><cell>Zhang et al. [524]</cell><cell></cell><cell>2018 Numerical data</cell><cell>10</cell><cell>S 2 V GD: Structural Stein Variational</cell><cell>×</cell></row><row><cell>Gong et al. [72] Sun et al. [294]</cell><cell></cell><cell>2019 Numerical data 2019 Numerical data</cell><cell>5 10</cell><cell>Gradient Descent Icebreaker functional BNNs(fBNNs)</cell><cell>√ √</cell></row><row><cell>Vadera</cell><cell>and</cell><cell>2019 Image processing</cell><cell>1</cell><cell>BDk</cell><cell>×</cell></row><row><cell cols="3">Marlin [292] Patacchiola et al. [295] Cheng et al. [296] Ravi and Beatson [361] 2019 Image processing 2019 Image processing 2019 Image processing Hendrycks et al. [243] 2019 Image processing</cell><cell>2 2 2 2</cell><cell>GP DIP: deep image prior AVI Self-supervision</cell><cell>√ √ √ √</cell></row><row><cell>Ilg et al. [249]</cell><cell></cell><cell>2019 Computer vision</cell><cell>1</cell><cell>SGDR (Stochastic Gradient Descent with</cell><cell>×</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>warm Restarts) and Bootstrapped en-</cell><cell></cell></row><row><cell cols="2">Ahn et al. [252] Haußmann et al. [256]</cell><cell>2019 Image processing 2019 Image processing</cell><cell>2 2</cell><cell>sembles UCL BEDL (Bayesian Evidential DL) + Reg</cell><cell>√ √</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(Regularization)</cell><cell></cell></row><row><cell>Foong et al. [258]</cell><cell></cell><cell>2019 Numerical data</cell><cell>9</cell><cell>Laplace approximation</cell><cell>×</cell></row><row><cell cols="2">Abdolshah et al. [287] White et al. [288] Balandat et al. [289] Galy-Fajou et al. [290] Lee et al. [291] Schwab et al. [312]</cell><cell>2019 Image processing 2019 Image processing 2019 Geographical data 2019 Toy datasets 2019 Image processing 2019 Image processing</cell><cell>1 1 1 7 4 2</cell><cell>MOBO BO BOTORCH CMGGPC BTAML CXPlain</cell><cell>× √ √ √ √ √</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Continued on next page</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>TABLE 6 -</head><label>6</label><figDesc>Continued from previous page</figDesc><table><row><cell>Study</cell><cell></cell><cell>Year Subject</cell><cell></cell><cell># datasets</cell><cell>Uncertainty method's name</cell><cell>Code</cell></row><row><cell>Malinin</cell><cell>and</cell><cell cols="2">2019 Image processing</cell><cell>5</cell><cell>N/A</cell><cell>√</cell></row><row><cell>Gales [314]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Wu et al. [323]</cell><cell></cell><cell cols="2">2019 Medical image pro-</cell><cell>1</cell><cell>DDMN: deep Dirichlet mixture net-</cell><cell>×</cell></row><row><cell cols="2">Gomez et al. [325] Northcutt et al. [340] Ovadia et al. [346]</cell><cell cols="2">cessing 2019 Image processing 2019 Image processing 2019 Image, text and cat-</cell><cell>3 2 3</cell><cell>works Targeted dropout CL Data shift</cell><cell>√ √ √</cell></row><row><cell cols="2">Biloš et al. [347] Zheng and Yang [348] Yildiz et al. [358] Wang et al. [363]</cell><cell cols="2">egorical data 2019 Toy data 2019 Image processing 2019 Image processing 2019 Time series</cell><cell>2 2 3 1</cell><cell>FD-Dir and WGP-LN MR ODE 2 VAE NLE loss</cell><cell>√ √ √ √</cell></row><row><cell>Tai et al. [364]</cell><cell></cell><cell cols="2">2019 Computer vision</cell><cell>1</cell><cell>UIA</cell><cell>×</cell></row><row><cell>De Ath et al. [257]</cell><cell></cell><cell cols="2">2020 Synthetic data</cell><cell>10</cell><cell>∈-shotgun</cell><cell>×</cell></row><row><cell>Foong et al. [301] Yao et al. [302]</cell><cell></cell><cell cols="2">2020 Image processing 2020 Image processing</cell><cell>5 3</cell><cell>ConvNP SI</cell><cell>× √</cell></row><row><cell></cell><cell></cell><cell>and</cell><cell>Numerical</cell><cell></cell><cell></cell></row><row><cell>Prijatelj et al. [303]</cell><cell></cell><cell cols="2">data 2020 Image processing</cell><cell>4</cell><cell>Bayesian evaluation</cell><cell>√</cell></row><row><cell>Herzog et al. [304]</cell><cell></cell><cell>2020 Medical</cell><cell>image</cell><cell>1</cell><cell>Bayesian aggregation</cell><cell>×</cell></row><row><cell></cell><cell></cell><cell>analysis</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Tuo and Wang [306] Acerbi [307]</cell><cell>2020 N/A 2020 N/A</cell><cell></cell><cell>N/A 5</cell><cell>BO VBMC+EIG/VIQR</cell><cell>× √</cell></row><row><cell>Zhao et al. [308]</cell><cell></cell><cell cols="2">2020 Numerical data</cell><cell>5</cell><cell>GEP</cell><cell>×</cell></row><row><cell>Li et al. [309]</cell><cell></cell><cell>2020 Care</cell><cell>(network)</cell><cell>1</cell><cell>DBGP</cell><cell>×</cell></row><row><cell>He et al. [310]</cell><cell></cell><cell cols="2">data 2020 Image processing</cell><cell>3</cell><cell>NTK</cell><cell>√</cell></row><row><cell></cell><cell></cell><cell cols="2">and toy data</cell><cell></cell><cell></cell></row><row><cell>Salem et al. [320]</cell><cell></cell><cell cols="2">2020 Numerical data</cell><cell>10</cell><cell>SNM-QD+: split normal mixture-</cell><cell>×</cell></row><row><cell cols="2">Hendrycks et al. [330]</cell><cell cols="2">2020 Image processing</cell><cell>3</cell><cell>quality-driven loss AugMix</cell><cell>√</cell></row><row><cell>Boiarov et al. [332]</cell><cell></cell><cell>2020 NLP</cell><cell></cell><cell>1</cell><cell>SPSA</cell><cell>×</cell></row><row><cell>Chun et al. [342]</cell><cell></cell><cell cols="2">2020 Image processing</cell><cell>7</cell><cell>Regularization techniques</cell><cell>×</cell></row><row><cell>Wang et al. [354]</cell><cell></cell><cell cols="2">2020 Image processing</cell><cell>5</cell><cell>TransCal</cell><cell>×</cell></row><row><cell>Lu et al. [362] Selvan et al. [365]</cell><cell></cell><cell cols="2">2020 Numerical data 2020 Medical image</cell><cell>2 2</cell><cell>DGPM cFlow</cell><cell>× √</cell></row><row><cell>Poggi et al. [366]</cell><cell></cell><cell cols="2">analysis 2020 Computer vision</cell><cell>1</cell><cell>Self-Teaching</cell><cell>√</cell></row><row><cell>Cui et al. [367]</cell><cell></cell><cell cols="2">2020 Time series</cell><cell>5</cell><cell>MMD</cell><cell>×</cell></row></table><note>Note: UA: Uncertainty-aware attention, EDL: Evidential Deep Learning, Vadam: Variational Adam, MaPML: Margin Preserving Metric Learning50</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>TABLE 7 :</head><label>7</label><figDesc>A summary of various UQ methods applied in computer vision and image processing tasks (sorted by year).</figDesc><table><row><cell>Study</cell><cell></cell><cell cols="8">Year Data source Application # Images/video # Classes Classifier</cell><cell>UQ method</cell><cell>Code</cell></row><row><cell cols="2">Kendall et</cell><cell cols="2">2016 aCambridge</cell><cell>Camera Re-</cell><cell cols="2">covering</cell><cell>a</cell><cell>4 classes,</cell><cell>Bayesian</cell><cell>Averaging</cell><cell>√</cell></row><row><cell>al. [397]</cell><cell></cell><cell cols="2">Landmarks</cell><cell>localization</cell><cell cols="3">ground area of</cell><cell>7 classes</cell><cell>CNN</cell><cell>MC dropout</cell></row><row><cell></cell><cell></cell><cell cols="2">dataset</cell><cell></cell><cell cols="3">up to 50,000</cell><cell></cell><cell>samples</cell></row><row><cell></cell><cell></cell><cell cols="2">(outdoor),</cell><cell></cell><cell cols="3">m2 including</cell><cell></cell><cell>obtained from</cell></row><row><cell></cell><cell></cell><cell>7</cell><cell>Scenes</cell><cell></cell><cell cols="2">1920×1080</cell><cell></cell><cell></cell><cell>the posterior</cell></row><row><cell></cell><cell></cell><cell cols="2">dataset</cell><cell></cell><cell cols="2">images,</cell><cell></cell><cell></cell><cell>Bernoulli</cell></row><row><cell></cell><cell></cell><cell cols="2">(indoor)</cell><cell></cell><cell cols="2">640×480</cell><cell></cell><cell></cell><cell>distribution of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>images</cell><cell></cell><cell></cell><cell></cell><cell>the Bayesian</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CNN's</cell></row><row><cell cols="2">Kendall et</cell><cell cols="2">2016 SUN RGB-</cell><cell>Scene un-</cell><cell>SUN</cell><cell cols="2">RGB-D:</cell><cell>CamVid:</cell><cell>Bayesian</cell><cell>weights MC sampling</cell><cell>√</cell></row><row><cell>al. [43]</cell><cell></cell><cell cols="2">D (Indoor),</cell><cell>derstanding</cell><cell>5285</cell><cell cols="2">training</cell><cell>11</cell><cell>SegNet</cell><cell>with dropout</cell></row><row><cell></cell><cell></cell><cell cols="2">CamVid</cell><cell></cell><cell>and</cell><cell></cell><cell>5050</cell><cell>classes,</cell></row><row><cell></cell><cell></cell><cell cols="2">(Outdoor)</cell><cell></cell><cell cols="3">testing images,</cell><cell>SUN</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">while images</cell><cell>RGB-D:</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>were</cell><cell cols="2">resized</cell><cell>37 classes</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>to</cell><cell cols="2">224x224,</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CamVid:</cell><cell>367</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">training images</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">and 233 testing</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>images</cell><cell></cell><cell>of</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">day and dusk</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>scenes,</cell><cell cols="2">while</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>images</cell><cell></cell><cell>were</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>resized</cell><cell></cell><cell>to</cell><cell></cell></row><row><cell cols="2">Kendall et</cell><cell cols="2">2017 CamVid</cell><cell>Semantic</cell><cell cols="3">360x480 600 and 1449,</cell><cell cols="2">11 and 40 BDL</cell><cell>MC dropout</cell><cell>√</cell></row><row><cell>al. [394]</cell><cell></cell><cell cols="2">and NYU</cell><cell>segmenta-</cell><cell>534</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Gal</cell><cell>et</cell><cell cols="2">v2, Make3D 2017 Synthetic</cell><cell>tion RL</cell><cell>N/A</cell><cell></cell><cell></cell><cell>N/A</cell><cell>NNs</cell><cell>Continuous</cell><cell>√</cell></row><row><cell>al. [465]</cell><cell></cell><cell cols="2">dataset,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>relaxation of</cell></row><row><cell></cell><cell></cell><cell>UCI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>dropout's</cell></row><row><cell></cell><cell></cell><cell cols="2">datasets,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>discrete</cell></row><row><cell></cell><cell></cell><cell cols="2">MNIST</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>masks, using</cell></row><row><cell>Ilg</cell><cell>et</cell><cell cols="2">2018 Sintel train</cell><cell>Optical</cell><cell>N/A</cell><cell></cell><cell></cell><cell>N/A</cell><cell>Multi-</cell><cell>BDL Uncertainty</cell><cell>√</cell></row><row><cell>al. [249]</cell><cell></cell><cell cols="2">clean, Sintel</cell><cell>flow</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>headed</cell><cell>estimates</cell></row><row><cell></cell><cell></cell><cell cols="2">train final,</cell><cell>estimation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>network</cell><cell>efficiently</cell><cell>a</cell></row><row><cell></cell><cell></cell><cell cols="2">KITTI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>architecture</cell><cell>single forward</cell></row><row><cell></cell><cell></cell><cell cols="2">2012+2015,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>that yields</cell><cell>pass</cell><cell>and</cell></row><row><cell></cell><cell></cell><cell cols="2">FlyingTh-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>multiple</cell><cell>without</cell><cell>the</cell></row><row><cell></cell><cell></cell><cell cols="2">ings3D</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>hypotheses</cell><cell>need</cell><cell>for</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>in a single</cell><cell>sampling or</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>network</cell><cell>ensembles</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>without</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>the need of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>sampling</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Continued on next page</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>TABLE 7 -</head><label>7</label><figDesc>Continued from previous page</figDesc><table><row><cell>Study</cell><cell></cell><cell cols="8">Year Data source Application # Images/video # Classes Classifier</cell><cell>UQ method</cell><cell>Code</cell></row><row><cell cols="2">Bhattacharyya</cell><cell>2018 Cityscapes</cell><cell cols="2">Long-term</cell><cell cols="3">2975 training,</cell><cell>20</cell><cell>RNN</cell><cell>MC</cell><cell>and</cell><cell>×</cell></row><row><cell cols="2">et al. [420]</cell><cell>dataset</cell><cell cols="2">on-board</cell><cell cols="3">500 validation</cell><cell></cell><cell>encoder-</cell><cell>minimizing</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">prediction</cell><cell>and</cell><cell></cell><cell>1525</cell><cell></cell><cell>decoder</cell><cell>the</cell><cell>KL</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Of people</cell><cell>test</cell><cell cols="2">video</cell><cell></cell><cell>+</cell><cell>CNN</cell><cell>divergence</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">sequences</cell><cell>of</cell><cell></cell><cell>+</cell><cell>LSTM-</cell><cell>of</cell><cell>its</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>length</cell><cell></cell><cell>1.8</cell><cell></cell><cell>Bayesian</cell><cell>approximate</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">seconds</cell><cell>(30</cell><cell></cell><cell>weight</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">frames) having</cell><cell></cell><cell>distribution</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">resolution</cell><cell>of</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">2048×1024</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>pixels</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Gast</cell><cell>and</cell><cell>2018 FlyingChairs,</cell><cell cols="2">Uncertainty</cell><cell>N/A</cell><cell></cell><cell></cell><cell>N/A</cell><cell>Lightweight</cell><cell>Apply</cell><cell>×</cell></row><row><cell cols="2">Roth [410]</cell><cell>Sintel,</cell><cell cols="2">prediction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>probabilis-</cell><cell>uncertainty</cell></row><row><cell></cell><cell></cell><cell>CIFAR10</cell><cell cols="2">on CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>tic</cell><cell>CNNs</cell><cell>propagating</cell></row><row><cell></cell><cell></cell><cell>and MNIST</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(FlowNe-</cell><cell>layers using</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>tADF and</cell><cell>Gaussians</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FlowNet-</cell><cell>(Building</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ProbOut</cell><cell>upon standard</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>and</cell><cell>maximum</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ProbOut)</cell><cell>conditional</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>likelihood</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>learning while</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>concentrating</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>on probabilis-</cell></row><row><cell cols="2">Lambert et</cell><cell>2018 ImageNet,</cell><cell>Image</cell><cell></cell><cell>N/A</cell><cell></cell><cell></cell><cell>30</cell><cell>tic outputs) CNN+LSTM Proposed</cell><cell>√</cell></row><row><cell>al. [525]</cell><cell></cell><cell>Multi-30K</cell><cell cols="2">classifica-</cell><cell></cell><cell></cell><cell></cell><cell>thousand</cell><cell>LUPI</cell></row><row><cell></cell><cell></cell><cell></cell><cell>tion</cell><cell>and</cell><cell></cell><cell></cell><cell></cell><cell>Flickr</cell><cell>(Learning</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">machine</cell><cell></cell><cell></cell><cell></cell><cell>images</cell><cell>Under</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">translation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Privileged</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Information)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>which makes</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>variance</cell><cell>of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>a function of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>the privileged</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>information.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Then</cell><cell>using</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>the privileged</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>information in</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>heteroscedas-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>tic</cell><cell>dropout</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>to</cell><cell>estimate</cell></row><row><cell cols="2">Kendall et</cell><cell>2018 CityScapes</cell><cell>Scene</cell><cell></cell><cell cols="3">2,975 training</cell><cell>20</cell><cell>Deep con-</cell><cell>uncertainty A principled</cell><cell>√</cell></row><row><cell>al. [526]</cell><cell></cell><cell></cell><cell cols="2">geometry</cell><cell>and</cell><cell></cell><cell>500</cell><cell></cell><cell>volutional</cell><cell>loss function</cell></row><row><cell></cell><cell></cell><cell></cell><cell>and</cell><cell></cell><cell cols="2">validation</cell><cell></cell><cell></cell><cell>encoder</cell><cell>which</cell><cell>can</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">semantics</cell><cell cols="2">images</cell><cell>at</cell><cell></cell><cell>followed</cell><cell>learn a relative</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">2048×1024</cell><cell></cell><cell></cell><cell>by</cell><cell>con-</cell><cell>weighting</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">resolution.</cell><cell></cell><cell></cell><cell>volutional</cell><cell>automatically</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1,525</cell><cell cols="2">images</cell><cell></cell><cell>decoders</cell><cell>from the data</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>are</cell><cell cols="2">withheld</cell><cell></cell><cell>and is robust</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>for</cell><cell cols="2">testing</cell><cell></cell><cell>to the weight</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">on an online</cell><cell></cell><cell>initialization</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">evaluation</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>server</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Continued on next page</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>TABLE 7 -</head><label>7</label><figDesc>Continued from previous page</figDesc><table><row><cell>Study</cell><cell></cell><cell cols="8">Year Data source Application # Images/video # Classes Classifier</cell><cell cols="2">UQ method</cell><cell>Code</cell></row><row><cell cols="2">Mukhoti et</cell><cell>2019 Cityscapes</cell><cell>Semantic</cell><cell cols="3">2975 imagesfor</cell><cell>50</cell><cell cols="2">Bayesian</cell><cell cols="2">MC dropout</cell><cell>×</cell></row><row><cell>al. [10]</cell><cell></cell><cell>dataset</cell><cell>segmenta-</cell><cell cols="2">training,</cell><cell>500</cell><cell></cell><cell cols="2">DeepLab</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>tion</cell><cell cols="2">images</cell><cell>for</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">validation, 1525</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">images</cell><cell>for</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Harakeh et</cell><cell>2019 Berkley</cell><cell>Object</cell><cell cols="2">testing BDD:</cell><cell>80K</cell><cell>BDD and</cell><cell>DNN</cell><cell></cell><cell cols="2">BayesOD</cell><cell>√</cell></row><row><cell>al. [403]</cell><cell></cell><cell>Deep Drive</cell><cell>detection</cell><cell cols="2">frames</cell><cell></cell><cell>KITTI: 7</cell><cell></cell><cell></cell><cell cols="2">(Bayesian-</cell></row><row><cell></cell><cell></cell><cell>(BDD) 100K</cell><cell></cell><cell cols="3">(70K/10K train-</cell><cell>common</cell><cell></cell><cell></cell><cell cols="2">based object</cell></row><row><cell></cell><cell></cell><cell>Dataset</cell><cell></cell><cell cols="3">ing/validation),</cell><cell>road</cell><cell></cell><cell></cell><cell cols="2">detectors)</cell></row><row><cell></cell><cell></cell><cell>(BDD),</cell><cell></cell><cell cols="2">KITTI:</cell><cell>7;</cell><cell>scene,</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>KITTI, MS</cell><cell></cell><cell>481</cell><cell cols="2">frames,</cell><cell>MS</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>COCO,</cell><cell></cell><cell>MS</cell><cell cols="2">COCO:</cell><cell>COCO:</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Pascal VOC</cell><cell></cell><cell>223K</cell><cell cols="2">frames</cell><cell>81, Pascal</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(118K/5K train-</cell><cell>VOC: 20</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">ing/testing),</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Pascal</cell><cell>VOC:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>5823</cell><cell cols="2">frames</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>He</cell><cell>et</cell><cell>2019 MS-COCO,</cell><cell>Object</cell><cell cols="3">(testing) For PASCAL</cell><cell>N/A</cell><cell>Faster</cell><cell>R-</cell><cell>A</cell><cell>new</cell><cell>√</cell></row><row><cell>al. [405]</cell><cell></cell><cell>PASCAL</cell><cell>detection</cell><cell>VOC:</cell><cell cols="2">20075k</cell><cell></cell><cell cols="2">CNN+FPN</cell><cell cols="2">bounding</cell></row><row><cell></cell><cell></cell><cell>VOC 2007</cell><cell></cell><cell cols="3">voc 2007 trainval</cell><cell></cell><cell cols="2">(Feature</cell><cell cols="2">box regression</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">images and 5k</cell><cell></cell><cell cols="2">Pyramid</cell><cell cols="2">loss (modeling</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">voc 2007 test</cell><cell></cell><cell cols="2">Network)+</cell><cell cols="2">bounding box</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">images</cell><cell></cell><cell></cell><cell cols="2">Mask-R-</cell><cell cols="2">predictions</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CNN</cell><cell></cell><cell cols="2">as well as</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ground-truth</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">bounding</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>boxes</cell><cell>as</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Gaussian</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">distribution</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>and</cell><cell>Dirac</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">delta function,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">respectively)</cell></row><row><cell>Liu</cell><cell>et</cell><cell>2019 Multiple</cell><cell>Depth</cell><cell>N/A</cell><cell></cell><cell></cell><cell>N/A</cell><cell cols="2">D-Net</cell><cell>BDL</cell><cell>×</cell></row><row><cell>al. [402]</cell><cell></cell><cell>indoor and</cell><cell>estimation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(CNNbased)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>outdoor</cell><cell>for 3D scene</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+</cell><cell>K-net</cell><cell></cell></row><row><cell></cell><cell></cell><cell>datasets</cell><cell>reconstruc-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(Kalman</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>tion</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">filter) + R-</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Net (based</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>on</cell><cell>U-Net</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>with</cell><cell>skip</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">connec-</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>tions)</cell><cell></cell><cell></cell></row><row><cell cols="2">Abbasnejad</cell><cell>2019 GuessWhat</cell><cell>Asking</cell><cell cols="2">155,281</cell><cell></cell><cell>N/A</cell><cell cols="4">RNN+LSTM Bayesian-</cell><cell>×</cell></row><row><cell>et al. [529]</cell><cell></cell><cell>dataset</cell><cell>goal-</cell><cell cols="2">dialogues</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">based model</cell></row><row><cell></cell><cell></cell><cell></cell><cell>oriented</cell><cell cols="2">related</cell><cell>to</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>questions</cell><cell cols="3">821, 955 ques-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">tion/answer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>pairs</cell><cell></cell><cell>with</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">vocabulary size</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">of 11,465 on</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">66,537 images</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>as</cell><cell>well</cell><cell>as</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">134,074 objects</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Continued on next page</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>TABLE 7 -</head><label>7</label><figDesc>Continued from previous page</figDesc><table><row><cell>Study</cell><cell></cell><cell cols="8">Year Data source Application # Images/video # Classes Classifier</cell><cell>UQ method</cell><cell>Code</cell></row><row><cell cols="2">Peterson et</cell><cell cols="2">2019 CIFAR10H</cell><cell cols="2">Improving</cell><cell cols="2">10,000 images,</cell><cell>N/A</cell><cell>CNN</cell><cell>Increasing</cell><cell>√</cell></row><row><cell>al. [417]</cell><cell></cell><cell>(for</cell><cell></cell><cell cols="2">robustness</cell><cell>50,000,</cell><cell>2,000,</cell><cell></cell><cell>distributional</cell></row><row><cell></cell><cell></cell><cell>training),</cell><cell></cell><cell>of</cell><cell></cell><cell cols="2">210,000, N/A</cell><cell></cell><cell>shift</cell></row><row><cell></cell><cell></cell><cell cols="2">CIFAR10,</cell><cell cols="2">adversarial</cell><cell>images</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">IFAR10.1v6,v4,</cell><cell cols="2">attacks</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">CINIC10,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">ImageNet-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Far</cell><cell>for</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Bertoni et</cell><cell>testing 2019 KITTI,</cell><cell></cell><cell>3d</cell><cell></cell><cell cols="2">7,481 training</cell><cell>N/A</cell><cell>DNN (2D</cell><cell>MC dropout</cell><cell>√</cell></row><row><cell>al. [413]</cell><cell></cell><cell>nuScenes</cell><cell></cell><cell cols="2">pedestrian</cell><cell cols="2">images, N/A</cell><cell></cell><cell>joints)</cell><cell>+</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">localization</cell><cell></cell><cell></cell><cell></cell><cell>a</cell><cell>light-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>weight</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>feed-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>forward</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>network</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(the</cell><cell>3D</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>location)</cell></row><row><cell>Asai</cell><cell>et</cell><cell cols="2">2019 NYU depth</cell><cell cols="2">Depth esti-</cell><cell>N/A</cell><cell></cell><cell>N/A</cell><cell>CNN Pixel-</cell><cell>Formulating</cell><cell>×</cell></row><row><cell>al. [401]</cell><cell></cell><cell cols="2">dataset V2</cell><cell cols="2">mation</cell><cell></cell><cell></cell><cell></cell><cell>wise regres-</cell><cell>regression</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>sion</cell><cell>with</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>estimation</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>of uncertainty</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>as MTL</cell></row><row><cell>Loquercio</cell><cell></cell><cell>2019 Udacity</cell><cell></cell><cell cols="2">End-to-end</cell><cell>N/A</cell><cell></cell><cell>N/A</cell><cell>DNN</cell><cell>Bayesian</cell><cell>×</cell></row><row><cell>et al. [411]</cell><cell></cell><cell>dataset</cell><cell></cell><cell cols="2">steering</cell><cell></cell><cell></cell><cell></cell><cell>inference MC</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">angle</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">prediction,</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">obstacle</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">motion</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">prediction</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">and closed-</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>loop</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">control of a</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Quadrotor</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Martinez, et</cell><cell cols="2">2019 CT scans of</cell><cell cols="2">Automatically</cell><cell cols="2">They divided</cell><cell>2</cell><cell>CNN (VNet</cell><cell>MC dropout</cell><cell>×</cell></row><row><cell>al. [408]</cell><cell></cell><cell cols="2">woven com-</cell><cell cols="2">segmenting</cell><cell>entire</cell><cell></cell><cell></cell><cell>3D)</cell><cell>both during</cell></row><row><cell></cell><cell></cell><cell cols="2">posite mate-</cell><cell>a</cell><cell>diverse</cell><cell cols="2">1001x1150x1150</cell><cell></cell><cell>training and</cell></row><row><cell></cell><cell></cell><cell>rials</cell><cell></cell><cell>set</cell><cell>of</cell><cell>volume</cell><cell>into</cell><cell></cell><cell>inference</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Volumetric</cell><cell cols="2">a set of 48</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CT</cell><cell>scans</cell><cell cols="2">sub-volume for</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>of</cell><cell>woven</cell><cell>training</cell><cell>and</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">composite</cell><cell cols="2">sets of 8 for</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">materials</cell><cell cols="2">both validation</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>and</cell><cell>testing</cell><cell></cell></row><row><cell>Postels</cell><cell>et</cell><cell>2019 CamVid</cell><cell></cell><cell cols="2">Semantic</cell><cell>steps N/A</cell><cell></cell><cell>11 out of</cell><cell>CNN</cell><cell>Sampling free</cell><cell>√</cell></row><row><cell>al. [409]</cell><cell></cell><cell></cell><cell></cell><cell cols="2">segmenta-</cell><cell></cell><cell></cell><cell>32</cell><cell>noise injection</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>tion</cell><cell>and</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">depth</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">regression</cell><cell></cell><cell></cell><cell></cell></row><row><cell>He</cell><cell>et</cell><cell>2019 Human</cell><cell></cell><cell cols="2">Human</cell><cell>N/A</cell><cell></cell><cell>N/A</cell><cell>CNN</cell><cell>Multivariate</cell><cell>×</cell></row><row><cell>al. [412]</cell><cell></cell><cell cols="2">3.6M, MPII</cell><cell>pose</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Gaussian</cell></row><row><cell></cell><cell></cell><cell cols="2">validation</cell><cell cols="2">estimation</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Continued on next page</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>TABLE 7 -</head><label>7</label><figDesc>Continued from previous page</figDesc><table><row><cell>Study</cell><cell></cell><cell cols="6">Year Data source Application # Images/video # Classes Classifier</cell><cell cols="2">UQ method</cell><cell>Code</cell></row><row><cell cols="2">Peretroukhin</cell><cell>2019 Synthetic</cell><cell>Probabilistic</cell><cell>DNN</cell><cell>called</cell><cell>N/A</cell><cell>N/A</cell><cell cols="2">Regression</cell><cell>×</cell></row><row><cell>et al. [530]</cell><cell></cell><cell>dataset,</cell><cell>regression</cell><cell cols="2">HydraNet</cell><cell></cell><cell></cell><cell>units</cell></row><row><cell></cell><cell></cell><cell>7-Scenes,</cell><cell>of elements</cell><cell>(extened</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>KITTI</cell><cell>of SO(3)</cell><cell>version</cell><cell>of</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">multi-headed</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Yu</cell><cell>et</cell><cell>2019 Market-</cell><cell>Person re-</cell><cell cols="2">networks) N/A</cell><cell>2</cell><cell>Distribution</cell><cell cols="2">Gaussian dis-</cell><cell>√</cell></row><row><cell>al. [415]</cell><cell></cell><cell>1501,</cell><cell>identification</cell><cell></cell><cell></cell><cell></cell><cell>Net (CNN,</cell><cell cols="2">tribution</cell></row><row><cell></cell><cell></cell><cell>DukeMTMC-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>with</cell><cell></cell></row><row><cell></cell><cell></cell><cell>ReID,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>random</cell><cell></cell></row><row><cell></cell><cell></cell><cell>CUHK01,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>feature</cell><cell></cell></row><row><cell>Zhang</cell><cell>et</cell><cell>CUHK03 2019 Train400,</cell><cell>Image</cell><cell>N/A</cell><cell></cell><cell>N/A</cell><cell>vectors) PRL (Prob-</cell><cell cols="2">UQ (aleatoric)</cell><cell>√</cell></row><row><cell>al. [531]</cell><cell></cell><cell>Test12,</cell><cell>restoration</cell><cell></cell><cell></cell><cell></cell><cell>abilistic</cell><cell>with</cell><cell>CVAE</cell></row><row><cell></cell><cell></cell><cell>Berkeley</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Residual</cell><cell cols="2">(Conditional</cell></row><row><cell></cell><cell></cell><cell>segmenta-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Learning)</cell><cell cols="2">Variational</cell></row><row><cell></cell><cell></cell><cell>tion dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Auto-</cell></row><row><cell></cell><cell></cell><cell>(BSD68)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">encoders)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>loss</cell></row><row><cell cols="2">Carbone et</cell><cell>2020 MNIST and</cell><cell>Gradient-</cell><cell cols="2">1000 test im-</cell><cell>N/A</cell><cell>BNNs</cell><cell cols="2">HMC and and</cell><cell>×</cell></row><row><cell>al. [418]</cell><cell></cell><cell>Fashion</cell><cell>based</cell><cell cols="2">ages from both</cell><cell></cell><cell></cell><cell cols="2">VI support</cell></row><row><cell></cell><cell></cell><cell>MNIST</cell><cell>adversarial</cell><cell>datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Harris</cell><cell>et</cell><cell>2020 CIFAR-10,</cell><cell>attacks Image,</cell><cell>N/A</cell><cell></cell><cell>N/A</cell><cell>MixUp,</cell><cell>Relative</cell><cell>√</cell></row><row><cell>al. [532]</cell><cell></cell><cell>Fashion</cell><cell>audio, text</cell><cell></cell><cell></cell><cell></cell><cell>FMix,</cell><cell>entropy</cell></row><row><cell></cell><cell></cell><cell>MNIST,</cell><cell>and point</cell><cell></cell><cell></cell><cell></cell><cell>FastText-</cell><cell cols="2">objective</cell></row><row><cell></cell><cell></cell><cell>ImageNet</cell><cell>cloud clas-</cell><cell></cell><cell></cell><cell></cell><cell>300d, CNN,</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>sification</cell><cell></cell><cell></cell><cell></cell><cell>FastText, bi-</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>(mixed</cell><cell></cell><cell></cell><cell></cell><cell>directional</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>sample data</cell><cell></cell><cell></cell><cell></cell><cell>LSTM,</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>augmenta-</cell><cell></cell><cell></cell><cell></cell><cell>BERT</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>tion)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Miolane</cell><cell></cell><cell>2020 Synthetic</cell><cell>Representation</cell><cell>N/A</cell><cell></cell><cell>N/A</cell><cell>VAE</cell><cell cols="2">Riemannian</cell><cell>×</cell></row><row><cell>and</cell><cell></cell><cell>datasets</cell><cell>learning</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>VAE</cell></row><row><cell cols="2">Holmes [533]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Zhou</cell><cell>et</cell><cell>2020 Synthetic</cell><cell>Cross-</cell><cell>N/A</cell><cell></cell><cell>N/A</cell><cell>Teacher-</cell><cell>UA</cell><cell>×</cell></row><row><cell>al. [534]</cell><cell></cell><cell>and GTAV,</cell><cell>domain</cell><cell></cell><cell></cell><cell></cell><cell>student</cell><cell cols="2">consistency</cell></row><row><cell></cell><cell></cell><cell>SYNTHA</cell><cell>semantic</cell><cell></cell><cell></cell><cell></cell><cell>network</cell><cell cols="2">regularization</cell></row><row><cell></cell><cell></cell><cell>and</cell><cell>segmenta-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>and</cell></row><row><cell></cell><cell></cell><cell>Cityscapes</cell><cell>tion</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">uncertainty-</cell></row><row><cell></cell><cell></cell><cell>datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>guided</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">consistency</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>loss</cell></row><row><cell>Zhang</cell><cell>et</cell><cell>2020 Six</cell><cell>RGB-D</cell><cell>N/A</cell><cell></cell><cell>N/A</cell><cell>Conditional</cell><cell>UC-Net</cell><cell>×</cell></row><row><cell>al. [535]</cell><cell></cell><cell>challenging</cell><cell>saliency</cell><cell></cell><cell></cell><cell></cell><cell>VAE</cell><cell></cell></row><row><cell></cell><cell></cell><cell>benchmark</cell><cell>detection</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>TABLE 8 :</head><label>8</label><figDesc>A summary of various UQ methods applied in medical application tasks (sorted by year).</figDesc><table><row><cell>Study</cell><cell></cell><cell>Year Classifier</cell><cell cols="2">Application Disease/cancer</cell><cell>#</cell><cell>UQ method</cell><cell></cell><cell></cell><cell>Code</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>datasets</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Leibig</cell><cell>et</cell><cell>2017 CNN</cell><cell cols="2">Classification Diabetic Retinopathy</cell><cell>3</cell><cell>MC dropout</cell><cell></cell><cell></cell><cell>×</cell></row><row><cell>al. [45]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Jungo</cell><cell>et</cell><cell>2017 CNN</cell><cell cols="2">Segmentation Brain tumor</cell><cell>1</cell><cell>MC dropout</cell><cell></cell><cell></cell><cell>×</cell></row><row><cell>al. [536]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Ozdemir et</cell><cell>2017 Bayesian</cell><cell cols="2">Segmentation Nodule detection</cell><cell>1</cell><cell>VI</cell><cell></cell><cell></cell><cell>×</cell></row><row><cell>al. [537]</cell><cell></cell><cell>CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Tanno</cell><cell>et</cell><cell>2017 CNN</cell><cell cols="2">Classification Brain tumor</cell><cell>2</cell><cell>VI</cell><cell></cell><cell></cell><cell>×</cell></row><row><cell>al. [538]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Jungo</cell><cell>et</cell><cell>2018 CNN</cell><cell cols="2">Segmentation Brain tumor</cell><cell>1</cell><cell>MC dropout</cell><cell></cell><cell></cell><cell>×</cell></row><row><cell>al. [52]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Kwon</cell><cell>et</cell><cell>2018 BNNs</cell><cell cols="2">Classification Cardiovascular</cell><cell>2</cell><cell>VI</cell><cell></cell><cell></cell><cell>×</cell></row><row><cell>al. [539]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ayhan</cell><cell>and</cell><cell>2018 DNNs</cell><cell>Data aug-</cell><cell>Fundus images</cell><cell>1</cell><cell>MC dropout</cell><cell></cell><cell></cell><cell>×</cell></row><row><cell cols="2">Berens [540]</cell><cell>(ResNet50)</cell><cell>mentation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Jungo</cell><cell>et</cell><cell>2018 U-net</cell><cell cols="2">Segmentation Brain tumor</cell><cell>2</cell><cell>MC dropout</cell><cell></cell><cell></cell><cell>×</cell></row><row><cell>al. [541]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Wang</cell><cell>et</cell><cell>2018 CNN</cell><cell cols="2">Segmentation Brain tumor</cell><cell>1</cell><cell>Weighted</cell><cell>loss</cell><cell cols="2">function,</cell><cell>×</cell></row><row><cell>al. [542]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">network-based uncertainty</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(Softmax</cell><cell cols="2">output)</cell><cell>and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">scribble-based</cell><cell cols="2">uncertainty</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(the geodesic distance to</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">different scribbles)</cell></row><row><cell>Moccia</cell><cell>et</cell><cell>2018 CNN</cell><cell>Classification</cell><cell>Surgical data of vari-</cell><cell>2</cell><cell cols="4">Superpixel (Spx)-based clas-</cell><cell>×</cell></row><row><cell>al. [543]</cell><cell></cell><cell></cell><cell>and tagging</cell><cell>ous diseases</cell><cell></cell><cell cols="4">sification of anatomical struc-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ture</cell><cell></cell><cell></cell></row><row><cell>Wang</cell><cell>et</cell><cell>2019 CNN</cell><cell cols="2">Segmentation Brain tumor</cell><cell>1</cell><cell>Ensemble</cell><cell></cell><cell></cell><cell>×</cell></row><row><cell>al. [33]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Tousignant et</cell><cell>2019 CNN</cell><cell cols="2">Classification Disability progression</cell><cell>2</cell><cell>MC dropout</cell><cell></cell><cell></cell><cell>×</cell></row><row><cell cols="3">al. [55] Roy et al. [57] 2019 Bayesian</cell><cell cols="2">(brain) Segmentation Brain segmentation</cell><cell>4</cell><cell cols="4">MC samples for voxel-wise</cell><cell>√</cell></row><row><cell>Jungo</cell><cell>and</cell><cell>QuickNAT 2019 U-Net</cell><cell cols="2">Segmentation Brain tumor</cell><cell>2</cell><cell>model Softmax</cell><cell cols="2">entropy,</cell><cell>MC</cell><cell>√</cell></row><row><cell cols="2">Reyes [544]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">dropout, Ensembles</cell></row><row><cell>Orlando</cell><cell>et</cell><cell>2019 U-Net</cell><cell cols="2">Segmentation OCT scans</cell><cell>2</cell><cell>MC dropout</cell><cell></cell><cell></cell><cell>×</cell></row><row><cell>al. [545]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ghesu</cell><cell>et</cell><cell>2019 DenseNet-</cell><cell cols="2">Classification Thoracic disease</cell><cell>2</cell><cell>MC dropout</cell><cell></cell><cell></cell><cell>×</cell></row><row><cell cols="2">al. [546] Baumgartner</cell><cell>121 2019 U-Net</cell><cell cols="3">Segmentation Thoracic and prostate 2</cell><cell>Ensemble</cell><cell></cell><cell></cell><cell>√</cell></row><row><cell cols="2">et al. [547]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Raczkowski</cell><cell>2019 Bayesian</cell><cell cols="2">Classification Colorectal cancer</cell><cell>1</cell><cell>VI</cell><cell></cell><cell></cell><cell>×</cell></row><row><cell cols="2">et al. [548]</cell><cell>CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Xue</cell><cell>et</cell><cell cols="3">2019 ResNet-101 Classification Skin cancer</cell><cell>1</cell><cell>Ensemble</cell><cell></cell><cell></cell><cell>×</cell></row><row><cell>al. [528]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Eaton-Rosen</cell><cell>2019 U-Net</cell><cell>Segmentation</cell><cell>Histopathological cell</cell><cell>2</cell><cell>MC dropout</cell><cell></cell><cell></cell><cell>×</cell></row><row><cell cols="2">et al. [549]</cell><cell></cell><cell>and</cell><cell>and white matter hy-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>regression</cell><cell>perintensity counting</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Continued on next page</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>TABLE 8 -</head><label>8</label><figDesc>Continued from previous page</figDesc><table><row><cell>Study</cell><cell></cell><cell cols="2">Year Classifier</cell><cell cols="3">Application Disease/cancer</cell><cell></cell><cell>#</cell><cell>UQ method</cell><cell>Code</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>datasets</cell></row><row><cell cols="2">di Scandalea</cell><cell>2019 U-Net</cell><cell></cell><cell cols="3">Segmentation Axon myelin</cell><cell></cell><cell>2</cell><cell>MC dropout</cell><cell>√</cell></row><row><cell cols="2">et al. [550]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Filos</cell><cell>et</cell><cell>2019 BDL</cell><cell></cell><cell cols="2">Classification DR</cell><cell></cell><cell></cell><cell>1</cell><cell>MC dropout, ensembles and</cell><cell>×</cell></row><row><cell>al. [551]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>VI</cell></row><row><cell cols="2">Ravanbakhsh</cell><cell cols="2">2019 conditional</cell><cell>Semantic</cell><cell cols="2">Cardiovascular</cell><cell></cell><cell>1</cell><cell>The scores generated by us-</cell><cell>×</cell></row><row><cell cols="2">et al. [552]</cell><cell>GAN</cell><cell></cell><cell>segmenta-</cell><cell cols="2">disease</cell><cell></cell><cell>ing the adversarial discrimi-</cell></row><row><cell></cell><cell></cell><cell>(cGAN)</cell><cell></cell><cell>tion</cell><cell></cell><cell></cell><cell></cell><cell>nator</cell></row><row><cell>Jena</cell><cell>and</cell><cell cols="2">2019 Bayesian</cell><cell cols="4">Segmentation Brain tumor, cell</cell><cell>3</cell><cell>MC dropout</cell><cell>×</cell></row><row><cell cols="2">Awate. [553]</cell><cell>DNN</cell><cell></cell><cell></cell><cell cols="3">membrane and chest</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Radiograph organ</cell></row><row><cell>Tanno</cell><cell>et</cell><cell>2019 CNN</cell><cell></cell><cell cols="2">Classification Brain</cell><cell cols="2">tumour</cell><cell>4</cell><cell>VI</cell><cell>×</cell></row><row><cell>al. [554]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(Glioma) and MS</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(multiple sclerosis)</cell></row><row><cell cols="2">Soberanis-</cell><cell cols="2">2019 CNN and</cell><cell cols="4">Segmentation Organ segmentation</cell><cell>1</cell><cell>MC dropout</cell><cell>×</cell></row><row><cell>Mukul</cell><cell>et</cell><cell>GCN</cell><cell></cell><cell></cell><cell cols="2">(pancreas)</cell><cell></cell></row><row><cell>al. [555]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Hu et al. [556] 2019 Probabilistic</cell><cell cols="2">Segmentation Lung</cell><cell cols="2">nodule</cell><cell>2</cell><cell>VI</cell><cell>×</cell></row><row><cell></cell><cell></cell><cell>U-Net</cell><cell></cell><cell></cell><cell>CT</cell><cell>dataset</cell><cell>and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">MICCAI2012 prostate</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MRI</cell><cell></cell><cell></cell></row><row><cell cols="3">Hu et al. [204] 2020 U-net</cell><cell></cell><cell>MRI recon-</cell><cell cols="3">Knee and brain MRI</cell><cell>2</cell><cell>MC dropout and deep en-</cell><cell>×</cell></row><row><cell></cell><cell></cell><cell>and</cell><cell>the</cell><cell>struction</cell><cell></cell><cell></cell><cell></cell><cell>sembles</cell></row><row><cell></cell><cell></cell><cell cols="2">Adaptive-</cell><cell>and Curve</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>CS-Net</cell><cell></cell><cell>fitting</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Luo</cell><cell>et</cell><cell>2020 DCN</cell><cell></cell><cell cols="3">Segmentation Cardiovascular</cell><cell></cell><cell>4</cell><cell>MC dropout</cell><cell>×</cell></row><row><cell>al. [557]</cell><cell></cell><cell>(Deep</cell><cell></cell><cell></cell><cell cols="2">disease</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">commensal</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">network)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hoebel</cell><cell>et</cell><cell>2020 U-Net</cell><cell></cell><cell cols="3">Segmentation Lung disease</cell><cell></cell><cell>1</cell><cell>MC dropout</cell><cell>×</cell></row><row><cell>al. [558]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Liu et al. [430] 2019 CNN</cell><cell></cell><cell cols="2">Classification sO 2</cell><cell></cell><cell></cell><cell>2</cell><cell>DSL (deep spectral learning)</cell><cell>×</cell></row><row><cell>Seeb öck</cell><cell>et</cell><cell cols="2">2019 Bayesian</cell><cell cols="4">Segmentation Retinal OCT scans</cell><cell>3</cell><cell>MC dropout</cell><cell>×</cell></row><row><cell>al. [559]</cell><cell></cell><cell>U-Net</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hiasa</cell><cell>et</cell><cell cols="2">2019 Bayesian</cell><cell cols="2">Segmentation Cancer</cell><cell></cell><cell></cell><cell>2</cell><cell>MC dropout</cell><cell>×</cell></row><row><cell>al. [560]</cell><cell></cell><cell>U-Net</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Xue</cell><cell>et</cell><cell cols="2">2019 BNN and</cell><cell cols="4">Classification Gigapixel phase im-</cell><cell>1</cell><cell>MC dropout and ensembles</cell><cell>×</cell></row><row><cell>al. [561]</cell><cell></cell><cell>U-Net</cell><cell></cell><cell></cell><cell>ages</cell><cell></cell><cell></cell></row><row><cell>LaBonte</cell><cell>et</cell><cell>2019 3D</cell><cell></cell><cell cols="4">Segmentation Material data (CT</cell><cell>2</cell><cell>MC dropout</cell><cell>×</cell></row><row><cell>al. [562]</cell><cell></cell><cell cols="2">Bayesian</cell><cell></cell><cell>scans)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Liao</cell><cell>et</cell><cell cols="2">2019 DenseNet,</cell><cell>Regression</cell><cell cols="2">Cardiovascular</cell><cell></cell><cell>1</cell><cell>MC dropout</cell><cell>×</cell></row><row><cell>al. [563] Raghu</cell><cell>et</cell><cell>LSTM 2019 N/A</cell><cell></cell><cell cols="3">diseases Classification DR</cell><cell></cell><cell>1</cell><cell>DUP (Direct Uncertainty Pre-</cell><cell>√</cell></row><row><cell>al. [564]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>diction )</cell></row><row><cell>Zhang</cell><cell>et</cell><cell>2019 ResNet</cell><cell></cell><cell>Reconstruction</cell><cell cols="2">Knee MRI</cell><cell></cell><cell>2</cell><cell>Active acquisition</cell><cell>×</cell></row><row><cell>al. [565]</cell><cell></cell><cell></cell><cell></cell><cell>and classifi-</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>cation</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Continued on next page</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30"><head>TABLE 8 -</head><label>8</label><figDesc>Continued from previous page</figDesc><table><row><cell>Study</cell><cell></cell><cell>Year Classifier</cell><cell cols="3">Application Disease/cancer</cell><cell></cell><cell>#</cell><cell cols="2">UQ method</cell><cell>Code</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>datasets</cell><cell></cell></row><row><cell cols="2">Ye et al. [333]</cell><cell>2020 Separable</cell><cell>Tissue</cell><cell cols="3">Brain dMRI (diffu-</cell><cell>1</cell><cell cols="2">Residual bootstrap strategy</cell><cell>×</cell></row><row><cell></cell><cell></cell><cell>dictionary-</cell><cell>micro-</cell><cell cols="3">sion magnetic reso-</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>LSTM</cell><cell>structure</cell><cell cols="3">nance imaging) scans</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>estimation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Xia et al. [381] 2020</cell><cell cols="4">Segmentation Pancreas and liver tu-</cell><cell>2</cell><cell>UMCT</cell><cell>×</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>mor</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Gantenbein et</cell><cell>2020 Revesible</cell><cell cols="4">Segmentation Lung and prostate</cell><cell>2</cell><cell cols="2">Variational function</cell><cell>×</cell></row><row><cell>al. [391]</cell><cell></cell><cell>PHiSeg</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bian</cell><cell>et</cell><cell>2020 CNN</cell><cell cols="4">Segmentation Cardiovascular and</cell><cell>2</cell><cell cols="2">UESM (Uncertainty Estima-</cell><cell>×</cell></row><row><cell>al. [437]</cell><cell></cell><cell></cell><cell></cell><cell cols="2">retinal OCT</cell><cell></cell><cell></cell><cell cols="2">tion and Segmentation Mod-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ule) + UCE (Uncertainty-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">aware Cross Entropy) loss</cell></row><row><cell>Donnat</cell><cell>et</cell><cell>2020 Hierarchial</cell><cell cols="3">Classification COVID-19</cell><cell></cell><cell>1</cell><cell cols="2">Stochastic</cell><cell>Expectation-</cell><cell>×</cell></row><row><cell>al. [566]</cell><cell></cell><cell>bayesian</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Maximization</cell></row><row><cell></cell><cell></cell><cell>network</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Mehrtash et</cell><cell>2020 U-Net</cell><cell cols="2">Segmentation Brain,</cell><cell>heart</cell><cell>and</cell><cell>5</cell><cell cols="2">Ensembles</cell><cell>×</cell></row><row><cell>al. [567]</cell><cell></cell><cell></cell><cell></cell><cell>prostate</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Wickstrøm et</cell><cell>2020 CNN</cell><cell cols="3">Segmentation Colorectal cancer</cell><cell></cell><cell>1</cell><cell cols="2">MC dropout</cell><cell>×</cell></row><row><cell>al. [568]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Carneiro</cell><cell>et</cell><cell>2020 ResNet,</cell><cell cols="2">Classification Polyp</cell><cell></cell><cell></cell><cell>2</cell><cell cols="2">MC integration</cell><cell>×</cell></row><row><cell>al. [569]</cell><cell></cell><cell>DenseNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Natekar</cell><cell>et</cell><cell>2020 DenseUnet,</cell><cell cols="3">Segmentation Brain tumor</cell><cell></cell><cell>1</cell><cell cols="2">TTD (test time dropout) for</cell><cell>×</cell></row><row><cell>al. [570]</cell><cell></cell><cell>ResUnet,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>VI</cell></row><row><cell></cell><cell></cell><cell>SimUnet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Li</cell><cell>and</cell><cell>2020 ResNet</cell><cell cols="4">Segmentation Colon and skin can-</cell><cell>2</cell><cell cols="2">MC dropout</cell><cell>×</cell></row><row><cell cols="2">Alstrøm [440]</cell><cell></cell><cell></cell><cell>cers</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dahal</cell><cell>et</cell><cell>2020 ResNet</cell><cell cols="3">Segmentation Cardiovascular</cell><cell></cell><cell>2</cell><cell>TTA,</cell><cell>HSE</cell><cell>(Horizontal</cell><cell>×</cell></row><row><cell>al. [435]</cell><cell></cell><cell></cell><cell></cell><cell>disease</cell><cell></cell><cell></cell><cell></cell><cell>stacked</cell><cell>ensemble),</cell><cell>MC</cell></row><row><cell cols="2">Li et al. [571]</cell><cell>2020 MLP (Mul-</cell><cell cols="2">Segmentation Autism</cell><cell></cell><cell></cell><cell>2</cell><cell cols="2">dropout DistDeepSHAP</cell><cell>√</cell></row><row><cell></cell><cell></cell><cell>tilayer Per-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(Distribution-based</cell><cell>Deep</cell></row><row><cell></cell><cell></cell><cell>ceptron)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Shapley value explanation)</cell></row><row><cell>Zheng</cell><cell>et</cell><cell>2020 ag-FCN</cell><cell cols="4">Segmentation Glands and infant</cell><cell>2</cell><cell>dd-AL</cell><cell>(Distribution</cell><cell>×</cell></row><row><cell>al. [572]</cell><cell></cell><cell>(attention</cell><cell></cell><cell cols="2">brain tissues</cell><cell></cell><cell></cell><cell cols="2">discrepancy-based AL)</cell></row><row><cell></cell><cell></cell><cell>gated fully</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>convo-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>lutional</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>network)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Wang</cell><cell>et</cell><cell>2020 ResNet</cell><cell cols="5">Classification Lung disease and DR 2</cell><cell cols="2">DRLA (Deep Reinforcement</cell><cell>×</cell></row><row><cell>al. [573]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>AL)</cell></row><row><cell>Quan</cell><cell>et</cell><cell>2020 ResNet</cell><cell cols="4">Classification EGD (Esophagogas-</cell><cell>2</cell><cell cols="2">Bayesian uncertainty esti-</cell><cell>×</cell></row><row><cell>al. [574]</cell><cell></cell><cell></cell><cell></cell><cell cols="3">troduodenoscopy)</cell><cell></cell><cell cols="2">mates and ensemble</cell></row><row><cell>Yuan</cell><cell>et</cell><cell>2020 DNNs</cell><cell cols="2">Classification Brain cell</cell><cell></cell><cell></cell><cell>2</cell><cell cols="2">Bayesian uncertainty</cell><cell>×</cell></row><row><cell>al. [575]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Chiou</cell><cell>et</cell><cell>2020 CycleGAN</cell><cell cols="3">Segmentation Prostate lesion</cell><cell></cell><cell>2</cell><cell cols="2">Gaussian sampling</cell><cell>×</cell></row><row><cell>al. [576]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Wang</cell><cell>et</cell><cell>2020 Teacher-</cell><cell cols="4">Segmentation Left Atrium and kid-</cell><cell>2</cell><cell cols="2">Double-uncertainty weighted ×</cell></row><row><cell>al. [577]</cell><cell></cell><cell>student</cell><cell></cell><cell cols="2">ney segmentation</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Continued on next page</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_31"><head>TABLE 8 -</head><label>8</label><figDesc>Continued from previous page</figDesc><table><row><cell>Study</cell><cell></cell><cell>Year Classifier</cell><cell></cell><cell>Application Disease/cancer</cell><cell>#</cell><cell>UQ method</cell><cell>Code</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>datasets</cell><cell></cell><cell></cell></row><row><cell cols="2">Li et al. [578]</cell><cell>2020 ResUNet</cell><cell></cell><cell>Segmentation Organ and skin lesion</cell><cell>2</cell><cell>Self-loop uncertainty</cell><cell>×</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>segmentation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Yang</cell><cell>et</cell><cell>2020 Deep</cell><cell>Q</cell><cell>Segmentation Catheter segmenta-</cell><cell>1</cell><cell>Hybrid constraints</cell><cell>×</cell></row><row><cell>al. [579]</cell><cell></cell><cell>learning</cell><cell></cell><cell>tion</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">and Dual-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>UNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Venturini et</cell><cell>2020 U-Net</cell><cell></cell><cell>Segmentation Brain volumes and</cell><cell>2</cell><cell>test-time augmentation and</cell><cell>×</cell></row><row><cell>al. [580]</cell><cell></cell><cell></cell><cell></cell><cell>healthy pregnant fe-</cell><cell></cell><cell>test-time dropout</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>males</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Yu et al. [581] 2020 ResNet</cell><cell></cell><cell>Classification Glaucoma</cell><cell>2</cell><cell>FusionBranch</cell><cell>×</cell></row><row><cell>Huang</cell><cell>et</cell><cell cols="2">2020 ReLayNet</cell><cell>Segmentation OCT (Optical coher-</cell><cell>1</cell><cell>MC dropout</cell><cell>×</cell></row><row><cell>al. [582]</cell><cell></cell><cell></cell><cell></cell><cell>ence tomography)</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Uncertainty estimation in deep learning with application to spoken language assessment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Malinin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">To trust or not to trust a classifier</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5541" to="5552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The need for biases in learning generalizations</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Laboratory for Computer Science Research . . .</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Epistemic uncertainty sampling</title>
		<author>
			<persName><forename type="first">V.-L</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Destercke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Üllermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Discovery Science</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="72" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Active learning: A survey</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Classification</title>
				<imprint>
			<publisher>Chapman and Hall/CRC</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="599" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Bayesian deep learning and uncertainty in computer vision</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Phan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>University of Waterloo</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>University of Cambridge</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Aleatoric and epistemic uncertainty in machine learning: A tutorial introduction</title>
		<author>
			<persName><forename type="first">E</forename><surname>Üllermeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Waegeman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09457</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Calibrating uncertainty models for steering angle estimation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hubschneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hutmacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Öllner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Transportation Systems Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1511" to="1518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Evaluating bayesian deep learning methods for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mukhoti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12709</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Spinalnet: Deep neural network with gradual input</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kabir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M J</forename><surname>Jalali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Atiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Srinivasan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03347</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recent advances in deep learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pourpanah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Machine Learning and Cybernetics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The need for uncertainty quantification in machine-assisted medical decision making</title>
		<author>
			<persName><forename type="first">E</forename><surname>Begoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kusnezov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="23" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Noise contrastive priors for functional uncertainty</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Irpan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davidson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.09289</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimization under uncertainty in the era of big data and deep learning: When machine learning meets mathematical programming</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Chemical Engineering</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page" from="434" to="448" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural network-based uncertainty quantification: A survey of methodologies and applications</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Kabir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="36" to="218" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
		</imprint>
		<respStmt>
			<orgName>California Univ San Diego La Jolla Inst for Cognitive Science, Tech. Rep.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Aleatory or epistemic? does it matter</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Kiureghian</surname></persName>
			<affiliation>
				<orgName type="collaboration">Risk Communication</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ditlevsen</surname></persName>
			<affiliation>
				<orgName type="collaboration">Risk Communication</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">Structural Safety</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5574" to="5584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On information and sufficiency</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The annals of mathematical statistics</title>
				<imprint>
			<date type="published" when="1951">1951</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">518</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Bayesian convolutional neural networks with bernoulli approximate variational inference</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02158</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="233" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Jospin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Laga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.06823</idno>
		<title level="m">Hands-on bayesian neural networks-a tutorial for deep learning users</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Towards bayesian deep learning: A survey</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01662</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Calibration of deep probabilistic models with decoupled bayesian neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Maro Ñas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic differentiation variational inference</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="430" to="474" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Bayesian learning for neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">118</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Representing inferential uncertainty in deep neural networks through sampling</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mcclure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kriegeskorte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, ICLR 2017-Conference Track Proceedings</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Single shot mc dropout approximation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Brach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ürr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03293</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Aleatoric uncertainty estimation with test-time augmentation for medical image segmentation with convolutional neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aertsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deprest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">338</biblScope>
			<biblScope unit="page" from="34" to="45" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Universal adversarial perturbation via prior driven uncertainty approximation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2941" to="2949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exploring uncertainty measures in deep networks for multiple sclerosis lesion detection and segmentation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Arbel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">101557</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Uncertaintyaware self-ensembling model for semi-supervised 3d left atrium segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer Assisted Intervention</title>
		<editor>D. Shen, T. Liu, T. M. Peters, L. H. Staib, C. Essert, S. Zhou, P.-T. Yap, and A. Khan</editor>
		<imprint>
			<biblScope unit="page" from="605" to="613" />
			<date type="published" when="2019">2019</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Spatial uncertainty sampling for end-to-end control</title>
		<author>
			<persName><forename type="first">A</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Soleimany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04829</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Accuracy, uncertainty, and adaptability of automatic myocardial asl segmentation using deep cnn</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Nayak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic Resonance in Medicine</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1863" to="1874" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bayesian uncertainty estimation for batch normalized deep networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Teye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research</title>
				<editor>
			<persName><forename type="first">J</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning, ser. Machine Learning Research<address><addrLine>Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07-15">10-15 Jul 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4907" to="4916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Communicationefficient stochastic gradient mcmc for neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4173" to="4180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Bayesian segnet: Model uncertainty in deep convolutional encoderdecoder architectures for scene understanding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02680</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Leveraging uncertainty information from deep neural networks for disease detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Leibig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Allken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Ayhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Berens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">17816</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Uncertainty-aware learning from demonstration using mixture density networks with sampling-free variance modeling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mixture density networks</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Network Research Group</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Towards uncertainty-assisted brain tumor segmentation and survival prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jungo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Knecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pérez-Beteta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molina-García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Pérez-García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wiest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries</title>
				<editor>
			<persName><forename type="first">A</forename><surname>Crimi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Kuijf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Menze</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Reyes</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="474" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Full-resolution residual networks for semantic segmentation in street scenes</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Uncertainty modeling and interpretability in convolutional neural networks for polyp segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wickstrøm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kampffmeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Machine Learning for Signal Processing</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Uncertainty-driven sanity check: Application to postoperative brain tumor cavity segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jungo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ermis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03106</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Prediction and uncertainty quantification of daily airport flight delays</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vandal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Livingston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Piho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zimmerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Predictive Applications and APIs</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="45" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Leveraging uncertainty estimates for predicting segmentation quality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno>abs/1807.00502</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Prediction of disease progression in multiple sclerosis patients using deep learning analysis of mri data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tousignant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lemaître</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Arbel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Imaging with Deep Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="483" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Exploiting uncertainty of deep neural networks for improving segmentation accuracy in mri images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Emami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Najarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M R</forename><surname>Soroushmehr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2322" to="2326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Bayesian quicknat: model uncertainty in deep whole-brain segmentation for structure-wise quality control</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Conjeti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wachinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D N</forename><surname>Initiative</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">195</biblScope>
			<biblScope unit="page" from="11" to="22" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">A systematic comparison of bayesian deep learning robustness in diabetic retinopathy tasks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Filos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G J</forename><surname>Rudner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kenton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Kroon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A bayesian deep learning framework for end-to-end prediction of emotion from heartbeat</title>
		<author>
			<persName><forename type="first">R</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Southern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Affective Computing</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">On the expressiveness of approximate inference in bayesian neural networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Foong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">1909</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Estimating uncertainty in neural networks for segmentation quality control</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Wright</surname></persName>
		</author>
		<ptr target="https://www.doc.ic.ac.uk/bglocker/public...,Tech.Rep." />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep Bayesian active learning for natural language processing: Results of a large-scale empirical study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11">Oct.-Nov. 2018</date>
			<biblScope unit="page" from="2904" to="2909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">On the importance of strong baselines in bayesian deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mukhoti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.09385</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Empirical study of mcdropout in various astronomical observing conditions</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kennamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Ihler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kirkby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Ideal-observer computation in medical imaging with use of markov-chain monte carlo techniques</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kupinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Hoppin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Clarkson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Barrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOSA A</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="430" to="438" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Bayesian probabilistic matrix factorization using markov chain monte carlo</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning, ser. ICML &apos;08</title>
				<meeting>the International Conference on Machine Learning, ser. ICML &apos;08</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="880" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Markov chain monte carlo and variational inference: Bridging the gap</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1218" to="1226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Stochastic gradient hamiltonian monte carlo</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1683" to="1691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Bayesian sampling using stochastic gradient thermostats</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Babbush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Skeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3203" to="3211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">On the convergence of stochastic gradient mcmc algorithms with high-order integrators</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2278" to="2286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning weight uncertainty with stochastic gradient mcmc for shape classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5666" to="5675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Icebreaker: Element-wise efficient information acquisition with a bayesian deep latent gaussian model</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tschiatschek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">831</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Consistency and fluctuations for stochastic gradient langevin dynamics</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Thiery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Vollmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="225" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">The loss surfaces of multilayer networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Arous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="192" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Cyclical stochastic gradient mcmc for bayesian deep learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.03932</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Thermostatassisted continuously-tempered hamiltonian monte carlo for bayesian learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">682</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Hybrid monte carlo</title>
		<author>
			<persName><forename type="first">S</forename><surname>Duane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Pendleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roweth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics letters B</title>
		<imprint>
			<biblScope unit="volume">195</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="216" to="222" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Improving predictive uncertainty estimation using dropouthamiltonian monte carlo</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vergara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valdenegro-Toro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jorquera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Computing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="4307" to="4322" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Canonical dynamics: Equilibrium phase-space distributions</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Hoover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review A</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1695</biblScope>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A unified formulation of the constant temperature molecular dynamics methods</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nosé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of chemical physics</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="511" to="519" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Bayesian learning via stochastic gradient langevin dynamics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11</title>
				<meeting>the 28th international conference on machine learning (ICML-11</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="681" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Markov-chain monte carlo approximation of the ideal observer using generative adversarial networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Anastasio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Imaging 2020: Image Perception, Observer Performance, and Technology Assessment</title>
				<imprint>
			<publisher>International Society for Optics and Photonics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">11316</biblScope>
			<biblScope unit="page">113160D</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Robust visual tracking based on variational autoencoding markov chain monte carlo</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">512</biblScope>
			<biblScope unit="page" from="1308" to="1323" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">The k-tied normal distribution: A compact parameterization of gaussian mean field posteriors in bayesian neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Swiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Veeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.02655</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Variational inference to measure model uncertainty in deep neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinbrener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pilz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10189</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Correlated parameters to accurately measure uncertainty in deep neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pilz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">An empirical bayes approach to statistics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability</title>
				<meeting>the Third Berkeley Symposium on Mathematical Statistics and Probability</meeting>
		<imprint>
			<publisher>University of California Press</publisher>
			<date type="published" when="1956">1956</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
	<note>Contributions to the Theory of Statistics</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Efficient priors for scalable variational inference in bayesian deep neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subedar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tickoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="773" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Improving mfvi in bayesian neural networks with empirical bayes: a study with diabetic retinopathy diagnosis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subedar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tickoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Filos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourht workshop on Bayesian Deep Learning</title>
				<imprint>
			<date type="published" when="2019">NeurIPS 2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Uncertainty-aware audiovisual activity recognition using deep bayesian variational inference</title>
		<author>
			<persName><forename type="first">M</forename><surname>Subedar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tickoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6300" to="6309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Modeling and planning under uncertainty using deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Manic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4442" to="4454" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Critical temperature prediction for a superconductor: A variational bayesian neural network approach</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Noumeir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Quach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Applied Superconductivity</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Multiplicative normalizing flows for variational bayesian neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2218" to="2227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Combining model and parameter uncertainty in bayesian neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Storvik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.07594</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Probabilistic spatiotemporal wind speed forecasting based on a variational bayesian deep learning model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Energy</title>
		<imprint>
			<biblScope unit="volume">260</biblScope>
			<biblScope unit="page">114259</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">A bayesian graph convolutional network for reliable prediction of molecular properties with uncertainty quantification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">36</biblScope>
			<biblScope unit="page" from="8438" to="8446" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Try depth instead of weight correlations: Mean-field is a less restrictive assumption for variational inference in deep networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bayesian Deep Learning Workshop at NeurIPS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Active learning enabled activity recognition</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A A H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pervasive and Mobile Computing</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="312" to="330" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Active learning: theory and applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stanford University USA</title>
				<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Deep bayesian active learning with image data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1183" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Bayesian active learning for classification and preference learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lengyel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subedar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tickoo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05323</idno>
		<title level="m">Specifying weight priors in bayesian deep neural networks with empirical bayes</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Batchbald: Efficient and diverse batch acquisition for deep bayesian active learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Amersfoort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Semisupervised bayesian active learning for text classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Burkhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Siekiera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bayesian Deep Learning Workshop at NeurIPS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Activeharnet: Towards on-device deep bayesian active learning for human activity recognition</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Gudur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sundaramoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Umaashankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 3rd International Workshop on Deep Learning for Mobile Systems and Applications</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Deep bayesian active semi-supervised learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Matthias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hanno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Machine Learning and Applications (ICMLA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="158" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Deep active learning for axon-myelin segmentation on histology data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Di Scandalea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Perone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boudreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen-Adad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">The relevance of bayesian layer positioning to model uncertainty in deep bayesian active learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lesnikowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12535</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Coresets for scalable bayesian logistic regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huggins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Broderick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4080" to="4088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Bayesian batch active learning as sparse subset approximation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pinsler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6356" to="6367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Crowdsourcing thousands of specialized labels: A bayesian active training approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shasha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Champ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pacitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1376" to="1391" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Bayesian methods for intelligent task assignment in crowdsourcing systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Decision Making: Uncertainty, Imperfection, Deliberation and Scalability</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">Bayesian semisupervised learning with deep generative models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09751</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">Bayesian generative active deep learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11643</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Personalizing activity recognition models with quantifying different types of uncertainty using wearable sensors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jafari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05424</idno>
		<title level="m">Weight uncertainty in neural networks</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">Bayesian recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02798</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<title level="m" type="main">Uncertainty-guided continual learning with bayesian neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02425</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Bayesian 3d convnets for action recognition from few examples</title>
		<author>
			<persName><forename type="first">M</forename><surname>De La Riva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
				<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<title level="m" type="main">Pairwise supervised hashing with bernoulli variational auto-encoder and self-control gradient estimator</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Dadaneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boluki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10477</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title level="m" type="main">From variational to deterministic autoencoders</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vergari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch Ölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">Uncertainty quantification with generative models</title>
		<author>
			<persName><forename type="first">V</forename><surname>Öhm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lanusse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Seljak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10046</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">Posterior inference unchained with EL 2 O</title>
		<author>
			<persName><forename type="first">U</forename><surname>Seljak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">Uncertainty analysis of vae-gans for compressive medical imaging</title>
		<author>
			<persName><forename type="first">V</forename><surname>Edupuganti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mardani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vasanawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pauly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11228</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">Fast uncertainty quantification of reservoir simulation with variational u-net</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title level="m" type="main">A variational u-net for conditional appearance and shape generation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<monogr>
		<title level="m" type="main">Cosmovae: Variational autoencoder for cmb image inpainting</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hamann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<monogr>
		<title level="m" type="main">A variational auto-encoder model for stochastic point processes</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mehrasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Jyothi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Predictable uncertainty-aware unsupervised deep anomaly segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Matsubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Uehara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Joint Conference on Neural Networks (IJCNN)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<title level="m" type="main">πvae: Encoding stochastic process priors with variational autoencoders</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Flaxman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01613</idno>
		<title level="m">Conditional neural processes</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">A deep learning just-in-time modeling approach for soft sensor based on variational autoencoder</title>
		<author>
			<persName><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemometrics and Intelligent Laboratory Systems</title>
		<imprint>
			<biblScope unit="volume">197</biblScope>
			<biblScope unit="page">103922</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<title level="m" type="main">Bayesian variational autoencoders for unsupervised out-of-distribution detection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Daxberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Deep gaussian processes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="207" to="215" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Avoiding pathologies in very deep networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="202" to="210" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Doubly stochastic variational inference for deep gaussian processes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Salimbeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Deisenroth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4588" to="4599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<title level="m" type="main">A gaussian process perspective on convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borovykh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.10798</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<title level="m" type="main">Implicit posterior variational inference for deep gaussian processes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jaillet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11998</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Multi-view representation learning with deep gaussian processes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<monogr>
		<title level="m" type="main">Global inducing point variational posteriors for bayesian neural networks and deep gaussian processes</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Ober</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Aitchison</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08140</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Gaussian processes for machine learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Lectures on Machine Learning</title>
				<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Scalable variational bayesian kernel selection for sparse gaussian process regression</title>
		<author>
			<persName><forename type="first">T</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K H</forename><surname>Low</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5997" to="6004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<monogr>
		<title level="m" type="main">Modeling uncertainty with hedged instance embedding</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gallagher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00319</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Inference in deep gaussian processes using stochastic gradient hamiltonian monte carlo</title>
		<author>
			<persName><forename type="first">M</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Murillo-Fuentes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7506" to="7516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Fast uncertainty estimates and bayesian model averaging of dnns</title>
		<author>
			<persName><forename type="first">W</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Deep Learning Workshop at UAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<monogr>
		<title level="m" type="main">Averaging weights leads to wider optima and better generalization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05407</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">A simple baseline for bayesian uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">164</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<monogr>
		<title level="m" type="main">Flipout: Efficient pseudo-independent weight perturbations on minibatches</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vicol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.04386</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b150">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<monogr>
		<title level="m" type="main">Adversarial examples, uncertainty, and transfer testing robustness in gaussian process hybrid deep networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bradshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G D G</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02476</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Gaussian yolov3: An accurate and fast object detector using localization uncertainty for autonomous driving</title>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="502" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<monogr>
		<title level="m" type="main">Yolov3: An incremental improvement</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<monogr>
		<title level="m" type="main">Fast and scalable bayesian deep learning by weightperturbation in adam</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tangkaratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.04854</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Learning structured weight uncertainty in bayesian neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1283" to="1292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<monogr>
		<title level="m" type="main">Matrix variate distributions</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Nagar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CRC Press</publisher>
			<biblScope unit="volume">104</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Structured and efficient variational deep learning with matrix gaussian posteriors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1708" to="1716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Convolutional gaussian processes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Van Der Wilk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hensman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2849" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Deep convolutional gaussian processes</title>
		<author>
			<persName><forename type="first">K</forename><surname>Blomqvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heinonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="582" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Calibrating deep convolutional gaussian processes</title>
		<author>
			<persName><forename type="first">G.-L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Bonilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Michiardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Filippone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1554" to="1563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<monogr>
		<title level="m" type="main">Translation insensitivity for deep convolutional gaussian processes</title>
		<author>
			<persName><forename type="first">V</forename><surname>Dutordoir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Der Wilk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Artemev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hensman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05888</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Sparse orthogonal variational inference for gaussian processes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Titsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1932" to="1942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Addressing failure prediction by learning model confidence</title>
		<author>
			<persName><forename type="first">C</forename><surname>Corbière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bar-Hen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2902" to="2913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<monogr>
		<title level="m" type="main">Uncertainty estimation via stochastic batch normalization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Atanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ashukha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Neklyudov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04893</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b165">
	<monogr>
		<title level="m" type="main">Variance networks: When expectation does not meet your expectations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Neklyudov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ashukha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03764</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b166">
	<monogr>
		<title level="m" type="main">Information theory, inference and learning algorithms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Mackay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Mac</surname></persName>
		</author>
		<author>
			<persName><surname>Kay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">A scalable laplace approximation for neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Botev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018-Conference Track Proceedings</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note>International Conference on Representation Learning</note>
</biblStruct>

<biblStruct xml:id="b168">
	<monogr>
		<title level="m" type="main">Attraction-repulsion actor-critic for continuous control reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mazoure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07543</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Uncertainty-based decision making using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 22th International Conference on Information Fusion (FUSION)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<monogr>
		<title level="m" type="main">Bayesian policy optimization for model uncertainty</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mandalika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Srinivasa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01014</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b171">
	<monogr>
		<title level="m" type="main">The uncertainty bellman equation and exploration</title>
		<author>
			<persName><forename type="first">B</forename><surname>O'donoghue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05380</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b172">
	<monogr>
		<title level="m" type="main">Uncertainty-aware reinforcement learning for collision avoidance</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Villaflor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01182</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Bayesian reinforcement learning: A survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations and Trends® in Machine Learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="359" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<monogr>
		<title level="m" type="main">Reinforcement learning through active inference</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tschantz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Millidge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Buckley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12636</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Benchmarking uncertainty estimates with deep reinforcement learning for dialogue policy optimisation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tegho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gašić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6069" to="6073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Successor uncertainties: exploration and uncertainty in temporal difference learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Janz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mazur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tschiatschek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4509" to="4518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Active perception in adversarial scenarios using maximum entropy deep reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>How</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<monogr>
		<title level="m" type="main">Fully bayesian recurrent neural networks for safe reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Benatan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">O</forename><surname>Pyzer-Knapp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03308</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Uncertainty-driven imagination for continuous deep reinforcement learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kalweit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boedecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="195" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<monogr>
		<title level="m" type="main">Deep bayesian bandits showdown: An empirical comparison of bayesian deep networks for thompson sampling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.09127</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Bootstrap estimated uncertainty of the environment model for model-based reinforcement learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3870" to="3877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<monogr>
		<title level="m" type="main">Epistemic risk-sensitive reinforcement learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dimitrakakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06273</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Safe reinforcement learning with model uncertainty estimates</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ötjens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Everett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>How</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8662" to="8668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<monogr>
		<title level="m" type="main">Estimating risk and uncertainty in deep reinforcement learning</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Clements</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-M</forename><surname>Robaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Delft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Slaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Toth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09638</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Exploiting action-value uncertainty to drive exploration in reinforcement learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>D'eramo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Restelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Joint Conference on Neural Networks (IJCNN)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<monogr>
		<title level="m" type="main">Mix-n-match: Ensemble and compositional methods for uncertainty calibration in deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kailkhura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07329</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Variable selection with rigorous uncertainty quantification using bayesian deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bayesian Deep Learning Workshop at NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6402" to="6413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<monogr>
		<title level="m" type="main">Maximizing overall diversity for improved uncertainty estimates in deep ensembles</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gifford</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07380</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b190">
	<monogr>
		<title level="m" type="main">Evaluating scalable bayesian deep learning methods for robust computer vision</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">K</forename><surname>Gustafsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Sch Ön</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01620</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Deep echo state networks with uncertainty quantification for spatio-temporal forecasting</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Wikle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Environmetrics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">e2553</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning in a handful of trials using probabilistic dynamics models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Calandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4754" to="4765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Accurate uncertainty estimation and decomposition in ensemble learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Kioumourtzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Coull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8950" to="8961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">The mbpep: a deep ensemble pruning algorithm providing high quality uncertainty prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2942" to="2955" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<monogr>
		<title level="m" type="main">Uncertainty in structured prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07650</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b196">
	<monogr>
		<title level="m" type="main">Ensemble distribution distillation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mlodozeniec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00076</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b197">
	<monogr>
		<title level="m" type="main">Pitfalls of in-domain uncertainty estimation and ensembling in deep learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ashukha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lyzhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06470</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b198">
	<monogr>
		<title level="m" type="main">Deep ensemble bayesian active learning: Addressing the mode collapse issue in monte carlo dropout via ensembles</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fulop</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03897</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b199">
	<monogr>
		<title level="m" type="main">Data-driven prognostics with predictive uncertainty estimation using ensemble of deep ordinal regression models</title>
		<author>
			<persName><forename type="first">V</forename><surname>Tv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Malhotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shroff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.09795</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b200">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bharadhwaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shkurti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04514</idno>
		<title level="m">Dibs: Diversity inducing information bottleneck in model ensembles</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b201">
	<monogr>
		<title level="m" type="main">High-quality prediction intervals for deep learning: A distribution-free, ensembled approach</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brintrup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neely</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07167</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b202">
	<monogr>
		<title level="m" type="main">Wasserstein variational gradient descent: From semi-discrete optimal transport to ensemble variational inference</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ambrogioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Guclu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Gerven</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02827</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b203">
	<monogr>
		<title level="m" type="main">Simple and accurate uncertainty quantification from bias-variance decomposition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pezzotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mavroeidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05582</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">Echo state networks for data-driven downhole pressure estimation in gas-lift oil wells</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Antonelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Camponogara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Foss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="106" to="117" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<monogr>
		<title level="m" type="main">Deep-esn: A multiple projection-encoding hierarchical reservoir computing framework</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05255</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">Online deep ensemble learning for predicting citywide human mobility</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shibasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sakuramachi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</title>
				<meeting>the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">Deep ensemble learning based probabilistic load forecasting in smart grids</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy</title>
		<imprint>
			<biblScope unit="volume">189</biblScope>
			<biblScope unit="page">116324</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<monogr>
		<title level="m" type="main">Simple and scalable epistemic uncertainty estimation using a single deep deterministic neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Van Amersfoort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02037</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">Sentiment analysis: Bayesian ensemble learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Fersini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Messina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Pozzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decision support systems</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="26" to="38" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<monogr>
		<title level="m" type="main">Uncertainty in neural networks: Bayesian ensembling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brintrup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Anastassacos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neely</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05546</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">Deep ensemble bayesian active learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fulop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bayesian Deep Learning Workshop at NeurIPS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main">Uncertainty in neural networks: Approximately bayesian ensembling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Leibfried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brintrup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 23rd International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">Linear maximum margin classifier for learning from uncertain data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tzelepis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mezaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2948" to="2962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">Targeting the uncertainty of predictions at patientlevel using an ensemble of classifiers coupled with calibration methods, venn-abers, and conformal predictors: A case study in ad</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guerreiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Madeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D N</forename><surname>Initiative</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page">103350</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">An ensemble uncertainty aware measure for directed hill climbing ensemble pruning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Partalas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="257" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">Addressing uncertainty in atomistic machine learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khorshidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Chemistry Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page">985</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<monogr>
		<title level="m" type="main">Nads: Neural architecture distribution search for uncertainty awareness</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ardywibowo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boluki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06646</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b218">
	<monogr>
		<title level="m" type="main">Optimal uncertainty-guided neural network training</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kabir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kavousi-Fard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Srinivasan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12761</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b219">
	<monogr>
		<title level="m" type="main">Bias-reduced uncertainty estimation for deep neural classifiers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Geifman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Uziel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08206</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">Single-model uncertainties for deep learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tagasovska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6414" to="6425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title level="a" type="main">Uncertainty estimation using a single deep deterministic neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Van Amersfoort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
				<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<monogr>
		<title level="m" type="main">Frequentist uncertainty estimates for deep learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tagasovska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00908</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b223">
	<monogr>
		<title level="m" type="main">Dropconnect is effective in modeling uncertainty of bayesian deep networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mobiny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moulik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04569</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b224">
	<analytic>
		<title level="a" type="main">Measures of uncertainty for neighborhood rough sets</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="226" to="235" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b225">
	<monogr>
		<title level="m" type="main">Accurate uncertainties for deep learning using calibrated regression</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kuleshov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00263</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main">The natural neural tangent kernel: Neural network training dynamics under natural gradient descent</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Rudner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourht workshop on Bayesian Deep Learning</title>
				<imprint>
			<date type="published" when="2019">NeurIPS 2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main">Ucam: Visual explanation using uncertainty based class activation maps</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Patro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lunayach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Namboodiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7444" to="7453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<monogr>
		<title level="m" type="main">Decomposition of uncertainty in bayesian deep learning for efficient and risk-sensitive learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Depeweg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Udluft</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.07283</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b229">
	<analytic>
		<title level="a" type="main">Modeling uncertainty by learning a hierarchy of deep neural connections</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Rohekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gurwicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nisimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Novik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4246" to="4256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b230">
	<analytic>
		<title level="a" type="main">Striking the right balance with uncertainty</title>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b231">
	<monogr>
		<title level="m" type="main">Implicit weight uncertainty in neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pawlowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rajchl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01297</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b232">
	<monogr>
		<title level="m" type="main">Idk cascades: Fast deep learning by learning not to overthink</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Crankshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tumanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00885</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b233">
	<analytic>
		<title level="a" type="main">Adversarial uncertainty quantification in physics-informed neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perdikaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">394</biblScope>
			<biblScope unit="page" from="136" to="152" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b234">
	<analytic>
		<title level="a" type="main">Mono-sf: Multi-view geometry meets single-view depth for monocular scene flow estimation of dynamic traffic scenes</title>
		<author>
			<persName><forename type="first">F</forename><surname>Brickwedde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2780" to="2790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b235">
	<analytic>
		<title level="a" type="main">On mixup training: Improved calibration and predictive uncertainty for deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thulasidasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Michalak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">899</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b236">
	<monogr>
		<title level="m" type="main">why should you trust my explanation?&quot; understanding uncertainty in lime explanations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Udell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12991</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b237">
	<monogr>
		<title level="m" type="main">Minimum uncertainty based detection of adversaries in deep neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sheikholeslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Giannakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02841</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b238">
	<monogr>
		<title level="m" type="main">Decoupled certainty-driven consistency loss for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">1901</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b239">
	<monogr>
		<title level="m" type="main">Efficient evaluation-time uncertainty estimation by improved distillation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Englesson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05419</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b240">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kirichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.07504</idno>
		<title level="m">Subspace inference for bayesian deep learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b241">
	<analytic>
		<title level="a" type="main">Distribution-free uncertainty quantification for kernel methods by gradient perturbations</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Csáji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Kis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="1677" to="1699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b242">
	<monogr>
		<title level="m" type="main">Using pre-training can improve model robustness and uncertainty</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09960</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b243">
	<monogr>
		<title level="m" type="main">Predictive uncertainty through quantization</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Veeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V D</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05500</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b244">
	<analytic>
		<title level="a" type="main">Quality control in radiotherapy-treatment planning using multi-task learning and uncertainty estimation</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Bragman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tanno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Eaton-Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hawkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Imaging with Deep Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b245">
	<analytic>
		<title level="a" type="main">Bayesian dark knowledge</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3438" to="3446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b246">
	<analytic>
		<title level="a" type="main">Vime: Variational information maximizing exploration</title>
		<author>
			<persName><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De Turck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1109" to="1117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b247">
	<analytic>
		<title level="a" type="main">Bayesian optimization with robust bayesian neural networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4134" to="4142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b248">
	<monogr>
		<title level="m" type="main">Uncertainty estimates for optical flow with multihypotheses networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ö</forename><surname>¸ic ¸ek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Galesso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Makansi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07095</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">81</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b249">
	<analytic>
		<title level="a" type="main">Uncertainty-aware attention for reliable interpretation and prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="909" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b250">
	<monogr>
		<title level="m" type="main">Bayesian policy gradients via alpha divergence dropout inference</title>
		<author>
			<persName><forename type="first">P</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02037</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b251">
	<analytic>
		<title level="a" type="main">Uncertainty-based continual learning with adaptive regularization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4394" to="4404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b252">
	<monogr>
		<title level="m" type="main">Learning structural weight uncertainty for sequential decision-making</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00085</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b253">
	<analytic>
		<title level="a" type="main">Evidential deep learning to quantify classification uncertainty</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sensoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3179" to="3189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b254">
	<analytic>
		<title level="a" type="main">Variational bayesian monte carlo</title>
		<author>
			<persName><forename type="first">L</forename><surname>Acerbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8213" to="8223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b255">
	<monogr>
		<title level="m" type="main">Bayesian prior networks with pac training</title>
		<author>
			<persName><forename type="first">M</forename><surname>Haussmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gerwinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00816</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b256">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">De</forename><surname>Ath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Everson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Fieldsend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rahat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.01873</idno>
		<title level="m">shotgun: -greedy batch bayesian optimisation</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b257">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Foong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.11537</idno>
		<title level="m">&apos;inbetween&apos;uncertainty in bayesian neural networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b258">
	<monogr>
		<title level="m" type="main">Bayesian graph neural networks with adaptive connection sampling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hasanzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hajiramezanali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boluki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duffield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04064</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b259">
	<monogr>
		<title level="m" type="main">Ensemble model patching: A parameter-efficient variational bayesian neural network</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Williams-King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09453</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b260">
	<analytic>
		<title level="a" type="main">Automated detection of presymptomatic conditions in spinocerebellar ataxia type 2 using monte carlo dropout and deep neural network techniques with electrooculogram signals</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stoean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stoean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Atencia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Velázquez-Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">R</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Joya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">3032</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b261">
	<monogr>
		<title level="m" type="main">Wat zei je? detecting outof-distribution translations with variational transformers</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08344</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b262">
	<analytic>
		<title level="a" type="main">Scalable bayesian uncertainty quantification in imaging inverse problems via convex optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Repetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wiaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="118" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b263">
	<analytic>
		<title level="a" type="main">Uncertainty quantification in cnn-based surface prediction using shape priors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Óthová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parisot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Puyol-Ant Ón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Shape in Medical Imaging</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="300" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b264">
	<monogr>
		<title level="m" type="main">Mumbo: Multi-task maxvalue bayesian optimization</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Moss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rayson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12093</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b265">
	<analytic>
		<title level="a" type="main">Bayesian image classification with deep convolutional gaussian processes</title>
		<author>
			<persName><forename type="first">V</forename><surname>Dutordoir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wilk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Artemev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hensman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1529" to="1539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b266">
	<monogr>
		<title level="m" type="main">Learning from the past: Continual meta-learning via bayesian graph modeling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04695</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b267">
	<analytic>
		<title level="a" type="main">Vcg under sybil (falsename) attacks-a bayesian analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gafni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tennenholtz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1966" to="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b268">
	<monogr>
		<title level="m" type="main">Uncertainty-aware multi-shot knowledge distillation for image-based object reidentification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05197</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b269">
	<analytic>
		<title level="a" type="main">Robust conditional gan from uncertainty-aware pairwise comparisons</title>
		<author>
			<persName><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">916</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b270">
	<analytic>
		<title level="a" type="main">Ranking information extracted from uncertainty quantification of the prediction of a deep learning model on medical time series data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Stoean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stoean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Atencia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rodríguez-Labrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Joya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">1078</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b271">
	<analytic>
		<title level="a" type="main">Radial and directional posteriors for bayesian deep learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Adamczewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5298" to="5305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b272">
	<monogr>
		<title level="m" type="main">Efficient and scalable bayesian neural nets with rank-1 factors</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Dusenberry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jerfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.07186</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b273">
	<analytic>
		<title level="a" type="main">Refining the variational posterior through iterative optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b274">
	<analytic>
		<title level="a" type="main">Specifying weight priors in bayesian deep neural networks with empirical bayes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subedar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tickoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4477" to="4484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b275">
	<monogr>
		<title level="m" type="main">Identifying causal effect inference failure with uncertainty-aware models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jesson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mindermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00163</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b276">
	<monogr>
		<title level="m" type="main">Can autonomous vehicles identify, recover from, and adapt to distribution shifts</title>
		<author>
			<persName><forename type="first">A</forename><surname>Filos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tigas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.14911</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b277">
	<analytic>
		<title level="a" type="main">Meta-learning pac-bayes priors in model averaging</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4198" to="4205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b278">
	<analytic>
		<title level="a" type="main">Meta-learning by adjusting priors based on extended pac-bayes theory</title>
		<author>
			<persName><forename type="first">R</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Meir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="205" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b279">
	<monogr>
		<title level="m" type="main">Bayesian prediction of future street scenes through importance sampling based optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.06939</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b280">
	<monogr>
		<title level="m" type="main">Quality of uncertainty quantification for bayesian neural network inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.09686</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b281">
	<monogr>
		<title level="m" type="main">Calibration of model uncertainty for dropout variational inference</title>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Laves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ihler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-P</forename><surname>Kortmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ortmaier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11584</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b282">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lorch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Graule</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Pradier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06287</idno>
		<title level="m">Output-constrained bayesian neural networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b283">
	<monogr>
		<title level="m" type="main">Learned uncertainty-aware (luna) bases for bayesian regression using multi-headed auxiliary networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lorsung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yacoby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11695</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b284">
	<monogr>
		<title level="m" type="main">Learning deep bayesian latent variable regression models that generalize: When nonidentifiability is a problem</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yacoby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00569</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b285">
	<analytic>
		<title level="a" type="main">A particle-based variational approach to bayesian non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Masood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="90" to="91" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b286">
	<monogr>
		<title level="m" type="main">Cost-aware multi-objective bayesian optimisation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abdolshah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03600</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b287">
	<monogr>
		<title level="m" type="main">Bananas: Bayesian optimization with neural architectures for neural architecture search</title>
		<author>
			<persName><forename type="first">C</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Savani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11858</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b288">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Balandat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Karrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Daulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Letham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bakshy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06403</idno>
		<title level="m">Botorch: Programmable bayesian optimization in pytorch</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b289">
	<monogr>
		<title level="m" type="main">Multiclass gaussian process classification made conjugate: Efficient inference via data augmentation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Galy-Fajou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Donner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Opper</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09670</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b290">
	<monogr>
		<title level="m" type="main">Learning to balance: Bayesian meta-learning for imbalanced and out-of-distribution tasks</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12917</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b291">
	<monogr>
		<title level="m" type="main">Assessing the robustness of bayesian dark knowledge to posterior uncertainty</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Vadera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01724</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b292">
	<monogr>
		<title level="m" type="main">A deep-learning based bayesian approach to seismic imaging and uncertainty quantification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Siahkoohi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rizzuti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Herrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04567</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b293">
	<monogr>
		<title level="m" type="main">Functional variational bayesian neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.05779</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b294">
	<monogr>
		<title level="m" type="main">Deep kernel transfer in gaussian processes for fewshot learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Patacchiola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>O'boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05199</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b295">
	<analytic>
		<title level="a" type="main">A bayesian perspective on the deep image prior</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gadelha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sheldon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5443" to="5451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b296">
	<monogr>
		<title level="m" type="main">Deeply uncertain: Comparing methods of uncertainty quantification in deep learning algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Caldeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10710</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b297">
	<monogr>
		<title level="m" type="main">Uncertainty quantification in deep residual neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wandzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kr Üger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.04905</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b298">
	<monogr>
		<title level="m" type="main">Dbsn: Measuring uncertainty through bayesian learning of deep neural network structures</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09804</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b299">
	<analytic>
		<title level="a" type="main">Distributed selection of continuous features in multilabel classification using mutual information</title>
		<author>
			<persName><forename type="first">J</forename><surname>González-L Ópez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b300">
	<monogr>
		<title level="m" type="main">Meta-learning stationary stochastic process prediction with convolutional neural processes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Foong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Bruinsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01332</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b301">
	<monogr>
		<title level="m" type="main">Stacking for non-mixing bayesian computations: The curse and blessing of multimodal posteriors</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vehtari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12335</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b302">
	<monogr>
		<title level="m" type="main">A bayesian evaluation framework for ground truth-free visual recognition tasks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Prijatelj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mccurrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.06711</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b303">
	<analytic>
		<title level="a" type="main">Integrating uncertainty in deep neural networks for mri based stroke analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Murina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ürr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wegener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="page">101790</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b304">
	<analytic>
		<title level="a" type="main">Deep directional statistics: Pose estimation with uncertainty quantification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Prokudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="534" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b305">
	<monogr>
		<title level="m" type="main">Uncertainty quantification for bayesian optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.01569</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b306">
	<monogr>
		<title level="m" type="main">Variational bayesian monte carlo with noisy likelihoods</title>
		<author>
			<persName><forename type="first">L</forename><surname>Acerbi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08655</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b307">
	<analytic>
		<title level="a" type="main">Probabilistic inference of bayesian neural networks with generalized expectation propagation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b308">
	<monogr>
		<title level="m" type="main">Deep bayesian gaussian processes for uncertainty estimation in electronic health records</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hassaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Canoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Salimi-Khorshidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rahimi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10170</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b309">
	<monogr>
		<title level="m" type="main">Bayesian deep ensembles via the neural tangent kernel</title>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.05864</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b310">
	<monogr>
		<title level="m" type="main">Uncertainty quantification for sparse deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ročková</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11815</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b311">
	<analytic>
		<title level="a" type="main">Cxplain: Causal explanations for model interpretation under uncertainty</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Karlen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">230</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b312">
	<monogr>
		<title level="m" type="main">Understanding measures of uncertainty for adversarial example detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08533</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b313">
	<analytic>
		<title level="a" type="main">Reverse kl-divergence training of prior networks: Improved uncertainty and adversarial robustness</title>
		<author>
			<persName><forename type="first">A</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="520" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b314">
	<analytic>
		<title level="a" type="main">Maximizing overall diversity for improved uncertainty estimates in deep ensembles</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gifford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4264" to="4271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b315">
	<monogr>
		<title level="m" type="main">Deep sub-ensembles for fast uncertainty estimation in image classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Valdenegro-Toro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.08168</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b316">
	<monogr>
		<title level="m" type="main">A deep ensemble model with slot alignment for sequenceto-sequence natural language generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Juraska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Karagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Walker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06553</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b317">
	<analytic>
		<title level="a" type="main">Crowd counting with decomposed uncertainty</title>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Olsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Ramamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">806</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b318">
	<analytic>
		<title level="a" type="main">Uncertainty quantification in multimodal ensembles of deep learners</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Bhuiyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Talbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third International Flairs Conference</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b319">
	<monogr>
		<title level="m" type="main">Prediction intervals: Split normal mixture from quality-driven deep ensembles</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Salem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Langseth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ramampiaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.09670</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b320">
	<analytic>
		<title level="a" type="main">Improving calibration of batchensemble with data augmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jerfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Dusenberry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TWorkshop on Uncertainty and Ro-bustness in Deep Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b321">
	<monogr>
		<title level="m" type="main">Rethinking softmax with cross-entropy: Neural network classifier as mutual information estimator</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10688</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b322">
	<monogr>
		<title level="m" type="main">Quantifying intrinsic uncertainty in classification via deep dirichlet mixture networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04450</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b323">
	<analytic>
		<title level="a" type="main">Large-scale distance metric learning with uncertainty</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8542" to="8550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b324">
	<monogr>
		<title level="m" type="main">Learning sparse networks using targeted dropout</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13678</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Hinton</note>
</biblStruct>

<biblStruct xml:id="b325">
	<analytic>
		<title level="a" type="main">Predictive uncertainty estimation via prior networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7047" to="7058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b326">
	<analytic>
		<title level="a" type="main">How deep are deep gaussian processes?</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Dunlop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Girolami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Teckentrup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2100" to="2145" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b327">
	<analytic>
		<title level="a" type="main">Using self-supervised learning can improve model robustness and uncertainty</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">674</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b328">
	<analytic>
		<title level="a" type="main">Verified uncertainty calibration</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3792" to="3803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b329">
	<monogr>
		<title level="m" type="main">Augmix: A simple data processing method to improve robustness and uncertainty</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02781</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b330">
	<monogr>
		<title level="m" type="main">Inhibited softmax for uncertainty estimation in neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mo Żejko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Susik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Karczewski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01861</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b331">
	<monogr>
		<title level="m" type="main">Simultaneous perturbation stochastic approximation for few-shot learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Boiarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Granichin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Granichina</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05152</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b332">
	<analytic>
		<title level="a" type="main">An improved deep network for tissue microstructure estimation with uncertainty quantification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">101650</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b333">
	<monogr>
		<title level="m" type="main">Stochastic segmentation networks: Modelling spatially correlated aleatoric uncertainty</title>
		<author>
			<persName><forename type="first">M</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Folgoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>De Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pawlowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Der Wilk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06015</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b334">
	<analytic>
		<title level="a" type="main">Temporal logics over finite traces with uncertainty</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Maggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Montali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pe Ñaloza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">225</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b335">
	<analytic>
		<title level="a" type="main">Learning and reasoning for robot sequential decision making under uncertainty</title>
		<author>
			<persName><forename type="first">S</forename><surname>Amiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Shirazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2726" to="2733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b336">
	<monogr>
		<title level="m" type="main">Uncertaintyaware deep classifiers using generative models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sensoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cerutti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saleki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04183</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b337">
	<analytic>
		<title level="a" type="main">Uncertainty-aware search framework for multi-objective bayesian optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Belakaria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deshwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Jayakodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Doppa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">52</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b338">
	<analytic>
		<title level="a" type="main">Uncertainty aware graph gaussian process for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Z.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4957" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b339">
	<monogr>
		<title level="m" type="main">Confident learning: Estimating uncertainty in dataset labels</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Northcutt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">L</forename><surname>Chuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00068</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b340">
	<monogr>
		<title level="m" type="main">Simple domain adaptation with class prediction uncertainty alignment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Manders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Marchiori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Van Laarhoven</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04448</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b341">
	<monogr>
		<title level="m" type="main">An empirical evaluation on robustness and uncertainty of regularization methods</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03879</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b342">
	<monogr>
		<title level="m" type="main">Uncertainty evaluation metric for brain tumour segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Filos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Arbel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14262</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b343">
	<monogr>
		<title level="m" type="main">Simple and principled uncertainty estimation with deterministic deep learning via distance awareness</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Padhy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bedrax-Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10108</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b344">
	<monogr>
		<title level="m" type="main">Uncertainty quantification for data-driven turbulence modelling with mondrian forests</title>
		<author>
			<persName><forename type="first">A</forename><surname>Scillitoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Girolami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.01968</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b345">
	<analytic>
		<title level="a" type="main">Can you trust your model&apos;s uncertainty? evaluating predictive uncertainty under dataset shift</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ovadia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b346">
	<analytic>
		<title level="a" type="main">Uncertainty on asynchronous time event prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Biloš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Charpentier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ünnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">860</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b347">
	<monogr>
		<title level="m" type="main">Unsupervised scene adaptation with memory regularization in vivo</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11164</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b348">
	<monogr>
		<title level="m" type="main">Improving regression uncertainty estimates with an empirical prior</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zelikman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Healy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12496</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b349">
	<analytic>
		<title level="a" type="main">Uncertainty-aware action advising for deep reinforcement learning agents</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hernandez-Leal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kartal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5792" to="5799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b350">
	<analytic>
		<title level="a" type="main">Building calibrated deep models via uncertainty matching with auxiliary interval predictors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Thiagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-T</forename><surname>Bremer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6005" to="6012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b351">
	<analytic>
		<title level="a" type="main">Deep model-based reinforcement learning via estimated uncertainty and conservative policy optimization</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6941" to="6948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b352">
	<analytic>
		<title level="a" type="main">Visual attention through uncertainty minimization in recurrent generative models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Standvoss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Quax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Van Gerven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioRxiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b353">
	<monogr>
		<title level="m" type="main">Transferable calibration with lower bias and variance in domain adaptation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08259</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b354">
	<analytic>
		<title level="a" type="main">Uncertainty autoencoders: Learning compressed representations via variational information maximization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2514" to="2524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b355">
	<analytic>
		<title level="a" type="main">Hashing with mutual information</title>
		<author>
			<persName><forename type="first">F</forename><surname>Cakir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2424" to="2437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b356">
	<analytic>
		<title level="a" type="main">Mihash: Online hashing with mutual information</title>
		<author>
			<persName><forename type="first">F</forename><surname>Cakir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
				<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="437" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b357">
	<analytic>
		<title level="a" type="main">Ode2vae: Deep generative second order odes with bayesian neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yildiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heinonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lahdesmaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">421</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b358">
	<monogr>
		<title level="m" type="main">Functional regularisation for continual learning with gaussian processes</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Titsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G D G</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11356</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b359">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00165</idno>
		<title level="m">Deep neural networks as gaussian processes</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b360">
	<analytic>
		<title level="a" type="main">Amortized bayesian meta-learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beatson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b361">
	<analytic>
		<title level="a" type="main">Interpretable deep gaussian processes with moments</title>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shafto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="613" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b362">
	<analytic>
		<title level="a" type="main">Deep uncertainty quantification: A machine learning approach for weather forecasting</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2087" to="2095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b363">
	<monogr>
		<title level="m" type="main">Visualbased autonomous driving deployment from a stochastic and uncertainty-aware perspective</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.00821</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b364">
	<monogr>
		<title level="m" type="main">Uncertainty quantification in medical image segmentation with normalizing flows</title>
		<author>
			<persName><forename type="first">R</forename><surname>Selvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Faye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Middleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02683</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b365">
	<analytic>
		<title level="a" type="main">On the uncertainty of self-supervised monocular depth estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3227" to="3237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b366">
	<monogr>
		<title level="m" type="main">Calibrated reliable regression using maximum mean discrepancy</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10255</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b367">
	<analytic>
		<title level="a" type="main">Slang: Fast structured covariance approximations for bayesian deep learning with natural gradient</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kunstner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6245" to="6255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b368">
	<analytic>
		<title level="a" type="main">Model selection in bayesian neural networks via horseshoe priors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">182</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b369">
	<analytic>
		<title level="a" type="main">Black-box alpha divergence minimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hernandez-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1511" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b370">
	<monogr>
		<title level="m" type="main">Probabilistic framework for solving visual dialog</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Patro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Namboodiri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.04800</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b371">
	<analytic>
		<title level="a" type="main">Radial bayesian neural networks: Beyond discrete support in large-scale bayesian deep learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">stat</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b372">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02803</idno>
		<title level="m">Neural tangents: Fast and easy infinite neural networks in python</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b373">
	<monogr>
		<title level="m" type="main">Adv-bnn: Improved adversarial defense through robust bayesian neural network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01279</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b374">
	<monogr>
		<title level="m" type="main">Bayesian neural networks with maximum mean discrepancy regularization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pomponi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Scardapane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Uncini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00952</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b375">
	<monogr>
		<title level="m" type="main">Towards principled uncertainty estimation for deep neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Harang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Rudd</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12278</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b376">
	<analytic>
		<title level="a" type="main">Bayesian structure learning by recursive bootstrap</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Rohekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gurwicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nisimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Novik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="525" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b377">
	<monogr>
		<title level="m" type="main">Amortized bayesian model comparison with evidential deep learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>D'alessandro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-C</forename><surname>Ürkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">K</forename><surname>Mertens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Öthe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10629</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b378">
	<monogr>
		<title level="m" type="main">Weighting is worth the wait: Bayesian optimization with importance sampling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ariafar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Mariet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09927</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b379">
	<monogr>
		<title level="m" type="main">Training binary neural networks using the bayesian learning rule</title>
		<author>
			<persName><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bachmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Khan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10778</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b380">
	<analytic>
		<title level="a" type="main">3d semi-supervised learning with uncertaintyaware multi-view co-training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3646" to="3655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b381">
	<analytic>
		<title level="a" type="main">Generalized zero-and few-shot learning via aligned variational autoencoders</title>
		<author>
			<persName><forename type="first">E</forename><surname>Schonfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8247" to="8255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b382">
	<analytic>
		<title level="a" type="main">Bayesian model-agnostic meta-learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Dia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7332" to="7342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b383">
	<monogr>
		<title level="m" type="main">Regularizing meta-learning via gradient dropout</title>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05859</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b384">
	<monogr>
		<title level="m" type="main">Deterministic variational inference for robust bayesian neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Meeds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Gaunt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03958</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b385">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tsuchida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brintrup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neely</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06076</idno>
		<title level="m">Expressive priors in bayesian neural networks: Kernel combinations and periodic functions</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b386">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Atanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ashukha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Struminsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.06943</idno>
		<title level="m">The deep weight prior</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b387">
	<monogr>
		<title level="m" type="main">Continual learning using bayesian neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barnaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Enshaeifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ganz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04112</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b388">
	<monogr>
		<title level="m" type="main">Discriminative jackknife: Quantifying uncertainty in deep learning via higher-order influence functions</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Alaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13481</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b389">
	<analytic>
		<title level="a" type="main">Stochastic normalizations as bayesian learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shekhovtsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Flach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="463" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b390">
	<monogr>
		<title level="m" type="main">Revphiseg: A memory-efficient neural network for uncertainty quantification in medical image segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gantenbein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Erdil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.06999</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b391">
	<analytic>
		<title level="a" type="main">Physics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zabaras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Koutsourelakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perdikaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">394</biblScope>
			<biblScope unit="page" from="56" to="81" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b392">
	<monogr>
		<title level="m" type="main">Uncertainty quantification, image synthesis and deformation prediction for image registration</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>University of North Carolina at Chapel Hill</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b393">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5574" to="5584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b394">
	<monogr>
		<title level="m" type="main">Predictive uncertainty quantification with compound density networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kristiadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Däubener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01080</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b395">
	<monogr>
		<title level="m" type="main">Fast predictive uncertainty for classification with bayesian deep networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hobbhahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kristiadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hennig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.01227</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b396">
	<analytic>
		<title level="a" type="main">Modelling uncertainty in deep learning for camera relocalization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE international conference on Robotics and Automation (ICRA</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4762" to="4769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b397">
	<analytic>
		<title level="a" type="main">Google photos labeled black people&apos;gorillas</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">USA Today</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b398">
	<monogr>
		<title level="m" type="main">Unsupervised data uncertainty learning in visual retrieval systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Misu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.02586</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b399">
	<analytic>
		<title level="a" type="main">Structured uncertainty prediction networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dorta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Simpson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5477" to="5485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b400">
	<analytic>
		<title level="a" type="main">Multi-task learning based on separable formulation of depth estimation and its uncertainty</title>
		<author>
			<persName><forename type="first">A</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="21" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b401">
	<analytic>
		<title level="a" type="main">Neural rgb (r) d sensing: Depth and uncertainty from a video camera</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">995</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b402">
	<monogr>
		<title level="m" type="main">Bayesod: A bayesian approach for uncertainty estimation in deep object detectors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Smart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03838</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b403">
	<analytic>
		<title level="a" type="main">Uncertainty estimation for deep neural object detectors in safety-critical applications</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Knol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 21st International Conference on Intelligent Transportation Systems (ITSC)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3873" to="3878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b404">
	<analytic>
		<title level="a" type="main">Bounding box regression with uncertainty for accurate object detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2888" to="2897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b405">
	<analytic>
		<title level="a" type="main">Efficient uncertainty estimation for semantic segmentation in videos</title>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-T</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="520" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b406">
	<analytic>
		<title level="a" type="main">Uncertainty gated network for land cover segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seguí</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vitrià</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="276" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b407">
	<analytic>
		<title level="a" type="main">Segmentation certainty through uncertainty: Uncertainty-refined binary volumetric segmentation under multifactor domain shift</title>
		<author>
			<persName><forename type="first">C</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Potter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Korbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b408">
	<analytic>
		<title level="a" type="main">Sampling-free epistemic uncertainty estimation using approximated variance propagation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Postels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ferroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Coskun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2931" to="2940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b409">
	<analytic>
		<title level="a" type="main">Lightweight probabilistic deep networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3369" to="3378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b410">
	<monogr>
		<title level="m" type="main">A general framework for uncertainty estimation in deep learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Loquercio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06890</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b411">
	<analytic>
		<title level="a" type="main">Structured aleatoric uncertainty in human pose estimation</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Gundavarapu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b412">
	<analytic>
		<title level="a" type="main">Monoloco: Monocular 3d pedestrian localization and uncertainty estimation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b413">
	<monogr>
		<title level="m" type="main">Augmented deep representations for unconstrained still/video-based face recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>College Park</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Maryland</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b414">
	<analytic>
		<title level="a" type="main">Robust person re-identification by modelling feature uncertainty</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="552" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b415">
	<analytic>
		<title level="a" type="main">Uncertainty modeling of contextual-connections between tracklets for unconstrained video-based face recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="703" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b416">
	<analytic>
		<title level="a" type="main">Human uncertainty makes classification more robust</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Battleday</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9617" to="9626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b417">
	<monogr>
		<title level="m" type="main">Robustness of bayesian neural networks to gradient-based attacks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Carbone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Laurenti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Patane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bortolussi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sanguinetti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04359</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b418">
	<analytic>
		<title level="a" type="main">Overcoming limitations of mixture density networks: A sampling and fitting framework for multimodal future prediction</title>
		<author>
			<persName><forename type="first">O</forename><surname>Makansi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Cicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7144" to="7153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b419">
	<analytic>
		<title level="a" type="main">Long-term on-board prediction of people in traffic scenes under uncertainty</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4194" to="4202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b420">
	<analytic>
		<title level="a" type="main">Uncertainty-aware cnns for depth completion: Uncertainty from beginning to end</title>
		<author>
			<persName><forename type="first">A</forename><surname>Eldesokey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Holmquist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Persson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b421">
	<analytic>
		<title level="a" type="main">Evaluating scalable bayesian deep learning methods for robust computer vision</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">K</forename><surname>Gustafsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Schon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="318" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b422">
	<analytic>
		<title level="a" type="main">Generalizing hand segmentation in egocentric videos with uncertainty-guided model adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">401</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b423">
	<monogr>
		<title level="m" type="main">Evaluating uncertainty estimation methods on 3d semantic segmentation of point clouds</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hochgeschwender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pl Öger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valdenegro-Toro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01787</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b424">
	<monogr>
		<title level="m" type="main">Mixup-cam: Weakly-supervised semantic segmentation via uncertainty regularization</title>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01201</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b425">
	<analytic>
		<title level="a" type="main">A survey on deep learning in medical image analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Bejnordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A A</forename><surname>Setio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ciompi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghafoorian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Van Der Laak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Sánchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="60" to="88" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b426">
	<analytic>
		<title level="a" type="main">Building trust in deep learning system towards automated disease detection</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">W</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9516" to="9521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b427">
	<analytic>
		<title level="a" type="main">Analyzing the role of model uncertainty for electronic health records</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Dusenberry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jerfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Health, Inference, and Learning</title>
				<meeting>the ACM Conference on Health, Inference, and Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="204" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b428">
	<analytic>
		<title level="a" type="main">Uncertainty-aware multi-view co-training for semi-supervised medical image segmentation and domain adaptation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="page">101766</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b429">
	<analytic>
		<title level="a" type="main">Deep spectral learning for label-free optical imaging oximetry with uncertainty quantification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Light: Science &amp; Applications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b430">
	<analytic>
		<title level="a" type="main">A probabilistic u-net for segmentation of ambiguous images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>De Fauw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Ledsam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6965" to="6975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b431">
	<analytic>
		<title level="a" type="main">Dr-graduate: uncertaintyaware deep learning-based diabetic retinopathy grading in eye fundus images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ara Újo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Aresta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mendonc ¸a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Penas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Maia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Â</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Mendonc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Campilho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="page">101715</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b432">
	<analytic>
		<title level="a" type="main">Accurate and robust deep learning-based segmentation of the prostate clinical target volume in ultrasound images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Avinash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Spadinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abolmaesumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Salcudean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="186" to="196" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b433">
	<analytic>
		<title level="a" type="main">Uncertainty estimation in deep neural networks for dermoscopic image classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Combalia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hueto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malvehy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vilaplana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="744" to="745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b434">
	<monogr>
		<title level="m" type="main">Uncertainty estimation in deep 2d echocardiography segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dahal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Khanal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.09349</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b435">
	<analytic>
		<title level="a" type="main">Neurreg: Neural registration and its application to image segmentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Myronenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3617" to="3626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b436">
	<analytic>
		<title level="a" type="main">Uncertainty-aware domain alignment for anatomical structure segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="page">101732</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b437">
	<monogr>
		<title level="m" type="main">A hierarchical probabilistic u-net for modeling multi-scale ambiguities</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13077</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b438">
	<analytic>
		<title level="a" type="main">One-dimensional modeling of fractional flow reserve in coronary artery disease: Uncertainty quantification and bayesian optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods in Applied Mechanics and Engineering</title>
		<imprint>
			<biblScope unit="volume">353</biblScope>
			<biblScope unit="page" from="66" to="85" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b439">
	<monogr>
		<title level="m" type="main">On uncertainty estimation in active learning for image segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Alstrøm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.06364</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b440">
	<analytic>
		<title level="a" type="main">Bayesian recurrent neural network for language modeling</title>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Ku</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="361" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b441">
	<monogr>
		<title level="m" type="main">Uncertainty detection in natural language texts</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vincze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Ph.D. dissertation, szte</note>
</biblStruct>

<biblStruct xml:id="b442">
	<analytic>
		<title level="a" type="main">A study of active learning methods for named entity recognition in clinical text</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Lasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Denny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="11" to="18" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b443">
	<monogr>
		<title level="m" type="main">Variational smoothing in recurrent neural network language models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09296</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b444">
	<analytic>
		<title level="a" type="main">An attention-based neural framework for uncertainty identification on social media texts</title>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tsinghua Science and Technology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="126" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b445">
	<monogr>
		<title level="m" type="main">Bdlob: Bayesian deep convolutional neural networks for limit order books</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zohren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roberts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10041</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b446">
	<analytic>
		<title level="a" type="main">Wat heb je gezegd? detecting out-of-distribution translations with variational transformers</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third workshop on Bayesian Deep Learning</title>
				<imprint>
			<date type="published" when="2018">NeurIPS 2018. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b447">
	<monogr>
		<title level="m" type="main">Analyzing uncertainty in neural machine translation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00047</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b448">
	<monogr>
		<title level="m" type="main">Dirichlet uncertainty wrappers for actionable algorithm accuracy accountability and auditability</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pujol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vitrià</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12628</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b449">
	<analytic>
		<title level="a" type="main">Quantifying uncertainties in natural language processing tasks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7322" to="7329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b450">
	<analytic>
		<title level="a" type="main">Bayesian neural networks for flight trajectory prediction and safety assessment</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Decision Support Systems</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page">113246</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b451">
	<analytic>
		<title level="a" type="main">Quantifying uncertainty in discrete-continuous and skewed data with bayesian deep learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vandal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kodra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nemani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Ganguly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2377" to="2386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b452">
	<analytic>
		<title level="a" type="main">Automatic depth determination for bayesian resnets</title>
		<author>
			<persName><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third workshop on Bayesian Deep Learning</title>
				<imprint>
			<date type="published" when="2018">NeurIPS 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b453">
	<analytic>
		<title level="a" type="main">Multitask learning for segmentation of building footprints with deep neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bischke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Helber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Folz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1480" to="1484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b454">
	<monogr>
		<title level="m" type="main">Learning an uncertaintyaware object detector for autonomous driving</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thakurdesai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11375</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b455">
	<analytic>
		<title level="a" type="main">Bayesian recurrent neural network models for forecasting and quantifying uncertainty in spatial-temporal data</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Wikle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">184</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b456">
	<monogr>
		<title level="m" type="main">Comparative study of parametric and representation uncertainty modeling for recurrent neural network language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>INTERSPEECH</publisher>
			<biblScope unit="page" from="3510" to="3514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b457">
	<analytic>
		<title level="a" type="main">Quantifying model form uncertainty in reynolds-averaged turbulence models with bayesian deep neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Geneva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zabaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">383</biblScope>
			<biblScope unit="page" from="125" to="147" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b458">
	<analytic>
		<title level="a" type="main">Rough extreme learning machine: A new classification method based on uncertainty measure</title>
		<author>
			<persName><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">325</biblScope>
			<biblScope unit="page" from="269" to="282" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b459">
	<analytic>
		<title level="a" type="main">Galaxy zoo: probabilistic morphology through bayesian cnns and active learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Walmsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lintott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fortson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kruk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Masters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Scarlata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monthly Notices of the Royal Astronomical Society</title>
		<imprint>
			<biblScope unit="volume">491</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1554" to="1574" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b460">
	<monogr>
		<title level="m" type="main">Model-predictive policy learning with uncertainty regularization for driving in dense traffic</title>
		<author>
			<persName><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Canziani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02705</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b461">
	<analytic>
		<title level="a" type="main">Bayesian semi-supervised learning for uncertainty-calibrated prediction of molecular properties and active learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">35</biblScope>
			<biblScope unit="page" from="8154" to="8163" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b462">
	<monogr>
		<title level="m" type="main">Bayesian modelling in practice: Using uncertainty to improve trustworthiness in medical applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ruhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tonutti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>De Bruin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Elbers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08619</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b463">
	<analytic>
		<title level="a" type="main">Correlated uncertainty for learning dense correspondences from noisy labels</title>
		<author>
			<persName><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="920" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b464">
	<analytic>
		<title level="a" type="main">Concrete dropout</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3581" to="3590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b465">
	<analytic>
		<title level="a" type="main">Deep network uncertainty maps for indoor navigation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Verdoja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kyrki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE-RAS 19th International Conference on Humanoid Robots (Humanoids)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="112" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b466">
	<analytic>
		<title level="a" type="main">Deep learning: A bayesian perspective</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Polson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sokolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian Analysis</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1275" to="1304" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b467">
	<analytic>
		<title level="a" type="main">Bayesian deep learning based method for probabilistic forecast of dayahead electricity prices</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brusaferri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matteucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Portolani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vitali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Energy</title>
		<imprint>
			<biblScope unit="volume">250</biblScope>
			<biblScope unit="page" from="1158" to="1175" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b468">
	<analytic>
		<title level="a" type="main">Semantic segmentation of small objects and modeling of uncertainty in urban remote sensing images using deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kampffmeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-B</forename><surname>Salberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
				<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b469">
	<analytic>
		<title level="a" type="main">Using bayesian deep learning to capture uncertainty for residential net load forecasting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Strbac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Power Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="188" to="201" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b470">
	<analytic>
		<title level="a" type="main">Multi-level fusion based 3d object detection from monocular images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2345" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b471">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion using a deep learning framework</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 24th International Conference on Pattern Recognition (ICPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2705" to="2710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b472">
	<analytic>
		<title level="a" type="main">A hybrid latent space data fusion method for multimodal emotion recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nemati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rohani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Basiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Y</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Makarenkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="172" to="948" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b473">
	<analytic>
		<title level="a" type="main">Versatile multiple choice learning and its application to vision computing</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6349" to="6357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b474">
	<analytic>
		<title level="a" type="main">Mothernets: Rapid deep ensemble learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wasay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hentschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Idreos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd MLSys Conference (MLSys)</title>
				<meeting>the 3rd MLSys Conference (MLSys)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b475">
	<monogr>
		<title level="m" type="main">Confident multiple choice learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03475</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b476">
	<analytic>
		<title level="a" type="main">The power of ensembles for active learning in image classification</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Beluch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Öhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9368" to="9377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b477">
	<monogr>
		<title level="m" type="main">Exploiting uncertainties from ensemble learners to improve decisionmaking in healthcare ai</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Vincentelli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.06063</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b478">
	<monogr>
		<title level="m" type="main">Efficient ensemble model generation for uncertainty estimation with bayesian approximation in segmentation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10754</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b479">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Veeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Swiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04694</idno>
		<title level="m">Hydra: Preserving ensemble diversity for model distillation</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b480">
	<analytic>
		<title level="a" type="main">An outline of a theory of three-way decisions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Rough Sets and Current Trends in Computing</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b481">
	<monogr>
		<title level="m" type="main">Info-gap decision theory: decisions under severe uncertainty</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ben-Haim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b482">
	<monogr>
		<title level="m" type="main">Decision making under deep uncertainty: From theory to practice</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Marchau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Bloemen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Popper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Springer Nature</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b483">
	<monogr>
		<title level="m" type="main">Meta-learning probabilistic inference for prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09921</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b484">
	<analytic>
		<title level="a" type="main">Uncertainty in modelagnostic meta-learning using variational inference</title>
		<author>
			<persName><forename type="first">C</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3090" to="3100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b485">
	<monogr>
		<title level="m" type="main">Empirical bayes transductive meta-learning with synthetic gradients</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Damianou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12696</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b486">
	<monogr>
		<title level="m" type="main">Few-shot relation extraction via bayesian meta-learning on relation graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><forename type="middle">A</forename><surname>Xhonneux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02387</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b487">
	<analytic>
		<title level="a" type="main">Variational adversarial active learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5972" to="5981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b488">
	<monogr>
		<title level="m" type="main">Neural ensemble search for performant and calibrated predictions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zaidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08573</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b489">
	<monogr>
		<title level="m" type="main">Learning to classify images without labels</title>
		<author>
			<persName><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12320</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b490">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b491">
	<analytic>
		<title level="a" type="main">Ecanet: Efficient channel attention for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">542</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b492">
	<monogr>
		<title level="m" type="main">Deep anomaly detection with outlier exposure</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04606</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b493">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b494">
	<analytic>
		<title level="a" type="main">Likelihood ratios for outof-distribution detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Poplin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Depristo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">718</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b495">
	<monogr>
		<title level="m" type="main">Revisiting one-vs-all classifiers for predictive uncertainty and out-of-distribution detection in neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Padhy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.05134</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b496">
	<monogr>
		<title level="m" type="main">Sde-net: Equipping deep neural networks with uncertainty estimates</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.10546</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b497">
	<analytic>
		<title level="a" type="main">Harnessing model uncertainty for detecting adversarial examples</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wistuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-I</forename><surname>Nicolae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Bayesian Deep Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b498">
	<monogr>
		<title level="m" type="main">Sampling-free learning of bayesian quantized neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cvitkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02992</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b499">
	<analytic>
		<title level="a" type="main">Samplingfree variational inference of bayesian neural networks by variance backpropagation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Haußmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Uncertainty in Artificial Intelligence. PMLR</title>
		<imprint>
			<biblScope unit="page" from="563" to="573" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b500">
	<analytic>
		<title level="a" type="main">Probabilistic face embeddings</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6902" to="6911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b501">
	<monogr>
		<title level="m" type="main">Stochastic prototype embeddings</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ridgeway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Mozer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11702</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b502">
	<analytic>
		<title level="a" type="main">Capsule routing via variational bayes</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D S</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Leontidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Kollias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3749" to="3756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b503">
	<analytic>
		<title level="a" type="main">Open problems for online bayesian inference in neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Loftin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Macglashan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bayesian Deep Learning Workshop at NeurIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b504">
	<monogr>
		<title level="m" type="main">Variational continual learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10628</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b505">
	<analytic>
		<title level="a" type="main">Natural variational continual learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tseran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Bui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Continual Learning Workshop@ NeurIPS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b506">
	<monogr>
		<title level="m" type="main">Clinical risk prediction with temporal probabilistic asymmetric multi-task learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12777</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b507">
	<monogr>
		<title level="m" type="main">Improving backtranslation with uncertainty-based confidence estimation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00157</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b508">
	<monogr>
		<title level="m" type="main">Exploring uncertainty measures for image-caption embedding-and-retrieval task</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Matsubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Uehara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08504</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b509">
	<analytic>
		<title level="a" type="main">Unified vision-language pre-training for image captioning and vqa</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b510">
	<analytic>
		<title level="a" type="main">Meshedmemory transformer for image captioning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stefanini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">587</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b511">
	<analytic>
		<title level="a" type="main">X-linear attention networks for image captioning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b512">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b513">
	<analytic>
		<title level="a" type="main">Hierarchical questionimage co-attention for visual question answering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b514">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b515">
	<analytic>
		<title level="a" type="main">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6904" to="6913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b516">
	<analytic>
		<title level="a" type="main">Deep modular coattention networks for visual question answering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6281" to="6290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b517">
	<analytic>
		<title level="a" type="main">Learning and the unknown: Surveying steps toward open world recognition</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Dhamija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gunther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Henrydoss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9801" to="9807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b518">
	<analytic>
		<title level="a" type="main">Assumed density filtering methods for learning bayesian neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Delle Fave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Yedidia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1589" to="1595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b519">
	<analytic>
		<title level="a" type="main">Ensemble sampling</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3258" to="3266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b520">
	<monogr>
		<title level="m" type="main">Ensemble slice sampling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Beutler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06212</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b521">
	<analytic>
		<title level="a" type="main">A q-learning-based multi-agent system for data classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pourpanah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mohamad-Saleh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="519" to="531" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b522">
	<analytic>
		<title level="a" type="main">An improved fuzzy artmap and q-learning agent model for pattern classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pourpanah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">359</biblScope>
			<biblScope unit="page" from="139" to="152" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b523">
	<analytic>
		<title level="a" type="main">Learning structural weight uncertainty for sequential decision-making</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1137" to="1146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b524">
	<analytic>
		<title level="a" type="main">Deep learning under privileged information using heteroscedastic dropout</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8886" to="8895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b525">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7482" to="7491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b526">
	<analytic>
		<title level="a" type="main">Uncertainty guided multi-scale residual learning-using a cycle spinning cnn for single image deraining</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yasarla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8405" to="8414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b527">
	<analytic>
		<title level="a" type="main">Robust learning at noisy labeled medical images: Applied to skin lesion classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 16th International Symposium on Biomedical Imaging</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1280" to="1283" />
		</imprint>
	</monogr>
	<note>ISBI 2019</note>
</biblStruct>

<biblStruct xml:id="b528">
	<analytic>
		<title level="a" type="main">What&apos;s to know? uncertainty as a guide to asking goal-oriented questions</title>
		<author>
			<persName><forename type="first">E</forename><surname>Abbasnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4155" to="4164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b529">
	<analytic>
		<title level="a" type="main">Deep probabilistic regression of elements of so (3) using quaternion averaging and uncertainty injection</title>
		<author>
			<persName><forename type="first">V</forename><surname>Peretroukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wagstaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="83" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b530">
	<monogr>
		<title level="m" type="main">Probabilistic residual learning for aleatoric uncertainty in image restoration</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01010</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b531">
	<monogr>
		<title level="m" type="main">Understanding and enhancing mixed sample data augmentation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Painter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pr Ügel-Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hare</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12047</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b532">
	<analytic>
		<title level="a" type="main">Learning weighted submanifolds with variational autoencoders and riemannian variational autoencoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Miolane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Holmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">511</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b533">
	<monogr>
		<title level="m" type="main">Uncertainty-aware consistency regularization for cross-domain semantic segmentation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08878</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b534">
	<analytic>
		<title level="a" type="main">Uc-net: uncertainty inspired rgb-d saliency detection via conditional variational autoencoders</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8582" to="8591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b535">
	<analytic>
		<title level="a" type="main">Towards uncertainty-assisted brain tumor segmentation and survival prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jungo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Knecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pérez-Beteta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molina-García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Pérez-García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wiest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Brainlesion Workshop</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="474" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b536">
	<monogr>
		<title level="m" type="main">Propagating uncertainty in multi-stage bayesian convolutional neural networks with application to pulmonary nodule detection</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ozdemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Woodward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Berlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00497</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b537">
	<analytic>
		<title level="a" type="main">Bayesian image quality transfer with cnns: exploring uncertainty in dmri super-resolution</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tanno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kaden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Sotiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Alexander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="611" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b538">
	<analytic>
		<title level="a" type="main">Uncertainty quantification using bayesian neural networks in classification: Application to biomedical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Paik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics &amp; Data Analysis</title>
		<imprint>
			<biblScope unit="volume">142</biblScope>
			<biblScope unit="page">106816</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b539">
	<analytic>
		<title level="a" type="main">Test-time data augmentation for estimation of heteroscedastic aleatoric uncertainty in deep neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Ayhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Berens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st Conference on Medical Imaging with Deep Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b540">
	<analytic>
		<title level="a" type="main">On the effect of inter-observer variability for a reliable estimation of uncertainty of medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jungo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ermis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blatti-Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wiest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="682" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b541">
	<analytic>
		<title level="a" type="main">Interactive medical image segmentation using deep learning with imagespecific fine tuning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Zuluaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aertsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Doel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deprest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1562" to="1573" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b542">
	<analytic>
		<title level="a" type="main">Uncertainty-aware organ classification for surgical data science applications in laparoscopy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Moccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wirkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kenngott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Vemuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Apitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>De Momi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Mattos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2649" to="2659" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b543">
	<analytic>
		<title level="a" type="main">Assessing reliability and challenges of uncertainty estimations for medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jungo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="48" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b544">
	<analytic>
		<title level="a" type="main">U2-net: A bayesian u-net model with epistemic uncertainty feedback for photoreceptor layer segmentation in pathological oct scans</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Orlando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Seeb Öck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bogunović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Klimscha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Grechenig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Gerendas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 16th International Symposium on Biomedical Imaging</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1441" to="1445" />
		</imprint>
	</monogr>
	<note>ISBI 2019</note>
</biblStruct>

<biblStruct xml:id="b545">
	<analytic>
		<title level="a" type="main">Quantifying and leveraging classification uncertainty for chest radiograph assessment</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Ghesu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Kalra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Digumarthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grbic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="676" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b546">
	<analytic>
		<title level="a" type="main">Phiseg: Capturing uncertainty in medical image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Tezcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Ötker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">J</forename><surname>Muehlematter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schawkat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Donati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="119" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b547">
	<analytic>
		<title level="a" type="main">Ara: accurate, reliable and active histopathological image classification framework with bayesian deep learning</title>
		<author>
			<persName><forename type="first">Ł</forename><surname>Raczkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mo Żejko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zambonelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Szczurek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b548">
	<analytic>
		<title level="a" type="main">As easy as 1, 2... 4? uncertainty in counting tasks for medical imaging</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Eaton-Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Varsavsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="356" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b549">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Di Scandalea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Perone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boudreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen-Adad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05143</idno>
		<title level="m">Deep active learning for axon-myelin segmentation on histology data</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b550">
	<monogr>
		<title level="m" type="main">Benchmarking bayesian deep learning with diabetic retinopathy diagnosis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Filos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Rudner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kenton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Kroon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b551">
	<monogr>
		<title level="m" type="main">Uncertainty-driven semantic segmentation through humanmachine collaborative learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nabi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00626</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b552">
	<analytic>
		<title level="a" type="main">A bayesian neural net to segment images with uncertainty estimates and good calibration</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Awate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Processing in Medical Imaging</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b553">
	<monogr>
		<title level="m" type="main">Uncertainty quantification in deep learning for safer neuroimage enhancement</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tanno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kaden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Grussu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bizzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Sotiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Alexander</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13418</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b554">
	<monogr>
		<title level="m" type="main">Uncertainty-based graph convolutional networks for organ segmentation refinement</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Soberanis-Mukul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02191</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b555">
	<analytic>
		<title level="a" type="main">Supervised uncertainty quantification for segmentation with multiple annotations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Knegt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Veeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="137" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b556">
	<analytic>
		<title level="a" type="main">Commensal correlation network between segmentation and direct area estimation for biventricle quantification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Howey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ohorodnyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">101591</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b557">
	<analytic>
		<title level="a" type="main">An exploration of uncertainty information for segmentation quality assessment</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hoebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Depeursinge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Imaging 2020: Image Processing</title>
				<imprint>
			<publisher>International Society for Optics and Photonics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">11313</biblScope>
			<biblScope unit="page">113131K</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b558">
	<analytic>
		<title level="a" type="main">Exploiting epistemic uncertainty of anatomy segmentation for anomaly detection in retinal oct</title>
		<author>
			<persName><forename type="first">P</forename><surname>Seeb Öck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Orlando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bogunović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Klimscha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="98" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b559">
	<analytic>
		<title level="a" type="main">Automated muscle segmentation from clinical ct using bayesian u-net for personalized musculoskeletal modeling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hiasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Otake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Takao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1030" to="1040" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b560">
	<analytic>
		<title level="a" type="main">Reliable deep-learning-based phase imaging with uncertainty quantification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optica</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="618" to="629" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b561">
	<monogr>
		<title level="m" type="main">We know where we don&apos;t know: 3d bayesian cnns for uncertainty quantification of binary segmentations for material simulations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Labonte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Roberts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10793</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b562">
	<analytic>
		<title level="a" type="main">On modelling label uncertainty in deep neural networks: Automatic estimation of intraobserver variability in 2d echocardiography quality assessment</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Girgis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vaseli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hetherington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rohling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abolmaesumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1868" to="1883" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b563">
	<analytic>
		<title level="a" type="main">Direct uncertainty prediction for medical second opinions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Blumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sayres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Obermeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mullainathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5281" to="5290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b564">
	<analytic>
		<title level="a" type="main">Reducing uncertainty in undersampled mri reconstruction with active acquisition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Muckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2049" to="2058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b565">
	<monogr>
		<title level="m" type="main">A bayesian hierarchical network for combining heterogeneous data sources in medical diagnoses</title>
		<author>
			<persName><forename type="first">C</forename><surname>Donnat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Miolane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D S P</forename><surname>Bunbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kreindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13847</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b566">
	<analytic>
		<title level="a" type="main">Confidence calibration and predictive uncertainty estimation for deep medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mehrtash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Tempany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abolmaesumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kapur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b567">
	<analytic>
		<title level="a" type="main">Uncertainty and interpretability in convolutional neural networks for semantic segmentation of colorectal polyps</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wickstrøm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kampffmeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">101619</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b568">
	<analytic>
		<title level="a" type="main">Deep learning uncertainty and confidence calibration for the five-class polyp classification from colonoscopy</title>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Z C T</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Burt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="page">101653</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b569">
	<analytic>
		<title level="a" type="main">Demystifying brain tumor segmentation networks: Interpretability and uncertainty analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Natekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krishnamurthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Computational Neuroscience</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b570">
	<analytic>
		<title level="a" type="main">Efficient shapley explanation for features importance estimation under uncertainty</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Dvornek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ventola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="792" to="801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b571">
	<analytic>
		<title level="a" type="main">Cartilage segmentation in high-resolution 3d micro-ct images via uncertainty-guided selftraining with very sparse annotation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M M</forename><surname>Perrine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Pitirri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kawasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Richtsmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="802" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b572">
	<analytic>
		<title level="a" type="main">Deep reinforcement active learning for medical image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="33" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b573">
	<analytic>
		<title level="a" type="main">An effective data refinement approach for upper gastrointestinal anatomy recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b574">
	<analytic>
		<title level="a" type="main">Few is enough: Task-augmented active meta-learning for brain cell classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mobiny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jahanipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Cicalese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Roysam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dragan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="367" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b575">
	<analytic>
		<title level="a" type="main">Harnessing uncertainty in domain adaptation for mri prostate lesion segmentation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Giganti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Punwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Panagiotaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="510" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b576">
	<analytic>
		<title level="a" type="main">Double-uncertainty weighted method for semisupervised learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="542" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b577">
	<analytic>
		<title level="a" type="main">Self-loop uncertainty: A novel pseudo-label for semi-supervised medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="614" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b578">
	<analytic>
		<title level="a" type="main">Deep q-network-driven catheter segmentation in 3d us by hybrid constrained semisupervised learning and dual-unet</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Kolen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="646" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b579">
	<analytic>
		<title level="a" type="main">Uncertainty estimates as data selection criteria to boost omni-supervised learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Venturini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Papageorghiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Namburete</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="689" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b580">
	<analytic>
		<title level="a" type="main">Difficulty-aware glaucoma classification with multi-rater consensus modeling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="741" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b581">
	<analytic>
		<title level="a" type="main">Heterogeneity measurement of cardiac tissues leveraging uncertainty information from image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Angelini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hendon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="782" to="791" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
