<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DEEP LEARNING WITH LOGGED BANDIT FEEDBACK</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Adith</forename><surname>Swaminathan</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Maarten de Rijke University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DEEP LEARNING WITH LOGGED BANDIT FEEDBACK</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new output layer for deep neural networks that permits the use of logged contextual bandit feedback for training. Such contextual bandit feedback can be available in huge quantities (e.g., logs of search engines, recommender systems) at little cost, opening up a path for training deep networks on orders of magnitude more data. To this effect, we propose a counterfactual risk minimization approach for training deep networks using an equivariant empirical risk estimator with variance regularization, BanditNet, and show how the resulting objective can be decomposed in a way that allows stochastic gradient descent training. We empirically demonstrate the effectiveness of the method by showing how deep networks -ResNets in particular -can be trained for object recognition without conventionally labeled images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Log data can be recorded from online systems such as search engines, recommender systems, or online stores at little cost and in huge quantities. For concreteness, consider the interaction logs of an ad-placement system for banner ads. Such logs typically contain a record of the input to the system (e.g., features describing the user, banner ad, and page), the action that was taken by the system (e.g., a specific banner ad that was placed) and the feedback furnished by the user (e.g., clicks on the ad, or monetary payoff). This feedback, however, provides only partial information -"contextual-bandit feedback" -limited to the actions taken by the system. We do not get to see how the user would have responded, if the system had chosen a different action (e.g., other ads or banner types). Thus, the feedback for all other actions the system could have taken is typically not known. This makes learning from log data fundamentally different from traditional supervised learning, where "correct" predictions and a loss function provide feedback for all actions.</p><p>In this paper, we propose a new output layer for deep neural networks that allows training on logged contextual bandit feedback. By circumventing the need for full-information feedback, our approach opens a new and intriguing pathway for acquiring knowledge at unprecedented scale, giving deep neural networks access to this abundant and ubiquitous type of data. Similarly, it enables the application of deep learning even in domains where manually labeling full-information feedback is not viable.</p><p>In contrast to online learning with contextual bandit feedback (e.g., <ref type="bibr" target="#b19">(Williams, 1992;</ref><ref type="bibr" target="#b0">Agarwal et al., 2014</ref>)), we perform batch learning from bandit feedback (BLBF) <ref type="bibr" target="#b1">(Beygelzimer &amp; Langford, 2009;</ref><ref type="bibr" target="#b13">Swaminathan &amp; Joachims, 2015a;</ref><ref type="bibr">b;</ref><ref type="bibr">c)</ref> and the algorithm does not require the ability to make interactive interventions. At the core of the new output layer for BLBF training of deep neural networks lies a counterfactual training objective that replaces the conventional cross-entropy objective. Our approach -called BanditNet -follows the view of a deep neural network as a stochastic policy. We propose a counterfactual risk minimization (CRM) objective that is based on an equivariant estimator of the true error that only requires propensity-logged contextual bandit feedback. This makes our training objective fundamentally different from the conventional cross-entropy objective for supervised classification, which requires full-information feedback. Equivariance in our context means that the learning result is invariant to additive translations of the loss, and it is more formally defined in Section 3.2. To enable large-scale training, we show how this training objective can be decomposed to allow stochastic gradient descent (SGD) optimization.</p><p>In addition to the theoretical derivation of BanditNet, we present an empirical evaluation that verifies the applicability of the theoretical argument. It demonstrates how a deep neural network architec-ture can be trained in the BLBF setting. In particular, we derive a BanditNet version of ResNet <ref type="bibr" target="#b6">(He et al., 2016)</ref> for visual object classification. Despite using potentially much cheaper data, we find that Bandit-ResNet can achieve the same classification performance given sufficient amounts of contextual bandit feedback as ResNet trained with cross-entropy on conventionally (full-information) annotated images. To easily enable experimentation on other applications, we share an implementation of BanditNet.<ref type="foot" target="#foot_0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Several recent works have studied weak supervision approaches for deep learning. Weak supervision has been used to pre-train good image features <ref type="bibr" target="#b8">(Joulin et al., 2016)</ref> and for information retrieval <ref type="bibr" target="#b3">(Dehghani et al., 2017)</ref>. Closely related works have studied label corruption on CIFAR-10 recently <ref type="bibr" target="#b20">(Zhang et al., 2016)</ref>. However, all these approaches use weak supervision/corruption to construct noisy proxies for labels, and proceed with traditional supervised training (using crossentropy or mean-squared-error loss) with these proxies. In contrast, we work in the BLBF setting, which is an orthogonal data-source, and modify the loss functions optimized by deep nets to directly implement risk minimization.</p><p>Virtually all previous methods that can learn from logged bandit feedback employ some form of risk minimization principle <ref type="bibr" target="#b17">(Vapnik, 1998)</ref> over a model class. Most of the methods <ref type="bibr" target="#b1">(Beygelzimer &amp; Langford, 2009;</ref><ref type="bibr" target="#b2">Bottou et al., 2013;</ref><ref type="bibr" target="#b13">Swaminathan &amp; Joachims, 2015a</ref>) employ an inverse propensity scoring (IPS) estimator <ref type="bibr" target="#b11">(Rosenbaum &amp; Rubin, 1983)</ref> as empirical risk and use stochastic gradient descent (SGD) to optimize the estimate over large datasets. Recently, the self-normalized estimator <ref type="bibr" target="#b16">(Trotter &amp; Tukey, 1956)</ref> has been shown to be a more suitable estimator for BLBF <ref type="bibr" target="#b15">(Swaminathan &amp; Joachims, 2015c)</ref>. The self-normalized estimator, however, is not amenable to stochastic optimization and scales poorly with dataset size. In our work, we demonstrate how we can efficiently optimize a reformulation of the self-normalized estimator using SGD.</p><p>Previous BLBF methods focus on simple model classes: log-linear and exponential models (Swaminathan &amp; Joachims, 2015a) or tree-based reductions <ref type="bibr" target="#b1">(Beygelzimer &amp; Langford, 2009</ref>). In contrast, we demonstrate how current deep learning models can be trained effectively via batch learning from bandit feedback (BLBF), and compare these with existing approaches on a benchmark dataset <ref type="bibr" target="#b9">(Krizhevsky &amp; Hinton, 2009)</ref>.</p><p>Our work, together with independent concurrent work <ref type="bibr" target="#b12">(Serban et al., 2017)</ref>, demonstrates success with off-policy variants of the REINFORCE <ref type="bibr" target="#b19">(Williams, 1992)</ref> algorithm. In particular, our algorithm employs a Lagrangian reformulation of the self-normalized estimator, and the objective and gradients of this reformulation are similar in spirit to the updates of the REINFORCE algorithm. This connection sheds new light on the role of the baseline hyper-parameters in REINFORCE: rather than simply reduce the variance of policy gradients, our work proposes a constructive algorithm for selecting the baseline in the off-policy setting and it suggests that the baseline is instrumental in creating an equivariant counterfactual learning objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BANDITNET: COUNTERFACTUAL RISK MINIMIZATION FOR DEEP NETS</head><p>To formalize the problem of batch learning from bandit feedback for deep neural networks, consider the contextual bandit setting where a policy π takes as input x ∈ X and outputs an action y ∈ Y.</p><p>In response, we observe the loss (or payoff) δ(x, y) of the selected action y, where δ(x, y) is an arbitrary (unknown) function that maps actions and contexts to a bounded real number. For example, in display advertising, the context x could be a representation of the user and page, y denotes the displayed ad, and δ(x, y) could be the monetary payoff from placing the ad (zero if no click, or dollar amount if clicked). The contexts are drawn i.i.d. from a fixed but unknown distribution Pr(X).</p><p>In this paper, a (deep) neural network is viewed as implementing a stochastic policy π. We can think of such a network policy as a conditional distribution π w (Y | x) over actions y ∈ Y , where w are the parameters of the network. The network makes a prediction by sampling an action y ∼ π w (Y | x), where deterministic π w (Y | x) are a special case. As we will show as part of the empirical evaluation, many existing network architectures are compatible with this stochastic-policy view. For example, any network f w (x, y) with a softmax output layer</p><formula xml:id="formula_0">π w (y | x) = exp(f w (x, y))</formula><p>y ∈Y exp(f w (x, y ))</p><p>(1) can be re-purposed as a conditional distribution from which one can sample actions, instead of interpreting it as a conditional likelihood like in full-information supervised learning.</p><p>The goal of learning is to find a policy π w that minimizes the risk (analogously: maximizes the payoff) defined as</p><formula xml:id="formula_1">R(π w ) = E x∼Pr(X) E y∼πw(Y |x) [δ(x, y)].<label>(2)</label></formula><p>Any data collected from an interactive system depends on the policy π 0 that was running on the system at the time, determining which actions y and losses δ(x, y) are observed. We call π 0 the logging policy, and for simplicity assume that it is stationary. The logged data D are n tuples of observed context x i ∼ Pr(X), action y i ∼ π 0 (Y | x i ) taken by the logging policy, the probability of this action p i ≡ π 0 (y i | x i ), which we call the propensity, and the received loss δ i ≡ δ(x i , y i ):</p><formula xml:id="formula_2">D = [(x 1 , y 1 , p 1 , δ 1 ) , . . . , (x n , y n , p n , δ n )] .<label>(3)</label></formula><p>We will now discuss how we can use this logged contextual bandit feedback to train a neural network policy π w (Y | x) that has low risk R(π w ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">COUNTERFACTUAL RISK MINIMIZATION</head><p>While conditional maximum likelihood is a standard approach for training deep neural networks, it requires that the loss δ(x i , y) is known for all y ∈ Y. However, we only know δ(x i , y i ) for the particular y i chosen by the logging policy π 0 . We therefore take a different approach following <ref type="bibr" target="#b10">(Langford et al., 2008;</ref><ref type="bibr" target="#b14">Swaminathan &amp; Joachims, 2015b)</ref>, where we directly minimize an empirical risk that can be estimated from the logged bandit data D. This approach is called counterfactual risk minimization (CRM) <ref type="bibr" target="#b14">(Swaminathan &amp; Joachims, 2015b)</ref>, since for any policy π w it addresses the counterfactual question of how well that policy would have performed, if it had been used instead of π 0 .</p><p>While minimizing an empirical risk as an estimate of the true risk R(π w ) is a common principle in machine learning <ref type="bibr" target="#b17">(Vapnik, 1998)</ref>, getting a reliable estimate based on the training data D produced by π 0 is not straightforward. The logged bandit data D is not only incomplete (i.e., we lack knowledge of δ(x i , y) for many y ∈ Y that π w would have chosen differently from π 0 ), but it is also biased (i.e., the actions preferred by π 0 are over-represented). This is why existing work on training deep neural networks either requires full knowledge of the loss function, or requires the ability to interactively draw new samples y i ∼ π w (Y | x i ) for any new policy π w . In our setting we can do neither -we have a fixed dataset D that is limited to samples from π 0 .</p><p>To nevertheless get a useful estimate of the empirical risk, we explicitly address both the bias and the variance of the risk estimate. To correct for sampling bias and handle missing data, we approach the risk estimation problem using importance sampling and thus remove the distribution mismatch between π 0 and π w <ref type="bibr" target="#b10">(Langford et al., 2008;</ref><ref type="bibr">Owen, 2013;</ref><ref type="bibr" target="#b14">Swaminathan &amp; Joachims, 2015b)</ref>:</p><formula xml:id="formula_3">R(π w ) = E x∼Pr(X) E y∼πw(Y |x) [δ(x, y)] = E x∼Pr(X) E y∼π0(Y |x) δ(x, y) π w (y | x) π 0 (y | x) . (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>The latter expectation can be estimated on a sample D of n bandit-feedback examples using the following IPS estimator <ref type="bibr" target="#b10">(Langford et al., 2008;</ref><ref type="bibr">Owen, 2013;</ref><ref type="bibr" target="#b14">Swaminathan &amp; Joachims, 2015b)</ref>:</p><formula xml:id="formula_5">RIPS (π w ) = 1 n n i=1 δ i π w (y i | x i ) π 0 (y i | x i ) . (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>This IPS estimator is unbiased and has bounded variance, if the logging policy has full support in the sense that ∀x, y : π 0 (y | x) ≥ &gt; 0. While at first glance it may seem natural to directly train the parameters w of a network to optimize this IPS estimate as an empirical risk, there are at least three obstacles to overcome. First, we will argue in the following section that the naive IPS estimator's lack of equivariance makes it sub-optimal for use as an empirical risk for high-capacity models. Second, we have to find an efficient algorithm for minimizing the empirical risk, especially making it accessible to stochastic gradient descent (SGD) optimization. And, finally, we are faced with an unusual type of bias-variance trade-off since "distance" from the exploration policy impacts the variance of the empirical risk estimate for different w.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">EQUIVARIANT COUNTERFACTUAL RISK MINIMIZATION</head><p>While Eq. ( <ref type="formula" target="#formula_5">5</ref>) provides an unbiased empirical risk estimate, it exhibits the -possibly severe -problem of "propensity overfitting" when directly optimized within a learning algorithm <ref type="bibr" target="#b15">(Swaminathan &amp; Joachims, 2015c)</ref>. It is a problem of overfitting to the choices y i of the logging policy, and it occurs on top of the normal overfitting to the δ i . Propensity overfitting is linked to the lack of equivariance of the IPS estimator: while the minimizer of true risk R(π w ) does not change when translating the loss by a constant (i.e., ∀x, y : δ(x, y) + c) by linearity of expectation,</p><formula xml:id="formula_7">c + min w E x∼Pr(X) E y∼πw(Y |x) [δ(x, y)] = min w E x∼Pr(X) E y∼πw(Y |x) [δ(x, y) + c]<label>(6)</label></formula><p>the minimizer of the IPS-estimated empirical risk RIPS (π w ) can change dramatically for finite training samples, and</p><formula xml:id="formula_8">c + min w 1 n n i=1 δ i π w (y i | x i ) π 0 (y i | x i ) = min w 1 n n i=1 (δ i + c) π w (y i | x i ) π 0 (y i | x i ) .<label>(7)</label></formula><p>Intuitively, when c shifts losses to be positive numbers, policies π w that put as little probability mass as possible on the observed actions have low risk estimates. If c shifts the losses to the negative range, the exact opposite is the case. For either choice of c, the choice of the policy eventually selected by the learning algorithm can be dominated by where π 0 happens to sample data, not by which actions have low loss.</p><p>The following self-normalized IPS estimator (SNIPS) addresses the propensity overfitting problem <ref type="bibr" target="#b15">(Swaminathan &amp; Joachims, 2015c)</ref> and is equivariant:</p><formula xml:id="formula_9">RSNIPS (π w ) = 1 n n i=1 δ i πw(yi|xi) π0(yi|xi) 1 n n i=1 πw(yi|xi) π0(yi|xi) .<label>(8)</label></formula><p>In addition to being equivariant, this estimate can also have substantially lower variance than Eq. ( <ref type="formula" target="#formula_5">5</ref>), since it exploits the knowledge that the denominator</p><formula xml:id="formula_10">S := 1 n n i=1 π w (y i | x i ) π 0 (y i | x i )<label>(9)</label></formula><p>always has expectation 1:</p><formula xml:id="formula_11">E[S] = 1 n n i=1 π w (y i | x i ) π 0 (y i | x i ) π 0 (y i | x i ) Pr(x i )dy i dx i = 1 n n i=1 1 Pr(x i )dx i = 1. (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>The SNIPS estimator uses this knowledge as a multiplicative control variate <ref type="bibr" target="#b15">(Swaminathan &amp; Joachims, 2015c)</ref>. While the SNIPS estimator has some bias, this bias asymptotically vanishes at a rate of O( 1 n ) <ref type="bibr" target="#b7">(Hesterberg, 1995)</ref>. Using the SNIPS estimator as our empirical risk implies that we need to solve the following optimization problem for training:</p><formula xml:id="formula_13">ŵ = arg min w∈ N RSNIPS (π w ).<label>(11)</label></formula><p>Thus, we now turn to designing efficient optimization methods for this training objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">TRAINING ALGORITHM</head><p>Unfortunately, the training objective in Eq. ( <ref type="formula" target="#formula_13">11</ref>) does not permit stochastic gradient descent (SGD) optimization in the given form (see Appendix C), which presents an obstacle to efficient and effective training of the network. To remedy this problem, we will now develop a reformulation that retains both the desirable properties of the SNIPS estimator, as well as the ability to reuse established SGD training algorithms. Instead of optimizing a ratio as in Eq. ( <ref type="formula" target="#formula_13">11</ref>), we will reformulate the problem into a series of constrained optimization problems. Let ŵ be a solution of Eq. ( <ref type="formula" target="#formula_13">11</ref>), and at that solution let S * be the value of the control variate for π ŵ as defined in Eq. ( <ref type="formula" target="#formula_10">9</ref>). For simplicity, assume that the minimizer ŵ is unique. If we knew S * , we could equivalently solve the following constrained optimization problem:</p><formula xml:id="formula_14">ŵ = arg min w∈ N 1 n n i=1 δ i π w (y i | x i ) π 0 (y i | x i ) subject to 1 n n i=1 π w (y i | x i ) π 0 (y i | x i ) = S * . (<label>12</label></formula><formula xml:id="formula_15">)</formula><p>Of course, we do not actually know S * . However, we can do a grid search in {S 1 , . . . , S k } for S * and solve the above optimization problem for each value, giving us a set of solutions { ŵ1 , . . . , ŵk }. Note that S is just a one-dimensional quantity, and that the sensible range we need to search for S * concentrates around 1 as n increases (see Appendix B). To find the overall (approximate) ŵ that optimizes the SNIPS estimate, we then simply take the minimum:</p><formula xml:id="formula_16">ŵ = arg min ( ŵj ,Sj ) 1 n n i=1 δ i π ŵj (yi|xi) π0(yi|xi) S j . (<label>13</label></formula><formula xml:id="formula_17">)</formula><p>This still leaves the question of how to solve each equality constrained risk minimization problem using SGD. Fortunately, we can perform an equivalent search for S * without constrained optimization. To this effect, consider the Lagrangian of the constrained optimization problem in Eq. ( <ref type="formula" target="#formula_14">12</ref>) with S j in the constraint instead of S * :</p><formula xml:id="formula_18">L(w, λ) = 1 n n i=1 δ i π w (y i | x i ) π 0 (y i | x i ) −λ 1 n n i=1 π w (y i | x i ) π 0 (y i | x i ) −S j = 1 n n i=1 (δ i −λ)π w (y i | x i ) π 0 (y i | x i ) +λS j .</formula><p>The variable λ is an unconstrained Lagrange multiplier. To find the minimum of Eq. ( <ref type="formula" target="#formula_14">12</ref>) for a particular S j , we need to minimize L(w, λ) w.r.t. w and maximize w.r.t. λ. ŵj = arg min</p><formula xml:id="formula_19">w∈ N max λ L(w, λ)<label>(14)</label></formula><p>However, we are not actually interested in the constrained solution of Eq. ( <ref type="formula" target="#formula_14">12</ref>) for any specific S j . We are merely interested in exploring a certain range S ∈ [S 1 , S k ] in our search for S * . So, we can reverse the roles of λ and S, where we keep λ fixed and determine the corresponding S in hindsight.</p><p>In particular, for each {λ 1 , . . . , λ k } we solve ŵj = arg min</p><formula xml:id="formula_20">w∈ N L(w, λ j ).<label>(15)</label></formula><p>Note that the solution ŵj does not depend on S j , so we can compute S j after we have found the minimum ŵj . In particular, we can determine the S j that corresponds to the given λ j using the necessary optimality conditions,</p><formula xml:id="formula_21">∂L ∂w = 1 n n i=1 ∂π w (y i | x i ) ∂w (δ i − λ j ) π 0 (y i | x i ) = 0 and ∂L ∂λ j = 1 n n i=1 π w (y i | x i ) π 0 (y i | x i ) − S j = 0,<label>(16)</label></formula><p>by solving the second equality of Eq. ( <ref type="formula" target="#formula_21">16</ref>). In this way, the sequence of λ j produces solutions ŵj corresponding to a sequence of {S 1 , . . . , S k }.</p><p>To identify the sensible range of S to explore, we can make use of the fact that Eq. ( <ref type="formula" target="#formula_10">9</ref>) concentrates around its expectation of 1 for each π w as n increases. Theorem 2 in Appendix B provides a characterization of how large the range needs to be. Furthermore, we can steer the exploration of S via λ, since the resulting S changes monotonically with λ: (λ a &lt; λ b ) and ( ŵa = ŵb are not equivalent optima in Eq. ( <ref type="formula" target="#formula_20">15</ref>)) ⇒ (S a &lt; S b ).</p><p>(17) A more formal statement and proof are given as Theorem 1 in Appendix A. In the simplest form one could therefore perform a grid search on λ, but more sophisticated search methods are possible too.</p><p>After this reformulation, the key computational problem is finding the solution of Eq. ( <ref type="formula" target="#formula_20">15</ref>) for each λ j . Note that in this unconstrained optimization problem, the Lagrange multiplier effectively translates the loss values in the conventional IPS estimate: We denote this λ-translated IPS estimate with Rλ IPS (π w ). Note that each such optimization problem is now in the form required for SGD, where we merely weight the derivative of the stochastic policy network π w (y | x) by a factor (δ i − λ j )/π 0 (y i | x i ). This opens the door for re-purposing existing fast methods for training deep neural networks, and we demonstrate experimentally that SGD with momentum is able to optimize our objective scalably. Similar loss translations have previously been used in on-policy reinforcement learning <ref type="bibr" target="#b19">(Williams, 1992)</ref>, where they are motivated as minimizing the variance of the gradient estimate <ref type="bibr" target="#b18">(Weaver &amp; Tao, 2001;</ref><ref type="bibr" target="#b5">Greensmith et al., 2004)</ref>. However, the situation is different in the off-policy setting we consider. First, we cannot sample new roll-outs from the current policy under consideration, which means we cannot use the standard variance-optimal estimator used in REINFORCE. Second, we tried using the (estimated) expected loss of the learned policy as the baseline as is commonly done in REINFORCE, but will see in the experiment section that this value for λ is far from optimal. Finally, it is unclear whether gradient variance, as opposed to variance of the ERM objective, is really the key issue in batch learning from bandit feedback. In this sense, our approach provides a rigorous justification and a constructive way of picking the value of λ in the off-policy settingnamely the value for which the corresponding S j minimizes Eq. ( <ref type="formula" target="#formula_16">13</ref>). In addition, one can further add variance regularization <ref type="bibr" target="#b14">(Swaminathan &amp; Joachims, 2015b)</ref> to improve the robustness of the risk estimate in Eq. ( <ref type="formula" target="#formula_22">18</ref>) (see Appendix D for details).</p><formula xml:id="formula_22">ŵj = arg min w 1 n n i=1 (δ i − λ j ) π w (y i | x i ) π 0 (y i | x i ) = arg min w Rλj IPS (π w ). (<label>18</label></formula><formula xml:id="formula_23">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EMPIRICAL EVALUATION</head><p>The empirical evaluation is designed to address three key questions. First, it verifies that deep models can indeed be trained effectively using our approach. Second, we will compare how the same deep neural network architecture performs under different types of data and training objectives -in particular, conventional cross-entropy training using full-information data. In order to be able to do this comparison, we focus on synthetic contextual bandit feedback data for training BanditNet that is sampled from the full-information labels. Third, we explore the effectiveness and fidelity of the approximate SNIPS objective.</p><p>For the following BanditNet experiments, we adapted the ResNet20 architecture <ref type="bibr" target="#b6">(He et al., 2016)</ref> by replacing the conventional cross-entropy objective with our counterfactual risk minimization objective. We evaluate the performance of this Bandit-ResNet on the CIFAR-10 ( <ref type="bibr" target="#b9">Krizhevsky &amp; Hinton, 2009)</ref> dataset, where we can compare training on full-information data with training on bandit feedback, and where there is a full-information test set for estimating prediction error.</p><p>To simulate logged bandit feedback, we perform the standard supervised to bandit conversion <ref type="bibr" target="#b1">(Beygelzimer &amp; Langford, 2009)</ref>. We use a hand-coded logging policy that achieves about 49% error rate on the training data, which is substantially worse than what we hope to achieve after learning. This emulates a real world scenario where one would bootstrap an operational system with a mediocre policy (e.g., derived from a small hand-labeled dataset) and then deploys it to log bandit feedback. This logged bandit feedback data is then used to train the Bandit-ResNet.</p><p>We evaluate the trained model using error rate on the held out (full-information) test set. We compare this model against the skyline of training a conventional ResNet using the full-information feedback  from the 50,000 training examples. Both the conventional full-information ResNet as well as the Bandit-ResNet use the same network architecture, the same hyperparameters, the same data augmentation scheme, and the same optimization method that were set in the CNTK implementation of ResNet20. Since CIFAR10 does not come with a validation set for tuning the variance-regularization constant γ, we do not use variance regularization for Bandit-ResNet. The Lagrange multiplier λ ∈ {0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0, 1.05} is selected on the training set via Eq. ( <ref type="formula" target="#formula_16">13</ref>). The only parameter we adjusted for Bandit-ResNet is lowering the learning rate to 0.1 and slowing down the learning rate schedule. The latter was done to avoid confounding the Bandit-ResNet results with potential effects from early stopping, and we report test performance after 1000 training epochs, which is well beyond the point of convergence in all runs.</p><p>Learning curve. Figure <ref type="figure" target="#fig_0">1</ref> shows the prediction error of the Bandit-ResNet as more and more bandit feedback is provided for training. First, even though the logging policy that generated the bandit feedback has an error rate of 49%, the prediction error of the policy learned by the Bandit-ResNet is substantially better. It is between 13% and 8.2%, depending on the amount of training data. Second, the horizontal line is the performance of a conventional ResNet trained on the full-information training set. It serves as a skyline of how good Bandit-ResNet could possibly get given that it is sampling bandit feedback from the same full-information training set. The learning curve in Figure <ref type="figure" target="#fig_0">1</ref> shows that Bandit-ResNet converges to the skyline performance given enough bandit feedback training data, providing strong evidence that our training objective and method can effectively extract the available information provided in the bandit feedback.</p><p>Effect of the choice of Lagrange multiplier. The left-hand plot in Figure <ref type="figure" target="#fig_1">2</ref> shows the test error of solutions ŵj depending on the value of the Lagrange multiplier λ j used during training. It shows that λ in the range 0.8 to 1.0 results in good prediction performance, but that performance degrades outside this area. The SNIPS estimates in the right-hand plot of Figure <ref type="figure" target="#fig_1">2</ref> roughly reflects this optimal range, given empirical support for both the SNIPS estimator and the use of Eq. ( <ref type="formula" target="#formula_16">13</ref>).</p><p>We also explored two other methods for selecting λ. First, we used the straightforward IPS estimator as the objective (i.e., λ = 0), which leads to prediction performance worse than that of the logging policy (not shown). Second, we tried using the (estimated) expected loss of the learned policy as the baseline as is commonly done in REINFORCE. As Figure <ref type="figure" target="#fig_0">1</ref> shows, it is between 0.130 and 0.083 for the best policies we found. Figure <ref type="figure" target="#fig_1">2</ref> (left) shows that these baseline values are well outside of the optimum range.</p><p>Also shown in the right-hand plot of Figure <ref type="figure" target="#fig_1">2</ref> is the value of the control variate in the denominator of the SNIPS estimate. As expected, it increases from below 1 to above 1 as λ is increased. Note that large deviations of the control variate from 1 are a sign of propensity overfitting <ref type="bibr" target="#b15">(Swaminathan &amp; Joachims, 2015c)</ref>. In particular, for all solutions ŵj the estimated standard error of the control variate S j was less than 0.013, meaning that the normal 95% confidence interval for each S j is contained in [0.974, 1.026]. If we see a ŵj with control variate S j outside this range, we should be suspicious of propensity overfitting to the choices of the logging policy and discard this solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS AND FUTURE WORK</head><p>We proposed a new output layer for deep neural networks that enables the use of logged contextual bandit feedback for training. This type of feedback is abundant and ubiquitous in the form of interaction logs from autonomous systems, opening up the possibility of training deep neural networks on unprecedented amounts of data. In principle, this new output layer can replace the conventional cross-entropy layer for any network architecture. We provide a rigorous derivation of the training objective, linking it to an equivariant counterfactual risk estimator that enables counterfactual risk minimization. Most importantly, we show how the resulting training objective can be decomposed and reformulated to make it feasible for SGD training. We find that the BanditNet approach applied to the ResNet architecture achieves predictive accuracy comparable to conventional full-information training for visual object recognition.</p><p>The paper opens up several directions for future work. First, it enables many new applications where contextual bandit feedback is readily available. Second, in settings where it is infeasible to log propensity-scored data, it would be interesting to combine BanditNet with propensity estimation techniques. Third, there may be improvements to BanditNet, like smarter search techniques for S, more efficient counterfactual estimators beyond SNIPS, and the ability to handle continuous outputs.</p><p>A APPENDIX: STEERING THE EXPLORATION OF S THROUGH λ. Proof. For the optimal w * , let</p><formula xml:id="formula_24">S = n i=1 π w * (y i | x i ) π 0 (y i | x i )<label>(31)</label></formula><p>be the control variate in the denominator of the SNIPS estimator. S is a random variable that is a sum of bounded random variables between 0 and max x,y</p><formula xml:id="formula_25">π w * (y | x) π 0 (y | x) ≤ 1 p . (<label>32</label></formula><formula xml:id="formula_26">)</formula><p>We can bound the probability that the control variate S of the optimum w * lies outside of [1− , 1+ ] via Hoeffding's inequality:</p><formula xml:id="formula_27">P (|S − 1| ≥ ) ≤ 2 exp −2n 2 2 n(1/p) 2 (33) = 2 exp −2n 2 p 2 . (<label>34</label></formula><formula xml:id="formula_28">)</formula><p>The same argument applies to any individual policy π w , not just w * . Note, however, that it can still be highly likely that at least one policy π w with w ∈ W shows a large deviation in the control variate for high-capacity W , which can lead to propensity overfitting when using the naive IPS estimator.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Learning curve of BanditNet. The x-axis is the amount of bandit feedback, the y-axis is the test error. Given enough bandit feedback, Bandit-ResNet converges to the skyline performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The x-axis shows the value of the Lagrange multiplier λ used for training. Left plot shows the test error. Right plot shows the value of the SNIPS objective and the normalizer S. The size of the training set is 50k bandit-feedback examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Theorem 1 .</head><label>1</label><figDesc>Let λ a &lt; λ b and let If the optima ŵa and ŵb are not equivalent in the sense that Rλa IPS (π ŵa ) = Rλa IPS (π ŵb ) and Rλ b IPS(π ŵa ) = Rλ b IPS (π ŵb ), then S a &lt; S b . ) . Then Rλ IPS (π w ) = f (w) − λg(w),(22)where g(w) corresponds to the value of the control variate S. Since ŵa and ŵb are not equivalent optima, we know thatf ( ŵa ) − λ a g( ŵa ) &lt; f ( ŵb ) − λ a g( ŵb ) (23) f ( ŵb ) − λ b g( ŵb ) &lt; f ( ŵa ) − λ b g( ŵa )(24)Adding the two inequalities and solving implies that⇒ f ( ŵa ) − λ a g( ŵa ) + f ( ŵb ) − λ b g( ŵb ) &lt; f ( ŵb ) − λ a g( ŵb ) + f ( ŵa ) − λ b g( ŵa ) (25) ⇔ λ a g( ŵa ) + λ b g( ŵb ) &gt; λ a g( ŵb ) + λ b g( ŵa ) (26) ⇔ (λ b − λ a ) g( ŵb ) &gt; (λ b − λ a ) g( ŵa )CHARACTERIZING THE RANGE OF S TO EXPLORE.Theorem 2. Let p ≤ π 0 (y | x) be a lower bound on the propensity for the logging policy, then constraining the solution of Eq. (11) to the w with control variate S ∈ [1 − , 1 + ] for a training set of size n will not exclude the minimizer of the true risk w * = arg min w∈W R(π w ) in the policy space W with probability at least 1 − 2 exp −2n 2 p 2 . (30)</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">http://www.joachims.org/banditnet/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research was supported in part by NSF Award IIS-1615706, a gift from Bloomberg, the Criteo Faculty Research Award program, and the Netherlands Organisation for Scientific Research (NWO) under project nr. 612.001.116. All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C APPENDIX: WHY DIRECT STOCHASTIC OPTIMIZATION OF RATIO</head><p>ESTIMATORS IS NOT POSSIBLE.</p><p>Suppose we have a dataset of n BLBF samples D = {(x 1 , y 1 , δ 1 , p 1 ) . . . (x n , y n , δ n , p n )} where each instance is an i.i.d. sample from the data generating distribution. In the sequel we will be considering two datasets of n + 1 samples, D = D ∪ {(x , y , δ , p )} and D = D ∪ {(x , y , δ , p )} where (x , y , δ , p ) = (x , y , δ , p ) and (x , y , δ , p ), (x , y , δ , p ) / ∈ D.</p><p>For notational convenience, let f i := δ i πw(yi|xi) π0(yi|xi) , and ḟi := ∇ w f i ; g i := πw(yi|xi) π0(yi|xi) , and ġi := ∇ w g i . First consider the vanilla IPS risk estimate of Eq. ( <ref type="formula">5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RIPS (π</head><p>To maximize this estimate using stochastic optimization, we must construct an unbiased gradient estimate. That is, we randomly select one sample from D and compute a gradient α((x i , y i , δ i , p i )) and we require that</p><p>Here the expectation is over our random choice of 1 out of n samples. Observe that α((x i , y i , δ i , p i )) = ḟi suffices (and indeed, this corresponds to vanilla SGD):</p><p>Other choices of α(•) can also produce unbiased gradient estimates, and this leads to the study of stochastic variance-reduced gradient optimization. Now let us attempt to construct an unbiased gradient estimate for Eq. ( <ref type="formula">8</ref>):</p><p>Suppose such a gradient estimate exists, β((x i , y i , δ i , p i )). Then,</p><p>This identity is true for any sample of BLBF instances -in particular, for D and D :</p><p>Subtracting these two equations,</p><p>The LHS clearly depends on {(x i , y i , δ i , p i )} n i=1 in general, while the RHS does not! This contradiction indicates that no construction of β that only looks at a sub-sample of the data can yield an unbiased gradient estimate of RSNIPS (π w ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D APPENDIX: VARIANCE REGULARIZATION</head><p>Unlike in conventional supervised learning, a counterfactual empirical risk estimator like RIPS (π w ) can have vastly different variances Var( RIPS (π w )) for different π w in the hypothesis space (and RSNIPS (π w ) as well) <ref type="bibr" target="#b14">(Swaminathan &amp; Joachims, 2015b)</ref>. Intuitively, the "closer" the particular π w is to the exploration policy π 0 , the larger the effective sample size (Owen, 2013) will be and the smaller the variance of the empirical risk estimate. For the optimization problems we solve in Eq. ( <ref type="formula">18</ref>), this means that we should trust the λ-translated risk estimate Rλj IPS (π w ) more for some w than for others, as we use Rλj IPS (π w ) only as a proxy for finding the policy that minimizes its expected value (i.e., the true loss). To this effect, generalization error bounds that account for this variance difference <ref type="bibr" target="#b14">(Swaminathan &amp; Joachims, 2015b)</ref> motivate a new type of overfitting control. This leads to the following training objective <ref type="bibr" target="#b14">(Swaminathan &amp; Joachims, 2015b)</ref>, which can be thought of as a more reliable version of Eq. ( <ref type="formula">18</ref>):</p><p>Here, Var( Rλj IPS (π w )) is the estimated variance of Rλj IPS (π w ) on the training data, and γ is a regularization constant to be selected via cross validation. The intuition behind this objective is that we optimize the upper confidence interval, which depends on the variance of the risk estimate for each π w . While this objective again does not permit SGD optimization in its given form, it has been shown that a Taylor-majorization can be used to successively upper bound the objective in Eq. ( <ref type="formula">35</ref>), and that typically a small number of iterations suffices to converge to a local optimum <ref type="bibr" target="#b14">(Swaminathan &amp; Joachims, 2015b)</ref>. Each such Taylor-majorization is again of a form</p><p>for easily computable constants A and B <ref type="bibr" target="#b14">(Swaminathan &amp; Joachims, 2015b)</ref>, which allows for SGD optimization.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Taming the monster: A fast and simple algorithm for contextual bandits</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The offset tree for learning with partial labels</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="129" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Counterfactual reasoning and learning systems: The example of computational advertising</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Quinonero-Candela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Portugaly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Snelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="3207" to="3260" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural ranking models with weak supervision</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Variance reduction techniques for gradient estimates in reinforcement learning</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Greensmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1471" to="1530" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Weighted average importance sampling and defensive mixture distributions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hesterberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="185" to="194" />
		</imprint>
	</monogr>
	<note type="report_type">Technometrics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning visual features from large weakly supervised data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasilache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="67" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Carlo theory, methods and examples</title>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wortman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<editor>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Owen</surname></persName>
		</editor>
		<editor>
			<persName><surname>Monte</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2008">2008. 2013</date>
			<biblScope unit="page" from="528" to="535" />
		</imprint>
	</monogr>
	<note>Exploration scavenging</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The central role of propensity score in observational studies for causal effects</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrica</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="41" to="55" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A deep reinforcement learning chatbot</title>
		<author>
			<persName><forename type="first">I</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pieper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mudumba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Brebisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M R</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Suhubdy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-09">September 2017</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Counterfactual risk minimization: Learning from logged bandit feedback</title>
		<author>
			<persName><forename type="first">A</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2015">2015a</date>
			<biblScope unit="page" from="814" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch learning from logged bandit feedback through counterfactual risk minimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1731" to="1755" />
			<date type="published" when="2015">2015b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The self-normalized estimator for counterfactual learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015c</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Conditional monte carlo for normal samples</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Trotter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Tukey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Monte Carlo Methods</title>
				<imprint>
			<date type="published" when="1956">1956</date>
			<biblScope unit="page" from="64" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
				<meeting><address><addrLine>Chichester, GB</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The optimal reward baseline for gradient-based reinforcement learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
				<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="538" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="1992-05">May 1992</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>CoRR, abs/1611.03530</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
