<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HAQ: Hardware-Aware Automated Quantization with Mixed Precision</title>
				<funder>
					<orgName type="full">Qualcomm</orgName>
				</funder>
				<funder>
					<orgName type="full">Xilinx</orgName>
				</funder>
				<funder>
					<orgName type="full">Intel</orgName>
				</funder>
				<funder>
					<orgName type="full">Samsung</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kuan</forename><surname>Wang</surname></persName>
							<email>kuanwang@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
							<email>zhijian@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yujun</forename><surname>Lin</surname></persName>
							<email>yujunlin@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
							<email>jilin@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Song</forename><surname>Han</surname></persName>
							<email>songhan@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HAQ: Hardware-Aware Automated Quantization with Mixed Precision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model quantization is a widely used technique to compress and accelerate deep neural network (DNN) inference.</head><p>Emergent DNN hardware accelerators begin to support mixed precision (1-8 bits) to further improve the computation efficiency, which raises a great challenge to find the optimal bitwidth for each layer: it requires domain experts to explore the vast design space trading off among accuracy, latency, energy, and model size, which is both timeconsuming and sub-optimal. There are plenty of specialized hardware for neural networks, but little research has been done for specialized neural network optimization for a particular hardware architecture. Conventional quantization algorithm ignores the different hardware architectures and quantizes all the layers in a uniform way. In this paper, we introduce the Hardware-Aware Automated Quantization (HAQ) framework which leverages the reinforcement learning to automatically determine the quantization policy, and we take the hardware accelerator's feedback in the design loop. Rather than relying on proxy signals such as FLOPs and model size, we employ a hardware simulator to generate direct feedback signals (latency and energy) to the RL agent. Compared with conventional methods, our framework is fully automated and can specialize the quantization policy for different neural network architectures and hardware architectures. Our framework effectively reduced the latency by 1.4-1.95? and the energy consumption by 1.9? with negligible loss of accuracy compared with the fixed bitwidth (8 bits) quantization. Our framework reveals that the optimal policies on different hardware architectures (i.e., edge and cloud architectures) under different resource constraints (i.e., latency, energy and model size) are drastically different. We interpreted the implication of different quantization policies, which offer insights for both neural network architecture design and hardware architecture design. * indicates equal contributions. 68 69 70 71 72 73 25 44 63 82 101 120 MobileNets (fixed 8-bit quantization) MobileNets (our flexible-bit quantization)</p><p>Latency (ms) Top-1 Accuracy (%)</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In many real-time machine learning applications (such as robotics, autonomous driving, and mobile VR/AR), deep neural networks is strictly constrained by the latency, energy, and model size. In order to improve the hardware efficiency, many researchers have proposed to directly design efficient models <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b1">2]</ref> or to quantize the weights and activations to low precision <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>Conventional quantization methods use the same number of bits for all layers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15]</ref>, but as different layers have different redundancy and behave differently on the hardware (computation bounded or memory bounded), it is necessary to use mixed precision for different layers (as shown in Figure <ref type="figure">1</ref>). This flexibility was originally not supported by chip vendors until recently the hardware manufacturers started to implement this feature: Apple released the A12 Bionic chip that supports mixed precision for the neural network inference <ref type="bibr" target="#b6">[7]</ref>; NVIDIA recently introduced the Turing GPU architecture that supports 1-bit, 4-bit, 8-bit and 16-bit arithmetic operations <ref type="bibr" target="#b21">[22]</ref>; Imagination launched a flexible neural network IP that supports per-layer bitwidth adjustment for  <ref type="table">1</ref>: Inference latency of MobileNet-V1 <ref type="bibr" target="#b12">[13]</ref> on three hardware architectures under different quantization policies.</p><p>The quantization policy that is optimized for one hardware is not optimal for the other. This suggests we need a specialized quantization solution for different hardware architectures. (HW1: BitFusion <ref type="bibr" target="#b25">[26]</ref>, HW2: BISMO <ref type="bibr" target="#b26">[27]</ref> edge accelerator, HW3: BISMO cloud accelerator, batch = 16).</p><p>both weights and activations <ref type="bibr" target="#b13">[14]</ref>. Besides industry, recently academia also works on the bit-level flexible hardware design: BISMO <ref type="bibr" target="#b26">[27]</ref> proposed the bit-serial multiplier to support multiplications of 1 to 8 bits; BitFusion <ref type="bibr" target="#b25">[26]</ref> supports multiplications of 2, 4, 8 and 16 bits in a spatial manner. However, a very missing part is how to determine the bitwidth of both weights and activations for each layer on different hardware accelerators. This is a vast design space: with M different neural network models, each with N layers, on H different hardware platforms, there are in total O(H ? M ? 8 2N ) * possible solutions. For a widely used ResNet-50 <ref type="bibr" target="#b9">[10]</ref> model, the size of the search space is about 8 100 , which is even larger than the number of particles in the universe. Conventional methods require domain experts (with knowledge of both machine learning and hardware architecture) to explore the huge design space smartly with rule-based heuristics, such as: we should retain more bits in the first layer which extracts low level features and in the last layer which computes the final outputs; also, we should use more bits in the convolution layers than in the fullyconnected layers because empirically, the convolution layers are more sensitive. As the neural network becomes deeper, the search space increases exponentially, which makes it infeasible to rely on hand-crafted strategies. Therefore, these rule-based quantization policies are usually sub-optimal, and they cannot generalize from one model to another. In this paper, we would like to automate this exploration process by a learning-based framework.</p><p>Another challenge is how to optimize the latency and the energy consumption of a given model on the hardware. A widely adopted approach is to rely on some proxy signals (e.g., FLOPs, number of memory references) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>. However, as different hardware behaves very differently, the performance of a model on the hardware cannot always be accurately reflected by these proxy signals. Therefore, it is important to directly involve the hardware architecture's * Assuming the bitwidth is 1 to 8 for both weights and activations.</p><p>performance feedback into the design loop. Also, as demonstrated in Table <ref type="table">1</ref>, the quantization solution optimized on one hardware might not be optimal on the other, which raises the demand for specialized policies for different hardware architectures.</p><p>To this end, we propose the Hardware-Aware Automated Quantization (HAQ) framework that leverages reinforcement learning to automatically predict the quantization policy given the hardware's feedback. The RL agent decides the bitwidth of a given neural network in a layer-wise manner. For each layer, the agent receives the layer configuration and statistics as observation, and it then outputs the action which is the bitwidth of weights and activations. We then leverage the hardware accelerator as the environment to obtain the direct feedback from hardware to guide the RL agent to satisfy the resource constraints. After all layers are quantized, we finetune the quantized model for one more epoch, and feed the validation accuracy after short-term retraining as the reward signal to our RL agent. During the exploration, we leverage the deep deterministic policy gradient (DDPG) <ref type="bibr" target="#b17">[18]</ref> to supervise our RL agent. We also studied the quantization policy on multiple hardware architectures: both cloud and edge neural network accelerators, with spatial or temporal multi-precision design.</p><p>The contribution of this paper has four aspects: 1. Automation: We propose an automated framework for quantization, which does not require domain experts and rule-based heuristics. It frees the human labor from exploring the vast search space of choosing bitwidths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Hardware-Aware: Our framework involves the hardware architecture into the loop so that it can directly reduce the latency, energy and storage on the target hardware instead of relying on proxy signals.</p><p>3. Specialization: For different hardware architectures, our framework can offer a specialized quantization policy that's exactly tailored for the target hardware architecture to optimize latency and energy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Design Insights:</head><p>We interpreted the different quantization polices learned for different hardware architectures.</p><p>Taking both computation and memory access into account, the interpretation offers insights on both neural network architecture and hardware architecture design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Quantization. There have been extensive explorations on compressing and accelerating deep neural networks using quantization. Han et al. <ref type="bibr" target="#b8">[9]</ref> quantized the network weights to reduce the model size by rule-based strategies: e.g., they used human heuristics to determine the bitwidths for convolution and fully-connected layers. Courbariaux et al. <ref type="bibr" target="#b4">[5]</ref> binarized the network weights into {-1, +1}; Rastegari et al. <ref type="bibr" target="#b23">[24]</ref> and Zhou et al. <ref type="bibr" target="#b32">[33]</ref> binarized each convolution filter</p><formula xml:id="formula_0">? ? ? ? a ? ? w 0 ? ? ? w ? a 0 ? ? ? ? ? a ? ? w 0 ? ? ? w ? a 0 ?</formula><p>BitFusion (On the Edge) </p><formula xml:id="formula_1">PE &amp; &lt;&lt; + ? ? ? ? ? a n ? ? w 0 ? ? ? w n ? a 0 + PE PE PE PE PE ? BISMO (On the Cloud) PE &amp; &lt;&lt; + ? ? ? ? ? a n ? ? w 0 ? ? ?</formula><formula xml:id="formula_2">+ ? ? ? ? ? a n ? ? w 0 ? ? ? w n ? a 0 + PE PE PE PE PE ? ? ? ? ? a ? ? w 0 ? ? ? w ? a 0 ? Figure 2:</formula><p>An overview of our Hardware-Aware Automated Quantization (HAQ) framework. We leverage the reinforcement learning to automatically search over the huge quantization design space with hardware in the loop. The agent propose an optimal bitwidth allocation policy given the amount of computation resources (i.e., latency, power, and model size). Our RL agent integrates the hardware accelerator into the exploration loop so that it can obtain the direct feedback from the hardware, instead of relying on indirect proxy signals.</p><p>into {-w, +w}; Zhu et al. <ref type="bibr" target="#b34">[35]</ref> mapped the network weights into {-w N , 0, +w P } using two bits; Zhou et al. <ref type="bibr" target="#b33">[34]</ref> used one bit for network weights and two bits for activations; Jacob et al. <ref type="bibr" target="#b14">[15]</ref> made use of 8-bit integers for both weights and activations. We refer the reader to the survey paper by Krishnamoorthi et al. <ref type="bibr" target="#b16">[17]</ref> for a more detailed overview. These conventional quantization methods either simply assign the same number of bits to all layers or require domain experts to determine the bitwidths for different layers, while our framework automates this design process, and our learning-based policy outperforms rule-based strategies.</p><p>AutoML. Many researchers aimed to improve the performance of deep neural networks by searching the network architectures: Zoph et al. <ref type="bibr" target="#b35">[36]</ref> proposed the Neural Architecture Search (NAS) to explore and design the transformable network building blocks, and their network architecture outperforms several human designed networks; Liu et al. <ref type="bibr" target="#b19">[20]</ref> introduced the Progressive NAS to accelerate the architecture search by 5? using sequential model-based optimization; Pham et al. <ref type="bibr" target="#b22">[23]</ref> introduced the Efficient NAS to speed up the exploration by 1000? using parameter sharing; Cai et al. <ref type="bibr" target="#b0">[1]</ref> introduced the path-level network transformation to effectively search the tree-structured architecture space. Motivated by these AutoML frameworks, He et al. <ref type="bibr" target="#b10">[11]</ref> leveraged the reinforcement learning to automatically prune the convolution channels. Our framework further explores the automated quantization for network weights and activations, and it takes the hardware architectures into consideration.</p><p>Efficient Models. To facilitate the efficient deployment, researchers designed hardware-friendly approaches to slim neural network models. For instance, the coarse-grained channel pruning methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21]</ref> prune away the entire channel of convolution kernels to achieve speedup. Recently, researchers have explicitly optimized for various aspects of hardware properties, including the inference latency and energy: Yang et al. <ref type="bibr" target="#b30">[31]</ref> proposed the energy-aware pruning to directly optimize the energy consumption of neural networks; Yang et al. <ref type="bibr" target="#b31">[32]</ref> reduced the inference time of neural networks on the mobile devices through a lookup table. Nevertheless, these methods are still rule-based and mostly focus on pruning. Our framework automates the quantization process by taking hardware-specific metric as direct rewards using a learning based method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>We model the quantization task as a reinforcement learning problem (Figure <ref type="figure">2</ref>). We use the actor-critic model with DDPG agent to give the action: bits for each layer. We collect hardware counters as constraints, together with accuracy as rewards to search the optimal quantization policy. We have three hardware environments that covers edge and cloud, spatial and temporal architectures for mixed-precision accelerator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Observation (State Space)</head><p>Our agent processes the neural network in a layer-wise manner. For each layer, our agent takes two steps: one for weights, and one for activations. In this paper, we introduce a ten-dimensional feature vector O k as our observation:</p><p>If the k th layer is a convolution layer, the state O k is</p><formula xml:id="formula_3">O k = (k, c in , c out , s kernel , s stride , s feat , n params , i dw , i w/a , a k-1 ), (<label>1</label></formula><formula xml:id="formula_4">)</formula><p>where k is the layer index, c in is #input channels, c out is #output channels, s kernel is kernel size, s stride is the stride, s feat is the input feature map size, n params is #parameters, i dw is a binary indicator for depthwise convolution, i w/a is a binary indicator for weight/activation, and a k-1 is the action from the last time step.</p><p>If the k th layer is a fully-connected layer, the state O k is</p><formula xml:id="formula_5">O k = (k, h in , h out , 1, 0, s feat , n params , 0, i w/a , a k-1 ), (<label>2</label></formula><formula xml:id="formula_6">)</formula><p>where k is the layer index, h in is #input hidden units, h out is #output hidden units, s feat is the size of input feature vector, n params is #parameters, i w/a is a binary indicator for weight/ activation, and a k-1 is the action from the last step.</p><p>For each dimension in the observation vector O k , we normalize it into [0, 1] to make them in the same scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Action Space</head><p>We use a continuous action space to determine the bitwidth. The reason that we do not use a discrete action space is because it loses the relative order: e.g., 2-bit quantization is more aggressive than 4-bit and even more than 8-bit. At the k th time step, we take the continuous action a k (which is in the range of [0, 1]), and round it into the discrete bitwidth value b k :</p><formula xml:id="formula_7">b k = round(b min -0.5 + a k ? (b max -b min + 1)),<label>(3)</label></formula><p>where b min and b max denote the min and max bitwidth (in our experiments, we set b min to 2 and b max to 8).</p><p>Resource Constraints. In real-world applications, we have limited computation budgets (i.e., latency, energy, and model size). We would like to find the quantization policy with the best performance given the constraint.</p><p>We encourage our agent to meet the computation budget by limiting the action space. After our RL agent gives actions {a k } to all layers, we measure the amount of resources that will be used by the quantized model. The feedback is directly obtained from the hardware accelerator, which we will discuss in Section 3.3. If the current policy exceeds our resource budget (on latency, energy or model size), we will sequentially decrease the bitwidth of each layer until the constraint is finally satisfied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Direct Feedback from Hardware Accelerators</head><p>An intuitive feedback to our RL agent can be FLOPs or the model size. However, as these proxy signals are indirect, they are not equal to the performance (i.e., latency, energy consumption) on the hardware. Cache locality, number of kernel calls, memory bandwidth all matters. Proxy feedback can not model these hardware functionality to find the specialized strategies (see Table <ref type="table">1</ref>).</p><p>Instead, we use direct latency and energy feedback from the hardware accelerator as resource constraints, which enables our RL agent to determine the bitwidth allocation policy from the subtle differences between different layers: e.g., vanilla convolution has more data reuse and better locality, while depthwise convolution <ref type="bibr" target="#b3">[4]</ref> has less reuse and worse locality, which makes it memory bounded. Such difference impacts the optimal quantization policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Quantization</head><p>We linearly quantize the weights and activations of each layer using the action a k given by our agent, as linearly quantized model only needs fixed point arithmetic unit which is more efficient to implement on the hardware.</p><p>Specifically, for each weight value w in the k th layer, we first truncate it into the range of [-c, c], and we then quantize it linearly into a k bits:</p><formula xml:id="formula_8">quantize(w, a k , c) = round(clamp(w, c)/s) ? s,<label>(4)</label></formula><p>where clamp(?, x) is to truncate the values into [-x, x], and the scaling factor s is defined as s = c/(2 a k -1 -1). In this paper, we choose the value of c by finding the optimal value x that minimizes the KL-divergence between the original weight distribution W k and the quantized weight distribution quantize(W k , a k , x):</p><formula xml:id="formula_9">c = arg min x D KL (W k || quantize(W k , a k , x)),<label>(5)</label></formula><p>where D KL (? || ?) is the KL-divergence that characterizes the distance between two distributions. As for activations, we quantize the values similarly except that we truncate them into the range of [0, c], not [-c, c] since the activation values (which are the outputs of the ReLU layers) are non-negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Reward Function</head><p>After quantization, we retrain the quantized model for one more epoch to recover the performance. As we have already imposed the resource constraints (latency, energy) by limiting the action space (Section 3.2), we define our reward function R to be only related to the accuracy:</p><formula xml:id="formula_10">R = ? ? (acc quant -acc origin ),<label>(6)</label></formula><p>where acc origin is the top-1 classification accuracy of the fullprecision model on the training set, acc quant is the accuracy of the quantized model after finetuning, and ? is a scaling factor which is set to 0.1 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Agent</head><p>For the RL agent, we leverage the deep deterministic policy gradient (DDPG) <ref type="bibr" target="#b17">[18]</ref>, which is an off-policy actorcritic algorithm for continuous control problem. In our environment, one step means that our agent makes an action to decide the number of bits assigned to the weights or activations of a specific layer, while one episode is composed of multiple steps, where our RL agent makes actions to all layers. We apply a variant form of the Bellman's Equation, where each transition in an episode is defined as</p><formula xml:id="formula_11">T k = (O k , a k , R, O k+1 ). During exploration, the Q- function is computed as Qk = R k -B + ? ? Q(O k+1 , w(O k+1 ) | ? Q ),<label>(7)</label></formula><p>and the loss function can be approximated by</p><formula xml:id="formula_12">L = 1 N s Ns k=1 ( Qk -Q(O k , a k | ? Q )) 2 ,<label>(8)</label></formula><p>where N s denotes the number of steps in this episode, and the baseline B is defined as an exponential moving average of all previous rewards in order to reduce the variance of the gradient estimation. The discount factor ? is set to 1 since we assume that the action made for each layer should contribute equally to the final result. Moreover, as the number of steps is always finite (bounded by the number of layers), the sum of the rewards will not explode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Implementation Details</head><p>In this section, we present the implementation details about RL exploration and finetuning quantized models.</p><p>Agent. The DDPG agent consists of an actor network and a critic network. Both using the same network architecture: they take the state vector and the action from the last time step as inputs and feed them into two separate fullyconnected layers with hidden sizes of 400. After that, we add the two hidden vectors together and go through another two fully-connected layers with hidden sizes of {300, 1}. As for the actor network, we use an additional sigmoid function to project the output into the range of [0, 1].</p><p>Exploration. Optimization of the DDPG agent is carried out using ADAM <ref type="bibr" target="#b15">[16]</ref> with ? 1 = 0.9 and ? 2 = 0.999. We use a fixed learning rate of 10 -4 for the actor network and 10 -3 for the critic network. During exploration, we employ the following stochastic process of the noise:</p><formula xml:id="formula_13">w ? (O k ) ? N trunc (w(O k | ? w k ), ? 2 , 0, 1),<label>(9)</label></formula><p>where N trunc (?, ?, a, b) is the truncated normal distribution, and w is the model weights. The noise ? is initialized as 0.5, and after each episode, the noise is decayed exponentially with a decay rate of 0.99.</p><p>Finetuning. During exploration, we finetune the quantized model for one epoch to help recover the performance (using SGD with a fixed learning rate of 10 -3 and momentum of 0.9). We randomly select 100 categories from ImageNet <ref type="bibr" target="#b5">[6]</ref> Hardware  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct extensive experiments to demonstrate the consistent effectiveness of our framework for multiple objectives: latency, energy, and model size.</p><p>Datasets and Models. Our experiments are performed on the ImageNet <ref type="bibr" target="#b5">[6]</ref> dataset. As our focus is on more efficient models, we extensively study the quantization of MobileNet-V1 <ref type="bibr" target="#b12">[13]</ref> and MobileNet-V2 <ref type="bibr" target="#b24">[25]</ref>. Both MobileNets are inspired from the depthwise separable convolutions <ref type="bibr" target="#b3">[4]</ref> and replace the regular convolutions with the pointwise and depthwise convolutions: MobileNet-V1 stacks multiple "depthwisepointwise" blocks repeatedly; while MobileNet-V2 uses the "pointwisedepthwisepointwise" blocks as its basic building primitives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Latency-Constrained Quantization</head><p>We first evaluate our framework under latency constraints on two representative hardware architectures: spatial and temporal architectures for multi-precision CNN. We show that it's beneficial to have specialized quantization policies for different hardware architectures. We systematically interpret the policy given by AI to guide future human designs.</p><p>Temporal Architecture. Bit-Serial Matrix Multiplication Overlay (BISMO) proposed by Yaman et al. <ref type="bibr" target="#b26">[27]</ref> is a classic temporal design of neural network accelerator on FPGA. It introduces bit-serial multipliers which are fed with one-bit digits from 256 weights and corresponding activations in parallel at one time and accumulates their partial products by shifting over time.</p><p>Spatial Architecture. BitFusion architecture proposed by Hardik et al. <ref type="bibr" target="#b25">[26]</ref> is a state-of-the-art spatial ASIC design for neural network accelerator. It employs a 2D systolic array of Fusion Units which spatially sum the shifted partial products of two-bit elements from weights and activations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Quantization policy for BISMO Architecture</head><p>Inferencing neural networks on edge devices and cloud severs can be quite different: batch size, memory bandwidth, peak FLOPs, etc.. We use Xilinx Zynq-7020 FPGA <ref type="bibr" target="#b29">[30]</ref> as   <ref type="figure">3</ref>: Quantization policy under latency constraints for MobileNet-V1. On edge accelerator, our RL agent allocates less activation bits to the depthwise convolutions, which echos that the depthwise convolutions are memory bounded and the activations dominates the memory access. On cloud accelerator, our agent allocates more bits to the depthwise convolutions and allocates less bits to the pointwise convolutions, as cloud device has more memory bandwidth and high parallelism, the network appears to be computation bounded.</p><p>our edge device and Xilinx VU9P <ref type="bibr" target="#b28">[29]</ref> as our cloud device. Table <ref type="table" target="#tab_2">2</ref> shows our experiment configurations on these two platforms along with their available resources.</p><p>As for comparison, we adopt the PACT <ref type="bibr" target="#b2">[3]</ref> as our baseline, which uses the same number of bits for all layers except for the first layer which extracts the low level features, they use 8 bits for both weights and activations as it has fewer parameters and is very sensitive to errors. We follow a similar setup for the first layer (8 bits), and explore the bitwidth allocation policy for all the other layers. Under the same latency, HAQ consistently achieved better accuracy than the baseline on both the cloud and the edge (Table <ref type="table" target="#tab_4">3</ref>). With similar accuracy, HAQ can reduce the latency by 1.4? to 1.95? compared with the baseline.</p><p>Interpreting the quantization policy. Our agent gave quite different quantization policy for edge and cloud accelerators (Figure <ref type="figure">3</ref>). For the activations, the depthwise convolution layers are assigned less bitwidth than the pointwise layers on the edge; while on the cloud device, the bitwidth of these two types of layers are similar. For weights, the bitwidth of these types of layers are nearly the same on the edge; while on the cloud, the depthwise convolution layers got more bitwidth than the pointwise convolution layers.</p><p>We explain the difference of quantization policy between edge and cloud by the roofline model <ref type="bibr" target="#b27">[28]</ref>. Many previous works use FLOPs or BitOPs as metrics to measure computation complexity. However, they are not able to directly reflect the latency, since there are many other factors influencing the hardware performance, such as memory access cost and degree of parallelism <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b20">21]</ref>. Taking computation and memory access into account, the roofline model assumes that applications are either computation-bound or memory bandwidth-bound, if not fitting in on-chip caches, depending on their operation intensity. Operation intensity is measured as operations (MACs in neural networks) per byte accessed. A lower operation intensity indicates suffering more from the memory access.</p><p>The bottom of Figure <ref type="figure">3</ref> shows the operation intensities (OPs per Byte) of convolution layers in the MobileNet-V1. Depthwise convolution is memory bounded, and the pointwise convolution is computation bounded. Our experiments show that when running MobileNet-V1 on the edge devices   with small batch size, its latency is dominated by the depthwise convolution layers. Since the feature maps take a major proportion in the memory of depthwise convolution layers, our agent gives the activations less bits. In contrast, when running MobileNet-V1 on the cloud with large batch size, our agent increases the bitwidth of depthwise convolution to preserve the accuracy at low memory overhead since depthwise convolution only takes a small proportion of the total weights. A similar phenomenon can be observed in Figure <ref type="figure" target="#fig_1">4</ref> on MobileNet-V2. Moreover, as the activation size in deeper layers gets smaller, they get assigned more bits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Quantization policy for BitFusion Architecture</head><p>In order to demonstrate the effectiveness of our framework on different hardware architectures, we further compare our framework with PACT <ref type="bibr" target="#b2">[3]</ref> under the latency constraints on the BitFusion <ref type="bibr" target="#b25">[26]</ref> architecture (Table <ref type="table" target="#tab_6">4</ref>). Our framework performs much better than the hand-craft policy with the same latency. It can achieve almost no degradation of accuracy with only half of the latency used by the original  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Energy-Constrained Quantization</head><p>We then evaluate our framework under the energy constraints. Similar to the latency-constrained experiments, we compare our framework with PACT <ref type="bibr" target="#b2">[3]</ref> that uses fixed number of bits without hardware feedback. From Table <ref type="table" target="#tab_8">5</ref>, we can clearly see that our framework outperforms the rule-based baseline: it achieves much better performance while consuming similar amount of energy. In particular, our framework is able to achieve almost no loss of accuracy with nearly half of the energy consumption of the original MobileNet-V1 model (from 31.03 to 16.57 mJ), which suggests that mixed precision with hardware-aware, specialized quantization policy can indeed help reduce the energy consumption.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Model Size-Constrained Quantization</head><p>Finally, we evaluate our framework under the model size constraints. Following Han et al. <ref type="bibr" target="#b8">[9]</ref>, we employ the k-means algorithm to quantize the values into k different centroids instead of using the linear quantization for compression, since k-means quantization can be more effective reducing the model size.</p><p>We compare our framework with Deep Compression <ref type="bibr" target="#b8">[9]</ref> on MobileNets and ResNet-50. From Table <ref type="table" target="#tab_10">6</ref>, we can see that our framework performs much better than Deep Compression: it achieves higher accuracy with the same model size. For compact models like MobileNets, Deep Compression significantly degrades the performance especially under aggressive quantization, while our framework can preserve the accuracy much better. For instance, when Deep Compression quantizes the weights of MobileNet-V1 to 2 bits, the accuracy drops significantly from 70.90 to 37.62; while our framework can still achieve 57.14 of accuracy with the same model size. The reason is our framework makes full use of the mixed precision by systematically searching the optimal quantization policy.</p><p>Discussions. In Figure <ref type="figure">5</ref>, we visualize the bitwidth allocation strategy for MobileNet-V2. From this figure, we can observe that our framework assigns more bitwidths to the weights in depthwise convolution layers than pointwise convolution layers. Intuitively, this is because the number of parameters in the former is much smaller than the latter. Comparing Figure <ref type="figure" target="#fig_1">4</ref> and Figure <ref type="figure">5</ref>, the policies are drasti-cally different under different optimization objectives (fewer bitwiths for depthwise convolutions under latency optimization, more bitwidths for depthwise convolutions under model size optimization). Our framework succeeds in learning to adjust its bitwidth policy under different constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose Hardware-Aware Automated Quantization (HAQ), an automated framework for quantization which does not require any domain experts and rulebased heuristics. We provide a learning based method that can search the quantization policy with hardware feedback. Compared with indirect proxy signals, our framework can offer a specialized quantization solution for different hardware platforms. Extensive experiments demonstrate that our framework performs better than conventional rule-based approaches for multiple objectives: latency, energy and model size. Our framework reveals that the optimal policies on different hardware architectures are drastically different, and we interpreted the implication of those policies. We believe the insights will inspire the future software and hardware co-design for efficient deployment of deep neural networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Quantization policy under latency constraints for MobileNet-V2 on BISMO. Similar to Figure3, depthwise layer is assigned with fewer bits on the edge accelerator, and pointwise layer is assigned with fewer bits on the cloud accelerator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Best Q. policy for HW1 16.29 ms 85.24 ms 117.44 ms Best Q. policy for HW2 19.95 ms 64.29 ms 108.64 ms</figDesc><table><row><cell></cell><cell cols="2">Inference latency on</cell></row><row><cell>HW1</cell><cell>HW2</cell><cell>HW3</cell></row><row><cell cols="2">Best Q. policy for HW3 19.94 ms 66.15 ms</cell><cell>99.68 ms</cell></row><row><cell>Table</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Batch PE Array AXI port Block RAM</figDesc><table><row><cell cols="2">Edge Zynq-7020</cell><cell>1</cell><cell>8?8</cell><cell>4?64b</cell><cell>140?36Kb</cell></row><row><cell>Cloud</cell><cell>VU9P</cell><cell>16</cell><cell>16?16</cell><cell cols="2">4?256b 2160?36Kb</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The configurations of edge and cloud accelerators.</figDesc><table><row><cell>to accelerate the model finetuning during exploration. After</cell></row><row><cell>exploration, we quantize the model with our best policy and</cell></row><row><cell>finetune it on the full dataset.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Latency-constrained quantization on BISMO (edge accelerator and cloud accelerator) on ImageNet. Our framework can reduce the latency by 1.4? to 1.95? with negligible loss of accuracy compared with the fixed bitwidth (8 bits) quantization.</figDesc><table><row><cell></cell><cell>8 6</cell><cell>depthwise: fewer bits</cell><cell cols="2">pointwise:more bits</cell><cell>Edge</cell></row><row><cell></cell><cell>4</cell><cell></cell><cell></cell></row><row><cell cols="2">2 4 #bit</cell><cell></cell><cell></cell><cell>layer</cell></row><row><cell></cell><cell>6</cell><cell></cell><cell></cell></row><row><cell></cell><cell>6</cell><cell></cell><cell></cell><cell>Cloud</cell></row><row><cell></cell><cell>4</cell><cell></cell><cell></cell></row><row><cell cols="2">2 4 #bit</cell><cell></cell><cell></cell><cell>layer</cell></row><row><cell></cell><cell>6</cell><cell></cell><cell></cell></row><row><cell></cell><cell>8</cell><cell cols="3">depthwise:more bits pointwise:fewer bits</cell></row><row><cell></cell><cell></cell><cell>#weight bit (pointwise)</cell><cell></cell><cell>#weight bit (depthwise)</cell></row><row><cell></cell><cell></cell><cell cols="2">#activation bit (pointwise)</cell><cell>#activation bit (depthwise)</cell></row><row><cell>log#</cell><cell>0 4 2</cell><cell></cell><cell></cell><cell>layer</cell></row><row><cell></cell><cell cols="2"># OPs per Byte (pointwise)</cell><cell></cell><cell># OPs per Byte (depthwise)</cell></row><row><cell cols="2">Figure</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Latency-constrained quantization on BitFusion</cell></row><row><cell>(MobileNet-V1 on ImageNet). Our framework can reduce</cell></row><row><cell>the latency by 2? with almost no loss of accuracy compared</cell></row><row><cell>with the fixed bitwidth (8 bits) quantization.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell>Energy-constrained quantization on BitFusion</cell></row><row><cell>(MobileNet-V1 on ImageNet). Our framework reduces the</cell></row><row><cell>power consumption by 2? with nearly no loss of accuracy</cell></row><row><cell>compared with the fixed bitwidth quantization.</cell></row><row><cell>MobileNet-V1 model (from 20.08 to 11.09 ms). Therefore,</cell></row><row><cell>our framework is flexible to provide specialized quantization</cell></row><row><cell>policy for different hardware platforms.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Model size-constrained quantization on ImageNet. Compared with Deep Compression<ref type="bibr" target="#b7">[8]</ref>, our framework achieves higher accuracy under similar model size (especially under high compression ratio). Quantization policy under model size constraints for MobileNet-V2. Our RL agent allocates more bits to the depthwise convolutions, since depthwise convolutions have fewer number of parameters.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>depthwise:more bits</cell><cell>pointwise:fewer bits</cell><cell>more params, fewer bits</cell></row><row><cell>#bits</cell><cell>6 2 4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>log#</cell><cell>4 2</cell><cell></cell><cell></cell><cell></cell><cell>layer</cell></row><row><cell></cell><cell>6</cell><cell>#params (pointwise)</cell><cell>#params (depthwise)</cell><cell>#weight bits (pointwise)</cell><cell>#weight bits (depthwise)</cell></row><row><cell cols="2">Figure 5:</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. We thank <rs type="institution">MIT Quest for Intelligence, MIT-IBM Watson AI Lab</rs>, <rs type="funder">Xilinx</rs>, <rs type="funder">Samsung</rs>, <rs type="funder">Intel</rs>, ARM, <rs type="funder">Qualcomm</rs>, and SONY for supporting this research. We thank <rs type="person">Google Cloud</rs> and <rs type="institution">AWS Machine Learning Research Awards</rs> for providing the computation resource.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Path-Level Network Transformation for Efficient Architecture Search</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">PACT: Parameterized Clipping Activation for Quantized Neural Networks</title>
		<author>
			<persName><forename type="first">Jungwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swagath</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I-Jen</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijayalakshmi</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kailash</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><surname>Gopalakrishnan</surname></persName>
		</author>
		<idno>arXiv, 2018. 1</idno>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Franc ?ois Chollet. Xception -Deep Learning with Depthwise Separable Convolutions</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1</title>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Ran El-Yaniv, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ImageNet -A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Apple describes 7nm a12 bionic chips</title>
		<author>
			<persName><surname>Eenews</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Efficient Methods and Hardware for Deep Learning</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008">2016. 1, 2, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">AMC: AutoML for Model Compression and Acceleration on Mobile Devices</title>
		<author>
			<persName><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Channel pruning for accelerating very deep neural networks</title>
		<author>
			<persName><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Andrew G Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<title level="m">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</title>
		<imprint>
			<date type="published" when="2005">2017. 1, 2, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Powervr neural network accelerator</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Imagination</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</title>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skirmantas</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam -A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Quantizing deep convolutional networks for efficient inference -A whitepaper</title>
		<author>
			<persName><forename type="first">Raghuraman</forename><surname>Krishnamoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Continuous control with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2004">2016. 2, 4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Runtime Neural Pruning</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progressive Neural Architecture Search. In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning efficient convolutional networks through network slimming</title>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoumeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Nvidia. Nvidia tensor cores</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient Neural Architecture Search via Parameter Sharing</title>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melody</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">XNOR-Net -ImageNet Classification Using Binary Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">MobileNetV2: Inverted Residuals and Linear Bottlenecks</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2018. 1, 2, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bit fusion: Bit-level dynamically composable architecture for accelerating deep neural network</title>
		<author>
			<persName><forename type="first">Hardik</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongse</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangzhen</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benson</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadi</forename><surname>Esmaeilzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bismo: A scalable bit-serial matrix multiplication overlay for reconfigurable computing</title>
		<author>
			<persName><forename type="first">Yaman</forename><surname>Umuroglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lahiru</forename><surname>Rasnayake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Magnus</forename><surname>Sjalander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FPL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Roofline: an insightful visual performance model for multicore architectures</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Waterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="65" to="76" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Ultrascale architecture and product data sheet: Overview</title>
		<author>
			<persName><surname>Xilinx</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Zynq-7000 soc data sheet: Overview</title>
		<author>
			<persName><surname>Xilinx</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Designing energy-efficient convolutional neural networks using energyaware pruning</title>
		<author>
			<persName><forename type="first">Tien-Ju</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Netadapt: Platform-aware neural network adaptation for mobile applications</title>
		<author>
			<persName><forename type="first">Tien-Ju</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Explicit loss-error-aware quantization for low-bit deep neural networks</title>
		<author>
			<persName><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9426" to="9435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zekun</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuheng</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DoReFa-Net -Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients. arXiv</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Trained Ternary Quantization</title>
		<author>
			<persName><forename type="first">Chenzhuo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Architecture Search with Reinforcement Learning. In ICLR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
