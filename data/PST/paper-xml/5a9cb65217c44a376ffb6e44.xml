<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">No Reference Quality Assessment for Screen Content Images With Both Local and Global Feature Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Yuming</forename><surname>Fang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiebin</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Leida</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Jinjian</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Weisi</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technology</orgName>
								<orgName type="institution">Jiangxi University of Finance and Economics</orgName>
								<address>
									<postCode>330032</postCode>
									<settlement>Nanchang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">China University of Mining and Technology</orgName>
								<address>
									<postCode>221116</postCode>
									<settlement>Xuzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Electronic Engineering</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710126</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<postCode>639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">No Reference Quality Assessment for Screen Content Images With Both Local and Global Feature Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C90746B7F331E7A67DC764CB02BF4AA9</idno>
					<idno type="DOI">10.1109/TIP.2017.2781307</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Screen content image</term>
					<term>visual quality assessment</term>
					<term>no reference quality assessment</term>
					<term>local feature</term>
					<term>global feature</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a novel no reference quality assessment method by incorporating statistical luminance and texture features (NRLT) for screen content images (SCIs) with both local and global feature representation. The proposed method is designed inspired by the perceptual property of the human visual system (HVS) that the HVS is sensitive to luminance change and texture information for image perception. In the proposed method, we first calculate the luminance map through the local normalization, which is further used to extract the statistical luminance features in global scope. Second, inspired by existing studies from neuroscience that high-order derivatives can capture image texture, we adopt four filters with different directions to compute gradient maps from the luminance map. These gradient maps are then used to extract the secondorder derivatives by local binary pattern. We further extract the texture feature by the histogram of high-order derivatives in global scope. Finally, support vector regression is applied to train the mapping function from quality-aware features to subjective ratings. Experimental results on the public large-scale SCI database show that the proposed NRLT can achieve better performance in predicting the visual quality of SCIs than relevant existing methods, even including some full reference visual quality assessment methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>medium of signal transmission in many multimedia applications. Currently, many relevant algorithms for SCI have been proposed, including SCI segmentation <ref type="bibr" target="#b3">[4]</ref>, SCI compression <ref type="bibr" target="#b4">[5]</ref>, SCI quality assessment <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, etc. Undoubtedly, visual quality of SCIs has a great influence on users' viewing experience. Generally, visual quality of SCIs might be degraded with various distortion types during image processing including acquisition, transmission and coding <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Therefore, it is highly desired to design effective visual quality assessment (VQA) metrics for SCIs in various SCI processing applications.</p><p>There have been a number of visual quality assessment (VQA) methods devised in recent years. Traditional VQA methods such as mean square error (MSE) and peak signal to noise (PSNR) have been widely used in industry due to their simple implementation. However, the predicted quality results from these methods are not consistent with subjective ratings, since these methods predict visual quality of images without considering the characteristics of the human visual system (HVS). In order to overcome the shortcomings of these methods, many advanced full reference (FR) methods which require complete reference information for image quality prediction with consideration of the characteristics of the HVS have been proposed, including structure similarity (SSIM) <ref type="bibr" target="#b5">[6]</ref> and its variants: multi-scale SSIM (MSSIM) <ref type="bibr" target="#b6">[7]</ref>, information content weighted SSIM (IWSSIM) <ref type="bibr" target="#b7">[8]</ref>, feature similarity (FSIM) <ref type="bibr" target="#b8">[9]</ref>, internal generative mechanism (IGM) <ref type="bibr" target="#b9">[10]</ref>, visual saliency-based index (VSI) <ref type="bibr" target="#b10">[11]</ref>, gradient magnitude similarity deviation (GMSD) <ref type="bibr" target="#b11">[12]</ref>.</p><p>There have been also many reduced reference (RR) <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, and no reference (NR) image quality estimation methods <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref> proposed in the past decades. For RR methods, only part of reference information is required for image quality prediction, and global features are always extracted as the indicator of image quality. For NR methods, they do not require any reference information for VQA. Fang et al. <ref type="bibr" target="#b42">[43]</ref> built nature scene statistics (NSS) models for NR VQA of contrast distorted images based on moment features. There have been several NR metrics <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b56">[57]</ref> proposed based on one type of local structure descriptor: the local binary pattern (LBP) <ref type="bibr" target="#b37">[38]</ref>. Zhang et al. designed a NR metric based on the joint generalized local binary pattern (GLBP) statistics in <ref type="bibr" target="#b43">[44]</ref>, and Freitas et al. proposed a NR model based on texture information extracted from multiple local ternary pattern (LTP) channels in the form of histograms <ref type="bibr" target="#b44">[45]</ref>. <ref type="bibr">Li et al.</ref> designed a NR VQA model to calculate structure degradation based on the perceptual property in the HVS that there are separate mechanisms in human visual cortex to process the first-and second-order patterns <ref type="bibr" target="#b56">[57]</ref>. Zhang et al. <ref type="bibr" target="#b57">[58]</ref> proposed NSS models with the consideration of fitting parameters of NSS models and the fitting errors between real distributions and fitted NSS models as well as the likelihood of fitting error. In natural image quality evaluator (NIQE) <ref type="bibr" target="#b16">[17]</ref>, the authors designed a NR VQA metric by using the deviations from the nature scene statistic model in natural images. <ref type="bibr">Zhang et al.</ref> recently proposed the integratedlocal NIQE (IL-NIQE) based on multivariate Gaussian model with three additional statistical features based on NIQE <ref type="bibr" target="#b45">[46]</ref>. Xue et al. <ref type="bibr" target="#b55">[56]</ref> designed a quality-aware clustering approach named QAC to learn a series of centroids, which are then used to calculate the visual quality scores of image patches. The overall perceptual quality of distorted image is obtained by average pooling of the quality scores of image patches.</p><p>Most existing VQA methods are designed for natural images and they are not effective in visual quality prediction of SCIs. Unlike natural images, SCIs include diverse forms of visual content, such as pictorial and textual regions. As shown in our previous study <ref type="bibr" target="#b15">[16]</ref>, the characteristics of SCIs and those of natural images are different greatly. Generally, the mean subtracted contrast normalized (MSCN) coefficients <ref type="bibr" target="#b38">[39]</ref> extracted from natural images follow generalized Gaussian distribution, while the distribution curve of MSCN coefficients from SCIs fluctuates greatly and a sharp pimpling appears <ref type="bibr" target="#b15">[16]</ref>. Thus, the VQA metrics designed for quality prediction of natural images are not effective for VQA of SCIs.</p><p>Recently, there have been some metrics designed for visual quality prediction of SCIs. Yang et al. <ref type="bibr" target="#b15">[16]</ref> conducted an user study for visual quality prediction of SCIs and presented a detailed analysis of the subjective experiment. In addition, they built an objective quality assessment metric for SCIs. Wang et al. <ref type="bibr" target="#b18">[19]</ref> designed a FR VQA metric for SCIs by incorporating local information content weighting and viewing field adaption. Fang et al. proposed a FR objective VQA model for SCIs based on uncertainty weighting with the consideration that the HVS is more sensitive to high-frequency information than other smooth regions in the visual scene <ref type="bibr" target="#b19">[20]</ref>. Gu et al. designed a structure-induced quality method (SIQM) by weighting the quality map with the proposed structural degradation strategy <ref type="bibr" target="#b20">[21]</ref>. Ni et al. proposed a VQA model for SCIs based on gradient direction denoted by GSS <ref type="bibr" target="#b52">[53]</ref> and edge information denoted by EMSQA <ref type="bibr" target="#b53">[54]</ref> and ESIM <ref type="bibr" target="#b54">[55]</ref>. In the study Wang et al. <ref type="bibr" target="#b21">[22]</ref> designed a RR image quality assessment (RR-IQA) metric for SCIs by considering visual perception of SCIs. Wang et al. <ref type="bibr" target="#b58">[59]</ref>, conducted studies on subjective quality evaluation of the compressed SCIs and designed a RR quality prediction model in wavelet domain. These methods mentioned above require the complete information of reference image. However, the original images are not available in most practical applications. Gu et al. proposed a NR quality estimation method for SCIs (BQMS) with structural degradation model and the free energy based perceptual theory, and established a large scale database to train NSS models <ref type="bibr" target="#b22">[23]</ref>. Shao et al. <ref type="bibr" target="#b24">[25]</ref> proposed a blind quality predictor for SCI (BLIQUP-SCI) from the perspective of sparse representation. These two NR VQA metrics designed for SCIs cannot obtain consistent visual quality estimation results with human perception. It is still challenging to design effective NR VQA metrics for SCIs.</p><p>It is well known that the HVS is sensitive to edge information <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b27">[28]</ref>, which is the basic component in textual regions and high-frequency parts of the pictorial regions in SCIs. Thus, we use the edge information to measure the visual distortion in SCIs. Meanwhile, the variation of luminance information is also highly correlated with the visual quality of visual scenes <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. In this study, we propose a novel no reference visual quality estimation method for SCIs by statistical luminance features and texture information (NRLT) with both local and global feature representation. First, we extract luminance map of SCIs through the local normalization to the original intensity map to calculate the statistical luminance features. Then four filters with different directions are adopted to compute the gradient maps based on the luminance map, which are denoted as the first order derivative information. The texture features are calculated by local binary patterns (LBPs) on the gradient maps which are denoted as the second order derivative information. We further utilize the histogram to represent statistical luminance and texture features in global scope. Support vector regression (SVR) is employed to train the visual quality estimation model from the quality-aware features to subjective ratings. Experimental results on a largescale SCI database show that the proposed NRLT can obtain better performance in estimating the perceptual quality of SCIs than other relevant ones, even including some FR methods.</p><p>There is much difference between the proposed method and other existing ones. In RR-IQA <ref type="bibr" target="#b21">[22]</ref>, the authors first computed the significance maps of original SCI and distorted versions by incorporating the primary visual information and the uncertainty measure, and utilized the histogram to represent the combined significance maps <ref type="bibr" target="#b21">[22]</ref>. The final quality score of the distorted SCI is obtained by comparing these two histograms. Compared with that method <ref type="bibr" target="#b21">[22]</ref>, the proposed method extract statistical luminance and texture features without any reference information to perceive the quality degradation of distorted SCIs. In BLIQUP-SCI <ref type="bibr" target="#b24">[25]</ref>, the authors used four FR methods including PSNR, SSIM, FSIM and VIF <ref type="bibr" target="#b30">[31]</ref> to generate the labels for SICs. However, these four FR methods are with poor performance in predicting the visual quality of SCIs <ref type="bibr" target="#b15">[16]</ref>. In the proposed method, we use the subjective quality scores of SCIs as the labels to train the visual quality prediction model. Meanwhile, BQMS <ref type="bibr" target="#b22">[23]</ref> needs to fit the relationship of the structure degradation features and the free energy feature before extracting the quality-aware features, while we use the histogram to represent the statistical luminance and texture features directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROPOSED METHOD</head><p>In this section, we describe the proposed NRLT for SCIs in detail. As shown in the study <ref type="bibr" target="#b15">[16]</ref>, the luminance change and texture variation influence the visual quality of SCIs greatly  as well as that for natural images <ref type="bibr" target="#b5">[6]</ref>. Meanwhile, the main difference between SCIs and natural images is that there are textual regions in SCIs as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, and the human attention is easily attracted to understand the meanings of the words. In this study, the luminance and texture features are employed to capture the perceptual distortion in SCIs. First, we apply the local contrast normalization to SCIs to eliminate the redundant information in visual scenes <ref type="bibr" target="#b38">[39]</ref>. Then the statistical luminance features are represented in the form of histogram. The four filters with different directions as defined in Fig. <ref type="figure" target="#fig_2">3</ref> are adopted to compute gradient maps, which are then used to extract the second order derivatives by employing LBP descriptor. Furthermore, we extract the texture information by the second order derivatives. The feature vector is composed of statistical luminance and texture features calculated in three scales. Finally, SVR is adopted to train the quality estimation model from the quality-aware feature vector to subjective ratings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Feature Extraction</head><p>As introduced above, we extract luminance and texture features to capture the variations of SCIs. Here, we extract the statistical luminance features from the normalized luminance map. The normalization operation is conducted as <ref type="bibr" target="#b16">[17]</ref>:</p><formula xml:id="formula_0">S (i, j ) = S(i, j ) -μ s σ s + C (1)</formula><p>where S(i, j ) and S (i, j ) denote the original and normalized values at location (i, j ); i ∈ {1, 2, . . . , I } and </p><formula xml:id="formula_1">μ s = M m=-M H h=-H ω (m,h) S(i + m, j + h) (<label>2</label></formula><formula xml:id="formula_2">)</formula><formula xml:id="formula_3">σ s = M m=-M H h=-H ω (m,h) [S(i + m, j + h) -μ s ] 2<label>(3)</label></formula><p>where {ω (m,h) |m = -M, . . . , M; h = -H, . . . , H } defines a unit-volume Gaussian window; M and H are set to 3. As shown in <ref type="bibr" target="#b38">[39]</ref>, the transformed luminance S (i, j ) follows the generalized Gaussian distribution (GGD) for natural images, while it is inconsistent for SCIs <ref type="bibr" target="#b15">[16]</ref>. Thus, after local normalization, we directly employ absolute value of the transformed luminance S (i, j ) to represent the global luminance information in the form of histogram. Here, the number of the histogram bins is set to 10 and we can obtain 10 elements</p><formula xml:id="formula_4">{ f 1 , f 2 • • • , f 10 } to represent statistical luminance features.</formula><p>The histogram is calculated as below:</p><formula xml:id="formula_5">f n = 1 I J I i=1 J j =1 (|S (i, j )|, Q(n)) (4) (a, b) = 1, a ∈ b 0, other wise (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where n is the bin index of the histogram in the range of <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref>; Q(n) denotes the interval of each bin; I and J are the image height and width, respectively. We have provided some luminance map samples in Fig. <ref type="figure" target="#fig_3">4</ref>. As shown in Fig. <ref type="figure" target="#fig_3">4</ref>, the changes of luminance maps of distorted SCIs degraded by different distortion types are different from each other compared with the luminance map of the original SCI. For Gaussian noise (GN) distortion, many white points appear in the associated luminance map randomly due to random distribution of GN. For Gaussian blur (GB) distorted SCIs, the edge information is preserved and the detailed structure information is destroyed, which can be clearly reflected by the luminance map and the high peak appearing in the corresponding distribution. Different from GB distorted SCIs, part of detailed structure information is preserved in motion blur (MB) distorted SCI. For contrast change (CC) distorted SCIs, the variation in the luminance map is smaller than those with other distortion types. And the corresponding luminance distribution of CC distorted SCI is similar to that of the original SCI. The reason might be that the distortion of contrast change always occurs in global scope and the local normalization is not sensitive to this type of distortion relatively. Meanwhile, the block artifacts are Considering that the visual content of visual scenes could be well represented by orientation information <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, and local orientation information in visual scenes is important to visual perception <ref type="bibr" target="#b33">[34]</ref>, we also extract the luminance information by using the histogram features of pairwise products of neighboring MSCN coefficients along different directions: horizontal (M H ), main-diagonal (M D1 ), vertical (M V ), and secondary-diagonal (M D2 ), as illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. The pairwise products are calculated as follows:</p><p>M H (i, j ) = S(i, j ) S(i, j + 1) (6)</p><formula xml:id="formula_7">M D1 (i, j ) = S(i, j ) S(i + 1, j + 1) (7) M V (i, j ) = S(i, j ) S(i + 1, j ) (8) M D2 (i, j ) = S(i, j ) S(i + 1, j -1)<label>(9)</label></formula><p>Fig. <ref type="figure">5</ref>. The visual samples of feature maps. The first and second rows show the corresponding gradient maps and distributions of (A1-A4) illustrated in Fig. <ref type="figure" target="#fig_3">4</ref>, the third and fourth rows show the the corresponding gradient maps and distributions of (D1-D4) illustrated in Fig. <ref type="figure" target="#fig_3">4</ref>.</p><p>The similar operations as described in Eqs. ( <ref type="formula">4</ref>) and ( <ref type="formula" target="#formula_5">5</ref>) are conducted based on M H , M D1 , M V and M D2 , for i ∈ {1, 2, . . . , I -1} and j ∈ {1, . . . , J -1}. Thus, we can obtain other 40 luminance features in the form of histogram. In total, there are 50 quality-aware features for luminance feature representation extracted for each distorted SCI with one scale.</p><p>Existing studies show that derivative information at different orders is highly correlated with different texture features of natural scenes <ref type="bibr" target="#b34">[35]</ref>- <ref type="bibr" target="#b36">[37]</ref>. The first-order derivative information is generally related to the slope and elasticity of a surface which characterize the geometric properties partially, while the second order derivatives can be used to capture the curvature related geometric properties <ref type="bibr" target="#b36">[37]</ref>. As we know, there are many words in textual regions of SCIs, where the distortion can be well captured by texture information as well as other high-frequency pictorial regions. The second order derivatives can effectively capture the variation of the tiny structure which has influence on visual distortion of SCIs. Here, we first calculate gradient maps denoted by first-order derivative information based on four filters with different directions, as defined in Fig. <ref type="figure" target="#fig_2">3</ref>. The gradient maps are calculated based on the luminance map as follows:</p><formula xml:id="formula_8">G 1 = g 1 S , (<label>10</label></formula><formula xml:id="formula_9">)</formula><formula xml:id="formula_10">G 2 = g 2 S , (<label>11</label></formula><formula xml:id="formula_11">)</formula><formula xml:id="formula_12">G 3 = g 3 S , (<label>12</label></formula><formula xml:id="formula_13">)</formula><formula xml:id="formula_14">G 4 = g 4 S , (<label>13</label></formula><formula xml:id="formula_15">)</formula><p>where S denotes luminance map of the corresponding SCI; g 1 , g 2 , g 3 and g 4 are the filters with four different directions as defined in Fig. <ref type="figure" target="#fig_2">3</ref>; G 1 , G 2 , G 3 and G 4 are the corresponding gradient maps extracted by convolution operation . We provide some gradient map samples in Fig. <ref type="figure">5</ref>. The GB distortion will affect the visual quality of SCI by which the non-connected regions in the SCI may connect and the high-frequency region may become smooth. The orientation information of texture regions in distorted SCI might be also influenced, thus we adopt four filters with different directions to capture the distortion.</p><p>In order to extract the second-order derivative information, we employ the rotation invariant uniform LBP <ref type="bibr" target="#b37">[38]</ref> based on these four gradient maps. The texture features are extracted from the second-order derivative information in the form of histogram. The ordinary LBP formula can be expressed as follows:</p><formula xml:id="formula_16">L B P E,L = E-1 i=0 θ(I i -I c )2 i , (<label>14</label></formula><formula xml:id="formula_17">)</formula><formula xml:id="formula_18">θ(I i -I c ) = ⎧ ⎪ ⎨ ⎪ ⎩ 1, (I i -I c ) ≥ 0 0, (I i -I c ) &lt; 0 (<label>15</label></formula><formula xml:id="formula_19">)</formula><p>where E and L represent the number of neighbors and the radius of the neighborhood; (I 0 , I 1 , . . . ,</p><formula xml:id="formula_20">I (E-1)</formula><p>) denote pixel values of E circularly symmetric neighboring pixels; I c represents intensity of the center pixel in the local region.</p><p>Based on the study <ref type="bibr" target="#b37">[38]</ref>, the local rotation invariant uniform LBP operator can be defined as follows:</p><formula xml:id="formula_21">RL B P E,L = ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ E-1 i=0 θ(I i -L c ), ψ(L B P E,L ) ≤ 2 E + 1, Otherwise (16) ψ(L B P E,L ) = θ(I (E-1) -I c ) -θ(I 0 -I c ) + E-1 i=0 θ(I i -I c ) -θ(I (i-1) -I c ) (<label>17</label></formula><formula xml:id="formula_22">)</formula><p>where ψ is used to calculate the number of bitwise transitions.</p><p>The rotation invariant uniform LBP would have E + 2 distinct patterns and each pattern can be mapped to one bin of a histogram. In our implementation, E is set to 8, and thus there would be 10 bins for each LBP histogram. We calculate texture features by the second-order derivative information based on four gradient maps with the same operation. Totally, there are 40 quality-aware features for texture information extracted for each distorted SCI with one scale. The histograms of texture information are shown in Fig. <ref type="figure">5</ref>, where we can observe that the histograms of texture information are different for different distortion types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Regression Model for Quality Prediction</head><p>Given a distorted SCI, we can obtain 90 features with the above operations for each scale, including 50 features for luminance information and the others for texture information. For a better feature representation, we resize images by downsampling with factors of 2 and 4. Thus, we can obtain three images with three different scales (original image, Image2 by downsampling with factor 2, and Image3 by downsampling with factor 4), we find that there is no improvement in predicting the visual quality of SCIs when the number of scales is more than 3. There are 270 features for each distorted SCI in total. SVR with radial basis function (RBF) kernel is employed as the mapping function from the quality-aware features to subjective ratings <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b49">[50]</ref> by utilizing the LIBSVM package <ref type="bibr" target="#b50">[51]</ref>. In the implementation, we divide the database into training and testing subsets randomly for 1000 times, with 80% as the training dataset and the rest as the testing dataset associated with 4 reference SCIs, and the median performance is reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Screen Content Image Database</head><p>To test the effectiveness of the proposed NRLT, we conduct comparison experiments on SCI database SIQAD <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. There are 1000 SCIs contained in this database in total, including 20 reference SCIs with diverse visual content and 980 distorted versions degraded by seven distortion types (GN, GB, MB, CC, JPEG2000, Layer Segmentation Based Coding (LSC), and JPEG), with seven degradation levels for each distortion type. Although the visual content of images in this database is diverse, there is no SCI for graphics. The textual content mainly includes English content. Since the subjective test for this database was conducted by using computer, image resolutions are adjusted with a certain size for displaying on computer. DMOS values are provided as subjective scores in this database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Methodology</head><p>Here, three commonly used approaches are adopted to calculate the correlation between objective scores (output by an algorithm) and subjective scores (user ratings for SCIs): Pearson Linear Correlation Coefficient (PLCC), Spearman Rank-order Correlation Coefficient (SRCC), and Root Mean Squared Error (RMSE). PLCC can be used to estimate the prediction accuracy, while SRCC can be adopted to evaluate the prediction monotonicity. RMSE is a measure of deviation between the subjective and objective quality scores. In general, a better perceptual quality prediction metric has higher PLCC and SRCC values, and lower RMSE value. Given the n-th image in the database (with N images totally), its subjective and objective quality scores are s n and o n . PLCC can be calculated as follows.</p><formula xml:id="formula_23">P LCC = N n=1 (s n -s)(o n -o) N n=1 (s i -s) * N n=1 (o i -o) , (<label>18</label></formula><formula xml:id="formula_24">)</formula><p>where s and o denote the mean values of s n and o n , respectively.</p><p>We can estimate SRCC as follows.</p><formula xml:id="formula_25">S RCC = 1 - 6 N n=1 d 2 n N(N 2 -1) , (<label>19</label></formula><formula xml:id="formula_26">)</formula><p>where d n is the difference between the n-th image's ranks in subjective and objective results. RMSE can be calculated as follows.</p><formula xml:id="formula_27">RM S E = N n=1 (o n -s n ) 2 N (<label>20</label></formula><formula xml:id="formula_28">)</formula><p>The estimated quality scores by different IQA metrics might have different ranges, we use a five-parameter mapping function to nonlinearly regress the quality scores into a common </p><formula xml:id="formula_29">f (x) = β 1 ( 1 2 - 1 1 + e (β 2 (x-β 3 )) ) + β 4 x + β 5 (<label>21</label></formula><formula xml:id="formula_30">)</formula><p>where (β 1 , . . . , β 5 ) are parameters to be fitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison Experiments</head><p>Here, we compare the proposed NRLT with the following state-of-the-art perceptual quality estimation methods: PSNR, SSIM <ref type="bibr" target="#b5">[6]</ref>, GMSD <ref type="bibr" target="#b11">[12]</ref>, MAD <ref type="bibr" target="#b40">[41]</ref>, IFC <ref type="bibr" target="#b39">[40]</ref>, SPQA <ref type="bibr" target="#b15">[16]</ref>, SQI <ref type="bibr" target="#b18">[19]</ref>, NIQE <ref type="bibr" target="#b16">[17]</ref>, ILNIQE <ref type="bibr" target="#b45">[46]</ref>, BRISQUE <ref type="bibr" target="#b38">[39]</ref>, GMLOG <ref type="bibr" target="#b49">[50]</ref>, GWH-GLBP <ref type="bibr" target="#b51">[52]</ref>, GSS <ref type="bibr" target="#b52">[53]</ref>, EMSQA <ref type="bibr" target="#b53">[54]</ref>, RR-IQA <ref type="bibr" target="#b21">[22]</ref>, BLIQUP-SCI <ref type="bibr" target="#b24">[25]</ref> and BQMS <ref type="bibr" target="#b22">[23]</ref>. Among these metrics, PSNR, IFC, MAD, GMSD, SPQA, SQI, GSS and EMSQA are FR VQA metrics. Specifically, SPQA, SQI, GSS and EMSQA are designed for VQA of SCIs. RR-IQA is a RR VQA method and it is designed for quality prediction of SCIs. NIQE, ILNIQE, BRISQUE, GMLOG, GWH-GLBP, BLIQUP-SCI and BQMS are NR VQA metrics, among which BLIQUP-SCI and BQMS are built specifically for blind quality prediction of SCIs. The performance of VQA metrics is evaluated by three different criteria: PLCC, SRCC and RMSE.</p><p>For FR methods, the results of SPQA, SQI, GSS and EMSQA are taken from the originally published papers. For other FR methods, we use the source code from their public websites to conduct the experiment. For RR and NR methods, the results of RR-IQA, BLIQUP-SCI and BQMS are taken from the originally published papers, and for NIQE and ILNIQE, the pre-trained models are used to calculate the visual quality scores of distorted SCIs directly. For other NR methods and the proposed NRLT, we randomly utilize 80% images in the database for training to learn a mapping function, which is subsequently used to calculate the perceptual quality scores of the rest distorted SCIs. There are 20 reference SCIs and each reference SCI is associated with 49 distorted versions. Thus, there are 784 distorted SCIs in training set associated with 16 reference SCIs, and the rest samples are used for testing. We repeat this operation with 1000 times, and the media performance is reported. The experimental results are shown in Tables <ref type="table" target="#tab_0">I</ref> and<ref type="table" target="#tab_0">II</ref>. From Table <ref type="table" target="#tab_0">I</ref>, we can observe that GMSD can obatin higher accuracy in estimating the perceptual quality of SCIs than other methods designed for quality prediction of natural images, including SSIM, IFC and MAD. Among all the compared metrics, NRLT can obtain competitive performance on perceptual quality evaluation of SCIs compared with FR methods designed for VQA of SCIs such as SPQA, SQI, GSS and EMSQA, and it can get better performance compared with these metrics designed for VQA of natural images.</p><p>From Table <ref type="table" target="#tab_0">II</ref>, we can observe that NIQE and ILNIQE get worse performance on visual quality estimation of SCIs than BRISQUE, RR-IQA, BQMS and NRLT. GMLOG and GWH-GLBP designed based on gradient information can obtain high performance in quality evaluation of SCIs and even better than some FR metrics, which demonstrates that gradient information can capture the distortion of SCIs well. Among the compared methods, NRLT can obtain the best performance of visual quality estimation of SCIs, as demonstrated by the highest PLCC and SRCC values, and the lowest RMSE value of NRLT in Table <ref type="table" target="#tab_0">I</ref>.</p><p>To demonstrate the robustness of the proposed NRLT, we conduct one more experiment by using distortion-based separation. There are 7 distortion types in this database. We use the images with 6 distortion types as the training set and the rest are used as testing set. The median performance is shown in Table <ref type="table" target="#tab_1">III</ref>. From this Table, we can observe that the proposed NRLT can also obtain the consistent results with subjective ratings in estimating the visual quality of SCIs. And it can obtain better performance than other existing related metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Performance Analysis</head><p>As shown in Table <ref type="table" target="#tab_0">I</ref>, compared with the following VQA metrics designed for natural images such as SSIM and GMSD, the VQA metrics including SPQA and SQI designed specifically for VQA of SCIs can achieve better performance in quality estimation for SCIs. The main reason is that these VQA methods designed for SCIs consider the difference between visual perception of pictorial and textual regions in SCIs. There are many words contained in SCIs which tend to attract human attention as well as other high-frequency pictorial regions. In SPQA, different visual quality calculation methods for pictorial and textual regions are designed under the consideration that the HVS perception to pictorial and textual regions are different, and an activity weighting strategy is proposed to predict the effect of pictorial and textual regions to the overall visual quality of SCIs. SQI is a FR VQA metric designed for VQA of SCIs by incorporating viewing field adaption based on the perceptual property that the extent of the visual field used to perceive useful information is much smaller in textual region than that in pictorial region. Also, information content weighting is used to combine the quality scores of pictorial and textual regions to obtain the final quality result of SCIs. Thus, SPQA and SQI can obtain better quality prediction performance for SCIs than these metrics designed for natural images.</p><p>In SPQA and SQI, SCI is divided into pictorial and textual regions for visual quality prediction. Different from these methods, SCI is regarded as a whole when its quality is predicted in GSS, EMSQA and NRLT. SPQA and SQI consider the differences between visual properties of pictorial and textual regions, and a simple linear combination of these quality scores of pictorial and textual regions is used to capture complicated interaction of visual perception for SCIs. As we can observe in Table <ref type="table" target="#tab_0">I</ref>, these methods which do not segment SCI into pictorial and textual regions including GSS, EMSQA and NRLT can also obtain good objective quality prediction results with high correlation with human subjective rating.</p><p>As shown in Table <ref type="table" target="#tab_0">II</ref>, NIQE and ILNIQE deliver lower performance than other methods for the reason that the used features meet a certain distribution of natural images and it is inconsistent with feature distributions of SCIs. Thus, multivariate Gaussian models learnt from natural images are not suitable for VQA of SCIs. Meanwhile, NIQE and ILNIQE measure the deviation of the statistics of distorted image patches from the statistics of reference image patches prelearned from high-quality images. On the contrary, we extract quality-aware features based on the characteristics of the HVS. The normalization operation is used to mimic the nonlinear masking process in early human vision and eliminate the redundant information of visual scenes. The luminance information is extracted in the form of histogram based on the normalized intensity map. Furthermore, we calculate the gradient maps with different directions as the first order derivatives, and LBP is used to compute the second order derivatives. The statistical texture features are obtained in the form of histogram based on the second order derivatives. With the statistical luminance and texture features, the proposed method can obtain the best quality prediction performance for SCIs among the compared NR methods, as shown by experimental results in Table <ref type="table" target="#tab_0">II</ref>.</p><p>Although RR-IQA <ref type="bibr" target="#b21">[22]</ref> uses the reference information to construct the quality prediction model, it employs the average pooling scheme to fuse the similarity of each corresponding bins of histograms, which leads to relatively poor performance of visual quality prediction of SCIs. BLIQUP-SCI <ref type="bibr" target="#b24">[25]</ref> belongs to opinion-unaware quality prediction method, since it does not need the subjective quality scores in the training phase. However, it uses four FR methods designed for natural images to generate the labels, and obtains the final quality score of SCI by a linear summation of local and global quality scores. It cannot obtain promising quality prediction performance of SCIs, since the used four FR methods cannot get good performance for quality prediction of SCIs <ref type="bibr" target="#b15">[16]</ref>. BQMS uses 1000 training samples to fit the linear regression model before NSS feature extraction. Different from BQMS, we utilize histograms to represent the luminance and texture features in the proposed method. Experimental results shown in Table <ref type="table" target="#tab_0">II</ref> demonstrate the effectiveness of the proposed NRLT.</p><p>Meanwhile, we also provide the scatter plots describing the distributions of objective quality scores against DMOS values from different NR methods including BRISQUE, GMLOG, GWH-GLBP and the proposed NRLT in Fig. <ref type="figure" target="#fig_4">6</ref>. From this figure, we can observe that the points from NRLT are more centralized than these from other existing methods, which demonstrates that the predicted visual quality scores by NRLT are more consistent with subjective ratings. These results are consistent with the experimental results in Table <ref type="table" target="#tab_0">II</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Feature Validation</head><p>The quality-aware features extracted in this study mainly include two parts: the first part is luminance features and the second part is texture features. As shown in Fig. <ref type="figure" target="#fig_3">4</ref>, the luminance maps are altered due to the existing distortion, and different distortion types lead to the distinct changes of luminance distributions. For GN distortion, many white points appear in the associated luminance map randomly. For CC distorted SCI, the variation in luminance map is smaller than these with other distortion types, and its corresponding luminance distribution is similar to that of the original SCI. The reason is that the distortion of contrast change always occurs  in global scope and the local normalization is not sensitive to this type of distortion. For GB distorted SCI, the edge information is preserved and the detailed structure information is destroyed, which can be clearly reflected by the luminance map. And high peak appears in the corresponding distribution of GB distorted SCI, as shown in Fig. <ref type="figure" target="#fig_3">4</ref>. On the contrast, part of detailed structure information can be preserved in MB distorted SCI. For GB and MB distorted SCIs, luminance features are more effective than texture features, since the distortions of luminance features are more distinguishable. Although the variations can be reflected by luminance distributions of JPEG, JPEG2000 and LSC distorted SCIs compared with the distribution of original SCI, the differences among distributions of JPEG, JPEG2000 and LSC distorted SCIs are slightly small, as shown in Fig. <ref type="figure" target="#fig_3">4</ref>. In these three cases, texture features are more effective than luminance features in measuring visual distortion of SCIs, since the distributions of texture features are more distinguishable than those of luminance features, as shown in Fig. <ref type="figure">5</ref>.</p><p>In this section, we conduct the comparison experiment to demonstrate the effect of the selected features on SIQAD. We divide the quality-aware features into two groups: luminance features and texture features. Only the luminance features or texture features are used to train the proposed model by SVR. The experimental results are shown in Table <ref type="table" target="#tab_2">IV</ref>. In this table, the first column lists the evaluation index, while the second and third columns list the experimental results by only luminance features or texture features. From this table, we can observe that the proposed method with statistical luminance features can obtain better performance than that with statistical texture features. Overall, the proposed method with both statistical luminance and texture features can obtain better performance than that with only one type of features (statistical luminance features or statistical texture features).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Parameter Setting</head><p>In this experiment, we explore the effect of the parameter setting of feature extraction. One parameter considered in this work is the size of Gaussian window used to calculate the luminance map, which is the basis of feature extraction. Inspired by <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b18">[19]</ref>, the extent of the visual field used to extract useful information in pictorial region is much larger than that in textual region. The sizes of the Gaussian window are set to 3*3, 5*5, 7*7 and 9*9 in the experiment for comparison. The experimental results are shown in Fig. <ref type="figure" target="#fig_5">7</ref>. As we can observe, the performance by the window size 7*7 is superior to those from other cases. Thus, we set the size of Gaussian window to 7*7. During feature extraction, we employ histogram to represent statistical luminance features. In general, the feature histogram with a large number of bins is unstable, while the feature histogram with a small number of bins cannot represent the characteristics of feature distributions well. We have conducted one comparison experiment by using different number of bins to evaluate the performance of feature histograms, where the bin is set to 5, 10, 15 and 20. The experimental results are shown in Fig. <ref type="figure" target="#fig_6">8</ref>. From this figure, we can see that the proposed method can obtain robust quality prediction performance with different histogram bins. We set the number of histogram bins to 10 for the optimal performance. For feature representation, we have extracted the quality-aware features with different number of scales. The experimental results are shown in Table <ref type="table" target="#tab_3">V</ref>. We set the number of histogram bins to 10 for the optimal performance. For feature representation, we have extracted the quality-aware features with different number of scales. The experimental results are shown in Table <ref type="table" target="#tab_3">V</ref>. From this table, we can see that there is no improvement in visual quality prediction of SCIs when the number of scales is more than three. Thus, we set the number of scales to three.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In this study, we have proposed a no reference quality assessment method for SCIs inspired by perceptual properties that the HVS is sensitive to luminance and texture information. The proposed NRLT metric includes two steps. In the first step, a luminance map is computed through local normalization and the histogram of the luminance map is used to represent the statistical luminance features of SCIs in the global scope. Furthermore, four filters with different orientations are used to calculate gradient maps of the luminance map, by which we extract LBP histogram features as statistical texture features for SCIs in the global scope. In the second step, SVR is adopted to train the quality prediction model from qualityaware features to visual quality of SCI. Experimental results demonstrate that the proposed NRLT can obtain superior performance against state-of-the-art approaches, even including some FR VQA metrics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The visual examples of SCIs [16].</figDesc><graphic coords="3,52.43,140.81,78.38,77.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Pairwise products computed along different directions: horizontal, main-diagonal, vertical, and secondary diagonal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Filters for calculating the gradient maps. j ∈ {1, 2, . . . , J } represent the spatial indices; μ s and σ s represent the mean and standard variance values of local region; C is a constant. μ s and σ s can be computed as:</figDesc><graphic coords="3,215.87,140.81,78.38,77.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The visual samples of feature maps. The first and fourth row show SCIs; the second and fifth rows show the corresponding luminance maps; the third and sixth rows show the corresponding luminance histograms. (A1) is the reference SCI. (A2), (A3), (A4), (D1), (D2), (D3), and (D4) represent distorted SCIs by GN, GB, MB, CC, JPEG, JPEG2000, and Layer Segmentation Based Coding.</figDesc><graphic coords="4,95.03,509.21,102.98,74.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The scatter plots of predicted quality scores by different methods against the DMOS values (192) on the SIQAD. The horizontal axis in each figure denotes predicted quality scores and the vertical axis in each figure represents the DMOS values. First row: BRISQUE and GMLOG; Second row: GWH-GLBP and NRLT.</figDesc><graphic coords="8,312.95,133.37,123.50,77.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The comparison results with different sizes of Gaussian window in terms of PLCC and SRCC.</figDesc><graphic coords="9,76.35,152.97,195.47,146.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The comparison results with different number of histogram bins in terms of PLCC and SRCC.</figDesc><graphic coords="9,339.32,57.84,196.48,148.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I EXPERIMENTAL</head><label>I</label><figDesc>RESULTS OF NRLT AND OTHER EXISTING FR METHODS ON SIQAD DATABASE TABLE II EXPERIMENTAL RESULTS OF NRLT AND OTHER EXISTING RR AND NR METHODS ON SIQAD DATABASE space as follows:</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III EXPERIMENTAL</head><label>III</label><figDesc>RESULTS OF NRLT AND OTHER EXISTING METHODS BY DISTORTION-BASED SEPARATION</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV EXPERIMENTAL</head><label>IV</label><figDesc>RESULTS OF DIFFERENT FEATURES</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V EXPERIMENTAL</head><label>V</label><figDesc>RESULTS OF DIFFERENT SCALES</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China under Grant 61571212 and Grant 61772388, and in part by the Natural Science Foundation of Jiangxi, China, under Grant 20071BBE50068, Grant 20171BCB23048, and Grant GJJ160420.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Depth coding based on depth-texture motion and structure similarities</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="275" to="286" />
			<date type="published" when="2015-02">Feb. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Nonlocal in-loop filter: The way toward next-generation video coding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE MultiMedia</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="16" to="26" />
			<date type="published" when="2016-06">Apr. Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Toward scalable systems for big data analytics: A technology tutorial</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="652" to="687" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Screen content image segmentation using robust regression and sparse decomposition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Minaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Emerg. Sel. Topics Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="573" to="584" />
			<date type="published" when="2015-12">Dec. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Overview of the emerging HEVC screen content coding extension</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50" to="62" />
			<date type="published" when="2016-01">Jan. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004-04">Apr. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Asilomar Conf. Signals, Syst., Comput</title>
		<meeting>IEEE Asilomar Conf. Signals, Syst., Comput</meeting>
		<imprint>
			<date type="published" when="2003-11">Nov. 2003</date>
			<biblScope unit="page" from="1398" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Information content weighting for perceptual image quality assessment</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1185" to="1198" />
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">FSIM: A feature similarity index for image quality assessment</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2378" to="2386" />
			<date type="published" when="2011-08">Aug. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Perceptual quality metric with internal generative mechanism</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="54" />
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">VSI: A visual saliency-induced index for perceptual image quality assessment</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4270" to="4281" />
			<date type="published" when="2014-08">Aug. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gradient magnitude similarity deviation: A highly efficient perceptual image quality index</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="684" to="695" />
			<date type="published" when="2014-02">Feb. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Perceptual visual quality metrics: A survey</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commun. Image Represent</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="297" to="312" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mean squared error: Love it or leave it? A new look at signal fidelity measures</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="117" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Subjective quality assessment of screen content images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Workshop QoMEX</title>
		<meeting>IEEE Int. Workshop QoMEX</meeting>
		<imprint>
			<date type="published" when="2014-09">Sep. 2014</date>
			<biblScope unit="page" from="257" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Perceptual quality assessment of screen content images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4408" to="4421" />
			<date type="published" when="2015-11">Nov. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Making a &apos;completely blind&apos; image quality analyzer</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2013-03">Mar. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Eye movements during reading, visual search, and scene perception: An overview</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Castelhano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rayner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cognitive and Cultural Influences on Eye Movements</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Rayner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Shem</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Yan</surname></persName>
		</editor>
		<imprint>
			<publisher>Eds. Psychology Press</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="3" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Objective quality assessment and perceptual compression of screen content images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1109/MCG.2016.46</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Appl., to be published</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Objective quality assessment of screen content images by uncertainty weighting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2016" to="2027" />
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Screen image quality assessment incorporating structural degradation measurement</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Circuits Syst</title>
		<meeting>IEEE Int. Symp. Circuits Syst</meeting>
		<imprint>
			<date type="published" when="2015-05">May 2015</date>
			<biblScope unit="page" from="125" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reduced-reference quality assessment of screen content images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCSVT.2016.2602764</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol., to be published</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning a blind quality evaluation engine of screen content images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">196</biblScope>
			<biblScope unit="page" from="140" to="149" />
			<date type="published" when="2016-07">Jul. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reduced-reference video quality assessment of compressed video sequences</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Ngan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1441" to="1456" />
			<date type="published" when="2012-10">Oct. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Toward a blind quality predictor for screen content images</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSMC.2017.2676180</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern., Syst., to be published</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The &apos;independent components&apos; of natural scenes are edge filters</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="3327" to="3338" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sensitivity for structure gradient in texture discrimination tasks</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Nothdurft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1957" to="1968" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Model of visual contrast gain control and pattern masking</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Amer. A, Opt. Image Sci</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2379" to="2391" />
			<date type="published" when="1997-09">Sep. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Local luminance and contrast in natural images</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Frazor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Geisler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1585" to="1598" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Independence of luminance and contrast in natural scenes and in the early visual system</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Frazor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bonin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Geisler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carandini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neurosci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1690" to="1697" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image information and visual quality</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="430" to="444" />
			<date type="published" when="2006-02">Feb. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visual orientation selectivity based structure description</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4602" to="4613" />
			<date type="published" when="2015-11">Nov. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mach bands and multiscale models of spatial vision: The role of first, second, and third derivative operators in encoding bars and edges</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Georgeson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Local derivative pattern versus local binary pattern: Face recognition with high-order local pattern descriptor</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="533" to="544" />
			<date type="published" when="2010-02">Feb. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">HSOG: A novel local image descriptor based on histograms of the second-order gradients</title>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4680" to="4695" />
			<date type="published" when="2014-11">Nov. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maenpaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002-07">Jul. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment in the spatial domain</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4695" to="4708" />
			<date type="published" when="2012-12">Dec. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An information fidelity criterion for image quality assessment using natural scene statistics</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>De Veciana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2117" to="2128" />
			<date type="published" when="2005-12">Dec. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Most apparent distortion: Fullreference image quality assessment and the role of strategy</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
		<idno>011006:1-01106:21</idno>
	</analytic>
	<monogr>
		<title level="j">J. Electron. Imag</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">No-reference image sharpness assessment in autoregressive parameter space</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3218" to="3231" />
			<date type="published" when="2015-10">Oct. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">No-reference quality assessment of contrast-distorted images based on natural scene statistics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="838" to="842" />
			<date type="published" when="2015-07">Jul. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Blind image quality assessment using the joint statistics of generalized local binary pattern</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Muramatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fujita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="210" />
			<date type="published" when="2015-02">Feb. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment based on statistics of local ternary pattern</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y L</forename><surname>Akamine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C Q</forename><surname>Farias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th IEEE Int. Conf. Quality Multimedia Exper</title>
		<meeting>8th IEEE Int. Conf. Quality Multimedia Exper</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A feature-enriched completely blind image quality evaluator</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2579" to="2591" />
			<date type="published" when="2015-08">Aug. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">No-reference image blur assessment based on discrete orthogonal moments</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bahrami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="50" />
			<date type="published" when="2016-01">Jan. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Reduced-reference image quality assessment by structural similarity estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3378" to="3389" />
			<date type="published" when="2012-08">Aug. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Orientation selectivity based visual pattern for reduced-reference image quality assessment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">351</biblScope>
			<biblScope unit="page" from="18" to="29" />
			<date type="published" when="2016-07">Jul. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Blind image quality assessment using joint statistics of gradient magnitude and Laplacian features</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4850" to="4862" />
			<date type="published" when="2014-11">Nov. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Symp. Technol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">No-reference quality assessment for multiply-distorted images in gradient domain</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="545" />
			<date type="published" when="2016-04">Apr. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Gradient direction for screen content image quality assessment</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-K</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1394" to="1398" />
			<date type="published" when="2016-10">Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Screen content image quality assessment using edge model</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-K</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2016-09">Sep. 2016</date>
			<biblScope unit="page" from="81" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">ESIM: Edge similarity for screen content image quality assessment</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-K</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4818" to="4831" />
			<date type="published" when="2017-10">Oct. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning without human scores for blind image quality assessment</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-K</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="995" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">BSD: Blind image quality assessment based on structural degradation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">236</biblScope>
			<biblScope unit="page" from="93" to="103" />
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Blind image quality assessment with improved natural scene statistics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digit. Signal Process</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2016-10">Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Subjective and objective quality assessment of compressed screen content images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Emerg. Sel. Topics Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="532" to="543" />
			<date type="published" when="2016-12">Dec. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
