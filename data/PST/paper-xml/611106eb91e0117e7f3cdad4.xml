<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Cost-Effective Entangling Prefetcher for Instructions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alberto</forename><surname>Ros</surname></persName>
							<email>aros@ditec.um.es</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Engineering Department</orgName>
								<orgName type="institution">University of Murcia Murcia</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexandra</forename><surname>Jimborean</surname></persName>
							<email>alexandra.jimborean@um.es</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Engineering Department</orgName>
								<orgName type="institution">University of Murcia Murcia</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Cost-Effective Entangling Prefetcher for Instructions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Instruction prefetching</term>
					<term>caches</term>
					<term>entangling</term>
					<term>correlation</term>
					<term>latency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prefetching instructions in the instruction cache is a fundamental technique for designing high-performance computers. There are three key properties to consider when designing an efficient and effective prefetcher: timeliness, coverage, and accuracy. Timeliness is essential, as bringing instructions too early increases the risk of the instructions being evicted from the cache before their use and requesting them too late can lead to the instructions arriving after they are demanded. Coverage is important to reduce the number of instruction cache misses and accuracy to ensure that the prefetcher does not pollute the cache or interacts negatively with the other hardware mechanisms.</p><p>This paper presents the Entangling Prefetcher for Instructions that entangles instructions to maximize timeliness. The prefetcher works by finding which instruction should trigger the prefetch for a subsequent instruction, accounting for the latency of each cache miss. The prefetcher is carefully adjusted to account for both coverage and accuracy. Our evaluation shows that with 40KB of storage, Entangling can increase performance up to 23%, outperforming state-of-the-art prefetchers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>As software-as-a-service and Cloud computing become increasingly popular, server and Cloud applications exhibit notoriously large instruction sets that do not fit in the first level instruction cache (L1I), leading to high L1I miss rates and therefore stalls. This causes significant performance degradation, in addition to wasteful energy expenditure and underutilization of resources.</p><p>Until now, processors have been traditionally designed for scientific and desktop applications, with very different characteristics, making them inefficient for the large and everincreasing instruction footprint of server applications. According to a study conducted by Google <ref type="bibr" target="#b24">[25]</ref> over three years on one of their Warehouse Scale Computing (WSC) live centers with tens of thousands of server machines running workloads and services used by billions of users, processors do useful work for only 10-20% of the time, stalling for more than 80% of the time. This analysis demonstrates that one of the main reasons of stalling is that instructions are not available for execution when running server applications, making the cache and memory systems of server processors a prime optimization target. In particular, this study identifies a significant and growing problem with L1I bottlenecks, due to the instruction footprint of server applications growing at a much higher rate per year than the size of the L1I, conclusions reinforced by several recent studies <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b26">[27]</ref>. They demonstrate that instruction fetching represents a considerable fraction of the memory stalls, together with data accesses, and underline the importance of prefetching for data centers.</p><p>Indeed, as memory latency has been recognized as a critical factor for performance, a plethora of prefetching techniques have been proposed over the last decades <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b47">[48]</ref>- <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b51">[52]</ref>. However, the research community focused predominantly on data prefetching and there is relatively little research done on instruction prefetching, despite its increasing importance with more and more applications being served through the Cloud.</p><p>Basic prefetch mechanisms include simple next line instruction prefetchers <ref type="bibr" target="#b10">[11]</ref> and next line prefetchers of arbitrary sizes <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b42">[43]</ref>, but more advanced ones have been proposed, from prefetchers guided by branch prediction <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b47">[48]</ref> and execution history <ref type="bibr" target="#b50">[51]</ref> to prefetchers that use idle hardware resources to fetch instructions (e.g. run-ahead helper threads <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b51">[52]</ref>).</p><p>One common technique for prefetching employs correlation <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b34">[35]</ref>, namely, building correlations between a memory reference and a previous event, such as memory reference streams, instruction addresses, or branch history, by exploiting temporal or spatial patterns. Temporal prefetchers are a class of correlation-based prefetchers that record sequences of cache misses and predict future misses by replaying the history, reaching higher coverage and accuracy than their predecessors <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, but incurring impractical storage costs.</p><p>Other types of prefetchers interact with hardware structures <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b37">[38]</ref>, such as the branch predictor (e.g. BTB directed), to gain insights into the program's execution ahead of time, however they require intrusive changes in the processor design.</p><p>Typically, prior work in prefetching has adopted look-ahead mechanisms <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b43">[44]</ref> to address both coverage and accuracy. Look-ahead prefetchers follow the execution path n steps in advance and prefetch the corresponding block (instruction, cache line, etc.). The steps may refer to instructions, branches, function calls, etc, while n is typically referred to as the look-ahead distance. Nevertheless, by employing a fixed look-ahead distance, such prefetchers are rigid and cannot timely serve all instruction misses: a long look-ahead distance would bring in the instruction too early, unnecessarily polluting the cache if the instruction is evicted before its use; a short look-ahead distance may prefetch the instruction too late, after it has been demanded by the processor. Yet, look-ahead is a popular technique, if a "good-enough" distance is identified through careful tuning.</p><p>Inspired by previous proposals <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b39">[40]</ref>, this work builds on the observation that different instructions have different fetch-latency and thus require different look-ahead distances for a timely and useful prefetch. Figure <ref type="figure" target="#fig_1">1</ref> illustrates the fraction of timely prefetches given a fixed look-ahead distance, over a selection of server workloads (see Section IV). The look-ahead distance represents the number of taken branches (discontinuities), akin to previous proposals <ref type="bibr" target="#b28">[29]</ref>. To generate this figure, we used a baseline without any prefetching and tracked the L1I misses and their latency using dedicated structures (see Section III for details). For each L1I miss, we computed how many discontinuities in advance a prefetch should be issued not to be late. This can be seen as an oracle to identify the optimal look-ahead distance for each miss and the percentage of total misses each distance would cover. Figure <ref type="figure" target="#fig_1">1</ref> shows the fraction of timely prefetches for distances between 1 and 10, indicating that the remaining fraction of L1I misses are covered with prefetching distances larger than 10. While for this study the look-ahead distance was fixed statically, it is akin to determining a suitable look-ahead distance dynamically, e.g. based on observations performed during a warm-up phase.</p><p>The first remark is that a look-ahead distance fixed for all misses is sub-optimal, as different misses require different distances, or even the same miss can require different distances depending on the execution path. In our proposal, we support several distances simultaneously (even for the same address).</p><p>Second, there is no fixed look-ahead distance to work well across all benchmarks. A look-ahead distance of 1 may prefetch 70% of the L1I misses in a timely manner for one application, but only 20% of the misses for another. At the other end, large look-ahead distances (10+) serve a considerable number of misses (up to 15%) and cannot be neglected in the design of an an effective prefetcher. Complementing Figure <ref type="figure" target="#fig_1">1</ref>, Figure <ref type="figure">2</ref> emphasizes prefetching pollution caused by wrong or early prefetchers (lack of accuracy) if a fixed lookahead distance is used. While some applications can tolerate an increase in the look-ahead distance without loosing accuracy, other (see top lines) can reduce accuracy by 10% when moving from a distance of 1 to 10.</p><p>The departure point of this proposal is timeliness, as a key metric for instruction prefetching. Figures <ref type="figure" target="#fig_1">1 and 2</ref> demonstrate that a fixed look-ahead distance leads to both low coverage (only few of the misses are served in a timely manner) and  low accuracy (only few of the issued prefetchers are useful).</p><p>To approach the performance of an ideal instruction cache (no L1I misses), we propose the Entangling Prefetcher for Instructions, or Entangling I-Prefetcher, 12 which, in contrast to its predecessors, is designed around the notion of timeliness. Entangling computes the latency of cache misses and entangles them with the cache accesses that should trigger the prefetch to ensure the timely arrival of the requested instructions. In this way, Entangling is robust and effective, agnostic to the application characteristics and achieves a 97.6% L1I hit rate, approaching the perfect L1I.</p><p>This paper makes the following contributions:</p><p>• Makes the observation that a significant fraction of L1I misses cannot be timely prefetched by employing a fixed look-ahead distance.</p><p>• Proposes an instruction prefetcher whose core design point is timeliness and demonstrate its effectiveness over a wide selection of benchmarks. • Proposes Entangling as a mechanism to identify the right prefetch time for each cache line -rather than a fixed look-ahead distance. Instead of learning the lookahead distance, which is central to many state-of-the-art methods, our prefetcher builds time aware correlations between past L1I events (handles) and L1I misses and learns which event the prefetch should be associated with so that it is timely.</p><p>• The results demonstrate that Entangling triggers timely prefetches delivering high coverage (88.2%) and accuracy (71.5%), and close to ideal L1I performance (97.6% L1I hit rate), offering the best area vs performance trade-off. • We designed a compression scheme and a clever encoding and table organization, that yields Entangling compact, with low storage demands, such that it makes best use of the allotted hardware budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. THE ENTANGLING I-PREFETCHER</head><p>The key conceptual contribution of the Entangling Iprefetcher is the entanglement (or pairing) of instructions, namely, the instruction upon whose execution should be triggered a timely prefetch for the instruction. In a more concise representation, we define as src-entangled the cache line (also referred to as source) that should trigger the prefetch of the dst-entangled cache line (destination) such that the requested line arrives timely.</p><p>To ensure timeliness, we compute the latency of each cache miss by subtracting the time the requested cache line enters the cache to a recorded timestamp of the cache miss. Once we know the latency of a miss, we can pair the missing cache line to a previous accessed cache line that took place at least latency cycles before the missing access, as shown in Figure <ref type="figure" target="#fig_2">3</ref>. To this end, the Entangling I-prefetcher records a history of the last L1I accessed cache lines. We track back the recorded history looking for the src-entangled cache line fitting our criteria and entangle it with the dst-entangled cache line that missed in cache. Next time the src-entangled line accesses the cache, it will trigger a timely prefetch for the dst-entangled line, transforming the previous miss into a hit.</p><p>As tracking all pairs of entangled cache lines in a program would require considerable storage requirements, the Entangling I-prefetcher only entangles heads of basic blocks, defined as follows. A basic block represents the set of consecutive cache lines, where consecutive refers to the program order of instructions, grouped in cache lines <ref type="bibr" target="#b10">[11]</ref>. The head of a basic block is therefore the first non-consecutive cache line that accesses the cache and the size of the basic block is the number of following consecutive lines. <ref type="foot" target="#foot_2">3</ref> Recording only the basic block head and its size suffices for efficient prefetching of contiguous cache lines. In order to further reduce the number of entangled lines, the Entangling I-prefetcher merges "almost" consecutive basic blocks, entangling only the head of the first basic block.</p><p>The prefetching engine is then triggered upon every cache access to a head of a basic block, and prefetches the entire basic block of the current cache line and of each of its dstentangled cache lines.</p><p>The entangling mechanism is versatile and by design can easily adapt to different (or multiple) execution paths or variations in latency. First, a src-entangled cache line can have multiple dst-entangled cache lines, such that they are all served timely. In turn, a dst-entangled cache line can  <ref type="figure">c, d</ref>, e and the second block, BB2 in access l. Basic block heads are marked with bold. The first column illustrates the history of accesses together with the cycle when each access executed (shown in brackets) and the occurrence of access l that misses in L1I and its latency. The second column illustrates that in order to prefetch BB2, we travel back in history latency cycles from access l and find the instruction that executed latency cycles ahead (access b, part of BB1). We then entangle BB2's head (access l) with the head of BB1 (access a). The third column shows the organization of the cache lines in basic blocks.</p><p>be linked to multiple src-entangled cache lines, thus it can timely prefetched regardless of the execution path. Second, to deal with fluctuations in latency, the Entangling prefetcher constantly creates new entangled pairs on misses and discards pairs that are no longer useful. Note that small latency fluctuations are often accommodated by default, given that the source of the entanglement is a basic block head and not the instruction that precisely matches the latency of fetching the target instruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. A COST-EFFECTIVE IMPLEMENTATION</head><p>This section describes a cost-effective implementation of the Entangling I-prefetcher. We start by presenting a basic implementation and continue with advanced techniques to improve accuracy and reduce storage overhead. Finally, we offer further implementation details and considerations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Basic implementation</head><p>As mentioned, the Entangling prefetcher triggers prefetches on each access to a source-entangled address, prefetching both the next lines within the current basic block and the whole basic block of each of its destination-entangled addresses. In what follows, we detail the hardware mechanisms that identify basic blocks and, for each L1I miss, the position of a timely prefetch (i.e. the source-entangled line). The required hardware and the interaction between the new structures is depicted in Figure <ref type="figure">4</ref>.</p><p>1) Computing basic block sizes: To compute the size of each dynamic basic block fetched by the processor front-end, we constantly track both the head (first) cache line of the current basic block being fetched and its size (see right-top part, Basic block, in Figure <ref type="figure">4</ref>). A simple comparison of the Entangled is a cache-like structure and each entry consists in a src-entangled cache line, its basic block size, and a compressed array of dst-entangled cache lines (for the advanced optimization techniques, a confidence counter is associated to each destination). The L1I, PQ and MSHR are extended with information on timing (the timestamp when the request was issued) and on the src-entangled (position of the source in the Entangled table and an access bit indicating if the access stems from a demand access or a prefetch).</p><p>current access with the head address plus the current size (bb?) indicates whether the new access is for the next line (same basic block) or a non-consecutive cache line. In the first case (+1), the size of the basic block is increased by one. In the second case (new), the basic block ends and we start tracking a new block. When detecting a new basic block, we first store the size of the basic block that just completed in the Entangled table, the core structure of our proposal that records the necessary information to issue prefetches (see left part of Figure <ref type="figure">4</ref>). If the basic block is already recorded in the Entangled table, we update its size to the maximum of the already stored size and the new size. This decision increases the coverage of the prefetcher at the cost of having extra false positives. Finally, we start tracking the new basic block, by updating the head register with the current cache line and resetting the size to 0.</p><p>This mechanism populates the Entangled table with basic blocks that will act as sources of entangled pairs.</p><p>2) Building entangled pairs for timely prefetching: Finding a suitable source-entangled cache line for each L1I miss requires two steps. First, we need to compute the latency of each L1I miss (or L1I prefetch) and second, to identify the cache line executed (at least) latency cycles before the miss. The head of its parent basic block will be the source and the address causing the L1I miss will be the destination of the entangled pair.</p><p>To find potential src-entangled cache lines, we store the recent history of basic block heads together with the timestamp of their first access to L1I in a small circular queue called the History buffer (right-bottom part of Figure <ref type="figure">4</ref>).</p><p>To compute the latency of a demand L1I miss, we require the start and end timestamps. For the start timestamp, we record the time of the demand miss along with the entry allocated by default in the miss status holding register (MSHR). Additionally, two other fields are added to each MSHR entry: an access bit (set to one for demand misses) and a pointer to the position of that access in the History buffer (if the access is a basic block head). Similarly, we keep track of the latency of the prefetches in order to compute the actual latency on late prefetches (a miss for an already prefetched cache line takes place). We extend the prefetch queue (PQ) such that, when a prefetch is issued, it stores the current time along with the currently allocated prefetch entry. An access bit is also added to the PQ, initialized to 0. If the prefetch misses in cache, it is automatically handled as a regular cache miss. Additionally, we ensure that the information held in the PQ is transferred to the MSHR entry allocated by the corresponding cache miss. Otherwise, the information of the PQ entry is discarded. Subsequent demand misses that find the MSHR entry allocated by a prefetch, i.e. with the access bit unset, simply enable the access bit. This indicates that the prefetch was late, as the access resulted in an L1I miss, despite the preceding prefetch.</p><p>The end timestamp corresponds to the time of the cache fill. Upon each cache fill, we check if the access bit in the MSHR is set, indicating that there was either a demand miss or a late prefetch. If in addition the entry has a valid pointer to the History buffer, we know that the miss corresponds to a basic block head. Under these conditions, the Entangling Iprefetcher attempts to find a src-entangled cache line for the newly cached line. The latency of the current memory access is computed by subtracting from the time of the cache fill (end) the timestamp recorded with the MSHR entry (start of the L1I miss). The source is then selected among the accesses that took place at least latency cycles before the miss and is identified by parsing backwards the History buffer, starting from the position of the current access in History. For misses without pointers to the History buffer, no src-entangled is searched for, as such misses will be covered by prefetching the full basic block starting from the head.</p><p>Once a src-entangled cache line is found, the Entangled table is updated and the corresponding src-entangled entry is paired with the newly cached line, which acts as dst-entangled. A src-entangled entry can have several entangled destinations. In the same way, a dst-entangled address can be paired with multiple src-entangled entries. This way, prefetching a cache line reached from different execution paths is automatically supported. The first and fourth column in Figure <ref type="figure" target="#fig_3">5</ref> illustrates when an entangled pair is added to the Entangled table.</p><p>3) Triggering the prefetches: The Entangled table is checked on cache accesses. In case of a hit (1) the entire basic block starting with the accessed cache line is prefetched (i.e., size lines starting from the second line in the basic block);</p><p>(2) for each dst-entangled, the entire basic block starting from dst-entangled is prefetched (for finding its size, the Entangled table is parsed again using the dst-entangled address).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Advanced techniques and optimizations</head><p>1) Adding confidence to deal with latency variation: The latency of a cache miss may vary depending on whether the cache line is fetched from L2, LLC, or main memory, or even due to contention when accessing the structures or routing the packets through the interconnection network. Therefore, an entangled pair that is timely once may not be timely on the next occurrence. To adapt to such variations and trigger timely prefetches we add a confidence counter (a two-bit saturated counter) to each entangled pair.</p><p>New entangled pairs are added to the Entangled table with the confidence set to the maximum value, as they are expected to be timely. The counter is decreased by one upon late or wrong prefetches and increased by one upon timely prefetches. When confidence reaches 0, the entangled pair is considered invalid. When adding a new entangled pair, if the array of destinations is full, the dst-entangled with the lowest confidence is replaced.</p><p>In order to update the confidence, we need to know (1) if the prefetch is late, timely, or wrong, and (2) which is the entangled pair that triggered the prefetch. The first piece of information is provided by the access bit which is copied from the PQ to the MSHR when a prefetch misses in the L1I, and from the MSHR to the L1I on a cache fill. Late prefetches are detected when a cache miss finds the access bit unset in the MSHR (or the PQ). Timely prefetches are detected on a cache hit that finds the access bit unset. Wrong and early prefetches are detected upon a cache line eviction with the access bit unset (the line was unnecessarily brought, i.e., not accessed before being evicted). Figure <ref type="figure" target="#fig_3">5</ref> depicts these scenarios. The second piece of information just requires actually to know the src-entangled address, since the dst-entangled address is the prefetched one. Hence, the src-entangled address is also stored in the PQ, and moved accordingly to the MSHR and L1I.</p><p>We implemented a version of the Entangling prefetcher adding context information in order to increase accuracy. The source was replicated for each different context, resulting in an overloaded Entangled table, that suffers from frequent evictions and consequently achieves lower performance. Moreover, context-based predictions may not always be correct, missing opportunities. Other studies also report little benefit when adding context information (e.g., Markov Predcitors <ref type="bibr" target="#b22">[23]</ref>, end of Section.2.1).</p><p>2) Merging spatio-temporal basic blocks: Reducing the number of entangled basic blocks in the Entangled table is key for keeping storage overhead manageable. To this end, we perform a merge of quasi-consecutive (in time) basic blocks whose addresses overlap or are consecutive (in space). Merging is critical when our prefetcher employs a low-budget Entangled table.</p><p>Merging is also aimed to address scenarios such as the sequence of accessed cache lines: ABCXCD, in which a basic block head C always hits in the cache because it was prefetched as part of another basic block (ABC) and was not evicted. However, D may lead to a substantial number of misses that would not be covered by the Entangling Iprefetcher since it is not a basic block head.</p><p>To address this issue, the size of each basic block is added to the History buffer. The History buffer is inspected on each computed basic block, and if the basic block can be merged with one of the previous basic blocks in the history (i.e., they are consecutive or have overlapping addresses), the size of the previous basic block is updated (the size of the basic block starting with A would become 4, i.e., ABCD) and the merged basic block (starting with C) is not recorded in the History).</p><p>Since we dedicate 6 bits to store the size of a basic block, merging is not performed if the resulting basic block size is larger than 64 cache lines.</p><p>3) Compressing destinations: The Entangled table uses different modes for encoding the array of dst-entangled line address and confidence on 63 bits, as follows: 3 bits for the mode + 60 bits of the dst-entangled line address and the confidence. The destination bits encode the least significant bits (signif B) of the dst-entangled line, starting from the most significant bit that differs from the src-entangled. The most significant bits can be inferred from the source. Since the distance between src-entangled and dst-entangled is typically small, destinations can be highly compressed.</p><p>The mode is a value between 1 and 6 which indicates how many destinations can be kept in the 60 bits of the array of dst-entangled cache lines and the associated confidence. Depending on how many significant bits are required, the number of destinations can vary. If one destination is encoded, the full virtual address of the cache line is stored. For the confidence we always use a 2-bit saturated counter. Table <ref type="table">I</ref> details the available modes.</p><p>All entries of the same dst-entangled array must be represented in the same mode. Hence, every time a new dstentangled entry is inserted, we compute the maximum between its mode and the mode of the previously recorded destinations. To improve compression, upon the eviction of a dst-entangled we re-compute the mode, to ensure that it is not unnecessarily set to a restricting value due to a destination that no longer exists.</p><p>Finally, to maximize the utilization of the Entangled table if the selected src-entangled line has not free destination entries, the prefetcher looks for a second src-entangled line, namely a cache line with the timestamp earlier than the one searched for. If the second line has not a free destination entry, the first line is inserted by evicting an old entry.</p><p>C. Further implementation details 1) Dealing with wrong-path execution: There are two problems arising from wrong-path execution: (i) polluting the cache with prefetches triggered on the wrong path and (ii) training the prefetcher with entangled pairs computed on the wrong path.</p><p>The solution to avoid issuing wrong-path prefetches is to trigger the prefetches when instructions retire, such that they are only issued on the correct path. In Entangled, the latency can be computed by accounting for the time the instruction takes to commit, in addition to the cache miss latency. However, even if the prefetch-on-retire solution is disregarded, the performance degradation may not be significant as the L1I is usually plenty of dead cache lines, that could be evicted without increasing miss rate <ref type="bibr" target="#b15">[16]</ref>. Since the Entangling prefetcher commonly entangles the destination with the most recent timely source, it tolerates L1I evictions better than fixed look-ahead alternatives.</p><p>To avoid polluting the tables with wrong-path information, Entangled keeps the speculatively computed pairs in a separate structure until the destination instruction commits and then updates the Entangled table.</p><p>2) Timing constraints: When triggering the prefetches, on a hit in the Entangled table, a maximum of 6 extra searches (average of 2.5, as presented in Section IV-D) are performed in the Entangled table to find the basic block sizes of the entangled destinations. The Entangled table is indexed with a simple XOR operation of the different bits of the address, and the 16 ways are searched in parallel. The time needed to retrieve the prefetching information is accounted for in the latency of the prefetches so that they are timely, regardless the latency of accessing the Entangled table. All other updates are done out of the critical path of issuing prefetches.</p><p>3) Memory Overhead: The History buffer is a 16-entry circular queue, with a 58-bit tag field, a 20-bit timestamp field, and a 6-bit basic block size field. A 4-bit register points to the head of the queue. The maximum basic block size is therefore 63 cache lines. The total memory required by this structures is 167 bytes.</p><p>The timing and src-entangled information is stored along with PQ (32 entries), MSHR (10 entries) and L1I cache (512 entries). The timing information consists of the time the request was issued (12 bits) and the position of the access in the History buffer (4 bits). The src-entangled information includes the position of the source in the Entangled table (4 bits for the way since we model 16-way entangled tables and 7, 8, or 9 bits for the set, depending of the size of the Entangled table: 2K, 4K or 8K entries, respectively) and an access bit. Once the miss is resolved, the timing information is no longer necessary, thus the L1I cache only records the srcentangled information. The total memory required to store the timing and src-entangled information is about 1KB (915 bytes, 984.25 bytes, and 1053.5 bytes for the 2K, 4K, and 8K entries configurations, respectively).</p><p>The Entangled table is a large set-associative cache that stores sources along with their maximum basic block size and destinations. It employs an enhanced FIFO replacement policy, in which the information in the entry selected for eviction can be reallocated to another entry that does not hold any entangled pair. It has 128, 256, or 512 sets (2K, 4K, and 8K entries configurations, respectively) and 16 ways per set. The tags are encoded using 10 bits, the basic block is encoded with 6 bits, and the format, destinations, and confidence bits are encoded on a total of 63 bits. This is the largest structure employed by our prefetcher and requires 19.81KB (2K entries), 39.63KB (4K entries), or 76.25KB (8K entries). Storing basic block sizes and entangled pairs in different structures is an alternative to a unified Entangled table, likely beneficial for low-storage configurations. We leave this study for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Physical addresses:</head><p>Recent ARM-based architectures employ large virtual L1 caches and as consequence can efficiently train the L1 prefetcher with virtual addresses <ref type="bibr" target="#b17">[18]</ref>. However, for x86 cores employing smaller virtually-indexed physically-tagged caches <ref type="bibr" target="#b19">[20]</ref>, placing the prefetcher in the physical address space is a likely alternative. Otherwise L1 accesses performed by the prefetches would add critical pressure to the translation look-ahead buffer (TLB). Although we have described our design for virtual addresses, it can perfectly work on physical address space, even reducing the storage requirements, e.g., for a 48-bit physical address space.</p><p>If the prefetcher is trained with physical addresses, the compression mechanism can be adapted using 46 bits for (dstentangled line addresses and confidence) as follows: 2 bits for the mode + 44 bits of the dst-entangled line address and the confidence. The mode takes a value between 1 and 4 which indicates how many destinations can be kept in the 44 bits of the array of dst-entangled cache lines and the associated confidence, as detailed in Table <ref type="table">II</ref>. Additionally, the History buffer holds cache line addresses represented on 42 bits instead of 58 bits for virtual. This way, our L1I Entangling prefetcher requires 16.59KB, 32.21KB, and 63.40KB for the versions with 2K, 4K, and 8K entries, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EVALUATION A. Methodology</head><p>We evaluate the Entangled I-prefetcher using a modified version of the ChampSim simulator <ref type="bibr" target="#b0">[1]</ref> employed for the 1st Instruction Prefetching Championship (IPC-1). The ChampSim version used for IPC-1 modeled a simple frontend <ref type="bibr" target="#b20">[21]</ref>. Our modified version extends the current model of the develop branch in ChampSim <ref type="bibr" target="#b1">[2]</ref>, which implements a more realistic decoupled front-end modeling Fetch-Directed Prefetching <ref type="bibr" target="#b37">[38]</ref>, a Branch Target Buffer (BTB), a Target Cache to predict the target of indirect branches <ref type="bibr" target="#b8">[9]</ref>, and a return address stack (RAS). Prefetches issued by the Fetch-Directed Prefetching engine are considered demand accesses, hence our baseline does not report any prefetch request. We extended it to model an out-of-order processor with a seven-stage pipeline as described by González et al. <ref type="bibr" target="#b16">[17]</ref> and different branch misprediction penalty (number of stages to be flushed) depending on the stage the misprediction is detected. The processor and memory hierarchy parameters aim to resemble the latest Intel's Sunny Cove machine. The main configuration parameters of our baseline system are shown in Table <ref type="table">III</ref>. The energy consumption of the cache hierarchy, taking into consideration the energy expenditure of tag accesses, reads, and writes to caches, has been modeled with CACTI-P <ref type="bibr" target="#b31">[32]</ref> for a 22nm process technology.</p><p>The version of ChampSim employed for the IPC-1 trained the L1I prefetchers with virtual addresses. However, as L1I prefetchers can also work with physical addresses, we also provide results training the L1I prefetchers with physical addresses. In that scenario, there is no guarantee that two consecutive virtual memory pages are consecutive in the physical space, slightly reducing the prefetcher's coverage.</p><p>ChampSim does not simulate wrong-path execution, and therefore no wrong-path prefetches are issued. Consequently, in a more realistic implementation, the accuracy of the prefetchers may be reduced. As we explain in Section III-C1, the Entangling prefetcher can avoid wrong-path pollution. All prefetchers evaluated in this work benefit from not modeling the wrong path. We leave for future work examining in detail the implications of wrong-path execution.</p><p>We test our prefetcher on the large secret traces provided in the 1st and 2nd Championship Value Prediction (CVP) <ref type="bibr" target="#b2">[3]</ref> and created by Qualcomm Datacenter Technologies. 4 The traces, which include a set of integer (compute int), floating point (compute fp), cryptography (crypto), and server (srv) 4 IPC-1 used a subset of these traces for evaluating the prefetchers. workloads, have been ported to the ChampSim format. We selected the 959 workloads that showed at least 1 MPKI (miss per kilo-instruction) at the L1I in our baseline configuration.</p><p>The complete analysis has been performed with this set of benchmarks. Additionally, to evaluate our prefetcher on a larger variety of benchmarks with different behaviours, we include performance results on applications from the Cloud-Suite <ref type="bibr" target="#b11">[12]</ref> that exhibit at least 1 MPKI in the L1I. Workloads run until the end after a short 20M-instruction warm-up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluated prefetchers</head><p>We evaluate in detail several state-of-the-art prefetching strategies:</p><p>• Next-line <ref type="bibr" target="#b7">[8]</ref>: A pure next-line prefetcher that always prefetches the next cache line given the current access. It adds no area overhead. • SN4L <ref type="bibr" target="#b5">[6]</ref> is a memory-efficient proposal that implements a 16K-bit vector, where the next four cache lines of the current access are prefetched if the corresponding bit is set, that is, if prefetching that cache line is expected to be useful. It requires only 2.06KB of storage.</p><p>• MANA <ref type="bibr" target="#b4">[5]</ref> is a refinement of SN4L-Dir-BTB <ref type="bibr" target="#b5">[6]</ref> that uses an 8-bit vector for consecutive prefetchers (previously proposed by PIF <ref type="bibr" target="#b13">[14]</ref>). It offers a good performance-area trade-off and it is representative of state-of-the-art BTBdirected instruction prefetchers. We evaluate the two lowcost configurations described by Ansari et al. <ref type="bibr" target="#b4">[5]</ref>: A 2Kentry MANA table (9KB) and a 4K-entry MANA table <ref type="bibr">(17.25KB)</ref>. We also show geometric IPC for an 8K-entry MANA table that requires 74.18KB.</p><p>• RDIP <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b33">[34]</ref> is a RAS-directed instruction prefetcher.</p><p>It records the return address stack and its context as signatures which are then consulted upon each call and return operations to trigger prefetching. We evaluate a 4K-entry miss table with 3 trigger prefetchers for discontinuities and an 8-bit vector for consecutive cache lines. The total storage is 63KB. We also evaluate the three first ranked proposals in IPC-1:</p><p>• D-JOLT <ref type="bibr" target="#b34">[35]</ref>  • Entangling is our proposed Entangling prefetcher. We model three different sizes for the Entangled table: 2K, 4K, and 8K entries. We perform a more aggressive merging of basic blocks in the low-budget configurations, considering merging distances of basic blocks recorded in the history buffer of 15, 6 and 5 for the 2K, 4K, and 8K configurations, respectively. The area requirements as computed in Section III-C3 are 20.87KB, 40.74KB, and 77.44KB, respectively. Finally, we also show the effect on increasing the cache size, instead of using the budget for the prefetching mechanism:</p><p>• L1I-64KB and L1I-96KB increase the associativity of the L1I from 8 ways to 16 ways and 24 ways, respectively, while keeping the L1I access latency to 4 cycles. • An Ideal instruction prefetcher where the L1I cache always returns a hit <ref type="bibr" target="#b33">[34]</ref>. It issues all necessary prefetches to the next level cache, thus modeling the pollution entailed by the L1I cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance results</head><p>We evaluate our Entangling prefetcher and compare it with state-of-the-art prefetchers, presenting a number of different metrics: coverage (percentage of L1I misses covered by prefetching), accuracy (percentage of useful prefetches with respect to the total number of prefetches issued), the L1I miss ratio, and instructions per cycle (IPC) as an indication of performance. Since we evaluate for a large number of proposals and applications (10000+ simulations), we first offer the geometric mean of the instructions per cycle of all the evaluated schemes and then focus on the prefetching techniques that require less that 64KB of storage. 1) Performance vs storage: Figure <ref type="figure" target="#fig_4">6</ref> summarizes the performance of all the prefetchers (geometric mean of the IPC obtained for the 959 CVP workloads, normalized with respect to our baseline) along with their storage requirements. The medium-and low-budget configurations are shown in blue, the high-budget configurations presented in IPC-1 in red, our Entangling prefetcher in green, the large L1I configurations in white, and the Ideal prefetcher as a black line. The Entangling prefetcher achieves 10.1% speedup with respect to the baseline configuration when using 77.44KB (Entangling-8K), while an Ideal L1I cache would offer 11.8% speedup. More interestingly, with 40.74KB overhead, Entangling-4K offers a good area-performance balance achieving 9.60% performance improvements, on par with other area demanding proposals: FNL+MMA 9.12% with 97KB overhead, D-JOLT 10.3% with 125KB, and EPI 10.4% with 127.9KB). For a low-budget configuration of 20.87KB, Entangling-2K offers good performance improvements of 7.50%. Entangling also outperforms all low-budget configurations of MANA and furthermore, the low-budget Entangling version outperforms the high-budget version of MANA.</p><p>2) IPC: Figure <ref type="figure">7</ref> shows the IPC normalized to a configuration without any L1I prefetcher across the CVP workloads. The normalized IPCs have been individually ordered from lower to higher for each configuration. Both the low-and medium-budget configurations of Entangling outperforms the other state of the art prefetchers. The medium-budget configuration of the Entangled prefetcher (4K) is very close to the ideal for many workloads, and only for a few of them, an ideal prefetcher gets significant improvements with respect to our proposal. More importantly, the Entangling prefetcher never gets performance degradation with respect to not using any prefetcher, as it clearly happens with a NextLine prefetcher, and other techniques.</p><p>3) L1I miss rate: Figure <ref type="figure">8</ref> shows the L1I miss ratio for the CVP workloads, again individually ordered from lower to higher for each configuration. The line labeled as no is the baseline configuration without a dedicated L1I prefetcher. The Entangling prefetcher significantly outperforms its competitors across all benchmarks, reducing drastically the miss rate. In the worst case, the Entangling prefetcher reduces the miss rate to just 10% when using 2K entries and to 5% miss rate when using 4K entries. The other evaluated prefetchers report a worst case of 20% miss rate or more. This way, the Entangling prefetcher approaches an ideal L1I cache. 4) L1I prefetcher coverage: Figure <ref type="figure">9</ref> shows the coverage (ratio of misses that became hits) of all prefetchers across all workloads, individually ordered from lower to higher. Mimicking the miss rate figure, the Entangling prefetcher shows a much higher coverage than the state-of-the-art prefetchers. For most workloads, Entagling-4K shows a coverage around 90%. Entangling-2K offers a coverage higher than 68% for most workloads. In contrast, the other prefetchers offer a coverage below 50% for most workloads.</p><p>5) L1I prefetcher accuracy: Figure <ref type="figure" target="#fig_1">10</ref> shows the accuracy (ratio of useful prefetches) across all workloads, individually ordered from lower to higher. Entangling achieves highest accuracy, being above 50% for most workloads, and reaching 90% accuracy for almost 10% of the workloads. RDIP is below 50% accuracy for more than 90% of the workloads, and MANA is below 50% for more than 80% of the workloads. The high accuracy of Entangling indicates that it is the most energy efficient prefetcher in terms of prefetches issued to the higher cache levels (L2, LLC, Memory), as it generates less useless traffic. 6) Energy consumption: Accuracy is a representative indicator of the energy expenditure, as a 100%-accurate prefetcher generates no extra traffic to the higher memory levels with respect to no prefetching. A non-accurate L1I prefetcher would pollute not only the L1I with accesses and cache lines, but also the L2 and LLC caches with unnecessary requests. Table <ref type="table" target="#tab_9">IV</ref> shows the energy expenditure for the caches employed in our system configuration. The Entangling prefetcher has the highest accuracy among the studied prefetchers, thus reducing the energy consumption at the L2 and LLC considerably (38.6% on average compared to NextLine). When accounting  also for the extra L1I accesses generated by the prefetches, the Entangled prefetcher still reduces the overall energy consumption of the memory hierarchy by 2.97%, 3.35%, and 3.39% for the 2K, 4K, and 8K configurations, respectively. In contrast, the most energy-efficient technique, RDIP, adds very few prefetches, and therefore few new accesses, however, many of these prefetches are late.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Analyzing the Entangling Prefetcher</head><p>We first detail the average performance obtained by isolating the proposed techniques for the three analyzed configurations of the Entangling prefetcher (Figure <ref type="figure" target="#fig_7">11</ref>). BB prefetches the whole basic block on the first access to its head (source). BBEnt extends BB and prefetches each dst-entangled cache line, while BBEntBB extends BB by prefetching each dstentangled basic block. Ent does not track basic blocks and entangles all cache lines missing in the cache. Finally, BBEntBB-Merge is our proposal, which extends BBEntBB with the mechanism for merging basic blocks. The key improvements come from entangling pairs of cache lines in a timely manner. Prefetching the dst-entangled basic block (BBEntBB) contributes to some extent to the improvements as less entangled pairs are required to be tracked with respect to Ent. Merging is more relevant for the smaller sizes of the prefetcher, since compression is essential for fitting in the reduced storage budget.</p><p>We also analyze the compression ratio of dst-entangled cache lines by showing in Figure <ref type="figure" target="#fig_1">12</ref> in which format are they represented. We present for each category of workloads provided in CVP (crypto, int, fp, and srv) the arithmetic mean and standard deviation of their workloads. We can observe that almost all destinations can be highly compressed in crypto, fp and int workloads. However, in srv workloads there is a nonnegligible fraction of destination that cannot be compressed. Still the compression rate is quite high in srv workloads, and the majority of the destinations are stored using just 18 bits. The fraction of destinations compressed using just 8 bits ranges from ≈25% in crypto and int to ≈10% in srv. Overall, the average number of destinations found on a hit in the Entangled table ranges from 2.5 for crypto workloads to 2.2 for srv workloads (Figure <ref type="figure" target="#fig_2">13</ref>).</p><p>Finally, we compute the number of prefetches issued on each hit in the Entangled table. Figure <ref type="figure" target="#fig_1">14</ref> and Figure <ref type="figure" target="#fig_3">15</ref> show the average number of cache lines of the currently accessed basic block (omitting the first cache line) and the  number of cache lines for the basic blocks of the entangled destinations (omitting the first cache line), respectively. Then, we can compute the average number of triggered prefetches with the following formula: bbsize + destinations * (1 + bbsize destination). This results in a number of prefetches ranging form ≈9 in srv workloads to ≈17 in fp workloads. Although this number is not dramatically high (our Entangling prefetcher shows the best accuracy among the evaluated prefetchers -Figure <ref type="figure" target="#fig_1">10</ref>), our prefetcher would benefit from a larger prefetch queue (32 entries employed in our evaluation), as less prefetches would be discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Working with physical addresses</head><p>We evaluated all the state-of-the-art prefetchers trained for physical addresses. Our results on the CVP workloads show that the Entangling prefetcher outperforms its competitors, achieving an IPC improvement (geometric mean) over our noprefetch baseline of 5.62%, 8.10%, and 8.87% when using 2K, 4K and 8K entries in the Entangled table, respectively. The trends are similar to the ones observed for virtual addresses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Other applications: the CloudSuite</head><p>Finally, we evaluate the prefetchers on a new class of benchmarks specifically designed to represent Cloud applications, with behaviors that may differ from the ones observed in the previous set of workloads. Figure <ref type="figure" target="#fig_4">16</ref> shows the performance improvements of our Entangled prefetcher for the applications of CloudSuite <ref type="bibr" target="#b11">[12]</ref> that show more than 1 MPKI in L1I. Again, we can observe that the Entangling prefetcher outperforms the other state-of-the-art prefetchers we have evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RELATED WORK</head><p>Driven by their impact on performance, prefetchers have evolved from simple next line prefetchers <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b44">[45]</ref>, to more complex techniques, as described below.</p><p>A common prefetching technique is the look-ahead, which follows the execution path n steps in advance and prefetches the corresponding block (instruction, cache line, etc.). One of the most recent and competitive look-ahead I-prefetchers is the FNL+MMA <ref type="bibr" target="#b43">[44]</ref>, which combines a Footprint Next Line (FNL) prefetcher and a Multiple Miss Ahead (MMA) prefetcher. FNL is an enhanced next line prefetcher that estimates if a block is worth to prefetch, while MMA identifies a "good-enough" look-ahead distance (n) and combines it with a technique to predict the nth next L1I miss. Yet, as we have shown, a fixed look-ahead distance impacts the prefetcher's accuracy and efficiency. Techniques to adjust the look-ahead distance dynamically using heuristics have also been proposed <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b46">[47]</ref>, however, the look-ahead still remains the same (fixed) for all cache misses during certain execution windows.</p><p>By entangling cache misses, the Entangling prefetcher goes along the lines of correlation-based prefetchers <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b34">[35]</ref>. Markov-based prefetchers <ref type="bibr" target="#b22">[23]</ref> use probabilities to predict and prefetch the next cache miss with a fixed look-ahead distance and select to prefetch all or some of the following predicted misses. TIFS <ref type="bibr" target="#b14">[15]</ref> records the history of L1I misses and predicts the next miss and the number of cache lines to be cached, thus being more accurate and timely than simple, next-line prefetchers. The Proactive Instruction Fetch (PIF) prefetcher <ref type="bibr" target="#b12">[13]</ref> improves performance by capturing the cache lines accessed by the committed instructions and instructions from handlers for OS interrupts. PIF operates on the correct-path, retire-order instruction stream, and records the exact instruction fetch sequence which is then used to compute spatial locality. This technique results in a 99.5% I-hit rate on the evaluated benchmarks, but incurs substantial storage overhead (beyond the limits considered in our evaluation). To capture the context of a miss caused by a function call, Returnaddress stack-directed instruction prefetching (RDIP) <ref type="bibr" target="#b28">[29]</ref> records the return address stack and its context as signatures which are then consulted upon each call and return operations to trigger prefetching. RDIP approaches the performance of PIF (within 2%) with significantly lower storage demands, while Entangling significantly outperforms RDIP (8%). A refined solution, D-JOLT <ref type="bibr" target="#b34">[35]</ref> builds more accurate contextbased signatures and further improves performance over its predecessor (RDIP), but it entails a large memory overhead to reach the performance reported by our prefetcher.</p><p>While Entangling and other correlating prefetchers (e.g. Markov, look-ahead based prefetchers such TIFS, PIF, etc) share similarities (events are correlated), the main difference consists in the way these correlations are built: based on distance (number of branches, accesses, misses, etc) vs. correlations (entanglings) based on timeliness (latency expressed in cycles). Previous correlation-based prefetchers use a fixed look-ahead distance, which we show in our motivation figures 1 and 2 that it cannot timely serve all misses.</p><p>For increasing accuracy, prefetchers that interact with the branch prediction mechanism have been proposed <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b49">[50]</ref>. For instance, Kumar et al <ref type="bibr" target="#b29">[30]</ref> leverage the branch target buffer (BTB) and simultaneously prefill the BTB with the branch instructions of each decoded block of instructions, in order to avoid BTB misses. This is achieved by leveraging the information of the I-prefetcher without adding any BTB storage overhead. Generally, instructions prefetchers that depend on the BTB (i.e. BTB-directed-prefetches) are considerably hindered by BTB misses and require significant changes in the processor <ref type="bibr" target="#b30">[31]</ref>. Even attempts to prefill the BTB <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b29">[30]</ref> suffer of high BTB miss rates for applications with very large instruction footprints (e.g. server workloads, the same that usually incur L1I misses). Shotgun <ref type="bibr" target="#b29">[30]</ref> dedicates a significant fraction of the BTB to unconditional (U-)branches and a smaller fraction to conditional (C-)branches, plus a third fraction for return instructions. While the U-branches are handled well thanks to the large dedicated storage, Shotgun remains ineffective for workloads with high C-branches miss rates, due to its mechanism to reactively pre-fill such branches. More recently, Ansari et al <ref type="bibr" target="#b5">[6]</ref> propose SN4L-Dis-BTB, a lightweight prefetcher that reduces storage demands. SN4L-Dis-BTB classifies the misses in three categories and provide tailored solutions for each:</p><p>(1) an enhanced N4L prefetcher to detect worthy-to-prefetch blocks dedicated to cover sequential misses, (2) a discontinuity prefetcher -based on the observation that discontinuities are introduced by branches -designed with extra care for storage and aimed to cover the remaining misses, and finally (3) a Confluence <ref type="bibr" target="#b25">[26]</ref>-like solution that pre-fills the BTB to avoid BTB misses. While this proposal is competitive for very low storage budgets, it cannot fully leverage a larger storage to offer high-performance. MANA <ref type="bibr" target="#b4">[5]</ref>, a follow-up version designed for higher budgets, brings some performance improvements, but is still less competitive than our Entangling prefetcher. Overall, the BTB-guided prefetchers are highly sensitive to BTB misses and branch prediction accuracy. While our Entangling prefetcher operates on basic block headswhich can be seen as branch targets, Entangling is not hindered by BTB misses, being based on cache events correlations, rather than following the branches to issue the prefetch. By learning and building correlations on all (or most frequent) execution paths, Entangling is not sensitive to the accuracy of the branch predictor and has its own mechanisms to deal with changes in execution paths (multiple sources for the same destination, confidence counters, etc). This approach based on correlating cache misses instead of focusing on the prediction of next branch target addresses yields Entangling more robust and less sensitive to predictions, compared to other state-ofthe-art techniques.</p><p>Other designs used stream buffers <ref type="bibr" target="#b23">[24]</ref> as additional hardware structures to prefetch sequences of successive cache lines starting at the miss target, akin to the basic block heads used by our prefetcher.</p><p>Rather than primarily targeting coverage, then accuracy, then latency, Entangling makes a bold step and targets time-liness first, using a novel approach that proves to be highly effective, thus achieving higher accuracy than its predecessors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>The Entangling prefetcher for instructions offers an alternative prefetching direction driven by timeliness. Entangling estimates the latency of the cache missing operations and entangles them with the instructions that should trigger the prefetch to ensure the timely arrival of the requested instructions. It pairs sequences of mostly sequential cache lines: the source sequence gets associated with one or more destination sequences. On a cache access to the first cache line in the source sequence, Entangling generates prefetches for all lines in the source sequence as well as for all lines in the associated destination sequences (if confident). The pairing between source and destinations is done by measuring the cache miss latency of the first line in a sequence, then using an auxiliary structure to locate an earlier sequence that started more than latency cycles ahead of the current sequence, and adding the current sequence to the destination sequence list.</p><p>By considering the sequences of cache lines granularity, Entangling subsumes the next line prefetching. It also uses a novel compression scheme, depending on the distance relationship between the source and destinations, and a clever encoding and table organization, which keeps storage at bay. The design does not require access to the branch prediction structures, does not add contention to the critical structures, and does not entail large associative searches. Thus, the implementation of the Entangling prefetcher is highly efficient without being intrusive in the processor design. It is robust and effective, agnostic to the application characteristics and achieves a 97.6% L1I hit rate, approaching a perfect L1I, clearly outperforming state-of-the-art proposals and offering a good area-performance trade-off.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This work was supported by the European Research Council under the European Union's Horizon 2020 research and innovation programme (grant agreement No 819134) and by the Ramón y Cajal Research Contract (RYC2018-025200-I).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Fraction of timely prefetches with respect to the look-ahead distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. A running example: The first basic block, BB1, consists in accesses a, b, c, d, e and the second block, BB2 in access l. Basic block heads are marked with bold. The first column illustrates the history of accesses together with the cycle when each access executed (shown in brackets) and the occurrence of access l that misses in L1I and its latency. The second column illustrates that in order to prefetch BB2, we travel back in history latency cycles from access l and find the instruction that executed latency cycles ahead (access b, part of BB1). We then entangle BB2's head (access l) with the head of BB1 (access a). The third column shows the organization of the cache lines in basic blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Actions taken on prefetch misses and cache accesses, misses, fills, and evictions. Entanglements are added for misses and late prefetches by searching for a source in the History buffer. The confidence counter is increased on prefetch hits and decreased on late and wrong prefetches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. IPC vs memory requirements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig. 7. IPC normalized to our baseline configuration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Fig. 9. Coverage of the instruction prefetchers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Breakdown of the contributions to performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 13 .Fig. 14 .</head><label>1314</label><figDesc>Fig. 13. Average number of entangled destinations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>is a refinement of RDIP. First, it implements more accurate context-based signatures. Second, it uses a dual look-ahead distance mechanism to generate prefetches. We evaluate an 8KB entry miss table, which gives a total storage of 125KB.• FNL-MMA<ref type="bibr" target="#b43">[44]</ref> combines a Footprint Next Line (FNL) prefetcher and a Multiple Miss Ahead (MMA) prefetcher. FNL is an enhanced next line prefetcher that estimates if a cache line is worth to prefetch, while MMA selects the look-ahead distance. We evaluate an 8K entry miss table which gives a total storage of 97KB. • EPI [41] a performance-oriented and hardly implementable -as the previous two version-of the Entangled prefetcher. It models highly associative structures (e.g., a +1000-entry history buffer, and a 34-way Entangled table which gives +8K entries). Its total storage requirements are 127.9KB. In addition, we also evaluate three different configurations of our cost-effective Entangling prefetcher:</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE IV AVERAGE</head><label>IV</label><figDesc>ENERGY CONSUMED AT EACH CACHE LEVEL (IN NJ) AND GEOMETRIC MEAN OF NORMALIZED ENERGY</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">no NextLine</cell><cell cols="4">SN4L MANA-2K MANA-4K Entangling-2K Entangling-4K</cell><cell>RDIP</cell></row><row><cell></cell><cell></cell><cell cols="2">Average L1I energy (nJ)</cell><cell>129222</cell><cell cols="2">144473 155196</cell><cell>162745</cell><cell>166312</cell><cell>185197</cell><cell>191868 144604</cell></row><row><cell></cell><cell></cell><cell cols="2">Average L1D energy (nJ)</cell><cell>329322</cell><cell cols="2">328837 327665</cell><cell>326637</cell><cell>325902</cell><cell>324513</cell><cell>323331 326236</cell></row><row><cell></cell><cell></cell><cell cols="2">Average L2C energy (nJ)</cell><cell>138535</cell><cell cols="2">125477 115300</cell><cell>107774</cell><cell>99581</cell><cell>78754</cell><cell>73942</cell><cell>76449</cell></row><row><cell></cell><cell></cell><cell cols="2">Average LLC energy (nJ)</cell><cell>78039</cell><cell>92826</cell><cell>78105</cell><cell>69385</cell><cell>65995</cell><cell>63543</cell><cell>60117</cell><cell>62095</cell></row><row><cell></cell><cell></cell><cell>Geomean (norm.)</cell><cell></cell><cell></cell><cell>1.0250</cell><cell>1.0043</cell><cell>0.9914</cell><cell>0.9786</cell><cell>0.9703</cell><cell>0.9665</cell><cell>0.9082</cell></row><row><cell>Average basic block size</cell><cell>0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0</cell><cell cols="5">crypto Entangling-2K Entangling-4K Entangling-8K fp int srv</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="6">Fig. 15. Average basic block size of entangled destinations.</cell><cell></cell><cell></cell></row><row><cell>Normalized IPC</cell><cell>1.0 1.02 1.04 1.06 1.08 1.1 1.12</cell><cell>NextLine SN4L MANA-4K MANA-2K</cell><cell cols="2">Entangling-2K Entangling-4K ideal</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>cassandra</cell><cell cols="2">cloud9</cell><cell>nutch</cell><cell>streaming</cell><cell></cell><cell></cell></row></table><note>Fig. 16. Normalized IPC for CloudSuite applications.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">A performance-oriented version of the Entangling Instruction Prefetcher<ref type="bibr" target="#b40">[41]</ref>,<ref type="bibr" target="#b41">[42]</ref> won the 1st Instruction Prefetch Championship (IPC1).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">The code of the Entangling prefetcher proposed in this paper is available at https://github.com/alberto-ros/EntanglingInstructionPrefetcher</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">Note that this is a dynamic view of a basic block and it can change depending on the execution path.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We would like to thank Ali Ansari, Gino Chacon, Nathan Gober, Daniel Jiménez, Tomoki Nakamura, Seth Pugsley, and André Seznec for their contributions to the ChampSim ecosystem, help in evaluating related work, and discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">ChampSim simulator</title>
		<ptr target="http://github.com/ChampSim/ChampSim" />
		<imprint>
			<date type="published" when="2020-05">May 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">ChampSim simulator, develop branch</title>
		<ptr target="https://github.com/ChampSim/ChampSim/tree/develop" />
		<imprint>
			<date type="published" when="2020-11">Nov. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The Second Championship Value Prediction</title>
		<ptr target="https://www.microarch.org/cvp1/" />
		<imprint>
			<date type="published" when="2020-11">Nov. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hardware support for prescient instruction prefetch</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Aamodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hammarlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th Int&apos;l Symp. on High-Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2004-02">Feb. 2004</date>
			<biblScope unit="page" from="84" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mana: Microarchitecting an instruction prefetcher</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Golshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sarbazi-Azad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 1st Instruction Prefetching Championship (IPC1)</title>
				<imprint>
			<date type="published" when="2020-05">May 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Divide and conquer frontend bottleneck</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sarbazi-Azad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">47th Int&apos;l Symp. on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2020-05">May 2020</date>
			<biblScope unit="page" from="65" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Asmdb: Understanding and mitigating front-end stalls in warehouse-scale computers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Nagendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>August</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kanev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Litz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">46th Int&apos;l Symp. on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2019-06">Jun. 2019</date>
			<biblScope unit="page" from="462" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Baer</surname></persName>
		</author>
		<title level="m">Microprocessor Architecture: From Simple Pipelines to Chip Multiprocessors</title>
				<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>1st ed.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Target prediction for indirect jumps</title>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th Int&apos;l Symp. on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="1997-06">Jun. 1997</date>
			<biblScope unit="page" from="274" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Instruction prefetching using branch prediction information</title>
		<author>
			<persName><forename type="first">I.-C</forename><forename type="middle">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1997 Int&apos;l Conf. on Computer Design (ICCD)</title>
				<imprint>
			<date type="published" when="1997-10">Oct. 1997</date>
			<biblScope unit="page" from="593" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A Primer on Hardware Prefetching, ser. Synthesis Lectures on Computer Architecture</title>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Clearing the clouds: A study of emerging scale-out workloads on modern hardware</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">O</forename><surname>Koberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alisafaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th Int&apos;l Conf. on Architectural Support for Programming Language and Operating Systems (ASPLOS)</title>
				<imprint>
			<date type="published" when="2012-03">Mar. 2012</date>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Proactive instruction fetch</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">44th Int&apos;l Symp. on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="2011-12">Dec. 2011</date>
			<biblScope unit="page" from="152" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cuckoo directory: A scalable directory for many-core systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Balet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th Int&apos;l Symp. on High-Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2011-02">Feb. 2011</date>
			<biblScope unit="page" from="169" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Temporal instruction fetch streaming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">41th Int&apos;l Symp. on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="2008-11">Nov. 2008</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Temporal ancestry prefetcher</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gober</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chacon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gratz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 1st Instruction Prefetching Championship (IPC1)</title>
				<imprint>
			<date type="published" when="2020-05">May 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Processor Microarchitecture: An Implementation Perspective, ser. Synthesis Lectures on Computer Architecture</title>
		<author>
			<persName><forename type="first">A</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Latorre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Magklis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Evolution of the samsung exynos cpu microarchitecture</title>
		<author>
			<persName><forename type="first">B</forename><surname>Grayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rupley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Zuraski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Quinnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kitchin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hensley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brekelbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">47th Int&apos;l Symp. on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2020-06">Jun. 2020</date>
			<biblScope unit="page" from="40" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tcp: Tag correlating prefetchers</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th Int&apos;l Symp. on High-Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2003-02">Feb. 2003</date>
			<biblScope unit="page" from="317" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Intel ® 64 and ia-32 architectures optimization reference manual</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<ptr target="www.intel.com" />
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rebasing instruction prefetching: An industry perspective</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nathella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sunwoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<date type="published" when="2020-10">Oct. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fix the code. don&apos;t tweak the hardware: A new compiler approach to voltage-frequency scaling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jimborean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Koukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Spiliopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Black-Schaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th Int&apos;l Symp. on Code Generation and Optimization (CGO)</title>
				<imprint>
			<date type="published" when="2014-02">Feb. 2014</date>
			<biblScope unit="page" from="262" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Prefetching using markov predictors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th Int&apos;l Symp. on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="1997-06">Jun. 1997</date>
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th Int&apos;l Symp. on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="1990-06">Jun. 1990</date>
			<biblScope unit="page" from="364" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Profiling a warehouse-scale computer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kanev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Darago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">42nd Int&apos;l Symp. on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page">158169</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Confluence: Unified instruction supply for scale-out servers</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">48th Int&apos;l Symp. on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
			<biblScope unit="page" from="166" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">I-spy: Context-driven conditional instruction prefetching with coalescing</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sriraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devietti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pokam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Litz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kasikci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">53rd Int&apos;l Symp. on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="2020-10">Oct. 2020</date>
			<biblScope unit="page" from="146" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Path confidence based lookahead prefetching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L N</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">49th Int&apos;l Symp. on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="2016-10">Oct. 2016</date>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rdip: Return-address-stack directed instruction prefetching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kolli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">46th Int&apos;l Symp. on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="2013-12">Dec. 2013</date>
			<biblScope unit="page" from="260" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Blasting through the front-end bottleneck with shotgun</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd Int&apos;l Conf. on Architectural Support for Programming Language and Operating Systems (ASPLOS)</title>
				<imprint>
			<date type="published" when="2018-03">Mar. 2018</date>
			<biblScope unit="page" from="30" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Boomerang: A metadata-free architecture for control flow delivery</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd Int&apos;l Symp. on High-Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2017-02">Feb. 2017</date>
			<biblScope unit="page" from="493" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cactip: Architecture-level modeling for sram-based structures with advanced leakage reduction techniques</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 Int&apos;l Conf. on Computer-Aided Design (ICCAD)</title>
				<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="page" from="694" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Best-offset hardware prefetching</title>
		<author>
			<persName><forename type="first">P</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd Int&apos;l Symp. on High-Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2016-03">Mar. 2016</date>
			<biblScope unit="page" from="469" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">ChampSim</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nakamura</surname></persName>
		</author>
		<ptr target="https://github.com/tomokinex/ChampSim" />
		<imprint>
			<date type="published" when="2020-11">Nov. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">D-jolt: Distant jolt prefetcher</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Koizumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Degawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shioya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 1st Instruction Prefetching Championship (IPC1)</title>
				<imprint>
			<date type="published" when="2020-05">May 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Wrong-path instruction prefetching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th Int&apos;l Symp. on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="1996-12">Dec. 1996</date>
			<biblScope unit="page" from="165" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fetching instruction streams</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Larriba-Pey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">35th Int&apos;l Symp. on Microarchitecture (MICRO)</title>
				<imprint>
			<date>Nov. 371-382</date>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fetch directed instruction prefetching</title>
		<author>
			<persName><forename type="first">G</forename><surname>Reinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd Int&apos;l Symp. on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="1999-12">Dec. 1999</date>
			<biblScope unit="page" from="16" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Optimizations enabled by a decoupled front-end architecture</title>
		<author>
			<persName><forename type="first">G</forename><surname>Reinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers (TC)</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="338" to="355" />
			<date type="published" when="2001-04">Apr. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Berti: A per-page best-request-time delta prefetcher</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 3rd Data Prefetching Championship</title>
				<imprint>
			<date type="published" when="2019-06">Jun. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The entangling instruction prefetcher</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jimborean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 1st Instruction Prefetching Championship (IPC1)</title>
				<imprint>
			<date type="published" when="2020-05">May 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The entangling instruction prefetcher</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="84" to="87" />
			<date type="published" when="2020-07">Jul. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Enlarging instruction streams</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers (TC)</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1342" to="1357" />
			<date type="published" when="2007-10">Oct. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The fnl+mma instruction cache prefetcher</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 1st Instruction Prefetching Championship (IPC1)</title>
				<imprint>
			<date type="published" when="2020-05">May 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sequential program prefetching in memory hierarchies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7" to="21" />
			<date type="published" when="1978-12">Dec. 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Effective instruction prefetching in chip multiprocessors for modern commercial applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Spracklen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th Int&apos;l Symp. on High-Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2005-02">Feb. 2005</date>
			<biblScope unit="page" from="225" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Feedback directed prefetching: Improving the performance and bandwidth-efficiency of hardware prefetchers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Srinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th Int&apos;l Symp. on High-Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2007-02">Feb. 2007</date>
			<biblScope unit="page" from="63" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Branch history guided instruction prefetching</title>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Tyson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Charney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Puzak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th Int&apos;l Symp. on High-Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2001-01">Jan. 2001</date>
			<biblScope unit="page" from="291" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Slipstream processors: Improving both performance and fault tolerance</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sundaramoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Purser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rotenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th Int&apos;l Conf. on Architectural Support for Programming Language and Operating Systems (ASPLOS)</title>
				<imprint>
			<date type="published" when="2000-11">Nov. 2000</date>
			<biblScope unit="page" from="257" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Instruction cache prefetching using multilevel branch prediction</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Veidenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1997 High Performance Computing, Int&apos;l Symp. (ISHPC)</title>
				<imprint>
			<date type="published" when="1997-11">Nov. 1997</date>
			<biblScope unit="page" from="51" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Execution history guided instruction prefetching</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Haga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th Int&apos;l Conf. on Supercomputing (ICS)</title>
				<imprint>
			<date type="published" when="2002-06">Jun. 2002</date>
			<biblScope unit="page" from="199" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Execution-based prediction using speculative slices</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Zilles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th Int&apos;l Symp. on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2001-06">Jun. 2001</date>
			<biblScope unit="page" from="2" to="13" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
