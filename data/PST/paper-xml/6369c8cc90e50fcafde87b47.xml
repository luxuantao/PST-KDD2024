<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Inductive Graph Transformer for Delivery Time Estimation</title>
				<funder ref="#_fQZ4aWB">
					<orgName type="full">Alibaba Group through Alibaba Innovative Research (AIR) Program and Alibaba-NTU Singapore Joint Research Institute</orgName>
					<orgName type="abbreviated">JRI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-11-05">5 Nov 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xin</forename><surname>Zhou</surname></persName>
							<email>xin.zhou@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinglong</forename><surname>Wang</surname></persName>
							<email>jinglong.wjl@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xingyu</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiqi</forename><surname>Shen</surname></persName>
							<email>zqshen@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cyril</forename><surname>Leung</surname></persName>
							<email>cleung@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Inductive Graph Transformer for Delivery Time Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-11-05">5 Nov 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2211.02863v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Delivery Time Estimation</term>
					<term>Estimated Time of package Arrival</term>
					<term>Inductive Graph Transformer</term>
					<term>Graph Convolutional Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Providing accurate estimated time of package delivery on users' purchasing pages for e-commerce platforms is of great importance to their purchasing decisions and post-purchase experiences. Although this problem shares some common issues with the conventional estimated time of arrival (ETA), it is more challenging with the following aspects: 1) Inductive inference. Models are required to predict ETA for orders with unseen retailers and addresses; 2) High-order interaction of order semantic information. Apart from the spatio-temporal features, the estimated time also varies greatly with other factors, such as the packaging efficiency of retailers, as well as the high-order interaction of these factors. In this paper, we propose an inductive graph transformer (IGT) that leverages raw feature information and structural graph data to estimate package delivery time. Different from previous graph transformer architectures, IGT adopts a decoupled pipeline and trains transformer as a regression function that can capture the multiplex information from both raw feature and dense embeddings encoded by a graph neural network (GNN). In addition, we further simplify the GNN structure by removing its non-linear activation and the learnable linear transformation matrix. The reduced parameter search space and linear information propagation in the simplified GNN enable the IGT to be applied in large-scale industrial scenarios. Experiments on real-world logistics datasets show that our proposed model can significantly outperform the state-of-the-art methods on estimation of delivery time. The source code is available at: https://github.com/enoche/IGT-WSDM23.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Computing methodologies ? Neural networks; Learning latent representations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The invention of the World Wide Web enables users to share information and products on the Web, which eventually drives the development of e-commerce platforms <ref type="bibr" target="#b29">[30]</ref>. The contemporary prevalence of e-commerce leads to millions of packages being delivered worldwide every day <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27]</ref>. Retailers on e-commerce platforms show consumers various information about their products and services, such as prices, descriptions, and estimated delivery time. The delivery time is valued more important than price when consumers have a deadline to receive their packages. It has been shown to affect the decisions of 87% online consumers <ref type="bibr" target="#b4">[5]</ref>. Hence, the accurate estimation of package arrival time (ETA) not only impacts the e-commerce revenue but also shapes the consumers' expectations. Either over-promising or under-promising ETA may result in detriment of online retailers and consumers.</p><p>In practice, the physical delivery time is always subject to uncertainty due to the packaging efficiency of retailers, inventory, scheduling, or transportation factors. In our scenario, we study the origin-destination (OD) delivery time estimation, which aims to predict the package delivery time given the information of retailer, OD addresses, and payment time. Although the OD travel time estimation is widely studied in the transportation domain <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22]</ref>, its challenges are usually different from those of OD ETA in the delivery scenario. For example, both problems face the challenges of the absence of route information and efficiently utilizing the spatio-temporal features. However, the OD ETA in the delivery scenario inevitably involves predicting orders with unseen retailers or addresses, etc. Furthermore, its prediction varies greatly according to multiplex signals (e.g., retailer, address, payment time) and the high-order interaction of these signals.</p><p>Existing works in the transportation domain <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22]</ref> either assume all origins and destinations can be observed for spatial network construction or make straightforward use of the raw features. Graph-based methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22]</ref> exploit the power of graph neural networks (GNNs) to learn the representations of trips. However, these approaches require that all addresses in the OD graph are present during model training, making them hard to be generalized to unseen trips. Another line of work <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b34">35]</ref> utilizes deep neural networks to predict ETA based on the raw features. However, they failed in capturing the high-order semantic information that occurred in the interactions between the attributes of a package (e.g., retailer, addresses, and payment time), which may also affect the accuracy of ETA prediction. The closest study is the food delivery time estimation conducted by Alibaba group on Ele.me <ref type="bibr" target="#b41">[42]</ref>. However, it focuses on city-level food delivery and highlights its contributions on feature engineering of existing models.</p><p>In this paper, we propose an inductive graph transformer (IGT) for the estimation of package delivery time on a national level. Contrary to currently coupled graph transformers <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17]</ref> that leverage transformer as a substitution of information aggregator in GNN, we decouple the GNN and transformer pipeline in IGT. Specifically, we use transformer as a regression function and frame collected orders into sentences of NLP. Analogous to word in a sentence for language processing, we denote retailer, addresses, and payment time as "element" in an order. In this way, we can represent orders as a heterogeneous graph with different types of nodes (i.e., element) and capture the high-order semantic relations with information propagation of GNN. Furthermore, we adapt our model to large-scale scenarios by simplifying GNN with the removal of non-linear activation and linear transformation matrix. In the following sections, we will be using the terms "element" and "node" interchangeably based on the discussed context. With our simplified GNN, the embeddings of unseen elements in an order can be approximated from the observed neighbor elements. In practice, orders have periodicity and recurring natures. We capture the temporal evolution of elements with a gated recurrent unit (GRU) <ref type="bibr" target="#b2">[3]</ref>. Finally, we fuse the node embeddings and raw features of orders into a transformer-like architecture for ETA prediction.</p><p>In summary, the contributions made in this work are as follows:</p><p>? We propose an inductive graph transformer for ETA prediction in package delivery. Instead of coupling transformer as an alternative attention mechanism for neighborhood information aggregation in GNNs, we use a decoupled architecture that feeds both extracted heterogeneous information from GNNs and raw features of orders into transformers for ETA prediction. ? We design a simplified heterogeneous graph convolutional network that can encode high-order semantic information of orders into latent element embeddings and dynamically update element embeddings with GRU. Specifically, we remove the non-linear activation and the linear transformation matrix of the current graph convolutional network to adapt to large-scale industrial scenarios. As a result, only the linear operator for information aggregation and propagation is retained in IGT. ? We conduct comprehensive experiments on two large-scale logistics datasets, and the experimental results show that the proposed IGT model is significantly superior to the existing methods.</p><p>It is worth noting that although we mainly focus on the ETA prediction for package delivery, the proposed IGT model could be applied to any other similar scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Estimated Time of Arrival</head><p>Previous ETA prediction methods can be categorized into two main groups: 1) route-based methods and 2) OD-based models.</p><p>2.1.1 Route-based Methods. Apparently, route-based models take route information into consideration. With the available of route information, a line of work uses (graph) neural networks to encode the spatial and temporal dependency. For example, WDR <ref type="bibr" target="#b32">[33]</ref> devises a Wide-Deep-Recurrent (WDR) architecture based on sequential features to formulate ETA as a regression problem. Com-pactETA <ref type="bibr" target="#b11">[12]</ref> and ConSTGAT <ref type="bibr" target="#b10">[11]</ref> exploit graph attention networks (GAT) <ref type="bibr" target="#b28">[29]</ref> on a spatio-temporal graph for representation learning. HetETA <ref type="bibr" target="#b14">[15]</ref> models different turning directions into a heterogeneous information graph and then learns the representation of the graph for ETA prediction. Note that the road structure in HetETA is assumed to be static, which is different from the package delivery in which graphs evolve dynamically. In logistics system, DeepETA uses a spatial-temporal sequential neural network to capture the regularity of the delivery pattern and the sequence of packages in a delivery route <ref type="bibr" target="#b34">[35]</ref>. The above networks assume the nodes are present in the training of the embeddings and do not apply to inductive settings.</p><p>2.1.2 OD-based Methods. Due to privacy consideration, it is usually difficult to obtain the path information in many application scenarios. Under such a setting, OD-based methods that utilize the features of origin and destination are proposed for ETA prediction. In <ref type="bibr" target="#b30">[31]</ref>, the authors propose a simple model to predict travel time of a given trip by weighing the travel time of neighbors. The neighbors are filtered according to both the spatial information of origin and destination and the temporal information of trip starting time. Li et al. develop a novel representation learning framework for travel time prediction, which uses road network, spatio-temporal smoothing prior, and additional trip attributes to represent the real world trip attributes <ref type="bibr" target="#b21">[22]</ref>. The model can distill road network information in historical trips for accurate ETA prediction. However, it assumes the path information can be observed in historical trips, which is lack of generalization. The most relevant existing work is the ETA prediction for food delivery <ref type="bibr" target="#b41">[42]</ref>, which is as complex as our problem because it is dependent on numerous features from couriers, restaurants, and traffic. However, this work mainly focuses on the exploitation of feature engineering to feed existing models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Transformers</head><p>Transformer <ref type="bibr" target="#b27">[28]</ref>, GNNs, and their variants have achieved great success in many fields. A line of work has been proposed recently to fuse the two as a graph transformer. For example, Yun et al. develop graph transformer networks on heterogeneous graphs targeting to transform a given heterogeneous graph into a meta-path based graph and then perform convolution consequently <ref type="bibr" target="#b36">[37]</ref>. Notably, their focus behind the use of attention framework is for interpreting the generated meta-paths. Another line of transformer on heterogeneous information networks is developed in <ref type="bibr" target="#b16">[17]</ref>. The proposed graph transformer uses transformer to substitute the attention mechanism in GAT. Furthermore, Zhou et al. propose a transformer based generative model which generates temporal graphs by directly learning from dynamic information in networks <ref type="bibr" target="#b37">[38]</ref>. In <ref type="bibr" target="#b9">[10]</ref>, the authors propose a generalization of transformer neural network architecture for homogeneous graphs of arbitrary structure. These existing methods generally employ the attention mechanism in transformer for information aggregation. It is difficult to apply them to inductive graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM STATEMENT</head><p>This paper targets to solve the ETA prediction problem based on historical order delivery information. In our scenario, the platform will generate an order either after consumers have paid for their products or when the consumers are viewing a product page. We formally define an order and the problem of delivery time estimation as follows.</p><p>Definition 3.1: Order. An order is represented as a tuple: x ? = (? ? , ? ? , ? ? , ? ? ), where ? ? denotes the retailer, ? ? , ? ? are the origin location and destination of the order, respectively. ? ? is the payment time of this order. Each element in an order tuple is coupled with a hand-crafted dynamic feature vector describing its statistical information. Thus, we can denote the dataset with ? historical orders by X = {x ? |? = 1, 2, ? ? ? , ? }, and the associated delivery time by y = {? ? |? = 1, 2, ? ? ? , ? }. A delivery time ? ? usually is calculated from the payment to the time when the order is signed.</p><p>Definition 3.2: Problem of Delivery Time Estimation. Given a set of historical orders X, the delivery time y and the input features Z, we aim to estimate the delivery time ? ? of a query order x ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">INDUCTIVE GRAPH TRANSFORMER</head><p>Since graph convolutional network (GCN) <ref type="bibr" target="#b19">[20]</ref> was first outlined to generalize convolutional neural networks (CNNs) on graph structured data, GCN and its variants have been successfully applied to various domains <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref>. As the order graph can be easily constructed from the historical orders. Each element in an order tuple is represented as node in the order graph, and two nodes are linked if they have occurred in the same order. Considering the computational complexity of dealing with large-scale order graphs, we further limit the links to adjacent elements of an order. Definition 4.1: Construction of Heterogeneous Graph. Specifically, given an order (? ? , ? ? , ? ? , ? ? ), the retailer node ? ? can only connect to the sender address node ? ? . The sender address node ? ? can link to both the retailer node ? ? and the receiver address ? ? , and so forth. That is, each order will generate only 6 edges instead of 12. In this way, the dense of the graph can be cut down by half. Meanwhile, the information from one node can be propagated to any other nodes via stacking more convolutional layers of GCN.</p><p>With the constructed heterogeneous graph, we can decouple the graph into a set of bipartite graphs and perform graph convolution on the bipartite graphs. The transformation operator and activation layer in GCN pose prohibitive cost for large-scale graphs. We simplify the graph convolutional layer with linear computation. However, apart from the structural information, the nodes in the graph may exhibit different temporal patterns. We adopt GRU to analyze temporal correlations on time-series axis at the node-level. GRU solves the vanishing gradient problem of a standard recurrent neural network (RNN) by using update gate and reset gate. Compared with the long short-term memory (LSTM) network, GRU is faster to train due to the fewer number of weights and parameters to update during training. The output of GRU as well as the raw input features are feed into a transformer variant for final prediction. Figure <ref type="figure" target="#fig_0">1</ref> shows the overall framework of the proposed IGT model. It consists of two main modules: 1) THEGCN (i.e., Temporal and Heterogeneous GCN) and 2) ETAformer (i.e., ETA Prediction with Transformers). Next, we introduce the details of each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">THEGCN</head><p>Suppose G = (V, E) be the order graph derived from historical orders, where V and E denote the node set and edge set respectively. Each node ? ? V and each edge ? ? E are associated with type mapping functions. Our scenario includes four types of nodes and one type of edges. Hence, the constructed graph will be a heterogeneous graph. We first introduce how GCN works on homogeneous graphs, then we show how to apply the convolutional operator to heterogeneous graphs. A typical GCN on homogeneous graphs that recursively updates the hidden embedding H ? at the ?-th layer is defined as:</p><formula xml:id="formula_0">H ? = ? ?H ?-1 W ? , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where W is the learnable transformation matrix and ? (?) is a nonlinear function, e.g., the ReLu function. ? = D-1/2 (A+I) D-1/2 is the re-normalization of the adjacency matrix A.</p><formula xml:id="formula_2">D = diag(? 1 , ? ? ? , ? |V | )</formula><p>is the diagonal degree matrix of A+I, where each entry on the diagonal is equal to the row-sum of the adjacency matrix</p><formula xml:id="formula_3">? ? = 1 + ? ? ? ? .</formula><p>Each node ? ? in the graph has a corresponding ?-dimensional latent feature vector h ? ? R ? . The initial feature matrix</p><formula xml:id="formula_4">H 0 = [h 0 , ? ? ? , h |V | ] ? stacks |V | feature vectors on top of one another.</formula><p>In order to apply the graph convolution operator on heterogeneous graphs, a couples of GCN variants are proposed in other domains <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b42">43]</ref>. However, they are both computationally expensive and prone to over-fitting. Inspired by the study of simplified homogeneous SGC <ref type="bibr" target="#b33">[34]</ref>, we design a heterogeneous GCN which towards computationally efficiency without compromising the performance. Compared with SGC <ref type="bibr" target="#b33">[34]</ref>, we further remove its transformation matrix to reduce the parameter search space. This allows our model can be trained with large-scale datasets. Given the heterogeneous graph G, we first construct a set of bipartite subgraphs based on the combinations of the node types. Suppose the number of node types is ?, the number of bipartite subgraphs will not exceed ? 2 =</p><p>(?)! 2!(?-2)! . In detail, we construct a sub-graph G ?,? from the original graph G based on the two node types ? and ?. In G ?,? , the type of a node belongs to either ? or ?, and any two nodes in subgraph G ?,? will be linked if they are connected in the original graph G. As G ?,? is undirected, we have G ?,? = G ?,? . We denote the adjacency matrix of G ?,? as A ?,? :</p><formula xml:id="formula_5">A ?,? = 0 R R T 0 ,<label>(2)</label></formula><p>and each entry</p><formula xml:id="formula_6">? ?? ? R is 1 if node ? ? G ?,? is connected with node ? ? G ?,? in G;</formula><p>Otherwise, ? ?? is set to 0. Following <ref type="bibr" target="#b19">[20]</ref>, we also perform the re-normalization trick on A ?,? , resulting as ??,? .</p><p>To perform convolution operation on bipartite graphs, we update the vanilla GCN in Eq. ( <ref type="formula" target="#formula_0">1</ref>) as:</p><formula xml:id="formula_7">H ? ?,? = ??,? H ?-1 ?,? ,<label>(3)</label></formula><p>where H ? ?,? denotes the embeddings of nodes in ? ?,? at the ?-th GCN layer. Suppose the number of nodes with type ? is ? ? , then H ? ? R ? ? ?? is the feature matrix of node type ? by stacking this type of nodes on top of one another. Analogously, H ? ?,? ? R (? ? +? ? )?? . Note that Eq. ( <ref type="formula" target="#formula_7">3</ref>) simplifies the vanilla GCN by removing both the collapsing weight matrix and the non-linear layer, which further simplifies SGC <ref type="bibr" target="#b33">[34]</ref> with the removal of the non-linear activation. Meanwhile, the reduction of trainable parameters prevents the model from over-fitting. The linear information propagation enables the model to be computationally efficient. We name this simplified graph convolution for bipartite graphs as Bipartite SGC in Figure <ref type="figure" target="#fig_0">1</ref>. As one type of nodes may be involved in multiple bipartite graphs and participate the information propagation simultaneously, we aggregate the propagated information for each type of nodes within each layer as follows,</p><formula xml:id="formula_8">H ? ? = AGGREGATE({H ? ?,? [: ? ? ] ? = 1, ..., ?, and G ?,? ? ?}),<label>(4)</label></formula><p>where AGGREGATE is an aggregation function, and ? is the number of node types. Here, we use the sum aggregation. Other aggregation operators <ref type="bibr" target="#b3">[4]</ref> can also be applied for aggregating the embeddings of nodes belonging to the same type. In Eq. ( <ref type="formula" target="#formula_8">4</ref>), we use the slice operator to extract the embeddings of ?-type nodes.</p><p>One benefit of such information aggregation within each layer is that the information from different types of nodes can be completely integrated with each other. The integrated information is then fed into the next layer for propagation. Another benefit is that THEGCN updates the embeddings of all nodes instead of a batch of nodes in each iteration to avoid the memory staleness problem <ref type="bibr" target="#b17">[18]</ref>.</p><p>With the information obtained in each layer, we update the nodes' final embeddings at time ? -1 by aggregating all intermediate embeddings. That is:</p><formula xml:id="formula_9">H ? -1 ? = AGGREGATE(H 0 ? , H 1 ? , ..., H ? ? ),<label>(5)</label></formula><p>where ? is number of propagation layers. The aggregation function AGGREGATE can be any functions discussed in <ref type="bibr" target="#b3">[4]</ref>. Here, we use the mean aggregation for final node updating.</p><p>To further fuse the temporal information into the learnt embeddings H ? -1 ? at time ? -1, we use the gating mechanisms to control and manage the flow of information.</p><formula xml:id="formula_10">H ? ? = UPDATE(H ? -1 ? , Z ? ? ),<label>(6)</label></formula><p>where Z ? ? is the raw input feature vectors of ?-type elements in orders at time ?. Here, UPDATE is a learnable update function, e.g., a recurrent neural network such as LSTM or GRU. In this work, we adopt the GRU network because it owns fewer number of gates and is as effective as LSTM.</p><p>Note that we use a normalized adjacency matrix constructed from historical orders to avoid data leakage in model training stage. However, in inference stage, we use the normalized adjacency matrix built upon both training and test data <ref type="bibr" target="#b13">[14]</ref>. Under inductive setting, our proposed THEGCN can fuse information from its neighbors within the same order x ? to learn the embeddings of unseen nodes. In extremely case, all nodes in x ? are never observed in training, which means that no neighbor information can be used. THEGCN adopts GRU to update the node's embeddings from raw input features. Hence, raw features will be used to estimate the delivery time of this order.</p><p>Algorithm 1 Pseudo-code for Inductive Graph Transformer (IGT) 1: Inputs: Set of orders X, the associated delivery time y and input feature vectors Z. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ETAformer</head><p>The standard transformer is designed for word prediction, and a sentence is treated as a fully connected graph. An order in our scenario is analogy to a sentence in natural language processing (NLP) with less than tens or hundreds of elements. As such, the ETA prediction is substantially amenable to transformers. Large transformer models can be trained on such fully connected graphs. Furthermore, each word attends to each other word in a sentence in the transformer architecture, which enables the high-order interactions among words. In NLP, the prediction of a word in a sentence can vary with context, other words etc. Equivalently, in our scenario, the ETA prediction relies not only on the raw features from retailers, addresses, payment time, but also the high-order interactions of these features. However, the standard transformer assume words are encoded within the same embedding space. In practice, the nodes in heterogeneous graph may vary in feature size as they belonging to different types. Thus, to encapsulate the input feature vectors into transformer, we first need to reshape the feature space. That is:</p><formula xml:id="formula_11">e ? = ? (z ? , h ? ? ),<label>(7)</label></formula><p>where z ? is the raw feature of node ?, h ? ? is the output embedding of node ? from THEGCN, and ? is a mapping function to align nodes with the same dimensionality. An intuitive function is using multilayer perceptron (MLP) to map features of different dimensional spaces to the same space. However, it introduces additional parameters to the model, so we adopt zero-padding on each node of an order. Finally, we reshape the order into x ? ? R ??? , where ? is the number of elements in an order and ? is the padded embedding size of each element.</p><p>For capturing the order-level information, we prepend a learnable header token similar to that of BERT <ref type="bibr" target="#b6">[7]</ref> and ViT <ref type="bibr" target="#b8">[9]</ref>. In implementation, we use a MLP with one hidden layer for token learning. On positional encodings, existing graph transformers design various methods to preserve the distance information between nodes. Our decoupled architecture utilizes the standard positional encoding of transformer. Because an order x ? in our scenario can be naturally framed as a sequence of elements. The header token e ?????? ? , positional encodings e ??? ? , and mapped node embeddings e ? of x ? are then fed into the transformer variants. Upon the output, we finally use a layer normalization (LN) and a single layer MLP to predict the ETA, as shown in Figure <ref type="figure" target="#fig_3">2</ref>.</p><p>Formally, we denote the raw feature of a query order x ? as z ? = (? 1 ? , ? 2 ? , ..., ? ? ? ), where ? ? ? is the raw feature of ?-th element in the order. The ETA prediction of order x ? based on the transformerlike models can be presented as follows,  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Training</head><p>For training the proposed IGT model, we utilize the Adam <ref type="bibr" target="#b18">[19]</ref> as the optimizer, and use the Mean Absolute Error (MAE) as our loss function. Considering a set of ? samples y = (? 1 , ? 2 , ? ? ? , ? ? )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>Positional encoding and zero-padding  representing the ground truth and the associated predictions ?, the MAE-based loss function is defined as follows:</p><formula xml:id="formula_12">L = MAE(y, ?) = 1 ? ? ?? ?=1 |? ? -?? |.<label>(9)</label></formula><p>The details of the optimization algorithm used to learn the IGT model are summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS 5.1 Experimental Datasets</head><p>We utilize two large-scale logistics datasets collected from one of the world's largest e-commerce platforms to evaluate the performance of IGT as well as the baselines. To facilitate our interpretation of the prediction results on different datasets, we plot the histogram of the diurnal pattern on order payment time in Figure <ref type="figure" target="#fig_5">3</ref>. From the plots, we can observe the payment time pattern is consistent with human regularity. The two datasets also vary in order volume, that is, the number of orders received in Hangzhou is almost an order of magnitude of Weihai at each payment time.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># of orders</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison Methods and Metrics</head><p>We compare the proposed IGT model with the following baselines. 1). Linear Regression (LR) models the delivery time of orders by training a linear regression model based on the raw features of orders. We use the LR in scikit-learn <ref type="bibr" target="#b24">[25]</ref> for evaluation. 2). XG-Boost <ref type="bibr" target="#b0">[1]</ref> is a gradient boosting decision tree based regression model. The input is the same as the LR model. We use the XG-BRegressor in xgboost<ref type="foot" target="#foot_0">1</ref> with MAE as its evaluation metric. 3).</p><p>MURAT <ref type="bibr" target="#b21">[22]</ref> captures underlying road network structures as well as spatio-temporal prior knowledge for OD travel time estimation. The road information is not available in our dataset, the OD graph is built upon the sender and receiver address. We use both ResNet18 and deep feed forward neural networks (DNN) to map the learned representations to target tasks. In our scenario, we found DNN performs better than ResNet18. The reported results in our paper are based on DNN. 4). xDeepFM <ref type="bibr" target="#b22">[23]</ref> learns both low-and highorder feature interactions in both explicit and implicit fashions in an extreme deep factorization machine framework. We transform the recommendation task into a regression task for ETA prediction. 5). TEMPrel <ref type="bibr" target="#b30">[31]</ref> is an OD-based approach for travel time estimation. It averages the scaled travel times of all neighboring routes with similar origin and destination for ETA prediction. 6). Deep-ETA <ref type="bibr" target="#b34">[35]</ref> is an end-to-end network for last-mile package delivery estimation. It not only takes route features into consideration but also encodes the spatial and temporal information in deep neural networks for ETA prediction. As no route information is available in our data, we use the sender and receiver addresses as the route history. 7). HetETA <ref type="bibr" target="#b14">[15]</ref> targets to estimate the arrival time of taxis within a city. It uses gated convolutional neural networks and ChebNet <ref type="bibr" target="#b5">[6]</ref> to capture the relations between through spatial and temporal perspectives. This method relies on the trajectory information. Again, as our data lacks this type of information, we perform Het-ChebNets only on the road network. The road network is built up as the same as MURAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance Comparison</head><p>All models are trained on a single GeForce RTX 2080 Ti with 11 GB memory. For traditional machine learning models, we feed full training data into LR, XGBoost, and TEMPrel for ETA prediction. Other models are trained in batches with a size of 8192. We monitor the loss over the validation set and apply early stopping if no improvement happens after 100 consecutive epochs. Considering the model convergence, the total epochs for training are fixed at 1000. We initialize the latent embeddings H 0 of nodes in G with the Xavier method <ref type="bibr" target="#b12">[13]</ref>, tune the number of layers ? of THEGCN in {1, 2, 3, 4, 5}, and the embedding size ? of nodes in {16, 32, 64, 128, 256}. We implement our model in PyTorch <ref type="bibr" target="#b23">[24]</ref>. Source code of IGT will be released upon acceptance. Following <ref type="bibr" target="#b21">[22]</ref>, we evaluate our model and all baselines under the metrics of MAE, Mean Absolute Percentage Error (MAPE), and Mean Absolute Relative Error (MARE). The definitions of MAPE and MARE are as follows, MAPE(y, ?) = 1</p><formula xml:id="formula_13">? ? ?? ?=1 ? ? -?? ? ? ,<label>(10)</label></formula><formula xml:id="formula_14">MARE(y, ?) = ? ?=1 |? ? -?? | ? ?=1 ? ? .<label>(11)</label></formula><p>Analogous to MAE defined in Eq. ( <ref type="formula" target="#formula_12">9</ref>), y denotes the ground truth of package delivery time, ? denotes the predicted delivery time, and ? denotes the number of samples in test data. Table <ref type="table">2</ref> presents the performance comparison between the proposed IGT model and the baselines on two datasets. The results on three metrics show our proposed model is capable of improving the best baseline by more than 6%. Especially on dataset D1, IGT gains significant improvement over the best baseline by up to 11%. Compared with D2 in Table <ref type="table" target="#tab_1">1</ref>, D1 is more sparse. The averaged number of orders for each retailer is 0.7 and 4.4 per day in D1 and D2, respectively. The results between D1 and D2 indicate that IGT performs better with sparse graph. However, sparsity may partially contribute to the performance. We analyze why the performance of the two datasets vary heavily through the perspective of information theory.</p><p>We utilize entropy to quantify the entropy of the distribution of delivery time under a payment time.</p><formula xml:id="formula_15">? ? = - ?? ? ? ? ?S ? ? (? ? ? )log? (? ? ? ),<label>(12)</label></formula><p>Table <ref type="table">2</ref>: Performance comparison on ETA prediction between our model and the baselines. We mark the best results on each dataset under each metric in Bold face, and the second best underlined. Improvement (%) is calculated between the best result achieved from the baselines and the result of our model. where S ? is the set of delivery time for orders placed at ?. The entropy is larger for orders with greater divergent delivery time that placed at the same time ?. As such, we plot the entropy of the two datasets with regard to the payment time in Figure <ref type="figure" target="#fig_6">4</ref>. The figure reveals dataset D1 has a larger averaged entropy value than that of dataset D2. As a result, the MAE value for D1 is much larger than D2 for all models. We will re-examine the variation of entropy in Section 5.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>As for the baselines, the performance of LR is poor mainly because it can only use the raw features for regression tasks. Other regression models such as XGBoost and xDeepFM can capture the interactions between features, thus achieving better performance. TEMPrel improves LR on both datasets implying the importance of spatial (i.e., longitude, latitude) and temporal information in ETA prediction. The performance of MURAT, DeepETA and HetETA is compromising as they are designed to take advantage of route information for ETA prediction. However, MURAT and HetETA utilize graph representation learning to learn the meaningful embeddings of OD links and achieve better performance over DeepETA. Similarly, our model also uses graph learning (i.e., GCN) to capture the structural information between orders. Nevertheless, we focus on node representation learning, and a link that connects retailer, addresses and payment time is represented by concatenating the embeddings of these nodes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>We perform ablative study on each component of IGT to examine its contribution in ETA prediction. Recalling that IGT consists of two components, namely THEGCN and ETAformer. The decoupled architecture of IGT enables us to perform ETA prediction on each component without the prerequisite of the other one. First, to evaluate THEGCN, we feed the concatenation of the latent embeddings As for ETAformer, it can cope with the raw features directly. We report the performance on the two datasets under each metric in Table <ref type="table" target="#tab_4">3</ref>.</p><p>The results show that ETAformer achieve better performance than THEGCN in ETA prediction. Because ETAformer acts on interorder, it further explores the high-order interactions between elements of an order. Comparing the performance of THEGCN with that of xDeepFM in Table <ref type="table">2</ref>, we can observe that merely using of THEGCN in IGT can obtain competitive performance to xDeepFM. Because THEGCN is also able to extract the high-order information between orders by virtue of linear information propagation. The training time in Table <ref type="table">2</ref> demonstrates THEGCN is significantly faster than ETAformer. The combination of THEGCN and ETAformer gains significant improvements over the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Hyper-parameter Study</head><p>We examine how the number of layers ? and the embedding size ? of nodes in THEGCN affect the performance of IGT. We collect the MAE performance of IGT under the grid-search of hyperparameters ? and ? on both datasets (D1 and D2) and plot the heatmap in Figure <ref type="figure">5</ref>. We observe that the dimensionality ? of the hidden embeddings in THEGCN has a more decisive impact on the ETA prediction. Stacking too many layers ? in THEGCN may result in over-smoothing <ref type="bibr" target="#b20">[21]</ref> and degenerate the prediction performance. A good choice for the dimensionality of hidden embeddings may depend on the number of orders in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Inductive and Transductive Learning of IGT</head><p>We further analyze the prediction results with regard to the types of nodes and the number of orders in each node type. Especially, we examine the prediction performance of IGT under inductive   setting where nodes have no historical orders. In our analysis, we compare IGT with xDeepFM to elucidate the use cases where IGT provide a significant edge. Payment time. We plot the ETA prediction performance of IGT and xDeepFM with regard to payment time in a day in Figure <ref type="figure">6</ref>, from which we can observe that our proposed model always shows better performance against the best baseline method xDeepFM at every payment time. Obviously, the performance of both models exhibits certain fluctuations over payment time. When comparing Figure <ref type="figure">6</ref> with Figure <ref type="figure" target="#fig_5">3</ref> and Figure <ref type="figure" target="#fig_6">4</ref>, we find the entropy of the delivery time on orders is highly correlated with the prediction results. More orders for training may not improve the ETA prediction performance of the models. It is rational because larger entropy makes the models unable to estimate the delivery time accurately. Both models show large prediction error around 15:00 on the two datasets. The reason is that courier companies usually dispatch the collected packages to the next station around 15:00, products ordered after this time need to wait for the next dispatching slot. In both datasets, our proposed IGT shows marginal improvements over xDeepFM around the peaks.</p><p>Retailer and address. As both datasets have orders with unseen retailers and addresses, we can evaluate the performance of IGT under inductive settings. We take data binning to reduce the cardinality of the number of orders with regard to retailer and address. To be specific, we divide the number of orders ? received by one retailer into groups: unseen (? = 0), small (0 &lt; ? ? 100), medium (100 &lt; ? ? 500) and large (500 &lt; ? ). Similarly, we divide the number of orders ? associated with sender addresses into groups: unseen (? = 0), small (0 &lt; ? ? 500), medium (500 &lt; ? ? 1000) and large (1000 &lt; ? ). We plot the prediction performance on groups with a size larger than 50 in Figure <ref type="figure" target="#fig_9">7</ref>. In inductive learning setting, IGT shows better prediction performance than xDeepFM on both datasets under retailer and address. Note that IGT reduces the MAE value by half of xDeepFM on unseen addresses of dataset D1.</p><p>Although the number of orders on unseen addresses is relatively small as the prediction values show higher variance, we can see IGT is superior to xDeepFM in inductive learning from the left part of Figure <ref type="figure" target="#fig_9">7</ref>. The right part of Figure <ref type="figure" target="#fig_9">7</ref> shows that both models do not always learn well with the accumulation of historical orders from the same retailer or address. Especially on dataset D1, larger number of orders from retailers degrades the performance of both models. Compared with the retailer perspective, the accumulation of orders on address shows continuous improvements from both models on the two datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ONLINE EXPERIMENT</head><p>To further evaluate the effectiveness of IGT, we evaluate its performance online. We evaluate the performance of IGT on the ecommerce platform for one week. We split out the entire user requests by 1/10 to test our model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND FUTURE WORK</head><p>In this paper, we propose an inductive graph transformer (IGT) to estimate the package delivery time. We use a decoupled architecture to capture both the high-order interactions of orders via a temporal and heterogeneous GCN and the interactions of elements in an order through a transformer-based model. The information of unseen nodes can be learnt by propagating messages from neighboring nodes in our model. Hence, both structural information and raw features can be utilized in the transformer-based model for accurate ETA prediction. In our model, IGT, we simplify GCN by removing both its non-linear layer and the transformation matrix. This allows industrial scenarios to apply IGT on large-scale datasets. Experimental results on two large-scale offline logistics datasets demonstrate the effectiveness of IGT in both inductive and transductive learning. We further evaluate the performance of the model in a national level for online test. IGT is promising in enhancing the user experiences of billions of customers in deployment on e-commerce platforms. For future work, we will focus on providing confidence intervals on ETA prediction instead of a point estimation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The overall framework of the proposed IGT model. There are two main modules in IGT: 1) THEGCN (i.e., Temporal and Heterogeneous GCN) and 2) ETAformer (i.e., ETA Prediction with Transformer).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>e</head><label></label><figDesc>? = ? (z ? , h ? ? ), ??????? ? = Transformer(e ?????? ? , e ??? ? , e ? ), ?? = MLP LN(? ?????? ? ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The architecture of ETAformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Diurnal pattern of orders. ?-axis is in log scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Entropy of the delivery time for orders placed at payment time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Hyper-parameter study of IGT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Inductive and transductive learning of IGT with regard to nodes with different number of historical orders.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>: Initialize latent embeddings H 0 of nodes in G with Xavier. 4: Extract bipartite subgraphs with node type ? and ? G ?,? from G and the latent embeddings H 0 ?,? from H 0 .</figDesc><table><row><cell>9:</cell><cell></cell><cell>H ? ?,? = ??,? H ?-1 ?,?</cell><cell>? bipartite SGC</cell></row><row><cell>10:</cell><cell cols="2">end for</cell><cell></cell></row><row><cell>11:</cell><cell cols="2">end for</cell><cell></cell></row><row><cell>12:</cell><cell cols="2">for ? in [1, ..., ?] do</cell><cell></cell></row><row><cell>13:</cell><cell cols="2">H ? ? = AGGREGATE({H ? ?,? [: ? ? ]})</cell><cell>? Eq. 4</cell></row><row><cell>14:</cell><cell cols="2">end for</cell><cell></cell></row><row><cell>15:</cell><cell>end for</cell><cell cols="2">? end layer propagation loop</cell></row><row><cell>16:</cell><cell cols="2">for ? in [1, ..., ?] do</cell><cell></cell></row><row><cell>17:</cell><cell>H ? -1 ?</cell><cell>= AGGREGATE(H 0 ? , H 1 ? , ..., H ? ? )</cell><cell>? Eq. 5</cell></row><row><cell>18:</cell><cell cols="3">H ? ? = UPDATE(H ? -1 ? , Z ? ? ) ? Eq. 6 for temporal update</cell></row><row><cell>19:</cell><cell>end for</cell><cell></cell><cell></cell></row><row><cell>20:</cell><cell cols="2">?? = ETAformer(H ? , Z ? )</cell><cell>? ETA prediction</cell></row><row><cell>21:</cell><cell cols="2">L = MAE(y ? , ?? )</cell><cell>? loss</cell></row><row><cell>22:</cell><cell cols="2">L.backward()</cell><cell>? back-propagate</cell></row><row><cell cols="2">23: end for</cell><cell></cell><cell></cell></row></table><note><p><p>2: Construct heterogeneous graph G from X. 35: for X ? ?? X do ? load a batch 6: for ? in [1, ..., ?] do ? layer-wise propagation 7: for ? in [1, ..., ?] do 8:</p>for ? in [1, ..., ?] do</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of experimental datasets.</figDesc><table><row><cell>Datasets</cell><cell>D1</cell><cell>D2</cell></row><row><cell># of orders</cell><cell cols="2">375,393 1,817,766</cell></row><row><cell># of retailers</cell><cell>4,729</cell><cell>8,741</cell></row><row><cell># of days</cell><cell>110</cell><cell>47</cell></row><row><cell>validation/test days</cell><cell>(10, 15)</cell><cell>(5, 10)</cell></row><row><cell># of orders in test</cell><cell>61,647</cell><cell>348,037</cell></row><row><cell># of orders with unseen retailers</cell><cell>913</cell><cell>930</cell></row><row><cell># of orders with unseen addresses</cell><cell>61</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Both datasets are orders targeting a fixed receiving city. Specifically, D1 is collected with all the orders sent to Weihai, China. D2 is for packages oriented to Hangzhou, China. The population of the receiving city of Hangzhou is 3 times more than it is in Weihai. From Table1, we can also observe that the retailers in D2 are relatively more active than those in D1 with an average of 208 orders. For ETA prediction, we follow the chronological data splitting for training, validation, and test. To be specific, we split the orders in the last 25 days of D1 by 10:15 as validation and test, the last 15 days of D2 by 5:10, orders from other days are used for training. In Table1, we denote the unseen nodes (e.g., retailers, addresses) as the nodes in test that never occurred in training. Note that the datasets also collect the evolving features of retailer, payment time, sender address, and receiver address.</figDesc><table><row><cell>5.1.1 Data Description and Statistics. The first dataset contains</cell></row><row><cell>0.38 million samples from 4,729 retailers (denoted by D1), ranging</cell></row></table><note><p>from January 3rd to April 22nd, 2021. The second dataset includes 1.8 million samples collected from 8,741 retailer (denoted by D2), ranging from June 3rd to July 22nd, 2021. The two datasets are not only differentiate in graph size, collected time, but also in collection regions.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of IGT.</figDesc><table><row><cell cols="2">Datasets Metric</cell><cell cols="3">THEGCN ETAformer IGT</cell></row><row><cell></cell><cell>MAE(hours)</cell><cell>11.79</cell><cell>10.67</cell><cell>10.53</cell></row><row><cell>D1</cell><cell>MAPE(%) MARE(%)</cell><cell>15.67 17.09</cell><cell>13.22 15.48</cell><cell>13.35 15.28</cell></row><row><cell></cell><cell>time/epoch(seconds)</cell><cell>2.66</cell><cell>10.47</cell><cell>13.34</cell></row><row><cell></cell><cell>MAE(hours)</cell><cell>7.99</cell><cell>7.66</cell><cell>7.39</cell></row><row><cell>D2</cell><cell>MAPE(%) MARE(%)</cell><cell>16.62 17.03</cell><cell>15.43 16.34</cell><cell>14.22 15.75</cell></row><row><cell></cell><cell>time/epoch(seconds)</cell><cell>11.31</cell><cell>42.86</cell><cell>63.06</cell></row><row><cell cols="5">obtained from THEGCN into a single layer MLP for ETA prediction.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>L</head><label></label><figDesc>10.68 10.77 10.92 10.96 11.07 10.53 10.94 10.84 10.98 11.03 10.62 10.86 10.95 10.91 10.88 10.59 10.83 10.90 10.94 10.78 10.61 10.77 10.82 10.92 10.96</figDesc><table><row><cell></cell><cell>1</cell><cell>11.0</cell><cell>1</cell><cell>8.71 7.71 7.88 7.51 7.74</cell></row><row><cell># of layers</cell><cell>2 3 4</cell><cell cols="2">10.8 L 2 3 4 # of layers</cell><cell>9.03 8.01 8.14 7.50 7.45 9.01 7.78 8.99 7.39 7.56 9.21 8.33 8.91 7.54 7.80</cell></row><row><cell></cell><cell>5</cell><cell>10.6</cell><cell>5</cell><cell>9.22 8.53 8.85 7.71 9.54</cell></row><row><cell></cell><cell>16 32 64 128 256 Embedding size D (D1)</cell><cell></cell><cell></cell><cell>16 32 64 128 256 Embedding size D (D2)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>The MAE, MAPE and MARE are 9.86 hours, 14.36% and 15.83%, respectively. Compared with Table2, the national level evaluation performance is between that of D1 and D2. The results indicate cities with a larger population usually have lower MAE on ETA prediction. Compared with the original served model, our model can reduce the average predictive error (i.e., MAE) by 15%.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://xgboost.readthedocs.io</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This research is supported, in part, by <rs type="funder">Alibaba Group through Alibaba Innovative Research (AIR) Program and Alibaba-NTU Singapore Joint Research Institute (JRI)</rs>(No. <rs type="grantNumber">AN-GC-2020-019</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_fQZ4aWB">
					<idno type="grant-number">AN-GC-2020-019</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd acm sigkdd international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with shift graph convolutional network</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Neural Information Processing Systems</title>
		<meeting>the 34th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sooner or later? Promising delivery speed in online retail</title>
		<author>
			<persName><forename type="first">Ruomeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhikun</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Golden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Promising Delivery Speed in Online Retail</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter</title>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Estimation of traffic density from drone-based delivery in very low level urban airspace</title>
		<author>
			<persName><forename type="first">Malik</forename><surname>Doole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joost</forename><surname>Ellerbroek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacco</forename><surname>Hoekstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Air Transport Management</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page">101862</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A generalization of transformer networks to graphs</title>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dwivedi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09699</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Constgat: Contextual spatial-temporal graph attention network for travel time estimation at baidu maps</title>
		<author>
			<persName><forename type="first">Xiaomin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jizhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingke</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haijin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2697" to="2705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Compacteta: A fast inference system for travel time prediction</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3337" to="3345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">HetETA: Heterogeneous Information Network Embedding for Estimating Time of Arrival</title>
		<author>
			<persName><forename type="first">Huiting</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yucheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kung</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. Association for Computing Machinery</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. Association for Computing Machinery<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2444" to="2454" />
		</imprint>
	</monogr>
	<note>Xiaohu Qie, and Jieping Ye</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stochastic origin-destination matrix forecasting using dual-stage graph convolutional, recurrent neural networks</title>
		<author>
			<persName><forename type="first">Jilin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenjuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><forename type="middle">S</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 36th International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1417" to="1428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Heterogeneous graph transformer</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
		<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="2704" to="2710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Representation Learning for Dynamic Graphs: A Survey</title>
		<author>
			<persName><forename type="first">Rishab</forename><surname>Seyed Mehran Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kshitij</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Kobyzev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><surname>Poupart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="73" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<title level="m">Adam: A method for stochastic optimization. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-Task Representation Learning for Travel Time Estimation</title>
		<author>
			<persName><forename type="first">Yaguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyrus</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1695" to="1704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">xdeepfm: Combining explicit and implicit feature interactions for recommender systems</title>
		<author>
			<persName><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongxia</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1754" to="1763" />
		</imprint>
	</monogr>
	<note>Xing Xie, and Guangzhong Sun</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ga?l</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hetegcn: heterogeneous graph convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Ragesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sundararajan</forename><surname>Sellamanickam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakrishna</forename><surname>Bairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Lingam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM International Conference on Web Search and Data Mining</title>
		<meeting>the 14th ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="860" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Characterizing the environmental impact of packaging materials for express delivery via life cycle assessment</title>
		<author>
			<persName><forename type="first">Yuehuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huabo</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zinuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanghan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjie</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cleaner Production</title>
		<imprint>
			<biblScope unit="volume">274</biblScope>
			<biblScope unit="page">122961</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Learning Representations</title>
		<meeting>the Sixth International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Electronic commerce: structures and issues</title>
		<author>
			<persName><forename type="first">Zwass</forename><surname>Vladimir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of electronic commerce</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3" to="23" />
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Simple Baseline for Travel Time Estimation Using Large-Scale Trip Data</title>
		<author>
			<persName><forename type="first">Hongjian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianfeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Hsuan</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fdgars: Fraudster detection via graph convolutional networks in online app review system</title>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Xion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of The 2019 World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="310" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to estimate the travel time</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="858" to="866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deepeta: A spatial-temporal sequential neural network model for estimating time of arrival in package delivery system</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lixia</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="774" to="781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7370" to="7377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Graph transformer networks</title>
		<author>
			<persName><forename type="first">Seongjun</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minbyul</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raehyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunwoo J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="11983" to="11993" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A data-driven graph generative model for temporal interaction networks</title>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lecheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="401" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Layer-refined Graph Convolutional Networks for Recommendation</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.11088</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03019</idno>
		<title level="m">SelfCF: A Simple Framework for Self-supervised Collaborative Filtering</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feijun</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.05969</idno>
		<title level="m">Bootstrap Latent Representations for Multimodal Recommendation</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Order Fulfillment Cycle Time Estimation for On-Demand Food Delivery</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kairong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxing</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2571" to="2580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hgcn: A heterogeneous graph convolutional network-based deep learning model toward collective classification</title>
		<author>
			<persName><forename type="first">Zhihua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinxin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokai</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingping</forename><surname>Bi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1161" to="1171" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
