<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video Object Annotation, Navigation, and Composition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
							<email>dgoldman@adobe.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Adobe Systems</orgName>
								<address>
									<addrLine>Inc. 801 N. 34th Street</addrLine>
									<postCode>98103</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chris</forename><surname>Gonterman</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98105-4615</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Brian</forename><surname>Curless</surname></persName>
							<email>curless@cs.washington.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98105-4615</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Salesin</surname></persName>
							<email>salesin@adobe.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Adobe Systems</orgName>
								<address>
									<addrLine>Inc. 801 N. 34th Street</addrLine>
									<postCode>98103</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98105-4615</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
							<email>seitz@cs.washington.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98105-4615</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Video Object Annotation, Navigation, and Composition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B1BEA0A94326D63C2B50AD38FCC2C8E8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H5.2 [Information interfaces and presentation]: User Interfaces. -Interaction styles Design</term>
					<term>Human Factors</term>
					<term>Algorithms Video annotation</term>
					<term>video navigation</term>
					<term>video interaction</term>
					<term>direct manipulation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We explore the use of tracked 2D object motion to enable novel approaches to interacting with video. These include moving annotations, video navigation by direct manipulation of objects, and creating an image composite from multiple video frames. Features in the video are automatically tracked and grouped in an off-line preprocess that enables later interactive manipulation. Examples of annotations include speech and thought balloons, video graffiti, path arrows, video hyperlinks, and schematic storyboards. We also demonstrate a direct-manipulation interface for random frame access using spatial constraints, and a drag-and-drop interface for assembling still images from videos. Taken together, our tools can be employed in a variety of applications including film and video editing, visual tagging, and authoring rich media such as hyperlinked video.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Present video interfaces demand that users think in terms of frames and timelines. Although frames and timelines are useful notions for many editing operations, they are less well suited for other types of interactions with video. In many cases, users are likely to be more interested in the higher level components of a video such as motion, action, character, and story. For example, consider the problems of attaching a moving annotation to an object in a video, or finding a moment in a video in which an object is in a particular place, or of composing a still from multiple moments in a video. Each of these tasks revolves around objects in the video and their motion. But although it is possible to compute object boundaries and to track object motion, present interfaces do not utilize this information for interaction.</p><p>In this paper we propose a framework using (2D) video object motion to enable novel approaches to user interaction.</p><p>Although the concept of object motion is not as high-level as character and story, it is a mid-level aspect of video that we believe is a crucial building block toward those higher-level concepts. We apply computer vision techniques to understand how various points move about the scene and segment this motion into independently moving objects. This information is used to simplify the interaction required to achieve a variety of video manipulation tasks.</p><p>In particular, we propose novel interfaces for three tasks that are under-served by present-day video interfaces: annotation, navigation, and image composition.</p><p>Video annotation. Video annotation is the task of associating graphical objects with moving objects on the screen. In existing interactive applications, only still images can be annotated, as in the "telestrator" system used in American football broadcasting (see Figure <ref type="figure" target="#fig_0">1</ref>). Using our system, however, such annotations can easily be attached to moving objects in the scene by novices with minimal user effort. Our system supports descriptive labels, illustrative sketches, thought and word bubbles communicating speech or intent, path arrows indicating motion, and hyperlinked regions (Figures <ref type="figure" target="#fig_2">3</ref><ref type="figure">4</ref><ref type="figure">5</ref><ref type="figure" target="#fig_5">6</ref><ref type="figure" target="#fig_6">7</ref>). Given the pre-computed object motion, the system can also determine when objects are occluded or change appearance significantly, and modify the appearance of the annotations accordingly (Figure <ref type="figure" target="#fig_7">8</ref>). We envision video object annotations being used in any field in which video is produced or used to communicate information.</p><p>Video navigation. The use of motion analysis also permits novel approaches to navigating video through direct manipulation. Typical approaches to navigating video utilize a linear scan metaphor, such as a slider, timeline, or fast-forward and rewind controls. However, using pre-computed object motion, the trajectories of objects in the scene can be employed as constraints for direct-manipulation navigation: In our system, clicking and dragging on objects in a video frame causes the video to immediately advance or rewind to a frame in which the object is located close to the ending mouse position. Contemporaneous work <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref> shows that this approach streamlines video tasks requiring selection of individual frames (for example, locating a "cut frame" on which to begin or end a shot). Our system also enables a novel mode of re-animation of video sequences using the mouse as input: We can direct the motion of an object such as a face by dragging it around the screen. This interface can be applied to "puppeteer" existing video, or to retime the playback of a video.</p><p>Video-to-still composition. Finally, we discuss the task of rearranging portions of a video to create a new still image. Agarwala et al. <ref type="bibr" target="#b0">[1]</ref> have previously explored the problem of combining a set of stills into a single, seamless composite. However, when the source material is video, the task of identifying the appropriate moments becomes a problem in and of itself. Many consumer cameras now support a "burst mode" in which a high-speed series of high-resolution photos is taken. Finding Cartier-Bresson's "decisive moment" in such an image stream, or composing a photomontage from multiple moments, is a significant challenge. Rav-Acha et al. <ref type="bibr" target="#b18">[19]</ref> have proposed editing videos using evolution of time-fronts, but have not presented a user interface for doing so. We demonstrate an interactive interface complementary to such algorithms, for composition of novel still images using a drag-and-drop metaphor to manipulate people or other objects in a video.</p><p>To achieve these interactions, our system first analyzes the video in a fully automatic preprocessing step that tracks the motion of image points across the video and segments those tracks into coherently moving groups. Although reliably extracting objects in video and tracking them over many frames is a hard problem in computer vision, the manipulations we support do not require perfect object segmentation and tracking, and can instead exploit low-level motion tracking and mid-level grouping information. Furthermore, aggregating point motion into coherent groups has a number of benefits that are critical for interaction: We can select a moving region using a single click, estimate object motion more robustly, and, to a limited extent, handle changes of object appearance, including occlusions. Our contributions include an automated preprocess for video interaction that greatly enriches the space of interactions possible with video, an intuitive interface for creating graphical annotations that transform along with associated video objects, a fluid interaction technique for scrubbing through video with single or multiple constraints, and a novel drag-and-drop approach to composing video input into new images by combining regions from multiple frames. The telestrator <ref type="bibr" target="#b29">[30]</ref>, popularly known as a "John Madden-style whiteboard," was invented by physicist Leonard Reiffel for drawing annotations on a TV screen using a light pen. This approach has also been adopted for individual sports instruction using systems like ASTAR <ref type="bibr" target="#b2">[3]</ref> that aid coaches in reviewing videos of athletic performance. However, as previously mentioned, annotations created using a telestrator are typically static, and do not overlay well on moving footage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related work</head><p>In recent years, broadcasts of many professional sporting events have utilized systems supplied by Sportvision <ref type="bibr" target="#b27">[28]</ref> to overlay graphical information on the field of play even while the camera is moving. Sportvision uses a variety of technologies to accomplish these overlays, including surveying, calibration, and instrumentation of the field of play or racing environment. Although this instrumentation and calibration allow graphics to be overlaid in real time during the broadcast, it requires expensive specialized systems for each different class of sporting event, and is not applicable to preexisting video acquired under unknown conditions.</p><p>Tracking has previously been used for video manipulation and authoring animations. For example, Agarwala et al. <ref type="bibr" target="#b1">[2]</ref> demonstrated that an interactive keyframe-based contour tracking system could be used for video manipulation and stroke animation authoring. However, their system required considerable user intervention to perform tracking. In contrast, our application does not require pixel-accurate tracking or object segmentation, so we can use more fully-automated techniques that do not produce pixel segmentations.</p><p>Our method utilizes the particle video approach of Sand and Teller <ref type="bibr" target="#b22">[23]</ref> to densely track points in the video. Object tracking is a widely researched topic in computer vision, and many other tracking approaches are possible; Yilmaz et al. <ref type="bibr" target="#b30">[31]</ref> recently surveyed the state of the art. However, particle video is especially well suited to interactive video applications because it provides a dense field of tracked points that can track fairly small objects, and even points in featureless regions. An important advantage of the particle video approach over other methods is that it produces tracks that are both spatially dense and temporally long-range.</p><p>Our grouping preprocess accomplishes some of the same goals as the object grouping technique of Sivic et al. <ref type="bibr" target="#b25">[26]</ref>, which tracks features using affine-covariant feature matching and template tracking, followed by a grouping method employing co-occurrence of tracks in motion groups. That method has shown significant success at grouping different views of the same object even through deformations and significant lighting changes. However, after some experimentation we found that it has several drawbacks for our application, which we discuss in depth in Section 6.</p><p>Balakrishnan and Ramos <ref type="bibr" target="#b17">[18]</ref> also developed a system with a novel navigation and annotation interface to video. However, their approach is based on a linear timeline model, and annotations apply only to individual frames in a video.</p><p>Thought and speech balloons have previously been employed in virtual worlds and chat rooms <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b12">13]</ref>, in which the associated regions are known a priori. Kurlander et al. <ref type="bibr" target="#b12">[13]</ref> specifically address the problem of balloon layout. However, their system was free to assign the placement of both the word balloons and the subjects speaking them, whereas our system is designed to associate thought and speech balloons with arbitrary moving video objects.</p><p>We are not the first to propose the notion of hyperlinked video as described in Section 4.1. To our knowledge, the earliest reference of this is the Hypersoap project <ref type="bibr" target="#b5">[6]</ref>. However, the authoring tool proposed in that work required extensive user annotation of many frames. Smith et al. <ref type="bibr" target="#b26">[27]</ref> taxonomized hyperlinks in dynamic media, but their implementation is limited to simple template tracking of whole objects. We believe our system offers a significantly improved authoring environment for this type of rich media.</p><p>Numerous previous works discuss composition of a still or a short video from a longer video, for the purposes of moving texture synthesis <ref type="bibr" target="#b24">[25]</ref>, modifying dialogue <ref type="bibr" target="#b3">[4]</ref>, animating video sprites <ref type="bibr" target="#b23">[24]</ref>, or video summarization <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. In this paper we advocate and demonstrate the approach of drag-and-drop manipulation as a tool for interactively directing such compositions.</p><p>Our system features a novel interface for scrubbing through video using direct manipulation of video objects. This technique is similar in spirit to the storyboard-based scrubbing approach of Goldman et al. <ref type="bibr" target="#b7">[8]</ref>, but permits manipulation directly on the video frame, rather than on an auxiliary storyboard image. The system of Goldman et al. required several minutes of user interaction to author a storyboard before this manipulation can be performed. In contrast, our approach requires only automated preprocessing to enable direct-manipulation navigation. Indeed, we propose that our navigation system can be used as part of an authoring system to create such storyboards, as described in Section 4.3. Our video navigation mechanism can be used to re-animate video of a person, in much the same way as the spacetime faces system <ref type="bibr" target="#b31">[32]</ref>, but without requiring a complex 3D shape acquisition system. Kimber et al. <ref type="bibr" target="#b11">[12]</ref> introduced the notion of navigating a video by directly manipulating objects within a video frame. They also demonstrate tracking and navigation across multiple cameras. However, their method uses static surveillance cameras and relies on whole object tracking, precluding navigation on multiple points of a deforming object such as we demonstrate in Figure <ref type="figure" target="#fig_9">10</ref>.</p><p>Contemporaneous works by Karrer et al. <ref type="bibr" target="#b10">[11]</ref> and Dragicevic et al. <ref type="bibr" target="#b6">[7]</ref> use flow-based preprocessing to enable real-time interaction that is very similar to our approach described in Section 4.2. Their research demonstrates that direct manipulation video browsing permits significant performance improvements for frame selection tasks. However, our work advances this concept in four important ways: First and foremost, this paper presents a general framework enabling a number of different kinds of direct video manipulations, not only navigation. Second, our preprocessing method achieves the long-range accuracy of object-tracking <ref type="bibr" target="#b11">[12]</ref> and feature-tracking methods <ref type="bibr" target="#b6">[7]</ref>, while also retaining the spatial resolution provided by optical-flowbased techniques <ref type="bibr" target="#b10">[11]</ref>. Our use of motion grouping also improves the robustness of the system to partial occlusions, and makes it possible to track points well even near the boundaries of objects, points which might otherwise "slip" off of one object onto another at some frame, causing an incorrect trajectory. Third, our "starburst" manipulator widget effectively represents the space of both simple and complex object motions, whereas the motion path arrows employed in earlier work are most effective at representing simple, smooth motion paths. The starburst widget is more effective in situations such as Figure <ref type="figure" target="#fig_9">10</ref>, in which it is more important to convey the range of feasible motion than an object's specific path through the video (in this case, a spiral). Fourth, our introduction of multiple constraints enables simple access to more elaborate object configurations, also as shown in Fig-ure <ref type="bibr" target="#b9">10</ref>. Finally, we introduce an inertial slider mechanism allowing access to frames in which an object is off-screen.</p><p>In some respects, the approaches of Karrer et al. and Dragicevic et al. have advantages over our framework. Most notably, both methods use a less expensive preprocessing approach than ours, potentially making them more widely applicable in practice. Both methods also include terms in their cost functions that discourage discontinuous jumps in time, which may be desirable for certain applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Overview</head><p>Our system consists of several off-line preprocessing stages (Section 2), followed by an interactive interface for video selection that is common to all our techniques (Section 3). The subsequent section describes applications; our interfaces for video annotation (Section 4.1), navigation (Section 4.2), and recomposition (Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRE-PROCESSING</head><p>Our preprocessing consists of two phases. First, point particles are placed and tracked over time (Section 2.1). Second, the system aggregates particles into consistent moving groups (Section 2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Particle tracking</head><p>To track particles, we apply the "particle video" long-range point tracking method <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b21">22]</ref>, which takes as input a sequence of frames and produces as output a dense cloud of particles, representing the motion of individual points in the scene throughout their range of visibility. Each particle has a starting and ending frame, and a 2D position for each frame within that range.</p><p>The key advantage of the particle video approach over either template tracking or optical flow alone is that it is both spatially dense and temporally long-range. In contrast, feature tracking is long-range but spatially sparse, and optical flow is dense but temporally short-range. Thus, particle video data is ideal for our applications, as we can approximate the motion of any pixel into any other frame by finding a nearby particle.</p><p>In the sections that follow, we will use the following notation: A particle track i is represented by a 2D position x i (t) at each time t during its lifetime t ∈ T (i). The total number of particles is denoted n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Particle grouping</head><p>For certain annotation applications, we find it useful to estimate groups of points that move together over time. Our system estimates these groupings using a generative K-affines motion model, in which the motion of each particle i is generated by one of K affine motions plus isotropic Gaussian noise:</p><p>x</p><formula xml:id="formula_0">i (t + Δt) = A L(i) (t)[x i (t)] + η (1)</formula><p>Here A k (t) represents the affine motion of group k from time t to time t + Δt, and η is zero-mean isotropic noise with standard deviation σ . (For notational convenience we write A k (t) as a general operator, rather than separating out the linear matrix multiplication and addition components.) In our system, Δt = 3 frames. Each particle is assigned a single group label 1 ≤ L(i) ≤ K over its entire lifetime. The labels L(i) are distributed with unknown probability</p><formula xml:id="formula_1">P[L(i) = k] = π k . We denote group k as G k = {i|L(i) = k}.</formula><p>Our system optimizes for the maximum likelihood model</p><formula xml:id="formula_2">Θ = (A 1 ,...,A K , π 1 ,...,π K , L(1),...,L(n)) (2)</formula><p>using an EM-style alternating optimization. Given the above generative model, the energy function Q can be computed as:</p><formula xml:id="formula_3">Q(Θ) = ∑ i ∑ t∈T (i) d(i,t) 2σ 2 -log(π L(i) )<label>(3)</label></formula><p>where 2 , the residual squared error.</p><formula xml:id="formula_4">d(i,t) = ||x i (t + Δt) -A L(i) (t)[x i (t)]||</formula><p>To compute the labels L(i), we begin by initializing them to random integers between 1 and K. Then, the following steps are iterated until Q converges:</p><p>• Affine motions A k are estimated by a least-squares fit of an affine motion to the particles G k .</p><p>• Group probabilities π k are computed as the numbers of particles in each group, weighted by particle lifespan, then normalized such that ∑ k π k = 1.</p><p>• Labels L(i) are reassigned to the label that minimizes the objective function Q(Θ) per particle, and the groups G k are updated.</p><p>In the present algorithm we fix σ = 1 pixel, but this could be included as a variable in the optimization as well.</p><p>The output of the algorithm is a segmentation of the particles into K groups. Figure <ref type="figure" target="#fig_1">2</ref> illustrates this grouping on one of our input datasets. Although there are some misclassified particles, the bulk of the particles are properly grouped. Our interactive selection interface, described in Section 3, can be used to overcome the minor misclassifications seen here. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VIDEO SELECTION</head><p>Several of our interactions require the selection of an object in the video, either for annotation or direct manipulation of that object. The object specification task is closely related to the problems of interactive video segmentation and video matting <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29]</ref>. However, these other methods are principally concerned with recovering a precise matte or silhouette of the object being selected. In contrast, the goal of our system is to recover the motion of a particular object or region of the image, without concern for the precise location of its boundaries.</p><p>In our system, the user selects video objects simply by painting a stroke or dragging a rectangle over a region R s of the image. This region is called the anchor region, and the frame on which it is drawn is called the anchor frame, denoted t s .</p><p>The anchor region defines a set of anchor tracks. For some applications, it suffices to define the anchor tracks M(s) as the set of all particles on the anchor frame that lie within the anchor region:</p><formula xml:id="formula_5">M(s) = {i|t s ∈ T (i), x i (t s ) ∈ R s } (4)</formula><p>However, this simplistic approach to selecting anchor tracks requires the user to scribble over a potentially large anchor region. The system can reduce the amount of user effort by employing the particle groupings computed in section 2.2.</p><p>Our interface uses the group labels of the particles in the anchor region to infer entire group selections, rather than individual particle selections. To this end, the system supports two modes of object selection. First, the user clicks once to select the group of points of which the closest track is a member. The closest track i * to point x 0 on frame t 0 is located as:</p><formula xml:id="formula_6">i * (x 0 ,t 0 ) = argmin {i|t 0 ∈T (i)} x 0 -x i (t 0 ) , (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>and the selected group is simply G L(i * (x,t)) . Second, the user paints a "sloppy" selection that includes points from multiple groups. The resulting selection consists of the groups that are well represented in the anchor region. Each group is scored according to the number |M k (s)| of its particles in the anchor region s. Then any group whose score is a significant fraction T G of the highest scoring group is accepted:</p><formula xml:id="formula_8">M k (s) = G k ∩ M(s) ∀1 ≤ k ≤ K (6) S k (s) = |M k (s)| max 1≤k≤K |M k (s)| (7) G(s) = {k|S k (s)≥T G } G k (8)</formula><p>The threshold T G is a system constant that controls the selection precision. When T G = 1, only the highest-scoring group argmax k |M k (s)| is selected. As T G approaches 0, any group with a particle in the selected region will be selected in its entirety. We have found that T G = 0.5 generally gives intuitive results.</p><p>Our system's affine grouping mechanism may group particles together that are spatially discontiguous. However, discontiguous regions are not always desired for annotation or manipulation. To address this, only the particles that are spatially contiguous to the anchor region are selected. This effect is achieved using a connected-components search through a pre-computed Delaunay triangulation of the particles on the anchor frame.</p><p>Both the group-scoring formula and connected components search take only a fraction of a second, and can therefore be recomputed on the fly while a paint stroke is in progress in order to give visual feedback to the user about the regions being selected.</p><p>By allowing more than one group to be selected, the user can easily correct the case of over-segmentation, such that connected objects with slightly different motion may have been placed in separate groups. In the case of annotation, if a user selects groups that move independently, the attached annotations will simply be a "best fit" to both motions.</p><p>Using groups of particles confers several advantages over independent particles. As previously mentioned, user interaction is streamlined by employing a single click or a loose selection to indicate a moving object with complex shape and trajectory. Furthermore, the large number of particles in the object groupings can be used to compute more robust motion estimates for rigidly moving objects. The system can also annotate objects even on frames where the original anchor tracks M(s) no longer exist due to partial occlusions or deformations. (However, our method is not robust to the case in which an entire group is occluded and later becomes visible again. This is a topic for future work.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">APPLICATIONS</head><p>Our interactive interface is patterned after typical drawing and painting applications, with the addition of a video timeline. The user is presented with a video window, in which the video can be scrubbed back and forth using either a slider or a direct manipulation interface described in Section 4.2. A toolbox provides access to a number of different types of annotations and interaction tool modes, which are applied using direct manipulation in the video window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Video annotations</head><p>Telestrator-like markup of video can be useful not only for sports broadcasting but also for medical applications, surveillance video, and instructional video. Film and video professionals can use annotations to communicate editorial information about footage in post-production, such as objects to be eliminated or augmented with visual effects. Annotations can also be used to modify existing video footage for entertainment purposes with speech and thought balloons, virtual graffiti, "pop-up video" notes, and other arbitrary signage.</p><p>In this Section, we describe our system's support for five types of graphical annotations, distinguished by their shape and by the type of transformations (e.g., translation / homography / similarity) with which they follow the scene. An annotation's motion and appearance is determined by the motion of its anchor tracks: Transformations between the anchor frame and other frames are computed using point correspondences between the features on each frame. Some transformations require a minimum number of correspondences, so if there are too few correspondences on a given frame -for example because the entire group is occluded -the annotation is not shown on that frame.</p><p>At present, we have implemented prototype versions of "graffiti," "scribbles," "speech balloons," "path arrows," and "hyperlinks." Graffiti. These annotations inherit a perspective deformation from their anchor tracks, as if they are painted on a planar surface such as a wall or ground plane. Given four or more non-collinear point correspondences, a homography is computed using the normalized direct linear transformation method described by Hartley and Zisserman <ref type="bibr" target="#b9">[10]</ref>.</p><p>Depending on the length of the video sequence, it may be time-consuming to compute the transformations of an annotation for all frames. Therefore, after the user completes the drawing of the anchor regions, the transformations of graffiti annotations are not computed for all frames immediately, but are lazily evaluated as the user visits other frames. Further improvements to perceived interaction speed are possible by performing the computations during idle callbacks between user interactions. Scribbles.</p><p>These simple typed or sketched annotations just translate along with the mean translation of anchor tracks. This annotation is ideal for simple communicative tasks, such as local or remote discussions between collaborators in film and video production. Word balloons. Inspired by Comic Chat <ref type="bibr" target="#b12">[13]</ref> and Rosten et al. <ref type="bibr" target="#b20">[21]</ref>, our system implements speech and thought balloons that reside at a fixed location on the screen, with a "tail" that follows the annotated object. The location of the speech balloon is optimized to avoid overlap with foreground objects and other speech balloons, while remaining close to the anchor tracks.</p><p>Path arrows. These annotations highlight a particular moving object, displaying an arrow indicating its motion onto a plane in the scene. To compute the arrow path we transform the motion of the centroid of the anchor tracks into the coordinate system of the background group in each frame. The resulting path is used to draw an arrow that transforms along with the background. By clipping the path to the current frame's edges, the arrow head and tail can remain visible on every frame, making it easy to determine the subject's direction of motion at any time. We believe this type of annotation could be used by surveillance analysts, or to enhance telestrator-style markup of sporting events.</p><p>Video hyperlinks. Our system can also be used to author dynamic regions of a video that respond to interactive mouse movements, enabling the creation of hyperlinked video <ref type="bibr" target="#b5">[6]</ref>. A prototype hyper-video player using our system as a front end for annotation is shown in Figure <ref type="figure" target="#fig_6">7</ref>. When viewing the video on an appropriate device, the user can obtain additional information about annotated objects, for example, obtaining purchase information for clothing, or additional references for historically or scientifically interesting objects. As a hyperlink, this additional information does not obscure the video content under normal viewing conditions, but rather allows the viewer to actively choose to obtain further information when desired. The hyperlinked regions in this 30-second segment of video were annotated using our interface in about 5 minutes of user time.</p><p>Marking occlusions. When an object being annotated is partially occluded, our system can modify an associated annotation's appearance or location, either to explicitly indicate the occlusion or to move the annotation to an un-occluded region. One indication of occlusion is that the tracked particles in the occluded region are terminated. Although this is a reliable indicator of occlusion, it does not help determine when the same points on the object are disoccluded, since the newly spawned particles in the disoccluded region are not the same as the particles that were terminated when the occlusion occurred. Here again we are aided by the group-  ing mechanism, since it associates these points on either side of the occlusion as long as there are other particles in the group to bridge the two sets. To determine if a region of the image instantiated at one frame is occluded at some other frame, the system simply computes the fraction of particles in the transformed region that belong to the groups present in the initial frame. An annotation is determined to be occluded if fewer than half of the points in the region belong to the originally-selected groups. Figure <ref type="figure" target="#fig_7">8</ref> shows a rectangular annotation changing color as it is repeatedly occluded and disoccluded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Video navigation using direct manipulation</head><p>Given densely tracked video, we can scrub to a different frame of the video by directly clicking and dragging on moving objects in the scene. We have implemented two variations on this idea: The first uses individual mouse clicks and drags, and the second uses multiple gestures in sequence.</p><p>The single-drag UI is implemented as follows: When the user clicks at location x 0 while on frame t 0 , the closest track i * is computed as in equation <ref type="bibr" target="#b4">(5)</ref>, and the offset between the mouse position and the track location is retained for future use: d = x 0x i * (t 0 ). Then, as the user drags the mouse to position x 1 , the video is scrubbed to the frame t * in which the offset mouse position x 1 + d is closest to the track position on that frame:</p><formula xml:id="formula_9">t * = argmin {t∈T (i * )} x 1 + d -x i * (t)<label>(9)</label></formula><p>Since this action mimics the behaviour of a traditional scrollbar or slider, we call it a "virtual slider." Figures <ref type="figure" target="#fig_8">9</ref> and<ref type="figure" target="#fig_9">10(ac</ref>) illustrate this behavior. When the mouse moves to position x 1 , the location of that track at frame 3 is closer to the offset mouse position x 1 + d, so the video is scrubbed to frame 3. Track A ends at frame 5, but the virtual slider is extended to later frames using the next closest track (track B, shown in blue).</p><p>This approach can cause discontinuous jumps in time when manipulating objects that travel away from and then back to their original screen locations. Such jumps can be eliminated by computing distances in (x, y,t) <ref type="bibr" target="#b10">[11]</ref> or (x, y, arc-length) <ref type="bibr" target="#b6">[7]</ref> spaces, as demonstrated in contemporaneous work. However, the discontinuous jumps can actually be preferable in situations such as that shown in Figure <ref type="figure" target="#fig_9">10</ref>, in which the user searches for a scene configuration without attending to exactly when in the video that configuration occurred. In such situations, ignoring temporal continuity gives more salient results and more immediate feedback.</p><p>Visualizing range of motion. To represent the range of motion afforded a virtual slider, we can display it as a line or path arrow overlaid on the image. However, for complex movements this representation may be confusing. Therefore, we have developed a novel representation for displaying range of motion: A "starburst" widget is placed at the constraint location such that the length of the starburst's spikes represent the spatial proximity of nearby frames on the slider. For simple linear motions the starburst widget simply looks like a compass with one arm pointing to the direction of movement forwards in time and another pointing to the direction of movement backwards in time. However, for more complex motions additional arms begin to appear, indicating that portions of the range of motion lie in other directions.</p><p>To display the starburst widget, we first bin the slider positions according to their orientation from the current position x i * (t 0 ). Then, a weight h θ is computed for each arm using all the positions in bin B θ :</p><formula xml:id="formula_10">v t = x i * (t) -x i * (t 0 )<label>(10)</label></formula><formula xml:id="formula_11">h θ = ∑ {t|x i * (t) ∈B θ } exp -||v t || 2 /σ 2 h (<label>11</label></formula><formula xml:id="formula_12">)</formula><p>The length l θ of the starburst arm in direction θ is then given by normalizing and scaling the weights:</p><formula xml:id="formula_13">l θ = k h θ / ∑ ω h ω (<label>12</label></formula><formula xml:id="formula_14">)</formula><p>where k controls the absolute scaling of the starburst. Longer arms are given an arrowhead, and bins with no points do not display any arm at all. This formula is designed to weight the track locations by a Gaussian distance falloff, so that nearby points on the slider are weighted more heavily than distant ones. Since the binning described above can cause aliasing for slider paths that lie just along the boundaries of the orientation bins, we adjust the orientation of the bins on each frame using PCA, centering a bin on the dominant direction of the slider in the local region. Examples of the starburst manipulators in context can be see in Figures <ref type="figure" target="#fig_9">10</ref> and<ref type="figure" target="#fig_10">11</ref>.</p><p>Handling occlusions. Because tracks can start and end on arbitrary frames, it is not always possible to reach a desired frame using a virtual slider constructed from a single track. Therefore we extend the virtual sliders using multiple tracks, as follows: If the last frame of the selected track</p><formula xml:id="formula_15">t max = max [t ∈ T (i * )]</formula><p>is not the end of the video, and the track is not at the edge of the frame, the system finds the next closest track j * on frame t max in the same group that extends at least to t max + 1. The portion of this track that extends beyond frame t max is then appended to the virtual slider, offset by the displacement x i * (t max )x j * (t max ) between tracks in order to avoid a discontinuity (see Figure <ref type="figure" target="#fig_8">9</ref>). This process is continued forwards until either the end of the video is reached, or the virtual slider reaches the edge of the frame, or the nextclosest track is too distant. The process is then repeated in reverse for the first frame of the track, continuing backwards.</p><p>In most cases this algorithm successfully extends the virtual slider throughout the visible range of the object's motion, including partial occlusions. However, our system does not track objects through full occlusions, so the virtual slider typically terminates when the object becomes fully occluded.</p><p>In order to overcome short temporary occlusions, we have added "inertia" to our virtual sliders, inspired by the inertial scrolling feature of Apple's iPhone: During mouse motion we track the temporal velocity of the virtual slider in frames per second. When the mouse button is released, this velocity decays over a few seconds rather than instantaneously. In this way, the user can reach frames just beyond the end of the virtual slider by "flicking" the mouse toward the end of the slider. This feature complements the accurate frame selection mechanism by providing a looser long-distance frame selection mechanism.</p><p>Some virtual sliders may have paths that fold back on themselves, or even spiral around, so that a linear motion of the mouse will jump nonlinearly to different frames. In this case, the temporal velocity of the slider when the mouse is released is not meaningful. Instead, we can retain the spatial constraint for several seconds, advancing its location using the spatial velocity of the mouse pointer before the button was released.</p><p>Unfortunately, spatial inertia cannot be used in all cases, because unlike temporal inertia, there is no way to reach past the end of the virtual sliders: Tracks do not extend beyond the boundaries of the video frame. Therefore our virtual sliders automatically choose between temporal inertia and spatial inertia by examining the predicted next position of the virtual slider: If they are in similar directions, temporal inertia is used. If they are in different directions, spatial inertia is used. We have found this heuristic tends to perform intuitively in most cases.</p><p>Multiple constraints. By extending the scrubbing technique described above to multiple particle paths, we also implement a form of constraint-based video control. The user sets multiple point constraints on different parts of the video image, and the video is advanced or rewound to the frame that minimizes the maximum distance from each particle to its constraint position. Here, c indexes over constraint locations x c , offsets d c , and constrained particles i * c , and C is the set of all constraints:</p><formula xml:id="formula_16">t * = argmin {t∈T (i * )} max c∈C x c + d c -x i * c (t)<label>(13)</label></formula><p>In our current mouse-based implementation, the user can sequentially set a number of fixed-location constraints and one dynamic constraint, controlled by the mouse. However, multiple dynamic constraints could be applied using a multi- One of the constraints will be met exactly while the other is ignored! Using the sum of squared errors for our cost function does not solve this problem, as it also meets the constraints unequally. However, by using the maximum distance in our cost function, the error is made equal for the two most competing constraints at the optimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Video-to-still composition</head><p>Another application enabled by our system is the seamless recomposition of a video into a single still image featuring parts of several different frames of the video. For example, one might wish to compose a still image from a short video sequence featuring multiple participants. Although there may be frames of the video in which one or two subjects are in frame, looking at the camera, and smiling, there may be no one frame in which all of the subjects look good. However, by using different frames of the video for different subjects we may be able to compose a single still frame that is better than any individual video frame in the sequence. Prior work <ref type="bibr" target="#b0">[1]</ref> assumes that the number of frames is small enough that a user can examine each frame individually to find the best frame for each subject. Such interfaces are therefore less appropriate for video input.</p><p>In our system, we take a drag-and-drop approach to video recomposition. Virtual sliders, as described in the previous section, are used to navigate through the video. In addition, we display only the video object under the mouse at the new frame, and the rest of the objects are "frozen" at their previous frame. This is accomplished as follows:</p><p>First, if the camera is moving, we estimate the motion of the background as a homography, using the group with the largest number of tracked points to approximate the background. Subsequently, when other frames are displayed, they are registered to the current frame using the estimated background motion, and all virtual slider paths are computed us-ing the registered coordinate frame<ref type="foot" target="#foot_0">1</ref> . As the mouse button is dragged and the frame changes, a rough mask of the object being dragged is computed using the same connectedcomponents search described in Section 3. This mask is used to composite the contents of the changing frame over the previously selected frame. In this way, these regions appear to advance or rewind through time and the object moves away from its previous location. We also composite the new frame contents within the matte of the object at the frame upon which the mouse button was depressed, in order to remove its original appearance from that region of the image. Although this is not guaranteed to show the proper contents of that image region (for example, a different object may have entered that region at the new frame), it is simple enough to run at interactive rates and usually produces plausible results. When the mouse button is released, a final composite is computed using graph cut compositing <ref type="bibr" target="#b0">[1]</ref> with the object mask as a seed region, removing most remaining artifacts. An illustration of drag-and-drop video recomposition is shown in Figure <ref type="figure" target="#fig_10">11</ref>.</p><p>Our system also supports another type of still composition using video; the schematic storyboards described by Goldman et al. <ref type="bibr" target="#b7">[8]</ref>. In that work, the user selected keyframes from an exhaustive display of all the frames in the video, and manually selected and labeled corresponding points in each of those frames. In contrast, our interface is extremely simple: The user navigates forward and backward in the video using either a standard timeline slider or the "virtual slider" interface, and presses a "hot key" to assign the current frame as a storyboard keyframe. When the first keyframe is selected, the interface enters "storyboard mode," in which the current frame is overlaid atop the storyboard as a ghosted image, in the proper extended frame coordinate system. Changing frames causes this ghosted image to move about the window, panning as the camera moves. The user can add new keyframes at any time, and the layout is automatically recomputed. As described by Goldman et al., the storyboard is automatically split into multiple extended frames as necessary, and arrows can be associated with moving objects using the object selection and annotation mechanisms already described. The resulting interaction is much easier to use than the original Goldman et al. storyboards work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">INFORMAL EVALUATION</head><p>We believe the tools we have demonstrated are largely unique to our system. However, it is possible to associate annotations to video objects using some visual effects and motion The black car is to the right of the white car on all frames of the input video. From left to right, the black car is dragged from its location on frame 1 to a new location on frame 66. The first three panels show the appearance during the drag interaction, and the fourth panel shows the final graph cut composite, which takes about 5 seconds to compute. Unlike a traditional image cut/paste, the black car appears larger as it is dragged leftwards, because it is being extracted from a later video frame. graphics software. We asked a novice user of Adobe After Effects -a commercial motion graphics and compositing tool with a tracking module -to create a single translating "scribble" text annotation on the car shown in Figure <ref type="figure" target="#fig_7">8</ref>. With the assistance of a slightly more experienced user, the novice spent over 20 minutes preparing this test in After Effects.</p><p>However, since After Effects is targeted at professionals, we also asked an expert user -a co-inventor and architect of After Effects -to perform the same annotation task using both After Effects and our "graffiti" tool. We gave him a quick 1 minute introduction to our tool, then asked him to annotate both the car and the crosswalk stripe with a deforming text label, as shown in Figure <ref type="figure" target="#fig_2">3</ref>. Using our tool, which he had not previously used, he completed the task in just 30 seconds using 10 mouse clicks and 12 keystrokes. Using After Effects, the same task took 7 minutes and 51 seconds, using over 74 mouse clicks, 52 click-and-drags, and 38 keystrokes. In a second trial he performed the same task in 3 minutes and 55 seconds, using over 32 mouse clicks, 36 click-and-drags, and 34 keystrokes. Although After Effects offers more options for control of the final output, our system is an order of magnitude faster to use for annotation because we perform tracking beforehand, and we only require the user to gesturally indicate video objects, annotation styles and contents.</p><p>We also assessed the usability and operating range of our system by using it ourselves to create storyboards for every shot in the 9-minute short film, "Kind of a Blur" <ref type="bibr" target="#b8">[9]</ref>. The main body of the film (not including head and tail credit sequences) was manually segmented into 89 separate shots. Of these, 25 were representable as single-frame storyboards with no annotations. For the remaining 64 shots, we preprocessed progressive 720 × 480 video frames, originally captured on DV with 4:1:1 chroma compression. Of the 64 shots attempted, 35 shots resulted in complete and acceptable storyboards. The remaining 29 were not completed satisfactorily for the following reasons (some shots had multiple problems). The particle video algorithm failed significantly on seventeen shots: Eleven due to motion blur, three due to large-scale occlusions by foreground objects, and three due to moving objects too small to be properly resolved. Of the remaining twelve shots, the grouping algorithm failed to converge properly for five shots. Six shots included some kind of turning or rotating object, but our system only effectively annotates translating objects. Nine shots were successfully pre-processed, but would require additional types of annotations to be effectively represented using storyboard notation. Although additional work is needed to expand our system's operating range, these results show promise for our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Limitations</head><p>One important limitation of our system is the length of time required to preprocess video. In our current implementation, the preprocess takes up to 5 minutes per frame for 720 × 480 input video, which is prohibitive for some of the potential applications described here. Although most of the preprocess is highly parallelizable, novel algorithms would be necessary for applications requiring "instant replay."</p><p>Many of the constraints on our method's operating range are inherited from the constraints on the underlying particle video method. This approach works best on sequences with large textured objects moving relatively slowly. Small moving objects are hard to resolve, and fast motion introduces motion blur, causing particles to slip across occlusion boundaries. Although the particle video algorithm is relatively robust to small featureless regions, it can hallucinate coherent motion in large featureless regions, which may be interpreted as separate groups in the motion grouping stage.</p><p>Another drawback is that when a video features objects with repetitive or small screen-space motions -like a subject moving back and forth along a single path, or moving directly toward the camera -it may be hard or impossible to reach a desired frame using the cost function described in Equation <ref type="formula" target="#formula_9">9</ref>. Other cost functions have been proposed to infer the user's intent in such cases <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>We have presented a system for interactively associating graphical annotations to independently moving video objects, navigating through video using the screen-space motion of objects in the scene, and composing new still frames from video input using a drag-and-drop metaphor. Our contributions include the application of an automated preprocess for video interaction, a fluid interface for creating graphical annotations that transform along with associated video objects, and novel interaction techniques for scrubbing through video and recomposing frames of video. The assembly of a well-integrated system enabling new approaches to video markup and interaction is, in itself, an important contribution. Our system performs all tracking and grouping offline before the user begins interaction, and our user interface hides the complexity of the algorithms, freeing the user to think in terms of high-level goals such as the placements of objects and the types and contents of their annotations, rather than low-level details of tracking and segmentation.</p><p>We believe our preprocessing method is uniquely well-suited to our applications. In particular, the long-range stability of the particle video tracks simplifies the motion grouping algorithm with respect to competing techniques: We had initially implemented the feature-based approach described by Sivic et al. <ref type="bibr" target="#b25">[26]</ref>, but encountered several important drawbacks of their approach for our interaction methods. First, the field of tracked and grouped points is relatively sparse, especially in featureless areas of the image. This scarcity of features makes them less suitable for the types of interactive applications that we demonstrate. Second, affine-covariant feature regions are sometimes quite large, and may therefore overlap multiple moving objects. (Both of these drawbacks also are problematic in the method of Dragicevic et al. <ref type="bibr" target="#b6">[7]</ref>.) However, a drawback of our grouping method is that it does require the number of motion groups to be specified a priori.</p><p>Although they work remarkably well, we do not consider our tracking and grouping algorithms to be a central contribution of this work. However, we find it notable that so much interaction is enabled by such a straightforward preprocess. We have no doubt that future developments in computer vision will improve upon our results, and we hope that researchers will consider user interfaces such as ours to be an important new motivation for such algorithms.</p><p>In conclusion, we believe interactive video object manipulation can become an important tool for augmenting video as an informational and interactive medium, and we hope this research has advanced us several steps closer to that goal.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A typical telestrator illustration. ( c Johntex, CC license [30])</figDesc><graphic coords="2,184.31,503.21,110.66,86.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Four frames from a video sequence, with particles colored according to affine groupings computed in Section 2.2. (video footage c 2005 Jon Goldman)</figDesc><graphic coords="4,59.87,604.37,113.54,75.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Graffiti</figDesc><graphic coords="5,456.23,203.45,102.62,70.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 4: Scribbles (footage c 2005 Jon Goldman)</figDesc><graphic coords="5,456.35,400.85,102.38,68.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 5: Word balloons (footage c 2005 Jon Goldman)</figDesc><graphic coords="5,456.35,518.57,102.38,68.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Path arrows</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: A video with highlighted hyperlinks to web pages. (video footage c 2005 Jon Goldman)</figDesc><graphic coords="6,61.07,524.45,226.82,169.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: A rectangle created on the first frame sticks to the background even when its anchor region is partially or completely occluded. The annotation changes from yellow to red to indicate occlusion.</figDesc><graphic coords="6,192.71,136.49,102.38,68.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: When the mouse is depressed at position x 0 on frame 2, track A (shown in red) is selected as the virtual slider track.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Our scrubbing interface is used to interactively manipulate a video of a moving head by dragging it to the left and right (a-c). Nearby frames are indicated by longer starburst arms. At the extremes of motion (c), some arms of the starburst disappear. Additional constraints are applied to open the mouth (d), or keep the mouth closed and smile (e).touch input device. Figures 10(d) and 10(e) illustrate facial animation using multiple constraints. To balance multiple constraints, we employ a max function in Equation 13 instead of a sum, because this function has optima at which the error for two opposing constraints is equal. As a simple example, consider two objects in a 1dimensional scene that travel away from the origin at two different speeds: x a (t) = v a t, x b (t) = v b t, v a = v b . Now imagine that these objects are constrained to positions x * a and x * b respectively. If we were to use a sum of absolute values |v a tx * a | + |v b tx * b | as our cost function, it would be minimized at either t = x * a /v a or t = x * b /v b :One of the constraints will be met exactly while the other is ignored! Using the sum of squared errors for our cost function does not solve this problem, as it also meets the constraints unequally. However, by using the maximum distance in our cost function, the error is made equal for the two most competing constraints at the optimum.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure11: Drag-and-drop composition of a still from video. The black car is to the right of the white car on all frames of the input video. From left to right, the black car is dragged from its location on frame 1 to a new location on frame 66. The first three panels show the appearance during the drag interaction, and the fourth panel shows the final graph cut composite, which takes about 5 seconds to compute. Unlike a traditional image cut/paste, the black car appears larger as it is dragged leftwards, because it is being extracted from a later video frame.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This is identical to the "relative flow dragging" described by Dragicevic et al.<ref type="bibr" target="#b6">[7]</ref> </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank Peter Sand for the use of his source code, Sameer Agarwal and Josef Sivic for discussions about motion grouping, Nick Irving at UW Intercollegiate Athletics for sample video footage, and Harshdeep Singh, Samreen Dhillon, Kevin Chiu, Mira Dontcheva and Sameer Agarwal for additional technical assistance. Special thanks to Jon Goldman for the use of footage from his short film Kind of a Blur <ref type="bibr" target="#b8">[9]</ref>. Amy Helmuth, Jay Howard, and Lauren Mayo appear in our video. Funding for this research was provided by NSF grant EIA-0321235, the University of Washington Animation Research Labs, the Washington Research Foundation, Adobe, and Microsoft.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Interactive digital photomontage</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dontcheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Colburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (Proc. SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="294" to="301" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Keyframe-based tracking for rotoscoping and animation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (Proc. SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="584" to="591" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="http://www.astarls.com" />
		<title level="m">ASTAR Learning Systems</title>
		<imprint>
			<date type="published" when="2006-01">2006. January-2008</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Video rewrite: Driving visual speech with audio</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Covell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Slaney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH 97</title>
		<meeting>SIGGRAPH 97</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Video matting of complex scenes</title>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="248" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hyperlinked video</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dakss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agamanolis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chalom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Bove</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">3528</biblScope>
			<biblScope unit="page" from="2" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video browsing by direct manipulation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bibliowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="237" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Schematic storyboarding for video visualization and editing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (Proc. SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="862" to="871" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Goldman</surname></persName>
		</author>
		<ptr target="http://phobos.apple.com/WebObjects/MZStore.woa/wa/viewMovie?id=" />
		<imprint>
			<date type="published" when="2005-01">197994758. 2005. January-2008</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note>Short film available online</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page">109</biblScope>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DRAGON: A direct manipulation interface for frame-accurate in-scene video navigation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="247" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Trailblazing: Video playback control by direct object manipulation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kimber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dunnigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Girgensohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1015" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Comic chat</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kurlander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Skelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIG-GRAPH &apos;96</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="225" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Video object cut and paste</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (Proc. SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="595" to="600" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The lessons of Lucasfilm&apos;s Habitat</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morningstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Farmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cyberspace: First Steps</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Benedikt</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="273" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Webcam synopsis: Peeking around the world</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gutman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Non-chronological video synopsis and indexing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fluid interaction techniques for the control and annotation of digital video</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UIST &apos;03</title>
		<meeting>UIST &apos;03</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamosaicing: Video mosaics with non-chronological time</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Making a long video short: Dynamic video synopsis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="435" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real-time video annotations for augmented reality</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rosten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Reitmayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Symp. on Visual Computing</title>
		<meeting>Intl. Symp. on Visual Computing</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Long-Range Video Motion Estimation using Point Trajectories</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIT</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Particle video: Long-range motion estimation using point trajectories</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Teller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR &apos;06</title>
		<meeting>CVPR &apos;06</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="2195" to="2202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Controlled animation of video sprites</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schödl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM/Eurographics Symp. on Comp. Animation</title>
		<meeting>ACM/Eurographics Symp. on Comp. Animation</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="121" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Video textures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schödl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH &apos;00</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="489" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Object level grouping for video shots</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schaffalitzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. J. of Comp. Vis</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="189" to="210" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An orthogonal taxonomy for hyperlink anchor generation in video streams using OvalTine</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stotts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-U</forename><surname>Kum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Conf. on Hypertext and Hypermedia</title>
		<meeting>ACM Conf. on Hypertext and Hypermedia</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Changing The Game</title>
		<author>
			<orgName type="collaboration">Sportvision</orgName>
		</author>
		<ptr target="http://www.sportvision.com" />
		<imprint>
			<date type="published" when="2006-01">2006. January-2008</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cohen. Interactive video cutout</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Colburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (Proc. SIG-GRAPH)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="585" to="594" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><surname>Wikipedia</surname></persName>
		</author>
		<author>
			<persName><surname>Telestrator</surname></persName>
		</author>
		<ptr target="http://en.wikipedia.org/w/index.php?title=Telestrator&amp;oldid=180785495" />
		<imprint>
			<date type="published" when="2006-01">2006. January-2008</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Object tracking: A survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2006-12">December 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Spacetime faces: high resolution capture for modeling and animation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (Proc. SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="548" to="558" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
