<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Modal Event Topic Model for Social Event Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shengsheng</forename><surname>Qian</surname></persName>
							<email>shengsheng.qian@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
							<email>tzzhang@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Shao</surname></persName>
							<email>shaojie@uestc.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sci-ences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Big Media Computing Center</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology</orgName>
								<address>
									<postCode>610051</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Modal Event Topic Model for Social Event Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">514FCB903087908114BB2A4D7BCE9923</idno>
					<idno type="DOI">10.1109/TMM.2015.2510329</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Event evolution</term>
					<term>multi-modality</term>
					<term>social event tracking</term>
					<term>social media</term>
					<term>topic model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the massive growth of social events in Internet, it has become more and more difficult to exactly find and organize the interesting events from massive social media data, which is useful to browse, search, and monitor social events by users or governments. To deal with this problem, we propose a novel multi-modal social event tracking and evolution framework to not only effectively capture multi-modal topics of social events, but also obtain the evolutionary trends of social events and generate effective event summary details over time. To achieve this goal, we propose a novel multi-modal event topic model (mmETM), which can effectively model social media documents, including long text with related images, and learn the correlations between textual and visual modalities to separate the visual-representative topics and non-visual-representative topics. To apply the mmETM model to social event tracking, we adopt an incremental learning strategy denoted as incremental mmETM, which can obtain informative textual and visual topics of social events over time to help understand these events and their evolutionary trends. To evaluate the effectiveness of our proposed algorithm, we collect a real-world dataset to conduct various experiments. Both qualitative and quantitative evaluations demonstrate that the proposed mmETM algorithm performs favorably against several state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>spread very fast, and there are substantial amounts of social events with multi-modality (e.g., images, videos, and text) in Internet.</p><p>In real-world scenarios, most of these multimedia contents associated with social events uploaded by users are related with some specific topics, and it is time-consuming for people to manually identify or cluster them. For example, users may want to know the whole theme evolutionary process of the event "Occupy Wall Street" from start to end. When they search for the recent related event on Google News or Flickr using the search engine to collect information given well-defined queries, they could get much information. It would be of great benefit if we could get the evolutionary trends of social events and visualize the theme pattern over time, which is the goal of event tracking and event evolution. Therefore, Given an event initialized with the first story, we need to recognize which subsequent stories describe the same event and mine the event theme patterns and obtain the evolution process over time, and then visualize these automatically.</p><p>Recently, mining and monitoring social event in social media has attracted extensive research interests, such as social event mining <ref type="bibr" target="#b0">[1]</ref>, social event detection and tracking <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b4">[5]</ref> and event evolution <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Different from the traditional event tracking and evolution problems, which generally involve a single modality such as textual information, social media data include unstructured metadata in multiple modalities. Generally, almost all existing work focuses on either textual features or images <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref> in isolation. Limit efforts have been devoted to analyzing these multi-modality in a unified way to model multimedia event content. In different social media platforms, social media events have rich multi-modal information, such as text, images, and videos, which complement each other and are helpful for the social event analysis <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. For example, given the same social events, they may have different textual information due to different users, but their visual information may be similar. Therefore, multi-modal feature fusion is useful for social event analysis <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref>. Reuter and Cimiano <ref type="bibr" target="#b16">[17]</ref> use multi-modal features to model the similarity of events and media data for their assignment. Though many multi-modal features, such as tag, time, location or visual features, are exploited, the importance and effectiveness of those features <ref type="bibr" target="#b17">[18]</ref> have not been studied in details until now. Moreover, most of these previous methods focus on feature designing rather than modeling the textual and visual information jointly and ignoring the semantic relationship among multiple modalities of social events. Thus, it is necessary and challenging to explore an effective multi-modal fusion strategy for social event tracking and evolution analysis. To deal with the above issues, many topic model methods <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref> have been proposed to explore multi-modal topics for social event analysis. For example, Corr-LDA <ref type="bibr" target="#b18">[19]</ref> and mm-LDA <ref type="bibr" target="#b19">[20]</ref> are proposed to capture the topic-level relations between images and annotations. However, Corr-LDA and mm-LDA assume a one-to-one correspondence between the topics of each modality and need strong relativity between textual and visual information. These approaches are restricted to using only short words, which typically correspond to category labels or tags of visual objects in the images. However, richer information in text is discarded in more realistic scenarios, especially in most of multi-modal event documents from Flickr or Google News. As shown in the left panel of Fig. <ref type="figure" target="#fig_0">1</ref>, each event document contains long text and the corresponding images, and the text and images do not satisfy the corresponding constraints. Therefore, we cannot simply adopt the traditional topic model methods. Moreover, as shown in the right panel of Fig. <ref type="figure" target="#fig_0">1</ref>, we can observe that Obama and New York can be well described by both text and images, while economy can only be represented by text documents. This indicates that the dependency relationships between textual and visual modalities like Obama and New York are semantic-representative. We treat these topic descriptions as visual-representative topics <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> that contain clear and concrete visual counterparts. On the other hand, some textual topic descriptions have no clear and concrete visual correspondences, such as Economy, Polity, Election. Such topics are non-visual-representative topics that are properly represented by textual information and difficult to be described by visual content. With the existing topic models, such as Corr-LDA and mm-LDA, we can only obtain the visual-representative topic but give up the non-visual-representative topic and neglect most of the text information in modeling social media documents. To overcome these issues, we propose a novel topic model method (multi-modal Event Topic Model, mmETM) to effectively fuse multi-modality information and consider visual-representative topics and non-visual-representative topics together.</p><p>In this paper, based on the mmETM, we propose a novel multi-modal social event tracking and evolution framework to obtain the evolutionary trends of social events and generate effective event summary details over time as shown in Fig. <ref type="figure">2</ref>, which has several modules: 1) The input is multimedia documents with time-ordered event data downloaded from Google News, including images and texts. Each social media document contains long text and its corresponding images. After pre-processing, they are input into the mmETM model. 2) The multi-modal event topic mining module is to effectively model multi-modal social event documents, which can learn the correlations between textual and visual modalities to separate the visual-representative topics and non-visual-representative topics.</p><p>3) In multi-modal event topic visualization, we can show the learned visual-representative topics and non-visual-representative topics, which can help understand the social events. 4) For event tracking and evolution, we adopt an incremental learning strategy to update the proposed mmETM model over time, denoted as incremental mmETM. In our tracking algorithm, we need to first initialize the mmETM model for each social event. Then, the coming event documents will be determined to which event in the next moment by similarity computing identification. Finally, event documents are assigned to the corresponding social events, and the mmETM will be updated incrementally. In this way, we can track multi-modal social event documents over time and show the whole evolutionary process of events with their topics. Compared with the existing methods, the contributions of our work are fourfold, listed as follows.</p><p>1) Our multi-modal event tracking and evolution framework is suitable for multimedia documents from various social media platforms, which can not only effectively capture their multi-modal topics, but also obtain the evolutionary trends of social events and generate effective event summary details over time. 2) Our proposed mmETM model can exploit the multi-modal property of social event, which can effectively model social media documents including long text with related images and learn the correlations between textual and visual modalities to separate the visual-representative topics and non-visual-representative topics. 3) We propose a novel incremental mmETM model for social event tracking, which can obtain the whole evolutionary process of events with textual and visual topics over time and help understand the events. 4) We collect a large-scale dataset for research on social event tracking with multi-modality information, and will release it for academic use. We evaluate our proposed model and demonstrate that it achieves much better performance than existing methods. The rest of the paper is organized as follows. In Section II, the related work is reviewed. The proposed algorithm is presented in Section III. In Section IV, we report and analyze extensive experimental results. Finally, we conclude the paper with future work in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this Section, we briefly review previous methods which are most related to our work including event tracking and topic model methods.</p><p>1) Social Event Detection and Tracking: With the massive growth of social events in Internet, how to recognize and monitor social event becomes more and more challenging. A lot of work has been done in the area of topic detection and tracking (TDT) <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b27">[28]</ref>. Most of these methods Fig. <ref type="figure">2</ref>. Multi-modal event tracking and evolution framework. The input is the multi-modality data collected from Google News including images and texts. Based on the input data, our algorithm can learn multi-modality topics and track multiple events. After tracking, for each event, it can be visualized with texts and images over time. Meanwhile, we can mine their semantic topics.</p><p>are based on single-modality (e.g., text, images) information or multi-modality information. In the single-modality analysis, many existing methods adopt visual information (e.g., images and videos) or textual information (e.g., names, time references, locations, title, tags, and description) in isolation <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b8">[9]</ref> to model event data for event detection and tracking. Diakopoulos et al. <ref type="bibr" target="#b9">[10]</ref> study event visualization and event analysis by analyzing Twitter messages related to media events. Makkonen et al. <ref type="bibr" target="#b8">[9]</ref> extract meaningful semantic features such as names, time references, and locations, and learn a similarity metric based on a single clustering partition. However, social media includes unstructured metadata in multiple modalities and these single-modality based methods ignore the multi-modal property of social event. Recently, social event analysis with multi-modality has received considerable attention. Xie et al. <ref type="bibr" target="#b5">[6]</ref> apply hierarchical HMM models over the low-level audio-visual features to discover spatio-temporal patterns, latent semantic analysis to find text clusters, and then fuse these multi-modal tokens to discover potential story topics. Lin et al. <ref type="bibr" target="#b28">[29]</ref> propose a novel non-negative matrix factorization framework by discovering multi-relational structures to model the photo stream data including image and short tags from social media streams. Zhang and Xu <ref type="bibr" target="#b3">[4]</ref> propose a CO-PMHT algorithm for cross domain multi-event tracking, which can track events by use of cross-domain knowledge and obtain their summary information over time. While the above methods focus on feature design methods to calculate the similarity of social event documents, which can improve experimental performance, the semantic relationship and importance of those features have not been studied in details. Different from the above methods, we make use of the rich multi-modal contents associated with social events, including long text information and visual information (e.g., images), and propose a novel topic model method based on the traditional mm-LDA model by considering non-visual-representative topics, which can efficiently model multi-modal social event documents.</p><p>2) Event Summarization: Multi-document summarization which addresses the information overload problem has drawn much attention in the past two decades. Gong and Liu <ref type="bibr" target="#b29">[30]</ref> propose a generic text summarization method that creates text summaries by ranking and extracting sentences from the original documents. Haghighi and Vanderwende <ref type="bibr" target="#b30">[31]</ref> present an exploration of generative probabilistic models by utilizing a hierarchical LDA-style model to represent content specificity as a hierarchy of topic vocabulary distributions for multi-document summarization. Zhou et al. <ref type="bibr" target="#b31">[32]</ref> present a novel two-layer summarization framework to summarize multiple disaster-related documents. Wang et al. <ref type="bibr" target="#b32">[33]</ref> propose a new multi-document summarization framework based on sentence-level semantic analysis and symmetric non-negative matrix factorization. However, most existing document summarization methods generate short summaries by selecting sentence from the text streams and ignore the rich visual information.</p><p>3) Social Event Analysis: Sentiment analysis based on social event content has drawn much attention in determining sentiment from underlying text streams. Hu et al. <ref type="bibr" target="#b33">[34]</ref> investigate whether social relations can help sentiment analysis by proposing a sociological approach to handle noisy and short texts for sentiment classification. In <ref type="bibr" target="#b7">[8]</ref>, the authors consider the problem of identifying the segments and topics of an event that garnered praise or criticism according to aggregated Twitter responses, and propose a flexible factorization framework to learn factors about segments, topics, and sentiments for event analytics via Twitter sentiment. The above methods mostly classify sentiments of document sources of the tweets around the event or provide insights into the event's segments and topics through the aggregated Twitter sentiment. In this paper, different from the traditional methods, which conduct the sentiment analysis based on social event content, we focus on social event tracking and evolution analysis via multi-modal event topic model learning.</p><p>4) Topic Model: Topic models, such as Latent Dirichlet Allocation (LDA) <ref type="bibr" target="#b34">[35]</ref> and Probabilistic Latent Semantic Analysis (PLSA) <ref type="bibr" target="#b35">[36]</ref>, have been widely applied to various applications and have many extensions, such as supervised Latent Dirichlet Allocation (SLDA) <ref type="bibr" target="#b36">[37]</ref>, dynamic latent Dirichlet allocation and its variations <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b37">[38]</ref>. Blei and Lafferty <ref type="bibr" target="#b6">[7]</ref> propose the dynamic topic model method which uses state space models on the natural parameters of the multinomial distributions that represent the topics to analyze the time evolution of topics in large document collections. AlSumait et al. <ref type="bibr" target="#b37">[38]</ref> propose an online LDA method, which extends the Gibbs sampling method and utilizes it to derive hyperparameters of the topic-word distribution at the next time slice. Hong et al. <ref type="bibr" target="#b38">[39]</ref> propose a time-dependent topic model for multiple text sources by considering each text stream to have both local and shared topics. However, the above topic model methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b38">[39]</ref> mainly focus on how to apply the model to textual corpora and may not be appropriate for multi-modal data in social media. To overcome these issues, the Corr-LDA <ref type="bibr" target="#b18">[19]</ref> is proposed to capture the topic-level relations between images and annotations. The mm-LDA <ref type="bibr" target="#b39">[40]</ref> considers multi-modal information, such as users' textual annotations and visual images, and is proposed for social relation mining. In <ref type="bibr" target="#b40">[41]</ref>, Putthividhya et al. present a novel topic-regression multi-modal Latent Dirichlet Allocation model (tr-mmLDA) for the task of image and video annotation. The Corr-LDA and mm-LDA assume a one-to-one correspondence between the topics of each modality, and the tr-mmLDA introduces a regression module to consider the correlation between different topics. In a word, these methods need strong relativity between the textual and visual information. Nevertheless, in more realistic scenarios, especially in most of multi-modal event documents from Flickr or Google News, a lot of long textual information without the corresponding visual information does not satisfy the constraint. To deal with the above issues, we propose a novel topic model method (mmETM) based on the traditional mm-LDA model by considering non-visual-representative topics, and we focus on the multi-modal data and model multi-modal document including long text and image to learn the correlations between the textual and visual modalities. Different from the proposed method, in <ref type="bibr" target="#b38">[39]</ref>, it mainly focuses on the text data mining across different platforms and is the multi-domain extension of the special words with background (SWB) model in <ref type="bibr" target="#b41">[42]</ref>. While the proposed model is the multi-modal extension of the Corr-LDA <ref type="bibr" target="#b18">[19]</ref> and the mm-LDA <ref type="bibr" target="#b19">[20]</ref> and is applied for social event tracking and evolution. Moreover, the method <ref type="bibr" target="#b38">[39]</ref> adopts the switch variable to obtain local topics of each platform and common topics of all platforms, while the proposed model introduces the switch variable to obtain two types of topics by separating the non-visual-representative topics and visual-representative topics in the multi-modal documents form the single platform. For event tracking, based on the proposed mmETM model, we adopt an incremental strategy according to traditional online LDA method <ref type="bibr" target="#b37">[38]</ref> to consider the temporal ordering about event documents to incrementally update the event model. Compared with other existing models, the proposed mmETM can not only construct strong relativity between textual and visual information, but also model textual information without the corresponding visual information. Therefore, our mmETM is much more suitable to model multi-modal social event documents for event tracking and evolution analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OUR PROPOSED ALGORITHM</head><p>In this Section, we first formally define our problem of multimodal social event tracking and evolution. We then introduce the proposed model, parameter inference, incremental parameter inference, and event tracking in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Preliminaries</head><p>An event is something that occurs at specific place and time associated with some specific actions <ref type="bibr" target="#b42">[43]</ref> and consists of many stories over time. Therefore, event tracking and evolution aims at linking together evolving and historical stories. In this paper, our aim is to track multiple events, and obtain their evolutionary trends and multi-modal topic patterns and generate effective event summary details over time. To describe multi-modal social event data, we adopt the traditional bag-of-word method for both image and text as other existing topic model methods <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b43">[44]</ref>. Here, a multimedia document consisting of long text and the corresponding images is thus summarized as a pair of vectors of word counts corresponding to visual and textual information, respectively. Specifically, an image word is denoted as a unit-basis vector of size with exactly one non-zero entry representing the membership to only one word in a dictionary of words. A text word is similarly defined for a dictionary of size . An image is a collection of word occurrences denoted by , and the text is a collection of word occurrences denoted by . Suppose we are given a collection of social media documents related to the same social event with a sliding windows time , where each document consists of two components: textual component and visual component . Note that the two components such as and are not necessary to be contained. Since the epoch is a discrete variable, we set time period for an epoch according to the evolution time of social events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-Modal Event Topic Model</head><p>It is well observed that the dependency relationships between textual and visual modalities of different semantic concepts are different <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>. In most of event documents containing long text and the corresponding images, some semantic topic descriptions can be represented well by both textual and visual content, and some topic descriptions are appropriately represented by textual content and have no clear visual correspondence, such as economy, history, etc. For the social event documents, we assume that the topic words are generated from two domains, including visual-representative topic space shared by  Table <ref type="table" target="#tab_0">I</ref> lists the key notations of our proposed mmETM model. In the mmETM model, a document could be a tagged photo, or a long news with images. Fig. <ref type="figure" target="#fig_1">3</ref> illustrates the graphical representation of mmETM. From the figure, we can see that the proposed model is based on the traditional mm-LDA model by considering non-visual-representative topics, which can effectively model multi-modal social event documents. Each document is associated with two different topic distributions: over topics shared between textual and visual modalities, and over topics unique to textual modality. Each kind of topics is probability distribution over textual or visual words. In the model, we use binary variable to control whether the topic word is generated from the visual-representative topic space or the non-visual-representative topic space. When or , the topic word is generated from the visual-representative topic space or the non-visual-representative topic space, respectively. We assume that all visual aspect words are generated from visual-representative topic space, i.e., . We omit the illustration of in the plate of visual aspect words for simplicity. Input multimedia documents in the epoch , our aim is to infer the two document-topic distributions and , and a set of visual-representative topics , and non-visual-representative topics . The represents that textual and visual information in a social event document share the same document-specific distribution over topics while the includes only textual information in a social event document. Accordingly, the generative process of mmETM for a document with visual words and textual words can be described as follows. Actually, during the model learning process, we assume the priors distributions follow symmetric Dirichlet, which are conjugate priors for multinomial (or Beta for Bernoulli). Each event document has two different kinds of features:</p><p>, which are textual and visual information, respectively. Note that, in some event document, its textual or visual information may miss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Model Inference</head><p>Exact inference is often intractable in many topic models and appropriate methods must be used, such as variational inference <ref type="bibr" target="#b34">[35]</ref> and Gibbs sampling <ref type="bibr" target="#b46">[47]</ref>. To estimate the latent variables conditioned on the observed variables, namely</p><p>, where is a binary switch, are topic assignments. We employ collapsed Gibbs sampling method to obtain samples of latent variables and estimate unknown parameters in the mmETM. In a Gibbs sampler, one iteratively samples new assignments of latent variables by drawing from the distributions conditioned on the previous state of the model. The derivation of update rules is detailed in the Appendix A. We list the update rules for the latent variables as follows: For the switch variable</p><formula xml:id="formula_0">(1) (2)</formula><p>where is the number of times that has been sampled in document and represents the number of times that has been sampled in , is the number of times of a word (textual word or visual word) in document assigned to visual-representative topic , and denotes the number of times a word (only textual word) in document assigned to non-visual-representative topic . For all the counts, subscript indicates that the th word is excluded from the computation.</p><p>Next, when we sample the textual topic variable</p><p>when where is the number of times of a textual word assigned to the visual-representative topic , while is the number of times of a textual word assigned to non-visual-representative topic .</p><p>Similarly, the update rule for latent visual topic variable of visual words is <ref type="bibr" target="#b3">(4)</ref> where is the number of times of a visual word assigned to the visual-representative topic .</p><p>After finishing the Gibbs sampling, we can estimate the parameters just like <ref type="bibr" target="#b46">[47]</ref>. Therefore, we have </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">end</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Incremental Parameter Inference</head><p>To track multiple events over time, we present an incremental inference method for our proposed mmETM, that sequentially updates the model at each epoch using the newly obtained event documents and the parameters of the previous epoch. Since an event consists of many stories over time, the incremental updating strategy allows our proposed mmETM model to work in an online mode. Our solution is motivated by the Empirical Bayes method <ref type="bibr" target="#b37">[38]</ref> via sampling words in the newly arrived event documents according to the distribution represented so far by the current model. Specifically, in the mmETM at a epoch, the count of words in topics is used to construct priors at the following epoch. We use the hyper-parameter as the prior observation counts on the number of times of words sampled from a topic. Therefore, the count of words in topics, resulted from the mmETM on event documents received at epoch , can be used as the priors for the epoch . At epoch , each document consists of two components: textual component and visual component , where , . Let denote the corresponding evolutionary matrix of topic-word distribution in which the columns are the topic-word counts, generated for documents received within the time specified by the previous epoch . Here, the parameters of a topic in visual-representative topics at epoch are determined by the topic's past distribution as follows:</p><p>(5) And, the parameters of a topic in non-visual-representative topics at epoch are determined by the topic's past distribution as follows: <ref type="bibr" target="#b5">(6)</ref> Note that, at epoch , the visual-representative topics , and non-visual-representative topics are drawn from a Dirichlet prior, where the priors are initialized to some constant as done in the original mmETM modeling. We can effectively simplify the incremental parameter inference by making use of the conjugancy property of Dirichlet and multinomial distributions. In our online topic model, by tracking the previous event documents as prior patterns, the main difference in this regard is that the inference problem in our online approach is solved by using chunks of the data instead of the whole set. An overview of the proposed online mmETM algorithm is shown in Algorithm 1. The inputs of the algorithm are: fixed Dirichlet values, and , which are used to initialize the priors and , respectively, at epoch 1. And, multimedia documents of an social event over time are . Here, is the number of stories according to the evolution time of social event. The outputs of the algorithm are: generative models including visual-representative topics , non-visual-representative topics , document-topic distributions and , and the evolution matrices .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Event Tracking</head><p>Through the incremental mmETM model learning, we can obtain the document-topic distributions for each social event over times. In order to track multiple events, we apply the similarity computing identification method into our incremental mmETM model. Based on the learned mmETM model at epoch, multiple event documents can be classified into their corresponding events at epoch using the similarity computing identification method. As a result, multiple events can be tracked over time.</p><p>Next, we will introduce our event tracking algorithm in details and show how to predict a coming multi-modal social event document. Denote the learned mmETM models for all events at epoch as . Here, is the number of events, is the output values of the learned mmETM model for the event at epoch , including document-topic distributions . At epoch , given a new social event multimedia document, we first sample the topic assignments of all tokens including text words and visual words from the learned mmETM models, . When the sampling is finished, we obtain the topic distribution of the new document and use the similarity computing identification method to predict the label of the new document. Specifically, we predict the class label of the new event document according to the ( <ref type="formula">7</ref>) and ( <ref type="formula">8</ref>)</p><formula xml:id="formula_2">(7) (8)</formula><p>Based on the predicted results, the mmETM models, , at epoch , will be updated with the event document of each event at epoch . In our event tracking algorithm, we need to first initialize the mmETM model for each social event. Secondly, many coming event documents will be determined to which class in the next moment according to <ref type="bibr" target="#b6">(7)</ref> and <ref type="bibr" target="#b7">(8)</ref>. Then, we can obtain the classified event documents and update the mmETM models according to Algorithm 1. In this way, we can track multi-modal social event documents from multiple events over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>In this Section, we show extensive experimental results on our collected dataset to demonstrate the effectiveness of our model. We first introduce dataset construction and then show feature extraction. Finally, we give results and analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset Collection</head><p>The evaluation dataset is constructed from online social media sources. Nowadays, there are already some public event datasets, such as the MediaEval social event detection(SED) <ref type="bibr" target="#b47">[48]</ref>. However, the existing MediaEval SED dataset organized and attended by people and illustrated by social media content created by people does not include current hot social events. To the best of our knowledge, there are no multi-modality social event datasets available for social event tracking and evolution analysis. Therefore, we collect the dataset by ourselves from the social media websites. In order to comprehensively evaluate our algorithm, we collect 8 different social events happened in the past few years. For these events, we manually create the introduction page of each event or download it from the Wikipedia page, <ref type="foot" target="#foot_0">1</ref> which contains the whole stories of each event. Specifically, the timeline of the event "Senkaku Islands dispute" can be manually obtained based on the content on the page. 2 We then search and download related text and its corresponding images from Google News based on the keywords in the whole timeline of each social event. Each social media document contains long text and its corresponding images. The detail of our collected dataset is shown in Table <ref type="table" target="#tab_2">II</ref>. The collected 8 social events cover a wide range of topics including politics, economics, military, society, and so on. For each social event, there are about 2000 to 9000 documents with about 40 to 90 epochs. In our social event dataset, many events have time overlap and the time horizon differs from 11 months to years, which makes the event tracking more difficult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature Extraction</head><p>For visual description, we extract the visual words from images at the region level instead of using low-level feature-based visual descriptors. That is, we represent visual words based on the regions. Previous work <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref> has validated that the visual regions can model visual structures and capture interpretable semantics, which is important for illustration of the mined multimodal topics of social event. In our implementation, each image in the dataset is segmented into multiple regions using rectangular grids with 3-level spatial pyramids. Each image patch is extracted and represented as 809 dimensional feature vector consisting of color moment, edge histogram, wavelet texture, LBP, and GIST features. Then, we conduct clustering on all regions and apply the affinity propagation algorithm <ref type="bibr" target="#b50">[51]</ref> to construct the visual codebook in all datastet. By hard assignment coding of each patch, each image can be described as the counts of the words in the codebook. For textual description, we use stemming method and stop words elimination and remove words with a corpus frequency less than 15 in the whole stories of the event, and take the commonly used vector space model to represent the text feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results and Analysis</head><p>In this section, we show experimental results and analysis, which include the parameter analysis, the qualitative evaluation of the mined visual and topics, and the quantitative results compared with the existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Topic Number Selection:</head><p>In topic modeling, how to set the topic number and in the mmETM model is not trivial.</p><p>In the experiments, we resort to the perplexity as the criterion for topic number selection. Perplexity is a standard measure for estimating how well one generative model fits the data. A lower 2 [Online]. Available: http://en.wikipedia.org/wiki/Senkaku_Islands_dispute perplexity score indicates better performance <ref type="bibr" target="#b34">[35]</ref>. In our case, since each event includes many stories, we use the average perplexities over the epoches, which reflects the generalizability of the model for new unseen documents. More formally, the joint probability of a test set of documents is calculated as perplexity</p><p>For each social event, we randomly select 10% of the documents as a held-out test data and train the model on the remaining documents. The average perplexities over the epochs of the incremental mmETM model with different setting of topic number and for the event "Senkaku Islands dispute" and "Greek protests" are shown in Figs. <ref type="figure" target="#fig_3">4(a</ref>) and (b), respectively. From the Fig. <ref type="figure" target="#fig_3">4</ref>(a), we can find that our model achieves the minimum perplexity when the topic number is set as and . Therefore, we set the optimal number of topic and to 10 and 25 for the event "Senkaku Islands dispute", respectively. Similarly, the topic and are set to 10 and 20 for the event "Greek protests", respectively. The topic number selection process for other social events is similar and the results are shown in Table <ref type="table" target="#tab_3">III</ref>. We choose the desired topic number and as shown in Table <ref type="table" target="#tab_3">III</ref> for the proposed model in the following experiments. Note that, we select the same topic number for the event tracking evaluation.</p><p>2) Qualitative Evaluation: In this section, we will qualitatively demonstrate the effectiveness of our proposed model for social event analysis. For simplicity, we first visualize the learned visual-representative and non-visual-representative topics in the first epoch of social event in the Fig. <ref type="figure" target="#fig_4">5</ref>, which can validate the effectiveness of our proposed mmETM model. In Fig. <ref type="figure" target="#fig_4">5</ref>, we show an example of the discovered topics for the event "United States Presidential Election" and "Occupy Wall Street", respectively. By providing a multi-modal information of the representative textual and visual words, it is very intuitive to interpret the social events with each associated  topic. For the visualization of visual-representative and non-visual-representative topics, we present the top-ranked textual words and visual patches for each visual-representative topic, and the top-ranked textual words for each non-visual-representative topic, which can help interpret the property of social events. The textual words and visual patches are sorted by the probability , , respectively. In each visual-representative topic, the coupling between and can indicate the correlations between the textual words and image patches. From Fig. <ref type="figure" target="#fig_4">5</ref>, we can see that the words of each topic show clear interpretations, which describe one property of the event, such as the "Obama" for "United States Presidential Election" and the "protester" for "Occupy Wall Street". The visual-representative topics represented by textual words and well relevant visual patches are meaningful and show high consistency between semantic concepts and visual content. In each non-visual-representative topic, the discovered topic words represented by textual information are difficult to be described by visual content, such as the "right" for "United States Presidential Election" and the "business" for "Occupy Wall Street". By mining the visual-representative and non-visual-representative topics based on the proposed mmETM model, we can obtain informative multi-modal topic words of events at the moment, which can be utilized for event evolution and event visualization.</p><p>3) Quantitative Evaluation: Experiments On Online Topic Modeling: To conduct quantitative evaluation of multi-modal topic mining, we measure the performance of online topic modeling with the comparison of the soft clustering quality. Moreover, to demonstrate the effectiveness of the proposed model to correctly model the relationships between the two modalities, we adopt text/image perplexity as a performance measure referred to <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b51">[52]</ref>. To demonstrate the effectiveness of our mmETM model, we compare it with the existing three baseline methods:</p><p>• LDA <ref type="bibr" target="#b34">[35]</ref>: this is a LDA-based extension by concatenating the textual and visual features; • correspondence LDA (Corr-LDA) <ref type="bibr" target="#b18">[19]</ref>: this model assumes a one-to-one correspondence among the topics of different modalities, which captures the topic-level relations between images and text annotations; and • mm-LDA <ref type="bibr" target="#b19">[20]</ref>: this model extends LDA to represent the joint distribution of images and text by using textual and visual information jointly. 4) The Comparison of Soft Clustering Quality: The soft clustering quality method is adopted to measure the quality of identifying the topic clusters on the test set. Because the proposed model is actually a soft clustering technique, the cluster with the highest probability is selected as its topic cluster for each event document including images and long text. In the experiment, we manually build a set of true topic clusters for each event as ground truth, where a cluster is made up of many documents that describe common topic content. Specifically, we invite five subjects who are graduates and familiar with the studied event analysis. Each annotator is assigned three event categories. Because it is time-consuming to label a large amount of data, we only randomly sample 500 documents including long text and related images for each event from the collected dataset. Each annotator manually defines some topic clusters related to each event and labels each document. Documents that describe common topic content are grouped into a cluster, which describes a certain topic of the event. The statistics of the built topic clusters for each event are summarized in Table <ref type="table" target="#tab_4">IV</ref>. We utilize the purity <ref type="bibr" target="#b52">[53]</ref> as the evaluation metric. The purity is a standard measure of clustering quality in tradition clustering problem. For each event, let denote the mined topics, and denote its manually labeled topic groups. More formally, the purity metric is defined as Purity <ref type="bibr" target="#b10">(11)</ref> where is the total number of topics in the ground truth, and a higher purity score means a better performance. The purity scores of different models are shown in Fig. <ref type="figure" target="#fig_5">6</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) The Comparison of Text/Image Perplexity:</head><p>To effectively measure the ability in tracking long text with correlations between two modalities, we adopt the text/image perplexity as a performance measure <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b51">[52]</ref>. Given the test image content, we need to compute the condition probability of long text. Specifically, the text/image perplexity of a set of testing data is calculated as the exponential of the negative normalized predictive log-likelihood using the training model, and it is defined as <ref type="bibr" target="#b11">(12)</ref> where <ref type="bibr" target="#b12">(13)</ref> In ( <ref type="formula">12</ref>) and ( <ref type="formula">13</ref>), the represents the visual word vector and the represents the textual word vector including of the visual-representative topic space and of the non-visual-representative topic space. After model training, we obtain the topictextual word distribution and the topic-visual distribution specific to the visual-representative topic , and the topic-textual word distribution specific to the non-visual-representative topic . The is an indicator function, which is defined as , and . We use the average text/image perplexities over the epoches in each event to reflect the ability of the model to correctly model relationships between the two modalities. The lower text/image perplexity score shows better performance, which is the same way as the evaluation of topic number selection. The results for different models are shown in Fig. <ref type="figure">7</ref>.</p><p>Based on the results of the comparisons of the soft clustering quality and text/image perplexity as shown in Figs. <ref type="figure" target="#fig_5">6</ref> and<ref type="figure">7</ref>, we can make the following conclusions.</p><p>1) The LDA shows inferior performance. This is due to the fact that LDA models textual and visual aspect words ob- scurely and cannot differentiate the associations between textual and visual words.</p><p>2) The tr-mmLDA, Corr-LDA, and mm-LDA models achieve better results by leveraging the information in texts and images to capture the dependencies between text words and visual words. However, the tr-mmLDA introduces a regression module to correlate the 2 sets of topics, and the Corr-LDA and MM-LDA assume a one-to-one correspondence between the topics of each modality. Their methods need to satisfy strong relativity between the textual and visual information.</p><p>3) The proposed mmETM consistently and significantly outperforms other state-of-the-art topic models. The major reason is that our mmETM can reasonably model the correlations between two modalities more accurately by separating the visual-representative topics and non-visual-representative topics, which makes better document representations in the latent semantic space. 6) The Comparison of Efficiency: In the event tracking and evolution step, we adopt an incremental learning strategy to update the proposed mmETM model over time. Unlike the proposed mmETM, our incremental mmETM is able to identify meaningful topics with no access to the entire data. Therefore, incremental mmETM can work in an online mode with the event consisting of many stories over time. Also, the incremental mmETM is generated from a small fraction of documents, which makes our model more superior than mmETM in terms of time and memory efficiency.</p><p>In Fig. <ref type="figure">8</ref>, it shows the computational cost time required for the incremental mmETM and the mmETM to generate the topic model at every time instance for the event evolution. We observe that incremental mmETM is much more efficient and requires approximately a constant time, because the runtime is proportional to the size of documents over time while the runtime required of the mmETM is accumulative. In addition, mmETM requires the whole data to be stored for future processing, while our incremental mmETM model stores only a metadata of the data in current time in terms of a small number of generative models.</p><p>Experiments on Event Tracking: To evaluate different event tracking methods, we adopt the popular mean average precision (MAP) as a metric, and manually label grough truth tracks for all the events, which can be obtained when we download the data by the corresponding queries. After event tracking, we know the associated label for each document of the story. Based on the tracking results, we can calculate the average precision for each event. Finally, we can calculate the mean of the average precisions of multiple events and obtain the MAP. To verify the effectiveness of multi-modal feature fusion, we add another comparative model which is a standard LDA-based method with only textual feature, denoted as "LDA(Text)". Moreover, we also add the experiment results of the proposed method using traditional SIFT visual descriptors, denoted as "mmETM-SIFT". We compare our event tracking algorithm with different models as described in Section IV-C-III-A. Note that, we apply our incremental updating strategy to implement all baseline methods.</p><p>Table <ref type="table" target="#tab_5">V</ref> and Fig. <ref type="figure">9</ref> show the comparisons of different topic models for multi-event tracking. Based on these results, we have the following observations.</p><p>1) The LDA(Text) model with only textual feature shows inferior tracking performance. This shows multi-modal feature fusion is useful for social event tracking. And, the LDA model shows much worse tracking performance than other multi-modal topic models. This is due to its incapability of learning the latent association relationships between textual and visual topics in modeling multi-modal event document.</p><p>2) The tr-mmLDA, Corr-LDA and mm-LDA models achieve better tracking results. In the multi-modal event tracking, the tr-mmLDA, Corr-LDA and mm-LDA are effective to capture the intrinsic relationships between text words and visual words. However, the correlations between the textual and visual modalities of different semantic concepts are different, especially long text with the corresponding images, which causes these models have limited performance. 3) Our proposed mmETM consistently outperforms other existing state-of-the-art topic models, which is consistent with the experimental results of online topic modeling in Section IV-C-III-A. We can also see that our proposed mmETM-SIFT and mmETM achieve the better than other models, and mmETM based on the region level shows slightly better performance than based on the SIFT visual descriptors. This is because the proposed model can not only construct strong relativity between textual and visual information, but also be able to model textual information without the corresponding visual information. Therefore, our mmETM is much more suitable for multi-modal social event analysis. Table V also presents the computational cost time of different models event tracking We can observe that our proposed mmETM model obtains higher accuracy with almost same time as Corr-LDA and mm-LDA. Based on the results, LDA(Text) costs the least time, because it uses only textual feature and ignores the visual feature. In a word, the experimental results demonstrate the effectiveness of our proposed model for multiple event tracking with multi-modality information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose a novel multi-modal social event tracking and evolution framework to obtain the evolutionary trends of social events and generate effective event summary details over time. Our proposed mmETM can model the multimodal property of social event and learn the correlations between textual and visual modalities to separate the visual-representative topics and non-visual-representative topics. To apply this model for social event tracking, we adopt an effective incremental updating strategy. We have conducted experiments on our collected dataset and extensive results have demonstrated that our model outperforms all other existing models. In the future, we will investigate more tasks under this framework, such as event summarization and event attribute mining in social media. Also, we tend to explore whether the tracking performance can be improved by considering different domains, such as, Flickr, Google News, YouTube.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX DERIVATION OF MULTI-MODAL EVENT TOPIC MODEL</head><p>The generative process corresponds to the following joint distribution of the hidden and observed variables on the whole dataset <ref type="bibr" target="#b13">(14)</ref> In order to sample from the joint distribution using Gibbs sampling, we need to obtain the full condition posterior distribution</p><p>, where denotes all sampled latent variables . We first derive the variable of . Canceling factors that do not depend on , and following the definition of conditional probability, we have <ref type="bibr" target="#b14">(15)</ref> Firstly, we derive the first component of <ref type="bibr" target="#b14">(15)</ref>. For the numerator, since is generated from a Bernoulli distribution whose Beta parameters are , and the Beta is the conjugate prior of Bernoulli, we could solve the Bernoulli-Beta integral using Gibbs sampling. Specifically <ref type="bibr" target="#b15">(16)</ref> where, is gamma function with the parameters and , is the number of times that has been sampled in document and represents the number of times that has been sampled in .</p><p>Next, we apply the <ref type="bibr" target="#b15">(16)</ref> to obtain the first fraction of (15)</p><p>(17) Now we can get when <ref type="bibr" target="#b17">(18)</ref> Similarly, when <ref type="bibr" target="#b18">(19)</ref> Now we turn to the second fraction of <ref type="bibr" target="#b14">(15)</ref>. Specifically, as is a multinomial distribution with the variable , and are conjugate pair of Multinomial-Dirichlet, we can obtain when <ref type="bibr" target="#b19">(20)</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Left panel shows the related content associated with the event including multi-modal information (e.g., images, text) in the social media platform. The example of multi-modal topic description for modeling social event is shown in the right panel. Here, we show the text and images for better understanding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Graphical models of the mmETM.</figDesc><graphic coords="5,79.98,232.14,168.00,238.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 ) 2 )</head><label>12</label><figDesc>For each visual-representative topics including textual topics and visual topics , draw a multinomial distribution over words, and . For each non-visual-representative topics , draw a multinomial distribution over words, . 3) For each document : • draw a binomial distribution over visual-representative topics versus non-visual-representative topics, ; • draw a multinomial distribution over visual-representative topics, ; and • draw a multinomial distribution over non-visual-representative topics, . • For each textual word in : • draw a binary switch ; • if , draw a non-visual-representative topic assignment ; • if , draw a visual-representative topic assignment ; and • draw a topic word from specific word distribution. • For each visual word in : • ; • draw a visual-representative topic assignment ; and • draw a topic word from -specific word distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Impact of different setting about topic number and for the event "Senkaku Islands dispute" and "Greek protests", respectively. (a) Senkaku Islands dispute. (b) Greek protests.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Illustration of samples of discovered topics for the event "Occupy Wall Street" and "United States Presidential Election" (VR represents visual-representative and NVR represents non-visual-representative topics.).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Prity scores of topic identification for different topic models on our collected dataset. Our proposed mmETM model consistently and significantly outperforms other existing topic models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig. 7. Text/image perplexity scores for different topic models on our collected dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,94.98,64.14,400.02,264.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I KEY</head><label>I</label><figDesc>NOTATIONS OF OUR PROPOSED MMETM MODEL</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II ILLUSTRATION</head><label>II</label><figDesc>OF THE EVENT NAME, DURATION TIME, AND NUMBER OF DOCUMENTS FOR EACH EVENT IN OUR COLLECTED SOCIAL EVENT DATASET</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III SELECTED</head><label>III</label><figDesc>NUMBER OF TOPICS FOR EACH SOCIAL EVENT</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV STATISTICS</head><label>IV</label><figDesc>OF BUILT TOPIC CLUSTERS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V MEAN</head><label>V</label><figDesc>AVERAGE PRECISION ON EVENT DATASET FOR DIFFERENT MODELS Fig. 9. Accuracy of different event tracking methods on our collected dataset. Compared with the existing methods, our mmETM achieves the best.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>[Online]. Available: http://www.wikipedia.org</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accordingly, the second fraction of (15) can be written as <ref type="bibr" target="#b20">(21)</ref> Finally, by combining (18), we have when <ref type="bibr" target="#b21">(22)</ref> Similarly, we have when <ref type="bibr" target="#b22">(23)</ref> The ( <ref type="formula">4</ref>) can be derived analogously.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Basic Research Program of China under Grant 2012CB316304, in part by the National Natural Science Foundation of China under Grant 61225009, Grant 61432019, Grant 61572498, Grant 61303173, Grant 61532009, Grant 61472115, Grant 61472379, Grant U1435211, and Grant 61572296, and in part by the Beijing Natural Science Foundation under Grant 4131004. The associate editor coordinating the review of this manuscript and approving it for publication was Prof. K. Selcuk Candan.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mining relationships among intervalbased events for classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGMOD</title>
		<meeting>SIGMOD</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="393" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On-line new event detection and tracking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Papka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic model vectors for complex video event recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Merler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Natsev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="101" />
			<date type="published" when="2012-02">Feb. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cross-domain multi-event tracking via CO-PMHT</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Multimedia Comput. Commun. Appl</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic visual concept learning for social event understanding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Hossain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="346" to="358" />
			<date type="published" when="2015-03">Mar. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discovering meaningful multimedia patterns with audio-visual concepts and associated text</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="2383" to="2386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dynamic topic models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Topic-conditioned novelty detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="688" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simple semantics in topic detection and tracking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Makkonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ahonen-Myka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salmenkivi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Retrieval</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="347" to="368" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Diamonds in the rough: Social media visual analytics for journalistic inquiry</title>
		<author>
			<persName><forename type="first">N</forename><surname>Diakopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kivran-Swaine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symp. Vis</title>
		<meeting>IEEE Symp. Vis</meeting>
		<imprint>
			<date type="published" when="2010-10">Oct. 2010</date>
			<biblScope unit="page" from="115" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multimodal news story clustering with pairwise visual near-duplicate constraint</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="188" to="199" />
			<date type="published" when="2008-02">Feb. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-objective optimization for multimodal visualization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kalamaras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Drosou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tzovaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1460" to="1472" />
			<date type="published" when="2014-08">Aug. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multimodal video indexing and retrieval using directed information</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Hero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="16" />
			<date type="published" when="2012-02">Feb. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Instant mobile video search with layered audio-video indexing and progressive transmission</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2242" to="2255" />
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning consistent feature representation for cross-modal multimedia retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="370" to="381" />
			<date type="published" when="2015-03">Mar. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cross-domain collaborative learning in social multimedia</title>
		<author>
			<persName><forename type="first">S</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM MM</title>
		<meeting>ACM MM</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="99" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Event-based classification of social media streams</title>
		<author>
			<persName><forename type="first">T</forename><surname>Reuter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cimiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICMR</title>
		<meeting>ICMR</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Heterogeneous features and model selection for event-based media classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICMR</title>
		<meeting>ICMR</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="151" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling annotated data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="127" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Matching words and pictures</title>
		<author>
			<persName><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1107" to="1135" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multimedia summarization for social events in microblog stream</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="216" to="228" />
			<date type="published" when="2015-02">Feb. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detecting visual text</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL HLT</title>
		<meeting>NAACL HLT</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="762" to="772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Quantifying visual-representativeness of social image tags using image tag clarity</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Bhowmick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Social Media Modeling Comput</title>
		<meeting>Social Media Modeling Comput</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bringing order to your photos: Event-driven classification of Flickr images based on social knowledge</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Firan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Nejdl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Paiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A generic framework for video annotation via semi-supervised learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1206" to="1219" />
			<date type="published" when="2012-08">Aug. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cross-domain feature learning in multimedia</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="64" to="78" />
			<date type="published" when="2015-01">Jan. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Boosted multifeature learning for cross-domain transfer</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOMCCAP</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Social event classification via boosted multimodal supervised latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Hossain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOMCCAP</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Discovering multirelational structure in social media streams</title>
		<author>
			<persName><forename type="first">Y.-R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kelliher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOMCCAP</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generic text summarization using relevance measure and latent semantic analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="19" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploring content models for multi-document summarization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="362" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generating textual storyline to improve situation awareness in disaster management</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IRI</title>
		<meeting>IRI</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="585" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-document summarization via sentence-level semantic analysis and symmetric matrix factorization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="307" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Exploiting social relations for sentiment analysis in microblogging</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WSDM</title>
		<meeting>WSDM</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="537" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-03">Mar. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UAI</title>
		<meeting>UAI</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Supervised topic models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On-line LDA: Adaptive topic models for mining text streams with applications to topic detection and tracking</title>
		<author>
			<persName><forename type="first">L</forename><surname>Alsumait</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barbará</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Domeniconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDM</title>
		<meeting>ICDM</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A timedependent topic model for multiple text streams</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gurumurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsioutsiouliklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="832" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Right buddy makes the difference: An early exploration of social relation analysis in multimedia applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM MM</title>
		<meeting>ACM MM</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="19" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Topic regression multi-modal latent Dirichlet allocation for image annotation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Putthividhya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Attias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Nagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="3408" to="3415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Modeling general and specific aspects of documents with a probabilistic topic model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chemudugunta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="241" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning approaches for detecting and tracking news events</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Archibald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intell. Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="32" to="43" />
			<date type="published" when="1999-08">Jul./Aug. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">DiscLDA: Discriminative learning for dimensionality reduction and classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="897" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning cross-modality similarity for multinomial data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2407" to="2414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Factorized multi-modal topic model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UAI</title>
		<meeting>UAI</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="843" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Nat. Academy Sci</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Social event detection at MediaEval 2013: Challenges, datasets, and evaluation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Reuter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MediaEval 2013 Multimedia Benchmark Workshop</title>
		<meeting>MediaEval 2013 Multimedia Benchmark Workshop</meeting>
		<imprint>
			<date type="published" when="2013-10">Oct. 2013</date>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Giant: Geo-informative attributes for location recognition and exploration</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MM</title>
		<meeting>MM</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="13" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of midlevel discriminative patches</title>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="73" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Clustering by passing messages between data points</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dueck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">315</biblScope>
			<biblScope unit="page" from="972" to="976" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Topic regression multi-modal latent Dirichlet allocation for image annotation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Putthividhya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Attias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Nagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="3408" to="3415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Empirical and theoretical comparisons of selected criterion functions for document clustering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="311" to="331" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
