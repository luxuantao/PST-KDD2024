<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Plenoptic Modeling: An Image-Based Rendering System</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Leonard</forename><surname>Mcmillan</surname></persName>
							<email>mcmillan@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Carolina at Chapel Hill †</orgName>
								<orgName type="institution">University of North</orgName>
								<address>
									<addrLine>Chapel Hill</addrLine>
									<postCode>3175, 27599</postCode>
									<settlement>Sitterson Hall</settlement>
									<region>CB, NC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gary</forename><surname>Bishop</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Carolina at Chapel Hill †</orgName>
								<orgName type="institution">University of North</orgName>
								<address>
									<addrLine>Chapel Hill</addrLine>
									<postCode>3175, 27599</postCode>
									<settlement>Sitterson Hall</settlement>
									<region>CB, NC</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Plenoptic Modeling: An Image-Based Rendering System</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5BA91F94D0599A63E8ABA55B4DC00ECD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.3.3 [Computer Graphics]: Picture/Image Generation-display algorithms, viewing algorithms</term>
					<term>I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-hidden line/ surface removal</term>
					<term>I.4.3 [Image Processing]: Enhancement-registration</term>
					<term>I.4.7 [Image Processing]: Feature Measurementprojections</term>
					<term>I.4.8 [Image Processing]: Scene Analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image-based rendering is a powerful new approach for generating real-time photorealistic computer graphics. It can provide convincing animations without an explicit geometric representation. We use the "plenoptic function" of Adelson and Bergen to provide a concise problem statement for image-based rendering paradigms, such as morphing and view interpolation. The plenoptic function is a parameterized function for describing everything that is visible from a given point in space. We present an image-based rendering system based on sampling, reconstructing, and resampling the plenoptic function. In addition, we introduce a novel visible surface algorithm and a geometric invariant for cylindrical projections that is equivalent to the epipolar constraint defined for planar projections.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In recent years there has been increased interest, within the computer graphics community, in image-based rendering systems. These systems are fundamentally different from traditional geometry-based rendering systems. In image-based systems the underlying data representation (i.e model) is composed of a set of photometric observations, whereas geometry-based systems use either mathematical descriptions of the boundary regions separating scene elements (B-rep) or discretely sampled space functions <ref type="bibr">(volumetric)</ref>.</p><p>The evolution of image-based rendering systems can be traced through at least three different research fields. In photogrammetry the initial problems of camera calibration, two-dimensional image registration, and photometrics have progressed toward the determination of three-dimensional models. Likewise, in computer vision, problems such as robot navigation, image discrimination, and image understanding have naturally led in the same direction. In computer graphics, the progression toward image-based rendering systems was initially motivated by the desire to increase the visual realism of the approximate geometric descriptions by mapping images onto their surface (texture mapping) <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Next, images were used to approximate global illumination effects (environment mapping) <ref type="bibr" target="#b4">[5]</ref>, and, most recently, we have seen systems where the images themselves constitute the significant aspects of the scene's description <ref type="bibr" target="#b7">[8]</ref>.</p><p>Another reason for considering image-based rendering systems in computer graphics is that acquisition of realistic surface models is a difficult problem. While geometry-based rendering technology has made significant strides towards achieving photorealism, creating accurate models is still nearly as difficult as it was ten years ago. Technological advances in three-dimensional scanning provide some promise in model building. However, they also verify our worst suspicions-the geometry of the real-world is exceedingly complex. Ironically, the primary subjective measure of image quality used by proponents of geometric rendering systems is the degree with which the resulting images are indistinguishable from photographs.</p><p>One liability of image-based rendering systems is the lack of a consistent framework within which to judge the validity of the results. Fundamentally, this arises from the absence of a clear problem definition. Geometry-based rendering, on the other hand, has a solid foundation; it uses analytic and projective geometry to describe the world's shape and physics to describe the world's surface properties and the light's interaction with those surfaces.</p><p>This paper presents a consistent framework for the evaluation of image-based rendering systems, and gives a concise problem definition. We then evaluate previous image-based rendering methods within this new framework. Finally, we present our own image-based rendering methodology and results from our prototype implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">THE PLENOPTIC FUNCTION</head><p>Adelson and Bergen <ref type="bibr" target="#b0">[1]</ref> assigned the name plenoptic function (from the latin root plenus, meaning complete or full, and optic pertaining to vision) to the pencil of rays visible from any point in space, at any time, and over any range of wavelengths. They used this function to develop a taxonomy for evaluating models of low-level vision. The plenoptic function describes all of the radiant energy that can be perceived from the point of view of the observer rather than the point of view of the source. They postulate "… all the basic visual measurements can be considered to characterize local change along one or two dimensions of a single function that describes the structure of the information in the light impinging on an observer."</p><p>Adelson and Bergen further formalized this functional description by providing a parameter space over which the plenoptic function is valid, as shown in Figure <ref type="figure">1</ref>. Imagine an idealized eye which we are free to place at any point in space (V x , V y , V z ). From there we can select any of the viewable rays by choosing an azimuth and elevation angle</p><p>Permission to make digital/hard copy of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, the copyright notice, the title of the publication and its date appear, and notice is given that copying is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. ©1995 ACM-0-89791-701-4/95/008…$3.50</p><p>(θ,φ) as well as a band of wavelengths, λ, which we wish to consider.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FIGURE 1. The plenoptic function describes all of the image information visible from a particular viewing position.</head><p>In the case of a dynamic scene, we can additionally choose the time, t, at which we wish to evaluate the function. This results in the following form for the plenoptic function:</p><p>(</p><p>In computer graphics terminology, the plenoptic function describes the set of all possible environment maps for a given scene. For the purposes of visualization, one can consider the plenoptic function as a scene representation. In order to generate a view from a given point in a particular direction we would need to merely plug in appropriate values for (V x , V y , V z ) and select from a range of (θ,φ) for some constant t.</p><p>We define a complete sample of the plenoptic function as a full spherical map for a given viewpoint and time value, and an incomplete sample as some solid angle subset of this spherical map.</p><p>Within this framework we can state the following problem definition for image-based rendering. Given a set of discrete samples (complete or incomplete) from the plenoptic function, the goal of image-based rendering is to generate a continuous representation of that function. This problem statement provides for many avenues of exploration, such as how to optimally select sample points and how to best reconstruct a continuous function from these samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PREVIOUS WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Movie-Maps</head><p>The Movie-Map system by Lippman <ref type="bibr" target="#b16">[17]</ref> is one of the earliest attempts at constructing an image-based rendering system. In Movie-Maps, incomplete plenoptic samples are stored on interactive video laser disks. They are accessed randomly, primarily by a change in viewpoint; however, the system can also accommodate panning, tilting, or zooming about a fixed viewing position. We can characterize Lippman's plenoptic reconstruction technique as a nearest-neighbor interpolation because, when given a set of input parameters (V x , V y , V z , θ, φ, t), the Movie-Map system can select the nearest partial sample. The Movie-Map form of image-based rendering can also be interpreted as a table-based evaluation of the plenoptic function. This interpretation reflects the database structure common to most imagebased systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Image Morphing</head><p>Image morphing is a very popular image-based rendering technique <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b27">[28]</ref>. Generally, morphing is considered to occur between two images. We can think of these images as endpoints along some path through time and/or space. In this interpretation, morphing becomes a method for reconstructing partial samples of the continuous plenoptic function along this path. In addition to photometric data, morphing uses additional information describing the image flow field. This information is usually hand crafted by an animator. At first θ φ</p><formula xml:id="formula_1">(V x , V y , V z ) p P θ φ λ V x V y V z t , , ,<label>, , , ( ) =</label></formula><p>glance, this type of augmentation might seem to place it outside of the plenoptic function's domain. However, several authors in the field of computer vision have shown that this type of image flow information is equivalent to changes in the local intensity due to infinitesimal perturbations of the plenoptic function's independent variables <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b12">[13]</ref>. This local derivative behavior can be related to the intensity gradient via applications of the chain rule. In fact, morphing makes an even stronger assumption that the flow information is constant along the entire path, thus amounting to a locally linear approximation. Also, a blending function is often used to combine both reference images after being partially flowed from their initial configurations to a given point on the path. This blending function is usually some linear combination of the two images based on what percentage of the path's length has been traversed. Thus, morphing is a plenoptic reconstruction method which interpolates between samples and uses local derivative information to construct approximations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">View Interpolation</head><p>Chen's and Williams' <ref type="bibr" target="#b7">[8]</ref> view interpolation employs incomplete plenoptic samples and image flow fields to reconstruct arbitrary viewpoints with some constraints on gaze angle. The reconstruction process uses information about the local neighborhood of a sample. Chen and Williams point out and suggest a solution for one of the key problems of image-based rendering-determining the visible surfaces. Chen and Williams chose to presort the quadtree compressed flow-field in a back-to-front order according to its (geometric) zvalue. This approach works well when all of the partial sample images share a common gaze direction, and the synthesized viewpoints are restricted to stay within 90 degrees of this gaze angle.</p><p>An image flow field alone allows for many ambiguous visibility solutions, unless we restrict ourselves to flow fields that do not fold, such as rubber-sheet local spline warps or thin-plate global spline warps. This problem must be considered in any general-purpose image-based rendering system, and ideally, it should be done without transporting the image into the geometric-rendering domain.</p><p>Establishing flow fields for a view interpolation system can also be problematic. Chen and Williams used pre-rendered synthetic images to determine flow fields from the z-values. In general, accurate flow field information between two samples can only be established for points that are mutually visible to both samples. This points out a shortcoming in the use of partial samples, because reference images seldom have a 100% overlap.</p><p>Like morphing, view interpolation uses photometric information as well as local derivative information in its reconstruction process. This locally linear approximation is nicely exploited to approximate perspective depth effects, and Chen and Williams show it to be correct for lateral motions relative to the gaze direction. View interpolation, however, adds a nonlinearity by allowing the visibility process to determine the blending function between reference frames in a closest-take-all (a.k.a. winner-take-all) fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Laveau and Faugeras</head><p>Laveau and Faugeras <ref type="bibr" target="#b14">[15]</ref> have taken advantage of the fact that the epipolar geometries between images restrict the image flow field in such a way that it can be parameterized by a single disparity value and a fundamental matrix which represents the epipolar relationship. They also provide a two-dimensional raytracing-like solution to the visibility problem which does not require an underlying geometric description. Their method does, however, require establishing correspondences for each image point along the ray's path. The Laveau and Faugeras system also uses partial plenoptic samples, and results are shown only for overlapping regions between views.</p><p>Laveau and Faugeras also discuss the combination of information from several views but primarily in terms of resolving visibility. By relating the reference views and the desired views by the homogenous transformations between their projections, Laveau and Faugeras can compute exact perspective depth solutions. The recon-struction process again takes advantage of both image data and local derivative information to reconstruct the plenoptic function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Regan and Pose</head><p>Regan and Pose <ref type="bibr" target="#b22">[23]</ref> describe a hybrid system in which plenoptic samples are generated on the fly by a geometry-based rendering system at available rendering rates, while interactive rendering is provided by the image-based subsystem. At any instant, a user interacts with a single plenoptic sample. This allows the user to make unconstrained changes in the gaze angle about the sample point. Regan and Pose also discuss local reconstruction approximations due to changes in the viewing position. These approximations amount to treating the objects in the scene as being placed at infinity, resulting in a loss of the kinetic depth effect. These partial updates can be combined with the approximation values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">PLENOPTIC MODELING</head><p>We claim that all image-based rendering approaches can be cast as attempts to reconstruct the plenoptic function from a sample set of that function. We believe that there are significant insights to be gleaned from this characterization. In this section, we propose our prototype system in light of this plenoptic function framework.</p><p>We call our image-based rendering approach Plenoptic Modeling. Like other image-based rendering systems, the scene description is given by a series of reference images. These reference images are subsequently warped and combined to form representations of the scene from arbitrary viewpoints. The warping function is defined by image flow field information that can either be supplied as an input or derived from the reference images.</p><p>Our discussion of the plenoptic modeling image-based rendering system is broken down into four sections. First, we discuss the representation of the plenoptic samples. Next, we discuss their acquisition. The third section covers the determination of image flow fields, if required. And, finally, we describe how to reconstruct the plenoptic function from these sample images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Plenoptic Sample Representation</head><p>The most natural surface for projecting a complete plenoptic sample is a unit sphere centered about the viewing position. One difficulty of spherical projections, however, is the lack of a representation that is suitable for storage on a computer. This is particularly difficult if a uniform (i.e. equal area) discrete sampling is required. This difficulty is reflected in the various distortions which arise in planar projections of world maps in cartography. Those uniform mappings which do exist are generally ill-suited for systematic access as a data structure. Furthermore, those which do map to a plane with consistent neighborhood relationships are generally quite distorted and, therefore, non-uniform.</p><p>A set of six planar projections in the form of a cube has been suggested by Greene <ref type="bibr" target="#b9">[10]</ref> as an efficient representation for environment maps. While this representation can be easily stored and accessed by a computer, it provides significant problems relating to acquisition, alignment, and registration when used with real, non-computer-generated images. The orthogonal orientation of the cube faces requires precise camera positioning. The wide, 90 degree field-of-view of each face requires expensive lens systems to avoid optical distortion. Also, the planar mapping does not represent a uniform sampling, but instead, is considerably oversampled in the edges and corners. However, the greatest difficulty of a cube-oriented planar projection set is describing the behavior of the image flow fields across the boundaries between faces and at corners. This is not an issue when the six planar projections are used solely as an environment map, but it adds a considerable overhead when it is used for image analysis.</p><p>We have chosen to use a cylindrical projection as the plenoptic sample representation. One advantage of a cylinder is that it can be easily unrolled into a simple planar map. The surface is without boundaries in the azimuth direction, which simplifies correspondence searches required to establish image flow fields. One short-coming of a projection on a finite cylindrical surface is the boundary conditions introduced at the top and bottom. We have chosen not to employ end caps on our projections, which has the problem of limiting the vertical field of view within the environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Acquiring Cylindrical Projections</head><p>A significant advantage of a cylindrical projection is the simplicity of acquisition. The only acquisition equipment required is a video camera and a tripod capable of continuous panning. Ideally, the camera's panning motion would be around the exact optical center of the camera. In practice, in a scene where all objects are relatively far from the tripod's rotational center, a slight misalignment offset can be tolerated.</p><p>Any two planar perspective projections of a scene which share a common viewpoint are related by a two-dimensional homogenous transform:</p><p>(2) where x and y represent the pixel coordinates of an image I, and x' and y' are their corresponding coordinates in a second image I'. This well known result has been reported by several authors <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b21">[22]</ref>. The images resulting from typical camera motions, such as pan, tilt, roll, and zoom, can all be related in this fashion. When creating a cylindrical projection, we will only need to consider panning camera motions. For convenience we define the camera's local coordinate system such that the panning takes place entirely in the xz plane.</p><p>In order to reproject an individual image into a cylindrical projection, we must first determine a model for the camera's projection or, equivalently, the appropriate homogenous transforms. Many different techniques have been developed for inferring the homogenous transformation between images sharing common centers of projection. The most common technique <ref type="bibr" target="#b11">[12]</ref> involves establishing four corresponding points across each image pair. The resulting transforms provide a mapping of pixels from the planar projection of the first image to the planar projection of the second. Several images could be composited in this fashion by first determining the transform which maps the Nth image to image N-1. These transforms can be catenated to form a mapping of each image to the plane of the first. This approach, in effect, avoids direct determination of an entire camera model by performing all mappings between different instances of the same camera. Other techniques for deriving these homogeneous transformations without specific point correspondences have also been described <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b24">[25]</ref>.</p><p>The set of homogenous transforms, H i , can be decomposed into two parts which will allow for arbitrary reprojections in a manner similar to <ref type="bibr" target="#b10">[11]</ref>. These two parts include an intrinsic transform, S, which is determined entirely by camera properties, and an extrinsic transform, R i , which is determined by the rotation around the camera's center of projection:</p><p>(3) This decomposition decouples the projection and rotational components of the homogeneous transform. By an appropriate choice of coordinate systems and by limiting the camera's motion to panning, the extrinsic transform component is constrained to a function of a single parameter rotation matrix describing the pan. </p><formula xml:id="formula_2">= x' u w ---- = y' v w ---- = u H i x S 1 -R i Sx = = R y θ cos 0 θ sin 0 1 0 θ sin - 0 θ cos =</formula><p>Since the intrinsic component's properties are invariant over all of the images, the decomposition problem can be broken into two parts: the determination of the extrinsic rotation component, R i , followed by the determination of an intrinsic projection component, S. The first step in our method determines estimates for the extrinsic panning angle between each image pair of the panning sequence. This is accomplished by using a linear approximation to an infinitesimal rotation by the angle . This linear approximation results from substituting for the cosine terms and for the sine terms of the rotation matrix. This infinitesimal perturbation has been shown by <ref type="bibr" target="#b13">[14]</ref> to reduce to the following approximate equations: <ref type="bibr" target="#b4">(5)</ref> where f is the apparent focal length of the camera measured in pixels, and (C x , C y ) is the pixel coordinate of the intersection of the optical axis with the image plane. (C x , C y ) is initially estimated to be at the center pixel of the image plane. A better estimate for (C x , C y ) is found during the intrinsic matrix solution.</p><p>These equations show that small panning rotations can be approximated by translations for pixels near the image's center. We require that some part of each image in the sequence must be visible in the successive image, and that some part of the final image must be visible in the first image of the sequence. The first stage of the cylindrical registration process attempts to register the image set by computing the optimal translation in x which maximizes the normalized correlation within a region about the center third of the screen. This is first computed at a pixel resolution, then refined on a 0.1 subpixel grid, using a Catmull-Rom interpolation spline to compute subpixel intensities. Once these translations, t i , are computed, Newton's method is used to convert them to estimates of rotation angles and the focal length, using the following equation: <ref type="bibr" target="#b5">(6)</ref> where N is the number of images comprising the sequence. This usually converges in as few as five iterations, depending on the original estimate for f. This first phase determines an estimate for the relative rotational angles between each of the images (our extrinsic parameters) and the initial focal length estimate measured in pixels (one of the intrinsic parameters).</p><p>The second stage of the registration process determines the S, or structural matrix, which describes various camera properties such as the tilt and roll angles which are assumed to remain constant over the group of images. The following model is used: <ref type="bibr" target="#b6">(7)</ref> where P is the projection matrix: <ref type="bibr" target="#b7">(8)</ref> and (C x , C y ) is the estimated center of the viewplane as described previously, σ is a skew parameter representing the deviation of the sampling grid from a rectilinear grid, ρ determines the sampling grid's aspect ratio, and f is the focal length in pixels as determined from the first alignment stage. The remaining terms, Ω x and Ω z , describe the combined effects of camera orientation and deviations of the viewplane's orientation from perpendicular to the optical axis. Ideally, the viewplane would be normal to the optical axis, but manufacturing tolerances allow these numbers to vary slightly <ref type="bibr" target="#b26">[27]</ref>. <ref type="figure">----------------------------------------------</ref></p><formula xml:id="formula_3">θ 1 O θ 2 ( ) + θ O θ 3 ( ) + x' x fθ - θ x C x - ( ) 2 f ---------------------------- - O θ 2 ( ) + = y' y θ x C x - ( ) y C y - ( ) f -</formula><formula xml:id="formula_4">- - O θ 2 ( ) + = 2π t i f ---     atan i 1 = N ∑ - 0 = S Ω x Ω z P = P 1 σ C -x 0 ρ C -y 0 0 f = (9)<label>(10)</label></formula><p>In addition, the ω z term is indistinguishable from the camera's roll angle and, thus, represents both the image sensor's and the camera's rotation. Likewise, ω x , is combined with an implicit parameter, φ, that represents the relative tilt of the camera's optical axis out of the panning plane. If φ is zero, the images are all tangent to a cylinder and for a nonzero φ the projections are tangent to a cone. This gives six unknown parameters, (C x , C y , σ, ρ, ω x , ω z ), to be determined in the second stage of the registration process. Notice that, when combined with the θ i and f parameters determined in the first stage, we have a total of eight parameters for each image, which is consistent with the number of free parameters in a general homogeneous matrix.</p><p>The structural matrix, S, is determined by minimizing the following error function: <ref type="bibr" target="#b10">(11)</ref> where I i-1 and I i represent the center third of the pixels from images i-1 and i respectively. Using Powell's multivariable minimization method <ref type="bibr" target="#b22">[23]</ref> with the following initial values for our six parameters, <ref type="bibr" target="#b11">(12)</ref> the solution typically converges in about six iterations. At this point we will have a new estimate for (C x , C y ) which can be fed back into stage one, and the entire process can be repeated.</p><p>The registration process results in a single camera model, S(C x , C y , σ, ρ, ω x , ω z , f), and a set of the relative rotations, θ i , between each of the sampled images. Using these parameters, we can compose mapping functions from any image in the sequence to any other image as follows: <ref type="bibr" target="#b12">(13)</ref> We can also reproject images onto arbitrary surfaces by modifying S. Since each image pixel determines the equation of a ray from the center-of-projection, the reprojection process merely involves intersecting these rays with the projection manifold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Determining Image Flow Fields</head><p>Given two or more cylindrical projections from different positions within a static scene, we can determine the relative positions of centers-of-projection and establish geometric constraints across all potential reprojections. These positions can only be computed to a scale factor. An intuitive argument for this is that from a set of images alone, one cannot determine if the observer is looking at a model or a full-sized scene. This implies that at least one measurement is required to establish a scale factor. The measurement may be taken either between features that are mutually visible within images, or the distance between the acquired image's camera positions can be used. Both techniques have been used with little difference in results.</p><p>To establish the relative relationships between any pair of cylindrical projections, the user specifies a set of corresponding points that are visible from both views. These points can be treated as rays in space with the following form: <ref type="figure">,</ref><ref type="figure">,</ref><ref type="figure">,</ref><ref type="figure">,</ref><ref type="figure">(</ref> ) <ref type="figure">----------------------------=</ref> C y image height 2 - <ref type="figure">------------------------------</ref></p><formula xml:id="formula_5">Ω x 1 0 0 0 ω x cos ω x sin - 0 ω x sin ω x cos = Ω z ω z cos ω z sin - 0 ω z sin ω z cos 0 0 0 1 = error C x C y σ ρ ω x ω z ,</formula><formula xml:id="formula_6">1 Correlation I i 1 - S 1 -R y i SI i , ( ) - i1 = n ∑ = C x image width 2 -</formula><formula xml:id="formula_7">= σ 0 = ρ 1 = ω x 0 = ω z 0 = I' i S 1 -R y i 1 + R y i 2 + R y i 3 + …R y j SI j = (14)</formula><p>where is the unknown position of the cylinder's center of projection, φ a is the rotational offset which aligns the angular orientation of the cylinders to a common frame, k a is a scale factor which determines the vertical field-of-view, and is the scanline where the center of projection would project onto the scene (i.e. the line of zero elevation, like the equator of a spherical map).</p><p>A pair of tiepoints, one from each image, establishes a pair of rays which ideally intersect at the point in space identified by the tiepoint. In general, however, these rays are skewed. Therefore, we use the point that is simultaneously closest to both rays as an estimate of the point's position, , as determined by the following derivation. <ref type="bibr" target="#b14">(15)</ref> where and are the tiepoint coordinates on cylinders A and B respectively. The two points, and , are given by ( <ref type="formula">16</ref>)</p><p>where <ref type="bibr" target="#b16">(17)</ref> This allows us to pose the problem of finding a cylinder's position as a minimization problem. For each pair of cylinders we have two sets of six unknowns, [(A x ,A y ,A z ,φ a ,k a ,C va ), (B x ,B y ,B z ,φ b ,k b , C vb )]. In general, we have good estimates for the k and C v terms, since these values are found by the registration phase. The position of the cylinders is determined by minimizing the distance between these skewed rays. We also choose to assign a penalty for shrinking the vertical height of the cylinder in order to bring points closer together. This penalty could be eliminated by accepting either the k or C v values given by the registration. We have tested this approach using from 12 to 500 tiepoints, and have found that it converges to a solution in as few as ten iterations of Powell's method. Since no correlation step is required, this process is considerably faster than the minimization step required to determine the structural matrix, S.</p><p>The use of a cylindrical projection introduces significant geometric constraints on where a point viewed in one projection might appear in a second. We can capitalize on these restrictions when we wish to automatically identify corresponding points across cylinders. While an initial set of 100 to 500 tiepoints might be established by hand, this process is far too tedious to establish a mapping for the entire cylinder. Next, we present a geometric constraint for cylindrical projections that determines the possible positions of a point given its position in some other cylinder. This constraint plays the same role that the epipolar geometries <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b8">[9]</ref>, used in the computer vision community for depth-from-stereo computations, play for planar projections.</p><p>First, we will present an intuitive argument for the existence of such an invariant. Consider yourself at the center of a cylindrical projection. Every point on the cylinder around you corresponds to a ray in space as given by the cylindrical epipolar geometry equation. When one of the rays is observed from a second cylinder, its path projects to a curve which appears to begin at the point corresponding to the origin of the first cylinder, and it is constrained to pass through</p><formula xml:id="formula_8">x a θ v , ( ) C a tD a θ v , ( ) + = D a θ v , (</formula><p>) <ref type="figure">------------------------------------------------------------------------------------------------------------------------</ref> <ref type="figure">------------------------------------------------------------------------------------------------------------------------=</ref> the point's image on the second cylinder.</p><formula xml:id="formula_9">φ a θ - ( ) cos φ a θ - ( ) sin k a C v a v -     = C a A x A y A z , , ( ) = C v a p pθ a v a θ b v b , , , ( ) x a x b - 2 ---------------- = θ a v a , ( ) θ b v b , ( ) x a x b x a C a tD a θ a v a , ( ) + = x b C b sD b θ b v b , ( ) + = t Det C a C b - D b θ b v b , ( ) D a θ a v a , ( ) D b θ b v b , ( ) × , , D a θ a v a , ( ) D b θ b v b , ( ) × 2 -</formula><formula xml:id="formula_10">= s Det C b C a - D a θ a v a , ( ) D a θ a v a , ( ) D b θ b v b , ( ) × , , D a θ a v a , ( ) D b θ b v b , ( ) × 2 -</formula><p>This same argument could obviously have been made for a planar projection. And, since two points are identified (the virtual image of the camera in the second projection along with the corresponding point) and, because a planar projection preserve lines, a unique, so called epipolar line is defined. This is the basis for an epipolar geometry, which identifies pairs of lines in two planar projections such that if a point falls upon one line in the first image, it is constrained to fall on the corresponding line in the second image. The existence of this invariant reduces the search for corresponding points from an O(N 2 ) problem to O(N).</p><p>Cylindrical projections, however, do not preserve lines. In general, lines map to quadratic parametric curves on the surface of a cylinder. Surprisingly, we can completely specify the form of the curve with no more information than was needed in the planar case.</p><p>The paths of these curves are uniquely determined sinusoids. This cylindrical epipolar geometry is established by the following equation. <ref type="bibr" target="#b17">(18)</ref> where <ref type="bibr" target="#b18">(19)</ref> This formula gives a concise expression for the curve formed by the projection of a ray across the surface of a cylinder, where the ray is specified by its position on some other cylinder.</p><p>This cylindrical epipolar relationship can be used to establish image flow fields using standard computer vision methods. We have used correlation methods <ref type="bibr" target="#b8">[9]</ref>, a simulated annealing-like relaxation method <ref type="bibr" target="#b2">[3]</ref>, and the method of differences <ref type="bibr" target="#b19">[20]</ref> to compute stereo disparities between cylinder pairs. Each method has its strengths and weaknesses. We refer the reader to the references for further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Plenoptic Function Reconstruction</head><p>Our image-based rendering system takes as input cylindrically projected panoramic reference images along with scalar disparity images relating each cylinder pair. This information is used to automatically generate image warps that map reference images to arbitrary cylindrical or planar views that are capable of describing both occlusion and perspective effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FIGURE 2. Diagram showing the transfer of the known disparity values between cylinders A and B to a new viewing position V.</head><p>We begin with a description of cylindrical-to-cylindrical mappings. Each angular disparity value, α, of the disparity images, can be readily converted into an image flow vector field, using the epipolar relation given by Equation <ref type="formula">18</ref>for each position on the cylinder, (θ, v). We can transfer disparity values from the known cylindrical pair to a new cylindrical projection <ref type="figure">--------------------------------------------------------------------------------C</ref> </p><formula xml:id="formula_11">v θ ( ) N x φ a θ - ( ) cos N y φ a θ - ( ) sin + N z k a -</formula><formula xml:id="formula_12">v a + = N C b C a - ( ) D a θ a v a , ( ) × = A B P V x y α β θ θ + α θ + β θ α + v θ α + ( ) , (</formula><p>)</p><p>in an arbitrary position, as in Figure <ref type="figure">2</ref>, using the following equations.</p><p>By precomputing for each column of the cylindrical reference image and storing in place of the disparity image, this transfer operation can be computed at interactive speeds.</p><p>Typically, once the disparity images have been transferred to their target, the cylindrical projection would be reprojected as a planar image for viewing. This reprojection can be combined with the disparity transfer to give a single image warp that performs both operations. To accomplish this, a new intermediate quantity, δ, called the generalized angular disparity is defined as follows: <ref type="bibr" target="#b20">(21)</ref> This scalar function is the cylindrical equivalent to the classical stereo disparity. Finally, a composite image warp from a given reference image to any arbitrary planar projection can be defined as ( Next, we enumerate each sheet such that the projected image of the desired viewpoint is the last point drawn. This simple partitioning and enumeration provides a back-to-front ordering for use by a painter's style rendering algorithm. This hidden-surface algorithm is a generalization of Anderson's <ref type="bibr" target="#b1">[2]</ref> visible line algorithm to arbitrary projected grid surfaces. Additional details can be found in <ref type="bibr" target="#b20">[21]</ref>.</p><p>At this point, the plenoptic samples can be warped to their new position according to the image flow field. In general, these new pixel positions lie on an irregular grid, thus requiring some sort of reconstruction and resampling. We use a forward-mapping <ref type="bibr" target="#b27">[28]</ref> reconstruction approach in the spirit of <ref type="bibr" target="#b26">[27]</ref> in our prototype. This involves computing the projected kernel's size based on the current disparity value and the derivatives along the epipolar curves.</p><p>While the visibility method properly handles mesh folds, we still must consider the tears (or excessive stretching) produced by the exposure of previously occluded image regions. In view interpolation <ref type="bibr" target="#b7">[8]</ref> a simple "distinguished color" heuristic is used based on the screen space projection of the neighboring pixel on the same scanline. This approach approximates stretching for small regions of occlusion, where the occluder still abuts the occluded region. And, for large exposed occluded regions, it tends to interpolate between the boundaries of the occluded region. These exposure events can be handled more robustly by combining, on a pixel-by-pixel basis, the results of multiple image warps according to the smallest-sized reconstruction kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS</head><p>We collected a series of images using a video camcorder on a leveled tripod in the front yard of one of the author's home. Accurate leveling is not strictly necessary for the method to work. When the data were collected, no attempt was made to pan the camera at a uniform angular velocity. The autofocus and autoiris features of the camera were disabled, in order to maintain a constant focal length during the collection process. The frames were then digitized at a rate of approximately 5 frames per second to a resolution of 320 by 240 pixels. An example of three sequential frames are shown below.</p><p>Immediately after the collection of the first data set, the process was repeated at a second point approximately 60 inches from the first. The two image sequences were then separately registered using the methods described in Section 4.2. The images were reprojected onto the surface of a cylinder with a resolution of 3600 by 300 pixels. The results are shown in Figures <ref type="figure" target="#fig_3">5a</ref> and<ref type="figure" target="#fig_3">5b</ref>. The operating room scene, in Figure <ref type="figure" target="#fig_3">5c</ref>, was also constructed using these same methods.</p><p>Next, the epipolar geometry was computed by specifying 12 tiepoints on the front of the house. Additional tiepoints were gradually added to establish an initial disparity image for use by the simulated annealing and method of differences stereo-correspondence routines. As these tiepoints were added, we also refined the epipolar geometry and cylinder position estimates. The change in cylinder position, however, was very slight. In Figure <ref type="figure" target="#fig_3">5d</ref>, we show a cylindrical image with several epipolar curves superimposed. Notice how the curves all intersect at the alternate camera's virtual image and vanishing point.</p><p>After the disparity images are computed, they can be interactively warped to new viewing positions. The following four images show various reconstructions. When used interactively, the warped images provide a convincing kinetic depth effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>The plenoptic function provides a consistent framework for imagebased rendering systems. The various image-based methods, such as morphing and view interpolation, are characterized by the different ways they implement the three key steps of sampling, reconstructing, and resampling the plenoptic function.</p><p>We have described our approach to each of these steps. Our method for sampling the plenoptic function can be done with equipment that is commonly available, and it results in cylindrical samples about a point. All the necessary parameters are automatically estimated from a sequence of images resulting from panning a video camera through a full circle.</p><p>Reconstructing the function from these samples requires estimating the optic flow of points when the view point is translated. Though this problem can be very difficult, as evidenced by thirty years of computer vision and photogrammetry research, it is greatly simplified when the samples are relatively close together. This is because there is little change in the image between samples (which makes the estimation easier), and because the viewer is never far from a sample (which makes accurate estimation less important).</p><p>Resampling the plenoptic function and reconstructing a planar projection are the key steps for display of images from arbitrary viewpoints. Our methods allow efficient determination of visibility and real-time display of visually rich environments on conventional workstations without special purpose graphics acceleration.</p><p>The plenoptic approach to modeling and display will provide robust and high-fidelity models of environments based entirely on a set of reference projections. The degree of realism will be determined by the resolution of the reference images rather than the number of primitives used in describing the scene. Finally, the difficulty of producing realistic models of real environments will be greatly reduced by replacing geometry with images.  <ref type="figure">--------------------------------------------</ref></p><formula xml:id="formula_15">= if θ 1 cos θ 2 then θ 1 cot θ 2 , thus a b &lt; cot &gt; cos &gt; AS i θ cos S i S j</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>a 12 a 13 a 21 a 22 a 23 a</head><label>1323</label><figDesc>31 a 32 a 33 x y 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 4 .</head><label>4</label><figDesc>FIGURE 4. A back-to-front ordering of the image flow field can be established by projecting the eye's position onto the cylinder's surface and dividing it into four toroidal sheets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIGURE 5 .</head><label>5</label><figDesc>FIGURE 5. Cylindrical images a and b are panoramic views separated by approximately 60 inches. Image c is a panoramic view of an operating room. In image d, several epipolar curves are superimposed onto cylindrical image a.</figDesc><graphic coords="7,54.48,142.88,503.04,54.24" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proceedings of SIGGRAPH 95 (Los Angeles, California,August 6-11, 1995)   </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We are indebted to the following individuals for their contributions and suggestions on this work: Henry Fuchs, Andrei State, Kevin Arthur, Donna McMillan, and all the members of the UNC/UPenn collaborative telepresence-research group. This research is supported in part by Advanced Research Projects Agency contract no. DABT63-93-C-0048, NSF Cooperative Agreement no. ASC-8920219, Advanced Research Projects Agency order no. A410, and National Science Foundation grant no. MIP-9306208. Approved by ARPA for public release -distribution unlimited.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>where <ref type="bibr" target="#b22">(23)</ref> and the vectors and are defined by the desired view as shown in Figure <ref type="figure">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FIGURE 3.</head><p>The center-of-projection, , a vector to the origin, , and two spanning vectors ( and ) uniquely determine the planar projection.</p><p>In the case where , the image warp defined by Equation <ref type="formula">22</ref>, reduces to a simple reprojection of the cylindrical image to a desired planar view. The perturbation introduced by allowing to vary over the image allows arbitrary shape and occlusions to be represented.</p><p>Potentially, both the cylinder transfer and image warping approaches are many-to-one mappings. For this reason we must consider visibility. The following simple algorithm can be used to determine an enumeration of the cylindrical mesh which guarantees a proper back-to-front ordering, (See Appendix). We project the desired viewing position onto the reference cylinder being warped and partition the cylinder into four toroidal sheets. The sheet boundaries are defined by the θ and v coordinates of two points, as shown in Figure <ref type="figure">4</ref>. One point is defined by the intersection of the cylinder <ref type="figure">-------------------------------------------</ref> <ref type="figure">-------------------------------------------</ref> <ref type="figure">--------------------------------------------------------</ref></p><p>with the vector from the origin through the eye's position. The other is the intersection with the vector from the eye through the origin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>We will show how occlusion compatible mappings can be determined on local spherical frames embedded within a global cartesian frame, W. The projected visibility algorithm for cylindrical surfaces given in the paper can be derived by reducing it to this spherical case. First, consider an isolated topological multiplicity on the projective mapping from S i to S j , as shown below Theorem 1: In the generic case, the points of a topological multiplicity induced by a mapping from S i to S j , and the two frame origins are coplanar.</p><p>Proof: The points of the topological multiplicity are colinear with the origin of S j since they share angular coordinates. A second line segment connects the local frame origins, S i and S j . In general, these two lines are distinct and thus they define a plane in three space.</p><p>Thus, a single affine transformation, A, of W can accomplish the following results.</p><p>• Translate S i to the origin • Rotate S j to lie on the x-axis • Rotate the line along the multiplicity into the xy-plane • Scale the system so that S j has the coordinate (1,0,0). With this transformation we can consider the multiplicity entirely within the xy-plane, as shown in the following figure. Thus, an occlusion compatible mapping, can be determined by enumerating the topological mesh defined on in an order of increasing , while allowing later mesh facets to overwrite previous ones. This mapping is occlusion compatible since, by Theorem 2, greater range values will always proceed lesser values at all multiplicities. Notice, that this mapping procedure only considers the changes in the local frame's world coordinates, and makes no use of the range information itself.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Plenoptic Function and the Elements of Early Vision</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Bergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Models of Visual Processing</title>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Landy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Anthony Movshon</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, Mass</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hidden Line Elimination in Projected Grid Surfaces</title>
		<author>
			<persName><forename type="first">D</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="1982-10">October 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Stochastic Approach to Stereo Vision</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Barnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technical Note</title>
		<imprint>
			<biblScope unit="volume">373</biblScope>
			<date type="published" when="1986-04-04">April 4, 1986</date>
		</imprint>
	</monogr>
	<note>SRI International</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Feature-Based Image Metamorphosis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Beier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Neely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (SIGGRAPH&apos;92 Proceedings)</title>
		<imprint>
			<date type="published" when="1992-07">July 1992</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="35" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Texture and Reflection in Computer Generated Images</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Blinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Newell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="542" to="547" />
			<date type="published" when="1976-10">October 1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Epipolar-Plane Image Analysis: An Approach to Determining Structure from Motion</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Marimont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A Subdivision Algorithm for Computer Display of Curved Surfaces</title>
		<author>
			<persName><forename type="first">E</forename><surname>Catmull</surname></persName>
		</author>
		<idno>UTEC-CSc-74-133</idno>
		<imprint>
			<date type="published" when="1974-12">December 1974</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Utah</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">View Interpolation for Image Synthesis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (SIGGRAPH&apos;93 Proceedings)</title>
		<imprint>
			<date type="published" when="1993-07">July 1993</date>
			<biblScope unit="page" from="279" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Three-dimensional Computer Vision: A Geometric Viewpoint</title>
		<author>
			<persName><forename type="first">O</forename><surname>Faugeras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, Massachusetts</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Environment Mapping and Other Applications of World Projections</title>
		<author>
			<persName><forename type="first">N</forename><surname>Greene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<date type="published" when="1986-11">November 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Self-Calibration from Multiple Views with a Rotating Camera</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1994-05">May 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fundamentals of Texture Mapping and Image Warping</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Heckbert</surname></persName>
		</author>
		<idno>No. UCB/CSD 89/516</idno>
	</analytic>
	<monogr>
		<title level="j">Dept. of EECS</title>
		<imprint>
			<date type="published" when="1989-06">June 1989</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>UCB</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Determining Optical Flow</title>
		<author>
			<persName><forename type="first">B</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transformation of Optical Flow by Camera Rotation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kanatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1988-03">March 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3-D Scene Representation as a Collection of Images and Fundamental Matrices</title>
		<author>
			<persName><forename type="first">S</forename><surname>Laveau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Faugeras</surname></persName>
		</author>
		<idno>No. 2205</idno>
	</analytic>
	<monogr>
		<title level="j">INRIA</title>
		<imprint>
			<date type="published" when="1994-02">February, 1994</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Techniques for Calibration of the Scale Factor and Image Center for High Accuracy 3D Machine Vision Metrology</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Robotics and Automation</title>
		<meeting>IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="1987-04-03">March 31 -April 3, 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Movie-Maps: An Application of the Optical Videodisc to Computer Graphics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lippman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH &apos;80 Proceedings</title>
		<imprint>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Computer Algorithm for Reconstructing a Scene from Two Projections</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Longuet-Higgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">293</biblScope>
			<date type="published" when="1981-09">September 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The Reconstruction of a Scene From Two Projections -Configurations That Defeat the 8-Point Algorithm</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Longuet-Higgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First IEEE Conference on Artificial Intelligence Applications</title>
		<meeting>the First IEEE Conference on Artificial Intelligence Applications</meeting>
		<imprint>
			<date type="published" when="1984-12">Dec 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An Iterative Image Registration Technique with an Application to Stereo Vision</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Joint Conference on Artificial Intelligence</title>
		<meeting>the Seventh International Joint Conference on Artificial Intelligence<address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A List-Priority Rendering Algorithm for Redisplaying Projected Surfaces</title>
		<author>
			<persName><forename type="first">Leonard</forename><surname>Mcmillan</surname></persName>
		</author>
		<idno>TR95-005</idno>
	</analytic>
	<monogr>
		<title level="j">Department of Computer Science</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>UNC</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Virtual Bellows: Constructing High Quality Stills from Video</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First IEEE International Conference on Image Processing</title>
		<meeting>the First IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="1994-11">November 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Flannery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Teukolsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Vetterling</surname></persName>
		</author>
		<title level="m">Numerical Recipes in C</title>
		<meeting><address><addrLine>Cambridge, Massachusetts</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="309" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Priority Rendering with a Virtual Reality Address Recalculation Pipeline</title>
		<author>
			<persName><forename type="first">M</forename><surname>Regan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH&apos;94 Proceedings</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image Mosaicing for Tele-Reality Applications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CRL</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1994-05">May 1994</date>
		</imprint>
		<respStmt>
			<orgName>DEC and Cambridge Research Lab</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Shape and Motion from Image Streams: a Factorization Method; Full Report on the Orthographic Case</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<idno>CMU-CS-92-104</idno>
		<imprint>
			<date type="published" when="1992-03">March 1992</date>
			<publisher>Carnegie Mellon University</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Versatile Camera Calibration Technique for High-Accuracy 3D Machine Vision Metrology Using Off-the-Shelf TV Cameras and Lenses</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1987-08">August 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Footprint Evaluation for Volume Rendering</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Westover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH&apos;90 Proceedings</title>
		<imprint>
			<date type="published" when="1990-08">August 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Digital Image Warping</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wolberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>IEEE Computer Society Press</publisher>
			<pubPlace>Los Alamitos, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
