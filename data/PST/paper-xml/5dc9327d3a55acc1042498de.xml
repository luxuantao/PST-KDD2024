<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2020 BLOCKWISE SELF-ATTENTION FOR LONG DOCUMENT UNDERSTANDING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Under review as a conference paper at ICLR 2020 BLOCKWISE SELF-ATTENTION FOR LONG DOCUMENT UNDERSTANDING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present BlockBERT, a lightweight and efficient BERT model that is designed to better modeling long-distance dependencies. Our model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training time, which also enables attention heads to capture either short-or long-range contextual information. We conduct experiments on several benchmark question answering datasets with various paragraph lengths. Results show that BlockBERT uses 18.7-36.1% less memory and reduces the training time by 12.0-25.1%, while having comparable and sometimes better prediction accuracy, compared to an advanced BERT-based model, RoBERTa.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent emergence of the pre-training and fine-tuning paradigm, exemplified by methods like ELMo <ref type="bibr" target="#b25">(Peters et al., 2018)</ref>, GPT-2 <ref type="bibr" target="#b26">(Radford et al., 2019)</ref>, BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, <ref type="bibr">XLNet (Yang et al., 2019)</ref> and RoBERTa <ref type="bibr" target="#b19">(Liu et al., 2019)</ref>, has drastically reshaped the landscape of the natural language processing research. These methods first pre-train a deep model with language model objectives using a large corpus and then fine-tune the model using in-domain supervised data for target applications. Despite its conceptual simplicity, this paradigm has reestablished the new state-ofthe-art baselines across various tasks, such as question answering <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, coreference resolution <ref type="bibr" target="#b14">(Joshi et al., 2019b)</ref>, relation extraction <ref type="bibr" target="#b29">(Soares et al., 2019</ref>) and text retrieval <ref type="bibr" target="#b17">(Lee et al., 2019;</ref><ref type="bibr" target="#b23">Nogueira &amp; Cho, 2019)</ref>, to name a few.</p><p>Building such models in practice, however, is an extremely resource-intensive process. For instance, the training of BERT-family models is notoriously expensive. <ref type="bibr" target="#b4">Devlin et al. (2019)</ref> report that it takes four days for pre-training BERT-Base/BERT-Large on 4/16 Cloud TPUs, respectively. In order to reduce the pre-training time of RoBERTa to 1 day, <ref type="bibr" target="#b19">Liu et al. (2019)</ref> use 1,024 V100 GPUs. One crucial factor that contributes to the long training time is the memory consumption of these deep models, as it directly affects the batch size. Although the fine-tuning stage is relatively inexpensive, the memory issue still restricts the scenarios in which BERT can be used. For instance, "it is currently not possible to re-produce most of the BERT-Large results on the paper using a GPU with 12GB-16GB of RAM, because the maximum batch size that can fit in memory is too small.<ref type="foot" target="#foot_0">1</ref> " Although one may think that model size is the main contributor to the large memory consumption, our analysis (Section 2.1) shows that one of the main bottlenecks is actually dot-product selfattention, operated in multiple layers of <ref type="bibr">Transformers (Vaswani et al., 2017)</ref>, the building block of BERT. As the attention operation is quadratic to the sequence length, this fundamentally limits the maximum length of the input sequence, and thus restricts the model capacity in terms of capturing long-distance dependencies. As a result, downstream tasks have to either truncate their sequences to leading tokens <ref type="bibr" target="#b23">(Nogueira &amp; Cho, 2019)</ref> or split their sequences with a sliding window <ref type="bibr" target="#b13">(Joshi et al., 2019a;</ref><ref type="bibr">b)</ref>. Ad-hoc handling of long sequences is also required in the pre-training stage, such as updating the model using only short sequences in the early stage <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>.</p><p>Common strategies for reducing memory consumption, unfortunately, do not work. For instance, shrinking the model by lowering the number of layers L, attention heads A, or hidden units H leads to significant performance degradation <ref type="bibr">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b4">Devlin et al., 2019)</ref> and does not address the long sequence issue. Alternatively, general low-memory training techniques, such as microbatching <ref type="bibr" target="#b11">(Huang et al., 2018)</ref> and gradient checkpointing <ref type="bibr" target="#b0">(Chen et al., 2016)</ref> essentially trade off training time for memory consumption, prolongs the already lengthy training process.</p><p>In this work, we explore a different strategy, sparsifying the attention layers, intending to design a lightweight and effective BERT that can model long sequences in a memory-efficient way. Our BlockBERT extends BERT by introducing sparse block substructures into the attention matrix to reduce both memory consumption and the number of floating point operations (FLOPs), which also enables attention heads to capture either short-or long-range contextual information. Compared to the previous method that also enforces sparsity (e.g., <ref type="bibr" target="#b1">Child et al. (2019)</ref>), our approach is much simpler mathematically and very easy to implement. More importantly, the results of experiments conducted on several benchmark question answering datasets with various paragraph lengths show that BlockBERT performs comparably or even better than the original BERT-family models, while enjoying an 18.7-36.1% reduction in memory usage and 12.0-25.1% reduction in training time.</p><p>The rest of the paper is organized as follows. Section 2 gives a brief introduction of the BERT model, along with an in-depth analysis of its memory usage during training time. We describe our proposed model in Section 3 and contrast it with existing methods that aim for creating a lighter model. Section 4 presents the experimental results and ablation studies, followed by a short survey of other related work in Section 5 and the conclusion in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND: MEMORY BOTTLENECK IN TRAINING BERT</head><p>We briefly review BERT and introduce its memory profiling in this section. Following the paradigm of language model pre-training and down-stream task fine-tuning, BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> consists of multiple layers of bidirectional Transformers <ref type="bibr">(Vaswani et al., 2017)</ref>, where each Transformer encoder has a multi-head self-attention layer and a position-wise feed-forward layer. Using the same notation as in <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, we denote the number of Transformer layers by L, the number of hidden units by H, the number of attention heads by A, the sequence length by N and the batch size by B. We also assume the feed-forward hidden unit size to be 4H.<ref type="foot" target="#foot_1">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">MEMORY PROFILING</head><p>Training BERT is a memory-intensive process. In order to identify the bottleneck, we follow the memory model proposed by <ref type="bibr" target="#b30">Sohoni et al. (2019)</ref>, where the memory usage throughout neural network training is categorized into three main types: (1) Model Memory is used to store model parameters; (2) Optimizer Memory is the additional memory used by the specific learning algorithm during the process; (3) Activation Memory consists of the outputs of each layer, which are cached for reuse in backpropagation to compute gradients.</p><p>Take BERT-Base training as an example. The model has 110M parameters, so the model memory uses 0.2 GB if stored in half-precision floating-point format (FP16). For Adam <ref type="bibr" target="#b15">(Kingma &amp; Ba, 2014)</ref>, the optimizer needs additional memory to store the gradients, first moments, and second moments of model parameters. If stored using the same precision, the optimizer memory should be three times of model memory. <ref type="foot" target="#foot_2">3</ref> To calculate the exact size of activation memory is not trivial because it depends heavily on the implementation of the toolkit. Instead, we measure it empirically by training BERT-Base using Adam with a memory profiler (more details are provided in Appendix A.2).</p><p>We use 32 NVIDIA V100 GPUs for training. Each single GPU thus consumes a mini-batch of size b = B/32 = 8. Figure <ref type="figure" target="#fig_1">1a</ref> shows the profiling result for a single GPU, where the model/optimizer/activation memory consumes 0.21/1.03/8.49 GB, resp. We can see that activation memory accounts for the vast majority of the total GPU memory (87.6%) and is clearly the bottleneck. Notice that although our analysis is done on BERT-Base, it can be easily generalized to BERT-Large and other models such as RoBERTa <ref type="bibr" target="#b19">(Liu et al., 2019)</ref> and <ref type="bibr">XLNet (Yang et al., 2019)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">GENERAL TECHNIQUES FOR REDUCING MEMORY USAGE IN MODEL TRAINING</head><p>Observing that activation memory is the bottleneck, we discuss the effectiveness of common memory reduction techniques for BERT training below.</p><p>Low Precision <ref type="bibr" target="#b21">(Micikevicius et al., 2017)</ref>: Low precision is to use half-precision or mixed-precision for training neural networks. This technique has been widely used in Transformer training <ref type="bibr" target="#b24">(Ott et al., 2019;</ref><ref type="bibr" target="#b19">Liu et al., 2019)</ref>. In this work, we already assume to use mixed-precision training by default, as indicated in the aforementioned analysis.</p><p>Microbatching <ref type="bibr" target="#b11">(Huang et al., 2018)</ref>: Microbatching is to split a batch into small microbatches (which can be fit into memory), and then run forward and backward passes on them separately with gradients for each micro-batch accumulated. Because it runs forward/backward pass multiple times for a single batch, it trades off time for memory.</p><p>Gradient Checkpointing <ref type="bibr" target="#b0">(Chen et al., 2016)</ref>: Gradient checkpointing saves memory by only caching activations of a subset of layers. The un-cached activations will be recomputed during backpropagation from the latest checkpoint. This strategy trades off time for memory by repeating computations that require large memory and will obviously extend the model training time.</p><p>Knowledge Distillation <ref type="bibr" target="#b9">(Hinton et al., 2015)</ref>: Knowledge distillation aims to compress and transfer knowledge from a teacher model to a simpler student model. However, knowledge distillation relies on a teacher model (which is still expensive in training time) and usually suffers from a certain degree of performance degradation.</p><p>As common techniques are limited in reducing both the training time and memory usage, we investigate how to optimize the dot-product attention layers and introduce our approach next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MODEL: BLOCKBERT</head><p>Following <ref type="bibr">(Vaswani et al., 2017)</ref>, the dot-product attention in Transformer is defined as:</p><formula xml:id="formula_0">Attention(Q, K, V ) = softmax QK √ d V .</formula><p>where Q, K, V ∈ R N ×d with N to be the sequence length and d to be a hidden dimension. As we can see, the inner product between Q and K consumes O(N 2 ) memory. One simple way to reduce memory consumption of attention is to sparsify the attention matrix. Suppose we have a masking matrix M ∈ {0, 1} N ×N . We define a masked version of attention as follows:</p><formula xml:id="formula_1">Attention(Q, K, V , M ) = softmax QK √ d M V ,<label>(1)</label></formula><p>with operator defined by</p><formula xml:id="formula_2">(A M )ij = Aij if Mij = 1 −∞ if Mij = 0 .</formula><p>In this work, we design M to be a sparse block matrix, which not only reduces memory and the number of floating point operations (FLOPs) but also benefits from efficient dense matrix support from deep learning frameworks, such as PyTorch and Tensorflow. More formally, we split the length-N input sequence into n partitions, with each partition of length N n . <ref type="foot" target="#foot_3">4</ref> The N × N attention matrix is then partitioned into n × n blocks, where each block matrix is of size N n × N n . A sparse block matrix M can be defined by a permutation π of {1, 2, • • • , n}:</p><formula xml:id="formula_3">Mij = 1 if π (i−1)n N + 1 = (j−1)n N + 1 0 otherwise.<label>(2)</label></formula><p>By writing Q, K, V as be block matrices, such that</p><formula xml:id="formula_4">Q = [Q 1 • • • Q n ] , K = [K 1 • • • K n ] and V = [V 1 • • • V n ]</formula><p>and pluging them into Equation 1, we can formally define Blockwise Attention as follows:</p><formula xml:id="formula_5">Blockwise-Attention(Q, K, V , M ) =        softmax Q 1 K π(1) √ d V π(1)</formula><p>. . .</p><formula xml:id="formula_6">softmax QnK π(n) √ d V π(n)        . (<label>3</label></formula><formula xml:id="formula_7">)</formula><p>As a result, it only needs to compute and store</p><formula xml:id="formula_8">Q i K π(i) (i = 1, • • • n), each of which has size N n × N n .</formula><p>In other words, BlockBERT reduces the corresponding O(N 2 ) memory consumption and FLOPs by a factor of n, since N n × N n × n = N ×N n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">BLOCKWISE MULTI-HEAD ATTENTION</head><p>Analogous to Multi-head Attention <ref type="bibr">(Vaswani et al., 2017)</ref>, we allow queries, keys, and values to be projected multiple times and perform blockwise attentions in parallel. Moreover, different blockwise attention heads can use different masking matrices. The outputs of multiple heads are then concatenated and aggregated with another linear projection. Let A be the number of attention heads and H the number of hidden units. Blockwise multi-head attention is formally defined by:</p><formula xml:id="formula_9">Blockwise-Multi-head-Attention(Q, K, V ) = Concat(head1, • • • headA)W O where headi = Blockwise-Attention(QW Q i , KW K i , V W V i , Mi), with d = H A , W Q i , W K i , W V i ∈ R</formula><p>H×d and the projection matrix W O ∈ R H×H . Each masking matrix M i is determined by permutation π i according to Equation 2. In particular, we choose π from permutations generated by shifting one position: σ = (2, 3, • • • , n, 1), i.e., we select π ∈ {σ, σ 2 , • • • , σ n }. For example, with 12 attention heads (A = 12) and 2 blocks (n = 2), one configuration can be assigning 10 heads to permutation (1, 2) and the other 2 heads to permutation (2, 1). Figure <ref type="figure">2</ref> illustrates the blockwise multi-head attention with the block numbers n ∈ {2, 3}.</p><p>Blockwise sparsity captures both local and long-distance dependencies in a memory-efficiency way, which is crucial for long-document understanding tasks. For instance, the identity permutation, i.e., (1, 2, • • • , n), enables each token to attend its nearby tokens in self-attention. Tokens within the same local group attend a long-distance group of tokens together in other permutations. Our proposed BlockBERT essentially replaces the multi-head attention layers in Transformer/BERT with blockwise multi-head attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ANALYSIS OF MEMORY USAGE REDUCTION</head><p>To validate our claim that BlockBERT with n×n blocks can reduce the O(N 2 ) memory use by a factor of n, we perform the same memory profiling as described in sections 2.1 and 2.2. Again, We fix the number of tokens in each GPU (b×N = 4096) and choose N from {128, 256, 512, 1024, 2048}. <ref type="foot" target="#foot_4">5</ref>As we can see from Figure <ref type="figure">3</ref> and Table <ref type="table">1</ref>, the empirical results align well with the theoretical values.</p><p>When we set block size to be 2 and 3 for BlockBERT, their estimated O(N 2 ) activation memory decreases to 1/2 and 1/3 of BERT's O(N 2 ) activation memory, resp. As shown in Table <ref type="table" target="#tab_1">2</ref>, for the sequence length N = 512, BlockBERT with 2 / 3 blocks saves 18.7% / 23.8% overall memory, resp. The saving is more significant for longer sequences. When N = 1024, the overall memory reduction of BlockBERT with 2 / 3 blocks is 27.3% / 36.1%, resp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We evaluate the pre-training and fine-tuning performance of BlockBERT. In particular, when n = 2, we denote 10:2 to be the configuration which distributes 10 heads to permutation (1, 2) and 2 to permutation (2, 1); when n = 3, we denote 8:2:2 to be the configuration which assigns 8, 2, 2 heads to permutation (1, 2, 3), (2, 3, 1), and (3, 1, 2), resp. We compare BlockBERT with the following baselines:</p><p>Google BERT The pre-trained base model from <ref type="bibr" target="#b4">Devlin et al. (2019)</ref>.</p><p>RoBERTa-2seq and RoBERTa-1seq We compare with two versions of RoBERTa <ref type="bibr" target="#b19">(Liu et al., 2019)</ref>. RoBERTa-2seq is trained with both masked language model (MLM) task and next sentence prediction (NSP) task, while RoBERTa-1seq refers to the pre-training model with only MLM task.</p><p>SparseBERT We pre-train BERT models with its Transformer encoder replaced by a Sparse Transformer encoder <ref type="bibr" target="#b1">(Child et al., 2019)</ref>. We set its sparsity hyper-parameters stride = 128 and expressivity c = 32. The attention masks used for Sparse Transformer encoder are illustrated in Figure <ref type="figure">5</ref>.  <ref type="table" target="#tab_1">2</ref>.</p><p>Besides memory saving, we also achieve a significant speedup. For example, when N = 1024, BlockBERT (n = 2) reduces the training time from RoBERTa's 9.7 days to 7.5 days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">FINE-TUNING TASKS</head><p>We evaluate BlockBERT on several question answering tasks, including SQuAD 1.1/2.0 <ref type="bibr" target="#b27">(Rajpurkar et al., 2018)</ref> and five other tasks from the MrQA shared task<ref type="foot" target="#foot_5">6</ref> -HotpotQA <ref type="bibr">(Yang et al., 2018</ref><ref type="bibr">), NewsQA (Trischler et al., 2017)</ref>, SearchQA <ref type="bibr" target="#b5">(Dunn et al., 2017)</ref>, TriviaQA <ref type="bibr" target="#b12">(Joshi et al., 2017)</ref> and NaturalQA <ref type="bibr" target="#b16">(Kwiatkowski et al., 2019)</ref> These QA datasets have different paragraph length distribution patterns and are thus ideal for testing the effectiveness of BlockBERT. For example, SQuAD, NaturalQA, and HotpotQA consist of mostly short paragraphs (shorter than 512), while paragraphs in SearchQA (average length 1,004) and TriviaQA (average length 934) have around 1,000 tokens. This means that for SearchQA and TriviaQA, a BERT model with sequence length N = 512 can only capture half of the context. The detailed paragraph length distributions can be found in Figure <ref type="figure">6</ref>.</p><p>For all the pre-trained models, we adopt the same fine-tuning QA setup from <ref type="bibr" target="#b4">Devlin et al. (2019)</ref>.</p><p>The tokenized paragraph (p 1 , • • • , p s ) and question (q 1 , • • • , q t ) are concatenated to be a sequence</p><formula xml:id="formula_10">[CLS]q 1 • • • q t [SEP]p 1 • • • p s [SEP].</formula><p>The sequence is then fed into the pre-trained model with two extra linear layers for predicting the start and end positions of the answer spans. The detailed fine-tuning setting is listed in Appendix A.4. BlockBERT (n=2) v.s. RoBERTa-1seq Comparing BlockBERT (n = 2) with RoBERTa-1seq on pre-trained model with N = 512, we observe an absolute F1 difference from 0.04 (in NaturalQA) to 1.18 (in NewsQA), with average difference to be 0.55. For N = 1024, BlockBERT achieves more comparable or even better performance (in SearchQA, NewsQA, and HotpotQA) to RoBERTa-1seq. The average difference on F1 reduces to 0.27. BlockBERT v.s. SparseBERT For N = 512, it is interesting that BlockBERT with 3 blocks (density 33.33%) performs better then SparseBERT (density 44.20%) in both SQuAD and MrQA tasks. Similar patterns can be observed for N = 1024. These results show that off-diagonal masking matrices, e.g., the masking matrix defined by permutation (2, 1), play crucial roles in BlockBERT. The heterogeneity of pre-training and fine-tuning sequence length may hurt performance. For example, in SQuAD, we do not see significant performance gain by using pre-trained models with N = 1024; in HotpotQA and NewsQA, longer sequence pre-training even hurts performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Long</head><p>Effect of #Blocks It is not surprising that BlockBERT with 2 blocks (n = 2) performs better than that with 3 blocks (n = 3), because it keeps more attention matrix entries. The biggest difference is in SQuAD 2.0 and NewsQA with N = 1024, where we observe an absolute loss of 1.6 F1 by increasing block number from 2 to 3. In summary, not only BlockBERT saves training time and memory, but it also has competitive and sometimes better performance, especially for tasks with longer sequences. This demonstrates the effectiveness of our blockwise multi-head attention approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ABLATION STUDY</head><p>We fix the assignment of attention heads in above experiments. For example, BlockBERT with sequence length N = 512 and 2 blocks is trained with ten heads using permutation (1, 2) and the other two using permutation (2, 1). However, we know that there are other ways to partition twelve attention heads, e.g., seven heads for permutation (1, 2) and the other five for permutation (2, 1). It would be interesting to see how the assignment of heads affects model performance. In this section, we grid search attention head assignments and plot their best validation performance in 1.2M training steps. The results are shown in Figure <ref type="figure" target="#fig_3">4</ref>.</p><p>Our observations are threefold: (1) Identity permutations, i.e., (1, 2) and (1, 2, 3), are important. As shown in Figure <ref type="figure" target="#fig_3">4</ref>, all optimal solutions assign considerable attention heads to block diagonal matrices, since those matrices enable each token to attend to its nearby tokens; (2) Non-identity permutations follow the rule of "vital few and trivial many." Although identity permutations are important, assigning all attention heads to them (corresponding to 12:0 and 12:0:0 in Figure <ref type="figure" target="#fig_3">4</ref>) significantly hurts performance, since the model can not learn long-term dependencies with only identity permutation; (3) Pre-training performance and fine-tuning performance are correlated but not always consistent. When n = 3, pre-training performance suggests 10:1:1 to be the best head assignment -ten heads for permutation (1, 2, 3), one head for (2, 3, 1) and one head for (3, 1, 2), but we observe that the configuration of 8:2:2 achieves better performance in fine-tuning tasks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>In this section, we review the related work of memory optimization for neural network training and recent efforts to simplify Transformer and BERT. In recent years, there is an increasing interest in training neural networks with low-memory <ref type="bibr" target="#b30">(Sohoni et al., 2019)</ref>. Mainstream techniques include low-precision training <ref type="bibr" target="#b21">(Micikevicius et al., 2017)</ref>, microbatching <ref type="bibr" target="#b11">(Huang et al., 2018)</ref>, gradient checkpointing <ref type="bibr" target="#b0">(Chen et al., 2016)</ref>. Another line of researches studies this problem from a theoretical perspective, including the recently proposed lottery ticket hypothesis <ref type="bibr" target="#b6">(Frankle &amp; Carbin, 2018)</ref>. Since the invention of Transformer <ref type="bibr">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b3">Dai et al., 2019)</ref> and its successful application on language model pre-training <ref type="bibr" target="#b4">(Devlin et al., 2019;</ref><ref type="bibr" target="#b26">Radford et al., 2019;</ref><ref type="bibr" target="#b3">Yang et al., 2019;</ref><ref type="bibr" target="#b19">Liu et al., 2019)</ref>, there have been several studies attempted to simplify it from different perspectives. The first line of research focuses on attention matrix sparsification, such as Star Transformer <ref type="bibr" target="#b8">(Guo et al., 2019)</ref>, Sparse Transformer <ref type="bibr" target="#b1">(Child et al., 2019</ref><ref type="bibr">), Adaptive Sparse Transformer (Correia et al., 2019;</ref><ref type="bibr" target="#b31">Sukhbaatar et al., 2019)</ref>, Log-Sparse Transformer <ref type="bibr" target="#b18">(Li et al., 2019)</ref>, etc. However, due to limited support for sparse tensor from current deep learning platforms, most of studies have to represent a sparse matrix using a dense matrix with a binary mask or rely on customized CUDA kernels <ref type="bibr" target="#b7">(Gray et al., 2017)</ref>. The second line of research attempts to prune redundant heads in Transformer, such as Voita et al. ( <ref type="formula">2019</ref>) and <ref type="bibr" target="#b20">Michel et al. (2019)</ref>. The third line of research focuses on knowledge distillation, including DistilBERT<ref type="foot" target="#foot_6">7</ref> which distills BERT using a smaller <ref type="bibr">BERT and Tang et al. (2019)</ref> which distills BERT with BiLSTM <ref type="bibr" target="#b10">(Hochreiter &amp; Schmidhuber, 1997)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work, we study lightweight BERT model with the goal of achieving both efficiency and effectiveness. We profile and analyze the memory bottlenecks of BERT, and focus on optimize dotproduct self-attention, which consumes quadratic memory with respect to the sequence length. To reduce both training time and memory consumption, we present BlockBERT, which sparsifies the attention matrices to be sparse block matrices. The proposed model achieves time and memory saving without significant loss of performance. In the future, we would like to explore more applications of BlockBERT on NLP tasks involving long sequences such as coreference resolution <ref type="bibr" target="#b14">(Joshi et al., 2019b</ref>) and document-level machine translation <ref type="bibr" target="#b22">(Miculicich et al., 2018)</ref>, and also non-NLP tasks such as protein sequence modeling <ref type="bibr" target="#b28">(Rives et al., 2019)</ref>. </p><formula xml:id="formula_11">B</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Memory Profiling for BERT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>4. 1</head><label>1</label><figDesc>PRE-TRAINING All the models follow the base setting, i.e., L = 12, H = 768, A = 12 and are trained on the same corpus -BooksCorpus and English Wikipedia with uncased word piece tokens. We fix the number of tokens per batch B × N = 131, 072, i.e., if sequence length N = 512 then batch size B = 256, if sequence length N = 1024 then batch size B = 128. The detailed pre-training configuration is listed in Table 6 in Appendix A.1. Moreover, the pre-training of SparseBERT and BlockBERT follows the RoBERTa-1seq setting, i.e., we drop the NSP (Next Sentence Prediction) task, and an input sequence is up to N tokens until it reaches a document boundary. A summary of the pretraining performance comparison between BlockBERT and RoBERTa-1seq is shown in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Sequence Pre-training Our observations are twofold. (1) Long sequence pretraining benefits long sequence fine-tuning. In TriviaQA and SearchQA, of which paragraph lengths are around 1024, pre-training models with N = 1024 achieve significantly better performance. (2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Ablation over blockwise attention heads assignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Pre-training Performance Analysis.</figDesc><table><row><cell>N</cell><cell>Model</cell><cell cols="4">Training Time (day) Memory (per GPU, GB) Heads Config. Valid. ppl</cell></row><row><cell></cell><cell>RoBERTa-1seq</cell><cell>6.62</cell><cell>9.73</cell><cell>-</cell><cell>3.58</cell></row><row><cell>512</cell><cell cols="2">BlockBERT n=2 5.83 (-12.0%)</cell><cell>7.91 (-18.7%)</cell><cell>10:2</cell><cell>3.56</cell></row><row><cell></cell><cell cols="2">BlockBERT n=3 5.80 (-12.5%)</cell><cell>7.32 (-23.8%)</cell><cell>8:2:2</cell><cell>3.71</cell></row><row><cell></cell><cell>RoBERTa-1seq</cell><cell>9.66</cell><cell>13.39</cell><cell>-</cell><cell>3.60</cell></row><row><cell>1024</cell><cell cols="2">BlockBERT n=2 7.51 (-22.3%)</cell><cell>9.73 (-27.3%)</cell><cell>9:3</cell><cell>3.57</cell></row><row><cell></cell><cell cols="2">BlockBERT n=3 7.23 (-25.1%)</cell><cell>8.55 (-36.1%)</cell><cell>8:2:2</cell><cell>3.63</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>. Since MrQA does not have an official test set, we follow Joshi et al. (2019a) who split the development set evenly to build a new development set and test set.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Table 3 and Table 4 report the experimental results. Dev set results on SQuAD 1.1/2.0. The result of XLNet(-Base) is from (Yang et al., 2019).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">SQuAD 1.1</cell><cell cols="2">SQuAD 2.0</cell></row><row><cell>N</cell><cell>Model</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell></row><row><cell>-</cell><cell>Human Perf.</cell><cell cols="4">82.30 91.20 86.80 89.40</cell></row><row><cell></cell><cell>Google BERT</cell><cell cols="4">81.19 88.45 74.08 77.16</cell></row><row><cell></cell><cell>XLNet</cell><cell>-</cell><cell>-</cell><cell cols="2">78.46 81.33</cell></row><row><cell></cell><cell>RoBERTa-2seq</cell><cell cols="4">82.91 89.78 75.79 79.17</cell></row><row><cell>512</cell><cell>RoBERTa-1seq</cell><cell cols="4">84.43 91.48 79.22 82.27</cell></row><row><cell></cell><cell>SparseBERT</cell><cell cols="4">80.49 88.09 74.15 76.96</cell></row><row><cell></cell><cell>BlockBERT n=2, 10:2</cell><cell cols="4">84.08 90.77 78.34 81.46</cell></row><row><cell></cell><cell cols="5">BlockBERT n=3, 8:2:2 82.37 89.64 77.33 80.33</cell></row><row><cell></cell><cell>RoBERTa-1seq</cell><cell cols="4">84.58 91.14 79.34 82.26</cell></row><row><cell>1024</cell><cell>SparseBERT BlockBERT n=2, 9:3</cell><cell cols="4">81.02 88.37 74.51 77.57 83.65 90.74 78.55 81.45</cell></row><row><cell></cell><cell cols="5">BlockBERT n=3, 8:2:2 82.74 90.05 76.79 79.84</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>MrQA test results (Tasks are sorted decreasingly by average paragraph length).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">SearchQA</cell><cell cols="2">TriviaQA</cell><cell cols="2">NewsQA</cell><cell cols="2">NaturalQA</cell><cell cols="2">HotpotQA</cell></row><row><cell>N</cell><cell>Model</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell></row><row><cell></cell><cell>Google BERT</cell><cell cols="10">74.94 80.37 70.18 75.35 51.27 66.25 66.13 78.29 60.50 77.08</cell></row><row><cell></cell><cell>RoBERTa-2seq</cell><cell cols="10">76.12 81.74 71.92 76.79 52.45 66.73 66.98 78.63 61.52 77.81</cell></row><row><cell>512</cell><cell>RoBERTa-1seq SparseBERT</cell><cell cols="10">77.09 82.62 73.65 78.22 56.13 70.64 67.14 79.07 62.77 79.28 73.36 79.01 68.71 73.15 51.18 65.47 65.53 77.46 58.54 74.85</cell></row><row><cell></cell><cell cols="11">BlockBERT n=2, 10:2 76.68 82.33 72.36 77.53 54.66 69.46 66.94 79.03 62.13 79.15</cell></row><row><cell></cell><cell cols="11">BlockBERT n=3, 8:2:2 75.54 81.07 72.05 76.74 53.82 68.39 66.14 78.47 60.64 77.46</cell></row><row><cell></cell><cell>RoBERTa-1seq</cell><cell cols="10">77.47 83.12 75.29 80.20 55.00 69.64 68.28 80.35 61.89 78.71</cell></row><row><cell>1024</cell><cell>SparseBERT BlockBERT n=2, 9:3</cell><cell cols="10">74.83 80.54 70.56 75.34 51.67 67.16 65.07 77.31 59.65 76.02 77.95 83.51 75.06 79.41 55.44 70.08 67.31 79.39 62.13 78.94</cell></row><row><cell></cell><cell cols="11">BlockBERT n=3, 8:2:2 76.98 82.76 74.78 79.28 53.48 68.50 65.91 78.20 61.89 78.18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Test time statistics (sec) for different input size (Batch size × Sequence length). OOM indicates out-of-memory.</figDesc><table><row><cell>× N</cell><cell cols="4">8×1024 16×1024 24×1024 32×1024</cell></row><row><cell>RoBERTa</cell><cell>0.1371</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell></row><row><cell cols="2">BlockBERT n=2 0.0990</cell><cell>0.1869</cell><cell>OOM</cell><cell>OOM</cell></row><row><cell cols="2">BlockBERT n=3 0.0954</cell><cell>0.1790</cell><cell>0.2634</cell><cell>OOM</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/google-research/bert</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">The default parameter settings for BERT-Base and BERT-Large can be found in Table5in Appendix A.1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">In the current PyTorch Adam implementation, the first and second moments are stored in single precision. Consequently, BERT's optimizer memory (1 GB) is five times of model memory (0.2 GB).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">We assume N can be divided by n. If not, we pad the input sequence to make N divisible.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">We use GPUs of 16 GB memory for profiling. BERT with N = 2048 fails due to an out-of-memory error.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">https://mrqa.github.io</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">https://github.com/huggingface/pytorch-transformers/tree/master/examples/distillation</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7">https://github.com/pytorch/fairseq/blob/master/fairseq/modules/sparse multihead attention.py.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8">https://github.com/huggingface/pytorch-transformers</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9">https://github.com/huggingface/transformers/blob/master/examples/benchmarks.py</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. Newsqa: A machine comprehension dataset. In Proceedings of the 2nd Workshop on Representation Learning for <ref type="bibr">NLP, pp. 191-200, 2017. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob</ref>   as an estimate to activation memory. When profiling BERT, we first pre-train it for 1000 steps, and then compute its model and optimizer memory. Finally, we esitmate its activation memory according to Equation <ref type="formula">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 SPARSEBERT</head><p>The sparse masking matrices we use for Sparse Transformer <ref type="bibr" target="#b1">(Child et al., 2019)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 FINE-TUNING SETTINGS</head><p>Our fine-tuning is implemented based on code base from HuggingFace 9 and SpanBERT <ref type="bibr" target="#b13">(Joshi et al., 2019a)</ref>. We use max sequence length=N , i.e., we allow fine-tuning task to input sequences as long as the pre-training model. If the input sequence is too long to fit the max sequence length=N constraints, we use a sliding window of stride 128 to split it. We grid search learning rate from {5e-6, 1e-5, 2e-5, 3e-5, 5e-5} and batch size from {16, 32}. The fine-tuning is performed for 4 epoches.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 TEST EFFICIENCY</head><p>We benchmark test efficiency of RoBERTa and our proposed BlockBERT. The benchmark code follows huggingface 10 . All experiments are run 30 times on a 32GB V100 GPU with half precision (FP16). We report the average running time at </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06174</idno>
		<title level="m">Training deep nets with memory cost</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><surname>Gonc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Correia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André Ft</forename><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName><surname>Martins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00015</idno>
		<title level="m">Adaptively sparse transformers</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT&apos; 2019</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volkan</forename><surname>Ugur Guney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05179</idno>
		<title level="m">Searchqa: A new q&amp;a dataset augmented with context from a search engine</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03635</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09224</idno>
		<title level="m">Gpu kernels for block-sparse weights</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Startransformer</title>
		<author>
			<persName><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT&apos; 2019</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1315" to="1325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Gpipe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06965</idno>
		<title level="m">Efficient training of giant neural networks using pipeline parallelism</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos; 17</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1601" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Daniel S Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10529</idno>
		<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09091</idno>
		<title level="m">Bert for coreference resolution: Baselines and analysis</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Natural questions: a benchmark for question answering research</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="453" to="466" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Latent retrieval for weakly supervised open domain question answering</title>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00300</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting</title>
		<author>
			<persName><forename type="first">Shiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyou</forename><surname>Yao Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.00235</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Are sixteen heads really better than one?</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10650</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03740</idno>
		<title level="m">Ganesh Venkatesh, et al. Mixed precision training</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Document-level neural machine translation with hierarchical attention networks</title>
		<author>
			<persName><forename type="first">Lesly</forename><surname>Miculicich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhananjay</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP&apos; 18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2947" to="2954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<title level="m">Passage re-ranking with bert</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01038</idno>
		<title level="m">fairseq: A fast, extensible toolkit for sequence modeling</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Know what you don&apos;t know: Unanswerable questions for squad</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03822</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">622803</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Matching the blanks: Distributional similarity for relation learning</title>
		<author>
			<persName><forename type="first">Baldini</forename><surname>Livio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03158</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Sharad</forename><surname>Nimit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Sohoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Megan</forename><surname>Richard Aberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Leszczynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Ré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10631</idno>
		<title level="m">Low-memory neural network training: A technical report</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Adaptive attention span in transformers</title>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07799</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12136</idno>
		<title level="m">Distilling taskspecific knowledge from bert into simple neural networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
