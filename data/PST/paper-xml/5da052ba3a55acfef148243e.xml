<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-04-16">16 Apr 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ouyu</forename><surname>Lan</surname></persName>
							<email>olan@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">University of Southern California ‡</orgName>
								<orgName type="institution" key="instit2">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">University of Southern California ‡</orgName>
								<orgName type="institution" key="instit2">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuchen</forename><surname>Bill</surname></persName>
							<email>yuchen.lin@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">University of Southern California ‡</orgName>
								<orgName type="institution" key="instit2">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">University of Southern California ‡</orgName>
								<orgName type="institution" key="instit2">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">He</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">University of Southern California ‡</orgName>
								<orgName type="institution" key="instit2">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">University of Southern California ‡</orgName>
								<orgName type="institution" key="instit2">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
							<email>xiangren@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">University of Southern California ‡</orgName>
								<orgName type="institution" key="instit2">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-04-16">16 Apr 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1910.04289v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sequence labeling is a fundamental framework for various natural language processing problems. Its performance is largely influenced by the annotation quality and quantity in supervised learning scenarios, and obtaining ground truth labels is often costly. In many cases, ground truth labels do not exist, but noisy annotations or annotations from different domains are accessible. In this paper, we propose a novel framework Consensus Network (CONNET) that can be trained on annotations from multiple sources (e.g., crowd annotation, cross-domain data...). It learns individual representation for every source and dynamically aggregates source-specific knowledge by a context-aware attention module. Finally, it leads to a model reflecting the agreement (consensus) among multiple sources. We evaluate the proposed framework in two practical settings of multi-source learning: learning with crowd annotations and unsupervised crossdomain model adaptation. Extensive experimental results show that our model achieves significant improvements over existing methods in both settings. We also demonstrate that the method can apply to various tasks and cope with different encoders. 1 * The first two authors contributed equally. 1 Our code can be found at https://github.com/ INK-USC/ConNet .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sequence labeling is a general approach encompassing various natural language processing (NLP) tasks including part-of-speech (POS) tagging <ref type="bibr" target="#b31">(Ratnaparkhi, 1996)</ref>, word segmentation <ref type="bibr" target="#b23">(Low et al., 2005)</ref>, and named entity recognition (NER) <ref type="bibr" target="#b28">(Nadeau and Sekine, 2007)</ref>. Typically, existing methods follow the supervised learning paradigm, and require high-quality annotations. While gold standard annotation is expensive and time-consuming, imperfect annotations are relatively easier to obtain from crowdsourcing (noisy labels) or other domains (out-of-domain). Despite their low cost, such supervision usually can be obtained from different sources, and it has been shown that multi-source weak supervision has the potential to perform similar to gold annotations <ref type="bibr" target="#b32">(Ratner et al., 2016)</ref>.</p><p>Specifically, we are interested in two scenarios: 1) learning with crowd annotations and 2) unsupervised cross-domain model adaptation. Both situations suffer from imperfect annotations, and benefit from multiple sources. Therefore, the key challenge here is to aggregate multi-source imperfect annotations for learning a model without knowing the underlying ground truth label sequences in the target domain.</p><p>Our intuition mainly comes from the phenomenon that different sources of supervision have different strengths and are more proficient with distinct situations. Therefore they may not keep consistent importance during aggregating supervisions, and aggregating multiple sources for a specific input should be a dynamic process that depends on the sentence context. To better model this nature, we need to (1) explicitly model the unique traits of different sources when training and (2) find best suitable sources for generalizing the learned model on unseen sentences.</p><p>In this paper, we propose a novel framework, named Consensus Network (CONNET), for sequence labeling with multi-source supervisions. We represent the annotation patterns as different biases of annotators over a shared behavior pattern. Both annotator-invariant patterns and annotator-specific biases are modeled in a decoupled way. The first term comes through sharing part of low-level model parameters in a multi-task learning schema. For learning the biases, we decouple them from the model as the transformations on top-level tagging model parameters, such that they can capture the unique strength of each annotator. With such decoupled source representations, we further learn an attention network for dynamically assigning the best sources for every unseen sentence through composing a transformation that represents the agreement among sources (consensus). Extensive experimental results in two scenarios show that our model outperforms strong baseline methods, on various tasks and with different encoders. CONNET achieves state-of-the-art performance on real-world crowdsourcing datasets and improves significantly in unsupervised crossdomain adaptation tasks over existing works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There exists three threads of related work with this paper, which are sequence labeling, crowdsourcing and unsupervised domain adaptation. Neural Sequence Labeling. Traditional approaches for sequence labeling usually need significant efforts in feature engineering for graphical models like conditional random fields (CRFs) <ref type="bibr" target="#b11">(Lafferty, 2001)</ref>. Recent research efforts in neural network models have shown that end-to-end learning like convolutional neural networks (CNNs) <ref type="bibr" target="#b24">(Ma and Hovy, 2016a)</ref> or bidirectional long short-term memory (BLSTMs) <ref type="bibr" target="#b13">(Lample et al., 2016)</ref> can largely eliminate humancrafted features.</p><p>BLSTM-CRF models have achieved promising performance <ref type="bibr" target="#b13">(Lample et al., 2016)</ref> and are used as our base sequence tagging model in this paper. Crowd-sourced Annotation. Crowd-sourcing has been demonstrated to be an effective way of fulfilling the label consumption of neural models <ref type="bibr" target="#b9">(Guan et al., 2017;</ref><ref type="bibr" target="#b18">Lin et al., 2019)</ref>. It collects annotations with lower costs and a higher speed from non-expert contributors but suffers from some degradation in quality. <ref type="bibr" target="#b3">Dawid and Skene (1979)</ref> proposes the pioneering work to aggregate crowd annotations to estimate true labels, and <ref type="bibr" target="#b39">Snow et al. (2008)</ref> shows its effectiveness with Amazon's Mechanical Turk system. Later works <ref type="bibr" target="#b4">(Dempster et al., 1977;</ref><ref type="bibr" target="#b6">Dredze et al., 2009;</ref><ref type="bibr" target="#b33">Raykar et al., 2010)</ref> focus on Expectation-Maximization (EM) algorithms to jointly learn the model and annotator behavior on classification.</p><p>Recent research shows the strength of multitask framework in semi-supervised learning <ref type="bibr" target="#b14">(Lan et al., 2018;</ref><ref type="bibr" target="#b2">Clark et al., 2018)</ref>, cross-type learning <ref type="bibr" target="#b42">(Wang et al., 2018)</ref>, and learning with entity triggers <ref type="bibr">(Lin et al., 2020)</ref>. <ref type="bibr" target="#b29">Nguyen et al. (2017)</ref>; <ref type="bibr" target="#b35">Rodrigues and Pereira (2018)</ref>; <ref type="bibr" target="#b38">Simpson et al. (2020)</ref> regards crowd annotations as noisy gold labels and constructs crowd components to model annotator-specific bias which were discarded during the inference process. It is worth mentioning that, it has been found even for human curated annotations, there exists certain label noise that hinders the model performance <ref type="bibr" target="#b43">(Wang et al., 2019)</ref>.</p><p>Unsupervised Domain Adaptation. Unsupervised cross-domain adaptation aims to transfer knowledge learned from high-resource domains (source domains) to boost performance on lowresource domains (target domains) of interests such as social media messages <ref type="bibr" target="#b16">(Lin et al., 2017)</ref>. Different from supervised adaptation (Lin and Lu, 2018), we assume there is no labels at all for target corpora. <ref type="bibr" target="#b37">Saito et al. (2017)</ref> and <ref type="bibr" target="#b36">Ruder and Plank (2018)</ref> explored bootstrapping with multitask tri-training approach, which requires unlabeled data from the target domain. The method is developed for one-to-one domain adaptation and does not model the differences among multiple source domains. <ref type="bibr" target="#b44">Yang and Eisenstein (2015)</ref> represents each domain with a vector of metadata domain attributes and uses domain vectors to train the model to deal with domain shifting, which is highly dependent on prior domain knowledge. <ref type="bibr" target="#b8">(Ghifary et al., 2016)</ref> uses an auto-encoder method by jointly training a predictor for source labels, and a decoder to reproduce target input with a shared encoder. The decoder acts as a normalizer to force the model to learn shared knowledge between source and target domains. Adversarial penalty can be added to the loss function to make models learn domain-invariant feature only <ref type="bibr" target="#b7">(Fernando et al., 2015;</ref><ref type="bibr" target="#b22">Long et al., 2014;</ref><ref type="bibr" target="#b26">Ming Harry Hsu et al., 2015)</ref>. However, it does not exploit domain-specific information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multi-source Supervised Learning</head><p>We formulate the multi-source sequence labeling problem as follows. Given K sources of supervision, we regard each source as an imperfect annotator (non-expert human tagger or models trained in related domains). For the k-th source data set</p><formula xml:id="formula_0">S (k) = {(x (k) i , y (k) i )} m k i=1 , we denote its i-th sentence as x (k) i</formula><p>which is a sequence of tokens:</p><formula xml:id="formula_1">x (k) i = (x (k) i,1 , • • • , x (k) i,N ).</formula><p>The tag sequence of the sentence is marked as</p><formula xml:id="formula_2">y (k) i = {y (k)</formula><p>i,j }. We define the sentence set of each annotators as</p><formula xml:id="formula_3">X (k) = {x (k) i } m k</formula><p>i=1 , and the whole training domain as the union of all sentence sets: X = (K) k=1 X (k) . The goal of the multi-source learning task is to use such imperfect annotations to train a model for predicting the tag sequence y for any sentence x in a target corpus T . Note that the target corpus T can either share the same distribution with X (Application I) or be significantly different (Application II). In the following two subsections, we formulate two typical tasks in this problem as shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>Application I: Learning with Crowd Annotations. When learning with crowd-sourced data, we regard each worker as an imperfect annotator (S (k) ), who may make mistakes or skip sentences in its annotations. Note that in this setting, different annotators tag subsets of the same given dataset (X ), and thus we assume there are no input distribution shifts among X (k) . Also, we only test sentences in the same domain such that the distribution in target corpus T is the same as well. That is, the marginal distribution of target corpus P T (x) is the same with that for each individual source dataset, i.e. P T (x) = P k (x). However, due to imperfectness of the annotations in each source, P k (y|x) is shifted from the underlying truth P (y|x) (illustrated in the top-left part of Fig. <ref type="figure" target="#fig_0">1</ref>). The multi-source learning objective here is to learn a model P T (y|x) for supporting infer-ence on any new sentences in the same domain.</p><p>Application II: Unsupervised Cross-Domain Model Adaptation. We assume there are available annotations in several source domains, but not in an unseen target domain. We assume that the input distributions P (x) in different source domains X (k) vary a lot, and such annotations can hardly be adapted for training a target domain model. That is, the prediction distribution of each domain model (P k (y|x)) is close to the underlying truth distribution (P (y|x)) only when x ∈ X (k) . For target corpus sentences x ∈ T , such a source model P k (y|x) again differs from underlying ground truth for the target domain P T (y|x) and can be seen as an imperfect annotators. Our objective in this setting is also to jointly model P T (y, x) while noticing that there are significant domain shifts between T and any other X (k) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Consensus Network</head><p>In this section, we present our two-phase framework CONNET for multi-source sequence labeling. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, our proposed framework first uses a multi-task learning schema with a special objective to decouple annotator representations as different parameters of a transformation around CRF layers. This decoupling phase (Section 4.2) is for decoupling the model parameters into a set of annotator-invariant model parameters and a set of annotator-specific representations. Secondly, the dynamic aggregation phase (Section 4.3) learns to contextually utilize the annotator representations with a lightweight attention mechanism to find the best suitable transformation for each sentence, so that the model can achieve a context-aware consensus among all sources. The inference process is described in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Base Model: BLSTM-CRF</head><p>Many recent sequence labeling frameworks <ref type="bibr" target="#b25">(Ma and Hovy, 2016b;</ref><ref type="bibr" target="#b27">Misawa et al., 2017)</ref> share a very basic structure: a bidirectional LSTM network followed by a CRF tagging layer (i.e. BLSTM-CRF). The BLSTM encodes an input sequence x = {x 1 , x 2 , . . . , x n } into a sequence of hidden state vectors h 1:n . The CRF takes as input the hidden state vectors and computes an emission score matrix U ∈ R n×L where L is the size of tag set. It also maintains a trainable transition matrix M ∈ R L×L . We can consider U i,j is the score of labeling the tag with id j ∈ {1, 2, ..., L} for i th word in the input sequence x, and M i,j means the transition score from i th tag to j th . The CRF further computes the score s for a predicted tag sequence y = {y 1 , y 2 , ..., y k } as</p><formula xml:id="formula_4">s(x, y) = T t=1 (U t,yt + M y t−1 ,yt ),<label>(1)</label></formula><p>and then tag sequence y follows the conditional distribution</p><formula xml:id="formula_5">P (y|x) = exp s(x, y) y∈Yx exp s(x, y)</formula><p>.</p><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Decoupling Phase: Learning annotator representations</head><p>For decoupling annotator-specific biases in annotations, we represent them as a transformation on emission scores and transition scores respectively. Specifically, we learn a matrix A (k) ∈ R L×L for each imperfect annotator k and apply this matrix as transformation on U and M as follows:</p><formula xml:id="formula_6">s (k) (x, y) = T t=1 (UA (k) ) t,yt + (MA (k) ) y t−1 ,yt .</formula><p>(3) From this transformation, we can see that the original score function s in Eq. 1 becomes an sourcespecific computation. The original emission and transformation score matrix U and M are still shared by all the annotators, while they both are transformed by the matrix A (k) for k-th annotator.</p><p>While training the model parameters in this phase, we follow a multi-task learning schema. That is, we share the model parameters for BLSTM and CRF (including W, b, M), while updating A (k)  only by examples in</p><formula xml:id="formula_7">S k = {X (k) , Y (k) }.</formula><p>The learning objective is to minimize the negative log-likelihood of all source annotations:</p><formula xml:id="formula_8">L = − log K k=1 |X (k) | i=1 P (y (k) i |x (k) i ) ,<label>(4)</label></formula><p>P (y</p><formula xml:id="formula_9">(k) i |x (k) i ) = exp s (k) (x (k) i , y (k) i ) y exp s (k) (x, y ) . (5)</formula><p>The assumption on the annotation representation A (k) is that it can model the pattern of annotation bias. Each annotator can be seen as a noisy version of the shared model. For the k-th annotator, A (k) models noise from labeling the current word and transferring from the previous label. Specifically, each entry A (k) i,j captures the probability of mistakenly labeling i-th tag to j-th tag. In other words, the base sequence labeling model in Sec. 4.1 learns the basic consensus knowledge while annotator-specific components add their understanding to predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The Aggregation Phase: Dynamically</head><p>Reaching Consensus</p><p>In the second phase, our proposed network learns a context-aware attention module for a consensus representation supervised by combined predictions on the target data. For each sentence in target data T , these predictions are combined by weighted voting. The weight of each source is its normalized F 1 score on the training set. Through weighted voting on such augmented labels over all source sentences X , we can find a good approximation of underlying truth labels.</p><p>For better generalization and higher speed, an attention module is trained to estimate the relevance of each source to the target under the supervision of generated labels. Specifically, we compute the sentence embedding by concatenating the last hidden states of the forward LSTM and the backward LSTM, i.e.</p><formula xml:id="formula_10">h (i) = [ − → h (i) T ; ← − h (i) 0 ]</formula><p>. The attention module inputs the sentence embedding and outputs a normalized weight for each source:</p><formula xml:id="formula_11">q i = softmax(Qh (i) ), where Q ∈ R K×2d . (6)</formula><p>where d is the size of each hidden state h (i) . Source-specific matrices {A (k) } K k=1 are then aggregated into a consensus representation A * i for sentence x i ∈ X by</p><formula xml:id="formula_12">A * i = K k=1 q i,k A (k) .<label>(7)</label></formula><p>In this way, the consensus representation contains more information about sources which are more related to the current sentence. It also alleviates the contradiction problem among sources, because it could consider multiple sources of different emphasis. Since only an attention model with weight matrix Q is required to be trained, the amount of computation is relatively small. We assume the base model and annotator representations are welltrained in the previous phase. The main objective in this phase is to learn how to select most suitable annotators for the current sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Parameter Learning and Inference</head><p>CONNET learns parameters through two phases described above. In the decoupling phase, each instance from source S k is used for training the base sequence labeling model and its representation A (k) . In the aggregation phase, we use aggregated predictions from the first phase to learn a lightweight attention module. For each instance in the target corpus x i ∈ T , we calculate its embedding h i from BLSTM hidden states. With these sentence embeddings, the context-aware attention module assigns weight q i to each source and dynamically aggregates source-specific representations {A (k) } for inferring ŷi . In the inference process, only the consolidated consensus matrix A * i is applied to the base sequence learning model. In this way, more specialist knowledge helps to deal with more complex instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Model Application</head><p>The proposed model can be applied to two practical multi-sourcing settings: learning with crowd annotations and unsupervised cross-domain model adaptation. In the crowd annotation learning setting, the training data of the same domain is annotated by multiple noisy annotators, and each annotator is treated as a source. In the decoupling phase, the model is trained on noisy annotations, and in the aggregation phase, it is trained with combined predictions on the training set. In the cross-domain setting, the model has access to unlabeled training data of the target domain and clean labeled data of multiple source domains. Each domain is treated as a source. In the decoupling phase, the model is trained on source domains, and in the aggregation phase, the model is trained on combined predictions on the training data of the target domain. Our framework can also extend to new tasks other than sequence labeling and cope with different encoders. We will demonstrate this ability in experiments.</p><p>Our method is also incorporated as a feature for controlling the quality of crowd-annotation in annotation frameworks such as AlpacaTag <ref type="bibr" target="#b18">(Lin et al., 2019)</ref> and LEAN-LIFE <ref type="bibr" target="#b15">(Lee et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate CONNET in the two aforementioned settings of multi-source learning: learning with crowd annotations and unsupervised cross-domain model adaptation. Additionally, to demonstrate the generalization of our framework, we also test our method on sequence labeling with transformer encoder in Appendix B and text classification with MLP encoder in Section 5.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>Crowd-Annotation Datasets. We use crowdannotation datasets based on the 2003 CoNLL shared NER task <ref type="bibr" target="#b40">(Tjong Kim Sang and De Meulder, 2003)</ref>. The real-world datasets, denoted as AMT, are collected by <ref type="bibr" target="#b34">Rodrigues et al. (2014)</ref>  domains: academic, bio, fiction, news, voyage, wiki, and interview. For NER task, we select the English portion of the OntoNotes v5 corpus <ref type="bibr" target="#b10">(Hovy et al., 2006)</ref>. The corpus is annotated with 9 named entities with data from 6 domains: broadcast conversation (bc), broadcast news (bn), magazine (mz), newswire (nw), pivot text (pt), telephone conversation (tc), and web (web). Multi-Domain Sentiment Dataset (MDS) v2.0 <ref type="bibr" target="#b0">(Blitzer et al., 2007)</ref> is used for text classification, which is built on Amazon reviews from 4 domains: books, dvd, electronics, and kitchen. Since the dataset only contains word frequencies for each review without raw texts, we follow the setting in Chen and Cardie (2018) considering 5,000 most frequent words and use the raw counts as the feature vector for each review.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiment Setup</head><p>For sequence labeling tasks, we follow <ref type="bibr" target="#b20">Liu et al. (2018)</ref> to build the BLSTM-CRF architecture as the base model. The dimension of characterlevel, word-level embeddings and LSTM hidden layer are set as 30, 100 and 150 respectively. For text classification, each review is represented as a 5000-d vector. We use an MLP with a hidden size of 100 for encoding features and a linear classification layer for predicting labels. The dropout with a probability of 0.5 is applied to the nonrecurrent connections for regularization. The network parameters are updated by stochastic gradient descent (SGD). The learning rate is initialized as 0.015 and decayed by 5% for each epoch. The training process stops early if no improvements in 15 continuous epochs and selects the best model on the development set. For the dataset without a development set, we report the performance on the 50-th epoch. For each experiment, we report the average performance and standard variance of 3 runs with different random initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Compared Methods</head><p>We compare our models with multiple baselines, which can be categorized in two groups: wrapper methods and joint models. To demonstrate the theoretical upper bound of performance, we also train the base model using ground-truth annotations in the target domain (Gold).</p><p>A wrapper method consists of a label aggregator and a deep learning model. These two components could be combined in two ways: (1) aggregating labels on crowd-sourced training set then feeding the generated labels to a Sequence Labeling Model (SLM) <ref type="bibr" target="#b21">(Liu et al., 2017)</ref>; (2) feeding multi-source data to a Multi-Task Learning (MTL) <ref type="bibr" target="#b42">(Wang et al., 2018)</ref> model then aggregating multiple predicted labels. We investigate multiple label aggregation strategies. CONCAT considers all crowd annotations as gold labels. MVT does majority voting on the token level, i.e., the majority of labels {y k i,j } is selected as the gold label for each token x i,j . MVS is conducted on the sequence level, addressing the problem of violating Begin/In/Out (BIO) rules. DS <ref type="bibr" target="#b3">(Dawid and Skene, 1979)</ref>, <ref type="bibr">HMM (Nguyen et al., 2017)</ref> and BEA <ref type="bibr" target="#b30">(Rahimi et al., 2019)</ref> induce consensus labels with probability models.</p><p>In contrast with wrapper methods, joint models incorporate multi-source data within the structure of sequential taggers and jointly model all individual annotators. CRF-MA models CRFs with Multiple Annotators by EM algorithm <ref type="bibr" target="#b34">(Rodrigues et al., 2014)</ref>. <ref type="bibr" target="#b29">Nguyen et al. (2017)</ref> augments the LSTM  <ref type="table" target="#tab_6">0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46</ref> Annotator ID architecture with crowd vectors. These crowd components are element-wise added to tags scores (Crowd-Add) or concatenated to the output of hidden layer (Crowd-Cat). These two methods are the most similar to our decoupling phase. We implemented them and got better results than reported. CL-MW applies a crowd layer to a CNNbased deep learning framework <ref type="bibr" target="#b35">(Rodrigues and Pereira, 2018)</ref>. Tri-Training uses bootstrapping with multi-task Tri-Training approach for unsupervised one-to-one domain adaptation <ref type="bibr" target="#b37">(Saito et al., 2017;</ref><ref type="bibr" target="#b36">Ruder and Plank, 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Learning with Crowd Annotations</head><p>Performance on real-world datasets. Tab. 1 shows the performance of aforementioned methods and our CONNET on two real-world datasets, i.e. AMT and AMTC<ref type="foot" target="#foot_0">2</ref> . We can see that CONNET outperforms all other methods on both datasets significantly on F 1 score, which shows the effectiveness of dealing with noisy annotations for higher-quality labels. Although CONCAT-SLM achieves the highest precision, it suffers from low recall. Most existing methods have the highprecision but low-recall problem. One possible reason is that they try to find the latent ground truth and throw away illuminating annotator-specific information. So only simple mentions can be classified with great certainty while difficult mentions fail to be identified without sufficient knowledge. In comparison, CONNET pools information from all annotations and focus on matching knowledge to make predictions. It makes the model be able to identify more mentions and get a higher recall. Case study. It is enlightening to analyze whether the model decides the importance of annotators given a sentence. Fig. <ref type="figure" target="#fig_3">3</ref> visualizes test F1 score of all annotators, and attention weights q i in Eq. 6  for 4 sampled sentences containing different entity types. Obviously, the 2nd sample sentence with ORG has higher attention weights on 1st, 5th and 33rd annotator who are best at labeling ORG. More details and cases are shown in Appendix A.1.</p><p>Ablation study. We also investigate multiple variants of two phases on AMT dataset, shown in Fig. <ref type="figure" target="#fig_4">4</ref>. We explore 3 approaches to incorporate source-specific representation in the decoupling phase (DP). CRF means the traditional approach as Eq. 1 while DP(1+2) is for our method as Eq. 3.</p><p>DP(1) only applies source representations A (k)  to the emission score U while DP(2) only transfers the transition matrix M. We can observe from the result that both variants can improve the result. The underlying model keeps more consensus knowledge if we extract annotator-specific bias on sentence encoding and label transition. We also compare 4 methods of generating supervision targets in the aggregation phase (AP). OMV uses ma-  Performance on simulated datasets. To analyze the impact of annotator quality, we split the origin train set into z folds and each fold could be used to train a CRF model whose reliability could be represented as r = 1/z assuming a model with less training data would have stronger bias and less generalization. We tried 5 settings where z = {5, 10, 15, 30, 50} and randomly select 5 folds for each setting. When the reliability level is too low, i.e. 1/50, only the base model is used for prediction without annotator representations. Shown in Fig. <ref type="figure" target="#fig_5">5</ref>(a), CONNET achieves significant improvements over MVT-SLM and competitive performance as Crowd-Cat, especially when annotators are less reliable.</p><p>Regarding the annotator quantity, we split the train set into 50 subsets (r = 1/50) and randomly select {5, 10, 15, 30, 50} folds for simulation. <ref type="bibr">Fig. 5(b)</ref> shows CONNET is superior to baselines and able to well deal with many annotators while there is no obvious relationship between the performance and annotator quantity in baselines. We can see the performance of our model increases as the number of annotators and, regardless of the number of annotators, our method consistently outperforms than other baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Cross-Domain Adaptation Performance</head><p>The results of each task on each domain are shown in Tab. 2. We can see that CONNET performs the best on most of the domains and achieves the highest average score for all tasks. We report the accuracy for POS tagging and classification, and the chunk-level F 1 score for NER. We can see that CONNET achieves the highest average score on all tasks. MTL-MVT is similar to our decoupling phase and performs much worse. Naively doing unweighted voting does not work well.</p><p>The attention can be viewed as implicitly doing weighted voting on the feature level. MTL-BEA relies on a probabilistic model to conduct weighted voting over predictions, but unlike our approach, its voting process is independent from the input context. It is probably why our model achieves higher scores. This demonstrates the importance of assigning weights to domains based on the input sentence.</p><p>Tri-Training trains on the concatenated data from all sources also performs worse than CONNET, which suggests the importance of a multi-task structure to model the difference among domains. The performance of Crowd-Add is unstable (high standard deviation) and very low on the NER task, because the size of the crowd vectors is not controllable and thus may be too large. On the other hand, the size of the crowd vectors in Crowd-Cat can be controlled and tuned. These two methods leverage domain-invariant knowledge only but not domain-specific knowledge and thus does not have comparable performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Analyzing Learned Attention</head><p>We analyzed the attention scores generated by the attention module on the OntoNotes dataset. For each sentence in the target domain we collected the attention score of each source domain, and finally the attention scores are averaged for each source-target pair. Fig. <ref type="figure" target="#fig_6">6</ref> shows all the sourceto-target average attention scores. We can see that some domains can contribute to other related domains. For example, bn (broadcast news) and nw (newswire) are both about news and they contribute to each other; bn and bc (broadcast conversation) are both broadcast and bn contributes to bc; bn and nw both contributes to mz (magzine) probably because they are all about news; wb (web) and tc (telephone conversation) almost make no positive contribution to any other, which is reasonable because they are informal texts compared to others and they are not necessarily related to the other. Overall the attention scores can make some sense. It suggests that the attention is aware of relations between different domains and can contribute to the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we present CONNET for learning a sequence tagger from multi-source supervision. It could be applied in two practical scenarios: learning with crowd annotations and cross-domain adaptation. In contrast to prior works, CONNET learns fine-grained representations of each source which are further dynamically aggregated for every unseen sentence in the target data. Experiments show that our model is superior to previous crowd-sourcing and unsupervised domain adaptation sequence labeling models. The proposed learning framework also shows promising results on other NLP tasks like text classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Analysis of ConNet with BLSTM Encoder</head><p>A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Case study on learning with crowd annotations</head><p>To better understand the effect and benefit of CONNET, we do some case study on AMTC realworld dataset with 47 annotators. We look into some more instances to investigate the ability of attention module to find right annotators in Fig. <ref type="figure" target="#fig_7">7</ref> and Tab. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Result of ConNet with Transformer Encoder</head><p>To demonstrate the generalization of our framework, we re-implement CONNET and some baselines (MTV-SLM, Crowd-Add, Gold) with Transformer-CRF as the base model. Specifically, the base model takes Transformer as the encoder for CRF, which has shown its effectiveness in many NLP tasks <ref type="bibr" target="#b41">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b5">Devlin et al., 2019)</ref>. Transformer models sequences with self-attention and eliminates all recurrence. Following the experimental settings from <ref type="bibr" target="#b41">(Vaswani et al., 2017)</ref>, we set the number of heads for multihead attention as 8, the dimension of keys and values as 64, and the hidden size of the feed-forward layers as 1024. We conduct experiments with crowd-annotation dataset AMTC on NER task and cross-domain dataset UD on POS task, which are described in Section 5.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the task settings for the two applications in this work: (a) learning consensus model from crowd annotations; (b) unsupervised cross-domain model adaptation.</figDesc><graphic url="image-1.png" coords="2,91.42,62.81,414.71,108.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of the CONNET framework. The decoupling phase constructs the shared model (yellow) and sourcespecific matrices (blue). The aggregation phase dynamically combines crowd components into a consensus representation (blue) by a context-aware attention module (red) for each sentence x.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualizations of (a) the expertise of annotators; (b) attention weights for sample sentences. More cases and details are described in Appendix A.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance of CONNET variants of decoupling phase (DP) and aggregation phase (AP).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance on simulated crowd-sourced NER data with (a) 5 annotators with different reliability levels; (b) different numbers of annotators with reliability r = 1/50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Heatmap of averaged attention scores from each source domain to each target domain.</figDesc><graphic url="image-4.png" coords="9,110.20,62.81,141.87,105.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Visualizations of (a) the expertise of annotators; (b) attention weights for additional sample sentences to Fig. 3. Details of samples are described in Tab. 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>using Amazon's Mechanical Turk where F1 scores of annotators against the ground truth vary from 17.60% to 89.11%. Since there is no development set in AMT, we also follow<ref type="bibr" target="#b29">Nguyen et al. (2017)</ref> to use the AMT training set and CoNLL 2003 development and test sets, denoted as AMTC. Overlapping sentences are removed in the training set, which is ignored in that work. Additionally, we construct two sets of simulated datasets to investigate the quality and quantity of annotators. To simulate the behavior of a non-expert annotator, a CRF model is trained on a small subset of training data and generates predictions on the whole set. Because of the limited size of training data, each model would have a bias to certain patterns.Cross-Domain Datasets. In this setting, we investigate three NLP tasks: POS tagging, NER and text classification. For POS tagging task, we use the GUM portion<ref type="bibr" target="#b45">(Zeldes, 2017)</ref> of Universal Dependencies (UD) v2.3 corpus with 17 tags and 7 Performance on real-world crowd-sourced NER datasets. The best score in each column excepting Gold is marked bold. * indicates number reported by the paper.</figDesc><table><row><cell>Methods</cell><cell></cell><cell>AMTC</cell><cell></cell><cell></cell><cell>AMT</cell><cell></cell></row><row><cell></cell><cell>Precision(%)</cell><cell>Recall(%)</cell><cell cols="2">F1-score(%) Precision(%)</cell><cell>Recall(%)</cell><cell>F1-score(%)</cell></row><row><cell>CONCAT-SLM</cell><cell cols="6">85.95(±1.00) 57.96(±0.26) 69.23(±0.13) 91.12(±0.57) 55.41(±2.66) 68.89(±1.92)</cell></row><row><cell>MVT-SLM</cell><cell cols="6">84.78(±0.66) 62.50(±1.36) 71.94(±0.66) 86.96(±1.22) 58.07(±0.11) 69.64(±0.31)</cell></row><row><cell>MVS-SLM</cell><cell cols="6">84.76(±0.50) 61.95(±0.32) 71.57(±0.04) 86.95(±1.12) 56.23(±0.01) 68.30(±0.33)</cell></row><row><cell>DS-SLM (Nguyen et al., 2017)</cell><cell>72.30  *</cell><cell>61.17  *</cell><cell>66.27  *</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HMM-SLM (Nguyen et al., 2017)</cell><cell>76.19  *</cell><cell>66.24  *</cell><cell>70.87  *</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MTL-MVT (Wang et al., 2018)</cell><cell cols="6">81.81(±2.34) 62.51(±0.28) 70.87(±1.06) 88.88(±0.25) 65.04(±0.80) 75.10(±0.44)</cell></row><row><cell>MTL-BEA (Rahimi et al., 2019)</cell><cell cols="6">85.72(±0.66) 58.28(±0.43) 69.39(±0.52) 77.56(±2.23) 67.23(±0.72) 72.01(±0.85)</cell></row><row><cell>CRF-MA (Rodrigues et al., 2014)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>49.40  *</cell><cell>85.60  *</cell><cell>62.60  *</cell></row><row><cell>Crowd-Add (Nguyen et al., 2017)</cell><cell cols="6">85.81(±1.53) 62.15(±0.18) 72.09(±0.42) 89.74(±0.10) 64.50(±1.48) 75.03(±1.02)</cell></row><row><cell>Crowd-Cat (Nguyen et al., 2017)</cell><cell cols="6">85.02(±0.98) 62.73(±1.10) 72.19(±0.37) 89.72(±0.47) 63.55(±1.20) 74.39(±0.98)</cell></row><row><cell>CL-MW (Rodrigues and Pereira, 2018)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>66.00  *</cell><cell>59.30  *</cell><cell>62.40  *</cell></row><row><cell>CONNET (Ours)</cell><cell cols="6">84.11(±0.71) 68.61(±0.03) 75.57(±0.27) 88.77(±0.25) 72.79(±0.04) 79.99(±0.08)</cell></row><row><cell>Gold (Upper Bound)</cell><cell cols="6">89.48(±0.32) 89.55(±0.06) 89.51(±0.21) 92.12(±0.31) 91.73(±0.09) 91.92(±0.21)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance on cross-domain data The best score (except the Gold) in each column that is significantly (p &lt; 0.05) better than the second best is marked bold, while those are better but not significantly are underlined.</figDesc><table><row><cell>jority voting of original annotations, while PMV</cell></row><row><cell>substitutes them with model prediction learned</cell></row><row><cell>from DP. AMV extends the model by using all pre-</cell></row><row><cell>diction, while AWV uses majority voting weighted</cell></row><row><cell>by each annotator's training F 1 score. The re-</cell></row><row><cell>sults show the effectiveness of AWV, which could</cell></row><row><cell>augment training data and well approximate the</cell></row><row><cell>ground truth to supervise the attention module for</cell></row><row><cell>estimating the expertise of annotator on the cur-</cell></row><row><cell>rent sentence. We can also infer labels on the test</cell></row><row><cell>set by conducting AWV on predictions of the un-</cell></row><row><cell>derlying model with each annotator-specific com-</cell></row><row><cell>ponents. However, it leads to heavy computation-</cell></row><row><cell>consuming and unsatisfying performance, whose</cell></row><row><cell>test F 1 score is 77.35(±0.08). We can also train</cell></row><row><cell>a traditional BLSTM-CRF model with the same</cell></row><row><cell>AMV labels. Its result is 78.93(±0.13), which is</cell></row><row><cell>lower than CONNET and show the importance of</cell></row><row><cell>extracted source-specific components.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>3. Sentence 1-12 contains a specific entity type respectively while 13-16 contains multiple different entities. Compared with expertise of annotators, we can see that the attention module would give more weight on annotators who have competitive performance and preference on the included entity type. Although top selected annotators for ORG has relatively lower expertise on ORG than PER and LOC, they are actually the top three annotators with highest expertise on ORG.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>1. Results are shown in Table 4. We can see our model outperforms over other baselines in both tasks and applications.</figDesc><table><row><cell></cell><cell>Defender [PER Hassan Abbas] rose to in-</cell></row><row><cell>1</cell><cell>tercept a long ball into the area in the 84th minute but only managed to divert it into the</cell></row><row><cell></cell><cell>top corner of [PER Bitar] 's goal .</cell></row><row><cell cols="2">2 [ORG Plymouth] 4 [ORG Exeter] 1</cell></row><row><cell></cell><cell>Hosts [LOC UAE] play [LOC Kuwait] and</cell></row><row><cell>3</cell><cell>[LOC South Korea] take on [LOC Indonesia]</cell></row><row><cell></cell><cell>on Saturday in Group A matches .</cell></row><row><cell></cell><cell>The former [MISC Soviet] republic was</cell></row><row><cell>4</cell><cell>playing in an [MISC Asian Cup] finals tie</cell></row><row><cell></cell><cell>for the first time .</cell></row><row><cell>5</cell><cell>[PER Bitar] pulled off fine saves whenever they did .</cell></row><row><cell>6</cell><cell>[PER Coste] said he had approached the player two months ago about a comeback .</cell></row><row><cell cols="2">7 [ORG Goias] 1 [ORG Gremio] 3</cell></row><row><cell>8</cell><cell>[ORG Portuguesa] 1 [ORG Atletico Mineiro] 0</cell></row><row><cell cols="2">9 [LOC Melbourne] 1996-12-06</cell></row><row><cell></cell><cell>On Friday for their friendly against</cell></row><row><cell></cell><cell>[LOC Scotland] at [LOC Murrayfield]</cell></row><row><cell>10</cell><cell>more than a year after the 30-year-old</cell></row><row><cell></cell><cell>wing announced he was retiring following</cell></row><row><cell></cell><cell>differences over selection .</cell></row><row><cell cols="2">11 Scoreboard in the [MISC World Series]</cell></row><row><cell cols="2">12 Cricket -[MISC Sheffield Shield] score .</cell></row><row><cell>13</cell><cell>" He ended the [MISC World Cup] on the wrong note , " [PER Coste] said .</cell></row><row><cell>14</cell><cell>Soccer -[ORG Leeds] ' [PER Bowyer] fined for part in fast-food fracas .</cell></row><row><cell>15</cell><cell>[ORG Rugby Union] -[PER Cuttitta] back for [LOC Italy] after a year .</cell></row><row><cell></cell><cell>[LOC Australia] gave [PER Brian Lara] an-</cell></row><row><cell></cell><cell>other reason to be miserable when they beat</cell></row><row><cell>16</cell><cell>[LOC West Indies] by five wickets in the</cell></row><row><cell></cell><cell>opening [MISC World Series] limited overs</cell></row><row><cell></cell><cell>match on Friday .</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Sample instances in Fig.3and Fig.7with NER annotations including PER (red), ORG (blue), LOC (violet) and MISC (orange).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Performance of methods with Transformer-CRF as the base model on crowd-annotation NER dataset AMTC and cross-domain POS dataset UD.</figDesc><table><row><cell></cell><cell></cell><cell>AMTC</cell><cell></cell><cell>UD</cell></row><row><cell></cell><cell>Precision(%)</cell><cell>Recall(%)</cell><cell>F1-score(%)</cell><cell>Accuracy(%)</cell></row><row><cell>MVT-SLM</cell><cell cols="3">72.21(±1.63) 51.72(±3.58) 60.21(±1.87)</cell><cell>87.23(±0.51)</cell></row><row><cell cols="4">Crowd-Add (Nguyen et al., 2017) 75.32(±1.41) 50.80(±0.30) 60.68(±0.67)</cell><cell>88.20(±0.36)</cell></row><row><cell>CONNET (Ours)</cell><cell cols="3">76.86(±0.33) 56.43(±3.32) 65.05(±2.32)</cell><cell>89.27(±0.31)</cell></row><row><cell>Gold (Upper Bound)</cell><cell cols="3">81.24(±1.25) 80.52(±0.37) 80.87(±0.79)</cell><cell>90.45(±0.71)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">We tried our best to re-implement the baseline methods for all datasets, and left the results blank when the re-implementation is not showing consistent results as in the original papers.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via Contract No. 2019-19051600007, NSF SMA 18-29268, and Snap research gift. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. We would like to thank all the collaborators in USC INK research lab for their constructive feedback on the work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multinomial adversarial networks for multi-domain text classification</title>
		<author>
			<persName><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1111</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
				<meeting>of NAACL-HLT<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1226" to="1240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence modeling with cross-view training</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1217</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1914" to="1925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation of observer error-rates using the em algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Skene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series C (Applied Statistics)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="28" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
				<meeting>of NAACL-HLT<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sequence learning from data with multiple labels</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><forename type="middle">Pratim</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML/PKDD Workshop on Learning from Multi-Label Data</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Joint cross-domain classification and subspace learning for unsupervised adaptation</title>
		<author>
			<persName><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatiana</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="60" to="66" />
		</imprint>
	</monogr>
	<note>Pattern Recognition Letters</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep reconstruction-classification networks for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengjie</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Who said what: Modeling individual labelers improves classification</title>
		<author>
			<persName><forename type="first">Melody</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Gulshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ontonotes: The 90\% solution</title>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
				<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
				<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Kaufmann</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1030</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
				<meeting>of NAACL-HLT<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised training using adversarial multi-task learning for spoken language understanding</title>
		<author>
			<persName><forename type="first">Ouyu</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6049" to="6053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Leanlife: A label-efficient annotation framework towards learning from explanation</title>
		<author>
			<persName><forename type="first">Dong-Ho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinyuan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Boschee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL (Demo)</title>
				<meeting>of ACL (Demo)<address><addrLine>Leonardo Neves, and Xiang Ren</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-channel bilstm-crf model for emerging named entity recognition in social media</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Bill Y Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenny</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL Workshop</title>
				<meeting>of ACL Workshop</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="160" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Triggerner: Learning with entity triggers as explanations for named entity recognition</title>
		<author>
			<persName><forename type="first">Dong-Ho</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prashant</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Shiralkar</surname></persName>
		</author>
		<author>
			<persName><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Alpacatag: An active learning-based crowd annotation framework for sequence tagging</title>
		<author>
			<persName><forename type="first">Dongho</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ouyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL (Demo)</title>
				<meeting>of ACL (Demo)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural adaptation layers for cross-domain named entity recognition</title>
		<author>
			<persName><forename type="first">Bill</forename><surname>Yuchen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling</title>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Heterogeneous supervision for relation extraction: A representation learning approach</title>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="46" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transfer joint matching for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaguang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
				<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1410" to="1417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A maximum entropy approach to chinese word segmentation</title>
		<author>
			<persName><forename type="first">Jin Kiat</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyuan</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGHAN Workshop</title>
				<meeting>of SIGHAN Workshop</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF</title>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016a</date>
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016b</date>
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with imbalanced cross-domain data</title>
		<author>
			<persName><forename type="first">Tzu</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yu Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-An</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Ren</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Chiang</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCV</title>
				<meeting>of ICCV</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4121" to="4129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Character-based bidirectional LSTM-CRF with words and characters for Japanese named entity recognition</title>
		<author>
			<persName><forename type="first">Shotaro</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Motoki</forename><surname>Taniguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasuhide</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoko</forename><surname>Ohkuma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL Workshop</title>
				<meeting>of ACL Workshop<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A survey of named entity recognition and classification</title>
		<author>
			<persName><forename type="first">David</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Lingvisticae Investigationes</publisher>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Aggregating and predicting sequence labels from crowd annotations</title>
		<author>
			<persName><forename type="first">An</forename><surname>Thanh Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessy</forename><surname>Junyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ani</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName><surname>Lease</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="299" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Massively multilingual transfer for ner</title>
		<author>
			<persName><forename type="first">Afshin</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="151" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A maximum entropy model for part-of-speech tagging</title>
		<author>
			<persName><forename type="first">Adwait</forename><surname>Ratnaparkhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Data programming: Creating large training sets, quickly. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">De</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Selsam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">R</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning from crowds</title>
		<author>
			<persName><forename type="first">Shipeng</forename><surname>Vikas C Raykar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerardo</forename><forename type="middle">Hermosillo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Valadez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Florin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><surname>Bogoni</surname></persName>
		</author>
		<author>
			<persName><surname>Moy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1297" to="1322" />
			<date type="published" when="2010-04">2010. Apr</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sequence labeling with multiple annotators</title>
		<author>
			<persName><forename type="first">Filipe</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardete</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="165" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning from crowds</title>
		<author>
			<persName><forename type="first">Filipe</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Strong baselines for neural semi-supervised learning under domain shift</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1096</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1044" to="1054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Asymmetric tri-training for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
				<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2988" to="2997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Low resource sequence tagging with weak labels</title>
		<author>
			<persName><forename type="first">Edwin</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cheap and fast-but is it good?: Evaluating non-expert annotations for natural language tasks</title>
		<author>
			<persName><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP, EMNLP &apos;08</title>
				<meeting>of EMNLP, EMNLP &apos;08<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
				<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cross-type biomedical named entity recognition with deep multi-task learning</title>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><surname>Langlotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="page">869</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Crossweigh: Training named entity tagger from imperfect annotations</title>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/IJCNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised multi-domain adaptation with feature embeddings</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
				<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="672" to="682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">The gum corpus: creating multilayer resources in the classroom. Language Resources and Evaluation</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Zeldes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="581" to="612" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
