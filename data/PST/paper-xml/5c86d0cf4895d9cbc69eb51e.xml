<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Where to Classify in Multi-view Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hayko</forename><surname>Riemenschneider</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Laboratory</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">András</forename><surname>Bódis-Szomorú</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Laboratory</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Julien</forename><surname>Weissenberg</surname></persName>
							<email>julienw@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Laboratory</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luc</forename><forename type="middle">Van</forename><surname>Gool</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Laboratory</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">K.U. Leuven</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Where to Classify in Multi-view Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3445FB5C9673432EDEC545D495283F29</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>semantic segmentation</term>
					<term>multi-view</term>
					<term>efficiency</term>
					<term>view selection</term>
					<term>redundancy</term>
					<term>ranking</term>
					<term>importance</term>
					<term>labeling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There is an increasing interest in semantically annotated 3D models, e.g. of cities. The typical approaches start with the semantic labelling of all the images used for the 3D model. Such labelling tends to be very time consuming though. The inherent redundancy among the overlapping images calls for more efficient solutions. This paper proposes an alternative approach that exploits the geometry of a 3D mesh model obtained from multi-view reconstruction. Instead of clustering similar views, we predict the best view before the actual labelling. For this we find the single image part that bests supports the correct semantic labelling of each face of the underlying 3D mesh. Moreover, our singleimage approach may surprise because it tends to increase the accuracy of the model labelling when compared to approaches that fuse the labels from multiple images. As a matter of fact, we even go a step further, and only explicitly label a subset of faces (e.g. 10%), to subsequently fill in the labels of the remaining faces. This leads to a further reduction of computation time, again combined with a gain in accuracy. Compared to a process that starts from the semantic labelling of the images, our method to semantically label 3D models yields accelerations of about 2 orders of magnitude. We tested our multi-view semantic labelling on a variety of street scenes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-view 3D reconstructions are common these days. Not only have tourist data become ubiquitous <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> but the images also often result from deliberate mobile mapping campaigns <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. The images have to exhibit sufficient redundancy -overlap -in order to be suited for Structure-from-Motion (SfM) and Multi-View Stereo (MVS) reconstruction. In the meantime, solutions have been worked out to keep the number of images within bounds, primarily for making the reconstruction pipelines applicable to larger scenes. For instance, the redundancy can be captured by measuring visual similarity between images, and the scene can be summarized, e.g. by constructing a graph of iconic views <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>⇒ ⇒</head><p>Fig. <ref type="figure">1</ref>. View overlap is ignored by existing work in semantic scene labelling, and features in all views for all surface parts are extracted redundantly and expensively (top left). In turn, we propose a fine-grained view selection (top right), as well as to reduce scene coverage (bottom left) by only classifying regions essential in terms of classification accuracy. The labels of the classified regions are then spread into all regions (bottom right). This sparsity increases efficiency by orders of magnitude, while also increasing the accuracy of the final result (bottom right vs. top left).</p><p>In the aftermath of SfM/MVS reconstruction processes arise recent efforts to make these 3D models widely applicable. An important step in that direction is to augment the models with semantic labels, i.e. to identify parts of the 3D data to belong to certain object classes (e.g. building, tree, car, etc), or object part classes (e.g. door, window, wheel, etc). Typically, the semantic labelling is carried out in all the overlapping images used for 3D reconstruction <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. This implies that many parts of the scene get labeled multiple times, resulting in a large computational overhead in the order of the redundancy of the image set. The runtime of semantic classification pipelines still lies between 10 s and 300 s per image <ref type="bibr" target="#b7">[8]</ref>. Worse, these speeds are reported for moderately sized images of 320 × 240 pixels, and not for the high-resolution megapixel-sized images common for SfM. The bottleneck of redundant labelling is not in the classification step <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>, but rather in feature extraction and description. Also, an extra step is needed after labelling the images, namely, to fuse the different labels of the same 3D patch in order to obtain a consistently labelled model.</p><p>We propose an alternative strategy to semantically label the 3D model. We start by producing the mesh model and then determine for each of its faces which single image is best suited to well capture the true semantic assignment of the face. Not only do we avoid to needlessly process a multitude of images for the same mesh face, but we also have the advantage that we can exploit both geometry (3D model) and appearance (image). Moreover, the accuracy of the semantic labelling will be shown to improve over that of multi-view labelling.</p><p>A somewhat similar problem is known from texture mapping or image-based rendering. There decisions have to be made about which image to use to render the local appearance of the model. As to avoid the texture getting blurred, it is also quite usual to look for the best source image among a set of possibilities. Most methods use criteria that are related to the size of the model patch in the image and the degree to which the view is orthogonal to the patch. One may expect to find the same criteria to dominate the choice in segmentation as well, but that intuition is misleading for our application, as we will also show.</p><p>On top of selecting a single view to get each face's label from, we speed the process up further by not providing explicit classification for all the faces. We will demonstrate that it suffices to do this for about 30% of the faces, whereas all remaining labels can be inferred from those that were extracted. Moreover, this second parsimony again increases the accuracy of labelling.</p><p>We demonstrate our semantic labelling approach for different street scenes. Yet the core of our method is general and can be applied to different types of scenes and objects. In keeping with the central goals of the paper, we achieve a speedup with about two orders of magnitude while improving the label accuracy. In summary our contributions are the following.</p><p>1. An alternative approach is proposed for multi-view semantic labelling, efficiently combining the geometry of the 3D model and the appearance of a single, appropriately chosen view -denoted as reducing view redundancy. 2. We show the beneficial effect of reducing the initial labelling to a well-chosen subset of discriminative surface parts, and then using these labels to infer the labels of the remaining surface. This is denoted as scene coverage. 3. As a result, we accelerate the labelling by two orders of magnitude and make a finer-grained labelling of large models (e.g. of cities) practically feasible. 4. Finally, we provide a new 3D dataset of densely labelled images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The research in the field of semantic segmentation has enjoyed much attention and success in the last years (+17% in 5 years on PASCAL <ref type="bibr" target="#b11">[12]</ref>). Yet most semantic segmentation approaches still rely on redundant independent 2D analysis. Only recently some dived into the 3D realm and exploit joining the domains.</p><p>In the 2D domain, the initial works dealt mostly with feature description and learning. <ref type="bibr" target="#b12">[13]</ref> introduced TextonBoost which exploits multiple texture filters with an effective boost learning algorithm. <ref type="bibr" target="#b13">[14]</ref> uses the output of the trained classifier as new feature input for training several cascades. Additional works included higher-order terms <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> and simplification by superpixels <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Others focused on better graphical models <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> or including detectors <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>None of the above focus on the scalability issue of large scenes and only operate on individual images. Pure 2D scalable semantic classification was addressed in <ref type="bibr" target="#b7">[8]</ref>, which reduces by nearest neighbor searching for images and superpixels.</p><p>For the 2D domain in streetside, where surfaces are more structured than in arbitrary scenes, fewer works have been carried out. <ref type="bibr" target="#b21">[22]</ref> pioneered the feel for architectural scene segmentation. <ref type="bibr" target="#b22">[23]</ref> carried out 2D classification with a generic image height prior. <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> both used streetside object detectors on top of local features to improve the classification performance. Yet classification is performed on 2D images. 3D is introduced only at a procedural level <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>.</p><p>[29] exploit temporal smoothness on highway scenes. The idea is that redundant time-adjacent frames should be consistently labeled, where assumption is that between frames the motion is not too strong (always forward looking and high-frame rate) and scene content is redundantly present.</p><p>For the 3D domain in streetside, <ref type="bibr" target="#b4">[5]</ref> were the first to combine sparse SfM and semantic classification. <ref type="bibr" target="#b2">[3]</ref> interleaved 2.5D depth estimation and semantic labelling. In these lines <ref type="bibr" target="#b29">[30]</ref> used dense 2.5D depth images for classification and <ref type="bibr" target="#b30">[31]</ref> used semantic segmentation for deciding where to use 2.5D depth for plane fitting. <ref type="bibr" target="#b31">[32]</ref> again worked only on sparse 3D data and yet provides a method for linking these different densities of the full 2D image and sparse 3D domain. <ref type="bibr" target="#b3">[4]</ref> classified 2D images and then aggregated their labels to provide an overhead map of the scene. This uses a homography assumption to aggregate the birdseye map of the scenes. Most accuracy problems arise because of occlusions and averaging of multiple views. Recently, <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> combined the creation of geometry with the semantic labelling implicitly evaluating all data redundantly.</p><p>Most related to our baseline are the works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> who used 3D meshes to directly label 3D scenes. This has the benefit of using 3D features and operating in one place to fuse the classification yet still requires description and classification. <ref type="bibr" target="#b9">[10]</ref> showed how a common 3D classification can speed up the labelling over redundant 2D classification. <ref type="bibr" target="#b8">[9]</ref> introduced decision tree fields for 3D labelling to learn which pairwise connections are important for efficient inference.</p><p>Yet in summary, all of the 3D semantic research uses all data redundantly. All images are fully analyzed, described and all its features classified.</p><p>Related work for the view selection has only been carried out on an image level. Before SfM, the visual graphs are analyzed and clustered for iconic scenes <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> to split the data into coherent scene parts. After SfM, camera and geometry information are used to select clusters and non-redundant views -again only at the image level <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>Our work is inspired by the related world of 3D model texturing, where the goal is to find an optimal single texture file for a 3D model <ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref>. Usually, for finding the single best texture, the largest projection in terms of area size or most fronto-parallel view is used in addition to lighting constancy constraints.</p><p>We propose to change this paradigm and only analyze the most discriminative views of the data. To the best of our knowledge, we are the first to actively exploit this redundancy in a multi-view semantic labeling. Further, we propose a novel view to select the best such view by selecting the best view according to its ability to classify the scene correctly.</p><p>A further note on 3D datasets, most related work only shows examples on small outdoor scenes of single coarse buildings or small indoor scenes like the NYU 3D scenes <ref type="bibr" target="#b39">[40]</ref>. The datasets for semantic streetside labeling consist of very few coarse labels (building, vegetation, road) and do not focus on the details of the scenes. For example, datasets like Leuven <ref type="bibr" target="#b2">[3]</ref>, Yotta <ref type="bibr" target="#b3">[4]</ref>, CamVid <ref type="bibr" target="#b4">[5]</ref> and the KITTI <ref type="bibr" target="#b5">[6]</ref> labelled for semantics by <ref type="bibr" target="#b9">[10]</ref> only contain these coarse scenes In this work, we move to finely detailed ground truth labels including building detail such as windows, doors, balconies, etc. Further, the dataset is used for SfM with high resolution images of 1-3 megapixels and pixel-accurate dense labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">3D Surface and Semantic Classification</head><p>Our final goal is to label each part of the scene -a 3D mesh surface -by detailed semantic labels (wall, window, door, sky, road, etc). We briefly describe the multi-view reconstruction methods to obtain the surface, the cues for semantic scene labelling, and then dive into the multi-view scene labelling problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-view Surface Reconstruction</head><p>Our input is a set of images which are initially fed to standard SfM/MVS algorithms to produce a mesh. SIFT features <ref type="bibr" target="#b40">[41]</ref> are extracted and matched across the images, and reconstructed along with the cameras by using incremental bundle adjustment <ref type="bibr" target="#b41">[42]</ref>. The estimated views are clustered and used to compute depth maps via dense MVS. Volumetric fusion is performed by tetrahedral partitioning of space over the obtained dense 3D point cloud, and by exploiting point-wise visibility information in a voting scheme <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>. The final surface is recovered using a robust volumetric graph cuts optimization <ref type="bibr" target="#b44">[45]</ref>.</p><p>The output of the reconstruction procedure is the set of cameras C = {c j } and a surface mesh M, which consists of a set of 3D vertices, a set of face edges and a set of triangular faces F = {f i }. Since we are about to assign semantic labels to faces f i , we will represent this mesh as a graph, where nodes correspond to mesh faces and edges correspond to face adjacencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Heavy vs. Light Features for Semantic Labelling</head><p>For semantic labelling, we extract simple 2D image and geometric features. The typical approach is to extract features for every location of every single image in the dataset. We deviate from this dense computational scheme to a sparse computation, which is a main contribution of this paper.</p><p>In contrast to related work <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, we split the features into two sets. The first set consists of features that will take longer time to compute:</p><formula xml:id="formula_0">X heavy = (L * , a * , b * , t, h, d, n),<label>(1)</label></formula><p>This is a 16-dimensional feature vector containing the CIELAB Lab * color components, 8 responses of the MR8 filter bank <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref> in vector t, the height h defined as the distance from the ground plane, the depth d w.r.t. the dominant plane (e.g. facade plane), and the surface normal n, shown in Figure <ref type="figure" target="#fig_1">3</ref>. One could use additional features here, e.g. dense SIFT, etc. See <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16]</ref> for inspiration.</p><p>To aggregate features over the projection of a face f ∈ F in any observing camera c, we use Sigma Points <ref type="bibr" target="#b47">[48]</ref>, which efficiently capture the first two statistical moments of the feature vectors.</p><p>The second set contains only lightweight features:</p><formula xml:id="formula_1">X light = (A 2D , A 3D , A 2D /A 3D , α),<label>(2)</label></formula><p>where A 3D is the area of a mesh face f ∈ F, A 2D is the area of its 2D projection in a specific camera c ∈ C, and α is the angle of observation of the face from c. It should be emphasized that X heavy relies on image content, whereas X light relies on geometric information only. In practice, calculation of X light takes only a fraction of the time (120 seconds for all 1.8 million faces and 428 camera views vs. 21+ hours needed to calculate X heavy for the Full428 dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-view Optimization for 3D Surface Labelling</head><p>We define a mesh graph G M = (F , E), where the nodes represent the triangular faces F = {f i } of the surface mesh M, and E is the set of graph edges, which encode 3D adjacencies between the faces. We aim to assign a label x i from the set of possible semantic labels L = {l 1 , l 2 , . . . , l L } to each of the n faces f i . A possible complete labelling of the mesh is denoted by x = (x 1 , x 2 , . . . , x n ).</p><p>A Conditional Random Field (CRF) is defined over this graph and we aim to find the Maximum-A-Posteriori (MAP) labelling x * of the surface mesh M. This is equivalent to an energy minimization problem of the general form</p><formula xml:id="formula_2">x * = argmin x∈L n E(x),</formula><p>which we solve by efficient multi-label optimization, namely, the alpha-expansion graphcuts <ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref>. Our energy consists of unary data terms for every face f i , and pairwise regularity terms for every pair of adjacent faces (f i , f j ).</p><formula xml:id="formula_3">E(x) = n fi∈F cj∈C Θ(f i , c j , x i ) + λ • (fi,fj )∈E Ψ (f i , f j , x i , x j )<label>( 3 )</label></formula><p>where cj Θ(f i , c j , x i ) is the potential (penalty) for face f i obtaining label x i . Θ(f i , c j , x i ) is a per-view subterm, which relies on the single specific projection (an observation) of face f i into view c j . It can be written as the log-likelihood</p><formula xml:id="formula_4">Θ(f i , c j , l) = -log p(l | X ij ),<label>(4)</label></formula><p>where X ij = X (f i , c j ) denotes the feature vector associated to the projection of face f i into camera c j , and p(l | X ij ) is the likelihood of label l ∈ L for this particular projection of the face. In our scenario, the likelihoods p(l | X) are provided by a random forest classifier trained on ground truth labels using the features described in Section 3.2.</p><p>The pairwise potential Ψ (f i , f j , x i , x j ) in Eq. 3 enforces spatially smooth labelling solutions over the mesh faces by penalizing occurrences of adjacent faces f i and f j obtaining different labels (x i = x j ). We use a Potts model</p><formula xml:id="formula_5">Ψ (x i , x j ) = 0 if x i = x j ∇ if x i = x j , (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where ∇ = 1 is a constant penalty. In the future, we plan to weight ∇ in function of the dihedral angles or plane distances between neighboring faces. The coefficient λ in Eq. 3 controls the balance between unary and pairwise, data and smoothness terms. A grid search showed that λ = 0.5 works best. Now for the fun part, it should be emphasized that each triangle f i is typically observed from multiple cameras c j . This redundant set of observations poses a computational challenge when extracting the feature vectors X (f i , c j ) over all views c j and for each face f i . In the classical formulation, every view is considered and the final unary potential is aggregated over all views (see the second sum over the camera set C in the unary term of Eq. 3). In our findings, this is unnecessary. In the following section we describe our model of view importance and how it can be used to reduce the redundant set of views to the single most discriminative view for a more efficient semantic scene classification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Multi-view Observation Importance</head><p>In a multi-view scenario redundancy is inherent due to the view overlaps needed for SfM/MVS. Prior work ignored the relationship between these views. In turn, we start by defining two characteristics of the computational burden. First, view redundancy R i is the number of redundant camera views a mesh face f i is observed in. See the top of Figure <ref type="figure">7</ref> and Table <ref type="table" target="#tab_0">1</ref> for some typical average view redundancy values. Each triangle of the scene is visible in up to 50 cameras ( R = 49) on average! We aim for zero view redundancy (R i ≡ 0, ∀f i ).</p><p>Second, we define (prior) scene coverage S as the percentage of mesh faces used for feature extraction and semantic classification. Traditionally, the entire scene is classified (S = 100%). However, small areas or parts of homogeneous areas may not need to be classified individually, as the graphcut optimization in Section 3 is capable of spreading the correct labelling into these regions from "covered" regions, i.e. regions where the unaries in Eq. 3 are actually evaluated.</p><p>Our method aims at reducing both the view redundancy and the scene coverage for an efficient classification, while also improving accuracy. An initial idea could be to use a single global texture by fusing all images, and to only use this texture for extracting and classifying the heavy features. However, as we will show, the visually best texture is not always the best for semantic classification. Hence, we avoid using a fused texture, and rather keep the rich multi-view environment to decide which views are discriminative, yet before classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ranking Observations by Importance</head><p>In this section, we are looking for the most discriminative view per mesh face in terms of semantic classification. Since SfM also delivers the exact camera models C = {c j }, we can accurately relate each 3D surface element f i (triangular mesh face) to each of the views c j , as shown in Figure <ref type="figure" target="#fig_2">4</ref>. For efficiency, we aim to eliminate observations which are redundant or less important.</p><p>For this, we introduce the term observation importance I, which deviates from the existing paradigms of pairwise view clustering and ranking. In our work, we require a relationship to the 3D scene, and define I ij per observation of a mesh face f i in any camera c j . Furthermore, our observation importance ranks according to usefulness for final semantic scene classification rather than for camera clustering or texturing.</p><p>Inspired by its success in texture mapping, we will rank the views by the simple texture features such as area and angle. However generally, we define a ranking function that weights the cheap geometric cues for predicting the likelihood of the final classifier performance. The goal is to rank each triangle projection without the heavy feature set. Our importance rank is defined as</p><formula xml:id="formula_7">I ij = p(f i is classified correctly in c j |X light ij ). (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>We learn to regress these probabilities, by requiring that I ij correlates with view and face-wise classification accuracies resulted from the classical scenario, i.e. when all views and all faces are used to extract all features. A view c j is reliable for classifying face f i if the semantic label x * i = argmin l∈L Θ(f i , c j , l) equals the ground truth label. Hence, for the training set, we extract all features and classify all observations of every mesh face. This provides binary labels for reliability (correct/incorrect). We use these and the features X light (including, e.g. area A 2D ij , observation angle α ij ) to train a meta-classifier. For this, we use random forests again and, according to Eq. 6, we use the final leaf probability, i.e. classifier confidence, as a measure of the importance I ij . Intuitively, views c j with small apparent area A 2D ij of face f i , or views observing the face from a sharper angle α ij should be less reliable. For completeness, we also experimented using individual features, such as area A 2D ij , angle α ij , class likelihood Θ defined in Eq. 4, or its entropy H[Θ], to replace the importance I ij .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Reducing View Redundancy and Scene Coverage</head><p>For both characteristics -view redundancy and scene coverage -we use the observation ranking in Eq. 6 to remove redundant views.</p><p>For view redundancy, we optimize for the best observation c * (f i ) of each face f i over all views c j ∈ C. This simplifies the energy function in Eq. 3 to</p><formula xml:id="formula_9">E R (x) = fi∈F Θ(f i , c * (f i ), x i ) + . . . , with c * (f i ) = argmax ∀cj∈C (I ij ),<label>(7)</label></formula><p>where we select only the maximally informative view per triangle instead of merging unary potentials from all observations. Thus, X heavy only needs to be extracted, described and classified in these most informative views.</p><p>For scene coverage, we only classify a subset of all triangles that are present in the surface mesh. We choose for each face f i the most informative view c * (f i ) having importance I i * . We then rank faces according to their values I i * and only use the set of top k faces F k ⊂ F for further heavy feature extraction, rather than the full set F . This further simplifies the energy to which contains unary potentials for only the top k mesh faces, i.e. we set the unaries of all remaining faces to zero. The smoothness term will take care of propagating labels into these areas. An optimal labelling over the complete face set F defines our final labelling solution (see bottom right of Figure <ref type="figure">1</ref>). This is where we again deviate from existing approaches, which evaluate all potentials as they have no means to rank them. Only a recent work <ref type="bibr" target="#b10">[11]</ref> introduced the so-called Expected Label Change (ELC) ranking after sampling where to evaluate Θ and running full optimization multiple times. In a multi-view scenario, our methods avoids such a redundant graphcut optimization to estimate the ranking, as we propose the light geometric features to directly estimate the ranking.</p><formula xml:id="formula_10">E S (x) = fi∈F k Θ(f i , c * (f i ), x i ) + λ • (fi,fj )∈E Ψ (f i , f j , x i , x j ) ( 8 )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Evaluation</head><p>In this section we analyze the effect of eliminating view redundancy and reducing scene coverage at the classification stage. As shown below our method considerably reduces computational burden, while showing that we not only maintain but can also improve the final classification accuracy.</p><p>We divide our experiments into two investigations summarized in Figure <ref type="figure" target="#fig_0">2</ref> and Table <ref type="table" target="#tab_0">1</ref>. First, we evaluate various importance measures as detailed in Section 4.1 to find the most discriminative view per mesh face. Second, we evaluate the effect of reducing the scene coverage at the classification stage.</p><p>Our datasets consist of three outdoor urban scenes annotated with ground truth labels, such as road, wall, window, door, street sign, balcony, door, sky, sidewalk, etc. CamVid <ref type="bibr" target="#b4">[5]</ref> is a public dataset. We use its sequence 0016E5, which contains the most buildings and frames. Note that SfM/MVS was only stable for a subset sequence of 102 of its 300 frames. We introduce the larger ETHZ RueMonge 2014 dataset (short: Full428) showing 60 buildings in 428 images covering 700 meters along Rue Monge street in Paris. It has dense and accurate ground-truth labels (Figure <ref type="figure" target="#fig_0">2</ref>). Sub28 is a smaller set of 28 images showing four buildings. The CamVid dataset is taken from a car driving forward on a road (with an average viewing angle of 40°) while in the other two datasets the human camera man points more or less towards the buildings (avg. angle ≈ 10°).</p><p>We split each dataset into independent training and testing buildings of roughly 50% of the images and train using all observations of all triangles of the training set. We train both classifiers using a random forest <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref> because of its inherent abilities to handle multiple classes, label noise and non-linearity of the features. The number of trees is optimized to 10 and depth to 20 levels.</p><p>Please note that our method to reduce view redundancy and scene coverage is general and the speedup generalizes to other semantic classification pipelines. Hence, to study the exact differences, we use the graphcut optimization explained in Eq. ( <ref type="formula" target="#formula_3">3</ref>) over all views as our main baseline (see Table <ref type="table" target="#tab_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Single Discriminative Views -Zero Redundancy</head><p>In this first experiment, we determine the most discriminative measure for observation importance. We evaluate the measures in terms of semantic scene classification using PASCAL IOU accuracy averaged over all classes. Table <ref type="table" target="#tab_0">1</ref> is a summary of our findings. Please look in the supplemental material and website for more detailed results.</p><p>As one would expect, exploiting all of the view redundancy and averaging the classifier confidence from each observation (SUMALL) provides stable results. However, these approaches do not provide any speedup and require all the heavy features to be extracted over all observations. Yet calculating all potentials is the time consuming task, hence we focus on how to find the best observation from cheap geometric features only. The measures to rank are apparent face area A 2D (AREA), viewing angle α (ANGLE), and our importance in Eq. 6 (LEARN). From the evaluations, we have three conclusions. First, on average using the 2D projection area works better than the viewing angle. This is likely due to more robust statistics of larger areas and implicit preference for closer views, as the viewing angle is scale-invariant. Despite the challenging datasets of hugely varying appearance (training to testing performance drops roughly by 30%), other experiments show that the view invariance of the classifier is inherently quite high, which could further explain why the minimum angle is not as useful.</p><p>Second and surprisingly, our findings show that neither the largest 2D area nor the most fronto-parallel view deliver the best performance. Rows 10-14 in Table <ref type="table" target="#tab_0">1</ref> show the average area/angle to change several units for slightly better results. This gain is higher for CamVid because of the steep forward-looking camera and also because of the different semantic classes. For more detail over the class-averaged measures in Figure <ref type="figure" target="#fig_3">5</ref>, we also looked at classwise results for area. For all datasets, the classes captured by changing thin 3D surfaces (pole, fence, door, window, sign/pole, etc) experience a gain in accuracy with less frontal projections. These findings suggest that for these classes slanted views better capture the 3D structure.</p><p>Overall, our learned combination of the light features works best, since it can balance the distortion of the area and the extreme viewing angles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Reduction of Scene Coverage</head><p>In the second experiment, we investigate how many total mesh faces are really essential for good performance semantic classification in multi-view scenarios. Going one step further, we reduce the scene coverage and only select the top k triangles after selecting the most discriminative view per triangle.</p><p>Here our baselines are a) using all redundancies and the zero redundancy of b) area and c) angle -all at full coverage. The results are shown as average over all classes (top) and as classwise results (bottom) in Figure <ref type="figure">6</ref>. First conclusion is that the area is usually better at selecting the important triangles for coverage. Its curve climbs faster and overall its accuracy is higher, except for steep-angled CamVid dataset. Here the angle measure works better, and overall our learned importance combining the two is best.</p><p>Second conclusion may surprise again, we can even get better than the baselines at full coverage (dashed lines)! This is explained by the smaller classes (which occur less frequently and cover less space). Not sampling these early, removes competition for the large classes, which perform much better here. Hence, it is the size of the area that matters. As the importance measure is less good at the early coverage (below 10% coverage), we visualized the three measures and learned that the area is spread across the scene where our learned ranking focuses more on high confidence classes like building and road.</p><p>Third and most important conclusion, for large classes it is enough to use 10% of the scene coverage to reach the baselines. Overall, around 30% scene coverage stable results are obtained for all classes. This means that 70% of the potentials usually calculated for semantic scene segmentation are not necessary. The same accuracy can be achieved by using our proposed observation importance and optimization over the graph neighborhood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this work we investigated methods for reducing the inherent data overlap of multi-view semantic segmentation. As the speeds for other parts have been improved, the bottleneck is the redundant feature extraction and classification.</p><p>By exploiting the geometry and introducing single discriminative views per detailed scene part (a triangle), we avoid the redundancy and only classify a single time. This provides a speedup in the order of the data redundancy.</p><p>Further, we showed that simple features used for texture mapping are not best when the goal is semantic scene classification. Our learned importance better combines the features like area and viewing angle and improves the ranking.</p><p>Lastly, we proposed further efficiency by reducing the scene coverage and classifying only 30% of the scene and still obtain accurate labels for the entire scene. All in all, after reducing the redundancy and coverage we even increase the overall accuracy.</p><p>For future work we noticed that the overall accuracy of the scene classification depends on the resolution of this mesh as too large triangles cover semantic units and small triangles are not reliable for classification. Hence we plan to find the best resolution and rank even features in terms of the their computational effort.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Dataset overview -most are coarsely labelled at low resolution. We use a pixel accurate labelling with fine details at 1-3 megapixel resolution. (rightmost).</figDesc><graphic coords="5,320.70,56.58,55.33,66.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Features like color and gradient filters are expensive since they are densely calculated in the entire image. Geometry-based are more light-weight. Extra features like denseSIFT should improve the baseline, yet are even heavier to calculate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Geometric link between 3D model and 2D image space. Contrary to related work in view clustering, we look for the best view c * (fi) per mesh triangle fi. For small viewing angles the texture is visually pleasing but not best for semantic classification.</figDesc><graphic coords="8,64.05,51.06,59.29,63.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Removing View Redundancy: showing accuracy for the single k-th ranked feature on x-axis (e.g. 1st largest area, 10th smallest angle, 4th learned importance) and average feature value (red dash). The smaller the area or the larger the angle, the worse performance gets. Our learned performance captures the combination of area and angle better. This is CamVid, other datasets are in supplemental material.</figDesc><graphic coords="12,60.54,56.61,103.66,85.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. Reducing Scene Coverage: showing accuracy over percentage of selected triangles within graph optimization. Dashed lines are accuracy at full coverage (allviews, maxarea, minangle, importance). On average 30% are sufficient to label the entire scene as correctly as 100% coverage! Last rows show classwise results (see text for details). a. Full428 (zooms below) b. Sub28 (failure ) c. CamVid (SfM part) Datasets</figDesc><graphic coords="13,74.58,134.76,93.34,76.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Summary of all results (details in supplemental). Semantic Segmentation accuracy (PASCAL IOU in %) for Full428, Sub28 and CamVid102 datasets. By reducing redundancy to zero and also scene coverage to 1/6th, we speedup by 2 orders of magnitude. Ranking by area is better than angle yet the 1st ranks are not best (bold).</figDesc><table><row><cell></cell><cell></cell><cell>Full428</cell><cell>Sub28</cell><cell cols="2">CamVid102 Description</cell></row><row><cell>Stats</cell><cell></cell><cell>1794k 428 (8) 9 ± 3</cell><cell>185k 28 (8) 8 ± 2</cell><cell>46k 102 (11) 50 ± 27</cell><cell># Triangles # Images (# Categories) Redundancy</cell></row><row><cell></cell><cell></cell><cell>35.77</cell><cell>26.05</cell><cell>42.61</cell><cell>MAP SUMALL (λ = 0)</cell></row><row><cell>Baseline</cell><cell>Eq. (3)</cell><cell>35.25 35.57 37.33 37.82</cell><cell>25.13 25.19 26.63 26.93</cell><cell>29.25 33.21 50.80 36.73</cell><cell>MAP MINENTROPY (λ = 0) MAP BESTPROB (λ = 0) GC SUMALL (baseline) GC MINENTROPY ∀Cj</cell></row><row><cell></cell><cell></cell><cell>38.27</cell><cell>25.42</cell><cell>37.31</cell><cell>GC MAXPROB ∀Cj</cell></row><row><cell>SingleView</cell><cell>Eq. (7)</cell><cell cols="4">37.38 (8px) 26.09 (18px) 52.19 (135px) Ranked 1st GC AREA (avg) 37.38 (8px) 26.60 (15px) 54.60 (62px) Ranked 4th GC AREA (avg) 35.73 (9°) 25.64 (8°) 47.84 (37°) Ranked 1st GC ANGLE (avg) 36.06 (15°) 26.34 (24°) 50.04 (41°) Ranked 4th GC ANGLE (avg) 37.04 (0.19) 26.19 (0.49) 52.62 (0.70) Ranked 1st GC LEARN (avg)</cell></row><row><cell></cell><cell></cell><cell cols="4">37.64 (0.18) 26.86 (0.47) 56.01 (0.63) Ranked 4th GC LEARN (avg)</cell></row><row><cell>Coverage</cell><cell>Eq. (8)</cell><cell cols="4">38.37 (15%) 28.28 (27%) 61.07 (35%) Best Accuracy (AREA) 37.68 (14%) 26.39 (12%) 57.08 (20%) 1st as Baseline (AREA) 35.73 (35%) 26.83 (74%) 54.37 (16%) Best Accuracy (ANGLE) 35.67 (35%) 25.76 (22%) 52.20 (13%) 1st as Baseline (ANGLE)</cell></row><row><cell></cell><cell></cell><cell cols="4">37.08 (35%) 27.97 (40%) 60.57 (31%) Best Accuracy (LEARN)</cell></row><row><cell></cell><cell></cell><cell cols="4">36.15 (33%) 25.96 (34%) 52.98 (13%) 1st as Baseline (LEARN)</cell></row><row><cell></cell><cell></cell><cell>1280min</cell><cell>88min</cell><cell>184min</cell><cell>TIME Full View Redundancy</cell></row><row><cell></cell><cell></cell><cell>11.9x</cell><cell>8.6x</cell><cell>52.6x</cell><cell>SPEEDUP Zero Redundancy</cell></row><row><cell>Timing</cell><cell></cell><cell>108min 7.1x 15min</cell><cell>10.2min 8.3x 1.2min</cell><cell>3.5min 5.0x 0.7min</cell><cell>TIME Zero Redundancy SPEEDUP 1st Coverage as Eq. (3) TIME 1st Coverage as Eq. (3)</cell></row><row><cell></cell><cell></cell><cell>85x</cell><cell>72x</cell><cell>262x</cell><cell>SPEEDUP Overall</cell></row><row><cell></cell><cell></cell><cell>+1.04%</cell><cell>+1.65%</cell><cell cols="2">+11.81% GAIN Overall (absolute)</cell></row><row><cell></cell><cell></cell><cell>103%</cell><cell>106%</cell><cell>124%</cell><cell>GAIN Overall (relative)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported by the European Research Council (ERC) under the project VarCity (#273940) at www.varcity.eu.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Size does matter: Improving object recognition and 3D reconstruction with cross-media analysis of image clusters</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gammeter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Quack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tingdahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2010, Part I</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Maragos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6311</biblScope>
			<biblScope unit="page" from="734" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeling and Recognition of Landmark Image Collections Using Iconic Scene Graphs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2008, Part I</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5302</biblScope>
			<biblScope unit="page" from="427" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint Optimization for Object Class Segmentation and Dense Stereo Reconstruction. Intern</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sturgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bastanlar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Clocksin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="122" to="133" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic dense visual semantic mapping from street-level imagery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sturgees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intern. Conf. on Intelligent Robots Systems</title>
		<meeting>Intern. Conf. on Intelligent Robots Systems</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Segmentation and recognition using structure from motion point clouds</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2008, Part I</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5302</biblScope>
			<biblScope unit="page" from="44" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">What, Where and How Many? Combining Object Detectors and CRFs</title>
		<author>
			<persName><forename type="first">Ľ</forename><surname>Ladický</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sturgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2010, Part IV</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Maragos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6314</biblScope>
			<biblScope unit="page" from="424" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SuperParsing: Scalable Nonparametric Image Parsing with Superpixels. Intern</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="329" to="349" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient 3D Scene Labeling Using Fields of Trees</title>
		<author>
			<persName><forename type="first">O</forename><surname>Koehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intern. Conf. on Computer Vision, ICCV</title>
		<meeting>IEEE Intern. Conf. on Computer Vision, ICCV</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shahrokni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition, CVPR</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition, CVPR</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Active MAP Inference in CRFs for Efficient Semantic Segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>De Nijs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intern. Conf. on Computer Vision, ICCV</title>
		<meeting>IEEE Intern. Conf. on Computer Vision, ICCV</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html" />
		<title level="m">The PASCAL Visual Object Classes Challenge (VOC 2012) Results</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">textonBoost: Joint appearance, shape and context modeling for multi-class object recognition and segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2006, Part I</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3951</biblScope>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Auto-context and its application to high-level vision tasks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust higher order potentials for enforcing label consistency. Intern</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="324" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Associative Hierarchical CRFs for Object Class Image Segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intern. Conf. on Computer Vision, ICCV</title>
		<meeting>IEEE Intern. Conf. on Computer Vision, ICCV</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantic image classification using consistent regions and individual context</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kluckner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mauthner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conference</title>
		<meeting>British Machine Vision Conference</meeting>
		<imprint>
			<publisher>BMVC</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-class segmentation with relative location prior. Intern</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Elidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="300" to="316" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stacked Hierarchical Labeling</title>
		<author>
			<persName><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2010, Part VI</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Maragos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6316</biblScope>
			<biblScope unit="page" from="57" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kraehenbuehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A dynamic conditional random field model for joint labeling of object and scene classes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2008, Part IV</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5305</biblScope>
			<biblScope unit="page" from="733" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Parsing images of architectural scenes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Grabler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intern. Conf. on Computer Vision, ICCV</title>
		<meeting>IEEE Intern. Conf. on Computer Vision, ICCV</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiple view semantic segmentation for street view images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intern. Conf. on Computer Vision, ICCV</title>
		<meeting>IEEE Intern. Conf. on Computer Vision, ICCV</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Irregular lattices for complex shape grammar facade parsing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Krispel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Thaller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Donoser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Havemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fellner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Three-Layered Approach to Facade Parsing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Martinović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weissenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2012, Part VII</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7578</biblScope>
			<biblScope unit="page" from="416" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Segmentation of building facades using procedural shape prior</title>
		<author>
			<persName><forename type="first">O</forename><surname>Teboul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koutsourakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition, CVPR</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition, CVPR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Parameterfree/pareto-driven procedural 3d reconstruction of buildings from ground-level sequences</title>
		<author>
			<persName><forename type="first">L</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teboul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koutsourakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Paragiosn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Procedural modeling of buildings</title>
		<author>
			<persName><forename type="first">P</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Haegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ulmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Intern. Conf. on Computer graphics and interactive techniques</title>
		<meeting>of the Intern. Conf. on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>SIGGRAPH</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint 2D-3D Temporally Consistent Semantic Segmentation of Street Scenes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Floros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semantic segmentation of urban scenes using dense depth maps</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2010, Part IV</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Maragos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6314</biblScope>
			<biblScope unit="page" from="708" to="721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Piecewise planar and non-planar stereo for urban scene reconstruction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gallup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition, CVPR</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition, CVPR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Co-inference for Multi-modal Scene Analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2012, Part VI</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7577</biblScope>
			<biblScope unit="page" from="668" to="681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Joint 3D Scene Reconstruction and Class Segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Haene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Angst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition, CVPR</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition, CVPR</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">3D Scene Understanding by Voxel-CRF</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intern. Conf. on Computer Vision, ICCV</title>
		<meeting>IEEE Intern. Conf. on Computer Vision, ICCV</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards Internet-scale Multiview Stereos</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition, CVPR</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition, CVPR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Overlapping camera clustering through dominant sets for scalable 3D reconstruction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mauro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leonardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conference</title>
		<meeting>British Machine Vision Conference</meeting>
		<imprint>
			<publisher>BMVC</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient View-Dependent Image-Based Rendering with Projective Texture-Mapping</title>
		<author>
			<persName><forename type="first">P</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Borshukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Rendering Workshop</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">3-D scene representation as a collection of images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Laveau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Faugeras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intern. Conf. on Computer Vision, ICCV</title>
		<meeting>IEEE Intern. Conf. on Computer Vision, ICCV</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">View interpolation for image synthesis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Intern. Conf. on Computer graphics and interactive techniques</title>
		<meeting>of the Intern. Conf. on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>SIGGRAPH</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Indoor Segmentation and Support Inference from RGBD Images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2012, Part V</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7576</biblScope>
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints. Intern</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Towards linear-time incremental structure from motion</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Intern. Symp. on 3D Data, Processing, Visualiz. and Transmission</title>
		<meeting>of Intern. Symp. on 3D Data, essing, Visualiz. and Transmission</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient Multi-View Reconstruction of Large-Scale Scenes using Interest Points, Delaunay Triangulation and Graph Cuts</title>
		<author>
			<persName><forename type="first">P</forename><surname>Labatut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Keriven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intern. Conf. on Computer Vision, ICCV</title>
		<meeting>IEEE Intern. Conf. on Computer Vision, ICCV</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">High Accuracy and Visibility-Consistent Dense Multi-view Stereo</title>
		<author>
			<persName><forename type="first">V</forename><surname>Hiep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Labatut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Keriven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="889" to="901" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>PAMI)</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-View Reconstruction Preserving Weakly-Supported Surfaces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jancosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition, CVPR</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition, CVPR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A statistical approach to texture classification from single images. Intern</title>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="61" to="81" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fast Anisotropic Gauss Filtering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Geusebroek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="938" to="943" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Semantic classification in aerial imagery by integrating appearance and height information</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kluckner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mauthner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV 2009, Part II</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R.-I</forename><surname>Taniguchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Maybank</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">5995</biblScope>
			<biblScope unit="page" from="477" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Fast approximate energy minimization via graph cuts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1222" to="1239" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="124" to="1137" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">What energy functions can be minimized via graph cuts?</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="159" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>PAMI)</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Shape quantization and recognition with randomized trees</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>August</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1545" to="1588" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
