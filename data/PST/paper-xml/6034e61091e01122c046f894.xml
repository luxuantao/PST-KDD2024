<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BayesPerf: Minimizing Performance Monitoring Errors Using Bayesian Statistics</title>
				<funder ref="#_3zjNJET #_5tTkM2e #_G2UAve5">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder>
					<orgName type="full">Xilinx</orgName>
				</funder>
				<funder ref="#_4f4Qs8g">
					<orgName type="full">IBM-ILLINOIS Center for Cognitive Computing Systems Research</orgName>
				</funder>
				<funder>
					<orgName type="full">Intel</orgName>
				</funder>
				<funder ref="#_SzyHEtM #_MUNfSMd #_gDFhS5x">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">IBM</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-02-22">22 Feb 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Subho</forename><forename type="middle">S</forename><surname>Banerjee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign Urbana</orgName>
								<address>
									<settlement>Illinois</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Saurabh</forename><surname>Jha</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign Urbana</orgName>
								<address>
									<settlement>Illinois</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zbigniew</forename><forename type="middle">T</forename><surname>Kalbarczyk</surname></persName>
							<email>kalbarcz@illinois.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Illinois at Urbana-Champaign Urbana</orgName>
								<address>
									<settlement>Illinois</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ravishankar</forename><forename type="middle">K</forename><surname>Iyer</surname></persName>
							<email>rkiyer@illinois.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Illinois at Urbana-Champaign Urbana</orgName>
								<address>
									<settlement>Illinois</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BayesPerf: Minimizing Performance Monitoring Errors Using Bayesian Statistics</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-02-22">22 Feb 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2102.10837v1[cs.DC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Performance Counter</term>
					<term>Sampling Errors</term>
					<term>Error Detection</term>
					<term>Error Correction</term>
					<term>Probabilistic Graphical Model</term>
					<term>Accelerator</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hardware performance counters (HPCs) that measure low-level architectural and microarchitectural events provide dynamic contextual information about the state of the system. However, HPC measurements are error-prone due to non determinism (e.g., undercounting due to event multiplexing, or OS interrupt-handling behaviors). In this paper, we present BayesPerf, a system for quantifying uncertainty in HPC measurements by using a domain-driven Bayesian model that captures microarchitectural relationships between HPCs to jointly infer their values as probability distributions. We provide the design and implementation of an accelerator that allows for low-latency and low-power inference of the BayesPerf model for x86 and ppc64 CPUs. BayesPerf reduces the average error in HPC measurements from 40.1% to 7.6% when events are being multiplexed. The value of BayesPerf in real-time decision-making is illustrated with a simple example of scheduling of PCIe transfers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS Concepts</head><p>? General and reference ? Performance; Measurement; ? Hardware ? Error detection and error correction; Hardware accelerators; ? Computing methodologies ? Learning in probabilistic graphical models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Hardware performance counters (HPCs) are widely used in profiling applications to characterize and find bottlenecks in application performance. Even though HPCs can count hundreds of different types of architectural and microarchitectural events, they are limited because those events are collected (i.e., multiplexed) on a fixed number of hardware registers (usually 4-10 per core). As a result, they are error prone because of application, sampling, and asynchronous collection behaviors borne out of multiplexing. Such behavior in HPC measurements is not a new problem, and has been known for the better part of a decade <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>Targeted Need. Traditional approaches of tackling HPC errors have relied on collecting measurements across several application runs, and then performing offline computations to (i) impute missing or errored measurements with new values (e.g., <ref type="bibr" target="#b42">[43]</ref>); or (ii) dropping outlier values to reduce overall error (e.g., <ref type="bibr" target="#b28">[29]</ref>). Both of these require time and compute resources for collecting training data and inference, thus are suitable for offline analysis (like profiling). These techniques are untenable in emergent applications that use HPCs as inputs to complete a feedback loop and make dynamic real-time decisions that affect system resources using a variety of machine learning (ML) methods. Examples include online performance hotspot identification (e.g., <ref type="bibr" target="#b13">[14]</ref>), userspace or runtime-level scheduling (e.g., <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b47">48]</ref>), and power and energy management (e.g., <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40]</ref>), as well as attack detectors and system integrity monitors <ref type="bibr" target="#b7">[8]</ref>. In such cases, the HPC measurement errors propagate, get exaggerated, and can lead to longer training time and poor decision quality (as illustrated in ?6.3). This is not surprising because ML systems are known to be sensitive to small changes in their inputs (e.g., in adversarial ML) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24]</ref>. As we will show in ?2, HPC measurement errors can be large (as much as 58%); hence they must be explicitly handled.</p><p>This paper presents BayesPerf, a system for quantifying uncertainty and correcting errors in HPC measurements using a domaindriven Bayesian model that captures micro-architectural relationships between HPCs. BayesPerf corrects HPC measurement errors at the system (i.e., CPU and OS) level, thereby allowing the downstream control and decisions models that use HPCs to be simpler, faster and use less training data (if used with ML). The proposed model is based on the insight that even though individual HPC measurements might be in error, groups of different HPC measurements that are related to one another can be jointly considered-to reduce the measurement errors-using the underlying statistical relationships between the HPC measurements. We derive such relationships by using design and implementation knowledge of the microarchitectural resources provided by CPU vendors <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref>. For example, the number of LLC misses, the size of DMA transactions, and the DRAM bandwidth utilization are related quantities, <ref type="foot" target="#foot_0">1</ref> and can be used to reduce measurement errors in each other.</p><p>Approach &amp; Contributions. The key contributions are:</p><p>(1) The BayesPerf ML Model. We present a probabilistic ML model that incorporates microarchitectural relationships to combine measurements from several noisy HPCs to infer their true values, as well as quantify the uncertainty in the inferred value due to noise. Hence allowing:</p><p>(a) improving decision-making with explicit quantification of HPC measurement uncertainty. (b) reduced need for aggressive (high-frequency) HPC sampling (which negatively impacts application performance) to capture high-fidelity measurements, thereby increasing our observability into the system. (2) The BayesPerf Accelerator. To enable the use of BayesPerf ML model in latency-critical, real-time decision-making tasks, this paper presents the design and implementation of an accelerator for Monte Carlo-based training and inference of the BayesPerf model. The accelerator exploits (a) high-throughput random-number generators. (b) maximal parallelism based on the statistical relationships mentioned above, to rapidly sample multiple parts of the BayesPerf model in parallel. (3) A Prototype Implementation. We describe an FPGA-based prototype implementation of the BayesPerf system (on a Xilinx Virtex 7 FPGA) for Linux running on Intel x86_64 (Sky Lake) and IBM ppc64 (Power9) processors. The BayesPerf system is designed to provide API-compatibility with Linux's perf subsystem <ref type="bibr" target="#b26">[27]</ref>, allowing it to be used by any userspace performance monitoring tool for both x86_64 and ppc64 systems. Our experiments demonstrated that BayesPerf reduces the average error in HPC measurements from 40.1% to 7.6% when events are being multiplexed, which is an overall 5.28? error reduction. Further, the BayesPerf accelerator provides an 11.8? reduction in power consumption, while adding less than 2% read latency overhead over native HPC sampling. ( <ref type="formula">4</ref>) Increasing training and model efficiency of decision-making tasks.</p><p>We demonstrate the generality of the BayesPerf system by integrating it with a high-level ML-based IO scheduler that controls transfers over a PCIe interconnect. We observed that the training time for the scheduler was reduced by 37% (~52 hr reduction) and the average makespan of scheduled workloads decreased by 19%. The remainder of the paper is organized as follows. First in ?2, we discuss the sources of HPC measurement errors. Then in ?3 we provide an overview of the design of the BayesPerf system. ?4 describes the formulation, training and inference of the ML model used to correct errors. ?5 describes the accelerator that allows inference on the ML model in real-time. Then in ?6 we discuss a prototype implementation and it's evaluation. Finally, in ?7 and ?8, we put BayesPerf in perspective of traditional methods, and describe future challenges, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: HPC Errors</head><p>Every modern processor has a logical unit called the Performance Monitoring Unit (PMU), which consists of a set of HPCs. An HPC counts how many times a certain event occurs during a time interval of a program's execution. The number and configurability of the HPCs vary across processor vendors and microarchitectures. For example, modern Intel processors have three fixed HPCs (which measure ISA-related events) and eight programmable HPCs per core (which measure microarchitectural events and are split between the SMT threads on the core) <ref type="bibr" target="#b20">[21]</ref>. The events measured by an HPC are vendor-specific and microarchitecture-dependent, and vary with processor models within the same microarchitecture. For example,  an Intel Haswell CPU has 400 programmable events, compared to the 1623 events on a HaswellX CPU; both have the same number of HPC registers per core (three + eight) <ref type="bibr" target="#b47">[48]</ref>. Therefore, one must carefully pick and configure which events to monitor with the available registers.</p><p>Reading HPCs. Performance counters can be read using: (1) Polling: The HPCs can be read at any instant by using specific instructions to write (to configure the HPC) and read (to poll the value of an HPC) model-specific registers (MSRs) that represent HPCs. For example, x86_64 uses specific instructions to read (i.e., rdmsr) from and write (i.e., wrmsr) to MSRs, respectively; both instructions require OS-level access privilege, and hence are performed by the OS on behalf of a user. Here, one HPC is programmed to count only one event during the execution of a program. Hence, polling is ineffective, as the number of events that can be simultaneously measured is limited by the number of available hardware registers. (2) Sampling: HPCs also support sampling of counters based on the occurrence of events, thereby letting multiple events timeshare a single HPC <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32]</ref>. This feature is enabled through a specific interrupt, called the Performance Monitoring Interrupt (PMI), which can be generated after the occurrence of a certain number of events (i.e., a predetermined threshold). The interrupt handler then polls (i.e., samples) the HPC. The multiplexing of events occurs through a separate scheduling interrupt that is triggered periodically to change the configuration of the HPCs and swap events in and out. The collected measurements are generally scaled to account for the time they were not scheduled to a HPC <ref type="bibr" target="#b11">[12]</ref>, and that can lead to making erroneous measurements. Sampling is necessary due to the severe disparity between the numbers of events types and the number of counters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sources of Errors.</head><p>In addition to the errors due to event multiplexing, HPCs demonstrate other modalities of measurement error. For example, HPC measurements can vary across runs because of OS activity, scheduling of programs in multitasking environments, memory-layout, and memory-pressure, and varied multi-processor interactions may change between different runs. Nondeterminism in OS behavior (e.g., servicing of hardware interrupts) also plays a significant role in HPC measurement errors <ref type="bibr" target="#b43">[44]</ref>. Performance counters have also been shown to over count certain events on some processors <ref type="bibr" target="#b43">[44]</ref>. Finally, the implementation of userspace and OS-kernel-level tools can cause different tools to provide different measurements for the same HPCs in strictly controlled environments for the same application. The variations in measurements may result from the techniques involved in acquiring them, e.g.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Samples</head><p>Ensure Overlap Inf Inf Inf Inference the point at which they start the counters, the reading technique (polling or sampling), the measurement level (thread, process, core, multiple cores), and the noise-filtering approach used.</p><p>Measurement Errors. As a result of this non-determinism, quantifying error in HPCs is difficult as there is no way to get "ground truth" measurements because of inherent variations in measurements. In this paper, we define HPC error as magnitude of difference between corresponding HPC measurements made in two runs of a workload, one in polling and other in sampling mode. The correspondence between the two HPC traces (time-series) is established by dynamic time warping <ref type="bibr" target="#b4">[5]</ref> that calculates an "alignment" between the two time series datasets using edit-distance. <ref type="foot" target="#foot_1">2</ref>Fig. <ref type="figure" target="#fig_1">1</ref> illustrates the net effect of measurement errors on the fidelity of an HPC counter using Linux's perf subsystem. In this case, the baseline dataset is collected using polling, and the target dataset is collected using sampling, each on 10 independent application runs capturing both variations in a single run, and variations across runs. We observe a 58 ? 9.3% average error in HPC measurements when 35 on-core events are being multiplexed on an Intel processor, compared to the baseline of polling 4 events at a time. <ref type="foot" target="#foot_2">3</ref>Errors in Derived Events. Such high error is particularly troubling, as it is quite conceivable to count 35 events simultaneously, particularly for measuring derived events. Derived events are obtained by combining individual HPC measurements in a mathematical expression. Consider for example, the "Backend_Bound_SMT" derived event on Intel BroadwellX processor. It measures the fraction of ?ops issue slots utilized in a core, and alone takes measurements from 16 HPCs to compute <ref type="bibr" target="#b6">[7]</ref>. This information might be valuable in a OS-level scheduler that controls an SMT processor, with the objective of minimizing interference between CPU-bound processes/threads. Often such information would be conflated with other derived metrics like "Memory_Bound" and "Frontend_Bound_SMT", which together would require the use of 29 unique counters. That according to Fig. <ref type="figure" target="#fig_1">1</ref> would incur an average error of ~45%. This is further exasperated by the fact that the HPCs need to be counted per-SMT thread, per-core, and per-socket. For example, in an average 2-socket server system this would imply collecting thousands of counters (i.e., 2784 HPCs = 29 counters ? 24 cores ? 2 sockets).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach Overview</head><p>Key Insight. The key insight that drives this work is that microarchitectural invariants (e.g. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b40">41]</ref>) can be applied to measured HPC data to estimate whether it is, in fact, in error (i.e., a detector). Further, we can quantify the "uncertainty" of an HPC measurement by quantifying the probability of deviation from that invariant (i.e., its egregiousness). When the above is applied to a group of HPC measurements, each targeting different microarchitectural units, the underlying invariants can be composed, encoded as statistical relationships, i.e., joint probability distributions, which can then be composed into larger probabilistic graphical models. We then use a Bayesian inference approach to integrate the data and prior knowledge of the system to effectively attenuate the high error measurements and significantly amplify correct measurements, all in real-time. This works in practice as the number of HPCs with lower errors are generally more numerous than those with higher error (also verified by our observations), hence they bias the aggregate results to the lower errored values. As a result, BayesPerf significantly outperforms traditional purely data-driven statistical approaches for outlier detection.</p><p>BayesPerf ML Model. Below, we provide a high-level description of the model, using the example illustrated in Fig. <ref type="figure" target="#fig_2">2</ref>. In this example, the goal is to measure (by multiplexing) a set of events {? ? , . . . , ? ? }, on a set of HPCs {? 1 , ? 2 , ? 3 }.</p><p>Deciding Schedules of HPCs: BayesPerf first determines a schedule of how the events are multiplexed on the HPCs. The schedule consists of a set of HPC configurations that are collected over time. We define an HPC configuration as a mapping between counters and events, that defines which counters are collected at an instant of time. The notation {? 1 , ? 2 , ? 3 } = {? ? , ? ? , ? ? } is used to define such a configuration, and imply that ? 1 counts ? ? . The scheduling process is driven primarily by the microarchitectural considerations of the available HPCs and the types of events that each one can measure, i.e., as not all HPCs can measure all events. Traditional HPC measurement tools, like the Linux perf subsystem trigger HPC configuration changes in a round-robin manner, based on a periodic hardware timer-driven interrupt (see Fig. <ref type="figure" target="#fig_2">2</ref>). BayesPerf uses a similar interrupt driven approach, but does not use roundrobin to build a schedule of configurations. It creates configurations of overlapping counters, such that each set of counters have "statistical relationships" to other events in preceding and subsequently scheduled configurations. For example, in Fig. <ref type="figure" target="#fig_2">2</ref>, ? ? and ? ? are such overlapping events. As we will show in ?4, these "statistical relationships" can be derived based on microarchitectural invariants (i.e., domain knowledge) that tie together the resources underlying the measurements. BayesPerf encodes those invariants as generative jointand conditional-probability distributions for the processors used in our experiments.</p><p>Inferring Unscheduled Events: At each instant of time, BayesPerf then uses sampled data from the overlapping events to compute a full posterior distribution (i.e., the likely values and their associated uncertainties) of the unscheduled events using a Bayesian inference approach. Consider ? ? in the second time slice of Fig. <ref type="figure" target="#fig_2">2</ref>. It is calculated using its' own samples from the previous time slice and the samples of ? ? (which is the event repeated across time slice one and two) in the current time slice. The result of the Bayesian inference using the sampled data is a probability distribution Pr(? ? ? |? ? -1 ? , ? ? ? ) at time ?; this distribution not only gives us an estimate of ? ? (i.e., by finding the most likely value of ? ? under the distribution), but also quantifies uncertainty (i.e., using the probability value Pr(? ? | . . . )) in that estimate. The compositional nature of Bayesian inference allows chain events across multiple time slices, if the overall set of events to be measured is large, albeit at the cost of larger uncertainty in the estimate. For example, in Fig. <ref type="figure" target="#fig_2">2</ref> the chain of events (? ? ? ? ? ) ? (? ? ? ? ? ) ? (? ? ? ? ? ) can be used directly estimate ? ? from samples of ? ? , but also transitively estimate it from samples of ? ? . Here "?" describes the above statistical relationships between events in a configuration (i.e., in a single time slice), and "?" describes data collected between overlapping events across time slices.</p><p>The BayesPerf system then allows an user to poll the posterior probability distributions of any of the events being collected. These distributions can be passed along (i.e., integrated) into higher-level ML/control frameworks or used directly to compute error bounds of HPC measurements.</p><p>BayesPerf Accelerator. Though the BayesPerf ML model is able to provide significantly higher-quality samples from the raw HPC measurements, it introduces the additional runtime overhead of performing Bayesian inference on every new measurement polled by the user. Consider Fig. <ref type="figure" target="#fig_3">3</ref>; it shows the average overhead (over 100 reads) of reading a HPC value using the Linux kernel's (perf subsystem) read() system call (i.e., polling), the x86_64 rdpmc instruction to read HPCs in userspace, a purely CPU implementation of the BayesPerf ML model (using TensorFlow Probability <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b41">42]</ref>), an FPGA accelerated version of BayesPerf (described later in ?5), and CounterMiner <ref type="bibr" target="#b28">[29]</ref> (described later in ?6 and used as a baseline in our evaluation). We observe that a single HPC read when the CPU implementation of BayesPerf is being used has approximately 9? longer latency than native polling of the HPC. In order to reduce the latency, we introduce an accelerator that parallelizes the process of computing posterior inference on the BayesPerf ML model. The accelerator largely builds upon our prior work <ref type="bibr" target="#b2">[3]</ref> in building MCMC accelerators that treats a lack of statistical dependencies between variables as a scope for parallel execution. Using the accelerator, BayesPerf adds less than 2% overhead in read latency compared to the native solution. Our implementation of the accelerator on a PCIe-attached FPGA device can take advantage of modern cache-coherent accelerator-processor communication protocols like CAPI <ref type="bibr" target="#b38">[39]</ref>, and essentially provide users with the same interface and same performance characteristics they could get if they were natively polling the OS for HPC measurements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The BayesPerf ML Model</head><p>In this section we first discuss formalization of the HPCs and events for a generic CPU. Then, in ?4.1, we discuss the problem of scheduling sets of performance counters onto available HPCs. Finally, in ?4.3, we discuss an inference strategy to compute the posterior distribution of a single event based on generated schedule and HPC measurement samples.</p><p>Formalism. We assume that every processor has a predetermined number of fixed and programmable HPCs. We refer to them as ? ? and ? ? , respectively. The HPCs themselves are indexed and referred to as ? 1 . . . ? ? ? for the fixed HPCs and ? 1 . . . ? ? ? for the programmable HPCs. The processor as a whole has a set ? = {? 1 , . . . , ? ? ? } of ? ? architectural and microarchitectural events that are measured using ? * and ? * . At any point in time, the programmable HPCs are configured to count any one of the events in ?. The instantaneous mapping between counters and events is called a configuration. Fixed HPCs are not considered in a configuration, as they cannot be programmed. Not all programmable HPCs will be able to count all events (i.e., all configurations might not be valid), depending on microarchitectural and implementation considerations. For example, an Intel off-core response event requires one HPC and one MSR register, and the L1D_PEND_MISS.PENDING event can be only counted on the third HPC on Haswell/Broadwell systems. Configuration validity constraints are known ahead of time, can be dynamically checked, and must always be satisfied. BayesPerf uses the Linux's builtin validity checker.</p><p>A sample ? ? is generated from an HPC ? ? (i.e., an interrupt is fired to read the value of a counter and store it in memory) when a particular threshold ? ?,? is reached on one of the fixed HPCs ? ? . <ref type="foot" target="#foot_3">4</ref>That process is denoted by ? ? ? ? ? if ? ? ? ? ?,? . In addition to the value of the counter, the sampling process also records two time measurements, ? ? ? and ? ? ? , where ? ? ? ? ? ? ? . They correspond to the total time the application has been running, and the total time for which an event has been sampled (i.e., it has been enabled), respectively. Traditional approaches (e.g., one that is used in Linux) use these times to correct HPC undercounting errors and assume that the true value of a performance counter is scaled according to ? ? ? ? ? ? ? ? ? ?/? ? ? . Statistical Dependencies. Some subsets of events in ? have statistical relationships between them. Those statistical relationships are described by joint probability distribution functions. For example, if ? 1 and ? 2 share such a relationship, then it is represented by their joint probability distribution Pr(? 1 , ? 2 ; ?). Where, ? refers to all tunable or learnable parameters of the distribution.</p><p>We assert that if nothing is known about the statistical relationships between the events, then Pr(? ? , . . . ) can be approximated by a neural network and trained using data from HPCs. However, for most real systems, knowledge about the underlying microarchitectural resources being counted in a HPC can be correlated together to describe Pr(? ? , . . . ). To do so, we use algebraic models of the composition of HPC measurements by using information about the CPU microarchitecture found in processor performance manuals <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b44">45]</ref>. For example, in an Intel x86 Sandy Bridge microarchitecture <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b44">45]</ref>, the fraction of cycles a CPU is stalled because of DRAM access is given by (1 -Mem_L3_Hit_Frac) ? STALLS_L2_PENDING /CLKS. Those stalls can be caused by either DRAM bandwidth issues or DRAM latency issues, which in turn can be measured as ORO_DRD_BW_Cycles /CLKS, and ORO_DRD_Any_Cycles /CLKS -ORO_DRD_BW_Cycles /CLKS respectively. Here, ORO_DRD_Any_Cycles, ORO_DRD_BW_Cycles, Mem_L3_Hit_Frac, STALLS_L2_PENDING, and CLKS correspond to a set of fixed and programmable events, which are related to each other via the algebraic relations described above. Given the equivalence of those three computed quantities, we can compute one, given values of the other. When some of these events are reported with measurement errors, the equivalence relationship becomes statistical (i.e., capture randomness because of errors). We then define a distribution function for individual events, where only valid combinations of the event values have a non zero probability of occurrence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Scheduling</head><p>Problem. Given statistical dependencies between events, we need to ensure that the configurations created for two consecutive time slices (i.e., scheduler quanta) have at least one overlapping event in order to establish either a first-order or a transitive statistical relationship between consecutive time slices. For example, if we have four events ? 1 to ? 4 that are related by ? (? 1 , ? 2 ) and ?(? 2 , ? 3 , ? 4 ), we must ensure that samples of ? 2 occur repeatedly across multiple time slices. Given (from a profiling application) an original schedule of configurations ? 1 ? ? 2 ? ? ? ? ? ? ? , where ? ? executes in time slice ?, into another schedule of ? ? ? s such that transitive statistical relationships hold, such that the validity criteria holds on each ? ? ? . In the case when it is not possible ensure the validity criteria on every ? ? ? , we break the chain of repeated events, and start over again from a valid configuration.</p><p>Solution. The first step of the scheduling process is to aggregate all the statistical dependencies available for the processor in question into a graphical structure. The graph is produced by expanding the scheduled chain ? 1 ? ? ? ? ? ? ? using the statistical relationships between the events in the chain. In the ML/statistics community, such a graph commonly referred to as probabilistic graphical model, and more specifically identified as a factor graph (FG) <ref type="bibr" target="#b25">[26]</ref>. Remember from above that the statistical dependencies between the events are specified as joint probability functions Pr(? ? ), <ref type="foot" target="#foot_4">5</ref>where ? ? ? ?. Using those functions, we generate a bipartite FG ? = (? ? {Pr 1 , . . . Pr ? }, {(?, Pr ? )|? ? ? ? ??}). The FG represents the joint distribution of all the events in the schedule, composed together from every individual joint distribution. Now, given the FG and two consecutive configurations from a schedule ? ? and ? ? +1 (with events ? ? , ? ? +1 ? ? respectively), our scheduling problem reduces to (i) finding whether ? ? and ? ? +1 share an event such that the transitive statistical dependency is met; and (ii) if they do not share such a dependency, producing the shortest sequence of</p><formula xml:id="formula_0">? ? * such that ? ? ? ? ? (1) ? ? ? ? ? ? ? +1 .</formula><p>Solution of the first of the two problems is straightforward. We do it by computing the Markov blanket <ref type="bibr" target="#b25">[26]</ref> of the sets ? ? and ? ? +1 under the factor graph. The Markov blanket ? ? ? of a variable ? ? in the factor graph defines a subset of ? ?? such that ? ? is conditionally independent of ? ?? given ? ? ? . If the Markov blankets of ? ? and ? ? +1 overlap (i.e., ? ? ? ? ? ? ? +1 ? ?), then we are guaranteed that there exists at least one event that shares transitive dependencies between the time slices. The second problem is a little more involved. It can be solved by finding the shortest path (assuming unit cost for each edge traversed) from each ? ? ? ? to each ? ? ? ? ? +1 in the FG. That can be accomplished using Djikstra's algorithm, checking validity of the path at every step. In addition to the graph traversal, one must also apply the following optimizations to prune unnecessary ? ? * s. <ref type="bibr">(</ref> ?+1 such that there is no change in the Markov blanket (i.e., ? ? ? = ? ? ?+1 ), then we can skip the transition ? ? ?+1 and instead transition to ? ? ?+2 . That situation can occur because the Markov blankets in individual traversals ? ? ? ? will change at every step; however, the union of all such blankets might not change. If it does not change, we have enough statistical information to skip the ? + 1 th step and go directly to ? + 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Checking Validity of the Configuration.</head><p>A key challenge in determining a valid transformation of a schedule is that of identifying the configurations that do not satisfy the microarchitectural constraints placed on HPCs. We check the validity of a new schedule using Linux's perf_event subsystem. It allows us to iterate over all HPCs in a configuration until it reaches an event that it fails to schedule, thereafter notifying the user of validity failure. To maximize the use of available counters, the perf iteration strategy starts with the most constrained events and goes to the least constrained events in a configuration. Linux's native scheduling for a group of events happens independently per PMU and per logical core. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Modeling Errors in Event Samples</head><p>The first step to computing the full posterior distribution is to model errors in the capture of samples from HPCs. Recall that we listed sources of such errors in ?2. For a single event ? programmed in an HPC ?, if the error in measurement ? ? can be modeled, then the measured/sampled values ? ? can be modeled in terms of the true value ? ? plus measurement noise ? ? , i.e., ? ? = ? ? + ? ? . Here, we focus only on random errors, by assuming zero systematic error. That is a valid assumption because the only reason for systematic errors will be hardware or software bugs. We assume that the error can be modeled as ? ? ? N (0, ?) for some unknown variance ?, hence Pr(? ? | ? ? ) = N (? ? , ?) <ref type="bibr" target="#b42">[43]</ref>. Now, given ? samples of HPC, we compute their sample mean ? and sample variance ?. A scaled and shifted Student's t-distribution describes the marginal distribution of the unknown mean of a Gaussian, when the dependence on variance has been marginalized out <ref type="bibr" target="#b14">[15]</ref>, i.e., ? ? ? ? + ? / ? ? ??????? (? = ? -1). In all our experiments, the confidence level of the t-distribution was set to 95%. Now, since the measurement error model for an HPC is stochastic, when samples from these models are used in the algebraic relationships described above, they too become stochastic in nature. The FG becomes one unified graphical representation of all of these statistical relationships, i.e., between the errored samples and true values of events, as well as among different events that measure related aspects of the CPU's microarchitecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Inference Strategy</head><p>Once we have computed a schedule that ensures that events with statistical dependencies between them are measured in consecutive time slices, the next goal is to utilize the measurements to produce a posterior distribution for an event. Recall Fig. <ref type="figure" target="#fig_2">2</ref>. In each scheduling time slice, we have measurements/samples from the current slice and the preceding slice. However, because of the transitive statistical dependencies, we would like to jointly compute inference for the FG (i.e., compute the posterior probability of some event in the FG given the sampled data) for some ? time slices into the past.</p><p>Our approach to performing this computation with low-latency guarantees utilizes the idea that one can break the larger problem into ? smaller parts, performing inference on each of the ? parts, and then put the results together to get an approximate posterior inference, i.e., similar to map-reduce. There are two difficulties with such algorithms, as they are usually constructed. First, each of the ? pieces has only partial information; as a result, for any of the pieces, a lot of computation is wasted in places that are contradicted by the other ? -1 pieces. Second, the partial results from the ? pieces must be carefully combined together to ensure that the prior (which is embedded into the FG model) is not counted multiple times. We use the Expectation Propagation (EP) algorithm <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34]</ref> to overcome those difficulties to perform the inference. The EP algorithm naturally lends itself to distributed inference on partitioned datasets <ref type="bibr" target="#b15">[16]</ref>. Hence we can perform inference on partitions of data, i.e., each scheduled configuration of the HPCs. In contrast, other techniques for Bayesian inference would require us to explicitly change the inference algorithm depending on the schedule of HPCs and the structure of the FG. Such changes might not be feasible for all possible schedules or all CPU architectures. The EP algorithm works by computing an effective region of overlap over our ? pieces, i.e., for each piece, we use an approximate prior computed over the other ? -1 pieces. The outline of the EP algorithm is illustrated in Alg. 1. The algorithm iteratively approximates a target density ? (?) (in our case the FG) with a density ?(?) that admits the same factorization, and uses a Gaussian mean field approximation <ref type="bibr" target="#b25">[26]</ref>.</p><p>Training. Training is not explicitly required for the proposed BayesPerf model. The advantage of using Bayesian models like FGs is that training on such models can be reduced to inference on the models' parameters. At runtime, for each time slice, we compute (infer) a full posterior distribution over the variables (i.e., ?) and parameters (i.e., ?) of the FG, and then use maximum likelihood estimation to pick the set of parameters (i.e. ?(???) ) that can explain a data trace generated by the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">The BayesPerf Implementation</head><p>In this section we describe the software and hardware components in which BayesPerf is deployed. Further, we describe the architecture and implementation of the BayesPerf accelerator that targets the execution of Alg. 1. Fig. <ref type="figure" target="#fig_5">4</ref> shows the architecture of the BayesPerf system, which works as follows.</p><p>Setup. BayesPerf is used by one or more "monitoring processes/threads" (labeled "Monitoring Application" in Fig. <ref type="figure" target="#fig_5">4</ref>) to monitor hardware threads of a "Target Process." The BayesPerf user API is identical to the the Linux perf subsystem, and hence any user space program that uses the standard Linux interface can transparently use BayesPerf. Using this API, the monitoring process registers events of interest (labelled as 1 in Fig. <ref type="figure" target="#fig_7">4 6</ref> ) with the userspace component of the BayesPerf system, labelled "BayesPerf Shim. " The shim represents a userspace driver <ref type="bibr" target="#b24">[25]</ref> that replicates the API of the Linux perf subsystem.</p><p>Linux perf. The shim registers HPCs on behalf of the user process with the Linux kernel. (labelled as 2 ). The kernel then manages the scheduling of performance counters onto the CPU (using the scheduling algorithm described in ?4.1). This step is labelled as 3 . When the target process raises performance monitoring interrupts (PMIs; labelled as 4 ), the Linux perf subsystem is responsible for reading the corresponding HPC and writing out the sampled value into a "ring buffer" (labelled as 5 ) that represents a segment of memory that is mapped into the address space of both the shim and the perf subsystem. The ring buffer represents a FIFO in which new  samples are enqueued by the kernel and read from the userspace process. The ring buffer automatically provides a mechanism for managing backpressure between the shim and kernel as new samples are dropped if the ring buffer is full.</p><p>Interfacing with the Accelerator. As we will discuss in ?6.1, we have prototyped the BayesPerf system on two different architectures: an Intel x86_64 and an IBM Power9 processor. The protocol for communication between the software and the BayesPerf accelerator (labelled as 6 ; described later) differ for the two architectures. On the Power9 system, we leverage CAPI 2.0 <ref type="bibr" target="#b38">[39]</ref>, a protocol that extends the processor's cache coherence protocol to PCIe-attached devices. In that case, as the accelerator can directly access the host memory, it can consume samples enqueued onto the ring buffer by the kernel (labelled as 5 ). It does so by snooping on cache invalidation messages for the cache lines corresponding to the ring buffer. Similarly, outputs of the accelerator are directly written back to the shim's virtual memory space. For Intel systems, the accelerator uses the base PCIe protocol and IOMMU-mediated PCIe DMA to read HPC samples and write the computed posterior distributions. Here, the shim must actively poll writes from the kernel to the ring buffer, and once the write has been made, initiate transfer of the samples to the FPGA. Similarly, the shim polls for interrupts from the accelerator that signify completion of computation, and initiates DMA transfers for the results. This added software interaction adds some latency overhead to the entire computation.</p><p>Polling Results. Finally, the monitoring application reads (polls) the results of the posterior computation in BayesPerf (labelled as 7 ) from ring buffers in the BayesPerf shim. These reads are always reads to the host memory of the CPU and do not need to initiate DMA requests with the accelerator. This design is able to mask almost all the latency that is added because of the added computation in BayesPerf (see Fig. <ref type="figure" target="#fig_3">3</ref>).</p><p>Multi-Threaded Applications. OS-level monitoring contexts, like processes or threads are dealt with at the level of BayesPerf shim. Hence, when an OS context switch occurs, the memory references of the perf ring buffers are changed by setting configuration registers on the accelerator using MMIO. When the new references are written, the accelerator begins pulling data from a different buffer in memory. As a result, the accelerator can be shared across threads that are concurrently executing on the host CPU.</p><p>The Accelerator. Fig. <ref type="figure" target="#fig_6">5</ref> illustrates the architecture of the BayesPerf accelerator. The accelerator exploits parallelism in the structure of Alg. 1 in two ways. First, we execute posterior inference on each of the ? time-slices in parallel (recall ?4.3). These parallel execution engines are labeled as "EP 1" through "EP k" and "Controller" in Fig. <ref type="figure" target="#fig_6">5</ref>. The EPs execute lines 3-6 of Alg. 1 in parallel, and communicate their results to the global controller, which synchronously updates ?(? ) and dispatches the new value to the idle EP. The values of the measurements from the HPCs (i.e., inputs) as well as the latest values of ?(? ) are stored in the on board DRAM. Our target FPGA board (which we will describe in ?6.1) supports 4 channels of 16GB LPDDR4 memory each. The input data and the current values of ?(? ) (which together comprise ? 100 MB of data) are replicated across those modules to allow concurrent reads from different EPs to progress simultaneously.</p><p>The second level of parallelism exploited by the model is in the computation of MCMC inference in each of the EPs. Those are represented by the "MCMC Sampler" blocks in Fig. <ref type="figure" target="#fig_6">5</ref>. They execute line 4 of Alg. 1 in parallel, by using MCMC to estimate Pr(? ? |? ) (i.e., the likelihood that the data ? ? is drawn from the local approximator ? ? ). Here we leverage our prior work, AcMC 2 <ref type="bibr" target="#b2">[3]</ref>, a high-level synthesis compiler for MCMC applications, to generate IPs that can generate samples from the target distributions of the HPC measurements. The HPC statistical relationships (i.e., the FG) are fed  (ii) to update the state of the sampler with one which passes the rejection sampling test criteria for each random-walk. Allotment of the samplers to EPs, and all subsequent communication between the EPs and samplers, happen over a network-on-chip (NoC) generated with CONNECT <ref type="bibr" target="#b34">[35]</ref>. This approach enables us to use samples from previous iterations as starting points for Markov-chain random walks. This optimization is possible only because we are using MCMC inside an EP algorithm, instead of by itself <ref type="bibr" target="#b19">[20]</ref>. The NoC uses a butterfly topology to allow communication between EPs and samplers, as well as between the samplers themselves (as is required by AcMC 2 ). All our experiments use a 16 port NoC, with 4 of those ports being connected to the EPs, and the remaining 12 to the MCMC samplers. This is the maximal configuration for which we were able to meet timing requirements on the FPGA for a 250 MHz clock.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation &amp; Discussion</head><p>This section discusses our experimental evaluation of the BayesPerf system and is organized as follows. First, in ?6.1, we describe the experimental setup and explore the performance, power, and area requirements of BayesPerf accelerators when programmed onto an FPGA. Then, in ?6.2 we evaluate the capabilities of the BayesPerf system in correcting measurement errors in HPCs. Finally, we demonstrate the integration of BayesPerf with ML-based resource management systems to improve their outcomes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experiment Setup</head><p>We evaluate BayesPerf on two system configurations: (i) an IBM AC922 dual-socket Power9 system (which we will refer to as the "ppc64" configuration), and (ii) a dual-socket Intel Xeon E5-2695 system (which we will refer to as the "x86" configuration). Both the systems are populated with two NVIDIA K80 GPUs, a single FDR Infiniband NIC, and a directly attached FPGA board (which we describe below). Both systems ran Ubuntu 18.04 with kernel version v4.15.0. Accelerator: FPGA. The FPGA accelerator was based on the architecture in ?5. All experiments were performed on an Alpha-Data ADM-PCIE-9V3 FPGA board (with Xilinx Virtex UltraScale+ VU3P-2 FPGA) clocked at 250 MHz. For the Power9 systems, the FPGA board was configured to use the CAPI 2.0 interface <ref type="bibr" target="#b38">[39]</ref>. For the x86 configuration, the FPGA board was configured to use PCIe3 x16 along with the Xilinx XRT drivers. The power and FPGA utilization metrics for the two configurations of the BayesPerf accelerator are listed in Table <ref type="table" target="#tab_2">1</ref>. In comparison to a 100W TDP of the Intel processor and a 190W TDP Power9 processor, the FPGA performs 5.8? and 11.8? better, respectively, in terms of power consumption. The BayesPerf-ppc64 FPGA read latency is shown in Fig. <ref type="figure" target="#fig_3">3</ref>. We observe that a single HPC read using the CPU implementation of BayesPerf has approximately 9? longer latency than native polling of the HPC. However, when the accelerator is being used, BayesPerf adds less than 2% overhead in read latency compared to the native solution. Compared to the BayesPerf-ppc64 implementation that uses CAPI, the BayesPerf-x86 has on average 15.8% larger latency. We can attribute that slowdown to the requirement that a userspace driver actively initiates DMA transfers to the FPGA accelerator, whereas the CAPI configuration snoops for cache invalidation messages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Error Reduction Due to BayesPerf</head><p>To demonstrate the efficacy of BayesPerf in correcting HPC measurement errors, we employed the 29 workloads from the HiBench suite <ref type="bibr" target="#b21">[22]</ref>, which span microbenchmarks, machine learning, SQL, web search, graph analytics, and streaming applications. They represent real-world application workloads used in a cloud environment. We used the two machines in our experiment to simulate a cluster. Each of the machines hosted 32 workers, and the Spark master was deployed on the x86 node. We measured 10 derived events for each of the microarchitectures, where each derived event corresponded to a group of HPCs to be measured and aggregated using a mathematical relationship. We do not detail the events here for lack of space. The metadata corresponding to the events for the x86 configuration can be found in the Linux kernel source tree <ref type="bibr" target="#b40">[41]</ref> for both the x86 and ppc64 configurations. In both cases, we measured all HPCs corresponding to the first 10 metrics.</p><p>Baselines. We use three baselines for comparison. First, we use Linux's inbuilt correction mechanism that uses enabled time and total time (recall from ?4) to correct for measurement errors. This is the most realistic baseline for users who would use the default configuration available in Linux. Second, we use a variance reduction technique called CounterMiner <ref type="bibr" target="#b28">[29]</ref> (CM), a state of the art HPC correction technique used in profiling analysis. Note that CM was originally meant to be used for offline analysis. As we will show in the remainder of this section, this requirement manifests as low average correction accuracy, with large variance, when used for online corrections. Third, we use the online technique by Weaver et. al. <ref type="bibr" target="#b42">[43]</ref> (referred to as "WM+Pin") for correcting instruction counts in x86 processors. WM+Pin only corrects the number of instructions executed and was originally meant to correct core performance metrics like IPC or CPI. Further, it requires intercepting instructions through Pin <ref type="bibr" target="#b27">[28]</ref> to collect opcodes for every dynamic instruction. This causes performance degradation of up to 198.2? across our benchmarks.</p><p>Error Correction. Fig. <ref type="figure" target="#fig_7">6</ref> shows the significant improvement in measurement values compared to the baseline. The average error across all benchmarks dropped from 39.25% and 40.1% for the "Linux (x86)" and "Linux (ppc64)," respectively, to 8.06% (i.e., 4.87?= 39.25% /8.06%) and 7.6% (i.e., 5.28?= 40.1% /7.6%). Similarly, when "BayesPerf (x86)" and "BayesPerf (ppc64)" are compared to "CM (x86)" and "CM (ppc64)," the average error dropped by 3.63?   (= 29.28% /8.06%) and 3.73? (= 28.31% /7.6%), respectively. Similar improvements were observed in the CM configuration. That corresponds to a nearly 40% improvement in the quality of the result of the ppc64 configuration. The normalized improvement in average error for each of the benchmark applications when using BayesPerf, compared to the two baselines is shown Fig. <ref type="figure" target="#fig_9">7</ref>. Recall from ?3, that error in measurement is computed as the similarity between two time series sequences of performance counter samples <ref type="bibr" target="#b4">[5]</ref>. In the case of the BayesPerf counters, we used a maximum likelihood estimator to provide the most likely value of the performance counter at a point in time. We normalize the similarity scores using an average similarity score between two runs of the application, where the HPCs were measured with polling. That way, we could correct for any OS-based nondeterminism in the result. Just like in ?2, where the magnitude of the error is a comparison between "polling" mode and "sampling" under Linux and CM (see Fig. <ref type="figure" target="#fig_7">6</ref>).</p><p>Scaling. Fig. <ref type="figure">8</ref> shows the scaling behavior of the BayesPerf method with increasing numbers of counters for the "KMeans" workload in the HiBench suite. We observe that BayesPerf consistently reduced error by as much as 34% as the number of counters scaled up from 10 to 35 (for Linux). Further, WM+Pin performs worse than CM as it only corrects instruction counts. This justifies our choice of using CM as the main baseline for the evaluation. Interestingly we find that floating point initialization, which is a major source of errors in <ref type="bibr" target="#b42">[43]</ref>, doesn't result in overcounts, indicating that the issue is resolved in modern CPUs.</p><p>Latency Overhead. Since BayesPerf, performs significantly more compute than either Linux or the CM configurations, it is expected to be a significantly higher latency. Recall from Fig. <ref type="figure" target="#fig_3">3</ref> that the difference in latency between BayesPerf (when implemented in software) and the Linux correction is nearly 9?. The BayesPerf accelerator is designed to mitigate the effects of this increased latency. Again, from Fig. <ref type="figure" target="#fig_3">3</ref>, we see that it successfully does so, reducing the 9? difference to 2%. This is on par with native HPC reads using rdpmc as well as kernel-assisted HPC reads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Case Study: BayesPerf in Feedback Loop</head><p>The core value of the BayesPerf approach in terms of it's error correction capability has been demonstrated in the previous section. Here we demonstrate the downstream value of BayesPerf to applications that use HPCs as inputs to control system resources. Examples of such applications include online performance hotspot identification (e.g., <ref type="bibr" target="#b13">[14]</ref>), userspace or runtime-level scheduling (e.g., <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b47">48]</ref>), and power and energy management (e.g., <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40]</ref>), as well as attack detectors and system integrity monitors <ref type="bibr" target="#b7">[8]</ref>. Further they often use as many as 45 HPCs in the case of <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b45">46]</ref>.</p><p>The Problem. We now look at a situation in which BayesPerf measurements can be integrated into higher-level decision-making frameworks to perform resource management decisions. In this part of the experiment, we used HPC measurements to augment  an Apache Spark Executor <ref type="bibr" target="#b46">[47]</ref> that needed to run a distributed shuffle operation (which is part of the HiBench TeraSort benchmark <ref type="bibr" target="#b21">[22]</ref>). Fig. <ref type="figure" target="#fig_10">9</ref> illustrates the rich dynamic information that can be extracted from HPC measurements, and how they can be used in higher-level controllers. Consider the case of a PCIe interconnect which is populated with NIC and GPU devices. Here, the Spark executor uses two GPUs to perform a halo exchange (for training a deep neural network). Fig. <ref type="figure" target="#fig_10">9</ref> shows the performance (in this case, bandwidth) of the exchange as "isolated" performance. If, at the same time, the application were to perform a distributed shuffle (across nodes in a cluster) using the NIC, we would observe that the original GPU-to-GPU communication is affected because of PCIe bandwidth contention at shared links. That phenomenon is shown as "contention" performance in Fig. <ref type="figure" target="#fig_10">9</ref>, and it can cause as much as a 0-1.8? slowdown, depending on the size of the PCIe transactions. Online bandwidth and transaction size monitoring (which is enabled by HPCs) can be used by a higher-level software framework to optimally schedule such transfers, so that the performance impact of shared resource contention is minimized. While the example is simple, it illustrates how errors in measurements can affect the ML algorithm, and hence the overall system performance.</p><p>We use two ML-based scheduling algorithms broadly based on those presented in <ref type="bibr" target="#b9">[10]</ref> and in our prior work <ref type="bibr" target="#b1">[2]</ref>. The first used collaborative filtering as the core ML algorithm, and the second used deep reinforcement learning. The goal of our ML-based scheduler was to decide which of the two NICs it would use to perform the shuffle operation, given that the GPUs were communicating with each other and contending for PCIe bandwidth. We simulated the GPU communication by using Tensorflow to train YoloNet on the ImageNet dataset.</p><p>The Models. The goal of this case study was to show the sensitivity of ML models to errors in their inputs (especially coming from HPCs). The inputs to the models included: (a) sampled HPC measurements corresponding to the numbers of allocating, full, partial, and non-snoop writes, (b) sampled HPC measurements corresponding to demand code reads and partial/MMIO reads, (c) DRAM Channel bandwidth utilization, (d) memory-bus bandwidth utilization, and (e) the size of data to be shuffled (in or out), and the NUMA node on which the data would be resident. Note that all of the above are derived events, computation of which required us to capture 32 unique HPC events. Out of which, 12 were collected for each physical core (i.e., used 432 HPCs = 12 events ? 18 cores ? 2 sockets), The first model, used collaborative filtering to impute values of application performance (in this case throughput) with data coming in from the inputs above, as well as data from training workloads of the SparkBench suite in HiBench. It is based on the technique presented in <ref type="bibr" target="#b9">[10]</ref>. The second model used a straightforward neural network: a 4-layer, fully connected ReLU-activated neural network with 36 neurons in layer 1, 16 neurons in each of layers 2 &amp; 3, and 2 neurons in the last layer. The two neurons in the last layer chose between the two NICs that were decided between as part of this task. The model was trained with actor-critic reinforcement learning based on the approach described in <ref type="bibr" target="#b1">[2]</ref>. The loss function used for training the model minimized the total time taken to complete the shuffle. The model was trained on the HiBench benchmark suite without the TeraSort benchmark, and then evaluated using the TeraSort benchmark. When BayesPerf was used, the MLE estimate from the posterior distribution of the HPC was passed into the network. The GPU marked "Training GPU" was used to perform the collaborative filtering and reinforcement learning as well as runtime inference on the system. It did not contend for the same PCIe resources as the workloads that was being scheduled GPUs.</p><p>Implementation Details: Training. Recall from ?4 that the BayesPerf model in itself does not require training. However, the two models described above require training. The model from <ref type="bibr" target="#b1">[2]</ref> learns by reinforcement. Hence, it does not have specific training and testing phases. The net epochs of data used to train the model are shown in Fig. <ref type="figure" target="#fig_11">10</ref>. For the model in <ref type="bibr" target="#b9">[10]</ref>, which has specific training and test datasets, we calibrate against bias by using threefold cross-validation (i.e., across applications in Fig. <ref type="figure" target="#fig_7">6</ref>).</p><p>Implementation Details: Hyperparameters. The hyperparameters used in the model are taken directly from <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b9">[10]</ref>. These parameters include learning rate, LSTM-unroll-length, and epoch lengths, among others. In addition, we follow the procedure set out in <ref type="bibr" target="#b9">[10]</ref> to determine the optimal value of sparsity. We sweep over the range between 30% and 80%. All results in this paper uses the optimal (found from our sweep) value of 75% sparsity.</p><p>Results. We compare the results of using the above model with BayesPerf and without, using two metrics.</p><p>Results: Training Time. The collaborative filtering model does not have an explicit training phase. For the deep learning model, Fig. <ref type="figure" target="#fig_11">10</ref> illustrates the difference in training time when errorcorrected measurements are used. In the figure, the loss is normalized using the time taken to run the same shuffle operation in a completely isolated system. We observed a nearly 37% reduction in the number of iterations before convergence. Each training iteration in Fig. <ref type="figure" target="#fig_11">10</ref> takes 63s; therefore, the overall saving of 37% corresponds to ~52 hr. The reason for the reduction is apparent: a 40% error in the inputs of the neural network is slowing down the optimization process. Moreover, we observe that the time to convergence is effected by (a) the magnitude of error reduction, as seen by the difference between the Linux-CM (12.5% decrease) and -BayesPerf (37% decrease) configurations; and (b) the timeliness of the error reduction, as seen by the difference between the CPU and accelerated versions of BayesPerf (28.5% decrease).</p><p>Results: Decision Quality. We observe that use of the MLbased scheduler (i.e., that makes Spark PCIe aware) leads to a 15.1 ? 2.2% and 22.3 ? 7.9% improvement in average shuffle completion time for the two models respectively. Addition of BayesPerf to the model results in a further 8.7 ? 0.9% and 19 ? 3.4% reduction in average shuffle latency, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Error Correction in HPCs Measurement errors due to sampling in HPCs have been observed and reported on for the past decade <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b47">48]</ref>. Methods for correction of sampled HPC values can be broadly grouped into two separate approaches. The first group of methods artificially imputes data in the collected samples by interpolating between two sampled events using linear or piece-wise linear interpolation (e.g., <ref type="bibr" target="#b40">[41]</ref>). The advantage of such interpolation methods is that they can be run in real time: however, they might not provide good imputations <ref type="bibr" target="#b47">[48]</ref>. The second group of methods correct measurements by dropping outlier values, instead of by adding new interpolated values. Such methods are at the other extreme: they cannot be run in real time, as they need the entire trace of an application before providing corrections. For example, Lv et al. <ref type="bibr" target="#b28">[29]</ref> use the Gumbel test for outlier detection, and Neill et. al. <ref type="bibr" target="#b32">[33]</ref> use fork-join aware agglomorative clustering to remove outlier points. These methods are not suitable for dynamic control situations that need online HPC correction. Further, the core statistical technique used by these variance reduction approaches assume that the underlying distribution of the data remains unchanged, however, most workload exhibit distinct stages where workload behavior and thus the underlying distribution of the HPCs will change.</p><p>In contrast to those techniques, BayesPerf corrects measurements by using statistical relationships between events. For welldocumented processors, such relationships can be known ahead of time, and the entire correction algorithm can be executed without any need to pre-collect data. The BayesPerf system (with its accelerator) allows nearly native latency access to the corrected HPCs, thereby enabling their use in dynamic control processes.</p><p>Using HPCs in Control. Several recent papers have explored the use of HPCs to perform higher-level resource management problems. Examples include online performance hotspot identification (e.g., <ref type="bibr" target="#b13">[14]</ref>), userspace or runtime-level scheduling (e.g., <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b47">48]</ref>), power and energy management (e.g., <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40]</ref>), and attack detectors and system integrity monitors <ref type="bibr" target="#b7">[8]</ref>. Most of the methods mentioned above do not explicitly use any techniques to correct for errors in HPC measurements. Further, while it is not impossible that some of the ML techniques can inherently correct for HPC errors, there are no guarantees that it does so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>It is crucial to have reliable instrumentation/measurement in commercial CPUs, as exemplified by the inclusion of the PEBS (precision event-based sampling) and LBR (last branch record) technologies in modern Intel processors. However, as we showed in this paper, such technology alone falls short of correcting errors in the values of HPCs accrued because of nondeterminism and sampling artifacts. This paper presented the design and evaluation of BayesPerf, an ML model and associated accelerator that allows for correction of noisy HPC measurements, reducing the average error in HPC measurements from 42.11% to 7.8% when events are being multiplexed. BayesPerf is the first step in realizing a general-purpose HPC-error-correction system for real x86 and ppc64 systems today and potentially for future processors. We believe it will form the basis for performing large-scale measurement/characterization studies that use HPC data (i.e., offline analysis), but also enable a slew of applications that can use the HPC data to make controldecisions in a computer system (i.e., online analysis).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Errors due to event multiplexing in HPC measurements across ten application runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of the BayesPerf ML model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Latency overhead of reading counters with BayesPerf compared to traditional methods on an x86 CPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1 3 : 4 : 5 : 6 : 7 :</head><label>134567</label><figDesc>General EP algorithm. Input: Target distribution ? (? ) = ? ? (? ) Output: Global approximation ? (? ) = ? ? (? ) 1: Choose initial ? ? (? ) 2: for ? ? {0, . . . ? -1}? until ? ? converges do ? -? (? ) ? ? (? ) /? ? (? ) ? Cavity distribution ? \? (? ) ? Pr(? ? |? )? -? (? ) ? MCMC ? ??? (? ) ? ? \? (? ) ? Local update ?? ? (? ) ? ? ??? (? ) /?(? ) ? (? ) ? ? (? )?? ? (? ) ? Global update 8: end for 9: return {? ? (? ) |? ? [0, ?) }As some PMUs are shared between threads of the same core or package, their availability may change depending on what events are being measured on the other cores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: High-level architecture of the BayesPerf system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Architecture of the BayesPerf accelerator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>FFigure 6 :</head><label>6</label><figDesc>Figure 6: Error in performance counter measurements across the HiBench benchmarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>t W o r d C o u n t T e r a S o r t R e p a r t it io n D F S IO E B a y e s K M e a n s G M M L R A L S G B T X G B o o s t L in e a r L D A P C A R F S V M S V D S c a n J o in A g g r e g a t e P a g e R a n k N u t c h In d e x in g N W e ig h t Id e n t it y R e p a r t it io n S t a t e f u lW o r d C o u n t F ix W in d o w Normalized Improvement BayesPerf vs Linux (x86) BayesPerf vs Linux (ppc64) BayesPerf vs CM (x86) BayesPerf vs CM (ppc64)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Normalized improvement in performance counter error measurements across the HiBench benchmarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Topology of test system in ?6.3 as well as the effect of the resource contention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Decrease in training time due to BayesPerf.and 20 were off-core events being collected per-socket (i.e., used 40 HPCs = 20 events ? 2 sockets).The first model, used collaborative filtering to impute values of application performance (in this case throughput) with data coming in from the inputs above, as well as data from training workloads of the SparkBench suite in HiBench. It is based on the technique presented in<ref type="bibr" target="#b9">[10]</ref>. The second model used a straightforward neural network: a 4-layer, fully connected ReLU-activated neural network with 36 neurons in layer 1, 16 neurons in each of layers 2 &amp; 3, and 2 neurons in the last layer. The two neurons in the last layer chose between the two NICs that were decided between as part of this task. The model was trained with actor-critic reinforcement learning based on the approach described in<ref type="bibr" target="#b1">[2]</ref>. The loss function used for training the model minimized the total time taken to complete the shuffle. The model was trained on the HiBench benchmark suite without the TeraSort benchmark, and then evaluated using the TeraSort benchmark. When BayesPerf was used, the MLE estimate from the posterior distribution of the HPC was passed into the network. The GPU marked "Training GPU" was used to perform the collaborative filtering and reinforcement learning as well as runtime inference on the system. It did not contend for the same PCIe resources as the workloads that was being scheduled GPUs.Implementation Details: Training. Recall from ?4 that the BayesPerf model in itself does not require training. However, the two models described above require training. The model from<ref type="bibr" target="#b1">[2]</ref> learns by reinforcement. Hence, it does not have specific training and testing phases. The net epochs of data used to train the model are shown in Fig.10. For the model in<ref type="bibr" target="#b9">[10]</ref>, which has specific training and test datasets, we calibrate against bias by using threefold cross-validation (i.e., across applications in Fig.6).Implementation Details: Hyperparameters. The hyperparameters used in the model are taken directly from<ref type="bibr" target="#b1">[2]</ref> and<ref type="bibr" target="#b9">[10]</ref>. These parameters include learning rate, LSTM-unroll-length, and epoch lengths, among others. In addition, we follow the procedure set out in<ref type="bibr" target="#b9">[10]</ref> to determine the optimal value of sparsity. We sweep over the range between 30% and 80%. All results in this paper uses the optimal (found from our sweep) value of 75% sparsity.Results. We compare the results of using the above model with BayesPerf and without, using two metrics.Results: Training Time. The collaborative filtering model does not have an explicit training phase. For the deep learning model, Fig.10illustrates the difference in training time when errorcorrected measurements are used. In the figure, the loss is normalized using the time taken to run the same shuffle operation in a completely isolated system. We observed a nearly 37% reduction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1) Removing Common Steps: If an intermediate step ? ? ? exists such that the Markov blankets ? ? 1 , ? ? 2 , . . . ? ? ? of events ? 1 , ? 2 , . . . ? ? overlap, the next transition state of the schedule can be condensed. That is, if there exists an ? * ? ? ? 1 ? . . . ? ? ? , then composition of statistical relationships can happen through ? * , instead of through the larger set of events, i.e., ? ? ?+1 ? ? (? ? ?+1 \ {? 1 , . . . ? ? }) ? ? * (2) Removing Redundant Steps: If there exists two steps ? ? ? and ? ?</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Area &amp; power for components of the BayesPerf FPGA for the x86_64 and ppc64 configurations.</figDesc><table><row><cell>Component</cell><cell></cell><cell cols="2">Utilization (%)</cell><cell></cell><cell>Power (W)</cell><cell></cell></row><row><cell></cell><cell cols="6">BRAM DSP FF LUT URAM Vivado Measured</cell></row><row><cell>x86-PCIe</cell><cell>62</cell><cell>78 52</cell><cell>81</cell><cell>58</cell><cell>11.2</cell><cell>17.2</cell></row><row><cell>ppc64-CAPI</cell><cell>71</cell><cell>66 49</cell><cell>79</cell><cell>58</cell><cell>10.5</cell><cell>16.1</cell></row><row><cell cols="7">into the compiler as a probabilistic program, i.e., a program in a do-</cell></row><row><cell cols="7">main specific language that can represent statistical dependencies</cell></row><row><cell cols="7">between program variables. AcMC 2 then automatically generates</cell></row><row><cell cols="7">efficient uniform random number generators, and automatically</cell></row><row><cell cols="7">synthesizes other statistical constraints in FG. Instead of using the</cell></row><row><cell cols="7">AcMC 2 -generated controllers for the MCMC samplers, we use the</cell></row><row><cell cols="7">EPs to directly control the pipelines of MCMC samplers. That is,</cell></row><row><cell cols="7">(i) to set and update configuration parameters like seed values; and</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In a simple processor, DRAM Bandwidth = (LLC misses ? Cache line size+ # DMA Transactions ? Transaction size)/Clocks.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>This definition of error is based on prior work on HPC errors<ref type="bibr" target="#b28">[29]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The experimental setup is described in detail in ?6.1.Adding More Registers? A relevant question to ask is whether the HPC-error problem will disappear if more HPC registers are added into future CPUs. The short answer is that it will not, because as we continue to add more monitors, the system complexity increases which is untenable in commercial CPUs that are often driven by other practical considerations. Hence, HPC counters will eventually always end up introducing the sampling-based error.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>In general, this triggering event occurs based on the number of clock cycles or number of instructions executed.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>We use the shorthand Pr ? = Pr(? ? ).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>* refers to annotations in Fig.4if not otherwise specified.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank the ASPLOS reviewers and our shepherd, <rs type="person">Alexandre Passos</rs>, for their valuable comments that improved the paper. We appreciate <rs type="person">S. Lumetta</rs>, <rs type="person">W-M. Hwu</rs>, <rs type="person">J. Xiong</rs>, and <rs type="person">J. Applequist</rs> for their insightful discussion and comments on the early drafts of this manuscript. This work is partially supported by the <rs type="funder">National Science Foundation (NSF)</rs> under grant Nos. <rs type="grantNumber">CNS 13-37732</rs>, <rs type="grantNumber">CNS 16-24790</rs>, and <rs type="grantNumber">CCF 20-29049</rs>; by the <rs type="funder">IBM-ILLINOIS Center for Cognitive Computing Systems Research</rs> (<rs type="grantNumber">C3SR</rs>), a research collaboration that is part of the <rs type="institution">IBM AI Horizon Network</rs>; and by <rs type="funder">IBM</rs>, <rs type="funder">Intel</rs>, and <rs type="funder">Xilinx</rs> through equipment donations. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF, <rs type="institution">IBM</rs>, <rs type="funder">Intel</rs>, or, <rs type="funder">Xilinx</rs>. <rs type="person">Saurabh Jha</rs> is supported by a 2020 IBM PhD fellowship.</p></div>
			</div>
			<div type="funding">
<div><p>Q K j X 0 O w I L D P b K T 2 c R 5 c r / 3 7 s u a w = " <rs type="projectName">&gt; A A A C D n i c d V C 7 S</rs> g N B F J 3 1 G e M r a i P Y D I a A R Q j 7 C C a N E L C x j G A e k A 3 L 7 O Q m G T L 7 Y G Z W C E u + w M Z f s b F Q x N b a z r 9 x N o m g o g f m c j j n X u 7 c 4 8 e c S W W a H 8 b K 6 t r 6 x m Z u K 7 + 9 s 7 u 3 X z g 4 b M s o E R R a N O K R 6 P p E A m c h t B R T H L q x A B L 4 H D r + 5 D L z O 7 c g J I v C G z W N o R + Q U c i G j B K l J a 9 Q c l P q W W V M P T s r j j v D F 9 h N w S N l D J 6 f F e r O v E L R r D h O t e 5 Y W B P b N k 1 b k 3 O z Z t U d b F X M O Y p o i a Z X e H c H E U 0 C C B X l R M q e Z c a q n x K h G O U w y 7 u J h J j Q C R l B T 9 O Q B C D 7 6 f y c G S 5 p Z Y C H k d A v V H i u f p 9 I S S D l N P B 1 Z 0 D U W P 7 2 M v E v r 5 e o Y b 2 f s j B O F I R 0 s W i Y c K w i n G W D B 0 w A V X y q C a G C 6 b 9 i O i a C U K U T z O s Q v i 7 F / 5 O 2 X b G c i n 1 d L T a O l 3 H k 0 A k 6 R W f I Q j X U Q F e o i V q I o j v 0 g J 7 Q s 3 F v P B o v x u u i d c V Y z h y h H z D e P g E j e Z m K &lt; / l a t e x i t &gt; {c 1 , c 2 , c 3 } = {e a , e b , e c } &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q K j X 0 O w I L D P b K T 2 c R 5 c r / 3 7 s u a w = " <rs type="projectName">&gt; A A A C D n i c d V C 7 S</rs> g N B F J 3 1 G e M r a i P Y D I a A R Q j 7 C C a N E L C x j G A e k A 3 L 7 O Q m G T L 7 Y G Z W C E u + w M Z f s b F Q x N b a z r 9 x N o m g o g f m c j j n X u 7 c 4 8 e c S W W a H 8 b K 6 t r 6 x m Z u K 7 + 9 s 7 u 3 X z g 4 b M s o E R R a N O K R 6 P p E A m c h t B R T H L q x A B L 4 H D r + 5 D L z O 7 c g J I v C G z W N o R + Q U c i G j B K l J a 9 Q c l P q W W V M P T s r j j v D F 9 h N w S N l D J 6 f F e r O v E L R r D h O t e 5 Y W B P b N k 1 b k 3 O z Z t U d b F X M O Y p o i a Z X e H c H E U 0 C C B X l R M q e Z c a q n x K h G O U w y 7 u J h J j Q C R l B T 9 O Q B C D 7 6 f y c G S 5 p Z Y C H k d A v V H i u f p 9 I S S D l N P B 1 Z 0 D U W P 7 2 M v E v r 5 e o Y b 2 f s j B O F I R 0 s W i Y c K w i n G W D B 0 w A V X y q C a G C 6 b 9 i O i a C U K U T z O s Q v i 7 F / 5 O 2 X b G c i n 1 d L T a O l 3 H k 0 A k 6 R W f I Q j X U Q F e o i V q I o j v 0 g J 7 Q s 3 F v P B o v x u u i d c V Y z h y h H z D e P g O N e N N F s t Y d w J q u B S K N 1 G g 5 J 1 E c x o F k r e D 8 e 3 M b z 9 x b U S s H n G S c D + i Q y V C w S h a 6 Y H 1 v X 6 5 4 l b d O c g q 8 X J S g R y N f v m r N 4 h Z G n G F T F J j u p 6 b o J 9 R j Y J J P i 3 1 U s M T y s Z 0 y L u W K h p x 4 2 f z U 6 f k 3 C o D E s b a l k I y V 3 9 P Z D Q y Z h I F t j O i O D L L 3 k z 8 z + u m G N 7 4 m V B J i l y x x a I w l</p><p>9 a C k 8 8 c w x 8 4 n z / h 0 4 1 m &lt; / l a t e x i t &gt; {e b , e c } &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 a m i m E b k L 3 C R 3 n N d Y L g B g i 9 r h x Y = " &gt; A A A B 8 3 i c d V D L S s N A F J 3 4 r P V V d S O 4 G S y C C w l J t b b u C m 5 c V r A P a E K Y T G / a o Z M H M x O h h P 6 G G x e K u P V n 3 P k 3 T t o I K n r g w u G c e 7 n 3 H j / h T C r L + j C W l l d W 1 9 Z L G + X N r e 2 d 3 c r e f l f G q a D Q o T G P R d 8 n E j i L o K O Y 4 t B P B J D Q 5 9 D z J 9 e 5 3 7 s H I V k c 3 a l p A m 5</p><p>j l M C s q Y S E A k Z w U D T i I Q g W x + w y f a G W I g j o i h S e q n M h J K O Q R k S N Z a / v V z y x u k K m i G Y u S V E F E F u C l G M V z w A P G Q C q O J T T Q g V T N + K Z g I Q p W O q a x D + P o U / + N d M + N u <rs type="person">F X W Y R F H C R h Y S K b N R A L X S D q i D K E</rs> r Q A p C z Z q P B o v x u u i d c k o Z g Q D x h v n W a k U = &lt; / l a t e x i t &gt; {c 1 , c 2 , c 3 } = {e a , e b , e c } &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q K j X 0 O w I L D P b K T 2 c R 5 c r / 3 7 s u a w = " <rs type="projectName">&gt; A A A C D n i c d V C 7 S</rs> g N B F J 3 1 G e M r a i P Y D I a A R Q j 7 C C a N E L C x j G A e k A 3 L 7 O Q m G T L 7 Y G Z W C E u + w M Z f s b F Q x N b a z r 9 x N o m g o g f m c j j n X u 7 c 4 8 e c S W W a H 8 b K 6 t r 6 x m Z u K 7 + 9 s 7 u 3 X z g 4 b M s o E R R a N O K R 6 P p E A m c h t B R T H L q x A B L 4 H D r + 5 D L z O 7 c g J I v C G z W N o R + Q U c i G j B K l J a 9 Q c l P q W W V M P T s r j j v D F 9 h N w S N l D J 6 f F e r O v E L R r D h O t e 5 Y W B P b N k 1 b k 3 O z Z t U d b F X M O Y p o i a Z X e H c H E U 0 C C B X l R M q e Z c a q n x K h G O U w y 7 u J h J j Q C R l B T 9 O Q B C D 7 6 f y c G S 5 p Z Y C H k d A v V H i u f p 9 I S S D l N P B 1 Z 0 D U W P 7 2 M v E v r 5 e o Y b 2 f s j B O F I R 0 s W i Y c K w i n G W D B 0 w A V X y q C a G C 6 b 9 i O i a C U K U T z O s Q v i 7 F / 5 O 2 X b G c i n 1 d L T a O l 3 H k 0 A k 6 R W f I Q j X U Q F e o i V q I o j v 0 g J 7 Q s 3 F v P B o v x u u i d c V Y z h y h H z D e P g E j e Z m K &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L A z O x k O 8 Y E z w y 7 F 3 I 3 d a 3 w D i o T 8 = " &gt; A A A C D n i c d V D L S g M x F M 3 U d 3 1 V 3 Q h u g k V w I S U z U 7 U b o e D G Z Q W r h U 4 Z M u k d G 5 p 5 k G S E M v Q L 3 P g r b l w o 4 t a <rs type="grantNumber">1 O / / G T K 2</rs> g o g d y O Z x z L z f 3 B K n g S h P y b p V m Z u f m F x a X y s s r q 2 v r l Y 3 N S 5 V k k k G b J S K R n Y A q E D y G t u Z a Q C e V Q K N A w F U w P C 3 8 q x u Q i i f x h R 6 l 0 I v o d c x D z q g 2 k l / Z 8 3 L m 2 w e Y + U 5 R X G + M T 7 C X g w 8 H G P y w K N Q b + 5 U q q b l u v e H a 2 B D H I c Q x 5 I g c 2 w 0 X 2 z U y Q R V N 0 f I r b 1 4 / Y V k E s W a C K t W 1 S a p 7 O Z W a M w H j s p c p S C k b 0 m v o G h r T C F Q v n 5 w z x n t G 6 e M w k e b F G k / U 7 x M 5 j Z Q a R Y H p j K g e q N 9 e I f 7 l d T M d N n o 5 j 9 N M Q 8 w + F 4 W Z w D r B R T a 4 z y U w L U a G U C a 5 + S t m A y o p 0 y b B s g n h 6 1 L 8 P 7 l 0 a v Z h j Z z X q 8 3 t a R y L a A f t o n 1 k o 2 P U R G e o h d q I o V t 0 j x 7 R k 3 V n P V j P 1 s t n a 8 m a z m y h H 7 B e P w A s</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_3zjNJET">
					<idno type="grant-number">CNS 13-37732</idno>
				</org>
				<org type="funding" xml:id="_5tTkM2e">
					<idno type="grant-number">CNS 16-24790</idno>
				</org>
				<org type="funding" xml:id="_G2UAve5">
					<idno type="grant-number">CCF 20-29049</idno>
				</org>
				<org type="funding" xml:id="_4f4Qs8g">
					<idno type="grant-number">C3SR</idno>
				</org>
				<org type="funded-project" xml:id="_SzyHEtM">
					<orgName type="project" subtype="full">&gt; A A A C D n i c d V C 7 S</orgName>
				</org>
				<org type="funded-project" xml:id="_MUNfSMd">
					<orgName type="project" subtype="full">&gt; A A A C D n i c d V C 7 S</orgName>
				</org>
				<org type="funded-project" xml:id="_gDFhS5x">
					<idno type="grant-number">1 O / / G T K 2</idno>
					<orgName type="project" subtype="full">&gt; A A A C D n i c d V C 7 S</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploiting Hardware Performance Counters with Flow and Context Sensitive Profiling</title>
		<author>
			<persName><forename type="first">Glenn</forename><surname>Ammons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">R</forename><surname>Larus</surname></persName>
		</author>
		<idno type="DOI">10.1145/258916.258924</idno>
		<ptr target="https://doi.org/10.1145/258916.258924" />
	</analytic>
	<monogr>
		<title level="j">SIGPLAN Not</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="85" to="96" />
			<date type="published" when="1997-05">1997. May 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Inductive-bias-driven Reinforcement Learning For Efficient Schedules in Heterogeneous Clusters</title>
		<author>
			<persName><forename type="first">Subho</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Kalbarczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravishankar</forename><surname>Iyer</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v119/banerjee20a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning (Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">Hal</forename><surname>Daum?</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Iii</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</editor>
		<meeting>the 37th International Conference on Machine Learning ( Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="629" to="641" />
		</imprint>
	</monogr>
	<note>PMLR, Virtual</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">AcMC2 : Accelerating Markov Chain Monte Carlo Algorithms for Probabilistic Models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Subho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><forename type="middle">T</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravishankar</forename><forename type="middle">K</forename><surname>Kalbarczyk</surname></persName>
		</author>
		<author>
			<persName><surname>Iyer</surname></persName>
		</author>
		<idno type="DOI">10.1145/3297858.3304019</idno>
		<ptr target="https://doi.org/10.1145/3297858.3304019" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<title level="s">ASPLOS &apos;19</title>
		<meeting>the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems<address><addrLine>Providence, RI, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="515" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Multikernel: A New OS Architecture for Scalable Multicore Systems</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Baumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Evariste</forename><surname>Dagand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Isaacs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Roscoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Sch?pbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhilesh</forename><surname>Singhania</surname></persName>
		</author>
		<idno type="DOI">10.1145/1629575.1629579</idno>
		<ptr target="https://doi.org/10.1145/1629575.1629579" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGOPS 22Nd Symposium on Operating Systems Principles</title>
		<meeting>the ACM SIGOPS 22Nd Symposium on Operating Systems Principles<address><addrLine>Big Sky, Montana, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="29" to="44" />
		</imprint>
	</monogr>
	<note>SOSP &apos;09)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using Dynamic Time Warping to Find Patterns in Time Series</title>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">J</forename><surname>Berndt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Clifford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Knowledge Discovery and Data Mining</title>
		<title level="s">AAAIWS&apos;94</title>
		<meeting>the 3rd International Conference on Knowledge Discovery and Data Mining<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="359" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Machine Learning for Load Balancing in the Linux Kernel</title>
		<author>
			<persName><forename type="first">Jingde</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subho</forename><forename type="middle">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><forename type="middle">T</forename><surname>Kalbarczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravishankar</forename><forename type="middle">K</forename><surname>Iyer</surname></persName>
		</author>
		<idno type="DOI">10.1145/3409963.3410492</idno>
		<ptr target="https://doi.org/10.1145/3409963.3410492" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th ACM SIGOPS Asia-Pacific Workshop on Systems</title>
		<meeting>the 11th ACM SIGOPS Asia-Pacific Workshop on Systems<address><addrLine>Tsukuba, Japan; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
	<note>APSys &apos;20</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<ptr target="https://software.intel.com/en-us/articles/intel-sdm" />
		<title level="m">Intel? 64 and IA-32 Architectures Software Developer Manuals</title>
		<imprint>
			<publisher>Intel Corp</publisher>
			<date type="published" when="2016">2016. 2019-03-05</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SoK: The Challenges, Pitfalls, and Perils of Using Hardware Performance Counters for Security</title>
		<author>
			<persName><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Antonakakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Polychronakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Monrose</surname></persName>
		</author>
		<idno type="DOI">10.1109/SP.2019.00021</idno>
		<ptr target="https://doi.org/10.1109/SP.2019.00021" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="20" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Out of Control: Stealthy Attacks against Robotic Vehicles Protected by Control-Based Techniques</title>
		<author>
			<persName><forename type="first">Pritam</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Karimibiuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Pattabiraman</surname></persName>
		</author>
		<idno type="DOI">10.1145/3359789.3359847</idno>
		<ptr target="https://doi.org/10.1145/3359789.3359847" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th Annual Computer Security Applications Conference</title>
		<meeting>the 35th Annual Computer Security Applications Conference<address><addrLine>San Juan, Puerto Rico; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="660" to="672" />
		</imprint>
	</monogr>
	<note>ACSAC &apos;19)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Paragon: QoS-aware Scheduling for Heterogeneous Datacenters</title>
		<author>
			<persName><forename type="first">Christina</forename><surname>Delimitrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
		<idno type="DOI">10.1145/2451116.2451125</idno>
		<ptr target="https://doi.org/10.1145/2451116.2451125" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<title level="s">ASPLOS &apos;13</title>
		<meeting>the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems<address><addrLine>Houston, Texas, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="77" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ian</forename><surname>Joshua V Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Langmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivas</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Patton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rif</forename><forename type="middle">A</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><surname>Saurous</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10604</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Tensorflow distributions. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reliable and Efficient Performance Monitoring in Linux</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dimakopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eranian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Koziris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bambos</surname></persName>
		</author>
		<idno type="DOI">10.1109/SC.2016.33</idno>
		<ptr target="https://doi.org/10.1109/SC.2016.33" />
	</analytic>
	<monogr>
		<title level="m">SC &apos;16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="396" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative and Multiphase Learning for Computer Systems Optimization</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Hoffmann</surname></persName>
		</author>
		<idno type="DOI">10.1145/3307650.3326633</idno>
		<ptr target="https://doi.org/10.1145/3307650.3326633" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th International Symposium on Computer Architecture</title>
		<meeting>the 46th International Symposium on Computer Architecture<address><addrLine>Phoenix, Arizona; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="39" to="52" />
		</imprint>
	</monogr>
	<note>ISCA &apos;19)</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Seer: Leveraging Big Data to Navigate the Complexity of Performance Debugging in Cloud Microservices</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dailun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meghna</forename><surname>Pancholi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Delimitrou</surname></persName>
		</author>
		<idno type="DOI">10.1145/3297858.3304004</idno>
		<ptr target="https://doi.org/10.1145/3297858.3304004" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<title level="s">ASPLOS &apos;19</title>
		<meeting>the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems<address><addrLine>Providence, RI, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="19" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><surname>Db Rubin</surname></persName>
		</author>
		<title level="m">Bayesian Data Analysis</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Chapman &amp; Hall</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aki</forename><surname>Vehtari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasi</forename><surname>Jyl?nki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuomas</forename><surname>Sivula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swupnil</forename><surname>Sahai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Blomstedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Schiminovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Robert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.4869</idno>
		<title level="m">Expectation propagation as a way of life: A framework for Bayesian inference on partitioned data</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deployment of Query Plans on Multicores</title>
		<author>
			<persName><forename type="first">Jana</forename><surname>Giceva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Roscoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Harris</surname></persName>
		</author>
		<idno type="DOI">10.14778/2735508.2735513</idno>
		<ptr target="https://doi.org/10.14778/2735508.2735513" />
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2014-11">2014. Nov. 2014</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="233" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Performance optimization and tuning techniques for IBM Power Systems processors including IBM POWER8</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bergner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Shalev Housfater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madhusudanan</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tulio</forename><surname>Magno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Mericas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Munroe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mauricio</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Schmidt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>IBM Redbooks</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stochastic variational inference</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Matthew D Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1303" to="1347" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Intel 64 and IA-32 architectures optimization reference manual</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<publisher>Intel Corporation, Sept</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Sparkbench: The Big Data Micro Benchmark Suite for Spark 2</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<ptr target="https://github.com/intel-hadoop/HiBench/" />
		<imprint>
			<date type="published" when="2016">2016. 19-November-2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<ptr target="https://software.intel.com/en-us/vtune-cookbook-top-down-microarchitecture-analysis-method" />
		<title level="m">Top-down Microarchitecture Analysis Method</title>
		<imprint>
			<date type="published" when="2019-11">2019. 19-November-2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ML-driven Malware for Targeting AV Safety</title>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengkun</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><forename type="middle">T</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravishankar K</forename><surname>Kalbarczyk</surname></persName>
		</author>
		<author>
			<persName><surname>Iyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The Userspace I/O HOWTO</title>
		<author>
			<persName><forename type="first">Hans-J?rgen</forename><surname>Koch</surname></persName>
		</author>
		<ptr target="https://www.kernel.org/doc/html/v4.12/driver-api/uio-howto.html" />
		<imprint>
			<date type="published" when="2006">2006. 19-November-2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">Probabilistic graphical models: principles and techniques</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<ptr target="https://perf.wiki.kernel.org/index.php/Main_Page" />
		<title level="m">Linux profiling with performance counters</title>
		<imprint>
			<date type="published" when="2019-11">2019. 19-November-2019</date>
		</imprint>
	</monogr>
	<note>Linux Community</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pin: building customized program analysis tools with dynamic instrumentation</title>
		<author>
			<persName><forename type="first">Chi-Keung</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Muth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harish</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artur</forename><surname>Klauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Lowney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">Janapa</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Hazelwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm sigplan notices</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="190" to="200" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">CounterMiner: Mining Big Performance Data from Hardware Counters</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO.2018.00056</idno>
		<ptr target="https://doi.org/10.1109/MICRO.2018.00056" />
	</analytic>
	<monogr>
		<title level="m">2018 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="613" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">MPX: Software for multiplexing hardware performance counters in multithreaded programs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>May</surname></persName>
		</author>
		<idno type="DOI">10.1109/IPDPS.2001.924955</idno>
		<ptr target="https://doi.org/10.1109/IPDPS.2001.924955" />
	</analytic>
	<monogr>
		<title level="m">Proceedings 15th International Parallel and Distributed Processing Symposium. IPDPS</title>
		<meeting>15th International Parallel and Distributed Processing Symposium. IPDPS</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Expectation Propagation for Approximate Bayesian Inference</title>
		<author>
			<persName><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Seventeenth Conference on Uncertainty in Artificial Intelligence<address><addrLine>Seattle, Washington; San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="362" to="369" />
		</imprint>
	</monogr>
	<note>UAI&apos;01</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Time Interpolation: So Many Metrics, So Few Registers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mytkowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hauswirth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Diwan</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO.2007.27</idno>
		<ptr target="https://doi.org/10.1109/MICRO.2007.27" />
	</analytic>
	<monogr>
		<title level="m">40th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="286" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fuse: Accurate Multiplexing of Hardware Performance Counters Across Executions</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Neill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andi</forename><surname>Drebes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoniu</forename><surname>Pop</surname></persName>
		</author>
		<idno type="DOI">10.1145/3148054</idno>
		<ptr target="https://doi.org/10.1145/3148054" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Archit. Code Optim</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">26</biblScope>
			<date type="published" when="2017-12">2017. Dec. 2017</date>
		</imprint>
	</monogr>
	<note>Article</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<author>
			<persName><forename type="first">Manfred</forename><surname>Opper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
		<idno type="DOI">10.1162/089976600300014881</idno>
		<ptr target="https://doi.org/10.1162/089976600300014881" />
	</analytic>
	<monogr>
		<title level="m">Gaussian Processes for Classification: Mean-Field Algorithms</title>
		<imprint>
			<date type="published" when="2000-11">2000. Nov. 2000</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2655" to="2684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">CONNECT: Re-examining Conventional Wisdom for Designing Nocs in the Context of FPGAs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">C</forename><surname>Papamichael</surname></persName>
		</author>
		<author>
			<persName><surname>Hoe</surname></persName>
		</author>
		<idno type="DOI">10.1145/2145694.2145703</idno>
		<ptr target="https://doi.org/10.1145/2145694.2145703" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/SIGDA International Symposium on Field Programmable Gate Arrays</title>
		<meeting>the ACM/SIGDA International Symposium on Field Programmable Gate Arrays<address><addrLine>Monterey, California, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="37" to="46" />
		</imprint>
	</monogr>
	<note>FPGA &apos;12)</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Tangram: Integrated Control of Heterogeneous Computers</title>
		<author>
			<persName><forename type="first">Pradyumna</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">L</forename><surname>Pothukuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Greathouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Erb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petros</forename><forename type="middle">G</forename><surname>Piga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josep</forename><surname>Voulgaris</surname></persName>
		</author>
		<author>
			<persName><surname>Torrellas</surname></persName>
		</author>
		<idno type="DOI">10.1145/3352460.3358285</idno>
		<ptr target="https://doi.org/10.1145/3352460.3358285" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture (Columbus</title>
		<title level="s">MICRO &apos;52</title>
		<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture (Columbus<address><addrLine>OH, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="384" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Yukta: Multilayer Resource Controllers to Maximize Efficiency</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Pothukuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Pothukuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Voulgaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Torrellas</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA.2018.00049</idno>
		<ptr target="https://doi.org/10.1109/ISCA.2018.00049" />
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="505" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">FIRM: An Intelligent Fine-grained Resource Management Framework for SLO-Oriented Microservices</title>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zbigniew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravishankar</forename><forename type="middle">K</forename><surname>Kalbarczyk</surname></persName>
		</author>
		<author>
			<persName><surname>Iyer</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/osdi20/presentation/qiu" />
	</analytic>
	<monogr>
		<title level="m">14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20). USENIX Association</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="805" to="825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">CAPI: A Coherent Accelerator Processor Interface</title>
		<author>
			<persName><forename type="first">J</forename><surname>Stuecheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Blaner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Siegel</surname></persName>
		</author>
		<idno type="DOI">10.1147/JRD.2014.2380198</idno>
		<ptr target="https://doi.org/10.1147/JRD.2014.2380198" />
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2015-01">2015. Jan 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Post-silicon CPU Adaptation Made Practical Using Machine Learning</title>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">J</forename><surname>Tarsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangeen</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Sebot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautham</forename><surname>Chinya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayesh</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chit-Kwan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Chappell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronak</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3307650.3322267</idno>
		<ptr target="https://doi.org/10.1145/3307650.3322267" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th International Symposium on Computer Architecture</title>
		<meeting>the 46th International Symposium on Computer Architecture<address><addrLine>Phoenix, Arizona; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="14" to="26" />
		</imprint>
	</monogr>
	<note>ISCA &apos;19)</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Linus</forename><surname>Torvald</surname></persName>
		</author>
		<idno>Ac- cessed 2020-03-05</idno>
		<ptr target="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/tools/perf/pmu-events/arch" />
		<title level="m">Linux Perf Subsystem Userspace Tools</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alp</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Adji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawen</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><surname>Blei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09787</idno>
		<title level="m">A library for probabilistic modeling, inference, and criticism</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Can hardware performance counters be trusted?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sally</forename><forename type="middle">A</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName><surname>Mckee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE International Symposium on Workload Characterization</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Non-determinism and overcount on modern hardware performance counter implementations</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terpstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISPASS.2013.6557172</idno>
		<ptr target="https://doi.org/10.1109/ISPASS.2013.6557172" />
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="215" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A Top-Down method for performance analysis and counters architecture</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yasin</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISPASS.2014.6844459</idno>
		<ptr target="https://doi.org/10.1109/ISPASS.2014.6844459" />
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">ADP: Automated Diagnosis of Performance Pathologies Using Hardware Events</title>
		<author>
			<persName><forename type="first">Wucherl</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><surname>Baugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangkyum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
		<idno type="DOI">10.1145/2254756.2254791</idno>
		<ptr target="https://doi.org/10.1145/2254756.2254791" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems</title>
		<meeting>the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems<address><addrLine>London, England, UK; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="283" to="294" />
		</imprint>
	</monogr>
	<note>SIGMETRICS &apos;12)</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Apache Spark: A Unified Engine for Big Data Processing</title>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reynold</forename><forename type="middle">S</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Wendell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tathagata</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Armbrust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangrui</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivaram</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<idno type="DOI">10.1145/2934664</idno>
		<ptr target="https://doi.org/10.1145/2934664" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2016-10">2016. Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">So Many Performance Events, So Little Time</title>
		<author>
			<persName><forename type="first">Gerd</forename><surname>Zellweger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Roscoe</surname></persName>
		</author>
		<idno type="DOI">10.1145/2967360.2967375</idno>
		<ptr target="https://doi.org/10.1145/2967360.2967375" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM SIGOPS Asia-Pacific Workshop on Systems</title>
		<meeting>the 7th ACM SIGOPS Asia-Pacific Workshop on Systems<address><addrLine>Hong Kong, Hong Kong; New York, NY, USA, Article</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
