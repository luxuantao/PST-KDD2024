<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Consistency Regularization for Cross-Lingual Fine-Tuning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bo</forename><surname>Zheng</surname></persName>
							<email>bzheng@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology ‡ Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
							<email>lidong1@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
							<email>shaohanh@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zewen</forename><surname>Chi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Saksham</forename><surname>Singhal</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology ‡ Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
							<email>tliu@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology ‡ Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
							<email>fuwei@microsoft.com</email>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Consistency Regularization for Cross-Lingual Fine-Tuning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fine-tuning pre-trained cross-lingual language models can transfer task-specific supervision from one language to the others. In this work, we propose to improve cross-lingual finetuning with consistency regularization. Specifically, we use example consistency regularization to penalize the prediction sensitivity to four types of data augmentations, i.e., subword sampling, Gaussian noise, code-switch substitution, and machine translation. In addition, we employ model consistency to regularize the models trained with two augmented versions of the same training set. Experimental results on the XTREME benchmark show that our method 1 significantly improves crosslingual fine-tuning across various tasks, including text classification, question answering, and sequence labeling.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pre-trained cross-lingual language models (Conneau and <ref type="bibr" target="#b7">Lample, 2019;</ref><ref type="bibr" target="#b6">Conneau et al., 2020a;</ref><ref type="bibr" target="#b4">Chi et al., 2020)</ref> have shown great transferability across languages. By fine-tuning on labeled data in a source language, the models can generalize to other target languages, even without any additional training. Such generalization ability reduces the required annotation efforts, which is prohibitively expensive for low-resource languages.</p><p>Recent work has demonstrated that data augmentation is helpful for cross-lingual transfer, e.g., translating source language training data into target languages <ref type="bibr" target="#b31">(Singh et al., 2019)</ref>, and generating codeswitch data by randomly replacing input words in the source language with translated words in target languages <ref type="bibr" target="#b30">(Qin et al., 2020)</ref>. By populating the dataset, their fine-tuning still treats training instances independently, without considering the inherent correlations between the original input and its augmented example. In contrast, we propose to utilize consistency regularization to better leverage data augmentation for cross-lingual fine-tuning. Intuitively, for a semantic-preserving augmentation strategy, the predicted result of the original input should be similar to its augmented one. For example, the classification predictions of an English sentence and its translation tend to remain consistent.</p><p>In this work, we introduce a cross-lingual finetuning method XTUNE that is enhanced by consistency regularization and data augmentation. First, example consistency regularization enforces the model predictions to be more consistent for semantic-preserving augmentations. The regularizer penalizes the model sensitivity to different surface forms of the same example (e.g., texts written in different languages), which implicitly encourages cross-lingual transferability. Second, we introduce model consistency to regularize the models trained with various augmentation strategies. Specifically, given two augmented versions of the same training set, we encourage the models trained on these two datasets to make consistent predictions for the same example. The method enforces the corpus-level consistency between the distributions learned by two models.</p><p>Under the proposed fine-tuning framework, we study four strategies of data augmentation, i.e., subword sampling <ref type="bibr" target="#b18">(Kudo, 2018)</ref>, code-switch substitution <ref type="bibr" target="#b30">(Qin et al., 2020)</ref>, Gaussian noise <ref type="bibr" target="#b0">(Aghajanyan et al., 2020)</ref>, and machine translation. We evaluate XTUNE on the XTREME benchmark <ref type="bibr" target="#b15">(Hu et al., 2020)</ref>, including three different tasks on seven datasets. Experimental results show that our method outperforms conventional fine-tuning with data augmentation. We also demonstrate that XTUNE is flexible to be plugged in various tasks, such as classification, span extraction, and sequence labeling.</p><p>We summarize our contributions as follows:</p><p>• We propose XTUNE, a cross-lingual finetuning method to better utilize data augmentations based on consistency regularization.</p><p>• We study four types of data augmentations that can be easily plugged into cross-lingual fine-tuning.</p><p>• We give instructions on how to apply XTUNE to various downstream tasks, such as classification, span extraction, and sequence labeling.</p><p>• We conduct extensive experiments to show that XTUNE consistently improves the performance of cross-lingual fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Cross-Lingual Transfer Besides learning crosslingual word embeddings <ref type="bibr" target="#b24">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b12">Faruqui and Dyer, 2014;</ref><ref type="bibr" target="#b14">Guo et al., 2015;</ref><ref type="bibr" target="#b35">Xu et al., 2018;</ref><ref type="bibr" target="#b33">Wang et al., 2019)</ref>, most recent work of cross-lingual transfer is based on pre-trained cross-lingual language models <ref type="bibr" target="#b7">(Conneau and Lample, 2019;</ref><ref type="bibr" target="#b6">Conneau et al., 2020a;</ref><ref type="bibr" target="#b4">Chi et al., 2020)</ref>. These models generate multilingual contextualized word representations for different languages with a shared encoder and show promising cross-lingual transferability.</p><p>Cross-Lingual Data Augmentation Machine translation has been successfully applied to the cross-lingual scenario as data augmentation. A common way to use machine translation is to finetune models on both source language training data and translated data in all target languages. Furthermore, <ref type="bibr" target="#b31">Singh et al. (2019)</ref> proposed to replace a segment of source language input text with its translation in another language. However, it is usually impossible to map the labels in source language data into target language translations for token-level tasks. <ref type="bibr" target="#b38">Zhang et al. (2019)</ref> used code-mixing to perform the syntactic transfer in cross-lingual dependency parsing. <ref type="bibr">Fei et al. (2020)</ref> constructed pseudo translated target corpora from the gold-standard annotations of the source languages for cross-lingual semantic role labeling. <ref type="bibr" target="#b11">Fang et al. (2020)</ref> proposed an additional Kullback-Leibler divergence self-teaching loss for model training, based on autogenerated soft pseudo-labels for translated text in the target language. Besides, <ref type="bibr" target="#b30">Qin et al. (2020)</ref> finetuned models on multilingual code-switch data, which achieves considerable improvements.</p><p>Consistency Regularization One strand of work in consistency regularization focused on regularizing model predictions to be invariant to small perturbations on image data. The small perturbations can be random noise <ref type="bibr" target="#b39">(Zheng et al., 2016)</ref>, adversarial noise <ref type="bibr" target="#b25">(Miyato et al., 2019;</ref><ref type="bibr" target="#b3">Carmon et al., 2019)</ref> and various data augmentation approaches <ref type="bibr" target="#b16">(Hu et al., 2017;</ref><ref type="bibr" target="#b37">Ye et al., 2019;</ref><ref type="bibr" target="#b34">Xie et al., 2020)</ref>. Similar ideas are used in the natural language processing area. Both adversarial noise <ref type="bibr" target="#b40">(Zhu et al., 2020;</ref><ref type="bibr" target="#b17">Jiang et al., 2020;</ref><ref type="bibr" target="#b22">Liu et al., 2020)</ref> and sampled Gaussian noise <ref type="bibr" target="#b0">(Aghajanyan et al., 2020</ref>) are adopted to augment input word embeddings.</p><p>Another strand of work focused on consistency under different model parameters <ref type="bibr" target="#b32">(Tarvainen and Valpola, 2017;</ref><ref type="bibr" target="#b2">Athiwaratkun et al., 2019)</ref>, which is complementary to the first strand. We focus on the cross-lingual setting, where consistency regularization has not been fully explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>Conventional cross-lingual fine-tuning trains a pretrained language model on the source language and directly evaluates it on other languages, which is also known as the setting of zero-shot cross-lingual fine-tuning. Specifically, given a training corpus D in the source language (typically in English), and a model f (•; θ) that predicts task-specific probability distributions, we define the loss of cross-lingual fine-tuning as:</p><formula xml:id="formula_0">L task (D, θ) = x∈D (f (x; θ), G(x)),</formula><p>where G(x) denotes the ground-truth label of example x, (•, •) is the loss function depending on the downstream task. Apart from vanilla cross-lingual fine-tuning on the source language, recent work shows that data augmentation is helpful to improve performance on the target languages. For example, <ref type="bibr" target="#b7">Conneau and Lample (2019)</ref> add translated examples to the training set for better cross-lingual transfer. Let A(•) be a cross-lingual data augmentation strategy (such as code-switch substitution), and D A = D ∪ {A(x) | x ∈ D} be the augmented training corpus, the fine-tuning loss is L task (D A , θ). Notice that it is non-trivial to apply some augmentations for tokenlevel tasks directly. For instance, in part-of-speech tagging, the labels of source language examples can not be mapped to the translated examples because of the lack of explicit alignments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">XTUNE: Cross-Lingual Fine-Tuning with Consistency Regularization</head><p>We propose to improve cross-lingual fine-tuning with two consistency regularization methods, so that we can effectively leverage cross-lingual data augmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Example Consistency Regularization</head><p>In order to encourage consistent predictions for an example and its semantically equivalent augmentation, we introduce example consistency regularization, which is defined as follows:</p><formula xml:id="formula_1">R 1 (D, θ, A) = x∈D KL S (f (x; θ) f (A(x); θ)), KL S (P, Q) = KL(stopgrad(P ) Q)+ KL(stopgrad(Q) P )</formula><p>where KL S (•) is the symmertrical Kullback-Leibler divergence. The regularizer encourages the predicted distributions f (x; θ) and f (A(x); θ) to agree with each other. The stopgrad(•) operation 2 is used to stop back-propagating gradients, which is also employed in <ref type="bibr" target="#b17">(Jiang et al., 2020;</ref><ref type="bibr" target="#b22">Liu et al., 2020)</ref>. The ablation studies in Section 4.2 empirically show that the operation improves fine-tuning performance.</p><p>2 Implemented by .detach() in PyTorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Model Consistency Regularization</head><p>While the example consistency regularization is conducted at the example level, we propose the model consistency to further regularize the model training at the corpus level. The regularization is conducted at two stages. First, we obtain a finetuned model θ * on the training corpus D:</p><formula xml:id="formula_2">θ * = arg min θ 1 L task (D, θ 1 ).</formula><p>In the second stage, we keep the parameters θ * fixed. The regularization term is defined as:</p><formula xml:id="formula_3">R 2 (D A , θ, θ * ) = x∈D A KL(f (x; θ * ) f (x; θ))</formula><p>where D A is the augmented training corpus, and KL(•) is Kullback-Leibler divergence. For each example x of the augmented training corpus D A , the model consistency regularization encourages the prediction f (x; θ) to be consistent with f (x; θ * ).</p><p>The regularizer enforces the corpus-level consistency between the distributions learned by two models.</p><p>An unobvious advantage of model consistency regularization is the flexibility with respect to data augmentation strategies. For the example of partof-speech tagging, even though the labels can not be directly projected from an English sentence to its translation, we are still able to employ the regularizer. Because the term R 2 is put on the same example x ∈ D A , we can always align the tokenlevel predictions of the models θ and θ * .  </p><formula xml:id="formula_4">I</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Full XTUNE Fine-Tuning</head><p>As shown in Figure <ref type="figure" target="#fig_0">1</ref>, we combine example consistency regularization R 1 and model consistency regularization R 2 as a two-stage fine-tuning process. Formally, we fine-tune a model with R 1 in the first stage:</p><formula xml:id="formula_5">θ * = arg min θ 1 L task (D, θ 1 ) + R 1 (D, θ 1 , A * )</formula><p>where the parameters θ * are kept fixed for R 2 in the second stage. Then the final loss is computed via:</p><formula xml:id="formula_6">L XTUNE = L task (D A , θ) + λ 1 R 1 (D A , θ, A ) + λ 2 R 2 (D A , θ, θ * )</formula><p>where λ 1 and λ 2 are the corresponding weights of two regularization methods. Notice that the data augmentation strategies A, A , and A * can be either different or the same, which are tuned as hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Augmentation</head><p>We consider four types of data augmentation strategies in this work, which are shown in Figure <ref type="figure" target="#fig_1">2</ref>. We aim to study the impact of different data augmentation strategies on cross-lingual transferability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Subword Sampling</head><p>Representing a sentence in different subword sequences can be viewed as a data augmentation strategy <ref type="bibr" target="#b18">(Kudo, 2018;</ref><ref type="bibr" target="#b29">Provilkov et al., 2020)</ref>. We utilize XLM-R <ref type="bibr" target="#b6">(Conneau et al., 2020a)</ref> as our pre-trained cross-lingual language model, while it applies subword tokenization directly on raw text data using SentencePiece <ref type="bibr" target="#b19">(Kudo and Richardson, 2018)</ref> with a unigram language model <ref type="bibr" target="#b18">(Kudo, 2018)</ref>. As one of our data augmentation strategies, we apply the on-the-fly subword sampling algorithm in the unigram language model to generate multiple subword sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Gaussian Noise</head><p>Most data augmentation strategies in NLP change input text discretely, while we directly add random perturbation noise sampled from Gaussian distribution on the input embedding layer to conduct data augmentation. When combining this data augmentation with example consistency R 1 , the method is similar to the stability training <ref type="bibr" target="#b39">(Zheng et al., 2016)</ref>, random perturbation training <ref type="bibr" target="#b25">(Miyato et al., 2019)</ref> and the R3F method <ref type="bibr" target="#b0">(Aghajanyan et al., 2020)</ref>. We also explore Gaussian noise's capability to generate new examples on continuous input space for conventional fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Code-Switch Substitution</head><p>Anchor points have been shown useful to improve cross-lingual transferability. <ref type="bibr" target="#b9">Conneau et al. (2020b)</ref> analyzed the impact of anchor points in pre-training cross-lingual language models. Following <ref type="bibr" target="#b30">Qin et al. (2020)</ref>, we generate code-switch data in multiple languages as data augmentation. We randomly select words in the original text in the source language and replace them with target language words in the bilingual dictionaries to obtain code-switch data. Intuitively, this type of data augmentation explicitly helps pre-trained cross-lingual models align the multilingual vector space by the replaced anchor points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Machine Translation</head><p>Machine translation has been proved to be an effective data augmentation strategy <ref type="bibr" target="#b31">(Singh et al., 2019)</ref> under the cross-lingual scenario. However, the ground-truth labels of translated data can be unavailable for token-level tasks (see Section 3), which disables conventional fine-tuning on the augmented data. Meanwhile, our proposed model consistency R 2 can not only serve as consistency regularization but also can be viewed as a self-training objective to enable semi-supervised training on the unlabeled target language translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Task Adaptation</head><p>We give instructions on how to apply XTUNE to various downstream tasks, i.e., classification, span extraction, and sequence labeling. By default, we use model consistency R 2 in full XTUNE. We describe the usage of example consistency R 1 as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Classification</head><p>For classification task, the model is expected to predict one distribution per example on n label types, i.e., model f (•; θ) should predict a probability distribution p cls ∈ R n label . Thus we can directly use example consistency R 1 to regularize the consistency of the two distributions for all four types of our data augmentation strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Span Extraction</head><p>For span extraction task, the model is expected to predict two distributions per example p start , p end ∈ R n subword , indicating the probability distribution of where the answer span starts and ends, n subword denotes the length of the tokenized input text. For Gaussian noise, the subword sequence remains unchanged so that example consistency R 1 can be directly applied to the two distributions. Since subword sampling and code-switch substitution will change n subword , we control the ratio of words to be modified and utilize example consistency R 1 on unchanged positions only. We do not use the example consistency R 1 for machine translation because it is impossible to explicitly align the two distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Sequence Labeling</head><p>Recent pre-trained language models generate representations at the subword-level. For sequence labeling tasks, these models predict label distributions on each word's first subword. Therefore, the model is expected to predict n word probability distributions per example on n label types. Unlike span extraction, subword sampling, code-switch substitution, and Gaussian noise do not change n word . Thus the three data augmentation strategies will not affect the usage of example consistency R 1 . Although word alignment is a possible solution to map the predicted label distributions between translation pairs, the word alignment process will introduce more noise. Therefore, we do not employ machine translation as data augmentation for the example consistency R 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head><p>Datasets For our experiments, we select three types of cross-lingual understanding tasks from XTREME benchmark <ref type="bibr" target="#b15">(Hu et al., 2020)</ref>, including two classification datasets: XNLI <ref type="bibr" target="#b8">(Conneau et al., 2018)</ref>, PAWS-X <ref type="bibr" target="#b36">(Yang et al., 2019)</ref>, three span extraction datasets: XQuAD <ref type="bibr" target="#b1">(Artetxe et al., 2020)</ref>, <ref type="bibr">MLQA (Lewis et al., 2020)</ref>, TyDiQA-GoldP <ref type="bibr" target="#b5">(Clark et al., 2020)</ref>, and two sequence labeling datasets: NER <ref type="bibr" target="#b27">(Pan et al., 2017)</ref>, POS <ref type="bibr" target="#b26">(Nivre et al., 2018)</ref>. The statistics of the datasets are shown in the supplementary document.</p><p>Fine-Tuning Settings We consider two typical fine-tuning settings from <ref type="bibr" target="#b6">Conneau et al. (2020a)</ref> and <ref type="bibr" target="#b15">Hu et al. (2020)</ref> in our experiments, which are (1) cross-lingual transfer: the models are finetuned on English training data without translation available, and directly evaluated on different target languages; (2) translate-train-all: translationbased augmentation is available, and the models are fine-tuned on the concatenation of English training data and its translated data on all target languages. Since the official XTREME repository<ref type="foot" target="#foot_0">3</ref> does not provide translated target language data for POS and NER, we use Google Translate to obtain translations for these two datasets.</p><p>Implementation Details We utilize XLM-R <ref type="bibr" target="#b6">(Conneau et al., 2020a)</ref> as our pre-trained cross-lingual language model. The bilingual dictionaries we used for code-switch substitution are from MUSE <ref type="bibr" target="#b20">(Lample et al., 2018)</ref>. <ref type="foot" target="#foot_1">4</ref> For languages that cannot be found in MUSE, we ignore these languages since other bilingual dictionaries might be of poorer quality. For the POS dataset, we use the average-pooling strategy on subwords to obtain word representation since part-of-speech is related to different parts of words, depending on the language. We tune the hyper-parameter and select the model with the best average results over all the languages' development set. There are two datasets without development set in multi-languages. For XQuAD, we tune the hyper-parameters with the development set of MLQA since they share the same training set and have a higher degree of overlap in languages. For TyDiQA-GoldP, we use the English test set  <ref type="bibr" target="#b6">(Conneau et al., 2020a)</ref> are taken from <ref type="bibr" target="#b15">(Hu et al., 2020)</ref>. Results of XLM-R large under the translate-train-all setting are from FILTER <ref type="bibr" target="#b11">(Fang et al., 2020)</ref>. The results of XTUNE are from the best models selected with the performance on the corresponding development set. as the development set. In order to make a fair comparison, the ratio of data augmentation in D A is all set to 1.0. The detailed hyper-parameters are shown in the supplementary document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Table <ref type="table" target="#tab_1">1</ref> shows our results on XTREME. For the cross-lingual transfer setting, we outperform previous works on all seven cross-lingual language understanding datasets. <ref type="foot" target="#foot_2">5</ref> Compared to XLM-R large baseline, we achieve an absolute 4.9-point improvement (70.0 vs. 74.9) on average over seven datasets.</p><p>For the translate-train-all setting, we achieved stateof-the-art results on six of the seven datasets. Com-pared to FILTER,<ref type="foot" target="#foot_3">6</ref> we achieve an absolute 2.1point improvement (74.4 vs. 76.5), and we do not need English translations during inference.</p><p>Table <ref type="table" target="#tab_2">2</ref> shows how the two regularization methods affect the model performance separately. For the cross-lingual transfer setting, XTUNE achieves an absolute 2.8-point improvement compared to our implemented XLM-R base baseline. Meanwhile, fine-tuning with only example consistency R 1 and model consistency R 2 degrades the averaged results by 0.4 and 1.0 points, respectively.</p><p>For the translate-train-all setting, our proposed model consistency R 2 enables training on POS and NER even if labels of target language translations Model en ar bg de el es fr hi ru sw th tr ur vi zh Avg.</p><p>Cross-lingual-transfer (models are fine-tuned on English training data without translation available)</p><p>R3F <ref type="bibr" target="#b0">(Aghajanyan et al., 2020)</ref>  Table <ref type="table">3</ref>: XNLI accuracy scores for each language. XLM-R large under the cross-lingual transfer setting are from <ref type="bibr" target="#b15">(Hu et al., 2020)</ref>. Results of XLM-R large under the translate-train-all setting are from <ref type="bibr" target="#b11">(Fang et al., 2020)</ref>. are unavailable in these two datasets. To make a fair comparison in the translate-train-all setting, we augment the English training corpus with target language translations when fine-tuning with only example consistency R 1 . Otherwise, we only use the English training corpus in the first stage, as shown in Figure <ref type="figure" target="#fig_0">1</ref>(a). Compared to XTUNE, the performance drop on two classification datasets under this setting is relatively small since R 1 can be directly applied between translation-pairs in any languages. However, the performance is significantly degraded in three question answering datasets, where we can not align the predicted distributions between translation-pairs in R 1 . We use subword sampling as the data augmentation strategy in R 1 for this situation. Fine-tuning with only model consistency R 2 degrades the overall performance by 1.1 points. These results demonstrate that the two consistency regularization methods complement each other. Be- sides, we observe that removing stopgrad degrades the overall performance by 0.5 points.</p><p>Table <ref type="table">3</ref> provides results of each language on the XNLI dataset. For the cross-lingual transfer setting, we utilize code-switch substitution as data augmentation for both example consistency R 1 and model consistency R 2 . We utilize all the bilingual dictionaries, except for English to Swahili and English to Urdu, which MUSE does not provide. Results show that our method outperforms all baselines on each language, even on Swahili (+2.2 points) and Urdu (+5.4 points), indicating our method can be generalized to low-resource languages even without corresponding machine translation systems or bilingual dictionaries. For translate-train-all setting, we utilize machine translation as data augmentation for both example consistency R 1 and model consistency R 2 . We improve the XLM-R large baseline by +2.2 points on average, while we still have +0.9 points on average compared to FILTER. It is worth mentioning that we do not need corresponding English translations during inference. Complete results on other datasets are provided in the supplementary document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis</head><p>It is better to employ data augmentation for consistency regularization than for conventional fine-tuning. As shown in Table <ref type="table" target="#tab_4">4</ref>, com- pared to employing data augmentation for conventional fine-tuning (Data Aug.), our regularization methods (XTUNE R 1 , XTUNE R 2 ) consistently improve the model performance under all four data augmentation strategies. Since there is no labeled data on translations in POS and the issue of distribution alignment in example consistency R 1 , when machine translation is utilized as data augmentation, the results for Data Aug. and XTUNE R 1 in POS, as well as XTUNE R 1 in MLQA, are unavailable. We observe that Data Aug. can enhance the overall performance for coarse-grained tasks like XNLI, while our methods can further improve the results. However, Data Aug. even causes the performance to degrade for fine-grained tasks like MLQA and POS. In contrast, our proposed two consistency regularization methods improve the performance by a large margin (e.g., for MLQA under code-switch data augmentation, Data Aug. decreases baseline by 1.2 points, while XTUNE R 1 increases baseline by 2.6 points). We give detailed instructions on how to choose data augmentation strategies for XTUNE in the supplementary document.</p><p>XTUNE improves cross-lingual retrieval. We fine-tune the models on XNLI with different settings and compare their performance on two crosslingual retrieval datasets. Following <ref type="bibr" target="#b4">Chi et al. (2020)</ref> and <ref type="bibr" target="#b15">Hu et al. (2020)</ref>, we utilize representations averaged with hidden-states on the layer 8 of XLM-R base . As shown in Table <ref type="table" target="#tab_5">5</ref>, we observe significant improvement from the translatetrain-all baseline to fine-tuning with only example consistency R 1 , this suggests regularizing the taskspecific output of translation-pairs to be consistent also encourages the model to generate language-invariant representations. XTUNE only slightly improves upon this setting, indicating R 1 between translation-pairs is the most important factor to improve cross-lingual retrieval task.</p><p>XTUNE improves decision boundaries as well as the ability to generate language-invariant representations. As shown in Figure <ref type="figure" target="#fig_2">3</ref>, we present t-SNE visualization of examples from the XNLI development set under three different settings. We observe the model fine-tuned with XTUNE significantly improves the decision boundaries of different labels. Besides, for an English example and its translations in other languages, the model fine-tuned with XTUNE generates more similar representations compared to the two baseline models. This observation is also consistent with the cross-lingual retrieval results in Table <ref type="table" target="#tab_5">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we present a cross-lingual fine-tuning framework XTUNE to make better use of data augmentation. We propose two consistency regularization methods that encourage the model to make consistent predictions for an example and its semantically equivalent data augmentation. We explore four types of cross-lingual data augmentation strategies. We show that both example and model consistency regularization considerably boost the performance compared to directly fine-tuning on data augmentations. Meanwhile, model consistency regularization enables semi-supervised training on the unlabeled target language translations. XTUNE combines the two regularization methods, and the experiments show that it can improve the performance by a large margin on the XTREME benchmark. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Classification</head><p>The two distribution in example consistency R 1 can always be aligned. Therefore, we recommend using machine translation as data augmentation if the machine translation systems are available. Otherwise, the priority of our data augmentation strategies is code-switch substitution, subword sampling and Gaussian noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Span Extraction</head><p>The two distribution in example consistency R 1 can not be aligned in translation-pairs. Therefore, it is impossible to use machine translation as data augmentation in example consistency R 1 . We prefer to use code-switch when applying example consistency R 1 individually. However, when the training corpus is augmented with translations, since the bilingual dictionaries between arbitrary language pairs may not be available, we recommend using subword sampling in example consistency R 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Sequence Labeling</head><p>Similar to span extraction, the two distribution in example consistency R 1 can not be aligned in translation-pairs. Therefore, we do not use machine translation in example consistency R 1 . Unlike classification and span extraction, sequence labeling requires finer-grained information and is more sensitive to noise. We found code-switch is worse than subword sampling as data augmentation in both example consistency R 1 and model consistency R 2 , it will even degrade performance for certain hyperparameters. Thus we recommend using subword sampling in example consistency R 1 , and use machine translation to augment the English training corpus if machine translation systems are available, otherwise subword sampling. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of our two-stage fine-tuning algorithm. The model parameters f (•; θ * ) in the second stage are copied from the first stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Cross-lingual data augmentation strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3</head><label>3</label><figDesc>Figure 3: t-SNE visualization of 100 examples in four languages from the XNLI development set (best viewed in color). We fine-tune the XLM-R base model on XNLI and use the hidden states of [CLS] symbol in the last layer. with different labels are represented with different colors. Examples in different languages are represented with different markers. The red lines connect English examples and their translations in target languages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>_I/_love/_to/_eat/_apple/s/. _I/_love/_to/_e/a/t/_app/l/es/. _/I/_lo/ve/_to/_e/at/_app/l/es/. I 喜欢 to essen apples.I liebe to eat 苹果. Ich 喜欢 to eat pommes.</figDesc><table><row><cell>Subword</cell><cell></cell></row><row><cell>Sampling</cell><cell></cell></row><row><cell>Gaussian</cell><cell></cell></row><row><cell>Noise</cell><cell></cell></row><row><cell></cell><cell>Embedding Layer</cell></row><row><cell>love to</cell><cell>I love to eat apples.</cell></row><row><cell>eat apples.</cell><cell></cell></row><row><cell>Code-</cell><cell></cell></row><row><cell>Switch</cell><cell></cell></row><row><cell>Machine Translation</cell><cell>Ich mag es, Äpfel zu essen J'adore manger des pommes.</cell></row><row><cell></cell><cell>我喜欢吃苹果。</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Evaluation results on the XTREME benchmark. Results of mBERT<ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>, XLM<ref type="bibr" target="#b7">(Conneau and Lample, 2019)</ref> and XLM-R large</figDesc><table><row><cell>Model</cell><cell cols="4">Pair Sentence XNLI PAWS-X POS Structure Prediction NER</cell><cell cols="3">Question Answering XQuAD MLQA TyDiQA</cell></row><row><cell>Metrics</cell><cell>Acc.</cell><cell>Acc.</cell><cell>F1</cell><cell>F1</cell><cell>F1/EM</cell><cell>F1/EM</cell><cell cols="2">F1/EM Avg.</cell></row><row><cell cols="7">Cross-lingual-transfer (models are fine-tuned on English training data without translation available)</cell><cell></cell></row><row><cell>mBERT</cell><cell>65.4</cell><cell>81.9</cell><cell>70.3</cell><cell>62.2</cell><cell cols="4">64.5/49.4 61.4/44.2 59.7/43.9 63.1</cell></row><row><cell>XLM</cell><cell>69.1</cell><cell>80.9</cell><cell>70.1</cell><cell>61.2</cell><cell cols="4">59.8/44.3 48.5/32.6 43.6/29.1 58.6</cell></row><row><cell cols="2">X-STILTs (Phang et al., 2020) 80.4</cell><cell>87.7</cell><cell>74.4</cell><cell>63.4</cell><cell cols="4">77.2/61.3 72.3/53.5 76.0/59.5 72.3</cell></row><row><cell>VECO (Luo et al., 2020)</cell><cell>79.9</cell><cell>88.7</cell><cell>75.1</cell><cell>65.7</cell><cell cols="4">77.3/61.8 71.7/53.2 67.6/49.1 71.4</cell></row><row><cell>XLM-Rlarge</cell><cell>79.2</cell><cell>86.4</cell><cell>72.6</cell><cell>65.4</cell><cell cols="4">76.6/60.8 71.6/53.2 65.1/45.0 70.0</cell></row><row><cell>XTUNE</cell><cell>82.6</cell><cell>89.8</cell><cell>78.5</cell><cell>69.3</cell><cell cols="4">79.4/64.4 74.4/56.2 74.8/59.4 74.9</cell></row><row><cell cols="7">Translate-train-all (translation-based augmentation is available for English training data)</cell><cell></cell></row><row><cell>VECO (Luo et al., 2020)</cell><cell>83.0</cell><cell>91.1</cell><cell>75.1</cell><cell>65.7</cell><cell cols="4">79.9/66.3 73.1/54.9 75.0/58.9 74.1</cell></row><row><cell>FILTER (Fang et al., 2020)</cell><cell>83.9</cell><cell>91.4</cell><cell>76.2</cell><cell>67.7</cell><cell cols="4">82.4/68.0 76.2/57.7 68.3/50.9 74.4</cell></row><row><cell>XLM-Rlarge</cell><cell>82.6</cell><cell>90.4</cell><cell>-</cell><cell>-</cell><cell cols="3">80.2/65.9 72.8/54.3 66.5/47.7</cell><cell>-</cell></row><row><cell>XTUNE</cell><cell>84.8</cell><cell>91.6</cell><cell>79.3</cell><cell>69.9</cell><cell cols="4">82.5/69.0 75.0/57.1 75.4/60.8 76.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation studies on the XTREME benchmark. All numbers are averaged over five random seeds.</figDesc><table><row><cell>Model</cell><cell cols="4">Pair Sentence XNLI PAWS-X POS Structure Prediction NER</cell><cell cols="3">Question Answering XQuAD MLQA TyDiQA</cell></row><row><cell>Metrics</cell><cell>Acc.</cell><cell>Acc.</cell><cell>F1</cell><cell>F1</cell><cell>F1/EM</cell><cell>F1/EM</cell><cell>F1/EM</cell></row><row><cell cols="7">Cross-lingual-transfer (models are fine-tuned on English training data without translation available)</cell><cell></cell></row><row><cell>XLM-Rbase</cell><cell>74.9</cell><cell>84.9</cell><cell>75.6</cell><cell>61.8</cell><cell cols="3">71.9/56.4 65.0/47.1 55.4/38.3</cell></row><row><cell>XTUNE</cell><cell>77.7</cell><cell>87.5</cell><cell>76.5</cell><cell>63.0</cell><cell cols="3">73.9/59.0 68.1/50.2 61.2/45.2</cell></row><row><cell cols="2">with only example consistency R1 77.6</cell><cell>87.2</cell><cell>76.3</cell><cell>62.4</cell><cell cols="3">73.6/58.6 67.6/49.7 60.7/44.4</cell></row><row><cell>with only model consistency R2</cell><cell>76.6</cell><cell>86.3</cell><cell>76.3</cell><cell>63.0</cell><cell cols="3">73.2/58.1 66.7/49.0 59.2/42.3</cell></row><row><cell cols="6">Translate-train-all (translation-based augmentation is available for English training data)</cell><cell></cell><cell></cell></row><row><cell>XLM-Rbase</cell><cell>78.8</cell><cell>88.4</cell><cell>-</cell><cell>-</cell><cell cols="3">75.2/61.4 67.8/50.1 63.7/47.7</cell></row><row><cell>XTUNE</cell><cell>80.6</cell><cell>89.4</cell><cell>77.8</cell><cell>63.7</cell><cell cols="3">78.1/64.4 69.7/52.1 65.9/51.1</cell></row><row><cell cols="2">with only example consistency R1 80.5</cell><cell>89.3</cell><cell>-</cell><cell>-</cell><cell cols="3">76.1/62.5 69.1/51.6 65.1/50.3</cell></row><row><cell>with only model consistency R2</cell><cell>78.9</cell><cell>88.5</cell><cell>76.6</cell><cell>63.5</cell><cell cols="3">77.4/63.4 68.7/51.1 64.5/48.7</cell></row><row><cell>remove stopgrad in R1</cell><cell>80.2</cell><cell>89.1</cell><cell>76.8</cell><cell>63.4</cell><cell cols="3">77.3/63.4 69.9/52.1 65.1/50.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison between different data augmentation strategies. "Data Aug." uses data augmentation for conventional fine-tuning. "XTUNE R1 " denotes finetuning with only example consistency R 1 . "XTUNE R2 " denotes fine-tuning with only model consistency R 2 .</figDesc><table><row><cell>Method</cell><cell>Model</cell><cell cols="2">XNLI POS</cell><cell>MLQA</cell></row><row><cell>Baseline</cell><cell>XLM-Rbase</cell><cell>74.9</cell><cell cols="2">75.6 65.0/47.1</cell></row><row><cell>Subword Sampling</cell><cell>Data Aug. XTUNER 1 XTUNER 2</cell><cell>75.3 76.5 75.8</cell><cell cols="2">75.8 64.7/46.7 76.3 67.4/49.5 76.3 66.7/49.0</cell></row><row><cell>Gaussian Noise</cell><cell>Data Aug. XTUNER 1 XTUNER 2</cell><cell>74.7 76.3 75.5</cell><cell cols="2">75.6 64.2/46.1 75.7 66.7/48.9 76.2 66.3/48.5</cell></row><row><cell>Code-Switch</cell><cell>Data Aug. XTUNER 1 XTUNER 2</cell><cell>76.5 77.6 76.8</cell><cell cols="2">75.1 63.8/45.9 75.8 67.6/49.7 76.1 66.3/48.6</cell></row><row><cell>Machine Translation</cell><cell>Data Aug. XTUNER 1 XTUNER 2</cell><cell>78.8 79.7 78.9</cell><cell cols="2">--76.6 68.7/51.1 67.8/50.1 -</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Results of cross-lingual retrieval with the models fine-tuned on XNLI.</figDesc><table><row><cell>Model</cell><cell cols="2">Tatoeba BUCC</cell></row><row><cell>XLM-Rbase (cross-lingual transfer)</cell><cell>74.2</cell><cell>78.2</cell></row><row><cell>XLM-Rbase (translate-train-all)</cell><cell>79.7</cell><cell>79.7</cell></row><row><cell>XTUNE (translate-train-all)</cell><cell>82.3</cell><cell>82.2</cell></row><row><cell>with only example consistency R1</cell><cell>82.0</cell><cell>82.1</cell></row><row><cell>with only model consistency R2</cell><cell>79.5</cell><cell>79.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>The best hyper-parameters used for XTUNE under the cross-lingual transfer setting. "SS", "CS", "MT" denote the data augmentation methods: subword sampling, code-switch substitution, and machine translation, respectively.</figDesc><table><row><cell></cell><cell cols="8">Variable XNLI PAWS-X POS NER XQuAD MLQA TyDiQA</cell></row><row><cell>Stage 1</cell><cell>A  *</cell><cell>CS</cell><cell>CS</cell><cell>SS</cell><cell>SS</cell><cell>CS</cell><cell>CS</cell><cell>SS</cell></row><row><cell>Stage 2</cell><cell>A A</cell><cell>CS CS</cell><cell>CS CS</cell><cell>SS SS</cell><cell>SS SS</cell><cell>SS SS</cell><cell>SS SS</cell><cell>SS SS</cell></row><row><cell>Hyper-parameters</cell><cell>λ 1 λ 2</cell><cell>5.0 5.0</cell><cell>5.0 2.0</cell><cell>5.0 0.3</cell><cell>5.0 5.0</cell><cell>5.0 5.0</cell><cell>5.0 5.0</cell><cell>5.0 5.0</cell></row><row><cell></cell><cell cols="8">Variable XNLI PAWS-X POS NER XQuAD MLQA TyDiQA</cell></row><row><cell>Stage 1</cell><cell>A  *</cell><cell>MT</cell><cell>MT</cell><cell>SS</cell><cell>SS</cell><cell>CS</cell><cell>CS</cell><cell>SS</cell></row><row><cell>Stage 2</cell><cell>A A</cell><cell>MT MT</cell><cell>MT MT</cell><cell>MT SS</cell><cell>MT SS</cell><cell>MT SS</cell><cell>MT SS</cell><cell>MT SS</cell></row><row><cell>Hyper-parameters</cell><cell>λ 1 λ 2</cell><cell>5.0 1.0</cell><cell>5.0 1.0</cell><cell>5.0 0.3</cell><cell>5.0 1.0</cell><cell>5.0 0.1</cell><cell>5.0 0.5</cell><cell>5.0 0.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>The best hyper-parameters used for XTUNE under the translate-train-all setting. "SS", "CS", "MT" denote the data augmentation methods subword sampling, code-switch substitution, and machine translation, respectively.</figDesc><table><row><cell>Method</cell><cell>Model</cell><cell cols="2">XNLI POS MLQA Avg.</cell></row><row><cell>-</cell><cell cols="2">XLM-Rbase 10.6 20.8</cell><cell>20.3 17.2</cell></row><row><cell>Subword Sampling</cell><cell cols="2">Data Aug. 10.5 20.5 XTUNER 1 10.2 20.2 XTUNER 2 10.6 20.1</cell><cell>20.2 17.1 19.6 16.7 19.8 16.8</cell></row><row><cell>Gaussian Noise</cell><cell cols="2">Data Aug. 10.8 20.6 XTUNER 1 10.5 20.7 XTUNER 2 10.8 20.2</cell><cell>19.8 17.1 19.8 17.0 19.7 16.9</cell></row><row><cell>Code-Switch</cell><cell>Data Aug. XTUNER 1 XTUNER 2</cell><cell>9.2 21.1 9.1 20.7 8.8 20.2</cell><cell>20.5 16.9 19.4 16.4 20.0 16.3</cell></row><row><cell>Machine Translation</cell><cell>Data Aug. XTUNER 1 XTUNER 2</cell><cell>7.2 6.9 7.2 19.6 --</cell><cell>17.9 -17.1 14.6 --</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Cross-lingual transfer gap, i.e., averaged performance drop between English and other languages in zero-shot transfer. A smaller gap indicates better transferability. For MLQA, we report the average of F1-scores and exact match scores.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Cross-lingual-transfer (models are fine-tuned on English training data without translation available)</figDesc><table><row><cell>Model</cell><cell>en</cell><cell>de</cell><cell>es</cell><cell>fr</cell><cell>ja</cell><cell>ko</cell><cell>zh</cell><cell>Avg.</cell></row><row><cell>XLM-Rlarge</cell><cell>94.7</cell><cell>89.7</cell><cell>90.1</cell><cell>90.4</cell><cell>78.7</cell><cell>79.0</cell><cell>82.3</cell><cell>86.4</cell></row><row><cell>XTUNE</cell><cell>96.0</cell><cell>92.5</cell><cell>92.2</cell><cell>92.7</cell><cell>84.9</cell><cell>84.2</cell><cell>86.6</cell><cell>89.8</cell></row><row><cell cols="7">Translate-train-all (translation-based augmentation is available for English training data)</cell><cell></cell><cell></cell></row><row><cell>FILTER (Fang et al., 2020)</cell><cell>95.9</cell><cell>92.8</cell><cell>93.0</cell><cell>93.7</cell><cell>87.4</cell><cell>87.6</cell><cell>89.6</cell><cell>91.5</cell></row><row><cell>XTUNE</cell><cell>96.1</cell><cell>92.6</cell><cell>93.1</cell><cell>93.9</cell><cell>87.8</cell><cell>89.0</cell><cell>88.8</cell><cell>91.6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0">github.com/google-research/xtreme</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">github.com/facebookresearch/MUSE</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2">X-STILTs<ref type="bibr" target="#b28">(Phang et al., 2020)</ref> uses additional SQuAD v1.1 English training data for the TyDiQA-GoldP dataset, while we prefer a cleaner setting here.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3">FILTER directly selects the best model on the test set of XQuAD and TyDiQA-GoldP. Under this setting, we can obtain 83.1/69.7 for XQuAD,</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="75" xml:id="foot_4">.5/61.1 for TyDiQA-GoldP.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Wanxiang Che is the corresponding author. This work was supported by the National Key R&amp;D Program of China via grant 2020AAA0106501 and the National Natural Science Foundation of China (NSFC) via grant 61976072 and 61772153.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Hyper-Parameters</head><p>For XNLI, PAWS-X, POS and NER, we fine-tune 10 epochs. For XQuAD and MLQA, we fine-tune 4 epochs. For TyDiQA-GoldP, we fine-tune 20 epochs and 10 epochs for base and large model, respectively. We select λ 1 in [1.0, 2.0, 5.0], λ 2 in [0.3, 0.5, 1.0, 2.0, 5.0]. For learning rate, we select in [5e-6, 7e-6, 1e-5, 1.5e-5] for large models, [7e-6, 1e-5, 2e-5, 3e-5] for base models. We use batch size 32 for all datasets and 10% of total training steps for warmup with a linear learning rate schedule. Our experiments are conducted with a single 32GB Nvidia V100 GPU, and we use gradient accumulation for large-size models. The other hyper-parameters for the two-stage XTUNE training are shown in Table <ref type="table">7</ref> and Table <ref type="table">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Results for Each Dataset and Language</head><p>We provide detailed results for each dataset and language below. We compare our method against XLM-R large for cross-lingual transfer setting, FIL-TER <ref type="bibr" target="#b11">(Fang et al., 2020)</ref> for translate-train-all setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D How to Select Data Augmentation Strategies in XTUNE</head><p>We give instructions on selecting a proper data augmentation strategy depending on the corresponding task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Cross-Lingual Transfer Gap</head><p>As shown in Table <ref type="table">9</ref>, the cross-lingual transfer gap can be reduced under all four data augmentation strategies. Meanwhile, we observe machine translation and code-switch substitution achieve a smaller cross-lingual transfer gap than the other two data augmentation methods. This suggests the data augmentation methods with cross-lingual knowledge have a greater improvement in crosslingual transferability. Although code-switch significantly reduces the transfer gap on XNLI, the improvement is relatively small on POS and MLQA under the cross-lingual transfer setting, indicating the noisy code-switch substitution will harm the cross-lingual transferability on finer-grained tasks. Fine-tune multilingual model on all target language target language training sets (translate-train-all)</p><p>FILTER <ref type="bibr" target="#b11">(Fang et al., 2020)</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Better fine-tuning by reducing representational collapse</title>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshat</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anchit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<idno>CoRR, abs/2008.03156</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the cross-lingual transferability of monolingual representations</title>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05">2020. July 5-10, 2020</date>
			<biblScope unit="page" from="4623" to="4637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">There are many consistent explanations of unlabeled data: Why you should average</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename></persName>
		</author>
		<idno>ICLR 2019</idno>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unlabeled data improves adversarial robustness</title>
		<author>
			<persName><forename type="first">Yair</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-14">2019. 2019. 2019, 8-14 December 2019</date>
			<biblScope unit="page" from="11190" to="11201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">In-foXLM: An information-theoretic framework for cross-lingual language model pre-training</title>
		<author>
			<persName><forename type="first">Zewen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saksham</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno>CoRR, abs/2007.07834</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tydi QA: A benchmark for information-seeking question answering in typologically diverse languages</title>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Nikolaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="454" to="470" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05">2020a. July 5-10</date>
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Crosslingual language model pretraining</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-14">2019. 2019. 2019, 8-14 December 2019</date>
			<biblScope unit="page" from="7057" to="7067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">XNLI: evaluating cross-lingual sentence representations</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1269</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10-31">2018. October 31 -November 4, 2018</date>
			<biblScope unit="page" from="2475" to="2485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Emerging cross-lingual structure in pretrained language models</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05">2020b. July 5-10, 2020</date>
			<biblScope unit="page" from="6022" to="6034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">FILTER: An enhanced fusion method for cross-lingual language understanding</title>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR, abs/2009.05166</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving vector space word representations using multilingual correlation</title>
		<author>
			<persName><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/e14-1049</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter</title>
				<meeting>the 14th Conference of the European Chapter<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computer Linguistics</publisher>
			<date type="published" when="2014-04-26">2014. 2014. April 26-30, 2014</date>
			<biblScope unit="page" from="462" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cross-lingual semantic role labeling with highquality translated training corpus</title>
		<author>
			<persName><forename type="first">Meishan</forename><surname>Hao Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05">2020. July 5-10, 2020</date>
			<biblScope unit="page" from="7014" to="7026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cross-lingual dependency parsing based on distributed representations</title>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/p15-1119</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computer Linguistics</publisher>
			<date type="published" when="2015-07-26">2015. 2015. July 26-31, 2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1234" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation</title>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
				<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07-18">2020. 13-18 July 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="4411" to="4421" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning discrete representations via information maximizing self-augmented training</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seiya</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eiichi</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017-08-11">2017. 2017. 6-11 August 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1558" to="1567" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SMART: Robust and efficient fine-tuning for pretrained natural language models through principled regularized optimization</title>
		<author>
			<persName><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05">2020. July 5-10, 2020</date>
			<biblScope unit="page" from="2177" to="2190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Subword regularization: Improving neural network translation models with multiple subword candidates</title>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1007</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07-15">2018. July 15-20. 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-2012</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10-31">2018. October 31 -November 4, 2018</date>
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
	<note>EMNLP 2018: System Demonstrations</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised machine translation using monolingual corpora only</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<idno>ICLR 2018</idno>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30">2018. April 30 -May 3, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">MLQA: evaluating cross-lingual extractive question answering</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruty</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><surname>Rinott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05">2020. July 5-10, 2020</date>
			<biblScope unit="page" from="7315" to="7330" />
		</imprint>
	</monogr>
	<note>Sebastian Riedel, and Holger Schwenk</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adversarial training for large neural language models</title>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno>CoRR, abs/2004.08994</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">VECO: Variable encoder-decoder pre-training for cross-lingual understanding and generation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fuli Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songfang</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Si</surname></persName>
		</author>
		<idno>ArXiv, abs/2010.16046</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>CoRR, abs/1309.4168</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: A regularization method for supervised and semisupervised learning</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin-Ichi</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin</forename><surname>Ishii</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2858821</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1979" to="1993" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rogier</forename><surname>Blokland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niko</forename><surname>Partanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Rießler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Universal dependencies</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Crosslingual name tagging and linking for 282 languages</title>
		<author>
			<persName><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1178</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-07-30">2017. July 30 -August 4</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1946" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">English intermediate-task training improves zero-shot crosslingual transfer too</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mon</forename><surname>Phu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Htut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haokun</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katharina</forename><surname>Vania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iacer</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName><surname>Bowman</surname></persName>
		</author>
		<idno>CoRR, abs/2005.13013</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">BPE-Dropout: Simple and effective subword regularization</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Provilkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitrii</forename><surname>Emelianenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Voita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-07-05">2020. July 5-10, 2020</date>
			<biblScope unit="page" from="1882" to="1892" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">CoSDA-ML: Multi-lingual code-switching data augmentation for zero-shot cross-lingual NLP</title>
		<author>
			<persName><forename type="first">Libo</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minheng</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/533</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
				<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="3853" to="3860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">XLDA: cross-lingual data augmentation for natural language inference and question answering</title>
		<author>
			<persName><forename type="first">Jasdeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>CoRR, abs/1905.11471</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<title level="s">Workshop Track Proceedings. OpenReview</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. 2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cross-lingual BERT transformation for zero-shot dependency parsing</title>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1575</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03">2019. 2019. November 3-7, 2019</date>
			<biblScope unit="page" from="5720" to="5726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual transfer of word embedding spaces</title>
		<author>
			<persName><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoki</forename><surname>Otani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexin</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1268</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10-31">2018. October 31 -November 4. 2018</date>
			<biblScope unit="page" from="2465" to="2474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">PAWS-X: A cross-lingual adversarial dataset for paraphrase identification</title>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1382</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03">2019. 2019. November 3-7, 2019</date>
			<biblScope unit="page" from="3685" to="3690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised embedding learning via invariant and spreading instance feature</title>
		<author>
			<persName><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00637</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
				<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-16">2019. June 16-20, 2019</date>
			<biblScope unit="page" from="6210" to="6219" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cross-lingual dependency parsing using code-mixed treebank</title>
		<author>
			<persName><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohong</forename><surname>Fu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1092</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03">2019. November 3-7, 2019</date>
			<biblScope unit="page" from="997" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improving the robustness of deep neural networks via stability training</title>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.485</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
				<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27">2016. June 27-30, 2016</date>
			<biblScope unit="page" from="4480" to="4488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">FreeLB: Enhanced adversarial training for natural language understanding</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
