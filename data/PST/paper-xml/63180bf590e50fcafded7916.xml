<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unifying Generative Models with GFlowNets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-09-06">6 Sep 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nikolay</forename><surname>Malkin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
						</author>
						<title level="a" type="main">Unifying Generative Models with GFlowNets</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-09-06">6 Sep 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2209.02606v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There are many frameworks for deep generative modeling, each often presented with their own specific training algorithms and inference methods. We present a short note on the connections between existing deep generative models and the GFlowNet framework (Bengio et al., 2021b), shedding light on their overlapping traits and providing a unifying viewpoint through the lens of learning with Markovian trajectories. This provides a means for unifying training and inference algorithms, and provides a route to construct an agglomeration of generative models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Preliminaries about GFlowNets</head><p>From a probabilistic modeling viewpoint, generative flow network (GFlowNet) <ref type="bibr">(Bengio et al., 2021a)</ref> is one kind of generative models that aims to sample x in proportion to a given reward function R(x). Recently, the community has experienced a progressive expansion of the concept of GFlowNets <ref type="bibr" target="#b13">(Malkin et al., 2022;</ref><ref type="bibr" target="#b10">Zhang et al., 2022;</ref><ref type="bibr">Deleu et al., 2022;</ref><ref type="bibr" target="#b10">Jain et al., 2022)</ref>. Concretely, a GFlowNet would sample a Markovian trajectory ? = (s 0 , s 1 , . . . , s n ) with length n. If not specially specified, we use the notation x = s n for the final state of the trajectory. This process has a natural connection to reinforcement learning <ref type="bibr" target="#b19">(Sutton &amp; Barto, 2005)</ref>. All the states s construct a directed acyclic graph (DAG) in the latent state space. Each trajectory starts from the same (abstract) initial state s 0 and runs to a different end point s n . The flow function F (? ) ? R + defined in <ref type="bibr">Bengio et al. (2021b)</ref> can be understood by an analogy of water amount in a water flow. Ideally, we want the amount of flow leading to x equals the given reward: ? =(s0,??? ,x) F (? ) = R(x).</p><p>GFlowNet has several training criterion. We start from the flow matching condition (Eq. 1). Define F (s, s ? ) (s?s ? )?? F (? ) as the edge flow function and F (s)</p><p>1 Mila -Quebec AI Institute 2 Facebook AI Research. Correspondence to: Dinghuai Zhang &lt;dinghuai.zhang@mila.quebec&gt;. ICML 2022 Beyond Bayes workshop. Copyright 2022 by the author(s).</p><p>s?? F (? ) as the state flow function. It's easy to see that</p><formula xml:id="formula_0">s F (s, s ? ) = s ?? F (s ? , s ?? ),<label>(1)</label></formula><p>as both side of the term equals F (s ? ). If one parametrizes the GFlowNet with the edge flow function F ? (?, ?), then Eq. 1 would be used as training loss (i.e., L(?, s</p><formula xml:id="formula_1">? ) = ( s F ? (s, s ? ) -s ?? F ? (s ? , s ?? )) 2 ).</formula><p>The detailed balance condition of GFlowNet writes</p><formula xml:id="formula_2">F (s)P F (s ? | s) = F (s ? )P B (s | s ? ) ,<label>(2)</label></formula><p>where P F (s ? |s) and P B (s|s ? ) are referred to as the forward and backward policy for the transition between different states. We can separately parametrize three models for</p><formula xml:id="formula_3">F ? (?), P F (?|?; ?), P B (?|?; ?).</formula><p>The detailed balance condition is closely related to the flow matching condition, in the sense that</p><formula xml:id="formula_4">P F (s ? |s) = F (s, s ? )/F (s) and P B (s|s ? ) = F (s, s ? )/F (s ? ).</formula><p>It suffices to determine a GFlowNet by determining its forward policy in theory.</p><p>What's more, the general trajectory balance condition of GFlowNet <ref type="bibr" target="#b13">(Malkin et al., 2022)</ref> wants to match GFlowNet's forward trajectory P F (? ) and the backward trajectory P B (? ), where</p><formula xml:id="formula_5">P F (? ) = n-1 t=0 P F (s t+1 |s t )<label>(3)</label></formula><formula xml:id="formula_6">P B (? ) = R (x) Z n-1 t=0 P B (s t |s t+1 ) .<label>(4)</label></formula><p>Here Z = x R(x) is the normalizing factor. We also write</p><formula xml:id="formula_7">P B (? |x) = n-1 t=0 P B (s t | s t+1 ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Hierarchical Variational Autoencoders</head><p>The evidence lower bound for bottom-up hierarchical VAEs (HVAEs) <ref type="bibr" target="#b15">(Ranganath et al., 2016)</ref> reads log p(x) ? ELBO(p, q)</p><p>(5)</p><formula xml:id="formula_8">E q(z1:L|x) [log p(x, z 1:L ) -log q(z 1:L |x)] (6) = E q(zL|zL+1)???q(z1|z2) log p(z 1 ) + L i=1 log p(z i+1 |z i ) q(z i |z i+1 ) ,<label>(7)</label></formula><p>where we denote x := z L+1 .</p><p>It is well known that this hierarchical ELBO can also be represented as D KL (q(z 1:L |x) p(z 1:L |x)) , where p(z 1:L |x) ? p(x|z 1:L )p(z 1:L ) and D KL denotes the KL divergence. In GFlowNets, we also aim to match the forward trajectory policy which ends with data x with the corresponding backward trajectory policy, i.e., P F (? ) ? P B (? ), conditioning on the event {x ? ? }, i.e., x is the last state of ? . Note that we have P (x|? ) = ?{x ? ? }. A slight difference between HVAEs and GFlowNets is that z 1:L , the latent variables of the hierarchical VAE, does not include x.</p><p>Observation 1. HVAE is a special kind of GFlowNets in the following sense: each trajectory is in the form of ?</p><formula xml:id="formula_9">= (z 1 , z 2 , ? ? ? , z L , z L+1 = x). The decoder of VAE, which samples z 1 ? ? ? ? ? z L ? z L+1 , corresponds to the forward policy; the encoder samples x ? z L ? ? ? ? ? z 1 ,</formula><p>and corresponds to the backward policy.</p><p>With x = z L+1 , we could then write</p><formula xml:id="formula_10">P F (? ) = p(z 1 ) L i=1 p(z i+1 |z i ),<label>(8)</label></formula><formula xml:id="formula_11">P B (? ) = p d (x) L i=1 q(z i |z i+1 ),<label>(9)</label></formula><p>where</p><formula xml:id="formula_12">p d (x) = R(x)</formula><p>Z is the true target density. We can see the i-th state in a GFlowNet trajectory s i corresponds to z i . A subtle difference is that in <ref type="bibr">Bengio et al. (2021b)</ref> we additionally have an abstract initial state s 0 which has no concrete meaning for some simplicity reasons. We ignore this difference here.</p><p>The following proposition reveals an equivalence between the two perspectives in Observation 1.</p><p>Proposition 2. Training hierarchical latent variable models with the KL-trajectory balance D KL (P B (? ) P F (? )) is equivalent to training HVAEs by maximizing its ELBO.</p><p>We remark that there is also top-down modeling for hierarchical VAE <ref type="bibr" target="#b16">(S?nderby et al., 2016;</ref><ref type="bibr">Child, 2021)</ref>, where the inference model takes the factorization form of q(z 1 |x)</p><p>L-1 i=1 q(z i+1 |z i ). However, we do not discuss it here as it does not have much connection to GFlowNets, although it has more application power in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Diffusion Models &amp; SDEs</head><p>Behavior of deep latent variable model in its infinite depth regime is studied by <ref type="bibr" target="#b20">Tzen &amp; Raginsky (2019)</ref>. In short, if the encoder and decoder take the following form</p><formula xml:id="formula_13">p(z i+1 |z i ) = N z i+1 ; z i + hf i (z i ), hg 2 i ) , (10) q(z i |z i+1 ) = N (z i ; z i+1 + h(-f i+1 (z i+1 ) (11) + g 2 i+1 ? log p i+1 (z i+1 )), hg 2 i+1 ),<label>(12)</label></formula><p>where we assume all z i have the same number of dimensionality as x and h = 1/L, then the hierarchical model is equivalent to a stochastic process in its diffusion limit ( h ? 0). <ref type="bibr">Huang et al. (2021)</ref>; <ref type="bibr">Kingma et al. (2021)</ref>; <ref type="bibr">Song et al. (2021)</ref> also study connections between hierarchical variational inference on latent variable models and diffusion.</p><p>Consider a stochastic differential equation (SDE) <ref type="bibr">(?ksendal, 1985)</ref> and its reverse time SDE <ref type="bibr" target="#b0">(Anderson, 1982</ref>)</p><formula xml:id="formula_14">dx = f (x) dt + g(t) dw t ,<label>(13)</label></formula><formula xml:id="formula_15">dx = f (x) -g 2 (t)? x log p t (x) d t + g(t) d wt , (<label>14</label></formula><formula xml:id="formula_16">)</formula><p>where f (?) and g(?) are given and x, w ? R D , and x, t, wt denote the reverse time version of x, t, w t . We define P h (x t+h |x t ) to be the transition kernel induced by the SDE in Eq. 13:</p><formula xml:id="formula_17">x t+h = x t + t+h t f (x ? ) d? + t+h t</formula><p>g(? ) dw ? , where h denotes an infinite small time step. We then make the following observation.</p><p>Observation 3. SDE is a special case of GFlowNets, in the sense that GFlowNet states take the time-augmented form s = (x t , t) for some t, the SDE models the forward policy of GFlowNets (i.e., how states should move forward) while the reverse time SDE models the backward policy of GFlowNets (i.e., how states should move backward). In this case, a trajectory is in the form of {x t }<ref type="foot" target="#foot_0">1</ref> t=0 .</p><p>Note that we cannot directly treat any x t to be GFlowNet state, because theory of GFlowNets requires the graph of all latent states is a DAG (i.e., cannot possess circles). This state augmenting operation also induces that in SDE case, the latent state graph of GFlowNets could not be arbitrary DAG, since any (x, t) ? (x ? , t ? ) with t ? t ? is forbidden. Without loss of generality, we assume x t state already contain time stamp in itself in the following context 1 .</p><p>We next point out an analogy between stochastic processes property and GFlowNet property:</p><p>Observation 4. Such property of stochastic process</p><formula xml:id="formula_18">p(x t-h , t -h)P h (x t |x t-h ) dx t-h = p(x t , t)P h (x t+h |x t ) dx t+h = p(x t , t),<label>(15)</label></formula><p>could be interpreted as GFlowNets flow matching condition:</p><formula xml:id="formula_19">s F (s, s ? ) = s ?? F (s ? , s ?? ) F (s ? ),<label>(16)</label></formula><p>for any s ? , where we also have F (s, s ? ) = F (s)P F (s ? |s).</p><p>We point out that Eq. 15 is a standard starting point for deriving the Fokker-Planck equation:</p><p>Proposition 5 <ref type="bibr">(?ksendal (1985)</ref>). Taking the limit as h ? 0, Eq. 15 implies</p><formula xml:id="formula_20">? t p(x, t) = -? x (p(x, t)f (x, t)) + 1 2 ? 2 x p(x, t)g 2 (x, t) . (<label>17</label></formula><formula xml:id="formula_21">)</formula><p>Equivalence between detailed balance and score matching. At the end of this section, we investigate such a setting where we want to model our own reverse process:</p><formula xml:id="formula_22">dx = f (x) -g 2 (t)s(x, t) d t + g(t) d wt ,<label>(18)</label></formula><p>where s(?, ?) : R D ? R ? R D is a neural network, and f (?) and g(?) are given 2 . We propose to use the detailed balance criterion of GFlowNet to learn this neural network. From the above discussion, we can see there is an analogy about F (s t ) ? p t (x). We show the validity of such strategy in the following proposition.</p><p>Proposition 6. GFlowNets' detailed balance condition</p><formula xml:id="formula_23">lim h?0 1 ? h (log p t (x t ) + log P F (x t+h |x t )- log p t+h (x t+h ) + log P B (x t |x t+h )) = 0,<label>(19)</label></formula><formula xml:id="formula_24">(?x t ? R D , ?t ? (0, 1)) is equivalent to ? ? (s(x t , t) -? x log p t (x)) = 0, ??, x t ? R D , ?t ? [0, 1],</formula><p>which is the optimal solution to score matching:</p><formula xml:id="formula_25">min s E x?pt E ? ? ? ? x s(x)? + 1 2 ? ? s(x, t) 2 , ?t.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Exact Likelihood Models</head><p>Autoregressive (AR) models sample p(x 1:i+1 |x 1:i ) sequentially for every dimension i to generate a vector data x.</p><p>In <ref type="bibr" target="#b10">Zhang et al. (2022)</ref>, the authors use an AR-like (with a learnable ordering) model to parametrize the GFlowNet. Indeed, if we define every (forward) action of GFlowNets is to add one pixel to the current state, and the backward policy is to turn one pixel into so-called "void" state, then AR models could be seen as GFlowNets. As a formal example, we analyze AR models with natural ordering.</p><p>Observation 7. Autoregressive model is a special kind of GFlowNets when</p><p>? s t := x 1:t is the GFlowNet state;</p><p>? P F (s t+1 |s t ) = p(x 1:i+1 |x 1:i ) is the forward policy;</p><p>2 f (x), s(x) could also be written as f (xt, t), s(xt, t) in a more strict/general way.</p><p>? P B (s t |s t+1 ) = ?{s t is the first t dimensions of s t+1 }, where ?{?} is the Dirac Delta distribution for continuous variables, and is the indicator function for discrete cases.</p><p>This modeling makes the latent graph of the GFlowNet to be a tree; alternatively, if we allow learnable ordering as in <ref type="bibr" target="#b10">Zhang et al. (2022)</ref>, the latent space is a general DAG. This is related to non-autoregressive modeling methods in NLP community <ref type="bibr" target="#b8">(Gu et al., 2018)</ref>.</p><p>Normalizing flow <ref type="bibr" target="#b7">(Dinh et al., 2015)</ref> (NF) is another way to sequentially construct desired data. It first samples z 1 from a base distribution (usually the standard Gaussian), and then doing a series of invertible transformations z i+1 = f (z i ) until we finally get x := z L+1 , where L denotes the number of transformation layers.</p><p>Observation 8. NF is a special kind of GFlowNets where deterministic forward and backward policies (except the first transition step), and s t = z t are GFlowNet states.</p><p>In this scenario, NF is a GFlowNet whose forward policy is to sample from the base distribution at the first step (hence a stochastic step), and conduct deterministic invertible transformations afterwards. We next discuss maximum likelihood estimation (MLE) of such models.</p><p>About MLE training. AR models and NFs are usually trained with MLE. Albeit the likelihood of general GFlowNets is intractable, we can estimate its lower bound:</p><formula xml:id="formula_26">log p T (x) = x?? P F (? ) d? (20) = log E PB (? |x) P F (? ) P B (? |x) (21) ? E PB (? |x) log P F (? ) P B (? |x) (22) = -D KL (P B (? |x) P F (? )) ,<label>(23)</label></formula><p>which could be again seen as one kind of trajectory balance as in Proposition 2. An IWAE-type bound <ref type="bibr" target="#b3">(Burda et al., 2016)</ref> is also applicable. If the backward policy is a fixed one (i.e., deterministic and without learnable parameters), we can directly use log P B (? |x)log P F (? ) as a sample based tractable training loss for x ? D, with ? ? P B (? |x). Furthermore, if we use AR natural ordering, then P B (? |x) only contains one trajectory, therefore log P B (? |x)-log P F (? ) is the same thing as the MLE loss of AR models. On the other hand, if the backward policy is learnable, one may need REINFORCE <ref type="bibr" target="#b21">(Williams, 2004)</ref> or reparametrization trick <ref type="bibr" target="#b11">(Kingma &amp; Welling, 2014)</ref> to optimize with regard to P B , making it a variational distribution over trajectories.</p><p>We summarize that both AR models and NFs are GFlowNets with tree-structured latent state graphs, mak-ing every terminating state could only be reached by one trajectory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Learning a Reward Function from Data</head><p>Energy-based model (EBM) can be used as the (negative log) reward function for GFlowNets training. We could use any GFlowNet modeling including those discussed in previous sections, and jointly train it together with an EBM. For instance, in the EB-GFN algorithm <ref type="bibr" target="#b10">(Zhang et al., 2022)</ref> a GFlowNet is used to amortize the computational MCMC process of the EBM contrastive divergence training. The two models (EBM and GFlowNet) are trained alternately.</p><p>Generative adversarial network (GAN) is closely related to EBM <ref type="bibr" target="#b4">(Che et al., 2020)</ref>, while its algorithm is more computationally efficient. However, though it may look reasonable at first glance, we cannot directly use the discriminator D(x) as the reward for GFlowNets training. If so, at the end of a perfect training, we would get an optimal discriminator x) and optimal GFlowNet generator distribution p g (x) ? D * (x). This cannot induce p g (x) = p d (x). In fact, if p g (x) = p d (x), we will have D * (x) ? 1/2 and p g (x) = p d (x) ? constant, which is impossible for general data with unbounded support. To fill this gap, we instead design the following algorithm that makes much more sense.</p><formula xml:id="formula_27">D * (x) = p d (x) p d (x)+pg(</formula><p>Proposition 9. An alternate algorithm which trains the discriminator to distinguish between generated data and true data, and trains the GFlowNet with negative energy as</p><formula xml:id="formula_28">log D(x) 1 -D(x) + log p g (x) (24)</formula><p>would result in a valid generative model.</p><p>Nonetheless, we unfortunately do not have access to the value of p g (x) if the generator is a general GFlowNet, which makes this algorithm an intractable one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have interpreted existing generative models as GFlowNets with different policies over sample trajectories. This provides some insight into the overlaps between existing generative modeling frameworks, and their connection to general-purpose algorithms for training them. Furthermore, this unification implies a method of constructing an agglomeration of varying types of generative modeling approaches, where GFlowNet acts as a general-purpose glue for tractable inference and training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proofs</head><p>A.1. Proposition 2</p><p>Proof. We have</p><formula xml:id="formula_29">D KL (P B (? ) P F (? )) = E p d (x)q(z1:L|x) log p d (x)q(z 1:L |x) p(z 1:L )p(x|z 1:L ) = -E p d (x) E q(z1:L|x) log p(x, z 1:L ) q(z 1:L |x) + E p d (x) [log p d (x)] = -E p d (x) [ELBO(p, q)] -H[p d (?)].</formula><p>Here H[p d (?)] denotes the entropy of the target distribution, which is a constant w.r.t. GFlowNet parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Proposition 5</head><p>Proof. First notice</p><formula xml:id="formula_30">? t p(x, t) lim h?0 1 h (p(x, t + h) -p(x, t)) = lim h?0 1 h p(x ? , t)P h (x|x ? ) dx ? -p(x, t) .</formula><p>Then for any function w(x), we have</p><formula xml:id="formula_31">w(x)? t p(x, t) dx = w(x) lim h?0 1 h p(x ? , t)P h (x|x ? ) dx ? -p(x, t) dx = lim h?0 1 h ( w(x) p(x ? , t)P h (x|x ? ) dx ? dx -w(x ? )p(x ? , t) P h (x|x ? ) dx dx ? ) = lim h?0 1 h p(x ? , t)P h (x|x ? ) (w(x) -w(x ? )) dx dx ? p(x ? , t) n=1 w (n) (x ? )D n (x ? )x ? = w(x ? ) n=1 - ? ?x ? n (p(x ? , t)D n (x ? )) dx ? , where D n (x ? ) = lim h?0 1 hn! P h (x|x ? )(x -x ?</formula><p>) n dx and the last step uses integral by parts. This tells us</p><formula xml:id="formula_32">? t p(x, t) = n=1 - ? ?x n (p(x, t)D n (x)) = -? x (p(x, t)f (x, t)) + 1 2 ? 2 x p(x, t)g 2 (x, t) ,</formula><p>which is essentially the Fokker-Planck equation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Proposition 6</head><p>From the above SDEs, we know that the forward and backward policy is</p><formula xml:id="formula_33">x t+h = x t + f (x t )h + ? hg(t) ? ? F , x t = x t+h + g 2 (t + h)s(x t+h ) -f (x t+h ) h + ? hg(t + h)? B ,</formula><p>where ? F , ? B ? N (0, I D ). Since we know h ? 0, the left and right side of Eq. 19 become with some smoothness assumptions on g(t) and f (x t )/g(t). This tells us that in order to satisfy the detailed balance criterion, we need to satisfy ? ? (s(x t , t) -? x log p t (x)) = 0, ??, x t ? R D , ?t ? [0, 1].</p><formula xml:id="formula_34">log P F (x t+h |x t ) -log P B (x t |x t+h ) = - D 2 log(2?g 2 t h) - 1 2g 2 t h ?x -hf t 2 + D 2 log(2?g</formula><p>Since ? could take any value, this is equivalent that the model s should match with score function ? x log p(x). Also, note that this is exactly sliced score matching <ref type="bibr" target="#b17">(Song et al., 2019)</ref>, which has a more practical formulation</p><formula xml:id="formula_35">E ? E x?p ? ? ? x s(x)? + 1 2 ? ? s(x) 2 .</formula><p>A.4. Proposition 9</p><p>When both models (the GFlowNet and the discriminator) are trained perfectly, we have</p><formula xml:id="formula_36">D * (x) = p d (x) p d (x) + p g (x)</formula><p>, Hence it is a valid generative model algorithm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>and thusp g (x) ? exp log D * (x) 1 -D * (x) + log p g (x) = exp log p d (x) p g (x) + log p g (x) = p d (x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>hf t+h + hg 2 t+h s(x t+h ) 2 , log p t+h (x t+h )log p t (x t ) = ?x ? ? x log p t (x) + O(h), where g t = g(t), f t = f (x t ), ?x = x t+hx t = ? hg t ?, ? ? N (0, I D ). Therefore, (logp t+h (x t+h )xlog p t (x t )) = ? ? ? x log p t (x), (x t+h |x t )log P B (x t |x t+h )) ? 2 + h 2 f t 2 -2h 3/2 g t ? ? f t + 1 2g t g 2 t+h h 3/2 hg 2 t ? 2 -2h 3/2 g t ? ? f t+h + 2g t g 2 t+h h 3/2 ? ? s(x t+h )</figDesc><table><row><cell cols="7">2 t+h h) + ?x -lim 1 2g 2 t+h h h?0 1 ? hg t</cell></row><row><cell>lim h?0</cell><cell>1 hg t ?</cell><cell>(logP F = lim h?0</cell><cell>D ? hg t</cell><cell cols="3">(log g t+h -log g t ) -</cell><cell>1 2g 3 t h 3/2 hg 2</cell></row><row><cell></cell><cell></cell><cell>= lim h?0</cell><cell cols="2">? ? f t g 2 t</cell><cell>-</cell><cell>f t+h g 2</cell></row></table><note><p>t t+h + ? T s(x t+h ) = ? T s(x t ),</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This is equivalent to define xt = (xt, t) and conduct discussion with xt instead.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reverse-time diffusion equation models</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D O</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stochastic Processes and their Applications</title>
		<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="313" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Flow network based generative models for noniterative diverse candidate generation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Korablyov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lahlou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bengio</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>GFlowNet foundations. arXiv preprint 2111.09266, 2021b</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>1509.00519</idno>
		<title level="m">Importance weighted autoencoders</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Your gan is secretly an energy-based model and you should use discriminator driven latent sampling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Paull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Very deep vaes generalize autoregressive models and can outperform them on images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bayesian structure learning with generative flow networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>G'ois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Emezue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rankawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Nice</surname></persName>
		</author>
		<idno>1410.8516</idno>
		<title level="m">Non-linear independent components estimation</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Non-autoregressive neural machine translation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A variational perspective on diffusion-based generative models and score matching</title>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garc?a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rector-Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F P</forename><surname>Dossou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Ekbote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kilgour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Simine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Biological sequence design with GFlowNets</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Variational diffusion models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Malkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>2201.13259</idno>
		<title level="m">Trajectory balance: Improved credit assignment in GFlowNets</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>?ksendal</surname></persName>
		</author>
		<title level="m">Stochastic differential equations. The Mathematical Gazette</title>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="65" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hierarchical variational models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ladder variational autoencoders</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>S?nderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maal?e</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>S?nderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sliced score matching: A scalable approach to density and score estimation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Maximum likelihood training of score-based diffusion models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Durkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="285" to="286" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Neural stochastic differential equations: Deep latent gaussian models in the diffusion limit</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raginsky</surname></persName>
		</author>
		<idno>1905.09883</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Machine Learning</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generative flow networks for discrete probabilistic modeling</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Malkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Volokhova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
