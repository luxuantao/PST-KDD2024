<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning to Detect Alzheimer&apos;s Disease from Neuroimaging: A Systematic Literature Review</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Dr</roleName><forename type="first">Mr</forename><surname>Amir Ebrahimighahnavieh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Methods and Programs in Biomedicine</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Raymond</forename><surname>Chiong</surname></persName>
							<email>raymond.chiong@newcastle.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Methods and Programs in Biomedicine</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The University of Newcastle University</orgName>
								<address>
									<postCode>2308</postCode>
									<settlement>Callaghan</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mr</forename><surname>Amir</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Newcastle University</orgName>
								<address>
									<postCode>2308</postCode>
									<settlement>Callaghan</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning to Detect Alzheimer&apos;s Disease from Neuroimaging: A Systematic Literature Review</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5A4A0F550ECFBB2BB20C4EB905BBB2DF</idno>
					<idno type="DOI">10.1016/j.cmpb.2019.105242</idno>
					<note type="submission">Received date: 8 July 2019 Revised date: 13 November 2019 Accepted date: 25 November 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>deep learning</term>
					<term>Alzheimer&apos;s disease</term>
					<term>convolutional neural networks</term>
					<term>recurrent neural networks</term>
					<term>auto-encoders</term>
					<term>transfer learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This is a PDF file of an article that has undergone enhancements after acceptance, such as the addition of a cover page and metadata, and formatting for readability, but it is not yet the definitive version of record. This version will undergo additional copyediting, typesetting and review before it is published in its final form, but we are providing this version to give early visibility of the article. Please note that, during the production process, errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With new artificial intelligence technologies, computer systems can be used to enhance the accuracy and speed of detecting diseases in hospitals, even those that have few medical experts. Advances in medical imaging and analysis have delivered powerful tools for detecting neurodegeneration, and there is great interest in using imaging information to diagnose a disease. It has recently been shown that a computer can make as accurate an assessment as a radiologist <ref type="bibr" target="#b0">[1]</ref>.</p><p>Alzheimer's Disease (AD) is an irreversible progressive neurodegenerative disorder that slowly destroys memory and leads to difficulty in communication and performing daily activities such as speaking and walking.</p><p>It is eventually fatal. AD is the most common type of dementia, comprising an estimated 60-80% of all dementia cases. It typically starts in middle or old age, possibly initiated by accumulation of protein in and around neurons, and leads to a steady deterioration in memory (associated with synaptic dysfunction, brain shrinkage, and cell death) <ref type="bibr" target="#b1">[2]</ref>. The first changes in the brain occur before cognitive decline begins, and some biomarkers may become abnormal at this early stage. Research suggests that brain changes related to AD may begin at least 20 years before symptoms appear <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>Patients at the initial stage of AD are classified as having Mild Cognitive Impairment (MCI) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, although not all patients with MCI will develop AD. MCI is a transitional stage from normal to AD, in which a person has mild changes in cognitive ability that are obvious to the person affected and to relatives but is still able to perform everyday activities. About 15-20% of people aged 65 or older have MCI, and 30-40% of individuals with MCI develop AD within 5 years <ref type="bibr" target="#b1">[2]</ref>. The conversion time ranges from 6 to 36 months but is typically 18 months. MCI patients can then be categorized as MCI converters (MCIc) or MCI non-convertors (MCInc), meaning the patient had or had not converted to AD within 18 months. There are also other subtypes of MCI that are rarely mentioned in the literature, such as early/late MCI.</p><p>The most significant risk factors for AD are family histories and the presence of related genes in a person's genome. An AD diagnosis is based on a clinical examination as well as a comprehensive interview of the patient and their relatives <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. Nevertheless, a ‗ground truth' diagnosis of AD can only be made via autopsy, which is not clinically helpful. A group of AD patients with an autopsy-confirmed diagnosis is utilized in <ref type="bibr" target="#b7">[8]</ref>.</p><p>Without ground truth data, patients need some other criteria to confirm AD. Such criteria could improve our understanding of AD, and make diagnosis possible for living patients. In 1984, NINCDS 1 and ADRDA 2   established criteria for the clinical diagnosis of AD; in 2007 they were revised based on memory impairment and the presence of at least one additional supportive feature: abnormal Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET) neuroimaging or abnormal cerebrospinal fluid amyloid and tau biomarkers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>. NIA 3 and the Alzheimer's Association have also begun revising diagnostic criteria for AD <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>. The new proposed diagnostic criteria include measures of brain amyloid, neuronal injury, and degeneration. It has recently been concluded that updates to the criteria are probably warranted every 3-4 years in order to incorporate new knowledge about the pathophysiology and progression of the disease <ref type="bibr" target="#b16">[17]</ref>. 1 National Institute of Neurological and Communicative Disorders and Stroke 2 Alzheimer's Disease and Related Disorders Association 3 National Institute on Aging</p><p>The Mini-Mental State Examination (MMSE) <ref type="bibr" target="#b17">[18]</ref> and the Clinical Dementia Rating (CDR) <ref type="bibr" target="#b18">[19]</ref> are two of the most frequently used tests in evaluating AD <ref type="bibr" target="#b19">[20]</ref>, although it should be noted that using them as ground truth labels for AD might be incorrect. Based on the criteria mentioned above, the reported accuracies of clinical diagnosis of AD compared to post-mortem diagnosis are in the range of 70-90% <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref>. Despite its limitations, clinical diagnosis is the best available reference standard <ref type="bibr" target="#b24">[25]</ref>. It is also worth noting that the availability of all the recognised biomarkers is quite limited.</p><p>In 2010, the number of people over 60 years old living with dementia was reported to be 35.6 million worldwide and 310,000 in Australasia. The numbers are expected to almost double every 20 years so that by 2050 there would be 115 million worldwide and 790,000 in Australasia <ref type="bibr" target="#b25">[26]</ref>. Dementia has become the second leading cause of death in Australia, with 13,126 cases reported in 2016 <ref type="bibr" target="#b26">[27]</ref>. The cost of nursing for AD patients and other types of dementia is expected to increase considerably, making AD one of the most expensive chronic diseases <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28]</ref>. Although a number of treatment strategies have been investigated to prevent or slow down the disease, success has been limited <ref type="bibr" target="#b28">[29]</ref>. In future, the early and accurate detection of AD is vital for appropriate treatment. Early detection of AD means patients can maintain their independence for longer; new research efforts will lead to better understanding of the disease process and the development of new treatments <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>. Considering all the above, there is a need for a multi-class clinical decision, unbiased by variable radiological expertise, which can automatically distinguish AD and its different stages from a Normal Control (NC). Generally, classifying AD patients from NCs or MCIs is not as valuable as predicting MCI conversion, because AD is clearly apparent without using any expertise when it is too late for treatment. Nevertheless, many studies still tackle the AD vs. NC problem, since it is helpful in other classification tasks, especially in understanding the early signs of AD. The most important and main challenge in AD assessment is to determine whether someone has MCI or not and to predict if an MCI patient will develop the disease. Although the available computer-aided systems are still not able to replace a medical expert, they can supply supporting information to improve the accuracy of clinical decisions. It should be noted that not all studies work on AD, MCI, or NC. Other stages of the disease such as early/late MCI are also considered.</p><p>Detecting AD using artificial intelligence is usually a challenge for researchers due to:  Low medical image acquisition quality and errors in pre-processing and brain segmentation.</p><p> Unavailability of a comprehensive dataset including a vast number of subjects and biomarkers.</p><p> Low between-class variance in different stages of AD. Sometimes the signs that distinguish AD, for example, brain shrinkage, can be found in a normal healthy brain of older people <ref type="bibr" target="#b31">[32]</ref>.</p><p> The ambiguity of boundaries between AD/MCI and MCI/NC based on AD diagnostic criteria <ref type="bibr" target="#b24">[25]</ref>.</p><p> Lack of expert knowledge, especially in identifying Regions-Of-Interest (ROIs) in the brain.</p><p> The complexity of medical images compared to the usual natural images.</p><p>There are a few review studies on AD detection using machine learning, which cover topics such as different types of classifiers, single-modal and multi-modal models, feature extraction algorithms, feature selection methods, validation approaches, and dataset properties <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref>. Also, competition challengessuch as CADDementia <ref type="foot" target="#foot_0">4</ref>  <ref type="bibr" target="#b24">[25]</ref>, TADPOLE 5  <ref type="bibr" target="#b35">[36]</ref>, The Alzheimer's Disease Big Data DREAM Challenge 6 <ref type="bibr" target="#b36">[37]</ref>, and the international challenge for automated prediction of MCI from MRI data <ref type="foot" target="#foot_3">7</ref> (hosted by the Kaggle platform) <ref type="bibr" target="#b37">[38]</ref> have been shown to be effective in AD analysis; they can provide unbiased comparisons of algorithms and tools on standardized data involving participants worldwide. In these studies and competitions, many different machine learning techniques have been investigated and evaluated, but traditional machine learning approaches are not satisfactory for dealing with such complicated issues as AD <ref type="bibr" target="#b38">[39]</ref>. Detecting AD is difficult, and successful classification calls for a strong ability to discriminate certain features among similar brain image patterns.</p><p>The increase in processing power of Graphics Processing Units (GPUs) has enabled the development of cutting-edge deep learning algorithms. Deep learning is a subset of machine learning in artificial intelligence that imitates the workings of the human brain in data processing and pattern recognition to solve complex decision-making problems. Methods based on deep learning have revolutionised performance in numerous areas, such as object recognition, detection, tracking, image segmentation, and audio classification. Successful deep learning in the classification of 2D natural images has benefited studies of deep learning in the domain of medical images <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>. In recent years, deep learning models, particularly Convolutional Neural Networks (CNNs), have performed well in the field of medical imaging for organ segmentation and disease detection <ref type="bibr" target="#b41">[42]</ref>.</p><p>Based on neuroimaging data, deep learning models can discover hidden representations, find links between different parts of images, and identify disease-related patterns. Deep learning models have been successfully applied to medical images such as structural MRI (simply called MRI in this paper), functional MRI (fMRI), PET, and Diffusion Tensor Imaging (DTI). In this way, researchers have recently begun using deep learning models for detecting AD from medical images <ref type="bibr" target="#b39">[40]</ref>; however, there is still a long way to go before deep learning techniques can be used to accurately detect AD. This paper aims to review the current state of AD detection using deep learning. In particular, we aim to set out how deep learning can be used in supervised and unsupervised modes to provide a better understanding of AD. We review AD detection using deep learning to ascertain recent findings and current trends.</p><p>A typical block diagram of a computer-aided AD detection system is shown in To begin, our search strategy and the inclusion/exclusion criteria are first set out to indicate how the papers were selected for review. Next, biomarkers for AD detection, especially brain scans, are explained. After that, data management methods for dealing with brain scans are discussed: voxel-based, slice-based, ROI-based, and patch-based (along with necessary pre-processing steps). Details of deep learning models used for AD detection are then described, together with the specific advantages of each. Finally, training parameters, datasets, and software platforms are discussed, followed by highlights and future challenges. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The review protocol</head><p>Since 2013 the exploration of new neural network structures has gained momentum, with much deeper models coming to the fore, especially for dealing with the processing of medical images <ref type="bibr" target="#b41">[42]</ref>. The importance of deep learning in AD detection has been revealed, and since 2017 the number of papers published in this area has increased rapidly, as can be seen in Figure <ref type="figure" target="#fig_3">2</ref>. Those numbers in Figure <ref type="figure" target="#fig_3">2</ref> show both pre-print and peer-reviewed papers, but do not include book chapters and theses. Specifically, there are 9 pre-prints and 105 peer-reviewed articles derived from our search and selection process.</p><p>In terms of classification accuracy, deep models are generally more accurate than general machine learning techniques <ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref><ref type="bibr" target="#b56">[57]</ref><ref type="bibr" target="#b57">[58]</ref><ref type="bibr" target="#b58">[59]</ref><ref type="bibr" target="#b59">[60]</ref>. Many different techniques based on deep learning have been applied to AD detection.</p><p>Nevertheless, a number of controversial findings exist, which motivated us to conduct this literature review, in order to see what the current state of play is, and what the future trends might be. Our main research question was to investigate whether deep learning techniques were capable of detecting AD using neuroimaging data.</p><p>Our systematic literature review follows a well-defined methodology that aims to be as fair and objective as possible, compared to a traditional review trying to summarize the main results <ref type="bibr" target="#b60">[61]</ref><ref type="bibr" target="#b61">[62]</ref><ref type="bibr" target="#b62">[63]</ref>. A systematic literature review consists of three main stages: planning, executing, and reporting. It aims to set a research question, then develop a review protocol, identify already available reviews, develop a comprehensive search strategy, select studies based on the selection criteria, analyze content, update the review protocol, perform a quality assessment, interpret results, and lastly produce the final document <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b64">65]</ref>.</p><p>The review protocol details how the review will be conducted. It outlines the research question, specifies the process to be followed, and sets out the conditions to be applied when selecting studies; there are quality metrics to ensure the studies chosen are relevant, and team members are given certain tasks in developing the review protocol. The review protocol here was designed by the first author, and reviewed and revised by the other coauthors. Mistakes and defects identified by co-authors in data collection and aggregation procedures were used to revise the research protocol and the research questions. Each study was reviewed at least three times to make sure that the extracted data fully complied with the final protocol. Data extraction was facilitated by having a the study protocol was first defined and later revised if any changes were made. Even if the information provided in a study was incomplete, the whole of the available data was extracted for each research question.</p><p>Statistics reported here relate to information provided in primary articles.</p><p>The research questions of this study are listed below, together with the sections that address those questions.</p><p> RQ1: What kind of biomarkers and factors are involved in AD detection? (Section 3 and  RQ7: What is the current state of AD detection accuracy using deep models? (Tables <ref type="table" target="#tab_8">4 to 9</ref> of Appendix 1).</p><p>The search strategy and inclusion/exclusion criteria are described in the next section, followed by a description of the quality assessment process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Search strategy</head><p>To identify contributions in AD detection, IEEE Xplore, ScienceDirect, SpringerLink, and ACM digital libraries were queried for papers containing -Alzheimer‖ and -deep‖ in the title, abstract, or keywords. Also, Web of Science and Scopus were queried to cross-check the findings and locate other papers in lesser-known libraries. These online databases were chosen since they offer the most important peer-reviewed full-text Scholar was used for forward-searching, that is, checking for citations of found papers to update our search and to look for other papers to ensure nothing was overlooked. The search process was performed by the first author and the last update was done on April 8 th , 2019.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Inclusion/exclusion criteria</head><p>The study selection criteria determine whether a study will be included or excluded from the systematic review. A pilot version of the selection criteria was defined on a subset of the primary studies and was further developed after the review protocol was finalized. In this section, the final version of the selection criteria is explained. Decisions about inclusion/exclusion were not affected by the names of the authors, their institutions, the journal, or year of publication.</p><p>Some analyses tried to distinguish AD from other brain abnormalities such as Parkinson, Down syndrome, Schizophrenia, and Autism. These illnesses or disorders are out of the scope of this research and therefore subjects categorized as NCs are considered to be completely healthy without any neurological/psychiatric disorder or treatment <ref type="bibr" target="#b65">[66]</ref>. In this study, papers that did not use at least one neuroimaging modality were excluded. This means that studies utilizing EEG, retina, visual attention, speech disfluencies, and the like, without involving brain scans, were not included. Moreover, studies without clearly reported results on classification problems (AD/MCI/NC) are not considered. In other words, we did not include papers investigating the estimation of, for example, MMSE score or time of conversion from MCI to AD, modeling AD progression, neuroimaging data completion, image processing techniques, or brain segmentation without clearly reported classification accuracy. When overlapping studies were reported in multiple publications (such as <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68]</ref>), all the publications were included so as to understand even small differences.</p><p>Finally, after performing a full-text search by the first author, the found papers were reduced to 114 articles written in English from 2013 onwards on deep learning for AD detection using neuroimaging modalities.</p><p>Among these 114 papers, Suk <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b68">[69]</ref><ref type="bibr" target="#b69">[70]</ref><ref type="bibr" target="#b70">[71]</ref><ref type="bibr" target="#b71">[72]</ref><ref type="bibr" target="#b72">[73]</ref><ref type="bibr" target="#b73">[74]</ref>, Aderghal <ref type="bibr" target="#b74">[75]</ref><ref type="bibr" target="#b75">[76]</ref><ref type="bibr" target="#b76">[77]</ref><ref type="bibr" target="#b77">[78]</ref><ref type="bibr" target="#b78">[79]</ref><ref type="bibr" target="#b79">[80]</ref>, and Cheng &amp; Liu <ref type="bibr" target="#b80">[81]</ref><ref type="bibr" target="#b81">[82]</ref><ref type="bibr" target="#b82">[83]</ref><ref type="bibr" target="#b83">[84]</ref><ref type="bibr" target="#b84">[85]</ref><ref type="bibr" target="#b85">[86]</ref> had the most papers.</p><p>According to Google Scholar, references <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b29">[30]</ref>, and <ref type="bibr" target="#b31">[32]</ref> are the most cited, whereas reference <ref type="bibr" target="#b86">[87]</ref>   Cross-sectional or longitudinal. The former evaluates each subject at a specific point in time, but the latter follows subjects over time <ref type="bibr">[58-60, 88, 89]</ref>.</p><p> Single-modality or multi-modality. In contrast to single-modality studies, multi-modality studies use more than one neuroimaging modality per subject so as to gain complementary information.</p><p> Automatic or manual. Although fully automatic systems are preferred, some studies involved manual intervention, especially to reduce brain segmentation errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Quality assessment</head><p>It is sometimes necessary to assess the quality of a study in order to support the inclusion/exclusion process or perform comparative analysis. In this secondary review study, there was a limited number of primary articles, and we did not need any other exclusion metric to reduce the number of studies. Nevertheless, a quality assessment is still helpful to interpret primary study findings or investigate whether quality differences explain differences in study results. It is also useful as a means of weighting the importance of individual studies and guiding recommendations for further research <ref type="bibr" target="#b60">[61]</ref><ref type="bibr" target="#b61">[62]</ref><ref type="bibr" target="#b62">[63]</ref>.</p><p>Quality assessment depends strongly on the type of systematic literature review and the contents of the studies, and so many studies do not perform it <ref type="bibr" target="#b89">[90,</ref><ref type="bibr" target="#b90">91]</ref>. There are no universal definitions of quality metrics for primary studies, and every systematic literature review has its own task-specific criteria. Quality criteria can be related to the types of articles (journal or conference), peer-reviewed or not, novelty of the presented idea, and completeness of the information provided. In our case, four different quality metrics were used including the article type (pre-print, conference, or journal), scientific impact (number of citations per year), study size (number of subjects in the dataset), and the completeness of the provided information (according to the research questions of this review). The following point-based system is used to evaluate the quality of each primary study:</p><p>Table <ref type="table">1</ref>. The defined quality metrics in our quality assessment process.  <ref type="table">1</ref>, the maximum possible score is 8, which only one article in our literature review achieved <ref type="bibr" target="#b54">[55]</ref>, followed by four articles with a score of 7 <ref type="bibr" target="#b91">[92]</ref><ref type="bibr" target="#b92">[93]</ref><ref type="bibr" target="#b93">[94]</ref><ref type="bibr" target="#b94">[95]</ref>. The distribution of quality scores of primary studies is given in Figure <ref type="figure" target="#fig_6">3</ref>, which shows that the minimum score was 1, and the average was 3.6. The quality score of each study is listed in Table <ref type="table" target="#tab_0">2</ref> of Appendix 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quality metric Points</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Article type</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Biomarkers and features in AD detection</head><p>Accurate AD detection at the beginning stages of the disease requires evaluation of some quantitative biomarkers. For detecting AD, several non-invasive neuroimaging modalities such as MRI, fMRI, and PET have been investigated. Of these biomarkers, MRI is the most widely available and used biomarker for AD detection and has demonstrated high performance in the literature <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b95">96]</ref>. It uses a powerful magnetic field and radiofrequency pulses to create a 3D representation of organs, soft tissues, and bones. fMRI reflects the changes associated with blood flow. PET is functional imaging technique based on nuclear medicine methods that can observe metabolic processes within the body.</p><p>In addition to multiple neuroimaging modalities, there are many other factors that are possibly relevant to AD detection: age, gender, educational level, speech pattern, EEG, retinal abnormalities, postural kinematic analysis, cerebrospinal fluid (CSF) biomarkers, neuropsychological measures (NMs), MMSE and CDR score, logical memory test (LM), as well as certain genes that are believed to be responsible for about 70% of the risk <ref type="bibr" target="#b34">[35]</ref>. These factors, together with the multiple neuroimaging modalities, can complicate the training of deep learning models. Figure <ref type="figure" target="#fig_7">4</ref> shows, in all the studied articles, the prevalence of single-modality and multimodality studies, the percentage of each neuroimaging modality among the single-modality approaches, and the percentage where grey matter (GM) measures were used in MRI-based studies. For simplicity, here only the prevalence of single-modality brain scans is shown (since the multi-modality category can be very complex, as shown in Table <ref type="table" target="#tab_0">2</ref> of Appendix 1).</p><p>In this section, pre-processing techniques for brain scans are first explained. We then detail different data management methods for dealing with 3D brain scans. Finally, we make a broad comparison of data management methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pre-processing</head><p>After sketching the neuroimaging modalities used for AD detection, we next need to look at the way that studies use these modalities in their deep learning architecture. As a preliminary, however, the necessary preprocessing steps need to be recognised. Most studies, especially those in machine learning, need pre-processing before the data can be manipulated. The final success of an intelligent rating system depends strongly on effective pre-processing. With the advent of deep learning techniques, some pre-processing steps have become less critical <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b82">83]</ref>. However, most studies still use pre-processing techniques on raw data such as intensity normalization, registration, tissue segmentation, skull stripping, and motion correction. At the same time, some new deep learning methods have been proposed for different pre-processing routines <ref type="bibr" target="#b96">[97]</ref>. In this section, the most common pre-processing techniques are set out.</p><p>Intensity normalization means mapping the intensities of all pixels or voxels onto a reference scale. The intensities are normalized so that similar structures have similar intensities <ref type="bibr" target="#b97">[98]</ref><ref type="bibr" target="#b98">[99]</ref><ref type="bibr" target="#b99">[100]</ref><ref type="bibr" target="#b100">[101]</ref><ref type="bibr" target="#b101">[102]</ref><ref type="bibr" target="#b102">[103]</ref>. The most commonly adopted approach is to use the N3 nonparametric non-uniform intensity normalization algorithm <ref type="bibr" target="#b103">[104]</ref>. N3 is a robust and well-established algorithm for sharpening histogram peaks so that any intensity non-uniformity is reduced. It has been applied to correct non-uniform tissue intensities in about 30% of studies. Another technique used in about 20% of our studies is smoothing with a Gaussian filter, usually with FWHM (full-width at half maximum) of between 5-8 mm; this reduces the noise level in the image while retaining the signal level <ref type="bibr" target="#b72">[73]</ref>.</p><p>Another method of intensity normalization is to shift the distribution of voxel intensities about zero (i.e., zerocentred), which was reported in 15% of studies. Some studies have used other special intensity normalization methods, for example, processing to remove magnetic field inhomogeneities that occurred during image acquisition.</p><p>Registration is the process of spatially aligning image scans to a reference anatomical space. It is essential due to the complexity of brain structures and the differences between the brains of different subjects. Image registration aids in standardizing the neuroimaging modalities regarding a common fixed-size template (such as MNI 8 ). This alignment makes it possible to compare the voxel intensities of brain scans from different subjects, ensuring that a certain voxel in one scan has the same anatomical position as in the brain of another patient.</p><p>However, registration is not only about using a standard space but is also sometimes used for co-registering multiple modalities. The anterior commissure (AC) and posterior commissure (PC) are two major anatomical landmarks in the brain, so another way of aligning image geometry is AC-PC correction: a brain that has been AC-PC aligned has the AC and PC in the same axial plane. Another pre-processing step is Gradwarp, which corrects geometrical distortions due to gradient non-linearity.</p><p>The role of tissue segmentation in MRI brain scanning is to measure the volume of tissue in each region.</p><p>Since neurodegeneration affects grey matter (GM) at its initial stages, especially in the medial temporal lobe region <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b104">105]</ref>, GM probability maps (where GM is compared with white matter, WM) are usually used as the input in classification problems. GM probability maps give a quantitative picture of the spatial distribution of this tissue in the brain where the brightness of each voxel reflects the amount of local GM. However, a different method has been used in which non-WM has been extracted from an MRI using a GM mask on the corresponding FDG-PET scan <ref type="bibr" target="#b105">[106]</ref>. Another widely pre-processing technique is skull stripping, which removes the bone of the skull from images. This can be used alone or together with cerebellum removal or neck removal.</p><p>The last technique is motion correction, where motion artefacts in brain scans are suppressed. Figure <ref type="figure" target="#fig_8">5</ref> shows the prevalence of each pre-processing technique in the literature. As can be seen, intensity normalization and registration are done in more than 50% of studies. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Input data management</head><p>The main aim of feature extraction techniques is to create a quantified set of accurate information such as shape, texture, and volume of different parts of the brain based on neuroimaging data. The information should convey the disease pattern and be readily classified. In general, every classification problem has three stages: feature extraction, feature dimension reduction, and finally classification. Thanks to the structure of deep learning models, all these steps can be merged into one. However, managing the whole neuroimaging modality Pre-processing technique is still a challenge. Considering all the studies reviewed here, approaches to input data management can roughly be grouped into four different categories, depending on the type of extracted features: voxel-based, slice-based, patch-based, and ROI-based <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>. The prevalence of each category is shown in Figure <ref type="figure">6</ref>, with more details in the following sections (note, however, that not all studies fall into these categories; for example, a feature extraction method was used in <ref type="bibr" target="#b106">[107,</ref><ref type="bibr" target="#b107">108]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 6.</head><p>The prevalence of each approach to input data management.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Voxel-based</head><p>The voxel-based approaches are the most straightforward analysis technique. They use voxel intensity values from the whole neuroimaging modalities or tissue components (GM/WM in MRI). This technique typically requires spatial co-alignment (registration), where the individual images of the brain are standardized to a standard three-dimensional space. Most studies in this category (about 70%) performed a full-brain image analysis in either single-modality or multi-modality mode. In the rest of the studies, however, tissue segmentation (the extraction of GM) was performed on MRI images before applying a deep model. Voxel-based studies performing tissue segmentation cannot be considered full-brain image analysis as they work on only a part of the brain. The advantage of tissue segmentation in MRI brain scans is explained in Section 3.1. In voxelbased machine learning methods, a feature dimension reduction technique is usually applied, but this is not necessarily useful in deep structures. Nevertheless, to overcome high feature dimensionality, a voxel preselection method can be employed to each neuroimaging modality independently; as an example, Ortiz and colleagues used the t-test algorithm in an ROI-based study to eliminate non-significant voxels and decrease computational load <ref type="bibr" target="#b108">[109]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Slice-based</head><p>Slice-based architectures assume that certain properties of interest can be reduced to two-dimensional images, reducing the number of hyper-parameters. Many studies have used their own unique technique to extract 2D image slices from a 3D brain scan, whereas others consider standard projections of neuroimaging modalities, such as sagittal or median plane, coronal or frontal plane, and axial or horizontal plane.</p><p>Nevertheless, none of the studies in this category performed a full brain analysis, since a 2D image slice cannot include all the information from a brain scan. In addition to using tissue segmentation, slice-based methods usually take in the central part of the brain and ignore the rest.</p><formula xml:id="formula_0">Voxel-based 21% Slice-based 27% Patch- based 9% ROI-based 34% Combinational 9%</formula><p>Axial projection is the most widely used view. For example, Farooq et al. used slice-based axial scans of GM volumes such that slices from the start and end, which contain no information, were discarded <ref type="bibr" target="#b109">[110]</ref>. Other examples have used median axial slices from an MRI <ref type="bibr" target="#b110">[111]</ref>, 166 axial slices of GM <ref type="bibr" target="#b111">[112]</ref>, 43 axial slices of fMRI <ref type="bibr" target="#b31">[32]</ref>, and 3 axial slices of MRI <ref type="bibr" target="#b112">[113]</ref>. In two papers, the last 10 slices along the axial plane of each subject were removed from the GM, as well as slices with zero mean pixels, while all the other slices were concatenated and used <ref type="bibr" target="#b113">[114,</ref><ref type="bibr" target="#b114">115]</ref>. Axial slices from fMRI data were also used in <ref type="bibr" target="#b115">[116,</ref><ref type="bibr" target="#b116">117]</ref>, and again the first 10 slices of each scan were removed as they contained no functional information. A similar effort by Qui et al. <ref type="bibr" target="#b117">[118]</ref> used three slices in the axial plane of an MRI, including anatomical areas previously reported as regions of interest, and these were correlated with AD and MCI. Luo and colleagues <ref type="bibr" target="#b118">[119]</ref> extracted seven groups of slices (5 slices in each group) of the mid-axial plane of an MRI, with one classifier per group.</p><p>An entropy-based sorting procedure <ref type="bibr" target="#b119">[120,</ref><ref type="bibr" target="#b120">121]</ref> was used to select 32 most informative slices from the axial plane of each MRI scan. In this method, the image entropy of each slice was computed from the histogram, which delivered a measure of variation in each slice, and the slices with the highest entropy values were considered the most informative. Although using these informative slices for training will provide robustness, high entropy is not necessarily discriminative. Wu et al. adopted a new method that combined 3 slices into an RGB colour image to meet the requirement of their CNN architectures <ref type="bibr" target="#b121">[122]</ref>. From among about 160 axial slices of MRI scans, the first 15 slices and the last 15 slices without anatomical information were discarded, resulting in about 130 slices for each scan. Next, 48 different slices were selected randomly from the remaining slices at intervals of 4, and thus 16 RGB colour images were generated for each scan.</p><p>According to Gunawardena et al. <ref type="bibr" target="#b51">[52]</ref>, the coronal view covers the three most important AD-related regions in the brain (hippocampal, cortex, and ventricle), and they used only a couple of image slices from the coronal plane of MRI scans. Under the assumption that the middle slices include areas that have essential features for classification, 20 <ref type="bibr" target="#b122">[123]</ref> and 7 <ref type="bibr" target="#b123">[124]</ref> mid-coronal slices of an MRI have been used. A similar approach <ref type="bibr" target="#b124">[125]</ref> emphasized the discriminative potential of the coronal view. Five sagittal slices of an MRI at the centre of the hippocampus <ref type="bibr" target="#b76">[77]</ref>, 62 mid-sagittal slices of GM <ref type="bibr" target="#b125">[126]</ref>, and one sagittal slice of MRI (including hippocampus)</p><p>were employed in <ref type="bibr" target="#b126">[127]</ref>. Gao and colleagues <ref type="bibr" target="#b127">[128]</ref> selected the 50 largest pieces of the sagittal plane from each MRI scan, and then removed the noisiest and less useful image slices; the value of 50 was chosen based on the opinion of neurologists.</p><p>Since using all three views of 3D scans may supply complementary features useful for classification, there are some studies that take all image views into account. For example, a majority voting strategy for deep networks designed for each view was applied in <ref type="bibr" target="#b93">[94,</ref><ref type="bibr" target="#b128">129]</ref>. In <ref type="bibr" target="#b84">[85,</ref><ref type="bibr" target="#b85">86]</ref> the decomposed image slices from each projection of FDG-PET scans were divided into a number of groups at specific intervals that had some overlap but no registration or segmentation. In related work <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b75">76]</ref>, the hippocampal region of MRI scans was utilized on all 3 projections, but with only 3 slices at the centre of the hippocampal region in each projection. A similar approach was used with morphological information of MRI images, such as cortical volume, surface area, average cortical thickness, and standard deviation of thickness in each ROI <ref type="bibr" target="#b129">[130]</ref>. Aderghal used a similar method except it used a multi-modality approach (MRI+DTI) <ref type="bibr" target="#b77">[78]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">ROI-based</head><p>Instead of being concerned with the whole brain, ROI methods focus on particular parts of the brain known to be affected in the early stages of AD. The definition of ROIs usually requires previous knowledge of the abnormal regions and a brain atlas such as the Automated Anatomical Labeling (AAL) <ref type="bibr" target="#b130">[131]</ref> or the Kabani reference work <ref type="bibr" target="#b131">[132]</ref>, combined with the long-term experience of researchers. In this way, the GM tissue volume of 93 ROIs only from MRI <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55]</ref> along with the mean intensity from PET of the same number of ROIs were computed as features in <ref type="bibr">[67-71, 87, 133-135]</ref>. Similarly, 83 functional regions from MRIs (GM)</p><p>and PET were extracted in <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b135">136]</ref>. Choi and colleagues <ref type="bibr" target="#b71">[72]</ref> computed GM tissue volumes of 93 ROIs, and then picked out regional abnormalities using a deep model of each region. In other work <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b136">137,</ref><ref type="bibr" target="#b137">138]</ref>,</p><p>Principal Component Analysis (PCA) was applied after extracting 93 ROI-based volumetric features from MRI and the same number of features for PET. In <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref>, 90 ROIs were extracted from fMRI images and the correlation coefficient between each possible pair of brain regions computed.  <ref type="bibr" target="#b58">[59]</ref>. Image patches were extracted in each of 62 ROIs of PET <ref type="bibr" target="#b98">[99]</ref> or MRI images in <ref type="bibr" target="#b97">[98]</ref>, while 85 ROIs from PET <ref type="bibr" target="#b48">[49]</ref> and 87 ROIs from PET and MRI (GM only) <ref type="bibr" target="#b92">[93]</ref> were extracted, from which these ROIs were further utilized in a patch-based method. In another study, 90 ROIs were extracted and then a brain network connectivity matrix calculated from multi-modal data <ref type="bibr" target="#b138">[139]</ref>.</p><p>The median slice and its closest neighbours inside a 3D bounding box of the hippocampus were chosen in <ref type="bibr" target="#b74">[75]</ref><ref type="bibr" target="#b75">[76]</ref><ref type="bibr" target="#b76">[77]</ref><ref type="bibr" target="#b77">[78]</ref>. This method was called the -2D+ approach‖ since they moved from a 3D volume to 2D images. Occurrence Matrix (GLCM) to describe the FD feature pattern. In a multi-modality study <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b79">80]</ref>, the left and right lobe of the hippocampus were selected as the most discriminative parts, and a deep model was designed for each region. Collazos-Huertas et al. <ref type="bibr" target="#b139">[140]</ref> used morphological measurements of different parts of MRI scans, including cortical and subcortical volumes, average thickness and standard deviation, and surface area. In another MRI study <ref type="bibr" target="#b140">[141]</ref>, the two hippocampi were segmented and a local 3D image patch extracted from the centre of each; a deep model was then used for classification. In <ref type="bibr" target="#b141">[142]</ref>, 430 features were selected, including cortical thickness, curvature, surface area, and volume, as well as the hippocampus, and analysed together with gender, age, and baseline MMSE total score. Highly correlated features were then removed to produce independent features. Finally, a random forest classifier was used for feature selection to identify the 20 most important features.</p><p>After discarding the first and last 10 sagittal slices, Karwath et al. used a deep model to extract informative ROIs from PET scans <ref type="bibr" target="#b101">[102]</ref>. Li et al. <ref type="bibr" target="#b50">[51]</ref> selected ROIs from GM segmentation of MRIs, and then calculated a weighted connectivity matrix of ROIs, which represents the connection strength between region pairs, to produce a final brain network. Instead of directly learning the topological features from complex brain networks, the method learns the corresponding eigenvalues of the matrix, giving a compact and complete feature representation. In <ref type="bibr" target="#b142">[143]</ref>, a voxel-based morphometric analysis of regional GM differences between two groups of patients (MCIc, MCInc) was used to obtain the 5 most significant ROIs related to GM damage. Ortiz et al.</p><p>[60] considered 42 ROIs that were closely related to AD, and then computed an estimate of the corresponding inverse covariance between regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Patch-based</head><p>A patch is defined as a three-dimensional cube. Patch-based approaches can capture disease-related patterns in a brain by extracting features from small image patches. The main challenge in patch-based methods is to choose the most informative image patches for capturing both local (patch-level) and global (image-level)</p><p>features <ref type="bibr" target="#b93">[94]</ref>. This approach has been used in a number of studies for AD detection <ref type="bibr" target="#b143">[144]</ref>. For example, Cheng et al. <ref type="bibr" target="#b81">[82]</ref> extracted 27 uniform fixed-size local patches of voxels, with 50% overlap, from each FDG-PET image. A similar approach was proposed in a multi-modality study <ref type="bibr" target="#b82">[83]</ref>. Somewhat differently, landmark-based methods have been used to automatically extract discriminative anatomical landmarks of AD from MRIs via group comparison of subjects; first, the top 50 discriminative AD-related landmark locations were identified (bilateral hippocampal, parahippocampal, and fusiform) using a landmark discovery algorithm, and then 27 fixed-size image patches around these detected landmarks were extracted <ref type="bibr" target="#b93">[94,</ref><ref type="bibr" target="#b94">95,</ref><ref type="bibr" target="#b102">103,</ref><ref type="bibr" target="#b144">145]</ref>. A similar patchbased approach was followed in a multi-modality study <ref type="bibr" target="#b145">[146]</ref>. In another study <ref type="bibr" target="#b146">[147]</ref>, the whole brain MRI was uniformly partitioned into different local regions of the same size, and several 3D patches were extracted from each region. Then the patches from each region were grouped into different clusters with the k-means clustering method before the final classification.</p><p>Suk and colleagues <ref type="bibr" target="#b72">[73]</ref> proposed a latent high-level feature representation method using classdiscriminative patches (based on a statistical significance test between classes) in one multi-modality study (GM of MRI + FDG-PET). They selected class-discriminative patches from two modalities before the final threelevel classifier (patch-level, mega-patch-level, and image-level). A similar approach <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b58">59]</ref> was applied in a longitudinal study with a difference in patch selection after patch extraction. The 100 most class-discriminative patches of fixed-size were selected in a greedy manner using less than 50% volume overlap. A 3D scalar field of DM was then calculated, based on estimated voxel deformations matching the baseline; follow-up MRIs were done for each subject before the final classification stage. In addition to these patch-based features, 113 volumes of ROIs were also extracted by Shi <ref type="bibr" target="#b58">[59]</ref>. A multi-scale approach that extracted data at multiple scales from smaller sub-regions (fine-scale) to larger sub-regions (coarse-scale) was suggested by Lu <ref type="bibr" target="#b48">[49]</ref>. First, 85 ROIs of FDG-PET were extracted, and then the voxels inside each ROI were further subdivided into patches at three different scales in preparation for the classification stage. The same approach was applied by Lu <ref type="bibr" target="#b92">[93]</ref> for a multi-modal study where the mean intensity of each patch in FDG-PET scans was used to form a feature vector representing metabolic activity, and the volume of each GM patch from MRI was used to represent the brain structure. A similar method was proposed by Lian <ref type="bibr" target="#b91">[92]</ref>, where anatomical landmarks were used as prior knowledge to efficiently filter out uninformative regions and assist in defining relatively informative patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Summary of biomarker and handling issues</head><p>When dealing with biomarkers and features, it needs to be recognised that MRI is the most prevalent type of neuroimaging modality. Although several studies have reported that MRI is more discriminative compared with PET <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b147">148]</ref> or DTI <ref type="bibr" target="#b77">[78]</ref>, others regard MRI to be as discriminative as PET <ref type="bibr">[69-71, 81, 87, 133, 146]</ref> or slightly less discriminative <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b92">93]</ref>. Since other studies regard DTI <ref type="bibr" target="#b138">[139]</ref> or fMRI <ref type="bibr" target="#b114">[115]</ref> as most helpful, comparison of neuroimaging modalities still needs further investigation.</p><p>Managing the input to deep models is also an important issue. Using 2D slices as input instead of the whole 3D image avoids generating millions of training parameters and results in simplified networks (at the cost, of course, of losing spatial dependency between adjacent slices). When using slice-based methods, sagittal <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b75">76]</ref> and coronal <ref type="bibr" target="#b51">[52]</ref> views are reported to be more discriminative, although axial views are the most widely used (some studies <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b85">86]</ref> say there is no significant difference between planes). In terms of classification power, multi-view studies have been shown to outperform single-view studies by capturing complementary information <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b85">86]</ref>. By way of contrast, voxel-based methods can obtain all 3D information in a single brain scan, but they typically treat all brain regions uniformly without any adaptation to special anatomical structures.</p><p>Moreover, voxel-based methods ignore local information, since they treat each voxel independently, and carry high feature dimensionality and high computational load. To overcome the high feature dimensionality, voxel preselection methods might be necessary. Additional benefits of voxel-based methods are presented in <ref type="bibr" target="#b105">[106,</ref><ref type="bibr" target="#b148">149]</ref>, where voxels are shown to be more valuable than 2D slices.</p><p>Raw brain scans inevitably suffer from noise, which arises from different sources and at different levels depending on the type of scan. Noise sources usually originate from random neural activity of the patient, operator issues, equipment, and the environment. A single brain scan contains a complex pattern of voxels and a large amount of data, creating difficulties in classifying and interpreting features. To classify images, it is therefore necessary to extract a limited number of discrete pre-defined representative regions instead of using full-brain image analysis. The strength of ROI-based methods is that they are easily interpreted and implemented in clinical practice. Although the dimensionality of ROI-based features depends on the number of defined ROIs, it is always smaller than with slice/voxel-based approaches, meaning the entire brain is represented by fewer features. With ROI-based studies, knowledge that only specific parts of the brain, particularly the hippocampus, are involved in AD is put to good use. The hippocampus is a complex brain structure located in the temporal lobe with a key role in learning and memory, making it one of the most significant regions for AD detection. At the initial stages of AD, the volume, shape, and texture of the hippocampus are already affected and have been used as a marker of early AD in various studies <ref type="bibr">[57, 75-80, 98, 99, 141]</ref>. Ali and colleagues reported that while the average reduction in volume of the hippocampus is between 0.24% and 1.73% per year, AD patients suffer shrinkage at between 2.2% and 5.9% <ref type="bibr" target="#b149">[150]</ref>. In this context, shape analysis was reported to be more sensitive than volumetry, in particular, at the MCI stage <ref type="bibr" target="#b6">[7]</ref>. According to Leandrou and colleagues <ref type="bibr" target="#b19">[20]</ref>, texture analysis can outperform both shape and volumetric analysis in classification accuracy. A few studies combined volume, thickness, shape, intensity, and texture features in the evaluating AD <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b150">151]</ref>, which may result in better classification performance. Note that although the regions mainly affected by AD are well-known, it should be remembered that other brain regions might also play a role in the diagnosis of AD/MCI; however, their contribution is still not well explored <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b70">71]</ref>.</p><p>While ROI-based feature extraction can considerably decrease feature dimensions, because of the coarsescale nature of ROIs some small abnormalities might be ignored. Also, an abnormal region might occupy only a small part of a pre-defined ROI, may have an irregular shape, and may be distributed over many incompletely known brain regions; if so, it could lead to loss of discriminative information and limit the representational power of the extracted features <ref type="bibr" target="#b94">[95]</ref>. Accordingly, there could be instability in classification performance <ref type="bibr" target="#b151">[152]</ref>.</p><p>On the other hand, since patch-based methods occupy the intermediate scale between voxel-based and ROIbased features, they can efficiently handle high feature dimensions and are sensitive to small changes <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b146">147]</ref>.</p><p>Because patch extraction does not require ROI identification, the necessity to involve a human expert is reduced compared to ROI-based approaches. In the end, Cheng and colleagues reported that patch-based methods are more accurate compared with voxel-based methods <ref type="bibr" target="#b144">[145]</ref>. However, challenges in selecting the most informative image patches still remain. By only using discriminative patches instead of all patches in a brain scan, Suk and colleagues found both enhanced classification performance and reduced computational cost <ref type="bibr" target="#b72">[73]</ref>.</p><p>A summary of data handling methods is given in Table <ref type="table" target="#tab_0">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strengths Limitations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sliced-based</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Avoids confronting with millions of parameters during training and results in simplified networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loses spatial dependencies in adjacent slices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Voxel-based</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Can obtain 3D information of a brain scan</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contains high feature dimensionality and high computation load Ignores the local information of the neuroimaging modalities as it treats each voxel independently</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROI-based</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Easily interpretable Has a low feature dimension Fewer features can reflect the entire brain Has limited available knowledge about the brain regions involved in AD Ignores detailed abnormalities</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Patch-based</head><p>Sensitive to small changes Does not require ROI identification</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Has challenges to select the most informative image patches</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A review on deep models for AD detection</head><p>The Convolutional Neural Network (CNN). Figure <ref type="figure" target="#fig_12">7</ref> shows the prevalence of each deep model in AD detection, and more details can be found in <ref type="bibr" target="#b152">[153]</ref>. There are also a few methods that cannot fit into this categorization scheme;</p><p>for example, Collazos-Huertas and colleagues proposed a deep supervised feature extraction method using General Stochastic Networks through supervised and layer-wise non-linear mapping learning <ref type="bibr" target="#b139">[140]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Unsupervised deep learning</head><p>Unsupervised deep learning networks try to obtain a task-specific representation from neuroimaging data.</p><p>The step from machine learning into deep learning was first taken by using an unsupervised deep neural network to extract high-level features <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b68">69]</ref>. Suk and colleagues developed an approach where they iteratively discarded uninformative features in a hierarchical manner to select features from MRI, PET, and CSF biomarkers <ref type="bibr" target="#b69">[70]</ref>. In the end, all of these studies used SVMs for classification. More details on unsupervised feature learning methods are given in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Auto-Encoder</head><p>The AE is a particular type of neural network consisting of two modules: an encoder and a decoder. An AE can obtain compressed representations from input data by minimizing the reconstruction error between the input and output values of the network. Encoding maps the data from the input space to a representation space to keep the most definitive features, whereas decoding maps it back into the input space, thus reconstructing the data.</p><p>For classification purposes, the features learned in the middle layer of an AE can be extracted and used as the pre-training phase for feature extraction and dimension reduction in an unsupervised way, followed by a classifier. Because of its simple and shallow structure, the representational power of AEs is very limited.</p><p>However, multiple AEs can be stacked in a configuration known as stacked AEs, which can considerably enhance the representational power by using the values of the hidden units of one AE as the input to the next.</p><p>The key characteristic of stacked AEs is their ability to learn or discover highly nonlinear and complex patterns.</p><p>As the depth of the model increases, higher-level representations are learned.</p><p>There are different variations of AEs: sparse AE, de-noising AE, convolutional AE, and variational AE.</p><p>General deep learning structures in this section usually consist of an AE and a softmax layer. Stacked sparse AEs with two <ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b135">136]</ref> or three <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b92">93]</ref> hidden layers and a softmax layer have been trained and finetuned in several single-modality <ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref> and multi-modality <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b92">93,</ref><ref type="bibr" target="#b135">136]</ref> studies. Ortiz and colleagues followed a similar approach with three hidden layers in a stacked de-noising AE <ref type="bibr" target="#b59">[60]</ref>. Shakeri and colleagues introduced a variational AE for finding the latent feature representation from hippocampus morphological variations <ref type="bibr" target="#b56">[57]</ref>. Moussavi-Khalkhali et al. used a partially cascaded architecture using sparse/denoising AEs with three hidden layers to extract high-level representations, integrating low-level features with a softmax layer <ref type="bibr" target="#b123">[124]</ref>. Kim et al. used a stacked sparse AE with three hidden layers from MRI, PET, and CSF images <ref type="bibr" target="#b133">[134]</ref>;</p><p>another AE was then used to fuse the high-level representation of all the data classified with a kernel-based extreme learning machine.</p><p>Choi and colleagues used a stacked denoising AE with two hidden layers to extract regional abnormalities <ref type="bibr" target="#b71">[72]</ref>. In a related fashion, a stacked de-noising sparse AE, which is a combination of both denoising and sparse AEs, has been proposed, an approach that uses three hidden layers together with an SVM for classification <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b58">59]</ref>. By concatenating the learned feature representation of stacked AE from MRI, PET, and CSF images with the original low-level features, Suk and co-workers were able to construct an augmented feature vector that was then fed into a multi-kernel SVM <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b70">71]</ref>. Three hidden layers were used for MRI, PET, and their concatenation, and two hidden layers were used for CSF. Other workers have used a sparse AE to extract features, followed by a 2D CNN <ref type="bibr" target="#b143">[144]</ref> or DNN <ref type="bibr" target="#b99">[100]</ref> for classification. Motivated by stacked de-noising AEs,</p><p>Majumdar and colleagues proposed a deep dictionary learning platform using both clean and noisy samples, finally using a neural network for classification <ref type="bibr" target="#b137">[138]</ref>. Li and collaborators used a combination of features from 3D CNN and multi-scale 3D convolutional AEs with three hidden layers, ultimately using a softmax layer for classification <ref type="bibr" target="#b83">[84]</ref>.</p><p>One of the uses of AEs is to find a good initialization for deep neural networks. For example, an AE was trained to find a suitable filter for convolution, and then the classification was achieved using a CNN <ref type="bibr" target="#b142">[143,</ref><ref type="bibr" target="#b145">146,</ref><ref type="bibr" target="#b148">149,</ref><ref type="bibr" target="#b153">154,</ref><ref type="bibr" target="#b154">155]</ref>, an arrangement whereby weights of the hidden layer form a matrix corresponding to convolutional filters. A high-level layer concatenation AE, which is a variant of the convolutional AE network, was used by Vu and co-workers to pre-train a CNN in which the high-resolution features from the encoding layer are concatenated with the corresponding decoding layer <ref type="bibr" target="#b105">[106]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Restricted Boltzmann Machine</head><p>The RBM is a single-layer undirected graphical model with a visible layer and a hidden layer. It adopts symmetric links between visible and hidden layers but has no connections among units within the same layer.</p><p>Like an AE, it can generate input data from hidden representations and has been used in a few studies. For </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Supervised deep learning</head><p>We have seen so far that a typical unsupervised method comprises a feature extractor, which is usually AEs, and a classifier, like an SVM. Supervised methods have higher popularity in our literature review, where feature extraction and classification are merged into one model. In this section, supervised methods for AD detection are reviewed. Further details about the methods used in each paper are set out in Table <ref type="table" target="#tab_0">2</ref> of Appendix 1, which summarizes a wide variety of deep models together with their biomarkers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Deep Neural Network</head><p>The DNN has the same configuration as a traditional Multi-Layer Perceptron (MLP) network, yet incorporates more stacked layers. It can discover complicated relations of input patterns and provides a better understanding of the nature of the data <ref type="bibr" target="#b49">[50]</ref>. DNNs are purely supervised and widely used in different research areas to discover previously unknown extremely abstract patterns and correlations. However, the training process of DNNs is not optimal, at least compared to SVMs, and its learning process is also very slow <ref type="bibr" target="#b38">[39]</ref>.</p><p>Bhatkoti et al. proposed a DNN with one hidden layer after a feature extraction step with a modified sparse AE <ref type="bibr" target="#b99">[100]</ref>. Amoroso and colleagues used a DNN with 11 layers that included a mixed optimal configuration of activation with a decreasing number of neurons in each layer, followed by a softmax at the end <ref type="bibr" target="#b141">[142]</ref>. Cui and co-workers used a DNN with two hidden layers to extract features and train an RNN <ref type="bibr" target="#b87">[88]</ref>. A three-stage deep feature learning and fusion framework for MRI, PET, and genetic data was proposed in <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68]</ref>, with the first stage involving the learning of latent representations of each modality and the last stage the learning of joint latent features. In each stage, a DNN was used, and each DNN consisted of several fully-connected hidden layers and one output softmax layer. A similar approach was used by Thung and collaborators who combined these modalities together with age, gender, and educational level <ref type="bibr" target="#b134">[135]</ref>. Although DNNs are mostly used in a supervised manner, an unsupervised approach with three hidden layers was suggested by Li and colleagues to extract high-level feature representation with a linear kernel SVM as the classifier <ref type="bibr" target="#b49">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Deep Polynomial Network</head><p>The DPN is another supervised deep learning algorithm, which may have similar, or even better, performance compared with DBNs and stacked AEs <ref type="bibr" target="#b155">[156]</ref>. Motivated by successful applications of DBNs and stacked AEs, DPNs can also be stacked to construct a much deeper configuration to further enhance representation performance. A multi-modal stacked DPN (SDPN) consisting of two-stage SDPNs has been proposed for feature extraction from multi-modal neuroimaging data <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b132">133]</ref>. Two SDPNs are first used to learn high-level features from MRI and PET, from where features are then fed to another stacked DPN to fuse multi-modal neuroimaging information. The final learned high-level features contain both the intrinsic properties of each modality and correlations between the modalities. The whole model was trained in an unsupervised manner with a linear SVM for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Convolutional Neural Network</head><p>CNNs are a type of deep neural network inspired by the visual cortex of the brain. They are the most successful deep model for image analysis and have been designed to better utilize spatial information by taking 2D/3D images as input and extracting features by stacking several convolutional layers; the result is a hierarchy of gradually more abstract features <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b41">42]</ref>. The main idea behind CNNs, and their major advantage, is to merge feature extraction and classification. The logic behind them is that training a classifier independently from the feature extraction stage can lead to poor learning performance, possibly due to the heterogeneous nature of the extracted features and the classifier. The structural information between neighbouring pixels or voxels is also very important for images. In the deep models we are considering, the inputs are mostly in vector form, yet vectorization inevitably destroys structural information in images. Also, in contrast to DNNs, due to pooling layers and shared weights in CNNs, the number of parameters are drastically reduced <ref type="bibr" target="#b41">[42]</ref>. In recent years, CNNs have become very popular and in image-based applications. However, the need for a large dataset can be considered a weakness of these models.</p><p>CNNs were first introduced in 1989 by LeCun and colleagues <ref type="bibr" target="#b156">[157]</ref>. Despite initial success, they have not been widely employed until recently when various new methods have emerged to efficiently train deep networks, and computer systems have improved <ref type="bibr" target="#b41">[42]</ref>. CNNs attracted great interest after deep CNNs achieved remarkable results in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) competitions, where they have been successfully applied to a dataset of about a million images that included 1000 different classes <ref type="bibr" target="#b157">[158]</ref>. Although many studies prefer to design their own structure based on CNNs, it is common to use wellknown and proven structures like LeNet <ref type="bibr" target="#b158">[159]</ref>, AlexNet <ref type="bibr" target="#b159">[160]</ref>, CaffeNet <ref type="bibr" target="#b160">[161]</ref>, VGGNet <ref type="bibr" target="#b161">[162]</ref>, GoogLeNet <ref type="bibr" target="#b162">[163]</ref>, ResNet <ref type="bibr" target="#b163">[164]</ref>, DenseNet <ref type="bibr" target="#b164">[165]</ref>, and Inception <ref type="bibr" target="#b165">[166]</ref>. The success of these models has already been reported in the literature, and especially demonstrated in ImageNet competitions; more details can be found in <ref type="bibr" target="#b152">[153,</ref><ref type="bibr" target="#b166">[167]</ref><ref type="bibr" target="#b167">[168]</ref><ref type="bibr" target="#b168">[169]</ref>. In the following subsection, the architecture of CNNs is reviewed, and then related studies using 2D/3D CNNs for AD detection are discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.1">CNN architecture</head><p>CNNs are made up of several convolutional layers, activation layers, pooling layers, fully-connected layers, and a softmax layer. Training a CNN includes a forward and backward stage to compute the loss cost between the predicted output and the ground truth labels, with penalties computed with chain rules. The first and fundamental layer is the convolutional layer, which convolves the input image with the learned filters and produces appropriate feature maps. While the first layers of the CNNs extract discriminative shift/scaleinvariant features of local image patches, the last layers permit task-specific classification using these features <ref type="bibr" target="#b152">[153]</ref>. A typical convolutional layer is usually followed by applying a nonlinear activation function such as a sigmoid, hyperbolic tangent (Tanh), or Rectified Linear Unit (ReLU) to build a feature map for each filter. This non-linearity enables models to learn more complex representations. ReLU has been employed in most studies, while sigmoid or Tanh is still popular. The second type of layer that comes after the convolutional layer is the pooling layer, which down-samples the input feature map by replacing each non-overlapping block with its maximum or average. Pooling helps reduce the number of parameters, feature dimensions, and computations in the network while keeping the most influential features more compact in low to higher layers. It therefore achieves a degree of robustness to certain distortions and geometric variations such as shift, scale, and rotation.</p><p>The fourth type of layer is the fully-connected layers that perform like traditional neural networks and contain typically about 90% of the parameters in a CNN <ref type="bibr" target="#b152">[153]</ref>. After a series of convolutional and pooling layers, 2D/3D feature maps are flattened into a 1D feature vector that no longer has spatial coordinates; a fullyconnected layer is then implemented. Fully-connected layers connect all feature elements in the previous layer to the output layer, helpful in learning non-linear relationships between the local features. Finally, the softmax layer classifies subjects by selecting the highest predicted probabilities for each label. The softmax function highlights the largest values in a vector while suppressing those that are significantly below the maximum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.2">2D CNNs</head><p>CNNs are specifically designed to recognize patterns in two-dimensional images. Many studies have used 2D CNNs for 3D neuroimaging. Also, according to <ref type="bibr" target="#b84">[85,</ref><ref type="bibr" target="#b85">86]</ref>, building a 3D CNN requires a larger number of parameters than 2D CNNs. Section 3.2.2 explained how different studies have extracted 2D information from 3D images by splitting volumetric data into image slices. Here, different deep architectures using 2D CNNs are discussed. The usual deep model here is a couple of convolutional layers paired with pooling layers and followed by fully-connected layers and a softmax layer. For instance, 2D CNNs with two <ref type="bibr" target="#b169">[170]</ref>, three <ref type="bibr" target="#b106">[107,</ref><ref type="bibr" target="#b107">108,</ref><ref type="bibr" target="#b129">130]</ref>, or five <ref type="bibr" target="#b170">[171]</ref> convolutional layers in a single-modality approach, or with four convolutional layers in a multi-modal study <ref type="bibr" target="#b138">[139]</ref>, have been employed. Other examples are several 2D CNNs with two convolutional layers in a couple of MRI image slices from the coronal view <ref type="bibr" target="#b51">[52]</ref>; six convolutional layers on one sagittal MRI slice including the hippocampus <ref type="bibr" target="#b126">[127]</ref>; and one convolutional layer together with a Polynomial Kernel SVM where the filters of the CNN was provided by an AE <ref type="bibr" target="#b142">[143]</ref>. Luo and co-workers used seven 2D CNNs on seven groups of slices consisting of three convolutional layers <ref type="bibr" target="#b118">[119]</ref>; this meant that if one or more of the classifiers classified a subject as AD, then the subject was definitely classified as AD. Aderghal et al. used another 2D</p><p>CNN architecture consisting of two convolutional layers on a few slices of the hippocampus <ref type="bibr" target="#b76">[77]</ref>. This architecture was further modified to fuse all different views with a majority vote <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b75">76]</ref>. To achieve better results it has been further developed as the core of a multi-modality approach to combining all views <ref type="bibr" target="#b77">[78]</ref>. A 2D</p><p>CNN with two convolutional layers has been used by taking, as weak learners, the predicted response values from multiple sparse regression models <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55]</ref>. Li and co-workers proposed the architecture of a spectral CNN based on a common CNN made up of convolutional layers, subsampling layers, and fully-connected layers <ref type="bibr" target="#b50">[51]</ref>.</p><p>The input layer of this CNN was the spectral domain representation of a brain network, where a set of eigenvalues represented the connections between regional pairs and the locations of nodes.</p><p>However, due to the absence of kernel sharing across the third dimension, a scheme of using 2D CNNs is inefficient in encoding the spatial information from 3D images <ref type="bibr" target="#b82">[83]</ref>. That is why Islam et al. designed three 2D</p><p>CNNs to obtain three different views of an MRI <ref type="bibr" target="#b128">[129]</ref>. Each CNN consisted of four convolutional layers and four dense blocks (each with 12 convolutional layers), and the final decision was made with majority voting. To directly capture the spatial information in a 3D image, a novel structure combining a CNN and an RNN has been proposed <ref type="bibr" target="#b84">[85,</ref><ref type="bibr" target="#b85">86]</ref>. In these studies, 2D CNNs were built to capture the intra-slice features (similar structures in a single slice), while RNN was used to extract the inter-slice features (similar structures in adjacent slices) for final classification. First, an FDG-PET scan was decomposed into several 2D image slices in the coronal, sagittal, and axial directions; the decomposed image slices were further partitioned into several groups, and for each group of slices, a deep 2D CNN with five convolutional layers was built and trained. After testing the classification performance of each CNN, the best CNN was selected with the highest classification accuracy from three different planes. Finally, CNNs and RNN were combined in each direction to obtain three prediction scores. The final classification was performed by weighted averaging of the three prediction scores obtained from the three different views <ref type="bibr" target="#b84">[85,</ref><ref type="bibr" target="#b85">86]</ref>.</p><p>Focusing on well-known 2D architectures and training from scratch, an adjusted LeNet and GoogLeNet have been used to classify 2D GM slices <ref type="bibr" target="#b113">[114]</ref> or fMRI slices <ref type="bibr" target="#b115">[116,</ref><ref type="bibr" target="#b116">117]</ref> or both <ref type="bibr" target="#b114">[115]</ref>. Farooq and colleagues used a 2D CNN based on GoogLeNet and ResNet on 2D MRI images <ref type="bibr" target="#b109">[110]</ref>. Kazemi et al. demonstrated that both</p><p>AlexNet and GoogLeNet performed well on 2D fMRI images for classifying different stages of AD <ref type="bibr" target="#b31">[32]</ref>.</p><p>However, GoogLeNet was reported to be more time consuming, and so AlexNet was chosen as the classifier. It is common practice to use proven pre-trained CNNs in the initialization stage for one domain-specific task and then re-train them for new tasks by fine-tuning the CNNs. This is possible because the lower layers of CNNs have more general features that can be applied to many tasks and are therefore able to be transferred from one application domain to another, a process known as -transfer learning‖. Using transfer learning from the ImageNet dataset, Wu and co-workers fine-tuned CaffeNet and GoogLeNet to predict the risk of conversion from MCI to AD on 2D MRI images <ref type="bibr" target="#b121">[122]</ref>. Similarly, several 2D CNNs, which have used image slices as input, have been built based on pre-trained VGGNet-16 <ref type="bibr" target="#b120">[121,</ref><ref type="bibr" target="#b122">123]</ref>, ResNet-18 <ref type="bibr" target="#b110">[111]</ref>, Inception-V4 <ref type="bibr" target="#b171">[172]</ref>, DenseNet-121 <ref type="bibr" target="#b124">[125]</ref>, VGGNet-16 and Inception-V4 <ref type="bibr" target="#b119">[120]</ref>, and GoogLeNet and ResNet-152 <ref type="bibr" target="#b111">[112]</ref>. Gao and collaborators fine-tuned ResNet-18 and used it together with RNN <ref type="bibr" target="#b127">[128]</ref>. Zheng et al. fine-tuned AlexNet for 2D images at the centre of each ROI of PET scans <ref type="bibr" target="#b98">[99]</ref>, selecting an ensemble of 30% of the well-performed AlexNets as the classifier using a voting strategy. Islam and colleagues applied transfer learning to an ensemble of three DenseNet styled models with different depths <ref type="bibr">(121-161-169)</ref>, where the final decision was made by majority voting <ref type="bibr" target="#b172">[173]</ref>. In another study, Qiu et al. independently trained two MLP models on MMSE and LM test results, and VGGNet-11 architecture was fine-tuned for three selected MRI slices <ref type="bibr" target="#b117">[118]</ref>. The predictions from these three models were further combined by using majority voting to make the final decision. In a different transfer learning concept, Wegmayr and colleagues used a 2D deep model based on Inception-V3 as a static feature extractor <ref type="bibr" target="#b112">[113]</ref>. In the study, only one additional linear layer was trained on top of the concatenated features.</p><p>The model achieved the same accuracy as the 3D-CNN model trained from scratch; however, it trained much quicker because the top-tuned layer had many fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.3">3D CNNs</head><p>Because neuroimaging provides 3D images, and there is a spatial relationship among the images, 3D CNNs are popular. Despite their complexity, AD detection must take the whole image or some ROIs as the input.</p><p>However, this may require training a large number of parameters on a small dataset, which may result in overfitting <ref type="bibr" target="#b82">[83]</ref>. In direct methods, 3D CNNs with twelve <ref type="bibr" target="#b173">[174]</ref>, five <ref type="bibr" target="#b174">[175]</ref>, and four <ref type="bibr" target="#b100">[101,</ref><ref type="bibr" target="#b175">176]</ref> convolutional layers have been utilized. Another network with seven convolutional layers, where three different filter sizes were chosen in its first convolutional layer to capture input features on different length scales, was used in <ref type="bibr" target="#b176">[177]</ref>. Li et al. used a combination of features from a 3D CNN with six convolutional layers and multi-scale 3D convolutional AEs, with three hidden layers and a softmax layer for classification <ref type="bibr" target="#b83">[84]</ref>. In a related approach, 3D CNNs were pre-trained with an AE using one <ref type="bibr" target="#b148">[149]</ref> or three <ref type="bibr" target="#b153">[154,</ref><ref type="bibr" target="#b154">155]</ref> convolutional layers. Vu and colleagues used two 3D CNNs, each with a convolutional layer pre-trained with a sparse AE, on two modalities, combining them with a fully-connected layer <ref type="bibr" target="#b145">[146]</ref>. Punjabi et al. used a 3D CNN with three convolutional layers and two fully-connected layers for each modality, combining all the layers with another fully-connected layer <ref type="bibr" target="#b147">[148]</ref>. Other proposals have been for 3D CNNs with five <ref type="bibr" target="#b177">[178]</ref> or seven <ref type="bibr" target="#b178">[179]</ref> convolutional layers fused with a DNN at the final fully connected layer. The input data used was clinical and genetic data for the DNN, and MRI for the 3D CNN. Feng et al. used two independent 3D CNNs, each with six convolutional layers for the MRI and PET images <ref type="bibr" target="#b55">[56]</ref>, and used a stacked bidirectional RNN instead of traditional fully-connected layers to gain further advanced semantic information.</p><p>Karwath and colleagues employed a 3D CNN classifier with seven convolutional layers to extract ROIs from PET voxel data <ref type="bibr" target="#b101">[102]</ref>. In this approach, the contribution of each voxel was calculated relative to the accuracy of the utilized 3D CNN, so that an inaccurate voxel could be excluded from input to the 3D CNN. In contrast to many deep learning methods that target the classification, this method targets the ROIs. Chen et al. used a 3D</p><p>CNN with seven convolutional layers for each ROI, with the final decision made by majority voting <ref type="bibr" target="#b97">[98]</ref>.</p><p>Khvostikov et al. used the 3D CNN for left and right lobes of the hippocampus in each modality, and then combined all CNNs with fully-connected layers <ref type="bibr" target="#b78">[79]</ref>. In this work, slightly different configurations were evaluated for AD vs. NC, and a configuration with six convolutional layers was selected. Two 3D CNNs were designed with five convolutional layers for the left and right lobes of the hippocampus, and the final classification was made by combining the prediction scores from both 3D CNNs <ref type="bibr" target="#b140">[141]</ref>. Liu and colleagues used 3D CNN models with five <ref type="bibr" target="#b94">[95]</ref> and six convolutional layers using a concatenation of features <ref type="bibr" target="#b93">[94]</ref>; the approach was to learn patch-based morphological features from each landmark and make a final classification using a majority voting strategy on all CNNs for all landmarks. A similar configuration with six convolutional layers was later proposed <ref type="bibr" target="#b102">[103]</ref>, in which the researchers concatenated features from all landmarks with fullyconnected layers, and incorporated personal information (e.g., age, gender, and education level) in another set of fully-connected layers. This deep learning framework thereby embedded personal information and automatically learnt MRI representations without requiring any expert knowledge of pre-defined features (similar to <ref type="bibr" target="#b93">[94,</ref><ref type="bibr" target="#b94">95]</ref>).</p><p>In another related paper, Cheng and colleagues utilized multiple 3D CNNs with four convolutional layers on several local image patches <ref type="bibr" target="#b144">[145]</ref>. The CNNs for the ensemble were selected according to the classification accuracy of the validation data, and jointly fine-tuned in the last few layers to be more adapted to the global classification task. Esmaeilzadeh et al. trained a 3D CNN with three convolutional layers on two classes (AD vs.</p><p>NC) <ref type="bibr" target="#b179">[180]</ref> then fine-tuned the weights so as to classify the subjects into three categories. Choi and colleagues, in a multi-modal study <ref type="bibr" target="#b180">[181]</ref>, used a 3D CNN with three convolutional layers for predicting MCI conversion. For on each ROI <ref type="bibr" target="#b79">[80]</ref>, finally, concatenating all models to produce the classification result. Note that the extension of 2D to 3D in these models creates considerable challenges, including an increased number of parameters and considerable memory and computational requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.4">Cascaded 2D/3D CNNs</head><p>Cascaded CNNs have been built to learn features from MRI and PET brain scans <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b82">83]</ref>. First, multiple deep 3D CNNs with four convolutional layers are constructed on different local image patches to transform the local brain image into more compact high-level features. Then a high-level 2D CNN with two convolutional layers is cascaded to combine the high-level features and generate the latent multi-modal correlation features of the corresponding image patches. Finally, these extracted features are combined by a fully-connected layer followed by a softmax layer for AD classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Recurrent Neural Network</head><p>In time series problems, such as in video applications, RNNs include a ‗memory to model' temporal dependency. In this scheme, past information is implicitly stored in hidden units called state vectors, and using these state vectors, the output of the current sequential input is computed by considering the current input data as well as all previous input data. RNNs are not as deep as DNNs or CNNs in terms of the number of layers, and they may have problems in memorizing long-term input data <ref type="bibr" target="#b38">[39]</ref>. They still require large datasets. Fortunately, substituting the simple perceptron hidden units with more complex units such as LSTM (Long Short-Term Memory) or GRU (Gated Recurrent Unit), which function as memory cells, helps considerably in overcoming the memory problem. The LSTM contains three gate units and a memory cell unit, a more complicated arrangement compared with a traditional RNN, but one that can effectively capture valuable information in a sequence <ref type="bibr" target="#b87">[88]</ref>. GRU is a simpler kind of LSTM with slightly better performance <ref type="bibr" target="#b84">[85,</ref><ref type="bibr" target="#b87">88]</ref>. Although 2D images involve spatial information instead of sequential information, a 3D image can be treated as a sequence of 2D images. Nowadays, RNNs are being increasingly applied to images <ref type="bibr" target="#b41">[42]</ref>.</p><p>A 2D CNN together with an RNN has been constructed and trained, in which the hierarchical 2D CNNs captured the intra-slice features (similar structures in a single slice), while the GRU was used to extract the inter-slice features (similar structures in adjacent slices) for final classification <ref type="bibr" target="#b84">[85,</ref><ref type="bibr" target="#b85">86]</ref>. Individual CNNs and GRU combination models for axial, sagittal, and coronal planes have been trained to produce three prediction scores. The final classification was performed by weighted averaging of the three prediction scores from three different planes. Feng et al. designed an independent 3D CNN for each modality, and to obtain more detailed information they used a stacked bidirectional RNN at the end instead of traditional fully-connected layers <ref type="bibr" target="#b55">[56]</ref>.</p><p>Cui and co-workers constructed and trained a model with two GRU layers to capture longitudinal changes from time-series data <ref type="bibr" target="#b87">[88]</ref>. To extract temporal features, two GRUs were stacked with a sequence of input feature vectors, which were generated from the first layer of an MLP. In the study, the GRUs utilized intra-image features and extracted longitudinal features. To facilitate disease diagnosis, Gao et al. designed an LSTM-based architecture <ref type="bibr" target="#b127">[128]</ref> to extract longitudinal features and capture pathological changes. Another longitudinal study based on RNNs was done by Lee and colleagues <ref type="bibr" target="#b88">[89]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparative analysis of different deep models</head><p>Unfortunately, most of the studies in our literature review did not submit their source code either to any software development hosting platform or to an online competition one. Therefore, it is not easy to impartially compare the studies with each other. In addition, it was noticed that most studies that compared their results to those of others did not actually implement the competitors' algorithms, but just reported their final accuracies.</p><p>Even if the competing algorithm was implemented, there is no guarantee it would be identical with the original.</p><p>Nevertheless, comparative information, collected from all the studies in our literature review is given in this section, together with our perspective on the individual studies.</p><p>Before comparing different deep models, it is first necessary to discuss transfer learning. Training a deep model from scratch was done in most of the studies; however, it is often inefficient to do so since the training process is time-consuming and a dataset of satisfactory size (millions of images) is required <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b122">123,</ref><ref type="bibr" target="#b127">128,</ref><ref type="bibr" target="#b166">167]</ref>.</p><p>Neuroimaging datasets typically have only hundreds of images, a circumstance that gives rise to over-fitting.</p><p>Transfer learning is faster and achieves better results compared to training from scratch <ref type="bibr" target="#b110">[111,</ref><ref type="bibr" target="#b112">113,</ref><ref type="bibr" target="#b119">120</ref>]. There appears to be close competition among well-known 2D CNNs such as GoogLeNet and ResNet-152 <ref type="bibr" target="#b109">[110,</ref><ref type="bibr" target="#b111">112]</ref>; however, it seems that Inception-V4, ResNet, and CaffeNet have outperformed GoogLeNet, VGGNet-16, and</p><p>AlexNet <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b119">120,</ref><ref type="bibr" target="#b121">122,</ref><ref type="bibr" target="#b171">172,</ref><ref type="bibr" target="#b183">184]</ref>. One paper reports that DenseNet outperforms ResNet and LeNet <ref type="bibr" target="#b146">[147]</ref>. A recent study that implements several well-known 2D CNNs using transfer learning is available <ref type="bibr" target="#b186">[187]</ref>.</p><p>When comparing deep models, in one single-modality study <ref type="bibr" target="#b58">[59]</ref> a combination of patch-based and ROIbased methods gave higher accuracy than an ROI-based method in another multi-modality study <ref type="bibr" target="#b43">[44]</ref> that has used stacked AEs. A combination of patch-based and ROI-based stacked AEs <ref type="bibr" target="#b92">[93]</ref> outperformed a combination of patch-based and voxel-based DBM <ref type="bibr" target="#b72">[73]</ref>. Voxel-based 3D CNN + stacked 3D AEs <ref type="bibr" target="#b153">[154]</ref> was reported to be more accurate compared with ROI-based stacked AEs <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b68">69]</ref> and RBMs <ref type="bibr" target="#b136">[137]</ref>. ROI-based stacked DPNs <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b132">133]</ref> outperformed ROI-based single DPNs, stacked AEs <ref type="bibr" target="#b68">[69]</ref>, and RBMs <ref type="bibr" target="#b136">[137]</ref>. An ROI-based ensemble of 3D</p><p>CNNs <ref type="bibr" target="#b97">[98]</ref> and AlexNets <ref type="bibr" target="#b98">[99]</ref> in two single-modality studies outperformed several multi-modality studies (ROIbased stacked AEs <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b68">69]</ref>, RBMs <ref type="bibr" target="#b136">[137]</ref>, and DBNs <ref type="bibr" target="#b108">[109]</ref>). Slice-based VGGNet-16 <ref type="bibr" target="#b122">[123]</ref> outperformed patch-based <ref type="bibr" target="#b143">[144]</ref> and voxel-based <ref type="bibr" target="#b148">[149]</ref> combination of AEs and CNNs. Slice-based Inception-V4 <ref type="bibr" target="#b119">[120]</ref>,</p><p>voxel-based 3D CNNs <ref type="bibr" target="#b105">[106,</ref><ref type="bibr" target="#b154">155,</ref><ref type="bibr" target="#b174">175,</ref><ref type="bibr" target="#b176">177,</ref><ref type="bibr" target="#b181">182]</ref>, voxel-based 3D CNN + stacked RNN <ref type="bibr" target="#b55">[56]</ref>, and patch-based ensemble of 3D and 2D CNNs <ref type="bibr" target="#b82">[83]</ref> were reported to be more accurate than AEs using other data management methods <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b143">144,</ref><ref type="bibr" target="#b148">149,</ref><ref type="bibr" target="#b153">154]</ref>. A combination of 3D and 2D CNNs <ref type="bibr" target="#b80">[81]</ref> outperformed 3D CNN+stacked 3D AEs <ref type="bibr" target="#b153">[154]</ref>. 3D CNN outperformed 2D CNN <ref type="bibr" target="#b125">[126]</ref> and voxel-based 3D CNN + stacked RNN <ref type="bibr" target="#b55">[56]</ref> outperformed patch-based ensemble of 3D CNNs + 2D CNN <ref type="bibr" target="#b82">[83]</ref>. Nevertheless, slice-based multi-view 2D CNNs + RNN <ref type="bibr" target="#b85">[86]</ref> was reported to have better performance compared with 2D CNN and 3D CNN.</p><p>Regarding input data management methods, ROI-based and patch-based methods are more efficient than others, but multi-view studies in slice-based methods have shown good performance too. Comparisons of these methods are summarized in Section 3.3. According to our review, unsupervised deep models are best utilized to extract features and feed them to a classifier. Among these models, a stacked AE can significantly improve the representational power of highly nonlinear and complex patterns. Another benefit of AEs is that they can find good initialization parameters for CNNs. However, with suitable initialization methods like Xavier <ref type="bibr" target="#b187">[188]</ref> and transfer learning, this advantage of AEs no longer holds. Supervised methods had more popularity in our literature review, enabling feature extraction and classification to be merged into a single model. They were reported <ref type="bibr" target="#b132">[133]</ref> to have better performance compared with AEs when a DPN or a stacked DPN is utilized.</p><p>Compared to SVMs, DNNs are well suited to vector-based problems where the training process is not optimal and the learning process is too slow <ref type="bibr" target="#b38">[39]</ref>.</p><p>Among the supervised methods, the main competition is between 3D CNNs and 2D CNNs (with or without RNNs), which are optimized for image-based problems. The former can capture 3D information from the 3D volume of a brain scan and has shown better performance compared with 2D CNNs <ref type="bibr" target="#b125">[126]</ref>. However, the complexity of training is an issue here, although it can be resolved using patch-based or ROI-based methods instead of voxel-based ones. On the other hand, 2D CNNs are easier to train. Nevertheless, a scheme employing 2D CNNs is not efficient in encoding the spatial information of the 3D images due to the absence of kernel sharing across the third dimension <ref type="bibr" target="#b82">[83]</ref>. That is the reason some studies consider all three views of a brain scan or use RNNs after 2D CNN to capture 3D information in adjacent image slices in a sequence of images. The effect of depth in CNNs has been examined by Wang <ref type="bibr" target="#b184">[185]</ref>, and the reported results show that shallow and very deep networks do not necessarily give good results. The strengths and limitations of each deep model are given in Table <ref type="table" target="#tab_5">3</ref>, with more details in <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b152">153,</ref><ref type="bibr" target="#b188">[189]</ref><ref type="bibr" target="#b189">[190]</ref><ref type="bibr" target="#b190">[191]</ref>. The superiority of CNNs in terms of accuracy, sensitivity, and specificity for highly-cited studies and for whole primary studies is reported in Section 7 and Tables <ref type="table" target="#tab_8">4 to 9</ref> of Appendix 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Datasets and software platforms</head><p>Although AD detection is a complex task, researchers do not have to work alone. Different online datasets and software packages are available to assist. Brain image analysis packages such as FreeSurfer 9 , FSL 10 , MIPAV 11 , and SPM 12 provide powerful tools for different automated pre-processing techniques <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b191">192]</ref>, which were explained in Section 3.1. Also, software packages such as MATLAB 13 , Keras 14 , Tensorflow 15 , Theano 16 , Caffe 17 , and Torch 18 are employed to implement deep models <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr" target="#b190">191,</ref><ref type="bibr" target="#b192">193]</ref>. The popularity of each software in our literature review is shown in Figure <ref type="figure" target="#fig_16">8</ref>. In addition, online datasets such as ADNI 19  <ref type="bibr" target="#b193">[194]</ref>,</p><p>available biomarkers such as neuroimaging modalities, genetic and blood information, and clinical and cognitive assessments. Among them all, ADNI is notable for being a longitudinal, multicentre study. It is the most common dataset in our literature review, being used in about 90% of studies by itself or in combination with others. ADNI was launched in 2003 by NIA, NIBIB 23 , FDA 24 , private pharmaceutical companies, and non-profit organizations as a $60 million, 5-year public/private partnership. It was a North American-based study that aimed to recruit 800 adults (about 200 cognitively normal older individuals, 400 people with MCI, and 200 people with early AD) to participate in the research and be followed for 2-3 years. Acquisition of this data is performed according to the ADNI protocol <ref type="bibr" target="#b193">[194]</ref>. ADNI subjects aged from 55 to <ref type="bibr" target="#b89">90</ref>  Another dataset is AIBL, funded by CSIRO 25 , which includes clinical and cognitive assessments, MRI, PET, biospecimen, and dietary/lifestyle assessments. The MIRIAD dataset is a database of MRI brain scans collected from participants at intervals of 2 weeks to 2 years; the study is designed to investigate the feasibility of using MRI for clinical trials of AD treatments. Finally, some studies prefer to employ their own local datasets. Further details about additional datasets and the total number of subjects in each paper are shown in Table <ref type="table" target="#tab_5">3</ref> of Appendix 1. Additional details on datasets and software packages can be found in <ref type="bibr" target="#b19">[20]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Training considerations</head><p>After the above introduction to biomarkers, deep models, and datasets, some concerns about training issues and parameters need to be addressed. Due to the complexity of deep models and neuroimaging modalities, many 23 National Institute of Biomedical Imaging and Bioengineering 24 Food and Drug Administration 25 Commonwealth Scientific and Industrial Research Organisation different parameters are involved. In this section, the considerations for training the parameters are reviewed.</p><p>All the parameter values in this section come from the studies reviewed here and are summarized in Table <ref type="table" target="#tab_8">4</ref>.</p><p>The first important matter is optimization algorithms used for training purposes, such as Stochastic Gradient Descent (SGD), Adam, Adadelta, RMSProp, and Adagrad. Although some studies have done comparisons <ref type="bibr" target="#b106">[107,</ref><ref type="bibr" target="#b184">185]</ref>, the most commonly used one is SGD (with or without momentum), which has been employed in about 60% of studies.</p><p>The second issue involves employing well-designed parameter initialization for deep models. Random initialization (especially close to zero with or without a specific distribution) and Xavier <ref type="bibr" target="#b187">[188]</ref> initialization of weights are the two most common methods, and the latter is frequently used to speed up the training of deeper networks. However, an initialization method first introduced by <ref type="bibr" target="#b197">[198]</ref> has been reported <ref type="bibr" target="#b169">[170]</ref> to be better than Xavier. Also, in transfer learning methods, weights coming from pre-trained networks (especially on ImageNet)</p><p>have recently attracted much attention. Each of these initialization methods was utilized more or less equally in the primary studies of this review.</p><p>Another issue in the process of training is the rate of learning. The base learning rate is usually within the range of 0.000001 to 0.5, with a typical value of 0.001. The momentum specifies the amount of the old weight change, which is added to the current change, with a typical value of 0.9. Training parameters interact, and there is no generally adopted set of values. However, the range and typical values utilized in the primary studies are shown in Table <ref type="table" target="#tab_8">4</ref>, and they can be used as a starting point for future research on AD detection using neuroimaging modalities. The mini-batch size refers to the number of training examples employed in a single iteration, usually within the range of 4 to 256 with a typical value of 64. The mini-batch size can usually be chosen almost arbitrarily, but small values add noise to the gradient and can make convergence harder, while excessively large mini-batch sizes are limited by memory and can cause convergence to a suboptimal local minimum. However, the accuracy of classification performance has been shown to improve by 4% as batch size increases from 4 to 48 <ref type="bibr" target="#b112">[113]</ref>.</p><p>There are some special techniques to improve generalization and robustness, increase learning speed, and reduce overfitting during the training of deep learning models. Batch normalization performs normalization for each mini-batch and then back-propagates the gradients through the normalization parameters <ref type="bibr" target="#b112">[113]</ref>. In about 50% of studies using 2D/3D CNNs, the use of batch normalization improved the speed, performance, and stability of deep models. Drop-out is a well-known regularization technique where some of the nodes are randomly dropped (forced to zero) to improve the generalization capability of a model. Neurons that are dropped out do not contribute to the forward pass and the backpropagation steps. This prevents nodes from coadapting weights, forcing them to act independently and reducing overfitting while also alleviating memory and computational issues <ref type="bibr" target="#b122">[123,</ref><ref type="bibr" target="#b148">149]</ref>. It is also more likely to discover local patterns and structures in the image and can overcome the problem of insufficient samples <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b136">137,</ref><ref type="bibr" target="#b148">149]</ref>. The fraction of deactivated nodes varied from 20% to 80% from one study to another, with a typical value of 50%. This technique has been employed in most of the studies in our review, consistently increasing performance <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b136">137,</ref><ref type="bibr" target="#b169">170,</ref><ref type="bibr" target="#b179">180,</ref><ref type="bibr" target="#b184">185]</ref>. However, it has been reported <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55]</ref> that dropout is useless in a batch-normalized network. Another regularization method, L1/L2 regularization, basically adds a penalty term as model complexity increases. This decreases the importance given to higher terms and steers the model towards a less complex equation, improving performance. The key difference between L1 and L2 regularization is in the penalty term, where the former adds -absolute value‖ as the penalty to the loss function and the latter adds the -squared magnitude‖ of coefficients. L1 and L2 are used alone or together in about 25% of studies <ref type="bibr" target="#b169">[170,</ref><ref type="bibr" target="#b179">180]</ref>.</p><p>With any dataset, it is vital to know how many subjects are needed for training. According to the literature, about 60-80% of scans are selected for the training set, and all the others for validation and testing. Apart from that, training data should be randomly chosen to ensure that it has a similar distribution to the original dataset.</p><p>After  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Highlights</head><p>This paper systematically reviews strategies for improving AD detection based on deep learning and neuroimaging modalities (with or without other biomarkers). In this section, the highlights that have emerged from this systematic review are listed.</p><p>First, regarding the classification task, classifying early MCI patients from NCs and predicting MCI conversion to AD are more valuable compared to others. Some related findings, such as the success of deep models compared with traditional machine learning methods, were not reported since many researchers have already discussed them. Similarly, longitudinal studies are known to be more sensitive to early disease-related changes in the brain, giving more accurate diagnosis <ref type="bibr" target="#b87">[88]</ref>.</p><p>A key finding is the crucial role of pre-processing of brain scans. The performance of an AD detection system depends in large measure on the quality of the neuroimaging.  Data augmentation.</p><p>The size of the training dataset is known to have a significant effect on the performance of a classifier on an unseen test set <ref type="bibr" target="#b184">[185]</ref>. The numbers of AD and MCI subjects can be very limited in each dataset, which is inadequate for testing deep models. The situation is worse for multi-modality studies. Therefore, some studies have combined datasets. Although combining different datasets will result in more heterogeneity, it does lead to the creation of a large and robust model for classification and prediction. Another way to solve the limited number of subjects in a dataset is to use data augmentation. Data augmentation is a strategy that increases the diversity of data available for training models, without actually collecting new data. Data augmentation techniques such as reflection, random translation, rotation, noise injection, Gamma correction, blurring, cropping, and scaling have been used, when required, in about 20% of studies to improve classification performance <ref type="bibr" target="#b110">[111,</ref><ref type="bibr" target="#b179">180]</ref>. In addition, longitudinal datasets provide several brain scans per subject at different time points, and while their original purpose was to investigate disease progression, they can also be used in a time-independent way for data augmentation <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b92">93,</ref><ref type="bibr" target="#b109">110,</ref><ref type="bibr" target="#b111">112,</ref><ref type="bibr" target="#b171">172]</ref>. However, adding additional images of the same subjects does not necessarily increase performance compared to increasing the number of subjects <ref type="bibr" target="#b112">[113]</ref>.</p><p>Scans from the same subject should not be used in both the training and test sets. Ignoring this factor leads to an -information leak‖ and overfitting to the individual patient instead of learning the general disease pattern, and this causes over-optimistic test results <ref type="bibr" target="#b106">[107,</ref><ref type="bibr" target="#b112">113,</ref><ref type="bibr" target="#b147">148,</ref><ref type="bibr" target="#b174">175]</ref>. While some studies explicitly avoid the problem by using only one image per subject, or use a correct train/test split <ref type="bibr" target="#b106">[107,</ref><ref type="bibr" target="#b147">148,</ref><ref type="bibr" target="#b176">177]</ref>, others did not. We conclude that the training and test sets must be augmented independently <ref type="bibr" target="#b128">[129]</ref>, even though some studies prefer not to use any augmentation at all <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b82">83]</ref>.</p><p> A balanced dataset is recommended.</p><p>Another issue is class imbalance (too few subjects in one class compared with others), which can be handled by either a data augmentation method or a reduction in the number of original scans from the over-sampled class <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b126">127]</ref>. Results from both balanced and unbalanced dataset suggest that accuracy slightly changes with the distribution of data in each class. Balancing the dataset can improve performance even if it makes the dataset smaller <ref type="bibr" target="#b111">[112,</ref><ref type="bibr" target="#b169">170]</ref>.</p><p> The success of CNNs.</p><p>Table <ref type="table" target="#tab_10">5</ref> shows the diversity of methods used in those papers with the highest average number of citations per year (from the year of publication to 2019). Here, the results are for the AD vs. NC classification. All the deep models mentioned are still in use for AD detection. However, the main competition seems to be between 3D CNNs and 2D CNNs (with or without RNNs). To encode the spatial information of 3D images, patch-based or ROI-based 3D CNNs are competing with multi-view slice-based 2D CNNs combined with RNNs <ref type="bibr" target="#b85">[86,</ref><ref type="bibr" target="#b125">126]</ref>. Although training a deep neural network from scratch is done in many studies, it is often not feasible to do so: the training process can take too long, or the dataset is too small <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b122">123,</ref><ref type="bibr" target="#b127">128,</ref><ref type="bibr" target="#b166">167]</ref>. While datasets for general object detection and classification have millions of images, neuroimaging datasets typically have only hundreds of images, which leads to over-fitting during training. Generally, it is helpful to use proven, pretrained CNNs on one dataset for initialization and then re-train them on another dataset using only fine-tuning of the CNNs (transfer learning). This is possible since the lower CNN layers include more general features that can benefit many classification tasks and can be transferred from one application domain to another. Transfer learning is faster and achieves better performance compared to training from scratch, even with distant tasks <ref type="bibr" target="#b110">[111,</ref><ref type="bibr" target="#b112">113,</ref><ref type="bibr" target="#b119">120]</ref>. The first transfer learning approach for AD detection using deep learning was set out in <ref type="bibr" target="#b143">[144]</ref>, which involved, after feature extraction with a sparse AE, a 2D CNN with one convolutional layer and a maxpooling layer, and finally, a neural network with a single hidden layer. It was shown that using natural images to train the AE enhanced classification performance in the following layers. A 3D CNN with three convolutional layers pre-trained with an AE on one MRI dataset and re-trained with another dataset was proposed in <ref type="bibr" target="#b153">[154,</ref><ref type="bibr" target="#b154">155]</ref>, where the generality of the features with the AE pre-trained on the CADDementia dataset was enhanced.</p><p>A 3D CNN with three convolutional layers was used for MRI images <ref type="bibr" target="#b179">[180]</ref>. First, the model was trained on two classes (AD vs. NC), and then a third class (MCI) was added and the weights were fine-tuned to classify the input into three categories. This fine-tuning strategy actually involved transfer learning from the domain of the two-class learned model to a three-class case, which was said to improve performance. Transfer learning was also done in <ref type="bibr" target="#b77">[78]</ref>, where three 2D CNNs with two convolutional layers (one for each view) were trained on MRI images. With a limited amount of DTI images and instead of training from scratch, this work used transfer learning of models that had been trained on the MRI dataset to the target DTI dataset. Finally, the combination of all the networks allowed the final decision to be made using a majority voting strategy. In another example, a CNN model trained on AD vs. NC was used to initialize the parameters of a 3D CNN model of MCIc vs. NC classification, decreasing training time and improving classification performance <ref type="bibr" target="#b82">[83]</ref>. Similarly, a CNN model trained on MCIc vs. NC was also used to initialize the parameters of a 3D CNN model for MCInc vs. NC classification. A simpler approach was used in <ref type="bibr" target="#b91">[92,</ref><ref type="bibr" target="#b129">130]</ref>, where a CNN was initially trained for AD vs. NC classification and then used for MCI conversion prediction. To summarize, although the usefulness and success of transfer learning depend on a similarity between datasets, using models pre-trained on ImageNet for transfer learning significantly increases accuracy compared to training from scratch <ref type="bibr" target="#b110">[111,</ref><ref type="bibr" target="#b112">113,</ref><ref type="bibr" target="#b119">120]</ref>. Even so, there are still points of agreement and disagreement for AD detection on well-known CNN models that had previously been shown to have good performance on ImageNet (see the last paragraph of Section 4.2.3.2 and <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b109">110,</ref><ref type="bibr" target="#b111">112,</ref><ref type="bibr" target="#b119">120,</ref><ref type="bibr" target="#b121">122,</ref><ref type="bibr" target="#b126">127,</ref><ref type="bibr" target="#b128">129,</ref><ref type="bibr" target="#b146">147,</ref><ref type="bibr" target="#b171">172,</ref><ref type="bibr" target="#b182">183,</ref><ref type="bibr" target="#b183">184,</ref><ref type="bibr" target="#b186">187]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Future challenges</head><p>Although deep learning methods have shown noteworthy results, there are still unresolved issues to solve before AD detection in clinical settings can be developed. These issues are mostly about data handling, fusing information from different biomarkers, and datasets. The key challenges are highlighted as follows.</p><p> More investigations are needed in patch-based and ROI-based studies.</p><p>Recognizing ROIs needs expert knowledge, which is still incomplete. Finding discriminative patches is also an issue.</p><p> Finding the optimal combination of different biomarkers is essential.</p><p>One of the most critical issues in multi-modality studies is a way to fuse information from all modalities.</p><p>The easiest way is feature concatenation, where extracted features from all inputs are concatenated and classified. However, directly concatenating data does not consider similar disease patterns in the same region of a brain from all modalities, and may result in an inaccurate detection model. In addition, including other factors such as genetic information is also a challenge.</p><p> Incomplete datasets in multi-modality studies must be resolved.</p><p>Another challenge in multi-modality studies is that the data is usually incomplete, and some modalities might be missing for some subjects. This means that if a single deep model is trained for all modalities, only those subjects with complete multi-modality data (perhaps about 70% of all multi-modality studies) can be used, which limits the scope of a model. To overcome this issue, a three-stage deep feature extraction and fusion framework for MRI, PET, and genetic data has been proposed <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68]</ref>. It begins with feature extraction from each modality, then joins the extracted features, and finally does the classification. In this way, all subjects can be used to train three individual deep learning models for three modalities. Furthermore, it is possible to have different numbers of hidden layers as well as different numbers of hidden neurons in each layer, making it possible to learn the latent representations in each modality and modality combination. A similar approach was reported <ref type="bibr" target="#b134">[135]</ref>, in which complete data was grouped into subsets using different modality combinations. As another possible solution, an image generation task can be formulated with an encoder/decoder deep neural network <ref type="bibr" target="#b100">[101,</ref><ref type="bibr" target="#b198">199]</ref>. This work models the general relationship between MRI and PET to predict missing PET scans from available MRI scans; then the predicted PET modality is concatenated with the MRI modality and used as an input pair for the discriminator network. To summarize, although studies on combining different biomarkers might show promising results, a comprehensive dataset including all these factors is currently not available. Put briefly, multi-modal studies suffer from a lack of generalisability. Whenever possible, the ability to consider all features and modalities is beneficial. More details on multi-modality studies, configurations, and challenges are set out in <ref type="bibr" target="#b199">[200,</ref><ref type="bibr" target="#b200">201]</ref>.</p><p> Data generation needs more investigations. Despite all the efforts to avoid overfitting, such as employing transfer learning and data augmentation, a lack of enough data samples causes major generalization issues. To tackle such problems, generative models can be employed, where data generation means generating new images from already existing images to expand the dataset. It has already been mentioned, for example, that the relationship between MRI and PET can be modeled so as to predict missing PET scans from available MRI scans <ref type="bibr" target="#b100">[101,</ref><ref type="bibr" target="#b198">199]</ref>. However, this area of research needs more work, and the effectiveness of data generation on medical images is still unknown.</p><p> Clear explanations of deep models are demanded. Table <ref type="table" target="#tab_5">3</ref> compares the strengths and limitations of each deep model for classification tasks. As shown in Figure <ref type="figure" target="#fig_12">7</ref>, CNNs are the most widely utilized deep structures. The success of CNNs is clearly shown in Table <ref type="table" target="#tab_10">5</ref>, but there is no clear way to select and design a CNN model for AD detection. This means that the number of convolutional and fully-connected layers, and the combination of all layers, must be done arbitrarily or based on prior experience. At the moment, many CNN models are being used, but researchers have not explained their selection methodology.</p><p> A benchmarking platform should be provided.</p><p>The choice of the dataset is important and can affect the results of the classifier. Since there are different datasets, different numbers of subjects, and even dissimilar subject number codes, comparing various methods is often not possible. Even for studies on the same dataset and with the same number of subjects and subject number code, the results may still not be comparable because a different fraction of subjects may be used as the training set and the test set. Further details about the results of each paper can be seen in Tables <ref type="table" target="#tab_8">4 to 9</ref> in Appendix 1. In these tables, the reported results of each paper for different classification targets are reported.</p><p>The average accuracy is about 92%, 83%, 80%, 79%, and 76% for NC vs. AD, MCI vs. AD, NC vs. MCI, Multi-class (NC, MCI, AD), and MCIc vs. MCInc, respectively, in line with the findings reported by Wang <ref type="bibr" target="#b184">[185]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>AD is one of the leading causes of death, especially in developed countries. Since the early detection of AD is a challenging task in clinics, the use of computer-based systems, together with medical experts, has much to recommend it in detecting AD. For this task, deep learning has attracted strong attention in recent years. In this paper, we have set out how deep learning has enabled the development of AD detection systems. We started this paper with the definition of AD and its symptoms, followed by an explanation of the current criteria for diagnosis and of related biomarkers such as MRI, PET, and fMRI. It is clear that combining these neuroimaging modalities can aid AD detection and can be used with other factors like memory test scores and genetic information to deliver a more accurate diagnosis.</p><p>In terms of pre-processing, intensity normalization and registration to a standard anatomical space is recommended. For image handling, ROI-based and patch-based methods have been reported to be more efficient compared with slice-based and voxel-based ones due to their ability to include only AD-related features in a brain scan. Many deep models have been discussed in this paper. In terms of classification method, CNNs have been used most frequently, with better-reported accuracies in this area compared to other deep models. As the final target for an AD detection system, an automatic multi-modal longitudinal approach is preferred. However, regardless of the final AD detection system, overfitting issues related to the dataset still need to be resolved.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The context here is to see what kind of biomarkers and factors can be used in AD detection, which datasets are available, what kind of pre-processing techniques are needed to deal with biomarkers (especially in neuroimaging), how to extract single features from 3D brain scans, which deep models are capable of capturing disease-related patterns of AD, and how to handle multi-modal data. Typical machine learning methods are composed of three main steps: feature extraction, feature dimension reduction, and classification. Nevertheless, researchers usually combine all these stages when using deep learning techniques. All the papers included in this review can be categorized in terms of inputs, what biomarkers have been used, how biomarkers have been managed, and what deep learning technique was employed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. A typical block diagram of a computer-aided AD detection system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>form for each of the research questions. The data extraction form was compiled when</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Papers using deep learning to detect AD over the years.</figDesc><graphic coords="8,190.01,490.16,240.52,95.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>proceedings covering the field of deep learning. The search terms used were expected to cover most, if not all, of the work incorporating deep learning methods for AD detection. In addition, Google</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>has the most number of citations per year. In this area of research, leading conferences are the IEEE International Symposium on Biomedical Imaging and the International Workshop on Machine Learning in Medical Imaging; and the leading journals are NeuroImage, Medical Image Analysis, and IEEE Journal of Biomedical and Health Informatics. The categories of AD studies based on deep learning fall into the following search terms: Detection or diagnosis. Although most studies work on AD detection, a few studies try to identify the nature of the disease, for example, with ROI extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Distribution of quality scores in primary studies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. (a) The prevalence of single-modality and multi-modality studies; (b) of the single-modality approaches, which neuroimaging modality was used; and (c) frequency of use of grey matter measures (GM) in MRI-based studies. All figures based on our literature review papers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The prevalence of each pre-processing technique in the literature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Bhatkoti et al.<ref type="bibr" target="#b99">[100]</ref> devised a patch-based representation of different brain sub-regions, including left and right hippocampus, mid-occipital, parahippocampus, vermis, and fusiform. Shakeri et al. extracted morphological features as 3D surface meshes from the hippocampus structure of MRIs<ref type="bibr" target="#b56">[57]</ref>. Dolph et al.<ref type="bibr" target="#b47">[48]</ref> extracted Fractal Dimension (FD) texture features, together with volumetric, cortical thickness, and surface area features of the segmented hippocampus, from MRIs and then calculated the statistical properties of the Gray-Level Co-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>goal of this section is to outline fundamental concepts and algorithms of deep learning techniques and their architectures that are found in AD detection. The methods are divided into unsupervised and supervised, which are further separated into Auto-Encoder (AE), Restricted Boltzmann Machine (RBM), Deep Neural Network (DNN), Deep Polynomial Network (DPN), Recurrent Neural Network (RNN), and 2D/3D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. The prevalence of each type of deep model used in AD detection from neuroimaging data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>example, Li et al. used a deep model consisting of a stack of RBMs to extract features in an unsupervised manner<ref type="bibr" target="#b136">[137]</ref>, finishing with a linear SVM for classification. Suk et al. stacked together multiple RBMs to transform the input features of an fMRI into an embedding low-dimensional space by detecting non-linear relations among ROIs<ref type="bibr" target="#b73">[74]</ref>. The Suk team first detected hierarchical non-linear functional relations, and then a Hidden Markov Model (HMM) was used to approximate the likelihood of the input features of the fMRI; for classification, fitting to the corresponding disease status was done with a linear SVM.Like AEs, RBMs can be stacked to construct a deep architecture known as a Deep Belief Network (DBN).The DBN has undirected connections at the top two layers and directed connections at the lower layers. A DBN with three hidden layers for voxel values of MRI (GM tissue) with a linear SVM as the classifier was proposed in<ref type="bibr" target="#b52">[53]</ref>. Ortiz and colleagues used a set of DBNs for all ROIs as feature extractors, with a linear SVM at the final stage for classification of all DBNs<ref type="bibr" target="#b108">[109]</ref>. As pointed out by Guo, despite the benefits, it is computationally expensive to create a DBN model because of the complicated initialization process<ref type="bibr" target="#b152">[153]</ref>. A Deep Boltzmann Machine (DBM) is also constructed by stacking multiple RBMs as building blocks to find a latent hierarchical feature representation. Yet, in contrast to DBNs, all the layers in DBMs form an undirected generative model following the RBM stacking. Although joint optimization is time-consuming in DBMs, and maybe impossible for large datasets, DBMs can deal with ambiguous inputs more robustly by incorporating top-down feedback<ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b152">153]</ref>. Instead of having noisy voxel values, Suk et al. found that a high-level 3D representation obtained via DBM was more robust to noise (and thus helped improve diagnostic performance), whereas a multi-modal DBM of a PET scan derived its features from paired patches of transformed values of GM tissue densities and voxel intensities (a linear SVM was used as the final classifier)<ref type="bibr" target="#b72">[73]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>discrimination between MCIc and MCInc, the network was first trained by AD/NC data and then directly transferred to classify MCIc from MCInc.Focusing on well-known 3D architectures and training from scratch, Karasawa et al. used a 3D CNN based on ResNet<ref type="bibr" target="#b181">[182]</ref>. Similar ones based on VGGNet and ResNet have been proposed<ref type="bibr" target="#b182">[183,</ref><ref type="bibr" target="#b183">184]</ref>. Cheng et al. used a 3D CNN structure inspired by LeNet with four convolutional layers for each image patch<ref type="bibr" target="#b81">[82]</ref>. In this work, each network was individually optimized on each patch, and the features extracted from all CNNs were stacked to form 3D feature maps of the structure. Further, to learn the global features, a deep 3D CNN was constructed at the highest level, followed by a fully-connected layer and a softmax layer for ensemble classification. Tang and co-workers used a 3D CNN based on VGGNet, with an extra shortcut to merge low-level and high-level feature information and alleviate gradient vanishing<ref type="bibr" target="#b125">[126]</ref>. Vu and colleagues used a 3D CNN based on VGGNet pre-trained by a high-level layer concatenation of AE<ref type="bibr" target="#b105">[106]</ref>. To address the problem of limited training data, Wang et al. introduced dense connections to 3D-CNN<ref type="bibr" target="#b184">[185]</ref>, with the dense connections improving information content and the propagation of gradients throughout the network. Senanayake and colleagues used a fusion pipeline in which information from multiple modalities was fused seamlessly through a 3D deep model based on DenseNet<ref type="bibr" target="#b185">[186]</ref>. In similar work, Li et al. constructed multiple 3D DenseNets with the same structure, and then selected the most discriminative DenseNets having high classification accuracy from the validation sets<ref type="bibr" target="#b146">[147]</ref>.Khvostikov  et al. applied a 3D deep model consisting of four sequential combinations of Inception block</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. The frequency with which pre-processing software (a) and deep learning software (b) appears in the literature review.</figDesc><graphic coords="31,309.07,453.67,182.19,110.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>of</cell></row></table><note><p><p><p> RQ5: What datasets and software platforms are applicable in this area? (Section 5 and Table</p>3</p>of Appendix 1)  RQ6: How can training parameters be chosen in the training process? (Section 6)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>A summary of data handling methods for AD detection.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>A summary of each deep model utilized in AD detection.</figDesc><table><row><cell>Models</cell><cell>Strengths</cell><cell>Limitations</cell></row><row><cell></cell><cell>Can represent highly nonlinear and</cell><cell>Learns to capture as much information</cell></row><row><cell></cell><cell>complex patterns</cell><cell>as possible rather than as much relevant</cell></row><row><cell>AE</cell><cell>Good initialization for CNNs</cell><cell>information.</cell></row><row><cell></cell><cell>Good for dimension reduction</cell><cell></cell></row><row><cell></cell><cell>Easy to implement</cell><cell></cell></row><row><cell></cell><cell>Can learn very good generative</cell><cell>Computationally expensive in the training process</cell></row><row><cell>RBM</cell><cell>model Able to create patterns if there</cell><cell></cell></row><row><cell></cell><cell>are missing data</cell><cell></cell></row><row><cell></cell><cell>Good for vector-based problems</cell><cell>Has a slow training process and not optimal for</cell></row><row><cell></cell><cell>Can handle datasets with a large</cell><cell>images</cell></row><row><cell>DNN</cell><cell>number of samples</cell><cell>Has generalization issues</cell></row><row><cell></cell><cell>Can detect complex</cell><cell></cell></row><row><cell></cell><cell>nonlinear relationships</cell><cell></cell></row><row><cell></cell><cell>Can effectively learn feature</cell><cell>Has limited performance due to the simple</cell></row><row><cell>DPN</cell><cell>representation from small samples</cell><cell>concatenation of the learned hierarchical features</cell></row><row><cell></cell><cell></cell><cell>from different layers</cell></row><row><cell>RNN</cell><cell>Good for sequential 2D images Good for longitudinal studies</cell><cell>Has issues related to the training process due to vanishing/exploding gradients</cell></row><row><cell>2D CNN</cell><cell>Good performance in local Easy to train feature extraction in images</cell><cell>Cannot encode the spatial information of the 3D images across the third dimension</cell></row><row><cell>CNN</cell><cell>Good performance in local</cell><cell>Computationally expensive in the training process</cell></row><row><cell>3D</cell><cell>feature extraction in images</cell><cell></cell></row><row><cell>CNN</cell><cell>Can capture 3D information from the</cell><cell></cell></row><row><cell></cell><cell>3D volume of a brain scan</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>have been recruited from over 50 sites across the U.S. and Canada. The primary goal of ADNI is to test whether serial MRI, PET, genetic, biospecimen, and clinical and neuropsychological assessments can be combined to measure the progression of MCI and early AD.OASIS is a project aimed at freely distributing brain MRI data, including two comprehensive datasets. The cross-sectional dataset includes MRI data of 416 subjects (young, middle-aged, non-demented, and demented older adults) aged 18 to 96. The longitudinal dataset includes MRI data of 150 subjects (non-demented and demented older adults) aged 60 to 96.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>training and to evaluate the success of the training procedure, a validation technique like cross-validation is necessary. Cross-validation is a statistical method for evaluating classifiers. The idea behind cross-validation is to use a fraction of the dataset to train the classifier, and then use the remainder as a new and unseen set to test the performance of the classifier. The most well-known method for cross-validation is k-fold. In k-fold, samples are divided into k folds. Subsequently, k iterations of training and validation are performed, so that each fold is used once and only once for validation. In our literature review, k was within the range of 5 to 20, with a typical value of 10 used in about 65% of studies. However, there are two drawbacks in k-fold cross-validation. The first is that the training and testing of the classifier have to be repeated k times, which increases computation time and cost, especially in deep models. Another drawback relates to the number of subjects in the dataset: large values of k result in a limited number of subjects in the validation dataset and eventually cause unreliable results. For this reason, some studies prefer to use the hold-out method (as reported in about 10% of studies) or use smaller values of k.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 .</head><label>4</label><figDesc>A summary of the parameters and techniques described in Section 6.</figDesc><table><row><cell cols="2">Method/Parameter Values/methods</cell><cell>Typical value/method</cell></row><row><cell>Optimization</cell><cell>SGD, Adam, Adadelta, RMSProp, Adagrad</cell><cell>SGD (with or without momentum)</cell></row><row><cell>Initialization</cell><cell>random, Xavier, transfer learning</cell><cell></cell></row><row><cell>Base learning rate</cell><cell>0.000001 -0.5</cell><cell>0.001</cell></row><row><cell>Momentum</cell><cell>0.9</cell><cell>0.9</cell></row><row><cell>Mini-batch size</cell><cell>4 to 256</cell><cell>64</cell></row><row><cell>Drop-out factor</cell><cell>20% to 80%</cell><cell>50%</cell></row><row><cell>Validation</cell><cell cols="2">hold-out, k-fold cross-validation with k:5-20 k-fold cross-validation with k =10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>ROI-based and patch-based methods are more efficient. ROI-based features have low feature dimensions and can be easily interpreted, whereas patch-based methods are sensitive to small abnormal changes in the brain. Both techniques are more efficient compared with slicebased and voxel-based ones. However, multi-view studies in slice-based methods and voxel pre-selection techniques in voxel-based methods deliver comparable performance.  Multi-modality studies outperform single-modality ones.Neuroimaging modalities such as MRI, PET, fMRI, and DTI are fundamental to AD detection. Other factors such as age, gender, educational level, memory test score, and genetic information are also helpful. Although the most discriminative neuroimaging modality is still controversial, combining them is likely to be most effective since it will reflect different aspects of AD and this is especially helpful for early AD detection and predicting conversion from the prodromal stages of the disease. Considering the results of single-modality and multimodality studies, there is a trade-off between higher accuracy and the financial cost of acquiring additional biomarkers. Generally speaking, multi-modality studies achieve better results compared with single-modality studies[56, 67, 69-71, 73, 78, 81, 87, 93, 133, 135, 139, 146, 148], which is expected due to the complexity and heterogeneity of AD.</figDesc><table /><note><p><p>At the very least, intensity normalization and registration need to be done. Other key factors are set out below.</p></p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 .</head><label>5</label><figDesc>A list of papers with the highest average number of citations per year.</figDesc><table><row><cell>Ref.</cell><cell>Data*</cell><cell>Deep model</cell><cell>Dataset</cell><cell cols="3">Results** ACC SEN SPE</cell></row><row><cell>Shi et al. [87]</cell><cell>MRI (GM), PET /R</cell><cell>Multi-modal stacked DPN and a linear kernel SVM</cell><cell>ADNI, 202 subjects</cell><cell cols="3">97.13 95.93 98.53</cell></row><row><cell>Suk et al. [73]</cell><cell>MRI (GM), PET /V+P</cell><cell>Multi-modal DBM with an SVM</cell><cell>ADNI, 398 subjects</cell><cell cols="3">95.35 94.65 95.22</cell></row><row><cell>Suk et al. [69, 71]</cell><cell>MRI (GM), PET, CSF /R</cell><cell>Stacked AEs with a multi-kernel SVM</cell><cell>ADNI, 202 subjects</cell><cell>98.8</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Payan &amp; Montana [149] MRI /V</cell><cell>Sparse AEs and 3D CNN</cell><cell>ADNI, 2265 scans</cell><cell>95.39</cell><cell>-</cell><cell>-</cell></row><row><cell>Liu et al [43, 44]</cell><cell>MRI (GM), PET /R</cell><cell>Stacked sparse AEs and a softmax layer</cell><cell>ADNI, 311 subjects</cell><cell cols="3">91.40 92.32 90.42</cell></row><row><cell>Liu et al. [94]</cell><cell>MRI /P</cell><cell>A 3D CNN models for each landmark with concatenation at final stages</cell><cell>ADNI+MIRIAD, 1526 scans</cell><cell cols="3">91.09 88.05 93.5</cell></row><row><cell>Ortiz et al. [109]</cell><cell>MRI (GM), PET /R</cell><cell>A set of DBNs for all ROIs and an SVM</cell><cell>ADNI, 275 subjects</cell><cell>90</cell><cell>86</cell><cell>94</cell></row><row><cell>Wang et al. [127]</cell><cell>MRI /S</cell><cell>A 2D CNN</cell><cell>OASIS + local data, 196 subjects</cell><cell cols="3">97.65 97.96 97.35</cell></row><row><cell>Suk et al. [55]</cell><cell>MRI (GM) /R</cell><cell>A combination of sparse regression models and a 2D CNN</cell><cell>ADNI, 805 subjects</cell><cell cols="3">91.02 92.72 89.94</cell></row><row><cell>Li et al. [137]</cell><cell>MRI, PET, CSF /R</cell><cell>PCA features, stacked RBMs and a linear kernel SVM</cell><cell>ADNI, 202 subjects</cell><cell>91.4</cell><cell>-</cell><cell>-</cell></row><row><cell>Hosseini-Asl et al. [154, 155]</cell><cell>MRI /V</cell><cell>A 3D CNN pre-trained with stacked 3D convolutional AEs</cell><cell>CADDementia + ADNI, 240 subjects</cell><cell>99.3</cell><cell>100</cell><cell>98.6</cell></row><row><cell>Lu et al. [93]</cell><cell>MRI (GM), PET /P+R</cell><cell>Multimodal and multiscale DNNs consist of 7 DNNs (each DNN: a stacked AE and a softmax layer)</cell><cell>ADNI, 1242 subjects</cell><cell>84.6</cell><cell>80.2</cell><cell>91.8</cell></row><row><cell>Korolev et al. [183]</cell><cell>MRI /V</cell><cell>3D CNN based on ResNet and VGGNet</cell><cell>ADNI, 231 subjects</cell><cell>88</cell><cell>-</cell><cell>-</cell></row><row><cell>Choi &amp; Jin [181]</cell><cell cols="2">FDG-PET, AV-45 PET /V A multi-modal 3D CNN</cell><cell>ADNI, 492 subjects</cell><cell>96</cell><cell>93.5</cell><cell>97.8</cell></row><row><cell>Sarraf &amp; Tofighi [115, 116]</cell><cell>fMRI /S</cell><cell>GoogLeNet and LeNet-5</cell><cell>ADNI, 144 subjects</cell><cell>100</cell><cell>-</cell><cell>-</cell></row><row><cell>Gupta et al. [144]</cell><cell>MRI /P</cell><cell>Sparse AE followed by a CNN and a neural network</cell><cell>ADNI, 755 scans</cell><cell cols="3">94.74 95.24 94.26</cell></row><row><cell cols="3">* S: Slice-based; R: ROI-based; V: Voxel-based; P: Patch-based</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">** ACC: Accuracy; SEN: Sensitivity; SPE: Specificity</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>http://caddementia.grandchallenge.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>The Alzheimer's Disease Prediction Of Longitudinal Evolution, https://tadpole.grand-challenge.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>http://dreamchallenges.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3"><p>https://www.kaggle.com/c/mci-prediction</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22" xml:id="foot_4"><p>Minimal Interval Resonance Imaging in Alzheimer's Disease, see ucl.ac.uk/drc/research/methods/minimalinterval-resonance-imaging-alzheimers-disease-miriad</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of interests</head><p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Accuracy of dementia diagnosis-A direct comparison between radiologists and a computerized method</title>
		<author>
			<persName><forename type="first">S</forename><surname>Klöppel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2969" to="2974" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Alzheimer&apos;s disease facts and figures: Includes a special report on the financial and personal benefits of early diagnosis</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Alzheimer&apos;s Association</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multivariate data analysis and machine learning in Alzheimer&apos;s disease with a focus on structural magnetic resonance imaging</title>
		<author>
			<persName><forename type="first">F</forename><surname>Falahati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Westman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Simmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Alzheimer&apos;s Disease</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="685" to="708" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mild cognitive impairment: Clinical characterization and outcome</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Waring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Ivnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Tangalos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kokmen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Archives of Neurology</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="303" to="308" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Amnestic MCI or prodromal Alzheimer&apos;s disease?</title>
		<author>
			<persName><forename type="first">B</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Lancet Neurology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="246" to="248" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automated cortical thickness measurements from MRI can accurately separate Alzheimer&apos;s patients from normal elderly controls</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Lerch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurobiology of Aging</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="30" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multidimensional classification of hippocampal shape features discriminates Alzheimer&apos;s disease and mild cognitive impairment from normal aging</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gerardin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1476" to="1486" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic classification of MR scans in Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">S</forename><surname>Klöppel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="681" to="689" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Clinical diagnosis of Alzheimer&apos;s disease report of the NINCDS-ADRDA work group under the auspices of department of health and human services task force on Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mckhann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Drachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Folstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Katzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Stadlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurology</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="939" to="939" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Research criteria for the diagnosis of Alzheimer&apos;s disease: Revising the NINCDS-ADRDA criteria</title>
		<author>
			<persName><forename type="first">B</forename><surname>Dubois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Lancet Neurology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="734" to="746" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mild cognitive impairment as a diagnostic entity</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Petersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Internal Medicine</title>
		<imprint>
			<biblScope unit="volume">256</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="183" to="194" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Introduction to the recommendations from the National Institute on Aging-Alzheimer&apos;s Association workgroups on diagnostic guidelines for Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Jack</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Alzheimer&apos;s &amp; Dementia</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="257" to="262" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The diagnosis of dementia due to Alzheimer&apos;s disease: Recommendations from the National Institute on Aging-Alzheimer&apos;s Association workgroups on diagnostic guidelines for Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Mckhann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Alzheimer&apos;s &amp; Dementia</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="263" to="269" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The diagnosis of mild cognitive impairment due to Alzheimer&apos;s disease: Recommendations from the National Institute on Aging-Alzheimer&apos;s Association workgroups on diagnostic guidelines for Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Alzheimer&apos;s &amp; Dementia</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="270" to="279" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imaging brain amyloid in Alzheimer&apos;s disease with Pittsburgh Compound-B</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Klunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Neurology: Official Journal of the American Neurological Association and the Child Neurology Society</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="306" to="319" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Brain beta-amyloid measures and magnetic resonance imaging atrophy both predict time-to-progression from mild cognitive impairment to Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Jack</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3336" to="3348" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Revisiting the framework of the National Institute on Aging-Alzheimer&apos;s Association diagnostic criteria</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Carrillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Alzheimer&apos;s &amp; Dementia</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="594" to="601" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mini-mental state‖: A practical method for grading the cognitive state of patients for the clinician</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Folstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Folstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Mchugh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Psychiatric Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="189" to="198" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The Clinical Dementia Rating (CDR): Current version and scoring rules</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurology</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2412" to="2414" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Quantitative MRI brain studies in mild cognitive impairment and Alzheimer&apos;s disease: A methodological review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Leandrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petroudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Kyriacou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Reyes-Aldasoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Pattichis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Reviews in Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="97" to="111" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Optimizing the diagnosis of early Alzheimer&apos;s disease in mild cognitive impairment subjects</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mattila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Alzheimer&apos;s Disease</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="969" to="979" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Clinico-neuropathological correlation of Alzheimer&apos;s disease in a community-based case series</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Geriatrics Society</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="564" to="569" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Accuracy of clinical criteria for AD in the Honolulu-Asia Aging Study, a population-based study</title>
		<author>
			<persName><forename type="first">H</forename><surname>Petrovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurology</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="226" to="234" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Clinicopathologic correlates in Alzheimer disease: assessment of clinical and pathologic diagnostic criteria</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kazee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lapham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hamill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Alzheimer Disease and Associated Disorders</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Standardized evaluation of algorithms for computer-aided diagnosis of dementia based on structural MRI: The CADDementia challenge</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Bron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="562" to="579" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The global prevalence of dementia: A systematic review and metaanalysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bryce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Albanese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wimo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Ferri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Alzheimer&apos;s &amp; Dementia</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="75" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m">Causes of death, Australia</title>
		<imprint>
			<publisher>Australian Bureau of Statistics</publisher>
			<date type="published" when="2015">2015. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Monetary costs of dementia in the United States</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hurd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Martorell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Delavande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Mullen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Langa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New England Journal of Medicine</title>
		<imprint>
			<biblScope unit="volume">368</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1326" to="1334" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Alzheimer&apos;s disease: Clinical trials and drug development</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mangialasche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Winblad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mecocci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kivipelto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Lancet Neurology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="702" to="716" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">World Alzheimer report 2011: The benefits of early diagnosis and intervention</title>
		<author>
			<persName><forename type="first">M</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bryce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ferri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Alzheimer. s Disease International</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Battle against Alzheimer&apos;s disease: The scope and potential value of magnetic resonance imaging biomarkers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Paquerault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Academic Radiology</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="509" to="511" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A deep learning pipeline to classify different stages of Alzheimer&apos;s disease from fMRI data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Houghten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)</title>
		<meeting>the IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Early diagnosis of Alzheimer&apos;s disease using machine learning techniques: A review paper</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Usman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management (IC3K)</title>
		<meeting>the 7th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management (IC3K)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="380" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automated identification of dementia using medical imaging: A survey from a pattern classification perspective</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Informatics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="27" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automatic classification of patients with Alzheimer&apos;s disease from structural MRI: A comparison of ten methods using the ADNI database</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cuingnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="766" to="781" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">TADPOLE Challenge: Prediction of longitudinal evolution in Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Marinescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.03909</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Crowdsourced estimation of cognitive decline and resilience in Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Alzheimer&apos;s &amp; Dementia</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="645" to="653" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A machine learning neuroimaging challenge for automated diagnosis of mild cognitive impairment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sarica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cerasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Quattrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Calhoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroscience Methods</title>
		<imprint>
			<biblScope unit="volume">302</biblScope>
			<biblScope unit="page" from="10" to="13" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep Learning for Medical Image Processing: Overview, Challenges and the Future</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Razzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Naz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zaib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Classification in BioApps</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="323" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep learning applications in medical image analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="9375" to="9389" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep learning in medical image analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-I</forename><surname>Suk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="221" to="248" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A survey on deep learning in medical image analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="60" to="88" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Early diagnosis of Alzheimer&apos;s disease with deep learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pujol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kikinis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 11th International Symposium on Biomedical Imaging (ISBI)</title>
		<meeting>the IEEE 11th International Symposium on Biomedical Imaging (ISBI)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1015" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multimodal neuroimaging feature learning for multiclass diagnosis of Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1132" to="1140" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Alzheimer&apos;s disease detection using sparse autoencoder, scale conjugate gradient and softmax output layer with fine tuning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Machine Learning and Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="17" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Clinical decision support for Alzheimer&apos;s disease based on deep learning and brain network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Communications</title>
		<meeting>the IEEE International Conference on Communications</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Early diagnosis of Alzheimer&apos;s disease based on resting-state brain networks and deep learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB)</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="244" to="257" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep learning of texture and structural features for multiclass Alzheimer&apos;s disease classification</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Dolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shboul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Samad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Iftekharuddin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks (IJCNN)</title>
		<meeting>the International Joint Conference on Neural Networks (IJCNN)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2259" to="2266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multiscale deep neural network based analysis of FDG-PET images for the early diagnosis of Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Popuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balachandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="26" to="34" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Beg, and the Alzheimer&apos;s Disease Neuroimaging Initiative</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Robust deep learning for improved classification of AD/MCI patients</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Thung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Machine Learning in Medical Imaging</title>
		<meeting>the International Workshop on Machine Learning in Medical Imaging</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="240" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Predicting clinical outcomes of Alzheimer&apos;s disease from complex brain networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Advanced Data Mining and Applications</title>
		<meeting>the International Conference on Advanced Data Mining and Applications</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="519" to="525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Applying convolutional neural networks for predetection of Alzheimer&apos;s disease from structural MRI data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gunawardena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rajapakse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kodikara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Mechatronics and Machine Vision in Practice (M2VIP</title>
		<meeting>the 24th International Conference on Mechatronics and Machine Vision in Practice (M2VIP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Structural MRI classification for Alzheimer&apos;s disease detection using deep belief network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Faturrahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Wasito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hanifah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mufidah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Information &amp; Communication Technology and System (ICTS)</title>
		<meeting>the 11th International Conference on Information &amp; Communication Technology and System (ICTS)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="37" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep ensemble sparse regression network for Alzheimer&apos;s disease diagnosis</title>
		<author>
			<persName><forename type="first">H.-I</forename><surname>Suk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Machine Learning in Medical Imaging</title>
		<meeting>the International Workshop on Machine Learning in Medical Imaging</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="113" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep ensemble learning of sparse regression models for brain disease diagnosis</title>
		<author>
			<persName><forename type="first">H.-I</forename><surname>Suk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="101" to="113" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>and the Alzheimer&apos;s Disease Neuroimaging Initiative</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">3D convolutional neural network and stacked bidirectional recurrent neural network for Alzheimer&apos;s disease diagnosis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elazab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Predictive Intelligence in Medicine</title>
		<meeting>the International Workshop on Predictive Intelligence in Medicine</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="138" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep spectral-based shape features for Alzheimer&apos;s disease classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lombaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kadoury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Spectral and Shape Analysis</title>
		<meeting>the International Workshop on Spectral and Shape Analysis</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="15" to="24" />
		</imprint>
	</monogr>
	<note>and the Alzheimer&apos;s Disease Neuroimaging Initiative</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Nonlinear feature transformation and deep fusion for Alzheimer&apos;s disease staging analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Machine Learning in Medical Imaging</title>
		<meeting>the International Workshop on Machine Learning in Medical Imaging</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="304" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Nonlinear feature transformation and deep fusion for Alzheimer&apos;s disease staging analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="487" to="498" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>and the Alzheimer&apos;s Disease Neuroimaging Initiative</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning longitudinal MRI patterns by SICE and deep learning: Assessing the Alzheimer&apos;s disease progression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Munilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Martínez-Murcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Górriz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Medical Image Understanding and Analysis</title>
		<meeting>the Annual Conference on Medical Image Understanding and Analysis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="413" to="424" />
		</imprint>
	</monogr>
	<note>Ramírez, and the Alzheimer&apos;s Disease Neuroimaging Initiative</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Lessons from applying the systematic literature review process within the software engineering domain</title>
		<author>
			<persName><forename type="first">P</forename><surname>Brereton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Kitchenham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Budgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khalil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Systems and Software</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="571" to="583" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Guidelines for performing systematic literature reviews in software engineering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kitchenham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Charters</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>Keele University &amp; University of Durham</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Procedures for performing systematic reviews</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kitchenham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>Keele University &amp; Empirical Software Engineering National ICT Australia Ltd</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Reviewing ensemble classification methods in breast cancer</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hosni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Abnane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Idri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M C</forename><surname>De Gea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L F</forename><surname>Alemán</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods and Programs in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="page" from="89" to="112" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Avatars and embodied agents in experimental information systems research: A systematic review and conceptual framework</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Aljaroodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Teubner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Australasian Journal of Information Systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Comparison of different MRI brain atrophy rate measures with clinical disease progression in AD</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurology</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="591" to="600" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Feature learning and fusion of multimodality neuroimaging and genetic data for multi-status Dementia diagnosis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Thung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Machine Learning in Medical Imaging</title>
		<meeting>the International Workshop on Machine Learning in Medical Imaging</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="132" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Effective feature learning and fusion of multimodality data using stage-wise deep neural network for dementia diagnosis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Thung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Brain Mapping</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1001" to="1016" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Deep learning-based feature representation for AD/MCI classification</title>
		<author>
			<persName><forename type="first">H.-I</forename><surname>Suk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting>the International Conference on Medical Image Computing and Computer-Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="583" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deep sparse multi-task learning for feature selection in Alzheimer&apos;s disease diagnosis</title>
		<author>
			<persName><forename type="first">H.-I</forename><surname>Suk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Structure and Function</title>
		<imprint>
			<biblScope unit="volume">221</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2569" to="2587" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Shen, and the Alzheimer&apos;s Disease Neuroimaging Initiative</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Latent feature representation with stacked auto-encoder for AD/MCI diagnosis</title>
		<author>
			<persName><forename type="first">H.-I</forename><surname>Suk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Structure and Function</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="841" to="859" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Shen, and the Alzheimer&apos;s Disease Neuroimaging Initiative</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Regional abnormality representation learning in structural MRI for AD/MCI diagnosis</title>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-I</forename><surname>Suk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Machine Learning in Medical Imaging</title>
		<meeting>the International Workshop on Machine Learning in Medical Imaging</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="64" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Hierarchical feature representation and multimodal fusion with deep learning for AD/MCI diagnosis</title>
		<author>
			<persName><forename type="first">H.-I</forename><surname>Suk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>and the Alzheimer&apos;s Disease Neuroimaging Initiative</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">State-space model with deep learning for functional dynamics estimation in resting-state fMRI</title>
		<author>
			<persName><forename type="first">H.-I</forename><surname>Suk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Wee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="292" to="307" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Classification of sMRI for Alzheimer&apos;s disease diagnosis with CNN: Single siamese networks with 2D+ϵ approach and fusion on ADNI</title>
		<author>
			<persName><forename type="first">K</forename><surname>Aderghal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Benois-Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Afdel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia Retrieval</title>
		<meeting>the ACM International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="494" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">FuseMe: Classification of sMRI images by fusion of deep CNNs in 2D+ ϵ projections</title>
		<author>
			<persName><forename type="first">K</forename><surname>Aderghal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Benois-Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Afdel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gwenaëlle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Workshop on Content-Based Multimedia Indexing</title>
		<meeting>the 15th International Workshop on Content-Based Multimedia Indexing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Classification of sMRI for AD diagnosis with convolutional neuronal networks: A pilot 2D+ϵ study on ADNI</title>
		<author>
			<persName><forename type="first">K</forename><surname>Aderghal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boissenin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Benois-Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Catheline</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Afdel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Multimedia Modeling</title>
		<meeting>the International Conference on Multimedia Modeling</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="690" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Classification of Alzheimer disease on imaging modalities with deep CNNs using cross-modal transfer learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Aderghal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khvostikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Benois-Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Afdel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Catheline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 31st International Symposium on Computer-Based Medical Systems (CBMS)</title>
		<meeting>the IEEE 31st International Symposium on Computer-Based Medical Systems (CBMS)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="345" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">3D CNN-based classification using sMRI and MD-DTI images for Alzheimer disease studies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khvostikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aderghal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Benois-Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Catheline</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.05968</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">3D Inception-based CNN with sMRI and MD-DTI data fusion for Alzheimer&apos;s disease diagnostics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khvostikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aderghal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Catheline</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Benois-Pineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03972</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">CNNs based multi-modality classification for AD diagnosis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Congress on Image and Signal Processing</title>
		<meeting>the 10th International Congress on Image and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
		<respStmt>
			<orgName>CISP-BMEI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Classification of Alzheimer&apos;s disease by cascaded convolutional neural networks using PET images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Machine Learning in Medical Imaging</title>
		<meeting>the International Workshop on Machine Learning in Medical Imaging</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="106" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Multimodality cascaded convolutional neural networks for Alzheimer&apos;s disease diagnosis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="295" to="308" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Alzheimer&apos;s Disease Neuroimaging Initiative</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Alzheimer&apos;s disease classification based on combination of multi-model convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Imaging Systems and Techniques (IST)</title>
		<meeting>the IEEE International Conference on Imaging Systems and Techniques (IST)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Combining convolutional and recurrent neural networks for Alzheimer&apos;s disease diagnosis using PET images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Imaging Systems and Techniques (IST)</title>
		<meeting>the IEEE International Conference on Imaging Systems and Techniques (IST)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Classification of Alzheimer&apos;s disease by combination of convolutional and recurrent neural networks using FDG-PET images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroinformatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2018">2018</date>
			<pubPlace>Art</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Multimodal neuroimaging feature learning with multimodal stacked deep polynomial networks for diagnosis of Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="173" to="183" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Longitudinal analysis for Alzheimer&apos;s disease diagnosis using RNN</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 15th International Symposium on Biomedical Imaging (ISBI)</title>
		<meeting>the IEEE 15th International Symposium on Biomedical Imaging (ISBI)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1398" to="1401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Predicting Alzheimer&apos;s disease progression using multi-modal deep learning approach</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-A</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1952">2019. 1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Systematic literature reviews in software engineering-A systematic literature review</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kitchenham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">P</forename><surname>Brereton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Budgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Linkman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Software Technology</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="15" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Trust in virtual teams: A multidisciplinary review and integration</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Hacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Thayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Australasian Journal of Information Systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Hierarchical fully convolutional network for joint atrophy localization and Alzheimer&apos;s disease diagnosis using structural MRI</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Multimodal and multiscale deep neural networks for the early diagnosis of Alzheimer&apos;s disease using structural MR and FDG-PET images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Popuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balachandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Beg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5697</biblScope>
			<date type="published" when="2018">2018</date>
			<pubPlace>Art</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Landmark-based deep multi-instance learning for brain disease diagnosis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="157" to="168" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Anatomical landmark based deep feature representation for MR images in brain disease diagnosis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-T</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1476" to="1485" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Applications of deep learning to MRI images: A survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Big Data Mining and Analytics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Deep learning for brain MRI segmentation: State of the art and future directions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Akkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galimzianova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hoogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Erickson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Digital Imaging</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="449" to="459" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Early identification of Alzheimer&apos;s disease using an ensemble of 3D convolutional neural networks and magnetic resonance imaging</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Brain Inspired Cognitive Systems</title>
		<meeting>the International Conference on Brain Inspired Cognitive Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="303" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Early diagnosis of Alzheimer&apos;s disease by ensemble deep learning using FDG-PET</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Intelligent Science and Big Data Engineering</title>
		<meeting>the International Conference on Intelligent Science and Big Data Engineering</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="614" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Early diagnosis of Alzheimer&apos;s disease: A multi-class deep learning framework with modified k-sparse autoencoder classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bhatkoti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Image and Vision Computing New Zealand (IVCNZ)</title>
		<meeting>the International Conference on Image and Vision Computing New Zealand (IVCNZ)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Deep adversarial learning for multi-modality missing data completion</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1158" to="1166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for the identification of regions of interest in PET scans: A study of representation learning for diagnosing Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karwath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hubrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Artificial Intelligence in Medicine in Europe</title>
		<meeting>the Conference on Artificial Intelligence in Medicine in Europe</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="316" to="321" />
		</imprint>
	</monogr>
	<note>and the Alzheimer&apos;s Disease Neuroimaging Initiative</note>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Deep multi-task multi-channel learning for joint classification and regression of brain status</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting>the International Conference on Medical Image Computing and Computer-Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">A nonparametric method for automatic correction of intensity nonuniformity in MRI data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Sled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Zijdenbos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="97" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Tracking pathophysiological processes in Alzheimer&apos;s disease: an updated hypothetical model of dynamic biomarkers</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Jack</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Lancet Neurology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="216" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Non-white matter tissue extraction and deep convolutional neural network for Alzheimer&apos;s disease detection</title>
		<author>
			<persName><forename type="first">T.-D</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N.-H</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="6825" to="6833" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">The impact of multi-optimizers and data augmentation on TensorFlow convolutional neural network performance</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Taqi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Al-Azzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Milanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)</title>
		<meeting>the IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="140" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Multivariate deep learning classification of Alzheimer&apos;s disease based on hierarchical partner matching independent component analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Aging Neuroscience</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">417</biblScope>
			<date type="published" when="2018">2018</date>
			<pubPlace>Art</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Ensembles of deep learning architectures for the early diagnosis of the Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Munilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Gorriz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramirez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Neural Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">07</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">A deep CNN based multi-class classification of Alzheimer&apos;s disease using MRI</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farooq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rehman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Imaging Systems and Techniques (IST)</title>
		<meeting>the IEEE International Conference on Imaging Systems and Techniques (IST)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Deep residual nets for improved Alzheimer&apos;s diagnosis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Valliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Soni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM International Conference on Bioinformatics</title>
		<meeting>the 8th ACM International Conference on Bioinformatics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="615" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Artificial intelligence based smart diagnosis of Alzheimer&apos;s disease and mild cognitive impairment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farooq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alnowami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Smart International Cities Conference</title>
		<meeting>the Smart International Cities Conference</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Alzheimer classification with MR images: Exploration of CNN performance factors</title>
		<author>
			<persName><forename type="first">V</forename><surname>Wegmayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haziza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1st Conference on Medical Imaging with Deep Learning (MIDL)</title>
		<meeting>1st Conference on Medical Imaging with Deep Learning (MIDL)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">Classification of Alzheimer&apos;s disease structural MRI data by deep learning convolutional neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sarraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tofighi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06583</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">DeepAD: Alzheimer′s disease classification via deep convolutional neural networks using MRI and fMRI</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sarraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tofighi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="volume">070441</biblScope>
			<biblScope unit="page">70441</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">Classification of Alzheimer&apos;s disease using fMRI data and deep learning convolutional neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sarraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tofighi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08631</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Deep learning-based pipeline to recognize Alzheimer&apos;s disease using fMRI data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sarraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tofighi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Future Technologies Conference</title>
		<meeting>the Future Technologies Conference</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="816" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Fusion of deep learning models of MRI scans, Mini-Mental State Examination, and logical memory test enhances diagnosis of mild cognitive impairment</title>
		<author>
			<persName><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Panagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Au</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">B</forename><surname>Kolachalama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Assessment &amp; Disease Monitoring</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="737" to="749" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Alzheimer&apos;s &amp; Dementia</note>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Automatic Alzheimer&apos;s disease recognition from MRI data using deep learning method</title>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Mathematics and Physics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">09</biblScope>
			<date type="published" when="1892">2017. 1892</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Towards Alzheimer&apos;s disease classification through transfer learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</title>
		<meeting>IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1166" to="1169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Convolutional neural network based Alzheimer&apos;s disease classification from magnetic resonance brain images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hemanth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Systems Research</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="147" to="159" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Discrimination and conversion prediction of mild cognitive impairment using convolutional neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantitative Imaging in Medicine and Surgery</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="992" to="1003" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">DemNet: A convolutional neural network for the detection of Alzheimer&apos;s disease and mild cognitive impairment</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Billones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J L D</forename><surname>Demetria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E D</forename><surname>Hostallero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Region 10 Conference</title>
		<meeting>the IEEE Region 10 Conference</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3724" to="3727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Feature fusion for denoising and sparse autoencoders: Application to neuroimaging data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moussavi-Khalkhali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jamshidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wijemanne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th IEEE International Conference on Machine Learning and Applications</title>
		<meeting>the 15th IEEE International Conference on Machine Learning and Applications</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="605" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for automated diagnosis of Alzheimer&apos;s disease and mild cognitive impairment using 3D brain MRI</title>
		<author>
			<persName><forename type="first">J</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Brain Informatics</title>
		<meeting>the International Conference on Brain Informatics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="359" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">A fast and accurate 3D fine-tuning convolutional neural network for Alzheimer&apos;s disease diagnosis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International CCF Conference on Artificial Intelligence</title>
		<meeting>the International CCF Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="115" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Classification of Alzheimer&apos;s disease based on eight-layer convolutional neural network with leaky rectified linear unit and max Pooling</title>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medical Systems</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Brain disease diagnosis using deep learning features from longitudinal MR images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint International Conference on Web and Big Data Asia-Pacific Web (APWeb) and Web-Age Information Management (WAIM)</title>
		<meeting>the Joint International Conference on Web and Big Data Asia-Pacific Web (APWeb) and Web-Age Information Management (WAIM)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="327" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Brain MRI analysis for Alzheimer&apos;s disease diagnosis using an ensemble system of deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Informatics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Convolutional neural networks-based MRI image analysis for the Alzheimer&apos;s disease prediction from mild cognitive impairment</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">777</biblScope>
			<date type="published" when="2018">2018</date>
			<pubPlace>Art</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tzourio-Mazoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="273" to="289" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">3D anatomical atlas of the human brain</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Kabani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">S717</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Multi-modality stacked deep polynomial network based feature learning for Alzheimer&apos;s disease diagnosis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 13th International Symposium on Biomedical Imaging (ISBI)</title>
		<meeting>the IEEE 13th International Symposium on Biomedical Imaging (ISBI)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="851" to="854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Identification of Alzheimer&apos;s disease and mild cognitive impairment using multimodal sparse hierarchical extreme learning machine</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Brain Mapping</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3728" to="3741" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Multi-stage diagnosis of Alzheimer&apos;s disease with incomplete multimodal data via multi-task deep learning</title>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Thung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-T</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="160" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Multi-phase feature representation learning for neurodegenerative disease diagnosis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pujol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kikinis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Australasian Conference on Artificial Life and Computational Intelligence</title>
		<meeting>the Australasian Conference on Artificial Life and Computational Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="350" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">A robust deep model for improved classification of AD/MCI patients</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Thung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1610" to="1616" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Noisy deep dictionary learning: Application to Alzheimer&apos;s disease classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Singhal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks (IJCNN)</title>
		<meeting>the International Joint Conference on Neural Networks (IJCNN)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2679" to="2683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">A novel multimodal MRI analysis for Alzheimer&apos;s disease based on convolutional neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
		<meeting>the 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="754" to="757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">MRIbased feature extraction using supervised general stochastic networks in dementia diagnosis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Collazos-Huertas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tobar-Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cárdenas-Peña</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Castellanos-Dominguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Work-Conference on the Interplay Between Natural and Artificial Computation</title>
		<meeting>the International Work-Conference on the Interplay Between Natural and Artificial Computation</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="363" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Hippocampus analysis based on 3D CNN for Alzheimer&apos;s disease diagnosis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Digital Image Processing</title>
		<meeting>the 10th International Conference on Digital Image Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">10806</biblScope>
			<biblScope unit="page">108065</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Deep learning reveals Alzheimer&apos;s disease onset in MCI subjects: Results from an international challenge</title>
		<author>
			<persName><forename type="first">N</forename><surname>Amoroso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience Methods</title>
		<imprint>
			<biblScope unit="volume">302</biblScope>
			<biblScope unit="page" from="3" to="9" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">A novel convolutional neural network model based on voxel-based morphometry of imaging data in predicting the prognosis of patients with mild cognitive impairment</title>
		<author>
			<persName><forename type="first">F</forename><surname>Çitak-Er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goularas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ormeci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neurological Sciences</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="52" to="69" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Natural image bases to represent neuroimaging data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ayhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="987" to="994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Classification of MR brain images by combination of multi-CNNs for AD diagnosis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Digital Image Processing</title>
		<meeting>the 9th International Conference on Digital Image Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10420</biblScope>
			<biblScope unit="page">1042042</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Multimodal learning using convolution neural network and sparse autoencoder</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-R</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Big Data and Smart Computing</title>
		<meeting>the IEEE International Conference on Big Data and Smart Computing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="309" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Alzheimer&apos;s disease diagnosis based on multiple cluster dense convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="101" to="110" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>and the Alzheimer&apos;s Disease Neuroimaging Initiative</note>
</biblStruct>

<biblStruct xml:id="b147">
	<monogr>
		<title level="m" type="main">Neuroimaging modality fusion in Alzheimer&apos;s classification using convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Punjabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Martersteck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Parrish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05105</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b148">
	<monogr>
		<title level="m" type="main">Predicting Alzheimer&apos;s disease: A neuroimaging study with 3D convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Payan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montana</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02506</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Automatic detection and classification of Alzheimer&apos;s disease from MRI using TANNN</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Seddik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Haggag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Applications</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="30" to="34" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Multi-method analysis of MRI images in early diagnostics of Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wolz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS one</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">e25446</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">COMPARE: Classification of morphological patterns using adaptive regional elements</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="93" to="105" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Deep learning for visual understanding: A review</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oerlemans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">187</biblScope>
			<biblScope unit="page" from="27" to="48" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Alzheimer&apos;s disease diagnostics by adaptation of 3D convolutional network</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hosseini-Asl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Keynto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>El-Baz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="126" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<monogr>
		<title level="m" type="main">Alzheimer&apos;s disease diagnostics by a deeply supervised adaptable 3D convolutional network</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hosseini-Asl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gimel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>El-Baz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.00556</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b155">
	<monogr>
		<title level="m" type="main">An algorithm for training polynomial networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Livni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1304.7045</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Multimedia</title>
		<meeting>the 22nd ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Association for the Advancement of Artificial Intelligence Conference on Artificial Intelligence (AAAI) 2017</title>
		<meeting>the 31st Association for the Advancement of Artificial Intelligence Conference on Artificial Intelligence (AAAI) 2017</meeting>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<monogr>
		<title level="m" type="main">A review on deep learning techniques applied to semantic segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Orts-Escolano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Villena-Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Garcia-Rodriguez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06857</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Is robustness the cost of accuracy? A comprehensive study on the robustness of 18 deep image classification models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="631" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Advances in deep learning approaches for image tagging</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">APSIPA Transactions on Signal and Information Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Exploring Alzheimer&apos;s anatomical patterns through convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Ortiz-Suárez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramos-Pollán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Symposium on Medical Information Processing and Analysis</title>
		<meeting>the 12th International Symposium on Medical Information Processing and Analysis</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10160</biblScope>
			<biblScope unit="page">10160</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<monogr>
		<title level="m" type="main">Detection of Alzheimers disease from MRI using convolutional neural network with Tensorflow</title>
		<author>
			<persName><forename type="first">G</forename><surname>Awate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bangare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pradeepini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Patil</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.10170</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">A novel deep learning based multi-class classification method for Alzheimer&apos;s disease detection using brain MRI data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Brain Informatics</title>
		<meeting>the International Conference on Brain Informatics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="213" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">An ensemble of deep convolutional neural networks for Alzheimer&apos;s disease detection and classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01675</idno>
	</analytic>
	<monogr>
		<title level="m">NIPS 2017 Workshop on Machine Learning for Health</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Automated classification of Alzheimer&apos;s disease and mild cognitive impairment using a single MRI and deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Basaia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage: Clinical</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">101645</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">An efficient 3D deep convolutional network for Alzheimer&apos;s disease diagnosis using MR images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bäckström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nazari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">Y</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Jakola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 15th International Symposium on Biomedical Imaging (ISBI)</title>
		<meeting>the IEEE 15th International Symposium on Biomedical Imaging (ISBI)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="149" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Shearlet based stacked convolutional network for multiclass diagnosis of Alzheimer&apos;s disease using the Florbetapir PET Amyloid imaging data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jabason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Swamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th IEEE International New Circuits and Systems Conference (NEWCAS)</title>
		<meeting>the 16th IEEE International New Circuits and Systems Conference (NEWCAS)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="344" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Classification of brain MRI with big data and deep 3D convolutional neural networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Wegmayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aitharaju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SPIE Medical Imaging</title>
		<meeting>the SPIE Medical Imaging</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">10575</biblScope>
			<biblScope unit="page">10575</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">A multi-modal convolutional neural network framework for the prediction of Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Spasov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Passamonti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Duggento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Toschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
		<meeting>the 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1271" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">A parameter-efficient deep learning approach to predict conversion from mild cognitive impairment to Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">S</forename><surname>Spasov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Passamonti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Duggento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">189</biblScope>
			<biblScope unit="page" from="276" to="287" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Toschi, and the Alzheimer&apos;s Disease Neuroimaging Initiative</note>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">End-to-end Alzheimer&apos;s disease diagnosis and biomarker identification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Esmaeilzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Belivanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Pohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Machine Learning in Medical Imaging</title>
		<meeting>the International Workshop on Machine Learning in Medical Imaging</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="337" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Predicting cognitive decline with deep learning of brain metabolism and amyloid imaging</title>
		<author>
			<persName><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioural Brain Research</title>
		<imprint>
			<biblScope unit="volume">344</biblScope>
			<biblScope unit="page" from="103" to="109" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>and the Alzheimer&apos;s Disease Neuroimaging Initiative</note>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Deep 3D convolutional neural network architectures for Alzheimer&apos;s disease diagnosis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Karasawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ohwada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Intelligent Information and Database Systems</title>
		<meeting>the Asian Conference on Intelligent Information and Database Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Residual and plain convolutional neural networks for 3D brain MRI classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Korolev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Safiullin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Belyaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dodonova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 14th International Symposium on Biomedical Imaging (ISBI)</title>
		<meeting>the IEEE 14th International Symposium on Biomedical Imaging (ISBI)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="835" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Visual explanations from deep 3D convolutional neural networks for Alzheimer&apos;s disease classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rangarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ranka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AMIA Annual Symposium</title>
		<meeting>the AMIA Annual Symposium</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1571" to="1580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Ensemble of 3D densely connected convolutional network for diagnosis of mild cognitive impairment and Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">333</biblScope>
			<biblScope unit="page" from="145" to="156" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Deep fusion pipeline for mild cognitive impairment diagnosis</title>
		<author>
			<persName><forename type="first">U</forename><surname>Senanayake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sowmya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dawes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 15th International Symposium on Biomedical Imaging (ISBI)</title>
		<meeting>the IEEE 15th International Symposium on Biomedical Imaging (ISBI)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1394" to="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Transfer learning for Alzheimer&apos;s disease detection on MRI images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ebrahimi-Ghahnavieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology (IAICT)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="133" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 13th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">A review on the application of deep learning in system health management</title>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yairi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mechanical Systems and Signal Processing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="241" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">Deep learning for healthcare: Review, opportunities and challenges</title>
		<author>
			<persName><forename type="first">R</forename><surname>Miotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Dudley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1236" to="1246" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Deep learning for health informatics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ravì</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="21" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Efficient morphometric techniques in Alzheimer&apos;s disease detection: Survey and tools</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vinutha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Venugopal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroscience International</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="19" to="44" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Deep learning for human affect recognition: Insights and new developments</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Rouast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">The Alzheimer&apos;s disease neuroimaging initiative (ADNI): MRI methods</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Jack</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Magnetic Resonance Imaging: An Official Journal of the International Society for Magnetic Resonance in Medicine</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="685" to="691" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">The Australian Imaging, Biomarkers and Lifestyle (AIBL) study of aging: methodology and baseline characteristics of 1112 individuals recruited for a longitudinal study of Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Psychogeriatrics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="672" to="687" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">Open Access Series of Imaging Studies (OASIS): Cross-sectional MRI data in young, middle aged, nondemented, and demented older adults</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Csernansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Buckner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1498" to="1507" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">MIRIAD-Public release of a multiple time point Alzheimer&apos;s MR imaging dataset</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">B</forename><surname>Malone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="33" to="36" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Deep learning based imaging data completion for improved brain disease diagnosis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting>the International Conference on Medical Image Computing and Computer-Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="305" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Deep multimodal learning: A survey on recent advances and trends</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ramachandram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="96" to="108" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">PET/MRI hybrid systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Mannheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seminars in Nuclear Medicine</title>
		<meeting>the Seminars in Nuclear Medicine</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="332" to="347" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
