<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ALP-KD: Attention-Based Layer Projection for Knowledge Distillation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Peyman</forename><surname>Passban</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yimeng</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mehdi</forename><surname>Rezagholizadeh</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ALP-KD: Attention-Based Layer Projection for Knowledge Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge distillation is considered as a training and compression strategy in which two neural networks, namely a teacher and a student, are coupled together during training. The teacher network is supposed to be a trustworthy predictor and the student tries to mimic its predictions. Usually, a student with a lighter architecture is selected so we can achieve compression and yet deliver high-quality results. In such a setting, distillation only happens for final predictions whereas the student could also benefit from teacher's supervision for internal components. Motivated by this, we studied the problem of distillation for intermediate layers. Since there might not be a one-to-one alignment between student and teacher layers, existing techniques skip some teacher layers and only distill from a subset of them. This shortcoming directly impacts quality, so we instead propose a combinatorial technique which relies on attention. Our model fuses teacher-side information and takes each layer's significance into consideration, then it performs distillation between combined teacher layers and those of the student. Using our technique, we distilled a 12-layer BERT (Devlin et al. 2019) into 6-, 4-, and 2-layer counterparts and evaluated them on GLUE tasks <ref type="bibr" target="#b23">(Wang et al. 2018)</ref>. Experimental results show that our combinatorial approach is able to outperform other existing techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Knowledge distillation (KD) <ref type="bibr" target="#b5">(Buciluǎ, Caruana, and Niculescu-Mizil 2006;</ref><ref type="bibr" target="#b13">Hinton, Vinyals, and Dean 2015)</ref> is a commonly-used technique to reduce the size of large neural networks <ref type="bibr" target="#b18">(Sanh et al. 2019)</ref>. Apart from this, we also consider it as a complementary and generic add-on to enrich the training process of any neural model <ref type="bibr" target="#b11">(Furlanello et al. 2018)</ref>.</p><p>In KD, a student network (S) is glued to a powerful teacher (T ) during training. These two networks can be trained simultaneously or T can be a pre-trained model. Usually, T uses more parameters than S for the same task, therefore it has a higher learning capacity and is expected to provide reliable predictions. On the other side, S follows its teacher with a simpler architecture. For a given input, both models provide predictions where those of the student are penalized by an ordinary loss function (using hard labels) as well as predictions received from T (also known as soft labels).</p><p>Training a (student) model for a natural language processing (NLP) task can be formalized as a multi-class classification problem to minimize a cross-entropy (ce) loss function, as shown in Equation <ref type="formula">1</ref>:</p><formula xml:id="formula_0">L ce = N X i=1 X w2V [ (y i = w)⇥ log p S (y i = w|x i , ✓ S )] (1)</formula><p>where (.) is the indicator function, V is a vocabulary set (or different classes in a multi-class problem), N is the number of tokens in an input sequence, and y is a prediction of the network S with a parameter set ✓ S given an input x.</p><p>To incorporate teacher's supervision, KD accompanies L ce with an auxiliary loss term, L KD , as shown in Equation <ref type="formula">2</ref>:</p><formula xml:id="formula_1">L KD = N X i=1 X w2V [p T (y i = w|x i , ✓ T )⇥ log p S (y i = w|x i , ✓ S )] (2)</formula><p>Since S is trained to behave identically to T , model compression can be achieved if it uses a simpler architecture than its teacher. However, if these two models are the same size KD would still be beneficial. What L KD proposes is an ensemble technique by which the student is informed about teacher's predictions. The teacher has better judgements and this helps the student learn how much it deviates from true labels.</p><p>This form of KD that is referred to as Regular KD (RKD) throughout this paper, only provides S with external supervision for final predictions, but this can be extended to other components such as intermediate layers too. The student needs to be aware of the information flow inside teacher's layers and this becomes even more crucial when distilling from deep teachers. Different alternatives have been proposed to this end, which compare networks' internal layers in addition to final predictions <ref type="bibr" target="#b15">(Jiao et al. 2019;</ref><ref type="bibr" target="#b21">Sun et al. 2020</ref><ref type="bibr">Sun et al. , 2019))</ref>, but they also suffer from other types of problems. The main goal in this paper is to study such models and address their shortcomings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Definition</head><p>To utilize intermediate layers' information (and other components in general), a family of models exists that defines a dedicated loss function to measure how much a student diverges from its teacher in terms of internal representations. In particular, if the goal is to distill from an n-layer teacher into an m-layer student, a subset of m (out of n) teacher layers is selected whose outputs are compared to those of student layers (see Equation 3 for more details). Figure <ref type="figure" target="#fig_0">1</ref> illustrates this concept. As the figure shows, each student layer is connected to a single, dedicated peer on the teacher side, e.g. the n-th teacher layer corresponds to the m-th student layer. Since outputs of these two layers are compared to each other, we hope that both models generate as similar outputs as possible at points n and m. With this simple technique, teacher's knowledge can be used to supervise student's intermediate layers.</p><p>Experimental results show that intermediate layer matching could be quite effective, but in our study we realized that it may suffer from two shortcomings:</p><p>• If n m, multiple layers in T have to be ignored for distillation but we know that those layers consist of precious information for which we spend expensive resources to learn. This issue is referred to as the skip problem in this paper.</p><p>• Moreover, it seems the way teacher layers are kept/skipped is somewhat arbitrary as there is no particular strategy behind it. Before training, we lack enough knowledge to judge which subset of teacher layers contributes more to the distillation process, so there is a good chance of skipping significant layers if we pick them in an arbitrary fashion. Finding the best subset of layers to distill from requires an exhaustive search or an expert in the field to signify connections. We refer to this issue as the search problem.</p><p>In order to resolve the aforementioned issues we propose an alternative, which is the main contribution of this paper. Our solution does not skip any layer but utilizes all information stored inside T . Furthermore, it combines teacher layers through an attention mechanism, so there is no need to deal with the search problem. We believe that the new notion of combination defined in this paper is as important as our novel KD architecture and can be adapted to other tasks too.</p><p>The remainder of this paper is organized as follows: First, we briefly review KD techniques used in similar NLP applications, then we introduce our methodology and explain how it addresses existing shortcomings. We accompany our methodology with experimental results to show whether the proposed technique is useful. Finally, we conclude the paper and discuss future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>KD was originally proposed for tasks other than NLP <ref type="bibr" target="#b5">(Buciluǎ, Caruana, and Niculescu-Mizil 2006;</ref><ref type="bibr" target="#b13">Hinton, Vinyals, and Dean 2015)</ref>. Kim and Rush (2016) adapted the idea and proposed a sequence-level extension for machine translation. Freitag, Al-Onaizan, and Sankaran (2017) took a step further and expanded it to a multi-task scenario. Recently, with the emergence of large NLP and language understanding (NLU) models such as <ref type="bibr">ELMO (Peters et al. 2018)</ref> and BERT <ref type="bibr" target="#b8">(Devlin et al. 2019</ref>) KD has gained extra attention. Deep models can be trained in a better fashion and compressed via KD, which is favorable in many ways. Therefore, a large body of work in the field such as Patient KD (PKD) <ref type="bibr">(Sun et al. 2019</ref>) has been devoted to compressing/distilling BERT (and similar) models.</p><p>PKD is directly related to this work, so we discuss it in more detail. It proposes a mechanism to match teacher and student models' intermediate layers by defining a third loss function, L P , in addition to L ce and L KD , as shown in Equation 3:</p><formula xml:id="formula_2">L P = N X i=1 m X j=1 || h i,j S ||h i,j S || 2 A(j) i ||A(j) i || 2 || 2 2 (3)</formula><p>where h i,j S is the output<ref type="foot" target="#foot_0">1</ref> of the j-th student layer for the i-th input. A subset of teacher layers selected for distillation is denoted with an alignment function A, e.g. A(j) = h l T implies that the output of the j-th student layer should be compared to the output of the l-th teacher layer (h i,j S $ h i,l T ). PKD is not the only model that utilizes internal layers' information. Other models such as TinyBERT <ref type="bibr" target="#b15">(Jiao et al. 2019)</ref> and MobileBERT <ref type="bibr" target="#b21">(Sun et al. 2020</ref>) also found it crucial for training competitive student models. However, as Equation <ref type="formula">3</ref>shows, in these models only m teacher layers (the number of teacher layers returned by A) can contribute to distillation. In the presence of deep teachers and small students, this limitation can introduce a significant amount of information loss. Furthermore, what is denoted by A directly impacts quality. If A skips an important layer the student model may fail to provide high-quality results.</p><p>To tackle this problem, Wu et al. <ref type="bibr" target="#b1">(2020)</ref> proposed a combinatorial technique, called CKD. In their model, A(j) returns a subset of teacher layers instead of a single layer. Those layers are combined together and distillation happens between the combination result and the j-th student layer, as shows in equation 4:</p><formula xml:id="formula_3">Ĉj =F c (h k T ); h k T 2 A(j) C j =F r ( Ĉj ) [ m j=1 A(j) ={h 1 T , ..., h n T } (4)</formula><p>where Ĉj is the result of a combination produced by the function F c given a subset of teacher layers indicated by A(j). In Wu et al. <ref type="bibr" target="#b1">(2020)</ref>, F c is implemented via a simple concatenation. Depending on the form of combination used in Equation <ref type="formula">4</ref>, there might be a dimension mismatch between Ĉj and the student layer h j S . Accordingly, there is another function, F r , to reform the combination result into a comparable shape to the student layer. CKD uses a single projection layer to control the dimension mismatch.</p><p>With the combination technique (concatena-tion+projection), CKD could solve the skip problem but the search problem still remains unanswered. Similar to PKD, CKD also requires a search process, but it looks for the best subset of teacher layers instead of the best single layer. These two models are directly related to this research so we consider them as baselines in our experiments.</p><p>The application of KD in NLP and NLU is not limited to the aforementioned models. Aguilar et al. <ref type="bibr" target="#b1">(2020)</ref> followed the same architecture as PKD but they introduced a new training regime, called progressive training. In their method, lower layers are trained first and training is progressively shifted to upper layers. They claim that the way internal layers are trained during KD can play a significant role. <ref type="bibr" target="#b16">Liu et al. (2019)</ref> investigated KD from another perspective. Instead of focusing on the compression aspect, they kept the size of student models equal to their teachers and showed how KD could be treated as a complementary training ingredient.</p><p>Tan et al. ( <ref type="formula">2019</ref>) squeezed multiple translation engines into one transformer <ref type="bibr" target="#b22">(Vaswani et al. 2017</ref>) and showed that knowledge can be distilled from multiple teachers. <ref type="bibr" target="#b25">Wei et al. (2019)</ref> introduced a novel training procedure where there is no need for an external teacher. A student model can learn from its own checkpoints. At each validation step, if the current checkpoint is better than the best existing checkpoint, student learns from it otherwise the best stored checkpoint is considered as a teacher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>For a given student model S and a teacher model T we show all intermediate layers with sets H S = {h 1 S , ..., h m S } and H T = {h 1 T , ..., h n T }, respectively. Based on the pipeline designed by current models for intermediate layer KD, there must be a connection between H S and H T during training and each student layer can only correspond to a single peer on the teacher side. As previously mentioned, layer connections are denoted by A.</p><p>A common heuristic to devise A is to divide teacher layers into m buckets with approximately the same sizes and pick only one layer from each <ref type="bibr" target="#b15">(Jiao et al. 2019;</ref><ref type="bibr">Sun et al. 2019)</ref>. Therefore, for the j-th layer of the student model, A(j) returns a single teacher layer among those that reside in the j-th bucket. Figure <ref type="figure" target="#fig_1">2</ref>.a illustrates this setting. Clearly, this is not the best way of connecting layers, because they are picked in a relatively arbitrary manner. More importantly, no matter what heuristic is used there still remain n m layers in this approach whose information is not used in distillation.</p><p>To address this issue, we simply propose a combinatorial alternative whereby all layers inside buckets are taken into consideration. Our technique is formulated in Equation <ref type="formula">5</ref>:</p><formula xml:id="formula_4">C j = X h k T 2A(j) ↵ jk h k T ↵ jk = exp(h j S . h k T ) P h k 0 T 2A(j) exp(h j S . h k 0 T ) [ m j=1 A(j) = H T = {h 1 T , ..., h n T } (5)</formula><p>This idea is similar to that of CKD, but we use an attention mechanism <ref type="bibr" target="#b2">(Bahdanau, Cho, and Bengio 2014)</ref> instead of a concatenation for layer combination. Experimental results demonstrate that this form of combination is more useful. We refer to this idea as Attention-based Layer Projection for KD or ALP-KD in short.</p><p>According to the equation, if a student layer associates with a particular bucket, all layers inside that bucket are combined/used for distillation and C j is a vector representation of such a combination. Our model benefits from all n teacher layers and skips none as there is a dedicated C vector for each student layer. Figure <ref type="figure" target="#fig_1">2</ref>.b visualizes this setting.</p><p>Weights (↵ values) assigned to teacher layers are learnable parameters whose values are optimized during training. They show the contribution of each layer to the distillation process. They also reflect the correlation between student and teacher layers, i.e. if a student layer correlates more with a set of teacher layers weights connecting them should receive higher values. In other words, that specific layer is playing the role of its teacher peers on the student side. To measure the correlation, we use the dot product in our experiments but any other function for similarity estimation could be used in this regard.</p><p>Equation 5 addresses the skip problem with a better combination mechanism and is able to provide state-of-the-art results. However, it still suffers from the search problem as it relies on buckets and we are not sure which bucketing strategy works better. For example, in Figure <ref type="figure" target="#fig_1">2</ref>.b the first bucket consists of the first three layers of the teacher but it does not mean that we cannot append a fourth layer. In fact, a bucket with four layers might perform better. Buckets can also share layers; namely, a teacher layer can belong to multiple buckets and can be used numerous times in distillation. These constraints make it challenging to decide about buckets and their boundaries, but it is possible to resolve this dilemma through a simple modification in our proposed model.</p><p>To avoid bucketing, we span the attention mask over all teacher layers rather than over buckets. To implement this extension, A(j) needs to be replaced with H T in Equation <ref type="formula">5</ref>. Therefore, for any student layer such as h j S there would be a unique set of n attention weights and C j would be a weighted average of all teacher layers, as shown in Equation <ref type="formula" target="#formula_5">6</ref>:</p><formula xml:id="formula_5">C j = X h k T 2A(j) ↵ jk h k T A(j) =H T 8j 2 {1, 2, ..., m}<label>(6)</label></formula><p>This new configuration, which is illustrated in Figure <ref type="figure" target="#fig_1">2</ref>.c, proposes a straightforward way of combining teacher layers and addresses both skip and search problems at the same time.</p><p>To train our student models, we use a loss function which is composed of L ce , L KD , and a dedicated loss defined for ALP-KD, as shown in Equation <ref type="formula" target="#formula_6">7</ref>:</p><formula xml:id="formula_6">L = L ce + ⌘L KD + L ALP L ALP = N X i=1 m X j=1 MSE(h i,j S , C i,j )<label>(7)</label></formula><p>where MSE() is the mean-square error and C i,j shows the value of C j when the teacher is fed with the i-th input. , ⌘, and are hyper-parameters of our model to minimize the final loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Study</head><p>A common practice in our field to evaluate the quality of a KD technique is to feed T and S models with instances of well-known datasets and measure their performance. We followed the same tradition in this paper and selected a set of eight GLUE tasks <ref type="bibr" target="#b23">(Wang et al. 2018)</ref> including CoLA, MNLI, MRPC, QNLI, QQP, RTE, SST-2, and STS-B datasets to benchmark our models.</p><p>In NLP/NLU settings, T is usually a pre-trained model whose parameters are only fine-tuned during training. On the other side, S can be connected to T to be trained thoroughly or can alternatively be initialized with T 's parameters to be fine-tuned similar to its teacher (which is more common). This helps the student network generate better results and converge faster. Because T is already a pre-trained model and S is initialized with its teacher, the term fine-tuning is more common than training in this context.</p><p>In our experiments, we chose the original BERT<ref type="foot" target="#foot_1">2</ref> (also known as BERT Base ) as our teacher. We are faithful to the experimental setting proposed by <ref type="bibr" target="#b8">Devlin et al. (2019)</ref> for this model. Therefore, our in-house version of it also has 12 layers with 12 attention heads and the hidden and feedforward dimensions are 768 and 3072, respectively. Our students are also BERT models only with fewer layers (|H S | = m ; m &lt; 12). We use the teacher BERT to initialize students but because the number of layers are different (12 6 = m) we only consider the first m layers of the teacher for initialization. We borrowed this idea from PKD <ref type="bibr">(Sun et al. 2019)</ref> in the interest of fair comparisons.</p><p>After initializing students, they are fine-tuned during training. The concept of fine-tuning and its details are comprehensively discussed in <ref type="bibr" target="#b8">Devlin et al. (2019)</ref>, so we skip that part and refer the reader to the original paper. We have the same pipeline for fine-tuning.</p><p>In order to maximize each model's performance we need to decide about the learning rate, batch size, the number of fine-tuning epochs, and , ⌘, and . To this end, we run a grid search similar to <ref type="bibr">Sun et al. (2019)</ref> and Wu et al. <ref type="bibr" target="#b1">(2020)</ref>. For our experiments, the batch size is set to 32 and the learning rate is selected from {1e 5, 2e 5, 5e 5}. ⌘ and take values from the set {0, 0.2, 0.5, 0.7} and = 1 ⌘ . We trained multiple models with different configurations and compared our results to RKD-and PKD-based students. To the best of our knowledge, these are the only alternatives that use BERT as a teacher and their students' architecture relies on ordinary Transformer blocks with the same size as ours, so any comparison to any other model with different settings would not be fair. Moreover, due to CKD's similarity to our approach we re-implemented it for our experiments. The original CKD model was proposed for machine translation and for the first time we evaluate its performance in NLU tasks. Table <ref type="table" target="#tab_0">1</ref>  S RKD is equivalent to a configuration known as DistilBERT in the literature <ref type="bibr" target="#b18">(Sanh et al. 2019)</ref>. To have precise results and a better comparison we trained/fine-tuned all models in the same experimental environment. Accordingly, we did not borrow any result from the literature but reproduced them. This is the reason we use the term equivalent for these two models. Furthermore, DistilBERT has an extra cosine embedding loss in addition to those of S RKD . When investigating the impact of intermediate layers in the context of KD, we wanted L P to be the only difference between RKD and PKD, so incorporating any other factor could hurt our investigation and we thus avoided the cosine embedding loss in our implementation.</p><p>PKD outperforms RKD with an acceptable margin in Table 1 and that is because of the engagement of intermediate layers. For S PKD , we divided teacher layers into 3 buckets (4 layers in each) and picked the first layer of each bucket to connect to student layers, i.e.</p><formula xml:id="formula_7">A(1) = h 1 T , A(2) = h 5 T ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and A(3) = h 9</head><p>T . There is no teacher layer assigned to the last layer of the student. This form of mapping maximizes PKD's performance and we figured out this through an empirical study.</p><p>Results discussed so far demonstrate that cross-model layer mapping is useful, but it can be improved even more if the skip issue is settled. Therefore, we trained two students using CKD. The setting for these models are identical to PKD, namely teacher layers are divided into 3 buckets. The first 4 teacher layers reside in the first bucket. The fifth to eighth layers are in the second bucket and the rest are covered by the third bucket. Layers inside the first bucket are concatenated and passed through a projection layer to match the student layers' dimension. The combination result for the first bucket is assigned to the first student layer (C 1 $ h 1 S ). The same procedure is repeated with the second and third buckets for h 2 S and h 3 S . Similar to PKD, there is no teacher layer connected to the last student layer. This configuration is referred to as No Overlap or NO in short (that indicates buckets share no layers with each other).</p><p>In addition to NO we designed a second configuration, PO, which stands for Partial Overlap. In PO, each bucket shares its first layer with the preceding bucket, so the first bucket includes the first to fifth layers, the second bucket includes the fifth to ninth layers, and from the ninth layer onward reside in the third bucket. We explored this additional configuration to see the impact of different bucketing strategies in CKD.</p><p>Comparing S CKD to S PKD shows that the combination (con-catenation+projection) idea is useful in some cases, but for others the simple skip idea is still better. Even defining different bucketing strategies did not change it drastically, and this leads us to believe that a better form of combination such as an attention-based alternative is required.</p><p>In S ALP extensions, we replace the CKD's concatenation with attention and results improve. ALP-KD is consistently better than all other RKD, PKD, and CKD variations and this justifies the necessity of using attention for combination. S ALP-NO and S ALP-PO also directly support this claim. In S ALP , we followed Equation 6 and spanned the attention mask over all teacher layers. This setting provides a model that requires no engineering adjustments to deal with skip and search problems and yet delivers the best result on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training deeper/shallower models than 4-layer students</head><p>We compared ALP-KD to other models and observed superior results. In this section, we design additional experiments to study its behaviour from different perspectives. The original idea of PKD was proposed to distill from a 12-layer BERT to a 6-layer student <ref type="bibr">(Sun et al. 2019</ref>). In such a scenario, only every other layer of the teacher is skipped and it seems the student model should not suffer from the skip problem dramatically. We repeated this experiment to understand if our combination idea is still useful or its impact diminishes when student and teacher models do not differ much. Table <ref type="table" target="#tab_1">2</ref> summarizes findings of this experiment. Among 6-layer students, S ALP-NO has the best average score which demonstrates that the combinatorial approach is still useful. Moreover, the supremacy of attention-based combination over the simple concatenation holds for this setting too. S ALP is the second best and our favorite model as it requires no layer alignment before training.</p><p>The gap between PKD and ALP-KD is narrowed in 6-layer models compared to 4-layer counterparts, and this might be due to an implicit relation between the size of the student model and need for combining intermediate layers. We focused on this hypothesis in another experiment and this time used the same teacher to train 2-layer students. In this scenario, student models are considerably smaller than the teacher with only 39M parameters. Results of this experiment are reported in Table <ref type="table" target="#tab_3">3</ref>.</p><p>For CKD and ALP-KD, we combine all teacher layers and distill into the first layer of the student. Similar to previous experiments, there is no connection between the last layer of 2-layer students and the teacher model, which means intermediate layer distillation only happens between h 1 S and H T . For PKD, we need to decide which teacher layer should be selected for distillation, for which we tried three configurations and selected the first (h 1 S $ h 1 T ), sixth (h 1 S $ h 6 T ), and twelfth (h 1 S $ h 12 T ) layers. S ALP still outperforms other students with a high margin and this time the gap between PKD and ALP-KD is even more visible. This result points out to the fact that when teacher and student models differ much, intermediate layer combination becomes vital.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Analysis</head><p>As part of our investigation, we tried to visualize attention weights to understand what happens during training and why ALP-KD leads to better results. Figure <ref type="figure" target="#fig_2">3</ref> illustrates this experiment. From the SST-2 dataset, we randomly selected 10 examples and stimulated both teacher and student models and emitted attention weights between the first layer of the student (h 1 S ) and all teacher layers (H T ). We carried out this experiment with all 2-, 4-, and 6-layer S ALP models. The x and y axes in the figure show the attention weights and 10 examples, respectively.</p><p>As Figure <ref type="figure" target="#fig_2">3a</ref> shows, the first half of the teacher model is more active, which is expected as we distill into the first layer T and h 1 S . This visualization demonstrates that all teacher layers participate in distillation and defining buckets or skipping layers might not be the best approach. A similar situation arises when distilling into the 4-layer model (Figure <ref type="figure" target="#fig_2">3b</ref>) as the first half is still more active. For the 6-layer model, we see a different pattern where there is a concentration in attention weights around the middle layers of the teacher and h 1 S is mainly fed by the layers h 4 T to h 7 T . Considering the distribution of attention weights in Figure <ref type="figure" target="#fig_2">3</ref>, any skip-and even concatenation-based approach would fail to reveal the maximum capacity of KD. Such models assume that a single teacher layer or a subset of adjacent layers affect student layers, whereas as we see almost all layers participate in distillation. Apart from previously reported results, this visualization again justifies the need for an attention-based combination in KD.</p><p>Sine our model emphasizes on intermediate layers and the necessity of having similar internal representations between student and teacher models, in addition to attention weights we also visualized the output of intermediate layers. The main idea behind this analysis is to show the information flow inside student models during training and how ALP helps them mimic their teacher. Figures <ref type="figure" target="#fig_4">4a and 4b</ref> illustrate this experiment.</p><p>We selected 100 random samples from the SST-2 dataset and visualized what hidden representations of S ALP , S PKD , and T models (from   Teacher-, ALP-, and PKD-related information is visualized with green, red, and blue colors, respectively. 4a and 4c include information about h 2 ALP , h 2 PKD and h 6 T and 4b and 4d report about h 3 ALP , h 3 PKD and h 9 T . In 4c and 4d, the x axis shows samples and the y axis is the cosine distance from the teacher. these inputs. Student models have 4 layers but due to the space limitation we only visualize the second (Figure <ref type="figure" target="#fig_4">4a</ref>) and third (Figure <ref type="figure" target="#fig_4">4b</ref>) layers' outputs, namely h 2 S and h 3 S . The output of each intermediate layer is a 768-dimensional vector. In order to be able to visualize, we extract the first two principle components of representations using the wellknown PCA algorithm <ref type="bibr">(Wold, Esbensen, and Geladi 1987)</ref>. During training, h 6 T and h 9 T are connected to h 2 S and h 3 S as their source of distillation in PKD, so we also included those teacher layers' outputs in our visualization. As the figure shows, ALP's outputs are closer to those of the teacher and it demonstrates that our technique helps train better students with closer characteristics to their teachers.</p><p>We conducted another complementary experiment where we took the output of the same teacher and student layers used in the previous experiment and measured their distance for all 100 examples. Results of this experiment are illus-trated in Figures <ref type="figure" target="#fig_4">4c and 4d</ref> for the second and third student layers, respectively. As it is seen, the distance of intermediate representations generated by PKD is farther than teachers' compared to ALP representations, e.g. the distance between h 20,2 PKD (the output of the second PKD layer for the 20-th example in Figure <ref type="figure" target="#fig_4">4c</ref>) and h 20,6   T is around 0.20 whereas this number is only 0.05 for ALP. This is an indication that the ALP student follows its teacher better than the PKD one.</p><p>To measure distance we used the Cosine similarity in this experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Work</head><p>In this paper, we discussed the importance of distilling from intermediate layers and proposed an attention-based technique to combine teacher layers without skipping them. Experimental results show that the combination idea is quite effective.</p><p>Our findings in this research can be summarized as follows: • To distil from deep teachers with multiple internal components combination seems essential. • As teacher and student models differ more in terms of the number of layers the role of combination-based models such as ALP-KD becomes more crucial. • Although a simple concatenation of layers would still be better than skipping, to obtain optimal results an attentionbased approach is required. • ALP-KD can be used to combine layers inside buckets and this approach is likely to yield state-of-the-art results, but if there is no enough knowledge to decide about buckets, a simple attention mask over all teacher layers could provide competitive results.</p><p>As our future direction, we are interested in applying ALP-KD to other tasks to distill from extremely deep teachers into compact students. Moreover, we will work on designing better attention modules. It seems techniques that are able to handle sparse structures could be more useful in this context. Finally, we like to adapt our model to combine other internal components such as attention heads.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Student and teacher models have m and n layers, respectively. Each node is an intermediate layer and links are cross-model connections. In this example, every other layer of the teacher is skipped in order to match the size of the student. The output of nodes connected to each other are compared via a loss function (shown with $) to ensure that the student model has similar internal representations as its teacher.</figDesc><graphic url="image-3.png" coords="2,120.75,184.34,105.00,115.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Three pairs of S and T networks with different forms of layer connections. In Figure (a), teacher layers are divided into 3 buckets and only one layer from each bucket is connected to the student side, e.g. h 5 T is the source of distillation for h 2 S (h 5 T $ h 2 S ). In Figure (b), a weighted average of teacher layers from each bucket is considered for distillation, e.g. A(2) = {h 4 T , h 5 T } and C 2 = ↵ 24 h 4 T + ↵ 25 h 5 T (C 2 $ h 2 S ). In Figure (c), there is no bucketing and all teacher layers are considered for projection. Links with higher color intensities have higher attention weights.</figDesc><graphic url="image-4.png" coords="4,90.00,54.00,166.50,166.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualizing attention weights between the first layer of the student model and all teacher layers for 10 samples from SST-2. Weights belong to S ALP with 2 (a), 4 (b), and 6 (c) layers. Stronger attention weights are visualized with higher intensities.</figDesc><graphic url="image-5.png" coords="6,319.50,212.92,248.23,66.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 4aFigure4b</figDesc><graphic url="image-8.png" coords="7,58.41,295.84,117.18,77.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The visualization of intermediate layers' outputs and their distance from the teacher in ALP and PKD students.Teacher-, ALP-, and PKD-related information is visualized with green, red, and blue colors, respectively. 4a and 4c include information about h 2 ALP , h 2 PKD and h 6 T and 4b and 4d report about h 3 ALP , h 3 PKD and h 9 T . In 4c and 4d, the x axis shows samples and the y axis is the cosine distance from the teacher.</figDesc><graphic url="image-9.png" coords="7,184.41,295.84,117.18,77.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Except the teacher (T BERT ) which is a 12-layer model, all other models have 4 layers. Apart from the number of layers, all students have the same architecture as the teacher. The first column shows what sort of problems each model suffers from. NKD stands for No KD which means there is no KD technique involved during training this student model. NO and PO are different configurations for mapping internal layers. Boldfaced numbers show the best student score for each column over the validation set. Scores in the first column are Matthew's Correlations. SST-B scores are Pearson correlations and the rest are accuracy scores.heads, hidden dimension etc) remains untouched. There is no connection between the teacher and S NKD and it is trained separately with no KD technique. Performance drops in this case but we still gain a lot in terms of memory as this new model only has 53M parameters. To bridge the performance gap between the teacher and S NKD we involve KD in the training process and train new models, S RKD and S PKD , with RKD and PKD techniques, respectively.</figDesc><table><row><cell>summarizes our experiments.</cell></row><row><cell>The teacher model with 12 layers and 109M parameters has the best performance for all datasets. 3 This model can be</cell></row><row><cell>compressed, so we reduce the number of layers to 4 and train</cell></row><row><cell>another model (S NKD ). The rest of the configuration (attention</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The teacher model T BERT has 12 and all other student models have 6 layers.</figDesc><table><row><cell>Problem</cell><cell cols="3">Model CoLA MNLI MRPC QNLI QQP RTE</cell><cell>SST-2 STS-B Average</cell></row><row><cell cols="2">N/A N/A skip, search S RKD T BERT S NKD skip, search S PKD search S CKD-NO 48.49 81.91 57.31 83.39 40.33 79.91 45.51 81.41 45.78 82.18 search S CKD-PO 46.99 81.99 search S ALP-NO 46.40 81.99 search S ALP-PO 46.02 82.04 none S ALP 46.81 81.86</cell><cell>86.76 81.86 83.82 85.05 83.82 83.82 85.78 84.07 85.05</cell><cell cols="2">91.25 90.96 68.23 92.67 88.82 87.57 90.21 65.34 90.02 88.49 88.21 90.56 67.51 91.51 88.70 89.31 90.73 68.23 91.51 88.56 89.53 90.64 67.51 91.40 88.73 89.44 90.82 67.51 91.17 88.62 89.71 90.64 68.95 91.86 88.81 89.16 90.56 68.23 91.74 88.72 89.67 90.73 68.59 91.86 88.68</cell><cell>82.42 77.97 79.65 80.17 80.25 80.05 80.52 80.07 80.41</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 )</head><label>1</label><figDesc>look like when stimulated with</figDesc><table><row><cell>Problem</cell><cell cols="4">Model CoLA MNLI MRPC QNLI QQP RTE</cell><cell>SST-2 STS-B Average</cell></row><row><cell cols="2">N/A N/A skip, search S RKD T BERT S NKD skip, search S PKD-1 skip, search S PKD-6 skip, search S PKD-12 search S CKD none S ALP</cell><cell>57.31 83.39 14.50 72.73 24.50 74.90 23.09 74.65 22.48 74.57 22.46 74.33 24.69 74.67 24.61 74.78</cell><cell>86.76 72.06 73.53 72.55 73.04 72.79 73.04 73.53</cell><cell cols="2">91.25 90.96 68.23 92.67 88.82 79.61 86.89 57.04 85.89 40.80 81.04 87.40 59.21 87.39 41.87 81.27 87.68 57.40 88.76 43.37 80.74 87.70 57.40 88.65 42.92 81.22 87.88 57.40 88.76 45.39 81.60 87.10 58.84 88.65 43.71 81.24 88.01 59.57 88.88 46.04</cell><cell>82.42 63.69 66.23 66.1 65.94 66.28 66.54 67.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The teacher model T BERT has 12 and all other student models have 2 layers. S PKD-l indicates that h l T is used for distillation.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">By the output, we mean the output of the layer for the CLS token. For more details about CLS see<ref type="bibr" target="#b8">Devlin et al. (2019)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://github.com/google-research/bert</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">Similar to other papers, we evaluate our models on validation sets. Testset labels of GLUE datasets are not publicly available and researchers need to participate in leaderboard competitions to evaluate their models on testsets.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank our anonymous reviewers as well as Chao Xing and David Alfonso Hermelo from Huawei Noah's Ark Lab for their valuable feedback.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Aguilar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Knowledge Distillation from Internal Representations</title>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<biblScope unit="page" from="7350" to="7357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The second pascal recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bar-Haim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Szpektor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second PASCAL challenges workshop on recognising textual entailment</title>
				<meeting>the second PASCAL challenges workshop on recognising textual entailment</meeting>
		<imprint>
			<publisher>Venice</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="6" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The Fifth PASCAL Recognizing Textual Entailment Challenge</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<editor>TAC</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Buciluǎ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00055</idno>
		<title level="m">Semeval-2017 task 1: Semantic textual similaritymultilingual and cross-lingual focused evaluation</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The PAS-CAL recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges Workshop</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Paraphrasing (IWP2005)</title>
				<meeting>the Third International Workshop on Paraphrasing (IWP2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Ensemble distillation for neural machine translation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sankaran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01802</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">T</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Born Again Neural Networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The third pascal recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing</title>
				<meeting>the ACL-PASCAL workshop on textual entailment and paraphrasing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">First Quora Dataset Release: Question Pairs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dandekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Csernai</surname></persName>
		</author>
		<ptr target="https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tinybert: Distilling bert for natural language understanding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.10351</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Kim, Y</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2019. 2016</date>
			<biblScope unit="page" from="1317" to="1327" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Sequence-Level Knowledge Distillation</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>ArXiv abs/1904.09482</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
				<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018">2019. 2018</date>
		</imprint>
	</monogr>
	<note>Deep contextualized word representations</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<title level="m">Squad: 100,000+ questions for machine comprehension of text</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Distil-BERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno>ArXiv abs/1910.01108</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
				<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Patient Knowledge Distillation for BERT Model Compression</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4314" to="4323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mobilebert: a compact task-agnostic bert for resourcelimited devices</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02984</idno>
		<ptr target="https://openreview.net/forum?id=S1gUsoR9YX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2020. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Multilingual Neural Machine Translation with Knowledge Distillation</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno>ArXiv abs/1804.07461</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12471</idno>
		<title level="m">Neural Network Acceptability Judgments</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Online Distilling from Checkpoints for Neural Machine Translation</title>
		<author>
			<persName><forename type="first">H.-R</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1932" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A Broad-Coverage Challenge Corpus for Sentence Understanding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
