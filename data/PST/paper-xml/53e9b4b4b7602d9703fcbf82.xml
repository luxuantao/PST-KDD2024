<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2002">2002 and 2005</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">F</forename><surname>Zhong</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Sichuan province Key Lab of Signal and Information Processing</orgName>
								<orgName type="institution">Southwest Jiaotong University</orgName>
								<address>
									<postCode>610031</postCode>
									<settlement>Chengdu</settlement>
									<country>China，</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer &amp; Information Engineering</orgName>
								<orgName type="institution">Yibin University</orgName>
								<address>
									<postCode>644000</postCode>
									<settlement>Yibin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Sichuan province Key Lab of Signal and Information Processing</orgName>
								<orgName type="institution">Southwest Jiaotong University</orgName>
								<address>
									<postCode>610031</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country>China, in</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">School of Computer and Information Engineering</orgName>
								<orgName type="laboratory">Si-Chuan province Key Lab of Signal and Information Processing</orgName>
								<orgName type="institution">Southwest Jiaotong University</orgName>
								<address>
									<addrLine>China. In 2005</addrLine>
									<settlement>Chengdu</settlement>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">Yibin University</orgName>
								<address>
									<settlement>Yibin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">school of Information Science and Technology at Southwest</orgName>
								<orgName type="institution">Jiaotong University</orgName>
								<address>
									<settlement>Chengdu, Sichuan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2002">2002 and 2005</date>
						</imprint>
					</monogr>
					<idno type="MD5">D33D3B08E74892652D4E86C69CB69DB6</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Linear discriminant analysis</term>
					<term>L2-norm</term>
					<term>outliers</term>
					<term>L1-norm</term>
					<term>optimization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Linear discriminant analysis (LDA) is a well-known dimensionality reduction technique which is widely used for many purposes. However, conventional LDA is sensitive to outliers because its objective function is based on the distance criterion using L2-norm. This paper proposes a simple but effective robust LDA version based on L1-norm maximization, which learns a set of local optimal projection vectors by maximizing the ratio of the L1-norm-based between-class dispersion and the L1-norm-based within-class dispersion. The proposed method is theoretically proved to be feasible and robust to outliers while overcoming the singular problem of the within-class scatter matrix for conventional LDA. Experiments on artificial datasets, standard classification datasets and three popular image databases demonstrate the efficacy of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>here are many dimensionality reduction techniques used to reduce the number of input variables to simplify data analysis problems. Among them, Principal Component Analysis (PCA) <ref type="bibr" target="#b0">[1]</ref> and Linear Discriminant Analysis (LDA) <ref type="bibr" target="#b1">[2]</ref> are two of the most popular ones. PCA learns a set of projection vectors so that the variance of given data in feature space is maximized. These projection vectors constitute a low-dimensional linear subspace by which the most representative feature information about original samples can be effectively captured. And LDA uses the class information to learn an optimal matrix that maximizes the between-class scatter while minimizing the within-class scatter in feature space. PCA deals with the data for the principal components without considering the underlying class structure, but the purpose of LDA is to achieve the maximal class discrimination. PCA has been used in industrial robotics <ref type="bibr" target="#b2">[3]</ref>, handprint recognition <ref type="bibr" target="#b3">[4]</ref> and face recognition <ref type="bibr" target="#b4">[5]</ref>. LDA has been used in human action recognition <ref type="bibr" target="#b5">[6]</ref>, face recognition <ref type="bibr" target="#b6">[7]</ref> and even in generic object recognition <ref type="bibr" target="#b7">[8]</ref>. It is generally believed that LDA is superior to PCA for object recognition, the detailed comparative study of PCA and LDA is carried out in literature <ref type="bibr" target="#b8">[9]</ref>.</p><p>However, conventional PCA and LDA are sensitive to outliers because their L2-norm distance criterion magnifies the effect of outliers by the square operation. It is well known that L1-norm is more robust to outliers than L2-norm <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>. Recently, several L1-norm-based PCA methods have been developed to improve the robustness to outliers. L1-PCA <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref> is formulated by applying maximum likelihood estimation to the training data and obtains the optimal solution by using the weighted median method and convex programming methods. R1-PCA <ref type="bibr" target="#b15">[16]</ref> combines the merits of PCA and those of L1-PCA. But, both L1-PCA and R1-PCA are complicated and computationally expensive. PCA-L1 <ref type="bibr" target="#b16">[17]</ref> is a fast, robust and rotational invariant L1-norm-based PCA which learns the local optimal projection axes by maximizing the L1-norm-based variance in the feature space. The optimization algorithm of PCA-L1 is intuitive, simple and easy to implement. As the report in <ref type="bibr" target="#b16">[17]</ref>, PCA-L1 obtains lower reconstruction error than L1-PCA and R1-PCA. <ref type="bibr">Li et al.</ref> generalizes this algorithm to 2DPCA to propose 2DPCA-L1 which has lower reconstruction error than PCA-L1, PCA and 2DPCA <ref type="bibr" target="#b17">[18]</ref>. In addition, sparse representation (SR) models have been developed for robust computer vision problems <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>. As Wright et al. point out, "what are critical, however, is whether the number of features is sufficiently large, whether there are sufficient training samples and whether the sparse representation is correctly harnessed" <ref type="bibr" target="#b20">[21]</ref>.</p><p>Recently, motivated by R1-PCA, Li Xi et al. <ref type="bibr" target="#b22">[23]</ref> propose a novel rotational invariant L1-norm (i.e. R1-norm) based LDA (termed as LDA-R1). However, the iterative algorithm based on eigenvalues decomposition used in LDA-R1 takes too much time to achieve convergence for a large dimensional input space. Similarly, motivated by the basic idea of PCA-L1, we propose the L1-norm-based LDA (termed as LDA-L1). By analyzing the objective function of conventional LDA, we get the transformed version of its objective function, which is formed using L2-norm. Referring to the L2-norm-based objective function of conventional LDA, the proposed method have its own objective function based on L1-norm, i.e. the ratio of the L1-norm-based between-class dispersion and the L1-norm-based within-class dispersion. LDA-L1 aims to obtain a set of projection axes by maximizing the L1-norm-based objective function. But it is very difficult to directly find the global optimal solution of the new LDA-L1 objective problem. To solve this problem, we present a scheme to obtain an approximate solution of the objective x j where y j is called the feature of x j . The optimal projection matrix can be gained by maximizing the following objective function <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>:</p><formula xml:id="formula_0">      b w tr J tr  S W S<label>(1)</label></formula><p>where tr(• ) denotes the trace of a matrix, S b is the between-class scatter matrix and S w is the within-class scatter matrix in the feature space. The measure J in equation ( <ref type="formula" target="#formula_0">1</ref>) is the well-known Fisher scalar for the measure of the class separability. S b and S w can be respectively calculated by</p><formula xml:id="formula_1">   1 1 C T bi ii i N N      S y y y y (2)    1 1 C T w j i j i C ij i N      S y y y y<label>(3)</label></formula><p>where the superscript T denotes the transposition operation, y is the global mean vector and i y is the mean vector of the ith class in the feature space. Substituting y j =W T x j into (2) and (3), we have that</p><formula xml:id="formula_2">   1 1 C T T b i i i i N N      WW S x x x x (4)    1 1 C T T w j i j i C ij i N      W x x x x W S<label>(5)</label></formula><p>where x and i x denote the global mean vector and the ith class mean vector of training samples. According to equation ( <ref type="formula">4</ref>) and ( <ref type="formula" target="#formula_2">5</ref>), the objective function (1) can be rewritten as</p><formula xml:id="formula_3">      T T b w tr J tr  W G W W W G W<label>(6)</label></formula><p>where</p><formula xml:id="formula_4">   1 1 C b i i i T i N N     G x x x x and    1 1 C T w C i j i j i ij N      G x x x x .</formula><p>If G w is nonsingular, the optimal projection matrix W opt can be obtained by computing the eigenvectors of 1 wb  GG corresponding to the n largest eigenvalues. But in many applications, one has to be confronted with the difficult problem that G w is always singular because the number of images in training set is often much lower than the size of each image. One popular method of overcoming it is to utilize PCA as a preprocessing step to reduce the dimensionality of the training data. Besides, there are other methods such as in <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. LDA-L1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem formulation</head><p>From the above description, LDA-L2 is aimed at solving the following optimization problem:</p><formula xml:id="formula_5">            1 1 1 1 opt C T T i i i C T T j i j i C i i ij arg max J tr arg max tr N N N                     W W W WW W x x x x W W x x x x<label>(7)</label></formula><p>By simply transforming, (7) can be rewritten as</p><formula xml:id="formula_6">    2 2 2 2 1 1 C T ii opt C T ji C i i ij arg max N        W W x x W x x W<label>(8)</label></formula><p>where 2  denotes L2-norm. From equation <ref type="bibr" target="#b7">(8)</ref>, the objective function of LDA-L2 is obviously based on L2-norm distance criterion. However, it is well known that L2-norm is sensitive to outliers because the square operator exaggerates the effect of the outliers. From a statistical point of view, the L1-norm-based methods are more robust to outliers than the L2-norm-based methods <ref type="bibr" target="#b16">[17]</ref>. Motivated by this idea, we present L1-norm-based LDA which gets the optimal projection matrix by solving the optimization problem as follows:</p><formula xml:id="formula_7">      1 1 1 1 C T ii opt C T ji C i i ij arg max F arg max N        WW W x x W W x x W     1 1 1 1 = C n T i k i T n C n T k j i C i k i k ij arg max N subject to            W w x x W W I w x x<label>(9)</label></formula><p>where 1  denotes L1-norm,  is the absolute value operation and W T W=I n is to ensure the orthonormality of the projection matrix. However, for n&gt;1, it is very difficult to find a global solution of <ref type="bibr" target="#b8">(9)</ref>. To overcome this problem, we can simplify <ref type="bibr" target="#b8">(9)</ref> into a series of n=1 optimization problems by a greedy search method. If n is set to 1, ( <ref type="formula" target="#formula_7">9</ref>) is transformed to the following optimization problem:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head> </head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>* =1</head><p>T arg max F subject to  w w w w w <ref type="bibr" target="#b9">(10)</ref> where F(w) is the objective function as follows:</p><formula xml:id="formula_8">          1 1 11 11 CC TT i i i i CC TT j i j i CC ii ii i j i j F NN              w x x w x x w w x x w x x<label>(11)</label></formula><p>The successive greedy solutions of (10) may be different from the optimal solution of ( <ref type="formula" target="#formula_7">9</ref>), but it is expected to provide a good approximation. However, the absolute value operation is not differentiable, which makes it difficult to directly solve the global optimal solution of (10). In the next section, an iterative algorithm to find a local optimal solution of (10) is presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Algorithm of finding a local optimal projection</head><p>Suppose 1 * m  wR is one projection vector of LDA-L1, which is a local optimal solution of <ref type="bibr" target="#b9">(10)</ref>. Because * w is the local convergence result of iterations, we can use w(t) to denote the result of iteration t and assume that w(t)≠0. The procedure of the iteration algorithm can be described as follows.</p><p>Step </p><formula xml:id="formula_9">          1 if 0 1 1 if 0 T i i T i ,t p t i , ,C ,t         w x x w x x  (12)           1 if 0 1 if 0 1 and T ji j T ji i ,t qt ,t i , ,C j C            w x x w x x  (13)</formula><p>Substituting <ref type="bibr" target="#b11">(12)</ref> and ( <ref type="formula">13</ref>) into <ref type="bibr" target="#b10">(11)</ref>, it follows that</p><formula xml:id="formula_10">          1 1 C T ii C T ji C i i ij t Ft t N        w x x w w x x           1 1 C T i i i C T j j i C i i ij t t N p t qt        w x x w x x (<label>14</label></formula><formula xml:id="formula_11">)</formula><p>Step 3. Updating w(t+1):</p><formula xml:id="formula_12">Let               1 1 11 C C j j i i i i C CC TT i i j i C i i ij i i i j t qt N p t t Nt                w xx xx d w x x x x (15) Renew w(t+1) by       1 t t t     w w d (16) Rescale       2 1 1 1 t t t     w w w</formula><p>and set t=t+1. Here, β is a parameter of updating rate which must take a small positive value. According to ( <ref type="formula" target="#formula_10">14</ref>), ( <ref type="formula">15</ref>) and ( <ref type="formula">16</ref>), w(t+1) is updated in the local ascending direction of the objective function based on the fixed tth iteration. Therefore, it is very easy that w(t+1) strides over the local maximum point if β takes a big or even infinite positive value. In the real experiments, we can try a series of different values of β, and select the parameter value which leads to the largest value of F(w). By this approach of appointing β, we can avoid the bad case that w(t+1) overshoots a local optimal point of F(w).</p><p>Step 4. Checking convergence: If F(w(t+1)) can"t increase significantly (measured by the increasing speed), the iteration is stopped and the local optimal * (t)  ww can be output. Otherwise, go to Step 2.</p><p>Theorem: With the above iterative procedure, F(w(t)) is a nondecreasing function of t.</p><p>Proof: At iteration t, we have the objective function</p><formula xml:id="formula_13">          1 1 C T ii C T ji C i i ij t Ft t N        w x x w w x x<label>(17)</label></formula><p>According to the definition of   i pt, the numerator of ( <ref type="formula" target="#formula_13">17</ref>) can be rewritten as</p><formula xml:id="formula_14">   1 C T i i i t N    w x x          1 C TT i i i i t t t N p t     w w u xx<label>(18)</label></formula><p>where</p><formula xml:id="formula_15">     1 C i i i i t N p t     u xx<label>(19)</label></formula><p>The denominator of ( <ref type="formula" target="#formula_13">17</ref>) can be rewritten as</p><formula xml:id="formula_16">                1 1 1 1 2 1 2 C T ji C T C j i j i T T C ji C T ji C i i i j ij ij i t tt t t                 w x x ww w x x w x x x x x x         1 11 22 T t t t t  w V w z<label>(20)</label></formula><p>where</p><formula xml:id="formula_17">       1 T C j i j i C j i ij t zt     V x x x x<label>(21)</label></formula><p>and</p><formula xml:id="formula_18">  t z is the vector with the entries        1 and T j j i i z t t i , ,C j C     w x x </formula><p>. Substituting <ref type="bibr" target="#b17">(18)</ref> and ( <ref type="formula" target="#formula_16">20</ref>) into ( <ref type="formula" target="#formula_13">17</ref>), we get that</p><formula xml:id="formula_19">                1 11 22 T T tt Ft t t t t   wu w w V w z<label>(22)</label></formula><p>Because the direct derivative of F is intractable, we introduce a surrogate function as follows: <ref type="bibr" target="#b22">(23)</ref> Note that w(t) is a constant vector at iteration t, so u(t), V(t) and z(t) are fixed values, only ψ is the variable in the function L(ψ). And the gradient of L(ψ) with respect to ψ can be computed as follows:</p><formula xml:id="formula_20">        1 11 22 T T t L tt      u Vz</formula><formula xml:id="formula_21">                        1 2 1 11 22 11 22 TT T L g t t t t t tt               V z u u V Vz<label>(24)</label></formula><p>Substituting w(t) into ( <ref type="formula" target="#formula_21">24</ref>), we can obtain the gradient value at the point w(t):</p><formula xml:id="formula_22">                          1 2 1 11 22 11 22 T T t t t t t gt t t t t    w V w z u w w V w z                     2 1 11 22 T T t t t t t t t t   w u V w w V w z<label>(25)</label></formula><p>Substituting u(t), V(t) and z(t) into ( <ref type="formula" target="#formula_22">25</ref>), ( <ref type="formula" target="#formula_22">25</ref>) can be rewritten as</p><formula xml:id="formula_23">                   2 1 1 11 1 C i i i C T ji C CC T i i j j i C C T ji C i i i i ij i i j ij gt N p t t N t q t t                         w xx w x x w x x x x w x x               1 1 11 C C j j i i i i C CC TT i i j i C i i ij i i i j t qt N p t t Nt                w xx xx d w x x x x<label>(26</label></formula><p>) According to <ref type="bibr" target="#b25">(26)</ref>, d(t) is exactly the vector that points to the local ascending direction of g(ψ) at the point w(t). So, according to <ref type="bibr" target="#b15">(16)</ref>, replacing ψ with w(t+1) and w(t), we gain that</p><formula xml:id="formula_24">        1 L t L t  ww , i.e.,             1 1 11 11 22 T T tt t t t t     wu w V w z                 1 11 22 T T tt Ft t t t t   wu w w V w z<label>(27)</label></formula><p>Obviously, both the right side of the above inequality and the denominator of the left side are non-negative, which implies that the numerator of the left side is also non-negative. The purpose of ( <ref type="formula" target="#formula_24">27</ref>) is to bridge the justification of F(w(t+1))≥F(w(t)). Thus, the next steps must justify the following inequality</p><formula xml:id="formula_25">                1 1 1 11 11 22 T T Ft tt t t t t       w wu w V w z<label>(28)</label></formula><p>Firstly, we introduce the following lemma. </p><p>Secondly, we proceed to analyze the right side of (28).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Because</head><p> </p><formula xml:id="formula_27">1 i pt  is the polarity of    1 T i t  w x x , the quantity      1 1 T ii t pt   w x x is always non-negative for any 1 i , , N   . But, it is possible that      1 T ii t pt  w</formula><p>x x is not always non-negative for all i. So, for the numerator of (28), the following inequality holds: <ref type="bibr" target="#b30">(31)</ref> On the other hand, for the denominator of (28), we can get the following:</p><formula xml:id="formula_28">             1 1 1 1 1 1 1 i C T i C T i i i C T i i i i i i t t t N N p t N p t            w x x w w xx xx     10 T tt    wu</formula><formula xml:id="formula_29">                 1 2 1 1 11 11 22 1 11 22 T T C ji C j i ij t t t t t t zt        w V w z w x x z      2 1 1 1 11 22 T C ji C j N i j i t min           ξ R w x x ξ<label>(32)</label></formula><p>According to <ref type="bibr" target="#b29">(30)</ref> and <ref type="bibr" target="#b31">(32)</ref>, the following holds:</p><formula xml:id="formula_30">   1 1 C T ji C i j i t      w x x         1 11 1 1 22 T t t t t     w V w z<label>(33)</label></formula><p>Combining ( <ref type="formula">31</ref>) and ( <ref type="formula" target="#formula_30">33</ref>), we have the following:</p><formula xml:id="formula_31">      1 1 1 1 C T ii C T ji C i i j i t t N         w x x w x x             1 1 11 11 22 T T tt t t t t      wu w V w z<label>(34)</label></formula><p>Finally, we find that the left side of ( <ref type="formula" target="#formula_31">34</ref>) is exactly equal to F(w(t+1)). Therefore, combining ( <ref type="formula" target="#formula_24">27</ref>) and ( <ref type="formula" target="#formula_31">34</ref>), we can get the conclusion:</p><formula xml:id="formula_32">        1 F t F t  ww (35)</formula><p>Besides, according to <ref type="bibr" target="#b16">(17)</ref>, we have that F(  w(t))=F(w(t)) where  is a scale constant. So, the projection vector w(t) can be normalized without effect for the optimization objective. Essentially, only the direction of the projection is the most important for the objective function.</p><p>Thus, we prove theoretically that F(w(t)) is nondecreasing with respect to iteration t. In fact, the trivial case that F(w(t)) remains unchangeable during the prophase of iteration procedure seldom happens. This ensures that w(t) moves toward a local maximum point of <ref type="bibr" target="#b9">(10)</ref>.</p><p>According to the above proof, w * is a local maximum point of the objective function F(w). However, it is very difficult to find the global boundary of the objective function and obtain its global maximum point. In the future, further work is aimed at these problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Learning multiple projection vectors</head><p>Until now, we have shown the algorithm that can learn one local optimal projection vector that maximizes the L1-norm-based objective function <ref type="bibr" target="#b9">(10)</ref>. We can easily extend it to learn multiple projection vectors by a greedy search method as follows.</p><p>Step 1. Initialization: set</p><formula xml:id="formula_33">      0 0 0 == = 1 and i j i j i i i , ,C j C       w 0 x x x x x x xx  ； ； 。</formula><p>Step2. Learning n projection vectors: </p><formula xml:id="formula_34">for k=1 to n              <label>11</label></formula><formula xml:id="formula_35">T kk k k k T j i j i k k j i k k k i i i i i , ,C j C             ww x x x x w w x x x x x x x x  -<label>(36)</label></formula><p>Apply the iteration algorithm to learn w k using the samples</p><formula xml:id="formula_36">  i k  xx and   ji k  xx end</formula><p>These projection vectors extracted by the above procedure are orthonormal. Obviously, w k is a unit vector because of the normalization during the iteration procedure. Next, we can prove that w k is orthogonal to w k-1 for all k (k= 2, … , n) as the following:</p><p>1. According to the initialized value of w(0), equation ( <ref type="formula">15</ref>) and ( <ref type="formula">16</ref>), w k is a linear combination of the samples</p><formula xml:id="formula_37">  i k  xx and   ji k  xx, so w k is in the space spanned by   i k  xx and   ji k  xx. 2. By multiplying 1 T k  w to both sides of   i k  xx and   ji k  xxin (36), we have that       1 1 1 1 1 11 = 0, T k k T T T k k k k kk i ii          w w w w w xx x x x x -       1 1 1 1 1 1 = 0. T k j i k T T T k j i k k k j i k            w x x w x x w w w x x 3. From 2, w k-1 is orthogonal to   i k  xx and   ji k</formula><p> xx, which shows that w k is orthogonal to w k-1 according to 1. Then, we can prove that w k is orthogonal to w k-2 for all k (k= 3, … , n) as follows:</p><p>1. By recursive calling (36), we have that  </p><formula xml:id="formula_38">            1 1 2 2 2 1 1 2 2 2 =; =. k TT m k k m k k k ji k TT m k k m k k j i k i i         </formula><formula xml:id="formula_39">            2 2 1 1 2 2 2 2 2 1 1 2 2 2 = =0; = =0. T k k T T T k m k k m k k k T k j i k T T T k m k k m k k j i k i i                 </formula><formula xml:id="formula_40">3. From 2, w k-2 is orthogonal to   i k  xx and   ji k  xx, which shows that w k is orthogonal to w k-2 because w k is a linear combination of   i k  xx and   ji k  xx.</formula><p>Therefore, on the base of the induction and summarization of the above two proof procedures, we can have that these extracted projection vectors are orthonormal, i.e. We note that the successive greedy solutions may not provide the global optimal solution of ( <ref type="formula" target="#formula_7">9</ref>), but it is expected to provide a series of good projection vectors that locally maximize the L1-norm-based dispersion ratio. Whereas, the following experimental results show that LDA-L1 is more robust to outliers than LDA-L2. On the other hand, LDA-L2 sometimes has to be confronted with the problem that the within-class scatter matrix is singular in practice. However, from the above description, LDA-L1 overcomes this problem completely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Analysis of computational cost</head><p>The computational complexity of each iteration of LDA-L1 is O((N+C)m). Provided that T  denotes the average number of iterations for learning one projection vector, the practical computational cost also involves T  and the number of projection vectors (i.e. n). Obviously, T  doesn"t depend on the dimension m of input space but depends on the updating parameter β, the sample number N and the class number C. So, LDA-L1 can be applied to the tasks with high dimensional input space without adding much computational cost.</p><p>However, the computational complexity of each iteration of LDA-R1 is O((N+C)m 2 n 2 ) which is much higher than that of LDA-L1. LDA-R1"s practical computational cost also involves its iteration number T which not only depends on the dimension m but also the parameter α, N and C. In fact, it is difficult for LDA-R1 to converge, so we set T max =20 following the literature <ref type="bibr" target="#b22">[23]</ref> in the experiments. On the other hand, the computational complexity of LDA-L2 is O((N+C+ n 2 )m).With no iteration procedure, LDA-L2"s practical computational cost must be the lowest one of three methods". In theory, it is not completely clear that LDA-L1"s practical computational cost must be lower than that of LDA-R1 because many parameters are involved. Only the iteration number can"t indicate the practical computational cost, so we have adopted an experimental approach to record each method"s learning time on real databases when the best classification result is obtained. It can be found that LDA-L2 is the fastest method for no iterations, but obtains the lowest classification accuracy. However, LDA-L1 is faster than LDA-R1 while keeping high classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>In this section, we have taken several experiments to compare the performances of LDA-L2, LDA-R1 and our proposed LDA-L1. In the first subsection, we have taken experiments on artificial datasets. In the second subsection, the experimental results on standard classification datasets are presented. Lastly, we give the experimental results in high-dimensional image classification on three image databases. Euclidean distance based nearest neighbor classifier <ref type="bibr" target="#b29">[30]</ref> is used for object classification. In addition, all experiments are executed on a portable computer system of Intel T2350 1.86GHz and 1GB RAM with Matlab 7.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiments on artificial datasets</head><p>This subsection presents two experiments on artificial datasets which follow Gaussian distribution. For obtaining the best performance, the control coefficient α of LDA-R1 is set to 0.2 and the updating parameter β of LDA-L1 is set to 0.002 for these two experiments. Because three methods are very fast for small low-dimensional training data, we do not compare their training time in this subsection.</p><p>In the first experiment, we create two Gaussian classes with covariance matrices being [0.05 0; 0 2] and means being [-2 0] and [2 0] respectively. Each class consists of 20 2D samples as depicted in Fig. <ref type="figure" target="#fig_6">1</ref>, where two classes are respectively specified by blue "o" and "x". Without any outlier, the real optimal projection vector should be =</p><formula xml:id="formula_41">[1 0] T , w ( 0   </formula><p>) for classification. Then, we extract the projection vectors using LDA-L2, LDA-R1 and LDA-L1 on the above samples as training dataset, respectively. Fig. <ref type="figure" target="#fig_6">1</ref>   For improving the reliability of the experimental results, each kind of test is repeated 20 times based on different datasets which are regenerated according to the above requests. However, the outlier is not changed. The number of extracted projection vector by each method is two. We record the correct classification rates and compute the average correct classification rates with or without outlier, respectively. The results are given in Table <ref type="table" target="#tab_2">I</ref>. As illustrated in Table <ref type="table" target="#tab_2">I</ref>, the average correct classification rates of LDA-L2, LDA-R1 and LDA-L1 are almost the same when the training dataset doesn"t contain any outlier. However, when the training dataset contains a significant outlier, the average correct classification rate of LDA-L2 is obviously decreased but the average correct classification rates of LDA-R1 and LDA-L1 maintain unchangeable. This indicates that both LDA-R1 and LDA-L1 are strongly robust to outlier when the training samples follow Gaussian distribution. In practice, there seldom exists such a normative dataset in real applications. Therefore, the following experiments will be performed on real datasets for strict challenges. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments on standard classification datasets</head><p>In this subsection, we take experiments on publicly available standard classification datasets coming from the UC Irvine machine learning repository <ref type="bibr" target="#b30">[31]</ref>. In our experiments, we select five datasets: Wisconsin Breast Cancer database, SPECTF Heart dataset, SPECT Heart dataset, Ionosphere database and Iris Plants database. The number of extracted projection vectors by three methods on the first four datasets is one. The number of extracted projection vectors on Iris Plants database is two. Wisconsin Breast Cancer database has two classes, benign and malignant. Excluding 16 samples that contain a single missing attribute value, benign contains 444 samples and malignant contains 239 samples. And each sample is composed of 9 attribute values. In the experiment on Wisconsin Breast Cancer database, the first ten samples of each class form the training set and the rest are used for the testing set. When the control coefficient α of LDA-R1 is set to 0.2 and the updating parameter β of LDA-L1 is set to 0.04, they obtain their own optimal correct classification rates which are shown in Table <ref type="table" target="#tab_4">II</ref> and Fig. <ref type="figure" target="#fig_0">2</ref>(a) draws one-dimensional projections of three methods. SPECTF Heart dataset describes diagnosing of cardiac Single Proton Emission Computed Tomography (SPECT) images. Each patient is classified into two categories: normal and abnormal. The database of 267 SPECT image sets (patients) was processed to extract 44 continuous feature patterns that summarize the original SPECT images. The pattern was further processed to obtain 22 binary feature patterns in SPECT Heart dataset. In the experiments on both datasets, forty samples of each class form the training set and the rest are used for the testing set. On SPECTF Heart dataset, when the control coefficient α of LDA-R1 is set to 0.3 and the updating parameter β of LDA-L1 is set to 0.005, they obtain their own optimal correct classification rates which are shown in Table <ref type="table" target="#tab_4">II</ref>. On SPECT Heart dataset, when the control coefficient α of LDA-R1 is set to 0.25 and the updating parameter β of LDA-L1 is set to 0.09, they obtain their own optimal correct classification rates which are shown in Table <ref type="table" target="#tab_4">II</ref>. Fig. <ref type="figure" target="#fig_0">2</ref>(b) draws one-dimensional projections of three methods on SPECTF Heart dataset, and Fig. <ref type="figure" target="#fig_0">2</ref>(c) draws one-dimensional projections of three methods on SPECT Heart dataset. Ionosphere database has two data classes, "bad" and "good". Class "bad" contains 126 samples, and class "good" includes 225 samples. And each sample is composed of 34 attribute values. The first fifty samples of each class constitute the training set and the remainders form the testing set. When the control coefficient α of LDA-R1 is set to 0.37 and the updating parameter β of LDA-L1 is set to 0.08, they obtain their own optimal correct classification rates which are shown in Table <ref type="table" target="#tab_4">II</ref> and Fig. <ref type="figure" target="#fig_0">2</ref>(d) draws one-dimensional projections of three methods. Iris Plants database contains three data classes, each consisting of 50 samples with 4 attribute values. Similarly, the first ten samples of each class constitute the training set and the remainders form the testing set. When the control coefficient α of LDA-R1 is set to 0.9 and the updating parameter β of LDA-L1 is set to 0.09, they obtain their own optimal correct classification rates which are shown in Table <ref type="table" target="#tab_4">II</ref> and Fig. <ref type="figure" target="#fig_8">3</ref> draws two-dimensional projections of three methods. Obviously, LDA-L1 and LDA-R1 have kept the classes more tightly clustered than LDA-L2. For all that, three methods keep good separation of three classes so that their correct classification rates are the same.</p><p>On the other hand, we record their learning time on five datasets in Table <ref type="table" target="#tab_4">II</ref>. It can be found that LDA-L2 is the fastest method for no iterations, but obtains low classification accuracy. However, LDA-L1 is faster than LDA-R1 while obtaining high classification accuracy. However, the results have exposed the inadaptability of the parameters of both LDA-L1 and LDA-R1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments on high-dimensional image databases</head><p>This part compares our proposed LDA-L1 with LDA-L2 and LDA-R1 in the field of high-dimensional image classification when outliers are present in datasets. We conduct experiments on three well-known image datasets: ORL face database <ref type="bibr" target="#b31">[32]</ref>, partial FERET face database <ref type="bibr" target="#b32">[33]</ref> and Binary Alphadigits database <ref type="bibr" target="#b33">[34]</ref>. On the other hand, for overcoming the singular problem of LDA-L2, conventional PCA is used as a preprocessing method for LDA-L2, LDA-R1 and LDA-L1. The dimensionality of PCA step is set to N-C and the maximum dimensionality of LDA-L2 is C-1 where N is the number of all training samples and C is the number of class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Results on ORL database</head><p>This experiment is conducted on ORL database which includes 400 images of 40 persons, every person providing 10 different images with the size of 112×92. All images are gray with 256 levels and some images were taken at different time. There are variations in facial expressions and facial details. The first five images of each subject are used for training samples and the rest five images are used for testing samples. The grayscale images are not preprocessed, except that 40 percent of these 200 training samples are randomly selected to be added rectangle noise as outliers. The rectangle noise consists of back or white dots in random distribution, its location in image is random and its sizes range from 20×20 to 60×60. Fig. <ref type="figure" target="#fig_9">4</ref> shows some face images without or with rectangle noise. For disclosing the relationship between the classification accuracy and the feature dimensions, experiments are performed under a series of feature dimensions. When the control coefficient α of LDA-R1 is set to 0.2 and the updating parameter β of LDA-L1 is set to 0.08, they obtain their own optimal classification performances. Fig. <ref type="figure" target="#fig_10">5</ref> shows the classification accuracy versus feature dimension on ORL database with occlusion. The optimal correct classification rates of LDA-L2, LDA-R1 and LDA-L1, the corresponding dimensionality and learning time are shown in Table <ref type="table" target="#tab_3">III</ref>.</p><p>Obviously, the correct classification rate of LDA-L1 is higher than those of LDA-L2 and LDA-R1. On the other hand, the training time of LDA-L1 is less than that of LDA-R1 but more than that of LDA-L2.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Results on partial FERET database</head><p>The selected partial FERET database includes 1200 images of 200 different subjects, each subject contains six images, which corresponds to the images whose names being "Ia", "Ib", "Ic", "Id", "Ie", "If", respectively. Fig. <ref type="figure">6</ref> (a) gives the samples of two subjects in partial FERET database. In our experiments, the facial portion of each original image is cropped based on the location of eyes and mouth, and scaled to 80×80 pixels. Then, the images named "Ia" are occluded with rectangle noise which is generated as in the above subsection. Some occluded images are shown in Fig. <ref type="figure">6</ref>     We aim to evaluate their optimal classification accuracy and computational performance. When the control coefficient α of LDA-R1 is set to 0.2 and the updating parameter β of LDA-L1 is set to 0.01, they obtain their own optimal classification performance. Similarly, the maximum iteration number of LDA-R1 is set to 20. We record their optimal correct classification rates and corresponding dimensionality in Table <ref type="table" target="#tab_6">IV</ref>. The performances of LDA-L1 and LDA-R1 are better than that of LDA-L2 in four tests. Except in Test #4, the performance of LDA-L1 is better than that of LDA-R1 in the first three tests. In all four experiments, the parameters of both LDA-L1 and LDA-R1 are fixed values, therefore the inadaptability of the updating parameter results in the bad performance of LDA-L1 in Test #4. However, the average classification accuracy of LDA-L1 is higher than those of the other two methods, which illustrates the efficacy of our proposed method. On the other hand, we record the learning time in Table <ref type="table" target="#tab_6">V</ref>. It is clear that LDA-L2 is much better than LDA-R1 and LDA-L1 in computational cost. However, LDA-L2 is so sensitive to outliers that its classification accuracy is very bad when outliers are present in training set. LDA-L1 outperforms LDA-R1 in computational cost while keeping the insensibility to outliers. Ten binary 20×16 digits of "0" through "9" are selected from Binary Alphadigits database to form the experimental dataset. There are 39 examples of each digit and all 390 samples are shown in Fig. <ref type="figure" target="#fig_12">7</ref>. We can find that most samples are relatively standardized, but a few samples are illegible and hardly distinguished visually. To some extent, those relatively normative samples can be viewed as inlying data and those illegible samples can be viewed as outlying data. We conduct objection recognition tests on the dataset. The first k (k= 6, 9, 12, 15, 18) samples of each digit are selected as the training set and the remaining 39-k images of every digit are used as the testing set. The control coefficient α of LDA-R1 is set to 0.2 and the updating parameter β of LDA-L1 is set to 0.01. We record the optimal correct classification rates and the corresponding learning time of LDA-L2, LDA-R1 and LDA-L1 which are shown in Table <ref type="table" target="#tab_7">VI</ref>. It can be found that LDA-L1 has obtained better classification accuracy than LDA-R1 and LDA-L2, and consumed less learning time than LDA-R1. On the other hand, the classification accuracy of LDA-L2 is unstable with the increasing of the training sample number. The key reason should be that some illegible samples are gradually included in the training set while the training number is increasing and LDA-L2 is sensitive to outliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Results on Binary Alphadigits database</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, a new method, called L1-norm-based linear discriminant analysis (LDA-L1), is proposed. LDA-L1 aims to learn a set of local optimal projection vectors by maximizing the ratio of the L1-norm-based between-class dispersion and the L1-norm-based within-class dispersion. The proposed LDA-L1 is more robust to outliers than conventional L2-norm-based LDA because L1-norm can suppress the negative effects of outliers to some extent. Meanwhile, the computational cost of LDA-L1 is much lower than that of LDA-R1 while keeping the insensibility to outliers. The effectiveness of the proposed method is shown by the experiments on artificial datasets, standard classification datasets and three image databases. On the other hand, the experimental results expose the inadaptability of the updating parameter. In the future, we try addressing this problem and exploiting a more efficient algorithm to find the global optimal solution of LDA-L1. And the robust two-dimensional LDA is being studied by maximizing the objective function based on L1-norm criterion. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Step 2 .</head><label>2</label><figDesc>Defining two polarity functions   i pt and   j qt :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>xxin above equations respectively, we get</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>w</head><label></label><figDesc>). We find that w LDA-L2 , w LDA-R1 and w LDA-L1 are almost close to the optimal projection vector. For comparing the robustness of LDA-L2, LDA-R1 and LDA-L1 to outlier, we introduce an additional outlier, i.e.<ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b9">10]</ref> specified by red "o" in Fig.1. Then we construct another training dataset based on the above dataset in which the first data point of class "o" is replaced with the outlier<ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b9">10]</ref>. Respectively, we use three methods to learn the projection is severely deviated from the original w LDA-L2 for the outlier"s interference. However, outlier LDA-L1 w is just slightly deviated from the original w LDA-L1 and closer to the optimal projection direction than the other two methods, which indicates that LDA-L1 is more robust to outlier than LDA-L2 and LDA-R1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Projection vectors learned by LDA-L2, LDA-R1 and LDA-L1 on an artificial dataset with/without outlier In the second experiment, we create three classes for comparing the classification accuracy of LDA-L2, LDA-R1 and LDA-L1. Each class contains 100 2D samples. Three simulated classes follow Gaussian distribution respectively with means being [0 5], [4 4] and [2 2], covariance matrices being [0.1 0; 0 1], [0.1 0; 0 1] and [1 0; 0 0.1]. Similarly, we introduce an additional outlier, i.e. [100, 100]. This experiment consists of two kinds of tests. In the first test, which discards the outlier, the</figDesc><graphic coords="6,354.25,467.30,169.90,153.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Fig. 2. One-dimensional projections of three methods on four datasets: (a) Wisconsin Breast Cancer database, (b) SPECTF Heart dataset, (c) SPECT Heart dataset, (d) Ionosphere database</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 3 .</head><label>3</label><figDesc>Two-dimensional projections on Iris Plants database dataset: (a) LDA-L2, (b) LDA-R1, (c) LDA-L1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. Some samples without/with occlusion in ORL database</figDesc><graphic coords="8,92.90,549.50,159.80,55.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Classification accuracy versus feature dimension on ORL database with occlusion</figDesc><graphic coords="8,371.40,170.30,135.50,116.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>(b). We design four tests called Test #1, Test #2, Test #3 and Test #4. Each test uses three images of every subject for training and the rest for testing. The training set of each test includes the occluded image "Ia" and two images being left or right view of each subject as follows. In Test #1, the training set contains occluded "Ia", "Ib" and "Ic", in Test #2, the training set contains occluded "Ia", "Id", and "Ie", in Test #3, the training set contains occluded "Ia", "Ic" and "Id", and in Test #4, the training set contains occluded "Ia", "Ib" and "Ie". (a) (b) Fig. 6. Samples in partial FERET database: (a) Samples of two subjects, (b) image "Ia" with occlusion of six subjects</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Digit samples of Binary Alphadigits database</figDesc><graphic coords="9,82.20,537.60,181.10,56.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The remainder of this paper is organized as follows. In Section II, conventional LDA is introduced. The proposed LDA-L1 is presented and the feasibility of the algorithm is theoretically proved in Section III. The experimental results are reported in Section IV. Lastly, Section V concludes this paper.</figDesc><table><row><cell cols="7">problem, which simplifies it to solve single local optimal</cell></row><row><cell cols="7">projection vector implemented by an iteration algorithm and gains multiple local optimal projection vectors by a greedy Linear Discriminant Analysis Based on L1-Norm Maximization search method.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fujin Zhong, Jiashu Zhang</cell></row><row><cell></cell><cell cols="6">II. CONVENTIONAL LDA</cell></row><row><cell>Let</cell><cell cols="2">1 [] N ,...,  X x x</cell><cell>R</cell><cell>mN </cell><cell cols="2">be the given training samples,</cell></row><row><cell cols="7">where m and N are the dimensionality of the input space and the</cell></row><row><cell cols="7">number of samples. And suppose the given samples contain C</cell></row><row><cell cols="6">classes, the ith class C i has N</cell><cell>i samples (</cell><cell></cell><cell>i N </cell><cell>N</cell><cell>).</cell></row><row><cell cols="7">Conventional LDA (termed as LDA-L2) aims to find an optimal</cell></row><row><cell cols="2">projection matrix</cell><cell cols="5">n [] 1 ,...,  W w w</cell><cell>R</cell><cell>mn </cell><cell>(n&lt;m) whose</cell></row><row><cell cols="2">columns {w</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>T</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>k } (k=1,...,n) constitute the bases of the n-dimension linear subspace. Projecting the sample x j onto W yields an n-dimension vector y j , i.e. y j =W T</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I AVERAGE</head><label>I</label><figDesc>CORRECT CLASSIFICATION RATES ON ARTIFICIAL DATASET</figDesc><table><row><cell>Methods</cell><cell>Without outlier (%)</cell><cell>With outlier (%)</cell></row><row><cell>LDA-L2</cell><cell>97.15</cell><cell>91.75</cell></row><row><cell>LDA-R1</cell><cell>97.29</cell><cell>97.29</cell></row><row><cell>LDA-L1</cell><cell>97.29</cell><cell>97.29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III CORRECT</head><label>III</label><figDesc>CLASSIFICATION RATES, THE CORRESPONDING DIMENSIONALITY AND LEARNING TIME ON ORL DATABASE</figDesc><table><row><cell>Methods</cell><cell>Accuracy(%)</cell><cell>Dimension</cell><cell>Time(sec.)</cell></row><row><cell>LDA-L2</cell><cell>79.50</cell><cell>39</cell><cell>0.2754</cell></row><row><cell>LDA-R1</cell><cell>84.00</cell><cell>20</cell><cell>7.8839</cell></row><row><cell>LDA-L1</cell><cell>87.00</cell><cell>65</cell><cell>2.1224</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II OPTIMAL</head><label>II</label><figDesc>CORRECT CLASSIFICATION RATES AND LEARNING TIME ON FIVE STANDARD CLASSIFICATION DATASETS</figDesc><table><row><cell>Methods</cell><cell cols="2">Wisconsin Breast Cancer Accuracy(%) Time(sec)</cell><cell cols="2">SPECTF Heart dataset Accuracy(%) Time(sec)</cell><cell cols="2">SPECT Heart dataset Accuracy(%) Time(sec)</cell><cell cols="2">Ionosphere database Accuracy(%) Time(sec)</cell><cell cols="2">Iris Plants Accuracy(%) Time(sec)</cell></row><row><cell>LDA-L2</cell><cell>86.88</cell><cell>0.0018</cell><cell>67.91</cell><cell>0.0135</cell><cell>76.47</cell><cell>0.0057</cell><cell>70.12</cell><cell>0.0061</cell><cell>97.50</cell><cell>0.0021</cell></row><row><cell>LDA-R1</cell><cell>91.55</cell><cell>0.0305</cell><cell>77.54</cell><cell>0.3508</cell><cell>78.07</cell><cell>0.1842</cell><cell>73.56</cell><cell>0.2276</cell><cell>97.50</cell><cell>0.0163</cell></row><row><cell>LDA-L1</cell><cell>92.91</cell><cell>0.0119</cell><cell>77.54</cell><cell>0.0887</cell><cell>81.28</cell><cell>0.0085</cell><cell>83.67</cell><cell>0.0314</cell><cell>97.50</cell><cell>0.0102</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V THE</head><label>V</label><figDesc>LEARNING TIME (SEC.) ON PARTIAL FERET DATABASE</figDesc><table><row><cell>Methods</cell><cell>Test #1</cell><cell>Test #2</cell><cell>Test #3</cell><cell>Test #4</cell><cell>Average</cell></row><row><cell>LDA-L2</cell><cell>5.093</cell><cell>5.156</cell><cell>5.163</cell><cell>5.169</cell><cell>5.145</cell></row><row><cell>LDA-R1</cell><cell>203.526</cell><cell>210.137</cell><cell>335.058</cell><cell>334.267</cell><cell>270.747</cell></row><row><cell>LDA-L1</cell><cell>33.521</cell><cell>52.018</cell><cell>46.706</cell><cell>31.181</cell><cell>40.857</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI OPTIMAL</head><label>VI</label><figDesc>CORRECT CLASSIFICATION RATES AND LEARNING TIME ON BINARY ALPHADIGITS DATASET</figDesc><table><row><cell>Methods</cell><cell cols="2">6 Training Sample Accuracy(%) Time(sec)</cell><cell cols="2">9 Training Sample Accuracy(%) Time(sec)</cell><cell cols="2">12 Training Sample Accuracy(%) Time(sec)</cell><cell cols="2">15 Training Sample Accuracy(%) Time(sec)</cell><cell cols="2">18 Training Sample Accuracy(%) Time(sec)</cell></row><row><cell>LDA-L2</cell><cell>74.24</cell><cell>0.0074</cell><cell>71.67</cell><cell>0.0196</cell><cell>74.81</cell><cell>0.0435</cell><cell>76.67</cell><cell>0.0772</cell><cell>69.52</cell><cell>0.1316</cell></row><row><cell>LDA-R1</cell><cell>75.76</cell><cell>0.2374</cell><cell>81.67</cell><cell>0.6213</cell><cell>84.81</cell><cell>1.4126</cell><cell>88.75</cell><cell>2.0334</cell><cell>89.04</cell><cell>4.5649</cell></row><row><cell>LDA-L1</cell><cell>79.09</cell><cell>0.2252</cell><cell>85.00</cell><cell>0.2453</cell><cell>88.15</cell><cell>0.7625</cell><cell>90.42</cell><cell>0.9771</cell><cell>90.00</cell><cell>1.1249</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors also extend special thanks to the associate editor and all reviewers for their constructive comments and advice.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was partially supported by National Science Foundation of P. R. China (Grant: 61271341 and 60971104), Research Fund for the Doctoral Program of Higher Education of China (Grant: 20090184110008), and Science &amp; Technology Key Plan Project of Chengdu (Grant: 10GGYB649GX-023).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
		<title level="m">Principal Component Analysis</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Use of Multiple Measures in Taxonomic Problems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Eugenics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="179" to="188" />
			<date type="published" when="1936">1936</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Subspace methods for robot vision</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Nene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Murase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="750" to="758" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An improvement of the auto-correlation matrix in pattern matching method and its application to handprinted &quot;HIRAGANA</title>
		<author>
			<persName><forename type="first">H</forename><surname>Murase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yoshimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Miyake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on IECE J64-D</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Eigenfaces for Recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="86" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-view human movement recognition based on fuzzy distances and linear discriminant analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Iosifidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tefas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nikolaidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="347" to="360" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Eigenfaces vs fisherfaces: recognition using class specific linear projection</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Hespanha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using discriminant Eigenfeatures for image retrieval</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Swets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Weng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="831" to="836" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">PCA versus LDA</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="233" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the unification of line processes, outlier rejection, and robust statistics with applications in early vision</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rangarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="91" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Robust subspace computation using L1 norm</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<idno>CMU-CS-03-172</idno>
		<ptr target="http://citeseer.ist.psu.edu/ke03robust.html" />
		<imprint>
			<date type="published" when="2003-08">Aug. 2003</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon Univ</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A L1-Norm PCA and a heuristic approach, Ordinal and Symbolic Data Analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baccini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Besse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Falguerolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ordinal and Symbolic Data Analysis</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Diday</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lechevalier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Opitz</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="359" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">L1-norm-based common spatial patterns</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="653" to="662" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust tensor analysis with L1-norm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="172" to="178" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust L1 norm factorization in the presence of outliers and missing data by alternative convex programming</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Int. Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="739" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">R1-PCA: Rotational invariant L1-norm principal component analysis for robust subspace factorization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="281" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Principal component analysis based on L1-norm maximization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="672" to="1680" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">L1-norm-based 2DPCA</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syts., Man, Cybern. B</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1170" to="1175" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Compressed signal reconstruction using the correntropy induced metric</title>
		<author>
			<persName><forename type="first">S</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Principe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech, and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="3845" to="3848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bregman iterative algorithms for L1-minimization with applications to compressed sensing</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goldfarb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Darbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="143" to="168" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Maximum correntropy criterion for robust face recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1561" to="1576" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Linear discriminant analysis using rotational invariant L1 norm</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="2571" to="2579" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Algorithms and networks for accelerated convergence of adaptive LDA</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matinfar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M S</forename><surname>Sadough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="473" to="483" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<title level="m">Introduction to Statistical Pattern Recognition, 2nd Edition</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Regularized discriminant analysis for face recognition</title>
		<author>
			<persName><forename type="first">I</forename><surname>Pima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aladjem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1945" to="1948" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A two-stage Linear Discriminant Analysis via QR-Decomposition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="929" to="941" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A direct LDA algorithm for high-dimensional data ---with application to face recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2067" to="2070" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Structured sparse principal component analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf</title>
		<meeting>Int. Conf<address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">May 13-15, 2010</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The Nearest Subclass Classifier: A Compromise between the Nearest Mean and Nearest Neighbor Classifier</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Veeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J T</forename><surname>Reinders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1417" to="1429" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">UCI Machine Learning Repository</title>
		<author>
			<persName><forename type="first">A</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Asuncion</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">The Orl Face</forename><surname>Database</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">K</forename><surname>Cambridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">At&amp;t</forename><surname>Olivetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">)</forename><surname>Research</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laboratories</forename></persName>
		</author>
		<ptr target="http://www.uk.research.att.com/facedatabase.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The FERET database and evaluation procedure for face recognition algorithms</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Rauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Comput</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="295" to="306" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<ptr target="&lt;http://www.cs.nyu.edu/~roweis/data.html&gt;" />
		<title level="m">Binary Alphadigits database</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
