<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Kinship Recognition of Families in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Joseph</forename><forename type="middle">P</forename><surname>Robinson</surname></persName>
							<email>robinson.jo@husky.neu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Shao</surname></persName>
							<email>mshao@umassd.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yue</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hongfu</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Timothy</forename><surname>Gillis</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
							<email>yunfu@ece.neu.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<postCode>02115</postCode>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">University of Massachusetts Dartmouth</orgName>
								<address>
									<postCode>02747</postCode>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Electrical and Computer Engineering</orgName>
								<orgName type="department" key="dep2">College of the Computer and Information Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<postCode>02115</postCode>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Visual Kinship Recognition of Families in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">273A592066E38086B88F9CC1CD481621</idno>
					<idno type="DOI">10.1109/TPAMI.2018.2826549</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2018.2826549, IEEE Transactions on Pattern Analysis and Machine Intelligence</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Large-Scale Image Dataset</term>
					<term>Kinship Verification</term>
					<term>Family Classification</term>
					<term>Semi-Supervised Clustering</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present the largest database for visual kinship recognition, Families In the Wild (FIW), with over 13, 000 family photos of 1, 000 family trees with 4-to-38 members. It took only a small team to build FIW by designing an efficient labeling tools and work-flow. To extend FIW, we further improved upon this process with a novel semi-automatic labeling scheme that used annotated faces and unlabeled text metadata to discover labels, which were then used, along with existing FIW data, for the proposed clustering algorithm that generated label proposals for all newly added data-both processes are shared and compared in depth, showing great savings in time and human input required. Essentially, the clustering algorithm proposed is semi-supervised and uses labeled data to produce more accurate clusters. We statistically compare FIW to related datasets, which unarguably shows enormous gains in overall size and amount of information encapsulated in the labels. We benchmark two tasks, kinship verification and family classification, at scales incomparably larger than ever before. Pre-trained CNN models fine-tuned on FIW outscores other conventional methods and achieved state-of-the art on the renowned KinWild datasets. We also measure human performance on kinship recognition and compare to a fine-tuned CNN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>V ISUAL kinship recognition has an abundance of prac- tical uses, such as issues of human trafficking and in missing children, problems from today's refugee crises, and social media platforms. Use cases exist for the academic world as well, whether for machine vision (e.g., reducing the search space in large-scale face retrieval) or a different field entirely (e.g., historical &amp; genealogical lineage studies). However, to the best of our knowledge, no reliable system exists in practice. This is certainly not due to a lack of effort from researchers, as there are already many works focused on kin-based problems <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>.</p><p>Challenges preventing visual kinship recognition from transitioning from research to reality are two-fold:</p><p>• Existing data resources for visual kinship are too small to capture true data distributions.</p><p>• Hidden factors of visual similarities/differences between blood relatives are complex and less discriminant than in other more conventional problems (e.g., object classification or even facial identification).</p><p>Clearly, a large image-set that properly represents families worldwide is needed, which also meets the capacity of more complex, data-driven models (i.e., deep learning), thus, motivating us to build the first large-scale image database for kinship recognition, Families In the Wild (FIW). FIW is made-up of rich label information that captures the complex, hierarchical structures of 1,000 unique family trees.</p><p>Families consist of an average of about 13 each (i.e., over 13,000 family photos), and family sizes range from 3-38 members, with most subject having multiple samples at various ages. FIW is the largest and most comprehensive database of its kind. 1  Deep learning can now be applied to the problem, as we demonstrate on two benchmarks, kinship verification and family classification. We fine-tune deep models to improve all benchmarks, and provide details on the training procedure. We also measure human performance on verification and compare with benchmarks.</p><p>We use a multimodal labeling model to optimize the annotation process. This includes a novel semi-supervised clustering method that works effectively in practice (i.e., generates label proposals for new data using existing labeled data as side information). For this, we increase the amount of available side information using existing labels (i.e., names), labeled faces, and text metadata collected with the family photos. We show a significant reduction in manual labor and time spent on labeling new data.</p><p>Families In the Wild (FIW) was first introduced in <ref type="bibr" target="#b22">[23]</ref>. This work adds to the previous work in a number of ways. Listed here are contributions made in this journal extension: 1) Added additional faces for verification and complete families for classification (Section 3). 2) Improved the labeling process with novel semisupervised clustering method (Section 4). 3) Boosted baseline scores using up-to-date deep learning approaches (Section 5). 4) Obtained state-of-the-art, on smaller datasets, via transferring CNN model fine-tuned on FIW. 5) Conducted kinship verification experiment on humans and compared with algorithms (Section 5.6). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS 2.1 Related Databases</head><p>The story of visual kinship recognition begins in 2010, at which time the first kin-based image collection (i.e., Cornel-lKin) was made public <ref type="bibr" target="#b0">[1]</ref>. CornellKin contains 150 parentchild face pairs (i.e., celebrities and their parents). Next came UB KinFace-I &amp; II <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, which addressed a new view of the problem-young and old face images of parents are paired with each child, with 600 face photos of 400 unique subjects (i.e., celebrities and politicians from the web). Then, KinWild I-II <ref type="bibr" target="#b25">[26]</ref> was released and used in a 2015 FG Challenge <ref type="bibr" target="#b26">[27]</ref>, also with parent-child pairs. Soon thereafter, Family101 <ref type="bibr" target="#b6">[7]</ref> was released as the first attempt of multi-class classification (i.e., one-to-many) for kinship recognition. Thus, it is an organized set of structured families <ref type="bibr" target="#b6">[7]</ref>, including 206 sets of parents and their children (i.e., core families) that belong to 101 unique family trees.</p><p>In 2015, TSKinFace <ref type="bibr" target="#b11">[12]</ref> was introduced in support of a new view of kinship recognition, tri-subject verification, where both parents and a child are used-Father/Mother-Daughter (FM-D) and Father/Mother-Son (FM-S) (i.e., twoto-one verification). TSKinFace contains a total of 513 FM-D and 502 FM-S pairs. However, even after all these contributions, there existed no single resource that satisfied the concerns of insufficient data. A single resource with the features of previous works, but in a more complete and abundant manner, was the underlying vision for FIW. As shown in Tables <ref type="table" target="#tab_0">1</ref> &amp;<ref type="table" target="#tab_2">2</ref>, and discussed in later sections, FIW far exceeds others in terms of number of families, face pairs, and relationship types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Automatic Kinship Recognition</head><p>As mentioned, Fang et al. <ref type="bibr" target="#b0">[1]</ref> first attempted kinship verification on parent-child face pairs. They proposed selecting the 14 (of 44) most effective hand-crafted features. Following this, researchers recognized that a child's face more closely resembles their parents at younger ages <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. In response, they used transfer subspace learning methods that uses the younger faces of parents to help fill the appearance gap between their older faces and that of their children. To benchmark the KinWild dataset, Lu et al. <ref type="bibr" target="#b27">[28]</ref> proposed a metric learning method used in Euclidean space called NRML and its multi-view counterpart (MNRML) that learns a common distance metric for multiple feature types. Fang et al. <ref type="bibr" target="#b6">[7]</ref> focused on one-to-many (i.e., family classification) by representing faces as a linear combination of sparse features (i.e., feature selection via lasso) of 12 facial parts encoded via a learned dictionary.</p><p>Progress made in kinship recognition, along with release of varying task protocols, coincides with an increasing availability of structured and labeled data. Although there have been several significant contributions, none have overcome the challenges posed earlier. Thus, a greater resource was needed to allow the potential for progress to increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Deep Kinship Recongition</head><p>Since the Alex Net CNN <ref type="bibr" target="#b28">[29]</ref> won the 2012 ImageNet Challenge <ref type="bibr" target="#b29">[30]</ref>, deep learning has achieved state-of-the-art in a wide range of machine learning tasks. Central to this frenzy has been facial recognition <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. In spite of this, there are only a few works on deep learning for kinship recognition <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>.</p><p>Deep learning has yet to show an advantage for visual kinship recognition, with metric learning seeming more promising. As mentioned in a recent literature review <ref type="bibr" target="#b37">[38]</ref>, the reason for this is due to insufficient amounts of data. In this work, we include several benchmarks on FIW using deep learning, obtaining a clear advantage in both tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Semi-Automatic Image Tagging &amp; Data Exploration</head><p>Automatic image tagging was recently done by first labeling a small amount of the data, and then using it as side information to help guide the clustering process in a semisupervised manner <ref type="bibr" target="#b38">[39]</ref>. Following this, we take advantage of side information from labeled FIW.</p><p>Previous works used image captions, whether from Flickr or other sources of images tagged by users, to discover labels and annotate images in an automatic fashion <ref type="bibr" target="#b39">[40]</ref>. Generally, methods mining text for image tags treat it as a problem of noisy labels <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>. CASIA-WebFace <ref type="bibr" target="#b42">[43]</ref>, a large-scale dataset for facial recognition, successfully extended the scale of the renowned LFW <ref type="bibr" target="#b43">[44]</ref>. By crawling the web, and leveraging knowledge from IMDB, multiple face samples for 10, 000 unique subjects were collected. Although related in the sense of automatic labeling, these problems are very different from the one we present here. We aim to add more data to underrepresented families of the FIW database, and doing so by using the existing labels for each family as side information to guide our semi-supervised clustering method. We wish to maximize the number of labeled faces available to facilitate the clustering in order to generate label proposals. For this, we use the existing FIW labeled faces and the text metadata of the unlabeled data to automatically tag faces using an iterative process governed by both visual and contextual evidence. As discussed in Section 5.4, our method consistently improves with increasing amounts of side information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FAMILIES IN THE WILD (FIW) DATABASE</head><p>We next cover the FIW database. First, we review the existing FIW and old labeling scheme <ref type="bibr" target="#b22">[23]</ref>. Then, we introduce the improved semi-automatic labeling process. Finally, we compare the two. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Existing FIW</head><p>Our goal for FIW was to collect and label about 10 family photos for 1,000 unique families. Ground-truth labels are of 2 types, photo-level (i.e., who is where in the image) and family-level (i.e., all members present, and the relationships between one another). Fig. <ref type="figure" target="#fig_3">4</ref> depicts the label types. FIW is organized as follows: each family is assigned a unique ID (i.e., FID), and pictures collected are also assigned a unique ID (i.e., PID). Finally, members added are assigned their own unique ID (i.e., MID). For instance, F ID 1 → MID 1 in P ID 1 refers to the first member of the first family in the first photo collected. The order of IDs is ambiguous, as assignments were made in the order that the family, member, and photo were added. Before introducing the new and improved semi-automatic process, we briefly review the process used initially in <ref type="bibr" target="#b22">[23]</ref>, which involved 3 steps: (1) Data Collection, (2) Data Labeling, and (3) Data Parsing.</p><p>For Data Collection, a team of 8 students from different parts of the world and with vast knowledge of famous persons, compiled a list of families and collected images with a primary focus on their place of origin (i.e., an attempt to compile a diverse family list). Table <ref type="table" target="#tab_3">3</ref> lists the ethnicity distributions of the 1,000 families. Note that this is not the exact distribution, as each family is counted once according to the root member for which the search was based (i.e., not per member, but per family). For instance, for Spielberg's family we consider just Stephen. Future work could entail adding more families from underrepresented ethnic groups, as the distribution still favors Caucasians.</p><p>For Data Preparation, we built a labeling tool to guide the process of generating the two label types. Labelers would work through all family photos on a family-by-family basis, specifying who is in each photo by clicking member faces and choosing their names from a drop-down menu. Names, genders, and relationships for members were only entered on the first image they are present-once added to the family the labelers just selected their names each time they appeared in a photo. For Data Parsing, all family photos were detected using classic HOG features trained on top of a linear classifier using image pyramids and sliding windows via DLIB <ref type="bibr" target="#b44">[45]</ref>. Faces were cropped and normalized as done in <ref type="bibr" target="#b45">[46]</ref>, and then resized to 224 × 224. Finally, the structure of the database was organized into a hierarchy of directories, FID→MID→Face-ID (i.e., 1, 000 folders, F 0001-F 1000, containing family labels and folders for MIDs with face samples of that member).</p><p>Even though it only took a small team to label 10,676 family photos and 1,000 families, the process relied heavily on human input. Plus, in the end, many families were not properly represented (i.e., either too few members, face samples, or family photos). Thus, we aim to reduce the manual labor and overall time requirements to add additional data provided various amounts of labels existed for each (i.e., 61 existing families and 4 replacement). We added replacement families (i.e., newly added families) to make up for cases of overlapping families or an insufficient online presence when searching for photos (i.e., unable to locate family photos for 2 of the under-represented families). Before we propose the semi-automatic labeling model, we first review the two benchmarks included in this work, along with the related statistics of each. We then present the new labeling process that enabled us to add additional data with far less manual labor and in just a fraction of the time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Preparation</head><p>Due to the nature of the label structure, FIW can serve as a resource for various types of vision tasks. For this, we benchmark the two popular tracks, kinship verification and family classification. Next, we introduce both these tasks and the means of preparing the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Kinship verification</head><p>Kinship verification aims to determine whether two faces are blood relatives (i.e., kin or non-kin). Prior research mainly focused on parent-child pairs (i.e., father-daughter (F-D), father-son (F-S), mother-daughter (M-D), and motherson (M-S)); some considered sibling pairs (i.e., brotherbrother (B-B), sister-sister (S-S), and brother-sister/mixed gender siblings (SIBS)). However, research in both psychology and computer vision revealed that different kin relations render different familial features, which motivated researchers to model different relationship types independently. With the existing image datasets used for kinship verification limited to, at most, 1,000 faces and typically only 4 relationship types, we believe such minimal data leads to overfitting and, hence, models that do not generalize well to unseen data captured in the wild. FIW currently supports 11 relationship types, 4 being introduced to the research community for the first-time (i.e., grandparent-grandchild) and, most importantly, each category is made up of many more pairs-418,000 face pairs in <ref type="bibr" target="#b22">[23]</ref> has increased to 644,000 after extending FIW via the proposed semi-supervised approach. The 11 relationship types provide a more accurate representation for real-world scenarios. As mentioned, FIW is structured such that the labels can be parsed for different types of tasks and experiments, and additional kinship types can easily be inferred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Family classification</head><p>Family classification aims to determine the family an unknown subject belongs to. Families are modeled using the faces of all but one family member, with the member left out used for testing. This one-to-many classification problem is a challenging problem that gets more challenging with more families. This is becuase families contain large intraclass variations that typically fool the feature extractors and classifiers, and each additional family further adds to the complexity of the problem. Additionally, and like conventional facial recognition, when the target is unconstrained faces in the wild <ref type="bibr" target="#b43">[44]</ref> (e.g., the variation in pose, illumination, expression, etc.), the problem continues to become more difficult. In <ref type="bibr" target="#b22">[23]</ref>, the experiment included only 316 families (i.e., families with 5+ members). In this extended version, we now can include 524 families with the added data. We next present the process followed to extend FIW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Extending FIW</head><p>The goal is to use the proposed semi-supervised model to generate label proposals for new data while using existing labels as side information to yield more accurate clusters. As explained in Section 5.4 and shown in Fig. <ref type="figure">7</ref>, the proposed clustering method only improves with more side information. Thus, we want to maximize the amount of side information (i.e., labeled faces) available. We do this by inferring highly confident labels by aligning faces and names from the unlabeled photos and corresponding text metadata. In addition, and when available, we model labeled data to discriminate between family members in a photo. In the end, clusters are saved as ground truth upon being verified by a human. Significant savings, in manual labor and overall time, resulted from using this labeling process.</p><p>A single family is processed at a time to reduce both the search and label spaces. We aim to discover labels with evidence from multiple modalities (i.e., visual and contextual information). This not only increases the amount of side information available for clustering, but also the sample count to use for discovering more labels as facename pairs. The cluster assignments (i.e., label proposals) were then manually inspected (i.e., validated).</p><p>We demonstrate the effectiveness of the new labeling scheme by comparing the number of user inputs (i.e., mouse clicks and keystrokes) and overall time with the process followed in <ref type="bibr" target="#b22">[23]</ref>. It took just a few inputs and a few minutes on average per family, opposed to hundreds of inputs and several minutes to over an hour (see Table <ref type="table" target="#tab_2">2</ref>). FIW now contains a larger amount of data.</p><p>We next explain the improved multi-modal scheme as a four-step process: (1) Data Collection, (2) Data Preparation, (3) Label Generation, and (4) Label Validation. The goal of (1) and ( <ref type="formula" target="#formula_3">2</ref>) is to prepare for clustering by gathering and increasing the side information available for (3), while (4) is to ensure correct label assignments for all newly collected data. In other words, we set out to increase the labeled sample pool (i.e., side information) by inferring labels for unlabeled faces, which adds to the set of training exemplars. The faces that are still unlabeled in (3) are clustered using all labels as side information. All newly added data is then verified by a human. The process is illustrated in Fig. <ref type="figure" target="#fig_4">5</ref>, which we next describe step-by-step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Step 1: Data Collection</head><p>The goal is to collect additional data for under-represented families of FIW. These families lacked in the number of members, faces, and/or family photos. In total, there were 65 families that were extended, with 1 family replaced due to a lack of available data, and 3 other overlapping families that were merged (i.e., Catherine, Duchess of Cambridge, along with her immediate family, merged with the Royal family, as her spouse Prince William share 2 children and, thus, the two families (i.e., sets of in-laws) are connected by kinship). Several new labels and relationships resulted from this merge, as the Royal family went from having 29 to 38 members, which is the largest family of FIW.</p><p>There were 2 requirements for the data collection: (1) rich text metadata that described the subjects in the photos and With the existing FIW labels, we next aim to increase the amount, both in labeled faces and member labels, using multiple modalities-names in metadata and scores of SVMs are used to automatically label some unlabeled data-face-name pairs were assumed labeled for cases of high confidence. Starting from profile pictures (i.e., 1 face, 1 name) and working towards less trivial scenarios (e.g., 3 faces and 2 names, with 2 faces from 1 member at different ages, like in P ID 3 ). This step adds to the amount of side information used for clustering Label Generation. Label proposals for remaining unlabeled faces are generated using the proposed semi-supervised clustering model that leverages labeled data as side information to better guide the process. Label Validation. A GUI designed to validate clusters and ensure clusters are matched to the proper labels. (2) at least 1 portrait face (or profile picture) for new family members. These requirements are for Step 2, as the label expansion is done for all new members by mining the trivial face-name pairs of portrait photos-portraits can align the single face to the single name with high confidence (more details provided in the following step).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Step 2: Data Preparation</head><p>We now aim to maximize the amount of side information available for the clustering process. For this, we took advantage of both labeled (i.e., faces and names) and unlabeled data (i.e., detected faces and text metadata) in order to automatically infer labels for some unlabeled faces (see Data Preparation in Fig. <ref type="figure" target="#fig_4">5</ref>). We next break down the different components used in this step for clarity.</p><p>Text metadata (i.e., image captions) are collected with all photos in Step 1. For each family, all metadata are processed using a Name Entity Recognition (NER) classifier <ref type="bibr" target="#b47">[48]</ref> to output a list of detected names. Then, a Look-Up- To address this, we first generated a LUT with detected references to each subject, and then augmented the search using additional tags (e.g., adding titles and last names). The LUTs are later used to find evidence in the text metadata of family members' presence in a photo.</p><p>New MIDs found in profile photos (e.g., P ID 1 in Fig. <ref type="figure" target="#fig_4">5</ref>)when processing a family, each image and that has a single face detected and just one name in its metadata is considered a profile photo. Profile photos are first to be processed. The name detected in the metadata is compared to all names for members stored in the LUTs. If there are no matches, the subject is then added as a new member in that family. A LUT of names is then generated for each new member, and the name of highest frequency (i.e., number of detection in all metadata) recorded as the name corresponding to their assigned MID (e.g., MID 6 for the sixth member).</p><p>Unlabeled and labeled faces are encoded as 4, 096D features from the fc 7 -layer of the pre-trained VGG-Face CNN model <ref type="bibr" target="#b31">[32]</ref>. One-vs-rest Support Vector Machine (SVM) models are trained for each member using labeled samples from all other members of that family as the negatives.</p><p>Next, profile photos are processed (i.e., 1 name and 1 face). Names that match an existing label are added to corresponding MID data pools, while mismatched names are added as a new MID with a LUT generated. This shows the benefit of including profile pictures for each new member, which makes it so all family members are known.</p><p>It is important to note that SVMs are updated each time a new labeled face is added.</p><p>Discovering labels continues in a similar fashion, except now the SVMs play a more critical role. Now moving on to images with 2 faces and 2 names, using the 2 SVMs of the respective members to classify the two faces. Provided high scores and no conflicts, labels were inferred. Cases with low confidence or conflicts were skipped, leaving those faces to be labeled via clustering. Next, photos with 3 faces and 3 names are processed, then 4 and 4, and so on and so fourth. After all one-to-one cases are processed, photos with a different number of names and faces are processed.</p><p>For each photo, only SVMs that correspond to a LUT with matching names are used. Thus, justifying a requirement of</p><p>Step 1-collect rich metadata in terms of specifying members present in photos. It should also be noted that some families benefited far more than others in this process. Nonetheless, roughly 25% of the 2, 973 added faces were correctly labeled by this simple multi-modal process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Step 3: Label Generation</head><p>Label proposals were generated for unlabeled faces using the proposed semi-supervised clustering method that takes advantage of labeled data as side information to guide the clustering. To get the most out of our model we automatically labeled additional data in Step 2, while identifying all new members being added to each family. Hence, the number of members (i.e., k) is known for each family.</p><p>More details, including the objective function and solution, are provided in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Step 4: Label Validation</head><p>Finally, clusters (i.e., labels) are validated by a human. This is a three-part process: validate and assign an MID to each cluster; validate each cluster, which is displayed in a grid of faces in the order of confidence score; specify gender and relationships of newly added members. As shown in Fig. <ref type="figure" target="#fig_4">5</ref>, a JAVA interface was designed to generate groundtruth for new data with just a few clicks of the mouse and no more than a couple of minutes per family. The inputs are cluster assignments for a family, with faces listed in order of confidence (i.e., cosine distance from centroid). MIDs were assigned in Step 2 (i.e., inferred from text, SVM scores, or both), which must also be validated. The outputs are labels for each PID and an updated relationship matrix (Fig. <ref type="figure" target="#fig_3">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.5">Discussion</head><p>Seven families of various sizes were used to compare the old <ref type="bibr" target="#b22">[23]</ref> and proposed labeling schemes-old scheme took 4,124 inputs in about 2.75 hours, and just 95 inputs in about 18.1 minutes via the new (see Table <ref type="table" target="#tab_4">4</ref>). Collecting and labeling the data for the extended FIW was done by a single person in days; it initially took a small team several months with the old scheme. Thus, demonstrating a significant savings in manual labor and time. A possible future direction is to use this scheme to extend families of FIW with video data. Another possibility is to use this method to extend the number of families, which, if on the order of thousands or more, then automating Step 1 could further reduce savings (i.e., web scrape for family information (e.g., Wiki) and photos (e.g., Google, Bing, etc.)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SEMI-SUPERVISED FACE CLUSTERING</head><p>Labeling is a human-necessary and expensive task for benchmarking data sets. Here we aim to accelerate the process by using some labeled data in advance. In this part, we demonstrate a novel semi-supervised clustering for labeling. Let X = {x i } be the data matrix with n instances and m features and S be a n × K side information matrix, which denotes n labeled data instances into K classes. Our goal is to make use of S to guide the remaining instances into K classes, where K ≤ K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Objective Function</head><p>Inspired by our previous work <ref type="bibr" target="#b38">[39]</ref>, a partition level constraint is used to make the learnt partition agree with partial human labels as much as possible. To demonstrate the  effectiveness of our labeling mode, K-means with cosine similarity is employed as the core clustering method to handle high-dimensional data due to its high efficiency and robustness. The following is our objective function,</p><formula xml:id="formula_0">min K k=1 xi∈C k f cos (x i , m k ) + λU c (S, H ⊗ S),<label>(1)</label></formula><p>where f cos is the cosine similarity, H is the final partition, H S = H ⊗ S is part of H which the instances are also in the side information S, m k is the centroid of C k , U c is the well-known Categorical Utility Function <ref type="bibr" target="#b48">[49]</ref> and λ is the trade-off parameter.</p><p>To better understand the last term in Eq. 1, we give the detailed calculation of U c . Given two partitions S and H S containing K and K clusters, respectively. Let n (S) kj denote the number of data objects belonging to both cluster</p><formula xml:id="formula_1">C (S) j in S and cluster C k in H S , n k+ = K j=1 n (S)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>kj , and n</head><formula xml:id="formula_2">(S) +j = K k=1 n (S) kj , 1 ≤ j ≤ K , 1 ≤ k ≤ K. Let p (S) kj = n (S) kj /n , p k+ = n k+ /n , and p (S) +j = n (S)</formula><p>+j /n . We then have a normalized contingency matrix (NCM), based on which a wide range of utility functions can be accordingly defined. For instance, the widely used category utility function can be computed as follows:</p><formula xml:id="formula_3">U c (H S , S) = K k=1 p k+ K j=1 ( p (S) kj p k+ ) 2 - K j=1 (p (S) +j ) 2 . (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>It is worthy to note that U c measures the similarity of two partitions, rather than two instances. The larger value of U c indicates the higher similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Solution</head><p>We notice that the first term in Eq. 1 is the standard Kmeans with cosine similarity. Could we still apply K-means optimization to solve the problem in Eq. 1? The answer is yes! Due to our previous work <ref type="bibr" target="#b49">[50]</ref>, we provide a new insight of U c by the following lemma.</p><p>Lemma 1. Given a fixed partition S, we have</p><formula xml:id="formula_5">U c (H S , S) = -||S -H S G|| 2 F + constant, (<label>3</label></formula><formula xml:id="formula_6">)</formula><p>where G is the centroid matrix of S according to H S .</p><p>By the above lemma, the second term in Eq. 1 can also be transformed into a K-means problem with squared Euclidean distance. Then a K-means-like algorithm can be used on the augmented matrix with modified distance function and centroid update rule for the final partition.</p><p>First an augmented matrix D is introduced as follows.</p><formula xml:id="formula_7">D = ⎡ ⎣ X S S X T 0 ⎤ ⎦ with X = ⎡ ⎣ X S X T ⎤ ⎦ , (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>where d i is the i-th row of D, which consists of two parts, d</p><p>i , d</p><p>(</p><p>i . The first two parts d</p><formula xml:id="formula_11">(1) i = (d i,1 , • • • , d i,dm</formula><p>) present the feature space and d</p><formula xml:id="formula_12">(2) i = (d i,dm+1 , • • • , d i,dm+K</formula><p>) denotes the label space. Zeros in D are the artificial elements, rather than the true values so that all zeros contribute to the computation of the distance and centroids, which inevitably interfere the cluster structure. To make the zeros in D not involved in the calculation, we give the new update rule for the centroids of D. Let m k = (m</p><formula xml:id="formula_13">(1) k , m (2)</formula><p>k ) be the k-th centroid C k of D, we modify the computation of centroids as follows.</p><formula xml:id="formula_14">m (1) k = di∈C k d (1) i |C k | , m<label>(2)</label></formula><formula xml:id="formula_15">k = di∈C k d (2) i |C k ∩ X S | . (<label>5</label></formula><formula xml:id="formula_16">)</formula><p>and the distance function is also adjusted as</p><formula xml:id="formula_17">f (d i , m k ) = f cos (d<label>(1)</label></formula><p>i , m</p><p>(1)</p><formula xml:id="formula_18">k ) + 1(d i ∈ S)f sqE (d<label>(2)</label></formula><p>i , m</p><p>(2)</p><formula xml:id="formula_19">k ).<label>(6)</label></formula><p>The correctness and convergence of the modified K-means is similar to one in <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We conduct four experiments: benchmark kinship verification and family classification; evaluate the proposed semisupervised clustering method at the core of the new labeling scheme; measure human performance on kinship verification and compare to top scoring algorithms.</p><p>The subsequent subsections are organized as follows. First, we review the visual features, metric learning methods, and deep learning that is common in all experiments. Then, we dive into the four experiments mentioned above. We introduce each independently, but with the same structure: experimental settings, experiment-specific training philosophy, and then results. Following this, we conclude the experiment portion of the report with a discussion highlighting key aspects and lessons learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setting</head><p>For the sake of organization, all low-level features and metric learning approaches used throughout are listed and described in this section (i.e., most are in two or more experiments, however, even those used for verification, for example, are still treated as common information, and, thus, is described alongside other items of preliminary information. Following concepts pertaining to "shallow" vision  methodology, we review specifications of the pre-trained CNNs used as off-the-shelf feature extractors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Feature Representations</head><p>Detected and aligned faces were normalized and encoded using low-level and CNN-based features. We next describe the descriptors used in this work-SIFT, LBP, pre-trained VGG-Face and ResNet CNNs-each having been widely used in visual kinship and facial recognition problems.</p><p>SIFT <ref type="bibr" target="#b50">[51]</ref> is amongst the most widely used feature type in object and face recognition. Here we follow the settings of <ref type="bibr" target="#b25">[26]</ref>: resize images to 64 × 64, then extract features from 16 × 16 blocks with a stride of 8 (i.e., 49 blocks that yields 128 × 49 = 6, 272D face feature).</p><p>LBP <ref type="bibr" target="#b51">[52]</ref> are renown for its effectiveness in tasks such as texture analysis and face recognition. We again follow the settings of <ref type="bibr" target="#b25">[26]</ref>: resize images to 64 × 64, divide into 16 × 16 non-overlapping blocks, and use a radius of 2 and sampling number of 8. Each block is represented as a 256D histograms (i.e., 256 × 16 = 4, 096D face encoding). VGG-Face <ref type="bibr" target="#b31">[32]</ref>, a pre-trained CNN with the topology of VGG-16: made-up of small convolutional kernels (i.e., 3 × 3) with a convolutional stride of 1 pixel. VGG-Face is trained on 2.6M face images of 2, 622 different celebrities. VGG has worked well on various face databases-97.4% in accuracy on YouTube Faces <ref type="bibr" target="#b52">[53]</ref>; 98.78% accuracy on Labeled Faces in the Wild <ref type="bibr" target="#b53">[54]</ref>. By removing the top two layers-softmax and last fully-connected layer (aka fc8-layer or fc 8 )-the CNN can be used as an off-the-shelf face encoder <ref type="bibr" target="#b54">[55]</ref>. Thus, models get trained on an auxiliary resource and employed on target data. Here, we fed faces through to the fc7-layer (aka fc 7 ), yielding a 4, 096D face encoding. ResNet-22 <ref type="bibr" target="#b55">[56]</ref> is a 22-layer residual CNN trained on CASIA-Webface <ref type="bibr" target="#b42">[43]</ref>. ResNet-22 has a different network topology than VGG (i.e., more layers made possible via skipping connections in residual blocks to ensure that the signal stays intact by superimposing an identity tensor). Faces were fed through to layer fc 8 (512D encoding).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Metric Learning</head><p>Metric learning is commonly used and, sometimes, designed for kinship problems. Four metric learning and graph embedding methods used previously for face-based problems are include: Information theoretic metric learning (ITML) <ref type="bibr" target="#b56">[57]</ref>, Discriminative Low-rank Metric Learning (DLML) <ref type="bibr" target="#b57">[58]</ref>, Locality Preserving Projections (LPP) <ref type="bibr" target="#b58">[59]</ref>, and Large Margin Nearest Neighbor (LMNN) <ref type="bibr" target="#b59">[60]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Deep Learning</head><p>Fine-Tuned CNNs. Centerface (CF) <ref type="bibr" target="#b55">[56]</ref> loss enhances the discriminative power of deeply learned features by adding a supervision signal to reduce the intra-class variations. SphereFace uses an angular softmax loss, and has most recently claimed state-of-the-art in facial recognition <ref type="bibr" target="#b60">[61]</ref>. We fine-tune both these CNNs on FIW.</p><p>Additionally, we include two state-of-the-art methods based on autoencoders (AE), graph regularized marginalized Stacked AE (GmDAE) <ref type="bibr" target="#b61">[62]</ref>, and marginalized denoising AE based metric learning (mDML) <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Kinship Verification</head><p>Kinship verification is a binary classification problem (i.e., true or false, aka kin or non-kin, respectfully). It is the one-toone view of kinship recognition, which is explained next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Experimental Setting</head><p>The protocol we followed is conventional in face-based tasks: 5-fold cross validation with no family-overlap between folds. There are 11 relationship types evaluated (statistics and types shown in Table <ref type="table" target="#tab_0">1</ref>).</p><p>For each pair type, we added negative (i.e., non-kin) pairs to the 5-folds-we randomly mismatched pairs in each fold until the number of negative and positive pairs are the same in each fold (i.e., negative pairs are added at random until it makes up 50% of the respective fold). Thus, the total number of positive and negative labels are equivalent.</p><p>For this task we included each feature, metric learning approach, and deep learning model listed above. We then fine-tuned the pre-trained CNN models on the FIW dataset, which is described in detail in the next subsection. To compare features, we computed cosine similarity between each pair, which was then compared to a threshold to classify each pair as either kin or non-kin.</p><p>Verification accuracy (i.e., average of 5-folds) and receiver operating characteristic (ROC) curves were used to evaluate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Training Philosophy</head><p>For ResNet-22 + CF, we fine-tuned the Centerface model on our FIW data. Training was done using four Titan X GPUs with a batch size of 256. The learning rate was initially set to 0.01, then drops to 0.001 and 0.0001 at the 800 and 1200 iterations, respectively. Training was complete after 1,600 iterations. The weight decay was set to 0.0005. For SphereFace <ref type="bibr" target="#b60">[61]</ref>, the settings are similar to ResNet-22+CF (i.e., same batchsize, learning rate, weight decay, and number of iterations), and with the angular margin set to 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Results</head><p>As listed Table <ref type="table" target="#tab_5">5</ref>, siblings pairs types tended to score the highest, followed by parent-child types, and then grandparentgrandchild. Thus, the wider the generational gap, the wider between appearances of faces.</p><p>SphereFace, which was fine-tuned on FIW, outperformed other benchmarks with an average accuracy of 69.18%, which is 1.31% and 2.11% better than ResNet-22+CF and mDML, respectively, which were top the scoring methods prior to the recent release of SphereFace. Also, out of the pretrained CNNs, VGG-Face scored 3.55% higher than ResNet-22, and both outperformed the low-level features (i.e., LBP &amp; SIFT). From such, encodings from VGG-Face were used as features for the metric learning and AE methods. Besides LMNN and DLML, which improved score by 0.15% and 1.30%, the other metric learning methods actually worsened the performance of the descriptors extracted from the pretrained VGG-Face CNN. This infers that faces encoded via VGG-Face are more discriminative when used off -the-shelf than when certain metrics are learned on top.</p><p>We show a significant boost in performance when finetuning CNNs on FIW data-all features from CNNs outperform the conventional shallow methods. The results show that the deep learning models better encode the complex representation needed to discriminate between kin/non-kin. An improvement to these benchmarks, perhaps via a deep network designed specifically for this task, is certainly a direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Family Classification</head><p>Family classification is a one-to-many problem. The goal is to determine which family an unseen subject came from. In other words, a set of families with a missing member to the model is provided. Then, the missing (i.e., unseen) members get classified as being from one of the families (i.e., closed form, as we currently assume that all members at test time belongs to one of the families modeled during training). We next review some details for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Experimental Setting</head><p>Data from 564 families by leaving a different single member out in each fold for testing, while data from all the other members were used for training (i.e., leave-one-out w.r.t. family members). Families with at least 5 members were used. Thus, the data was split into 5-folds with no family overlap between folds (i.e., a minimum of 4 family members for training and 1 for testing). Each fold contained roughly 2,700 images-about that many faces used to test each split, while about the rest, about 12,800 faces, were used for training (i.e., a total of 13,420 images).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Training Philosophy</head><p>VGG-Face and ResNet-22 CNNs were fine-tuned on FIW by replacing the loss layers of the pre-trained CNNs with a softmax loss to predict the 564 family classes. There were a few differences: VGG-16 used a fixed learning rate of 0.0001, a batch size of 128, and trained for 800 iterations on one Titan X GPU; ResNet-22 used the same batch size and number of iterations, but with a larger learning rate 0.001, which too was fixed. The larger learning rate was set for its batch normalization layer <ref type="bibr" target="#b64">[65]</ref>. For ResNet-22 + CF and SphereFace, we followed the same training process used for verification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Results</head><p>We report the accuracy scores for four runs (see Table <ref type="table" target="#tab_6">6</ref>).</p><p>As shown, the top-1 accuracy for modeling one-vs-rest linear SVMs on top of deep VGG-Face features was just 3.04%. Then, by replacing the softmax layer to target the number of families (i.e., 524), and fine-tuning on FIW, the top-1 accuracy was improved (i.e., +7.38% to 10.42%). ResNet-22, also fine-tuned by replacing softmax layer, showed the second to highest accuracy with 14.17%, which outscored the top performing CNN on verification (i.e., SphereFace). The top performance was obtained with the fine-tuned ResNet-22 using Centerface (CF) loss with 16.18%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Proposed Semi-Supervised Clustering</head><p>To demonstrate the effectiveness of our semi-supervised model, we cluster FIW data using various amounts of familylevel labels as side information. We simulate two settings for evaluation-all data and just unlabeled data-shown as bold and dotted lines, respectively (see Fig. <ref type="figure">7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Experimental Setting</head><p>We used 23, 979 faces from 996 family classes. Faces were encoded using a pre-trained VGG-Face (i.e., fc 7 ). We varied the ratio of unlabeled data to side information across the horizontal axis up to 50% percent of labeled clusters, while the y-axis denotes the clustering performance on the rest of the unlabeled data by NMI. We compared to a pairwise constrained clustering method, LCVQE <ref type="bibr" target="#b65">[66]</ref>, which is also a K-means-based constrained clustering method and transforms the partition level side information into 'mustlink' and 'cannot-link' constraints. We used K-means as a baseline (i.e., no side-information).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Results</head><p>Fig. <ref type="figure">7</ref> shows that more side information consistently boosts the performance of our method. Even on the unlabeled data, our method exceeds the K-means baseline, which further validates the effectiveness of using our method in semi-automatic labeling scenarios. For LCVQE, the pairwise constraints make the cluster structure unpredictable, vulnerable to deviate from the true one, and, thus, perform worse than the baseline. This shows that imposing hard constraints on side information, like 'must-link' and 'cannotlink', may even damper results. On the contrary, our model leverages the side information to only only improve when more is added.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Transfer-Learning Experiment</head><p>To demonstrate the FIW ability to generalize, we fine-tune the ResNet CNN model on the entire dataset and assess the model on a smaller, non-overlapping image collection. Specifically, we achieve state-of-the-art performance when fine-tuned CNN is used to encode faces of the renown KinWild datasets (see Table <ref type="table" target="#tab_7">7</ref>). For KinWild I, we get a 4% increase in performance with an overall accuracy of 82.4%; for KinWild II, there is a 5.6% improvement at 86.6%.</p><p>Although an improvement was obtained, it is interesting to note the significant boost in accuracy for KinWild I when comparing F-D to all other types, and especially M-D. Clearly, the small sample size provided by this dataset does not properly represent the data distribution of these pair types, while FIW has noticeably less variance between scores of parent-child types. Regardless of the high score had for F-D BIU, our fine-tuned network both outperforms all others in terms of overall accuracy, while providing less variation between scores. Though, again, this variance is considered to be a result of the sample size of the smaller dataset, as we see tighter score range amongst all parentchild types evaluated for FIW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Human Performance on FIW</head><p>We evaluate humans performing kinship verification with a subset of FIW pairs. Although others conducted similar experiments <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b66">[67]</ref>, this was done with a larger sample set made up of more relationship types (Case 1); additionally, we conducted a second evaluation for the Boolean case only (Case 2). We now discuss experimental settings, results, and analyses of the two human experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.1">Experimental Setting</head><p>We first sampled FIW to obtain a list of pairs representative of a fair data distribution (i.e., different and diverse families with faces of various ages). Face pairs, both positive and negative samples, were from different photos. Also, we used, at most, one positive and/one negative per member. We rigorously examined and, in some ways, handcrafted the list to prevent external factors from contaminating the experiment (i.e., replaced face images of poorer quality and pairs with famous people). Thus, efforts were spent to better ensure a fair, non-biased assessment. We also used just the faces to avoid anything besides facial appearance that impact human responses <ref type="bibr" target="#b67">[68]</ref>. The same list of images was used for both experiments: evaluating pairs per specific relationship types and for the Boolean case only.</p><p>We used a Google Form to collect responses, and the university network along with social media to recruit volunteers. Answers were anonymous, although we collected demographic information (i.e., ethnicity, country of origin, and gender). Some volunteers made submissions on both experiments; however, scores or answers were not revealed. Also, there was nearly a year between from when the two experiments were conducted-with the Boolean case being a follow-up experiment to analyze how specifying the specific relationship types influence responses. Users chose from predefined responses: Related, Unrelated, or Skip.</p><p>Participants were asked to Skip if they had prior knowledge of one or both subjects, regardless of any knowledge about relationships (i.e., any pair containing a face that could be named was to be skipped). Face pair-types were processed in no special order: a type-by-type basis for Case 1, then shuffled at random for Case 2. There were a total of 420 face pairs sampled from 9 categories (see Table <ref type="table">8</ref>).</p><p>We had 75 and 110 volunteers for Case 1 &amp; 2, respectively. No training of any sort was provided. In both cases, approximate distribution of demographics are as follows: 45% Caucasian, 35% Asian, 10% Hispanic/ Latino, 4% African American, and 1% Arab; 65% were born in the United States, 30% from China, and just over 1-2% from South America, Middle East, and the Philippines; 55% males and 45% Females. No specific demographics were targeted (i.e., a matter who volunteered on social media, per request of the authors, etc.). Future work could involve a greater emphasis on demographics, both in overall distribution of volunteers and intended analysis. Here, we hope to lay the framework for such a study, along with many other interesting directions a study on human ability to recognize kinship can take.</p><p>To compare human performance to benchmarks, we fine- tune SphereFace CNN on the 764 families that were not included in face pairs used for the human evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.2">Results</head><p>We summarize both cases of the human evaluation using box plots (see Fig. <ref type="figure">8</ref>).</p><p>In Case 1, the minimum scores across most categories are below random (i.e., &lt; %50). In response, we confirmed that no single person scored lowest in more than 1 of the 9 categories. Another observation is the distribution of averages, and its mean of 57.5%, had the smallest variance-no average below 50% or above 67.5%, which indicates that no single, or more than a few subjects, dominated the average scores for the better of the worse. Examining the pairs where errors were made, three conclusions can be made : (1) especially for relationship types spanning 1 or more generations (i.e., parents and grandparents), the common pairs consistently marked incorrectly are cases were the face of the expected elder is at a younger age or the face of the descendant appears older (e.g., grandfather in his thirties and grandson in his fifties); (2) different ethnic groups typically made common mistakes on face pairs of different backgrounds; (3) females often deviated from males on the mistakes made that are common and across different ethnic groups-varying females were always the top scorer, but never the same twice. Apparently, nature and nurture can play a role in humans' ability to do kinship verification as well. There are many interesting directions for future work (e.g., even larger and more diverse subject pool, or samples <ref type="bibr">TABLE 8</ref> Face pair counts for human evaluation on kinship verification. with added semantics like full body views or entire photos with background context).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SIBS P-C GP-GC Total</head><p>For Case 2, we evaluated humans' ability to recognize kinship in faces, but, this time, without specifying the relationship. From this, we were aiming to determine whether the relationship direction and face age impacted human responses. Overall, the mean values barely changed, however, the set of pairs commonly marked wrong did-relationship direction does seem to worsen human ability to recognize kinship when the direction of the relationship contradicts with the age appearance of face pairs; however, in cases without the age contradiction, knowledge of the relationship type helps humans to determine whether or not the face pairs are of that type (i.e., even though the set of common pairs incorrectly classified changed, the overall mean did not, as the average fell between 57-58% in both cases). Fig. <ref type="figure" target="#fig_7">9</ref> shows face pairs most commonly classified correctly or incorrectly considering both cases.</p><p>Quantitatively, human performers scored an average of 57.5%. This is comparable to hand-crafted features such as LBP and SIFT, but nearly 15% lower than our finetuned CNN (i.e., the SphereFace CNN fine-tuned for this experiment scores 72.15%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION AND FUTURE WORK</head><p>The labels of FIW are dynamic in structure-labels can be parsed to use the data in various ways. For instance, siblings can be split between those who share one and both parents. Even a slight change in paradigm can drastically change the study-use both parents for verification (i.e., tri-subject verification <ref type="bibr" target="#b11">[12]</ref>); use child photos only to test with for family classification. Besides, we still need to improve our visual recognition capability for kinship in current benchmarks. Then, it only seems natural to aim for fine-grained categorization of entire family trees (i.e., the ultimate achievement). On a different note, generative modeling is another interesting research track to pursue (e.g., given a couple and predict the offspring, or samples of their baby and predict the baby's appearance as an adult). Even other pair types (e.g., greatand great-great-grandparents, cousins, aunts, uncles, etc.). Also, the labeling framework introduced in this work could be used to video data to the families of FIW, which can be serve as a resource for template-based search and retrieval, or consider emotional responses and facial expressions. 2  It is expected that as we, the research community, advance on this problem, so will FIW and its uses, especially when considering the potential for inter-discipline collaborations (e.g., nature-based). Whether nature-base studies, 2. Get code &amp; interface, https://github.com/visionjo/FIW KRT generative or predictive modeling, we expect FIW to inspire new types of problems. We anticipate the list of uses to only grow when FIW is in the hands of researchers from around the globe. In the end, we aim to attract more experts to the problem of kinship recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>Families In the Wild (FIW) is the first large-scale dataset available for visual kinship recognition. We annotated complex hierarchical relationships with only a small team in a fast and efficient manner-providing the largest labeled collection of family photos to-date. FIW was structured to support multiple tasks with its dynamic label structure. We provided several benchmarks for kinship verification and family classification. Pre-trained CNNs were used as offthe-shelf face encoders, which outperformed conventional methods. Results for both tasks were further improved by fine-tuning the CNN models on FIW. We measured human observers and compared their performance to the machine vision algorithms, showing that CNN models already surpass humans in recognizing kinship. The size of FIW, along with the labels representing complex tree structures of 1, 000 families, makes it difficult to pinpoint the exact directions FIW will lead. Improving upon the benchmarks is one route, which is the focus of past, current, and future data challenges based on FIW. Also, additional task evaluations (e.g., search &amp; retrieval and tri-subject), along with cross-discipline studies (e.g., nature-based and human perception) are also promising directions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Photos of families sampled randomly from FIW (i.e., 8 of 1, 001).</figDesc><graphic coords="2,47.99,106.61,61.47,54.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Samples of 11 pair types of FIW. Each type is of a unique pair randomly selected from a set of diverse families to show variation in ethnicity, while four faces of each individual depict age variations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Database statistics: Horizontal and vertical axes represent counts for photos and faces per family, respectively. Bubble size and color represent counts for members and average faces per member, respectively.</figDesc><graphic coords="3,311.45,43.16,253.09,162.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Visual of the 2 label types of FIW, Family-level (FID) and Photolevel (PID). FID has individual family member (MID) and relationship information. PIDs contain information of MIDs + their locations in photos.</figDesc><graphic coords="4,63.23,146.60,85.35,56.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Semi-automatic labeling pipeline. Data Collection. Photos and text metadata are collected for underrepresented families in FIW and assigned unique IDs (i.e., PIDs). Each new member requires at least 1 profile picture (e.g., Brandon in P ID 1 ) to add to known labels. Data Preparation. With the existing FIW labels, we next aim to increase the amount, both in labeled faces and member labels, using multiple modalities-names in metadata and scores of SVMs are used to automatically label some unlabeled data-face-name pairs were assumed labeled for cases of high confidence. Starting from profile pictures (i.e., 1 face, 1 name) and working towards less trivial scenarios (e.g., 3 faces and 2 names, with 2 faces from 1 member at different ages, like in P ID 3 ). This step adds to the amount of side information used for clustering Label Generation. Label proposals for remaining unlabeled faces are generated using the proposed semi-supervised clustering model that leverages labeled data as side information to better guide the process. Label Validation. A GUI designed to validate clusters and ensure clusters are matched to the proper labels.</figDesc><graphic coords="5,332.03,171.02,60.25,66.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Relationship type specific ROC curves.</figDesc><graphic coords="8,138.56,203.65,69.53,57.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Samples used for human evaluation. Each column displays pairs most commonly marked correctly and incorrectly, and in cases for where the correct answer were true and false. Each of these pairs were properly classified by the fine-tuned CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Face pair counts for FIW and other kinship image collections, showing FIW far outdoes all others in the 7 sibling &amp; parent-child pair types. Plus, introduces 4 grandparent-grandchild types for the 1 st time. Table2further characterizes FIW and Fig.10shows samples for each pair type.</figDesc><table><row><cell></cell><cell></cell><cell>siblings</cell><cell></cell><cell></cell><cell cols="2">parent-child</cell><cell></cell><cell></cell><cell cols="2">grandparent-grandchild</cell><cell></cell><cell>Total</cell></row><row><cell></cell><cell>B-B</cell><cell>S-S</cell><cell>SIBS</cell><cell>F-D</cell><cell>F-S</cell><cell>M-D</cell><cell>M-S</cell><cell>GF-GD</cell><cell>GF-GS</cell><cell>GM-GD</cell><cell>GM-GS</cell><cell></cell></row><row><cell>KinWild I [28]</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>134</cell><cell>156</cell><cell>127</cell><cell>116</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>533</cell></row><row><cell>KinWild II [28]</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>250</cell><cell>250</cell><cell>250</cell><cell>250</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1,000</cell></row><row><cell>Sibling Face [47]</cell><cell>232</cell><cell>211</cell><cell>277</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>7 2 0</cell></row><row><cell>Group Face [8]</cell><cell>40</cell><cell>32</cell><cell>53</cell><cell>69</cell><cell>69</cell><cell>62</cell><cell>70</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>395</cell></row><row><cell>FIW(Ours) [23]</cell><cell>103,724</cell><cell>39,978</cell><cell>73,506</cell><cell>92,088</cell><cell>129,846</cell><cell>82,160</cell><cell>112,618</cell><cell>7,078</cell><cell>4,830</cell><cell>6,512</cell><cell>4,614</cell><cell>656,954</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>Comparison of FIW with related datasets.</figDesc><table><row><cell>Dataset</cell><cell>No.</cell><cell>No.</cell><cell>No.</cell><cell>Age</cell><cell>Family</cell></row><row><cell></cell><cell>Family</cell><cell>People</cell><cell>Faces</cell><cell>Varies</cell><cell>Trees</cell></row><row><cell>CornellKin [1]</cell><cell>150</cell><cell>300</cell><cell>300</cell><cell></cell><cell></cell></row><row><cell>UBKinFace [16], [25]</cell><cell>200</cell><cell>400</cell><cell>600</cell><cell></cell><cell></cell></row><row><cell>KFW-I [26]</cell><cell></cell><cell>533</cell><cell>1,066</cell><cell></cell><cell></cell></row><row><cell>KFW-II [26]</cell><cell></cell><cell>1,000</cell><cell>2,000</cell><cell></cell><cell></cell></row><row><cell>TSKinFace [12]</cell><cell>787</cell><cell>2,589</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Family101 [7]</cell><cell>101</cell><cell>607</cell><cell>14,816</cell><cell></cell><cell></cell></row><row><cell>FIW [23]</cell><cell>1,000</cell><cell>10,676</cell><cell>30,725</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3</head><label>3</label><figDesc>Ethnicity distribution of FIW. Mix are families with 2+ ethnicities (e.g., Bruce Lee (Asian) and wife Linda (Caucasian) with 2 children (Mix).</figDesc><table><row><cell>Caucasian</cell><cell>Spanish/Latino</cell><cell>Asian</cell><cell>African/AA</cell><cell>Arabic</cell><cell>Mix</cell></row><row><cell>64%</cell><cell>10.7%</cell><cell>9.1%</cell><cell>8.2%</cell><cell>2.0%</cell><cell>6.0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4</head><label>4</label><figDesc>Previous (white) vs new (shaded) labeling processes compared in terms of inputs (keyboard and mouse clicks) and time (hours:minutes:seconds).</figDesc><table><row><cell></cell><cell>Bruce Lee</cell><cell>Michael Jordan</cell><cell>John Malone</cell><cell>Craig Mccaw</cell><cell>Marco Reus</cell><cell>British Royal</cell><cell>Michael Jackson</cell><cell>Total</cell></row><row><cell>Inputs (count)</cell><cell>551</cell><cell>97</cell><cell>153</cell><cell>178</cell><cell>35</cell><cell>1,838</cell><cell>1,272</cell><cell>4,124</cell></row><row><cell>Inputs (count)</cell><cell>12</cell><cell>6</cell><cell>10</cell><cell>15</cell><cell>7</cell><cell>21</cell><cell>24</cell><cell>95</cell></row><row><cell>Time (h:m:s)</cell><cell>0:15:08</cell><cell>0:5:31</cell><cell>0:5:18</cell><cell>0:6:16</cell><cell>0:4:24</cell><cell>1:25:23</cell><cell>0:44:52</cell><cell>2:46:52</cell></row><row><cell>Time (h:m:s)</cell><cell>0:1:11</cell><cell>0:0:31</cell><cell>0:1:05</cell><cell>0:0:56</cell><cell>0:0:31</cell><cell>0:6:44</cell><cell>0:7:13</cell><cell>0:18:11</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5</head><label>5</label><figDesc>Averaged verification accuracy scores (%) for 5-fold experiment on FIW. Note that there was no family overlap between folds.</figDesc><table><row><cell></cell><cell></cell><cell>siblings</cell><cell></cell><cell></cell><cell cols="2">parent-child</cell><cell></cell><cell></cell><cell cols="2">grandparent-grandchild</cell><cell></cell><cell>Acc. +/-Std.</cell></row><row><cell>Method</cell><cell>B-B</cell><cell>S-S</cell><cell>SIBS</cell><cell>F-D</cell><cell>F-S</cell><cell>M-D</cell><cell>M-S</cell><cell>GF-GD</cell><cell>GF-GS</cell><cell>GM-GD</cell><cell>GM-GS</cell><cell></cell></row><row><cell>LBP [52]</cell><cell>55.52</cell><cell>57.49</cell><cell>55.39</cell><cell>55.05</cell><cell>53.77</cell><cell>55.69</cell><cell>54.65</cell><cell>55.79</cell><cell>55.92</cell><cell>54.00</cell><cell>55.36</cell><cell>55.33 ± 1.01</cell></row><row><cell>SIFT [63]</cell><cell>57.86</cell><cell>59.34</cell><cell>56.91</cell><cell>56.37</cell><cell>56.24</cell><cell>55.05</cell><cell>56.45</cell><cell>57.25</cell><cell>55.35</cell><cell>57.29</cell><cell>56.74</cell><cell>56.80 ± 1.17</cell></row><row><cell>ResNet-22 [56]</cell><cell>65.57</cell><cell>69.65</cell><cell>60.12</cell><cell>59.45</cell><cell>60.27</cell><cell>61.45</cell><cell>59.37</cell><cell>55.37</cell><cell>58.15</cell><cell>59.74</cell><cell>59.70</cell><cell>61.34 ± 3.81</cell></row><row><cell>VGG-Face [32]</cell><cell>69.67</cell><cell>75.35</cell><cell>66.52</cell><cell>64.25</cell><cell>63.85</cell><cell>66.43</cell><cell>62.80</cell><cell>62.06</cell><cell>63.79</cell><cell>57.40</cell><cell>61.64</cell><cell>64.89 ± 4.68</cell></row><row><cell>+ITML [57]</cell><cell>57.15</cell><cell>61.61</cell><cell>56.98</cell><cell>58.07</cell><cell>54.73</cell><cell>57.26</cell><cell>59.09</cell><cell>62.52</cell><cell>59.60</cell><cell>62.08</cell><cell>59.92</cell><cell>59.00 ± 2.44</cell></row><row><cell>+LPP [59]</cell><cell>67.61</cell><cell>66.22</cell><cell>71.01</cell><cell>62.54</cell><cell>61.39</cell><cell>65.04</cell><cell>63.54</cell><cell>63.50</cell><cell>59.96</cell><cell>60.00</cell><cell>63.53</cell><cell>64.03 ± 3.32</cell></row><row><cell>+LMNN [60]</cell><cell>67.11</cell><cell>68.33</cell><cell>66.88</cell><cell>65.66</cell><cell>67.08</cell><cell>68.07</cell><cell>66.16</cell><cell>61.90</cell><cell>60.44</cell><cell>63.68</cell><cell>60.15</cell><cell>65.04 ± 3.00</cell></row><row><cell>+GmDAE [62]</cell><cell>68.05</cell><cell>68.55</cell><cell>67.33</cell><cell>66.53</cell><cell>68.30</cell><cell>68.15</cell><cell>66.71</cell><cell>62.10</cell><cell>63.93</cell><cell>63.84</cell><cell>63.10</cell><cell>66.05 ± 2.36</cell></row><row><cell>+DLML [58]</cell><cell>68.03</cell><cell>68.87</cell><cell>67.97</cell><cell>65.96</cell><cell>68.00</cell><cell>68.51</cell><cell>67.21</cell><cell>62.90</cell><cell>63.96</cell><cell>63.11</cell><cell>63.55</cell><cell>66.19 ± 2.36</cell></row><row><cell>+mDML [37]</cell><cell>69.10</cell><cell>70.15</cell><cell>68.11</cell><cell>67.90</cell><cell>66.24</cell><cell>70.39</cell><cell>67.40</cell><cell>65.20</cell><cell>66.78</cell><cell>63.11</cell><cell>63.45</cell><cell>67.07 ± 2.44</cell></row><row><cell>ResNet+CF [64]</cell><cell>69.88</cell><cell>69. 54</cell><cell>69.54</cell><cell>68.15</cell><cell>67.73</cell><cell>71.09</cell><cell>68.63</cell><cell>66.37</cell><cell>66.45</cell><cell>64.81</cell><cell>64.39</cell><cell>67.87 ± 2.15</cell></row><row><cell>SphereFace [61]</cell><cell>71.94</cell><cell>77.30</cell><cell>70.23</cell><cell>69.25</cell><cell>68.50</cell><cell>71.81</cell><cell>69.49</cell><cell>66.07</cell><cell>66.36</cell><cell>64.58</cell><cell>65.40</cell><cell>69.18 ± 3.68</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6</head><label>6</label><figDesc>Family classification accuracy scores (%) using 524 families.</figDesc><table><row><cell>Run ID Network(s)</cell><cell>Acc.</cell></row><row><cell>Run-1 VGG-Face, fc 7 (4,096D)+one-vs-rest SVMs</cell><cell>3.04</cell></row><row><cell cols="2">Run-2 VGG-Face, replaced softmax (564D)+fine-tuned 10.42</cell></row><row><cell>Run-3 ResNet-22 + softmax (564D)</cell><cell>14.17</cell></row><row><cell>Run-4 SphereFace, fc 5 (512D)</cell><cell>13.86</cell></row><row><cell>Run-5 ResNet-22 + CF, fc 5 (512D)</cell><cell>16.18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 7</head><label>7</label><figDesc>Accuracy (%) for KinWild I &amp; II. CNN fine-tuned on FIW top scorer. Fig.7. Results for clustering families using different amounts of side information. As clearly depicted, our method obtains the top performance. Moreover, a distinct increase in NMI for our method is shown with an increase in the amounts of side information.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>KinWild-I</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>KinWild-II</cell><cell></cell><cell></cell></row><row><cell></cell><cell>FD</cell><cell>FS</cell><cell>MD</cell><cell>MS</cell><cell>Avg.</cell><cell>FD</cell><cell>FS</cell><cell>MD</cell><cell>MS</cell><cell>Avg.</cell></row><row><cell>LBP [52]</cell><cell>72.8</cell><cell>79.5</cell><cell>71.7</cell><cell>68.1</cell><cell>73.0</cell><cell>70.8</cell><cell>78.4</cell><cell>69.0</cell><cell>73.2</cell><cell>72.9</cell></row><row><cell>SIFT [63]</cell><cell>73.9</cell><cell>81.4</cell><cell>76.4</cell><cell>71.1</cell><cell>75.7</cell><cell>72.2</cell><cell>78.8</cell><cell>82.2</cell><cell>79.6</cell><cell>78.2</cell></row><row><cell>NRML (LBP)</cell><cell>81.4</cell><cell>69.8</cell><cell>67.2</cell><cell>72.9</cell><cell>72.8</cell><cell>79.2</cell><cell>71.6</cell><cell>72.2</cell><cell>68.4</cell><cell>72.9</cell></row><row><cell>NRML (HOG)</cell><cell>83.7</cell><cell>74.6</cell><cell>71.6</cell><cell>80.0</cell><cell>77.5</cell><cell>80.8</cell><cell>72.8</cell><cell>74.8</cell><cell>70.4</cell><cell>74.7</cell></row><row><cell>BIU (LBP)</cell><cell>85.5</cell><cell>76.5</cell><cell>69.9</cell><cell>74.4</cell><cell>76.6</cell><cell>84.2</cell><cell>79.5</cell><cell>76.0</cell><cell>77.0</cell><cell>79.2</cell></row><row><cell>BIU (HOG)</cell><cell>86.9</cell><cell>76.5</cell><cell>70.6</cell><cell>79.8</cell><cell>78.4</cell><cell>87.5</cell><cell>80.8</cell><cell>79.8</cell><cell>75.6</cell><cell>81.0</cell></row><row><cell>VGG-Face [32]</cell><cell>72.0</cell><cell>77.6</cell><cell>78.3</cell><cell>80.6</cell><cell>77.1</cell><cell>68.8</cell><cell>74.4</cell><cell>76.6</cell><cell>74.6</cell><cell>73.6</cell></row><row><cell>ResNet + CF</cell><cell>78.0</cell><cell>83.7</cell><cell>87.0</cell><cell>80.8</cell><cell>82.4</cell><cell>87.7</cell><cell>86.0</cell><cell>86.7</cell><cell>87.4</cell><cell>86.6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. #, NO. #, MARCH 2017</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards computational models of kinship verification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1577" to="1580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Kinship verification in multilinear coherent spaces</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Multimedia Tools and Applications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sparse coding based kinship recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Technology IV: Proceedings of the 4th International Conference on Multimedia Technology</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2015-03">March 2015. 2015</date>
			<biblScope unit="page">115</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Who do i look like? determining parent-offspring resemblance via gated autoencoders</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1757" to="1764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A feature subtraction method for image based kinship verification under uncontrolled environments</title>
		<author>
			<persName><forename type="first">X</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1573" to="1577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neighbors based discriminative feature difference learning for kinship verification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Visual Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="258" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Kinship classification by modeling facial feature heredity</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Loui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2983" to="2987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graph-based kinship recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dibeklioglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="4287" to="4292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large margin multi-metric learning for face and kinship verification in the wild</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ACCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="252" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning a genetic measure for kinship verification using facial images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Problems in Engineering</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inheritable fisher vector feature for kinship verification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Puthenputhussery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE 7th International Conference on Biometrics Theory, Applications and Systems (BTAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tri-subject kinship verification: Understanding the core of a family</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/1501.02555</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic verification of parent-child pairs from face images</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bottino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">U</forename><surname>Islam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="326" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Detecting siblings in image pairs</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bottino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Laurentini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">De</forename><surname>Simone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Visual Computer</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1333" to="1345" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Leveraging appearance and geometry for kinship verification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kambhamettu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="5017" to="5021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Kinship verification through transfer learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Joint Conference on Artificial Intelligence</title>
		<meeting>the 22nd International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2539" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Kinship verification using facial images by robust similarity learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical Problems in Engineering</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discriminative multimetric learning for kinship verification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1169" to="1178" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Prototype-based discriminative feature learning for kinship verification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2535" to="2545" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A talking profile to distinguish identical twins</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nejati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="771" to="778" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kinship verification from facial images under uncontrolled conditions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM international conference on Multimedia</title>
		<meeting>the 19th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ensemble similarity learning for kinship verification from facial images in the wild</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="40" to="48" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Families in the wild (fiw): Large-scale kinship image database and benchmarks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<idno type="DOI">10.1145/2964284.2967219</idno>
		<ptr target="http://doi.acm.org/10.1145/2964284.2967219" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference, ser. MM &apos;16</title>
		<meeting>the 2016 ACM on Multimedia Conference, ser. MM &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="242" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Understanding kin relationships in a photo</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1046" to="1056" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Genealogical face recognition based on ub kinface database</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X M</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR Workshop on Biometrics</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neighborhood repulsed metric learning for kinship verification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="331" to="345" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The fg 2015 kinship verification in the wild evaluation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bottino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">Ul</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Figueiredo</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Kinship verification in the wild: The first kinship verification competition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Castrill Ón-Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lorenzo-Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bottino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Figuieiredo</forename><surname>Vieira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Biometrics (IJCB)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Kinship verification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2015-09">September 2015</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="148" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Who do i look like? determining parent-offspring resemblance via gated autoencoders</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Convolutional fusion network for face verification in the wild</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Kinship verification on families in the wild with marginalized denoising metric learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Kinship verification from faces: Methods, databases and challenges</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Boutellaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Clustering with partition level side information</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Data Mining</title>
		<meeting>International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Text mining for automatic image tagging</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hassan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics: Posters, ser. COLING &apos;10</title>
		<meeting>the 23rd International Conference on Computational Linguistics: Posters, ser. COLING &apos;10<address><addrLine>PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to tag using noisy labels</title>
		<author>
			<persName><forename type="first">E</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECML</title>
		<meeting>ECML</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mining weakly labeled web facial images for search-based face annotation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="166" to="179" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning face representation from scratch</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-10">October 2007</date>
			<biblScope unit="page" from="7" to="49" />
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts, Amherst, Tech. Rep</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dlib-ml: A machine learning toolkit</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A new problem in face image analysis: finding kinship clues for siblings pairs</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Bottino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">De</forename><surname>Simone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Laurentini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vieira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Incorporating nonlocal information into information extraction systems by gibbs sampling</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd annual meeting on association for computational linguistics. Association for Computational Linguistics</title>
		<meeting>the 43rd annual meeting on association for computational linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Reinterpreting the category utility function</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mirkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">K-means-based consensus clustering: A unified view</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="155" to="169" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Face description with local binary patterns: Application to face recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2037" to="2041" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Face recognition in unconstrained videos with matched background similarity</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="529" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts, Amherst, Tech. Rep.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pre-trained d-cnn models for detecting complex events in unconstrained videos</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Commercial+ Scientific Sensing and Imaging. International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">710</biblScope>
			<biblScope unit="page" from="98" to="710O" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Informationtheoretic metric learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Discriminative low-rank metric learning for face recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Locality preserving projections</title>
		<author>
			<persName><forename type="first">X</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural information processing systems</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">153</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2009-02">Feb. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Marginalized denoising autoencoder via graph regularization for domain adaptation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="156" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<date type="published" when="2005-06">June 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Recognizing families in the wild (rfiw)</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gillis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of theh</title>
		<meeting>theh</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">K-means with large and noisy constraint sets</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pelleg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Machine Learning</title>
		<meeting>European Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="674" to="682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Where are kin recognition signals in the human face?</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Martello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Maloney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Unconstrained face recognition: Establishing baseline human performance via crowdsourcing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Best-Rowden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bisht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Klontz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics (IJCB)</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2014">2014. 2014</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
