<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial Laser Beam: Effective Physical-World Attack to DNNs in a Blink</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ranjie</forename><surname>Duan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Swinburne University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">†</forename><forename type="middle">Xiaofeng</forename><surname>Mao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Alibaba Group 3 EPFL</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Swinburne University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yun</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Swinburne University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuefeng</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Alibaba Group 3 EPFL</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shaokai</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuan</forename><surname>He</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Alibaba Group 3 EPFL</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adversarial Laser Beam: Effective Physical-World Attack to DNNs in a Blink</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Though it is well known that the performance of deep neural networks (DNNs) degrades under certain light conditions, there exists no study on the threats of light beams emitted from some physical source as adversarial attacker on DNNs in a real-world scenario. In this work, we show by simply using a laser beam that DNNs are easily fooled. To this end, we propose a novel attack method called Adversarial Laser Beam (AdvLB), which enables manipulation of laser beam's physical parameters to perform adversarial attack. Experiments demonstrate the effectiveness of our proposed approach in both digital-and physical-settings. We further empirically analyze the evaluation results and reveal that the proposed laser beam attack may lead to some interesting prediction errors of the state-of-the-art DNNs. We envisage that the proposed AdvLB method enriches the current family of adversarial attacks and builds the foundation for future robustness studies for light. † Works done when intern at Alibaba ‡ Code is available at https://github.com/RjDuan/Advlight</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Natural phenomena may play the role of adversarial attackers, e.g. a blinding glare results in a fatal crash of a Tesla self-driving car. What if a beam of light can adversarially attack a DNN? Further, how about using a beam of light, specifically the laser beam, as the weapon to perform attacks. If we can do that, with the fastest speed in the world, the laser beam could achieve the fastest attack with no doubts. As shown in Figure <ref type="figure" target="#fig_2">1</ref>, by using an off-the-shelf lighting device such as a laser pointer, the attacker can maliciously shoot a laser beam onto the target object to make the self-driving car fail to recognize target objects correctly.</p><p>We regard the attack illustrated in Figure <ref type="figure" target="#fig_2">1</ref> as a new type of adversarial attack, which is crucial but not yet exploited. Up to now, most researchers study the security of DNNs by exploring various adversarial attacks in digitalsettings, where input images are added with deliberately Figure <ref type="figure" target="#fig_2">1</ref>: An example. When the camera of self-driving car captures object shot by the laser beam, it recognizes "trolleybus" as "amphibian" and "street sign" as "soap dispenser". crafted perturbations and then fed to the target DNN model <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b16">18]</ref>. However, in physical-world scenarios, images are typically captured by cameras and then directly fed to the target models, where attackers cannot directly manipulate the input image. Some recent efforts in developing physical-world attacks are addressed in <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">14]</ref>. The physical-world adversarial examples typically require large perturbations, because small perturbations are hard to be captured by cameras. In addition, the attacking effects of adversarial examples of small perturbations can be easily mitigated in complex physical-world environments <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b6">7]</ref>. Meanwhile, physical-world adversarial examples require high stealthiness to avoid being discovered by either the victim or defender before performing an attack successfully. Thus for creating physical-world adversarial examples, there is always a compromise between stealthiness and adversarial strength.</p><p>Most existing physical-world attacks adopt a "stickerpasting" setting, i.e., the attacker prints adversarial perturbation as a sticker and then pastes it onto the target object <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. These attacks achieve the stealthiness of adversaries with extra efforts of designing adversarial perturbation or camouflaging adversarial images and finding 1 arXiv:2103.06504v1 [cs.LG] 11 Mar 2021 (a) AdvCam <ref type="bibr" target="#b6">[7]</ref> (b) RP2 <ref type="bibr" target="#b1">[2]</ref> (c) AdvLB (Ours) the most effective area in the target object to impose them <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b6">7]</ref>. Besides the challenge of stealthiness, the "sticker-pasting" setting also requires the target object to be physically accessible by the attacker to paste stickers. However, this may not be always possible. Several works explore physical-world threats beyond the "sticker-pasting" setting: Shen et al. <ref type="bibr" target="#b20">[22]</ref> and Nguyen et al. <ref type="bibr" target="#b17">[19]</ref> proposed using a projector to project the adversarial perturbation on the target to perform an attack. However, these works still require manual effort to craft adversarial perturbation.</p><p>In our work, we propose a new type of physical-world attack, named adversarial laser beam (AdvLB). Unlike existing methods, we utilize the laser beam as adversarial perturbation directly rather than crafting adversarial perturbation from scratch. As AdvLB does not require physically changing the target object to be attacked as in the "stickerpasting" setting, it features high flexibility to attack any object actively, even from a long distance. In terms of stealthiness, a visual comparison between our proposed attack and other works can be seen in Figure <ref type="figure" target="#fig_0">2</ref>. Though the adversarial example generated via AdvLB may appear less stealthy than some generated by other approaches such as AdvCam. AdvLB may introduce high temporal stealthiness due to its unique physical-attack mechanism. Specifically, with the nature of light, AdvLB can perform the attack in a blink right before the attacked target object gets captured by the imaging sensor, and thus avoid being noticed by victims and defenders in advance. Existing works focus on security issues of DNNs in the daytime whilst potential security threats at night are often ignored. Our proposed AdvLB provides a complementary in this regard. Figure <ref type="figure" target="#fig_0">2</ref> illustrates the advantage of AdvLB when the lighting condition is poor.</p><p>To launch such an attack, we formulate the laser beam with a group of controllable parameters. Then we devise an optimization method to search for the most effective laser beam's physical parameters in a greedy manner. It enables finding where and how to make an effective physical-world attack in a black-box setting. We further apply a strategy called k-random-restart to avoid falling into the local optimum, which increases the attack success rate.</p><p>We conduct extensive experiments for evaluation of the proposed AdvLB. We first evaluate AdvLB in a digitalsetting, which is able to achieve 95.1% attack success rate on a subset of ImageNet. We further design two physicalworld attacks including indoor and outdoor tests, achieving 100% and 77.43% attack success rates respectively. Then ablation studies are presented on the proposed AdvLB. Furthermore, we analyze the prediction errors of DNNs caused by the laser beam. We find the causes of prediction errors could be roughly divided into two categories. 1). The laser beam's color feature changes the raw image and forms a new cue for DNNs. 2). The laser beam introduces some dominant features of specific classes, especially lighting related classes, e.g. candle. When the laser beam and target object appear simultaneously, there is a chance that the DNN is more biased towards the feature introduced by the laser beam and thus resulting in misclassification. These interesting empirical findings open a door for both attackers and defenders to investigate how to better manipulate this new type of attack. Our major contributions are summarized as follows:</p><p>• We propose a new type of physical-world attack based on the laser beam named AdvLB, which leverages light itself as adversarial perturbation. It provides high flexibility for attacker to perform the attack in a blink. Besides, the deployment of such attack is rather simple: by using a laser pointer, it may become a common threat due to its convenience to perform attack.</p><p>• We conduct comprehensive experiments to demonstrate the effectiveness of AdvLB. Specifically, we perform physical test to validate AdvLB and show the real-world threats of laser beam by simply using a laser pointer. Thus AdvLB can be a useful tool to explore such threats in the real-world.</p><p>• We make an in-depth analysis of the prediction errors caused by the AdvLB attack to have revealed some interesting findings which would motivate future study on AdvLB from the perspectives of attackers and defenders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and Related Work</head><p>Adversarial attack was first proposed by Szegedy et al. <ref type="bibr" target="#b21">[23]</ref>, aiming to generate perturbations superimposed on clean images to fool a target model. Given a target model f , adversarial examples can be crafted by one or more steps of perturbation following the direction of adversarial gradients <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b16">18]</ref> or optimized perturbation with a given loss <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>. Adversarial examples can be either generated from an image itself (in the digital setting) or produced by capturing an attacked physical scene via image sensors such as cameras (in the physical setting) <ref type="bibr" target="#b13">[15]</ref>.</p><p>Adversarial attack in digital settings. Most attacks are developed in a digital setting. And their perturbations are bounded by a small norm-ball to ensure that imperceptible to human observers. Normally, l 2 and l ∞ are the most commonly used norms <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">18]</ref>. Some other works also explore adversarial examples beyond bounded setting. They make modifications on the secondary attributes (e.g. color <ref type="bibr">[13,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b26">28]</ref>, texture <ref type="bibr" target="#b22">[24]</ref>) to generate adversarial examples. Besides, there are also several works that propose changing physical parameters <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b15">17]</ref> while preserving critical components of images to create adversaries. However, digital attacks have a strong assumption that the attacker has access to modify the input image of the target model directly, which is not practical in real-world scenarios.</p><p>Adversarial attack in physical settings. Kurakin et al. <ref type="bibr" target="#b13">[15]</ref> first showed the existence of adversarial examples in the physical-world by printing digital adversary in the physical-world, and then recaptured by cameras. <ref type="bibr" target="#b13">[15]</ref>, and its follow-up work adopted such setting, including pasting adversarial patch on either traffic-sign <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7]</ref> or t-shirt <ref type="bibr" target="#b24">[26]</ref>, or camouflaging the adversarial patch into specific shape (e.g. eye-glasses frames <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b1">2]</ref>, etc.). Due to various physical-world conditions such as viewpoint shifts, camera noise, and other natural transformations <ref type="bibr" target="#b0">[1]</ref>, the physically realized adversarial examples always require various adaptations over a distribution of transformations to adapt to physical-world conditions <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b1">2]</ref>. Thus the "stickerpasting" attacks generate large adversarial perturbation inevitably. However, there exist a certain period of time between deploying the "sticker-pasting" attacks and performing attacks successfully. Thus the "sticker-pasting" attacks require high stealthiness to avoid being noticed by human observers in advance. Stealthiness for large perturbation is always a challenge for "sticker-pasting" physical-world attacks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">21]</ref>. Also, these attacks require attacker physically pasting the stickers on the target objects, however, not every object is easily accessible or reachable in realworld, e.g. traffic sign on a high pole. Crafted perturbation in physical setting suffers from the loss in adversarial strength when converting from digital to physical-setting. In contrary, our method simply leverages the light itself as adversarial perturbation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Physical attacks with lighting devices.</head><p>There exist some works using lighting devices to generate adversarial attacks. Some utilized a projector to perform the physicalworld attacks against face recognition systems <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b17">19]</ref>. These attacks craft adversarial perturbation and then project the perturbation onto the target to perform the attack. Zhou et al. <ref type="bibr" target="#b28">[30]</ref> proposed to deploy LED light on the cap to fool the face recognition systems, which requires careful deployment of the lighting device. Comparatively, existing methods require efforts to craft the adversarial perturbation while our method is much easier to perform an adversarial attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Recall the typical definition of adversarial example x adv in image classification, given an input image x ∈ R m with class label y, a DNN classifier f : R m → R J . The classifier f associates with a confidence score f j (x) to class j. An adversarial example x adv commonly satisfies two properties: 1) argmax j∈J f (x adv ) = argmax j∈J f (x), 2) x adv − x p ≤ . In which, the first property requires that x adv is classified differently with x by target model f . The second property requires that the perturbation of x adv is imperceptible enough so that x adv is stealthy for human observers.</p><p>In our context, as example shown in Figure <ref type="figure" target="#fig_1">3</ref>, the aim of our proposed attack method is to find a vector of parameters θ of the laser beam l that makes the resultant image x l θ being misclassified by the target model f . We constrain the width w of laser beam l to satisfy the requirement on the stealthiness of x adv in digital-setting. This section is organized as follows. We first present the definition of the laser beam. Then we model the physical laser beams with a set of parameters and optimize these parameters to create an adversarial example for the given image x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Laser Beam Definition</head><p>Laser is distinguished from other light sources by its coherence, including temporal coherence and spatial coherence. With high temporal coherence, laser is able to emit light with a very narrow spectrum even in a single color. The spatial coherence enables a laser beam to stay narrow over a long distance. Our proposed attack is based on these unique properties of the laser beam. We formulate the laser beam l with a set of physical parameters denoted by θ. We consider four key parameters to define a laser beam l including wavelength (λ), layout (r, b), width (w), and intensity α. We define each parameter as follows.</p><p>• Wavelength (λ). Wavelength (λ) denotes the color of the laser beam. We only consider wavelengths in the range of visible light (380 nm to 750 nm). We define a conversion function that converts λ to a RGB tuple * .</p><p>* http://www.noah.org/wiki/Wavelength_to_RGB_in_ Python • Layout (r, b). We treat the laser beam as a line. We use a tuple (r, b), which includes an angle (r) and intercept (b) to determine the beam line. The propagation path of the laser beam can be denoted by y = tan(r) • x + b. We also consider that the laser beam extincts during the transmission. Thus we define an attenuation function with inverse square † to simulate the reduction of luminous intensity of laser beam during transmission.</p><p>• Width (w). The width of the laser beam depends on two factors: the distance between the laser beam and the camera, and the coherence degree of the laser beam. The wider the laser beam is, the more perceptible. We set a threshold on the beam width in digitalsetting to avoid over-obstructiveness.</p><p>• Intensity (α). The intensity of the laser beam depends on the power of the laser device. Laser beam with stronger intensity looks brighter. In our context, we use α to denote the intensity of the laser beam l.</p><p>We define constraint vectors min and max to restrict the range of each parameter in θ. The constraints are adjustable.</p><p>We then adopt a simple linear image fusion method to fuse clean image x and laser beam layer l θ .</p><p>x</p><formula xml:id="formula_0">l θ = x + l θ<label>(1)</label></formula><p>where x l θ represents the image x imposed by a specific laser beam l θ defined by a vector of parameters θ, then clipped to a valid range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Laser Beam Adversarial Attack</head><p>Algorithm. The focus of the proposed attack is to search for a vector of physical parameters θ of laser beam l given image x, aiming to result in misclassification by f . We define a search space Θ, formally, Θ = {θ|θ = [λ, r, b, w, α], min ≤ θ ≤ max }, where is a list of constraints. In our context we consider the most practical scenario: an attacker cannot attain the knowledge of the target model but only the confidence score f y (x) with given input image x on predicted label y. In our proposed AdvLB, we exploit by using the confidence score as the adversarial loss. Thus the objective is formulated as minimizing the confidence score on correct label, we construct an adversarial image x l θ by solving the following objective:</p><formula xml:id="formula_1">argmin θ f y (x l θ ), s.t. θ ∈ [ min , max ]<label>(2)</label></formula><p>Inspired by Guo et al.'s work <ref type="bibr" target="#b10">[11]</ref>, we exploit the confidence scores given by the target model, and propose a greedy method to search for vector θ to generate adversarial laser † https://en.wikipedia.org/wiki/Inverse-square_ law beam. In a brief, we repeatedly attempt to update the current vector of parameters θ with one of stronger adversarial strength instructed by f y (x l θ ), which indicates the confidence of correct class y given laser beam with current parameter θ . If f y (x l θ ) decreases, we then update the vector of parameters θ with θ . We first define the basis for search as set Q, which includes a series of predefined candidate vectors q. We define a candidate q as follows: the minimal update on l θ is one unit in either λ, r, b, w or α. We set the max step numbers with t max . During the search, we pick a random vector q ∈ Q multiplying with step size s ∈ S. If either f y (x + l θ+q ) or f y (x + l θ−q ) decreases, then we update θ by current θ . The search ends when the current x l θ is predicted with a label argmax f (x l θ ) = argmax f (x) or the max steps t max is reached. However, we found such a greedy search process is prone to getting stuck into local optimum. To this end we further apply a strategy called k-random-restart, which introduces more randomness into the search algorithm. Specifically, k-random-restart restarts the search process k times. For each time we use θ with different initializations. We find such a simple strategy greatly improves the effectiveness of the search process. The pseudo-code of AdvLB is shown in Algorithm 1.  Randomly pick q ∈ Q, s ∈ S; As the algorithm illustrates, the proposed method takes a test image x, a set of candidate Q, a flexible step size S, classifier f and max steps t max as input decided by the at-tacker. Details of the algorithm have been explained above. The algorithm finally returns a successful parameter list θ of laser beam, which is used for further instructing the deployment of the attack in the physical-world.</p><formula xml:id="formula_2">6 q ← q × s ; 7 θ ← θ ± q; 8 θ ← clip(θ , min , max ); 9 conf = f y (x l θ ); 10 if conf * ≥ conf then 11 θ ← θ ; 12 conf * ← conf ;</formula><p>Physical Adaptation To make adversaries generated by AdvLB physically realized, we adopt two strategies. 1) Physically adapted constraints . We consider the practical limitations for an attacker to perform the attack. To this end, we denote that is physically adapted according to the real-world conditions to perform the attack. 2) Batch of transformed inputs. AdvLB instructs where to perform an effective attack, however, a laser beam with exact layout (r, b) could be hard to reproduce. Thus we apply AdvLB on a batch of transformed images X T = {x |x = T (x)} where T represents a random transformation function including rotation, translation, or addition of noise. With returned batch of θ on given x, we then have an effective range, e.g. an effective range of angle r, to perform laser attack in the physical-world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setting</head><p>We test our proposed attacks in both digital-and physical-settings. We use ResNet50 <ref type="bibr" target="#b11">[12]</ref> as the target model for all the experiments. We randomly selected 1000 laser pointers (power: 5mW) to generate low-powered laser beams with wavelength 450nm, 532nm, and 680nm respectively. We use a Google Pixel4 smartphone to take photos. In ablation study, we set a group of experiments to test the impact of different parameters on the adversarial effects of laser beam. The target models adopt a black-box setting that during the attack we only use the prediction scores given by the model. For all the tests we use attack success rate (%) as the metric to report effectiveness, which is the proportion of successful attacks among the total number of test images examined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation of AdvLB</head><p>We first evaluate our proposed AdvLB in a digital setting, then we demonstrates how our proposed AdvLB could be a useful tool to explore the real-world adversarial examples resulted from laser beam.</p><p>Digital Test We apply our proposed attack method on 1000 correctly classified images selected from ImageNet, and craft an adversarial example for each test image with simulated laser beam. The success rate is 95.1% with 834 queries on average of proposed AdvLB. We also present some attack results generated by AdvLB as shown in Figure <ref type="figure" target="#fig_6">5</ref>. The first column shows the test images we aim to attack, and each row denotes a series of adversarial examples generated by AdvLB. Figure <ref type="figure" target="#fig_6">5</ref> shows some interesting results. For example, when shining a laser beam with yellow color, the king snake is then misclassified as corn, and there is indeed some similarity between the texture of king snake and corn. Other adversarial examples show similar phenomenon: 'Loggerhead' + Laser beam (blue) −→ 'Jellyfish', 'Radio' + Laser beam (red) −→ 'Space heater'. The results show the link among the adversarial class, original class, and the laser beam with a specific wavelength. We will give more discussion in Section 5.</p><p>Physical Test AdvLB aims to be an effective tool to explore real-world threats of laser beam on DNNs. Different from targeted attack in the physical-world, whose evaluation on success is rather intuitive: whether it can fool the DNNs with the class that attacker expects consistently in the physical-world. While untargeted attack is defined as fooling the DNN into any incorrect classes. With amount of uncertainties in the physical-world environment, the misclassification could be caused by natural noise rather than the untargeted attack. Thus evaluation on the effectiveness of untargeted attack in physical-setting requires more careful design of the experiments.</p><p>To validate AdvLB can be reproduced by laser beam in the physical-world, we design a strict experimental setting to perform an indoor test. We use three different laser pointers with wavelengths of 450nm, 532nm, 680nm respectively to perform the attack. The target objects include a banana, a conch, and a stop sign. We set the background in black to avoid introducing unnecessary noise into the experiment. For the test, we first capture the target object by cellphone, then we use our proposed AdvLB to find where to perform attack with given test captured image. We then reproduce the attack in such a setting with the returned parameter list θ by AdvLB. We set constraint on parameter λ according to the wavelength of used laser pointer. The experimental results are summarized in Figure <ref type="figure" target="#fig_7">6</ref>.</p><p>As Figure <ref type="figure" target="#fig_7">6</ref> shows, the proposed AdvLB is able to  achieve 100% attack success rate in such a strict experimental setting. Digital attacks by AdvLB are almost consistent with physical-world attacks by reproduced attack by laser pointers, that the top-3 classification results are similar. In which, the laser beam with λ = 680nm is harder to capture and the beam is not as coherent as the other two beams, thus the predicted results in digital and physical are slightly different. In summary, our proposed AdvLB can almost reflect the threats of laser beam in the physical-world, thus can be used to explore the potential real-world threats caused by laser beam. We further conduct an outdoor test. When a self-driving car approaches the stop sign, even if it fails to recognize the stop sign for merely a short time window, it can lead to a fatal accident. We first apply the method mentioned in Section 3.2 with given captured stop sign. We then shoot the laser beam from position onto targeted stop sign with given returned effective attack range. Figure <ref type="figure" target="#fig_8">7</ref> illustrates some attack results. Overall, there is an attack success rate of 77.43% of the test. These results further demonstrate the real-world by laser beams. Thus our proposed AdvLB could be a very meaningful tool to explore such threats. Currently, a weakness of our proposed method is that it is still limited in attacking in a dynamic environment, we leave this as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>Here, we conduct a series of experiments on ImageNet to study the adversarial effect of laser beam with different parameters: 1) Wavelength (λ), 2) Layout (r, b), 3) Width (w), and another ablation study on how k in k-random-restart impacts the attack success rate of AdvLB. We acknowledge the intensity α is an important property, especially for improving stealthiness of laser beam. We will do more study on α in the future work. In the following experiments, we fix it as 1.0. Also, for the study on each parameter of the laser beam, we fix the other parameters.</p><p>Wavelength (λ) Here, we show how the wavelength λ impacts the adversarial effects of laser beam. We test the adversarial effects of laser beam with wavelength (λ) in the range of visible light (380 nm, 750nm). We fix other parameters as: r = 45 • , b = 0, and w = 20. These values are selected based on extensive experiments, which are effective for finding adversarial images. In this ablation, we perform the tests on ResNet50 with 1000 randomly collected images from ImageNet. We combine the images with laser beams using Eq. ( <ref type="formula" target="#formula_0">1</ref>) and then feed the resultant images to the target model. Table <ref type="table" target="#tab_0">1</ref> shows the success rates of simulated laser beams with different wavelength λ. As shown in Table <ref type="table" target="#tab_0">1</ref>, the simulated laser beam with λ = 580 can even achieve 58.9% success rate. Note that for all these experiments, the simulated laser beams are added on unknown images and then fed to unknown target models directly. The results show that the laser beams have universal adversarial effects on different images.</p><p>Width (w) We then evaluate how the width w impacts the adversarial effect of laser beam. We set the threshold of width as 40, occupied at most 1/10 of the whole image. Again, we fix other parameters of laser beam as constants: λ = 400, r = 30 and b = 50. Wider laser beam can improve the success rate from 30.80% up to 47.69% with width from 1 to 40. However, even with the smallest w = 1 (1 pixel width), there is still an impressive success rate (30.80%).</p><p>Layout  In Figure <ref type="figure" target="#fig_10">8</ref>, the left column shows that the attack success rate of laser beam is highly related to its layout. Meanwhile, the right column illustrates two adversarial images (layout with higher attack success rates), which indicate that the center (where the laser beam illuminates) is more likely to create a successful adversarial example.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Analysis of DNNs' Prediction Errors To better understand the mechanism that the laser beam enables adversarial attacks, we further perform an empirical study with ImageNet to explore the errors caused by the laser beam. Roughly, we find there are two categories of errors as shown in Figure <ref type="figure" target="#fig_12">9</ref>. The laser beam performs as a kind of per- turbation, which either cancels or changes the original feature of a clean image and brings new cues for DNNs. An example is shown in Figure <ref type="figure" target="#fig_12">9</ref>, when the laser beam with wavelength 400nm shines on the hedgehog, whose spines combined with the blue color brought by laser beam form a new cue similar to cardoon for the DNN, thus resulting in misclassification. Results shown in Figure <ref type="figure" target="#fig_6">5</ref> also support for this claim. Also, we find the laser beam itself performs as dominant features for some specific classes (e.g. swab shown in Figure <ref type="figure" target="#fig_12">9</ref>). We further perform a batch of experiments and then summarize the statistics: which class's percentage rises the most before and after adding the laser beam. We report both top-1 and top-5 rise as shown in Table 3. For each case, we report both percentages of specific class before and after adding laser beam on ImageNet. As shown in Table <ref type="table" target="#tab_3">3</ref>, the laser beam with different wavelengths λ indeed increases the percentage of some specific classes. For example, when adding a laser beam with wavelength λ = 380, the percentage of class "Candle" increases the most from 0.19% to 6.21%. The results imply that the laser beam itself serves as dominant feature for some classes, such as spotlight, volcano, etc. Thus when adding the laser beam to the clean image, it has the chance that the model is more biased towards the feature brought by the laser beam. We further use the CAM <ref type="bibr" target="#b27">[29]</ref> to highlight the bias of model when the laser beam is added to the clean image (as shown in Figure <ref type="figure" target="#fig_13">10</ref>). By adding light beam even on the corner of image, the model is more biased towards "Bubble" and "Volcano", thus give wrong top-1 predictions. Defense of Adversarial Laser Beam Besides revealing the potential threats of AdvLB, in this work, we also try to suggest an effective defense for laser beam attack. Similar to adversarial training, we progressively improve the robustness by injecting the laser beam as perturbations into the data for training. We do not find the worst-case perturbations at each training step like adversarial training <ref type="bibr" target="#b16">[18]</ref>. As seeking worst-case laser beam perturbations by AdvLB needs much more computation cost, it may cause the overall training unaffordable. By contrast, we find that training with randomly added laser beams as augmentation can partly strengthen the model under laser beam attacks, and has no negative impact on the recognition of clean images. We use timm ‡ to train the ResNet50 robust model. The model was optimized on 16 2080Ti GPUs by SGD with a cosine annealing learning rate schedule from 0.1 to 1e-5. We add random laser beams on input images with 50% probability for data augmentation. The other hyperparameters are consistent with the reported settings § . We sum- ‡ https : / / github . com / rwightman / pytorch -imagemodels § https : / / github . com / rwightman / pytorch -imagemodels/blob/master/docs/results.md marize the results in Table <ref type="table" target="#tab_4">4</ref>. Except for the attack success Besides, we found an intriguing phenomenon that the accuracy of the model on clean images does not decrease after adding random laser beam augmentation, instead increases slightly by 0.21%. We suggest that introducing additional light source as enhancement makes the model more robust to the confusion as analyzed in Section 5, thus increases the generalization ability of DNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>In this paper, we propose AdvLB to utilize the laser beam as adversarial perturbation to generate adversarial examples, which can be applied in both digital-and physicalsettings. The proposed attack reveals the existence of an easily implemented real-world threat on DNNs. Some findings resulted from our work open a promising direction for crafting adversarial perturbation by utilizing light (i.e., laser beam) rather than generating perturbation manually. The proposed AdvLB is particularly useful in studying the security threats of vision systems at poor light conditions, that could be a meaningful complementary to current physicalworld attacks.</p><p>In the future, we will improve our proposed AdvLB to be more adapted to dynamic environment. In addition, we will consider the parameter of light intensity into optimization to create a more stealthy adversarial example with simulated laser beam. We will also explore the possibility of using other light pattern (e.g. spot light) and light source (e.g. natural light) to craft adversarial attacks. Furthermore, we will apply AdvLB on other computer vision tasks including object detection and segmentation. Moreover, effective defense strategies against such attacks will be another crucial and promising direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visual comprison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example.</figDesc><graphic url="image-5.png" coords="3,308.86,306.18,236.25,79.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 :</head><label>1</label><figDesc>Pseudocode of AdvLB Input: Input x; Candidate vectors Q; Step size S; classifier f ; Max #steps t max ; Output: A vector of parameters θ; 1 conf * ← f y (x); 2 for i ← 1 to k do 3 Initialization: θ ∼ Θ(λ, r, b, w, α) ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4 for t ← 1</head><label>41</label><figDesc>to t max do 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>15 if</head><label>15</label><figDesc>argmax f (x l θ ) = argmax f (x)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Experiment devices.</figDesc><graphic url="image-6.png" coords="5,73.74,450.59,188.99,124.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Adversarial examples generated by AdvLB.</figDesc><graphic url="image-7.png" coords="6,74.86,72.00,445.50,196.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Indoor test.</figDesc><graphic url="image-8.png" coords="6,50.11,298.52,236.25,166.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Outdoor test.</figDesc><graphic url="image-9.png" coords="6,308.86,320.29,236.25,68.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(r, b) We then show how the selection of layout (r, b) impacts the attack success rates of laser beams. The range of r is [0 • , 180 • ], and b in range [0, 400]. We fix other physical parameters as constants: λ = 580, w = 20. For efficiency concern, we only sample 100 correctly classified images from ImageNet, and summarize the results in Figure 8, where each point denotes the success rate of current layout.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Ablation of Layout (r, b).</figDesc><graphic url="image-10.png" coords="7,61.93,467.27,212.62,125.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>k-random-restart (k) We choose different k of 1, 20, 50, 100, 200 to run the experiment. The results are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Two types of errors caused by AdvLB.</figDesc><graphic url="image-11.png" coords="7,320.68,293.21,212.62,130.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: CAM for test images and adversarial images generated by AdvLB.</figDesc><graphic url="image-13.png" coords="8,50.11,326.61,236.25,68.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation of Wavelength (λ).</figDesc><table><row><cell>Wavelength λ (nm)</cell><cell>380</cell><cell>480</cell><cell>580</cell><cell>680</cell></row><row><cell>Suc. rate (%)</cell><cell cols="4">34.03 48.01 58.93 44.10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 ,</head><label>2</label><figDesc>the attack success rate is improved gradually with increase of k. It suggests that we do find better parameters of laser beam with more random restarts.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation of k-random-restart (k).</figDesc><table><row><cell>Restart num. (k)</cell><cell>1</cell><cell>50</cell><cell>100</cell><cell>200</cell></row><row><cell>Suc. rate (%)</cell><cell cols="4">72.80 89.60 92.20 95.10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Statistics of Error caused by AdvLB.</figDesc><table /><note>λ Top1 Pred. Percent. (%) Top5 Pred. Percent. (%) 380∼470 Feather boa 0.10 → 2.20 Feather boa 0.32 → 8.76 470∼560 Tennis ball 0.10 → 2.46 Spotlight 0.64 → 7.98 560∼650 Rapeseed 0.13 → 1.94 Candle 0.19 → 6.21 650∼740 Volcano 0.11 → 2.14 Gold fish 0.26 → 6.63</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of ResNet50 with and without Defense on AdvLB. ResNet50 rob 78.40 (↑ 0.21) 77.20 2576 rate, we adopt another metric named queries. Queries are the number of times that an attacker needs to query the output from target model before searching for best parameters to attack successfully. Our AdvLB only uses 834 queries on average to break through the original ResNet50 ori with a high success rate of 95.1%. In contrast, the defense model ResNet50 rob can effectively reduce the attack success rate to 77.2% based on running 2576 queries, showing a certain degree of defense ability against AdvLB.</figDesc><table><row><cell>Models</cell><cell>Std. acc(%)</cell><cell cols="2">Suc. rate(%) Queries</cell></row><row><cell>ResNet50 ori</cell><cell>78.19</cell><cell>95.10</cell><cell>834</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Synthesizing robust adversarial examples</title>
		<author>
			<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adversarial patch</title>
		<author>
			<persName><forename type="first">Dandelion</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurko</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martín</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><surname>Gilmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adversarial examples are not easily detected: Bypassing ten detection methods</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Workshop on Artificial Intelligence and Security</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IEEE S&amp;P</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ead: elastic-net attacks to deep neural networks via adversarial examples</title>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Boosting adversarial attacks with momentum</title>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhou</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adversarial camouflage: Hiding physical-world attacks with natural styles. CVPR</title>
		<author>
			<persName><forename type="first">Ranjie</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Ak Qin</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust physical-world attacks on deep learning models</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Eykholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Earlence</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tadayoshi</forename><surname>Kohno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atul</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Rahmati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust physicalworld attacks on deep learning visual classification</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Eykholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Earlence</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Rahmati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atul</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tadayoshi</forename><surname>Kohno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples. ICLR</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simple black-box adversarial attacks</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yurong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gordon Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radha</forename><surname>Poovendran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
				<imprint>
			<date type="published" when="2016">2016. 2018</date>
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Universal physical camouflage attacks on object detectors</title>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengying</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changqing</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Perceptual-sensitive gan for generating adversarial patches</title>
		<author>
			<persName><forename type="first">Aishan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anlan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Beyond pixel norm-balls: Parametric adversaries using an analytically differentiable renderer</title>
		<author>
			<persName><forename type="first">Hsueh-Ti Derek</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Jacobson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks. ICLR</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2018. 1, 2, 3, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adversarial light projection attacks on face recognition systems: A feasibility study</title>
		<author>
			<persName><forename type="first">Sunpreet</forename><forename type="middle">S</forename><surname>Dinh-Luan Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Colorfool: Semantic adversarial colorization</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Ali Shahin Shamsabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Sanchez-Matilla</surname></persName>
		</author>
		<author>
			<persName><surname>Cavallaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition</title>
		<author>
			<persName><forename type="first">Mahmood</forename><surname>Sharif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sruti</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lujo</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">K</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Vla: A practical visible light-based attack on face recognition systems in physical world</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zelin</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liehuang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Du</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>ACM IMWUT</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Physical adversarial textures that fool visual object tracking</title>
		<author>
			<persName><forename type="first">Reza</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anqi</forename><surname>Wiyatno</surname></persName>
		</author>
		<author>
			<persName><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving transferability of adversarial examples with input diversity</title>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adversarial t-shirt! evading person detectors in a physical world</title>
		<author>
			<persName><forename type="first">Kaidi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaoyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengshu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongge</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11099</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adversarial attacks beyond the image space</title>
		<author>
			<persName><forename type="first">Xiaohui</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Siang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weichao</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards large yet imperceptible adversarial image perturbations with perceptual color distance</title>
		<author>
			<persName><forename type="first">Zhengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoran</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Invisible mask: Practical attacks on face recognition with infrared</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weili</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kehuan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.04683</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
