<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A hierarchical genetic fuzzy system based on genetic programming for addressing classification with highly imbalanced and borderline data-sets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012-10-03">3 October 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Victoria</forename><surname>López</surname></persName>
							<email>vlopez@decsai.ugr.es</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Computer Science and Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Research Center on Information and Communications Technology</orgName>
								<orgName type="institution" key="instit1">CITIC-UGR</orgName>
								<orgName type="institution" key="instit2">University of Granada</orgName>
								<address>
									<postCode>18071</postCode>
									<settlement>Granada</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alberto</forename><surname>Fernández</surname></persName>
							<email>alberto.fernandez@ujaen.es</email>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Jaén</orgName>
								<address>
									<postCode>23071</postCode>
									<settlement>Jaén</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">María</forename><surname>José Del Jesus</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Jaén</orgName>
								<address>
									<postCode>23071</postCode>
									<settlement>Jaén</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Francisco</forename><surname>Herrera</surname></persName>
							<email>herrera@decsai.ugr.es</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Computer Science and Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">Research Center on Information and Communications Technology</orgName>
								<orgName type="institution" key="instit1">CITIC-UGR</orgName>
								<orgName type="institution" key="instit2">University of Granada</orgName>
								<address>
									<postCode>18071</postCode>
									<settlement>Granada</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A hierarchical genetic fuzzy system based on genetic programming for addressing classification with highly imbalanced and borderline data-sets</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2012-10-03">3 October 2012</date>
						</imprint>
					</monogr>
					<idno type="MD5">7A20B513177CFEB93EE6A6F7DEAD7127</idno>
					<idno type="DOI">10.1016/j.knosys.2012.08.025</idno>
					<note type="submission">Received 18 October 2011 Received in revised form 3 June 2012 Accepted 24 August 2012</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Fuzzy rule based classification systems Hierarchical fuzzy partitions Genetic rule selection Tuning Imbalanced data-sets Borderline examples</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lots of real world applications appear to be a matter of classification with imbalanced data-sets. This problem arises when the number of instances from one class is quite different to the number of instances from the other class. Traditionally, classification algorithms are unable to correctly deal with this issue as they are biased towards the majority class. Therefore, algorithms tend to misclassify the minority class which usually is the most interesting one for the application that is being sorted out.</p><p>Among the available learning approaches, fuzzy rule-based classification systems have obtained a good behavior in the scenario of imbalanced data-sets. In this work, we focus on some modifications to further improve the performance of these systems considering the usage of information granulation. Specifically, a positive synergy between data sampling methods and algorithmic modifications is proposed, creating a genetic programming approach that uses linguistic variables in a hierarchical way. These linguistic variables are adapted to the context of the problem with a genetic process that combines rule selection with the adjustment of the lateral position of the labels based on the 2-tuples linguistic model.</p><p>An experimental study is carried out over highly imbalanced and borderline imbalanced data-sets which is completed by a statistical comparative analysis. The results obtained show that the proposed model outperforms several fuzzy rule based classification systems, including a hierarchical approach and presents a better behavior than the C4.5 decision tree.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Learning from imbalanced data-sets is an issue that has attracted a lot of attention in machine learning research <ref type="bibr" target="#b32">[29,</ref><ref type="bibr" target="#b54">51]</ref>. This problem is characterized by a class distribution where the number of examples in one class is outnumbered by the number of examples in the other class. The presence of imbalanced data-sets is dominant in a high number of real problems including, but not limited to, medical diagnosis, fraud detection, finances, risk management, network intrusion and so on. Additionally, the positive or minority class is usually the one that has the highest interest from the learning point of view and it also implies a great cost when it is not well classified <ref type="bibr" target="#b20">[17,</ref><ref type="bibr" target="#b60">57]</ref>.</p><p>A standard classifier that seeks accuracy over a full range of instances is frequently not suitable to deal with imbalanced learning tasks, since it tends to be overwhelmed by the majority class thus misclassifying the minority examples. This situation becomes critical when the minority class is greatly outnumbered by the majority class, generating an scenario of highly imbalanced data-sets where the performance deterioration is amplified. However, some studies have shown that imbalance for itself is not the only factor that hinders the classification performance <ref type="bibr" target="#b40">[37]</ref>. There are several data intrinsic characteristics which lower the learning effectiveness. Some of these handicaps within the data are the presence of small disjuncts <ref type="bibr" target="#b56">[53]</ref>, the overlap between the classes <ref type="bibr" target="#b29">[26]</ref> or the existence of noisy <ref type="bibr" target="#b52">[49]</ref> and borderline <ref type="bibr" target="#b47">[44]</ref> samples. There is no need to say that when the classification data share an skewed data distribution together with any of the aforementioned situations, the performance degradation is intensified <ref type="bibr" target="#b22">[19,</ref><ref type="bibr" target="#b45">42,</ref><ref type="bibr" target="#b56">53]</ref>.</p><p>A large number of approaches have been proposed to deal with the class imbalance problem. Those solutions fall largely into two major categories. The first is data sampling in which the training data distribution is modified to obtain a set with a balanced distribution. Standard classifiers are thus helped to obtain a correct identification of data <ref type="bibr" target="#b12">[9,</ref><ref type="bibr" target="#b9">6]</ref>. The second is through algorithmic modification where the base learning methods are modified to consider the imbalanced distribution of the data. In this manner, base learning methods change some of its internal operations accordingly <ref type="bibr" target="#b60">[57]</ref>.</p><p>Fuzzy Rule-Based Classification Systems (FRBCSs) <ref type="bibr" target="#b37">[34]</ref> are useful and well-known tools in the machine learning framework. They provide a good trade-off between the empirical precision of traditional engineering techniques and the interpretability achieved through the use of linguistic labels whose semantic is close to the natural language. Specifically, recent works have shown that FRBCSs have a good behavior dealing with imbalanced data-sets by means of the application of instance preprocessing techniques <ref type="bibr" target="#b23">[20]</ref>.</p><p>The hybridization between fuzzy logic and genetic algorithms leading to Genetic Fuzzy Systems (GFSs) <ref type="bibr" target="#b15">[12,</ref><ref type="bibr" target="#b33">30]</ref> is one of the most popular approaches used when different computational intelligence techniques are combined. A GFS is basically a fuzzy system augmented by a learning process based on evolutionary computation. Among evolutionary algorithms, Genetic Programming (GP) <ref type="bibr" target="#b42">[39]</ref> is a development of classical genetic algorithms that evolve tree-shaped solutions using variable length chromosomes. GP has been used in FRBCSs to learn fuzzy rule bases <ref type="bibr" target="#b10">[7]</ref> profitting from its high expressive power and flexibility.</p><p>However, the disadvantage of FRBCSs is the inflexibility of the concept of linguistic variable because it imposes hard restrictions on the fuzzy rule structure <ref type="bibr" target="#b8">[5]</ref> which may suppose a loss in accuracy when dealing with some complex systems, such as high dimensional problems, the presence of noise or overlapped classes. Many different possibilities to enhance the linguistic fuzzy modeling have been considered in the specialized literature. All of these approaches share the common idea of improving the way in which the linguistic fuzzy model performs the interpolative reasoning by inducing a better cooperation among the rules in the Knowledge Base (KB). This rule cooperation may be induced acting on three different model components:</p><p>Approaches acting on the whole KB. This includes the KB derivation <ref type="bibr" target="#b46">[43]</ref> and a hierarchical linguistic rule learning <ref type="bibr" target="#b17">[14]</ref>. Approaches acting on the Rule Base (RB). The most common approach is rule selection <ref type="bibr" target="#b38">[35]</ref> but also multiple rule consequent learning <ref type="bibr" target="#b14">[11]</ref> could be considered. Approaches acting on the Data Base (DB). For example a priori granularity learning <ref type="bibr" target="#b16">[13]</ref> or membership function tuning <ref type="bibr" target="#b4">[1]</ref>.</p><p>In this work, we present a procedure to obtain an Hierarchical Fuzzy Rule Based Classification System (HFRBCS) to deal with imbalanced data-sets. In order to do so, this model introduces modifications both at the data and algorithm level. This procedure is divided into three different steps:</p><p>1. A preprocessing technique, the Synthetic Minority Oversampling Technique (SMOTE) <ref type="bibr" target="#b12">[9]</ref>, is used to balance the distribution of training examples in both classes. 2. A hierarchical knowledge base (HKB) <ref type="bibr" target="#b17">[14]</ref> is generated, using the GP-COACH (Genetic Programming-based learning of COmpact and ACcurate fuzzy rule-based classification systems for High-dimensional problems) algorithm <ref type="bibr" target="#b10">[7]</ref> to build the RB.</p><p>The GP-COACH algorithm has been modified to extend a classical KB into a HKB, integrating a rule expansion process to create high granularity rules in each generation of the algorithm. The usage of a HKB implies an adaptation of the components to allow the interaction of the different granularities in the RB population. 3. A post-processing step involving rule selection and the application of the 2-tuples based genetic tuning is applied to improve the overall performance.</p><p>The combination of these steps constitutes a convenient approach to solve the problem of classification with imbalanced data-sets. First of all, the preprocessing technique compensates the number of instances for each class easing the learning process for the consequent procedures. Then, the step to learn the HKB is used to address the imbalanced problem together with some of the data intrinsic characteristics that difficult the learning. This HKB process is appropriate because it increases the accuracy by reinforcing those problem subspaces that are specially difficult in this environment, such as borderline instances <ref type="bibr" target="#b47">[44]</ref>, small disjuncts <ref type="bibr" target="#b40">[37]</ref> or overlapping regions <ref type="bibr" target="#b29">[26]</ref>. Finally, the post-processing step refines the results achieved by the previous process. The integration of these schemes completes our proposal, which will be denoted as GP-COACH-H (GP-COACH Hierarchical).</p><p>We will focus on two difficult situations in the scenario of imbalanced data, such as highly imbalanced and borderline imbalanced classification problems. For that, we have selected a benchmark of 44 and 30 problems respectively from KEEL data-set repository<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b5">[2]</ref>. We will perform our experimental analysis focusing on the precision of the models using the Geometric Mean of the true rates (GM) <ref type="bibr" target="#b7">[4]</ref>. This study will be carried out using non-parametric tests to check whether there are significant differences among the obtained results <ref type="bibr" target="#b28">[25]</ref>.</p><p>This work is structured in the following way. First, Section 2 presents an introduction of classification with imbalanced problems, describing its features, the SMOTE algorithm and the metrics that are used in this framework. Next, Section 3 introduces the proposed approach. Sections 4 and 5 describe the experimental framework used and the analysis of results, respectively. Next, the conclusions achieved in this work are shown in Section 6. Finally, we include an appendix with the detailed results for the experiments performed in the experimental study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Imbalanced data-sets in classification</head><p>In this section we delimit the context in which this work is content, briefly introducing the problem of imbalanced classification. Then, we will describe the preprocessing technique that we have applied in order to deal with the imbalanced data-sets: the SMOTE algorithm <ref type="bibr" target="#b12">[9]</ref>. We finish this section describing the evaluation metrics that are used in this specific problem with respect to the most common ones in classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The problem of imbalanced data-sets</head><p>In some classification problems, the number of examples that represent the diverse classes is very different. Specifically, the imbalance problem occurs when one class is represented only by a few number of examples, while the others are represented by a large number of examples <ref type="bibr" target="#b54">[51,</ref><ref type="bibr" target="#b32">29]</ref>. In this paper, we focus on two-class imbalanced data-sets, where there is a positive (minority) class, with the lowest number of instances, and a negative (majority) class, with the highest number of instances.</p><p>This problem is prevalent in many real world applications, such as medical diagnosis <ref type="bibr" target="#b48">[45,</ref><ref type="bibr" target="#b51">48]</ref>, anomaly detection <ref type="bibr" target="#b41">[38]</ref>, image analysis <ref type="bibr" target="#b11">[8]</ref> or bioinformatics <ref type="bibr" target="#b31">[28]</ref>, just referencing some of them. Furthermore, it is usual that positive classes are the most interesting from the application point of view so it is crucial to correctly identify these cases. The importance of this problem in the aforementioned uses has increased the attention towards it, which has been considered one of the 10 challenging problems in data mining <ref type="bibr" target="#b59">[56]</ref>.</p><p>Although these issues occur frequently in data, many data mining methods do not naturally perform well under these circumstances. In fact, many only work optimally when the classes in data are relatively balanced. Furthermore, the performance of algorithms is usually more degraded when the imbalance increases because positive examples are more easily forgotten. That situation is critical in highly imbalanced data-sets because the number of positive instances in the data-set is negligible and that situation increases the difficulty that most learning algorithms have in detecting positive regions. Figs. 1 and 2 depict two data-sets with low imbalance and high imbalance respectively.</p><p>However, the imbalanced data-set is also affected by some circumstances that make the learning more difficult. For example, metrics that have been used traditionally seem inappropriate in this scenario when they ascribe a high performance to a trivial classifier that predicts all samples as negative. This behavior is wrapped up in the inner way of building an accurate model, preferring general rules with good coverage for the negative class and disregarding more specific rules which are the ones associated to the positive class.</p><p>An important issue that appear in imbalanced data-sets is the presence of borderline examples. Inspired by Kubat and Matwin <ref type="bibr" target="#b43">[40]</ref> we may distinguish between safe, noisy and borderline exam- These borderline examples make difficult to determine a correct discrimination of the classes. For instance, Napierala et al. <ref type="bibr" target="#b47">[44]</ref> present in a series of experiments in which it is shown that the degradation in performance of a classifier in an imbalanced scenario is strongly affected by the number of borderline examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Addressing imbalanced data-sets: use of preprocessing and SMOTE algorithm</head><p>A large number of approaches have been proposed to deal with the class-imbalance problem <ref type="bibr" target="#b54">[51,</ref><ref type="bibr" target="#b44">41,</ref><ref type="bibr" target="#b45">42]</ref>. These approaches can be categorized in two groups: the internal approaches that create new algorithms or modify existing ones to take the classimbalance problem into consideration <ref type="bibr" target="#b7">[4]</ref> and external approaches that preprocess the data in order to diminish the effect of their class imbalance <ref type="bibr" target="#b9">[6,</ref><ref type="bibr" target="#b26">23,</ref><ref type="bibr" target="#b30">27]</ref>. Furthermore, cost-sensitive learning solutions incorporating both approaches assume higher misclassification costs with samples in the positive class and seek to minimize the high cost errors <ref type="bibr" target="#b20">[17,</ref><ref type="bibr" target="#b60">57]</ref>. The great advantage of the external approaches is that they are more versatile, since their use is independent of the classifier selected. Furthermore, we may preprocess all data-sets before-hand in order to use them to train different classifiers. In this manner, the computation time needed to prepare the data is only required once. According to this, in this work we have chosen an oversampling method which is a reference in this area: the SMOTE algorithm <ref type="bibr" target="#b12">[9]</ref> and a variant called SMOTE + ENN <ref type="bibr" target="#b9">[6]</ref>.</p><p>In this approach, the positive class is over-sampled by taking each positive class sample and introducing synthetic examples along the line segments joining any/all of the k positive class nearest neighbors. Depending upon the amount of over-sampling required, neighbors from the k nearest neighbors are randomly chosen. This process is illustrated in Fig. <ref type="figure" target="#fig_3">3</ref>, where x i is the selected point, x i1 to x i4 are some selected nearest neighbors and r 1 to r 4 the synthetic data points created by the randomized interpolation.</p><p>Synthetic samples are generated in the following way: take the difference between the feature vector (sample) under consideration and its nearest neighbor. Multiply this difference by a random number between 0 and 1, and add it to the feature vector under consideration. This causes the selection of a random point along the line segment between two specific features. This approach effectively forces the decision region of the positive class to become more general. An example is detailed in Fig. <ref type="figure" target="#fig_4">4</ref>.</p><p>In short, its main feature is to form new positive class examples by interpolating between several positive class examples that lie together. Thus, the overfitting problem is avoided and causes the decision boundaries for the positive class to spread further into the negative class space.</p><p>Nevertheless, class clusters may be not well defined in cases where some negative class examples might be invading the positive class space. The opposite can also be true, since interpolating positive class examples can expand the positive class clusters, introducing artificial positive class examples too deeply into the negative class space. Inducing a classifier in such a situation can lead to over-fitting. For this reason we will also consider in this work a hybrid approach, ''SMOTE+ENN'', where the Wilson's Edited Nearest Neighbor Rule <ref type="bibr" target="#b57">[54]</ref> is used after the SMOTE application to remove any example from the training set misclassified by its three nearest neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Evaluation in imbalanced domains</head><p>The measures of the quality of classification are built from a confusion matrix (shown in Table <ref type="table" target="#tab_0">1</ref>) which records correctly and incorrectly recognized examples for each class.   The most used empirical measure, accuracy (Eq. ( <ref type="formula" target="#formula_0">1</ref>)), does not distinguish between the number of correct labels of different classes, which in the ambit of imbalanced problems may lead to erroneous conclusions. For example a classifier that obtains an accuracy of 90% in a data-set with a 90% of negative instances, might not be accurate if it does not cover correctly any positive class instance.</p><formula xml:id="formula_0">Acc ¼ TP þ TN TP þ FN þ FP þ TN<label>ð1Þ</label></formula><p>Because of this, instead of using accuracy, more appropriate metrics in this situation are considered. Two common measures, sensitivity and specificity (Eqs. ( <ref type="formula" target="#formula_1">2</ref>) and ( <ref type="formula" target="#formula_2">3</ref>)), approximate the probability of the positive (negative) label being true. In other words, they assess the effectiveness of the algorithm on a single class.</p><formula xml:id="formula_1">sensitivity ¼ TP TP þ FN<label>ð2Þ</label></formula><formula xml:id="formula_2">specificity ¼ TN FP þ TN<label>ð3Þ</label></formula><p>The metric used in this work is the geometric mean of the true rates <ref type="bibr" target="#b7">[4,</ref><ref type="bibr" target="#b43">40]</ref>, which can be defined as</p><formula xml:id="formula_3">GM ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi TP TP þ FN Á TN FP þ TN r<label>ð4Þ</label></formula><p>This metric attempts to maximize the accuracy of each one of the two classes with a good balance. It is a performance metric that links both objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The hierarchical genetic programming fuzzy rule based classification system with rule selection and tuning (GP-COACH-H)</head><p>In this section, we will describe our proposal to obtain a hierarchical FRBCS through the usage of GP and applying rule selection together with 2-tuples lateral tuning, denoted as GP-COACH-H. This proposal is defined through its components in the following way: Section 3.1 presents a brief introduction of FRBCSs in order to contextualize the algorithm; next, Section 3.2 describes the GP-COACH algorithm <ref type="bibr" target="#b10">[7]</ref> which is the linguistic rule generation method based on GP that we have used as base for our proposal of a hierarchical rule base generation method; later, in Section 3.3, the building of the hierarchical fuzzy rule based classification is detailed, mentioning the modifications the hierarchical procedure introduces in the knowledge base generation and in the basic running of the GP-COACH algorithm; subsequently, Section 3.4</p><p>shows the selection of the best cooperative rules and the tuning of the databases in a genetic process where both objectives collaborate; and finally, Section 3.5 summarizes the description of the proposal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Fuzzy rule based classification systems</head><p>FRBCSs are useful and well-known tools in the machine learning framework since they can provide an interpretable model for the end user. A FRBCS has two main components: the Inference System and the KB. In a linguistic FRBCS, the KB is composed of a RB, constituted by a set of fuzzy rules, and the DB that stores the membership functions of the fuzzy partitions associated to the input variables. If expert knowledge of the problem is not available, it is necessary to use some Machine Learning process to obtain the KB from examples.</p><p>Any classification problem consists of N training patterns x p = (x p1 , . . . , x pn ), p = 1,2,. . . , m from M classes where x pi is the ith attribute value (i = 1,2,. . ., n) of the pth training pattern.</p><p>In this work, we use fuzzy rules of the following form to build our classifier:</p><formula xml:id="formula_4">Rule R j : If x 1 is b A j 1 and . . . and x n is b A j n then Class ¼ C j with RW j<label>ð5Þ</label></formula><p>where R j is the label of the jth rule, x = (x 1 , . . . , x n ) is an ndimensional pattern vector, b A j i is a set of linguistic labels fL 1 i or . . . or L l k i g joined by a disjunctive operator, C j is a class label, and RW j is the rule weight <ref type="bibr" target="#b36">[33]</ref>. We use triangular membership functions as linguistic labels whose combination will form an antecedent fuzzy set. This kind of rule is called a DNF fuzzy rule.</p><p>To compute the rule weight, many heuristics have been proposed <ref type="bibr" target="#b39">[36]</ref>. In our proposal, we compute the rule weight as the fuzzy confidence or Certainty Factor (CF) <ref type="bibr" target="#b18">[15]</ref>, showed in Eq. ( <ref type="formula" target="#formula_5">6</ref>):</p><formula xml:id="formula_5">RW j ¼ CF j ¼ P xp2ClassC j l b A j ðx p Þ P N p¼1 l b A j ðx p Þ<label>ð6Þ</label></formula><p>where l b A j ðx p Þ is the matching degree of the pattern x p with the antecedent part of the fuzzy rule R j .</p><p>GP-COACH-H uses the normalized sum fuzzy reasoning method <ref type="bibr" target="#b18">[15]</ref> for classifying new patterns by the RB, a general reasoning model for combining information provided by different rules, where each rule promotes the classification with its consequent class according to the matching degree of the pattern with the antecedent part of the fuzzy rule together with its weight. The total sum for each class is computed as follows:</p><formula xml:id="formula_6">Sum Class h ðx p Þ ¼ P R j 2RB;C j ¼h l b A j ðx p Þ Á CF j max c¼1;...;M P R j 2RB;C j ¼c l b A j ðx p Þ Á CF j<label>ð7Þ</label></formula><formula xml:id="formula_7">Classðx p Þ ¼ arg maxðSum Class h ðx p ÞÞ<label>ð8Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The GP-COACH algorithm</head><p>The GP-COACH algorithm <ref type="bibr" target="#b10">[7]</ref> is a genetic programming-based algorithm for the learning of fuzzy rule bases. We will use this method as a base for our hierarchical model modifying its behavior to include the different granularity levels into its inner way of running.</p><p>This algorithm is a genetic cooperative-competitive learning approach where the whole population represents the RB obtained. Each individual in the population codifies a rule. These rules are DNF fuzzy rules (Eq. ( <ref type="formula" target="#formula_4">5</ref>)) which allow the absence of some input features and are generated according to the production rules of a context-free grammar. As DB we are using linguistic partitions  with the same number of linguistic terms for all input variables, composed of symmetrical triangular-shaped and uniformly distributed membership functions. There are two evaluation functions in the GP-COACH algorithm: a local fitness function, known as raw_fitness, to evaluate the performance of each rule and a global fitness function, known as global_fitness, to evaluate the behavior of the whole rule population. The raw_fitness is computed according to Confidence (shown in Eq. ( <ref type="formula" target="#formula_5">6</ref>)) and Support, which measure the accuracy of the rule and the extent of knowledge of the rule respectively:</p><formula xml:id="formula_8">SupportðR j Þ ¼ P xp2ClassC j l A j ðx p Þ N C j<label>ð9Þ</label></formula><p>where N C j is the number of examples that belong to the same class that the one determined in the consequent of the rule. Therefore, the raw_fitness is computed in the following way:</p><formula xml:id="formula_9">raw fitnessðR j Þ ¼ a Á ConfidenceðR j Þ þ ð1 À aÞ Á SupportðR j Þ<label>ð10Þ</label></formula><p>Finally, it is important to point out that each time that an individual is evaluated it is also necessary to modify its certainty degree. On the other hand, the global_fitness score measure is defined as follows:</p><formula xml:id="formula_10">global fitness ¼ w 1 Á accuracy þ w 2 Á ð1:0 À Var N Þ þ w 3 Á ð1:0 À Cond N Þ þ w 4 Á ð1:0 À Rul N Þ ð<label>11Þ</label></formula><p>where Var N and Cond N are the normalized values of the average number of variables and conditions in the rules, and Rul N is the normalized number of rules in the population respectively. The GP-COACH algorithm also includes a mechanism for maintaining the diversity in the population: the token competition procedure <ref type="bibr" target="#b58">[55]</ref>, inspired by the following natural behavior: when an individual finds a good place to live, it will maintain its position there preventing the others to share its position unless they are stronger. Each example in the training set is called a token and the rules in the population compete to acquire as many tokens as possible. When a rule matches an example, it tries to seize the token, however, this token will be assigned to the stronger rule that matches the example. Stronger individuals exploit their dominant position by seizing as many tokens as they can. The other ones entering the same position will have their strength decreased because they cannot compete with the stronger ones, by the addition of a penalization in the fitness score of the individual. Therefore, to model this behavior, a penalized_function is defined:</p><formula xml:id="formula_11">penalized fitnessðR j Þ ¼ raw fitnessðR j Þ Á countðR j Þ idealðR j Þ if idealðR j Þ &gt; 0; 0; otherwise (<label>ð12Þ</label></formula><p>where raw_fitness(R j ) is the fitness score obtained from the evaluation function (Eq. ( <ref type="formula" target="#formula_9">10</ref>)), count(R j ) is the number of tokens that the individual actually seized and ideal(R j ) is the total number of tokens that it can seize, which is equal to the number of examples that the individual matches. As a result of the token competition, there can be individuals that cannot grab any token. These individuals are considered as irrelevant, and they are eliminated from the population because all of their examples are covered by other stronger individuals.</p><p>Once the token competition mechanism has been applied, it is possible that some of the examples in the training set are not covered by any of the rules in the population. The generation of new specific rules covering these examples improves the diversity in the population, and helps the evolutionary process to easily find stronger and more general rules covering these examples. Therefore, GP-COACH learns rule sets having two different types of fuzzy rules: a core of strong and general rules (primary rules) that cover most of the examples, and a small set of weaker and more specific rules (secondary rules) that are only used if there are not any primary rule matching the example. These secondary rules are generated by the Chi et al. algorithm <ref type="bibr" target="#b13">[10]</ref> over the set of training examples that are left uncovered by the primary rules. This scaly scheme is used in rule based algorithms to cover in a better way the data space <ref type="bibr" target="#b55">[52]</ref>. GP-COACH uses four different genetic operators to generate new individuals during the evolutionary process:</p><p>1. Crossover: A part in the first parent is randomly selected and exchanged by another part, randomly selected, in the second one. 2. Mutation: It is applied to a variable in the rule randomly chosen.</p><p>The mutation can add a new label to the label set associated to the variable, remove a label from the label set associated to the variable or exchange one label in the label set associated to the variable with another one not included. 3. Insertion: It adds a new variable to the parent rule with at least one linguistic label. 4. Dropping condition: It selects one variable and removes its conditions from the rule.</p><p>These operations only generate one offspring each time they are applied.</p><p>Fig. <ref type="figure">5</ref> shows the pseudocode associated to the GP-COACH algorithm. This method begins creating a random initial population according to the rules in the context-free grammar. Each individual in this population is then evaluated. After that, the initial population is kept as the best evolved population and its global fitness score is computed. Then, the initial population is copied to the current population and the evolutionary process begins:</p><p>1. An offspring population, with the same size than the current one, is created. Parents are selected by using the tournament selection mechanism and children are created by using one of the four genetic operators. The genetic operator selection is done in a probabilistic way according to a given probability. 2. Once the offspring population is created, it is joined to the current population, creating a new population whose size is double the current population size. Individuals in this new population are sorted according to their fitness and the token competition mechanism is applied. Secondary rules are created if some examples remain uncovered.</p><p>Fig. <ref type="figure">5</ref>. The GP-COACH algorithm.</p><p>3. The global fitness score measure is then calculated for this new population. We check whether this new fitness is better than the one stored for the best population, updating the best population and fitness if necessary. In any case, the new population is copied as the current population in order to be able to apply the evolutionary process again.</p><p>The evolutionary process ends when the stop condition is verified, that is when a number of evaluations is reached. Then, the population kept as the best one is returned as the solution to the problem and GP-COACH finishes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Hierarchical fuzzy rule based classification system construction</head><p>HFRBCs try to improve the performance of fuzzy rule based systems in data subspaces that are particularly difficult. In order to do so, instead of the classical definition of the KB, we use an extension known as HKB <ref type="bibr" target="#b17">[14]</ref>, which is composed of a set of layers. We will divide this subsection in two parts: the first part is devoted to the presentation of the HKB, its components and some general guidelines about how to build it; the second part is devoted to the integration of the HKB into the inner way of running of the GP-COACH algorithm which we have used as base for our proposal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Hierarchical knowledge base</head><p>In order to overcome the inflexibility of the concept of linguistic variable which degrades the performance of algorithms in complex search spaces, we extend the definition of the standard KB into an hierarchical one that preserves the original model descriptive power and increases its accuracy. This HKB is composed of a set of layers. We define a layer by its components in the following way: layerðt; nðtÞÞ ¼ DBðt; nðtÞÞ þ RBðt; nðtÞÞ ð13Þ</p><p>with n(t) being the number of linguistic terms that compose the partitions of layer, DB(t, n(t)) (t-linguistic partitions) being the DB which contains the linguistic partitions with granularity level n(t) of layer, and RB(t, n(t)) (t-linguistic rules) being the RB formed by those linguistic rules whose linguistic variables take values in the former partitions. The number of linguistic terms in the t-linguistic partitions is defined in the following way:</p><formula xml:id="formula_12">nðtÞ ¼ ðnð1Þ À 1Þ Á 2 tÀ1 þ 1<label>ð14Þ</label></formula><p>with n(1) being the granularity of the initial fuzzy partitions. This set of layers is organized as a hierarchy, where the order is given by the granularity level of the linguistic partition defined in each layer. That is, given two successive layers t and t + 1 then the granularity level of the linguistic partitions of layer t + 1 is greater than the ones of layer t. This causes a refinement of the previous layer linguistic partitions. As a consequence of the previous definitions, we can now define the HKB as the union of every layer t HKB ¼ [</p><formula xml:id="formula_13">t layerðt; nðtÞÞ<label>ð15Þ</label></formula><p>Our proposal considers a two-layer HKB, i.e. starting with an initial layer t, we produce layer t + 1 in order to extract the final system of linguistic rules. This fact allows the approach to build a significantly more accurate modeling of the problem space. First of all, we need to build the two-layer HDB. The first level layer is built by the usage of linguistic partitions with the same number of linguistic terms for all input variables, composed of symmetrical triangular-shaped and uniformly distributed membership functions. The second layer, is built preserving all the membership function modal points, corresponding to each linguistic term, through the higher layers of the hierarchy and adding a new linguistic term between each two consecutive terms of the t-linguistic partition reducing the support of these linguistic terms in order to keep place for the new one, which is located in the middle of them. Fig. <ref type="figure">6</ref> shows the linguistic partitions from one level to another, with n(1) = 3 and n(2) = 5.</p><p>The second step affects the generation of the HRB which is composed by the RB of layer t and a RB of layer t + 1. Two measures of error are usually used to build a RB of layer t + 1 from a layer RB of layer t: a global measure, which is used to evaluate the complete RB, and a local measure, used to determine the goodness of the rules. We calculate these measures similarly to other HFRBCS methodologies focused on classification problems <ref type="bibr" target="#b24">[21]</ref>. The global measure used is the accuracy per class, computed as:</p><formula xml:id="formula_14">Acc i ðX i ; RBÞ ¼ jx p 2 X i =FRMðx p ; RBÞ ¼ Classðx p Þj jX i j<label>ð16Þ</label></formula><p>where j j is the number of patterns, X i is the set of examples of the training set that belong to the ith class, FRM(x p , RB) is the class prediction of the pattern using the rules in the RB with the FRM used by the GP-COACH algorithm, and Class(x p ) is the class label for example x p . The local measure utilized is the accuracy for a rule, computed over the whole training set as</p><formula xml:id="formula_15">AccðX; R j Þ ¼ jX þ ðR j Þj jXðR j Þj<label>ð17Þ</label></formula><p>It is important to remind that since we are using the normalized sum approach as FRM, X + (R j ) and X(R j ) are defined as X(R j ) is the set of examples that have a matching degree with the rule higher than 0 where this compatibility has contributed to classify the sample as the class label of the rule. X + (R j ) is the set of examples that have a matching degree with the rule higher than 0 where this compatibility has contributed to classify the sample as the class label of the rule and where the predicted class corresponds with the class label of the example.</p><p>DB <ref type="bibr" target="#b4">(1,</ref><ref type="bibr" target="#b6">3)</ref> DB(2,5) For each example in the training set, we obtain a set of rules that have contributed to the classification when we compute the global measure. Therefore, when we try to compute X + (R j ) and X(R j ) we have for each rule the set of examples where the current rule has contributed to its classification.</p><formula xml:id="formula_16">3 1</formula><p>Once we have computed the global measure and the local measure, we characterize the rules as good or bad according to the following calculation:</p><formula xml:id="formula_17">If (Acc(X,R j ) 6 (1 À a) Á Acc i (X i ,RB)) Then R j = good rule Else R j = bad rule</formula><p>Good rules are kept in the rule population while bad rules are deleted from the current population. Then, new high granularity rules are created using as linguistic rule generator with the DB associated to layer t + 1 and adopting as training set for this task a subset of the original training set including examples that meets some specified conditions. If after the generation of these rules we find repeated rules we only keep one copy of them, or if we find contradictory rules (rules with the same antecedent but with different consequents) we maintain the rule with a higher rule weight in the RB while the others are removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Integration of a HKB in the GP-COACH algorithm</head><p>The usage of a HKB in the inner way of running of the GP-COACH algorithm induces some changes in its structure. For example, the existence of the HRB which is composed by the RB of layer t and a RB of layer t + 1 forces the GP-COACH algorithm to provide a mechanism to maintain these two RB levels. In our case, these RBs are merged and are evolved together in the different generations computed in the GP-COACH algorithm.</p><p>The rule population used in the algorithm is now a mixed population that combines primary rules and secondary rules where the secondary rules present different granularities. In this kind of population, genetic operators obtain rules according to the type of parent rule: primary rules obtain primary rules while secondary rules obtain secondary rules maintaining the granularity of the original rule. The only restriction in the application of the genetic operations appears in the usage of the crossover operation where the rules selected for the generation of a new rule must have the same granularity.</p><p>The global fitness score is modified to consider the different granularities of the rules in the population. The new global fitness function is:</p><formula xml:id="formula_18">global fitness ¼ w 1 Á accuracy þ w 2 Á ð1:0 À Var N Þ þ w 3 Á 1:0 À ðCond Low N Á R Low þ Cond High N Á R HighÞ R þ w 4 Á ð1:0 À Rul N Þ<label>ð18Þ</label></formula><p>where Var N is the normalized average number of variables, Cond_Low N is the normalized average number of conditions in low granularity rules, Cond_High N is the normalized average number of conditions in high granularity rules, Rul N is the normalized number of rules and R_Low,R_High,R are the number of low granularity rules, high granularity rules and total number of rules respectively.</p><p>To generate the high granularity rules some additional steps are performed just after the final step of a GP-COACH generation which is the construction of secondary rules for examples that have not been covered with the current rule base. This process is done performing the following operations:</p><p>1. The rules that compose the rule set are classified as good rules or bad rules as explained in the previous subsection. 2. Good rules are kept in the rule population and bad rules are directly deleted. 3. New high granularity rules are created using as linguistic rule generator the Chi et al. algorithm <ref type="bibr" target="#b13">[10]</ref> with the DB associated to layer t + 1 and adopting as training set for this task the examples that were classified by the rules that were considered bad rules. 4. Repeated and contradictory rules are searched for and only one copy of the best performing is kept.</p><p>Usually, when creating a hierarchical rule base, another step is added to improve the performance of the final model: a hierarchical rule selection step. In our case, since the hierarchical expansion of rules is embedded into each generation of the GP-COACH algorithm, adding a genetic selection process would increase considerably the run time of the approach. Therefore, this rule selection step is appended after the GP-COACH generations end combined with a tuning step to take advantage of the synergy between these refinements of the KB. Furthermore, GP-COACH tries to obtain a compact rule population with the token competition procedure making thus this delay of the rule selection step possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Hierarchical rule base selection and lateral tuning</head><p>In this last step, we analyze the use of genetic algorithms to select and tune a compact and cooperative set of fuzzy rules that obtain a high performance starting from the hierarchical rules generated in the previous step. In order to do so, we consider the approach used by Alcalá et al. <ref type="bibr" target="#b4">[1]</ref> that uses the linguistic 2-tuples representation <ref type="bibr" target="#b35">[32]</ref>. This representation allows the lateral displacement of the labels considering only one parameter (symbolic translation parameter), which involves a simplification of the tuning search space that aids the obtaining of optimal models. Particularly this happens when it is combined with a rule selection within the same process enabling it to take advantage of the positive synergy that both techniques present. In this way, this process for contextualizing the membership functions permits them to achieve a better covering degree while maintaining the original shapes, which results in accuracy improvements without a significant loss in the interpretability of the fuzzy labels. The symbolic translation parameter of a linguistic term is a number within the interval [ À0.5,0.5) that expresses the domain of a label when it is moving between its two lateral labels. Let us consider a set of labels S representing a fuzzy partition. Formally, we have the pair, (s i , a i ), s i 2 S, a i 2 [À0.5, 0.5). An example is illustrated in Fig. <ref type="figure" target="#fig_6">7</ref> where we show the symbolic translation of a label represented by the pair (S 2 , À0.3). Alcalá et al. <ref type="bibr" target="#b4">[1]</ref> proposed two different rule representation approaches, a global approach and a local approach. In our algorithm, the tuning is applied to the level of linguistic partitions (global approach). In this way, the pair (X i , label) takes the same tuning value in all the rules where it is considered. For example, X 1is (High, 0.3) will present the same value for those rules in which the pair ''X 1 is High'' was initially considered. This proposal decreases the tuning problem complexity, greatly easing the derivation of optimal models.</p><p>To accomplish this rule selection and lateral tuning process, we consider the use of a specific genetic algorithm, the CHC evolutionary algorithm <ref type="bibr" target="#b21">[18]</ref> with the same scheme described in our previous works <ref type="bibr" target="#b24">[21,</ref><ref type="bibr" target="#b25">22]</ref>. In the remainder of this section, we describe the specific features of our new tuning approach, which involves the codification of the solutions and initial gene pool, chromosome evaluation, crossover operator and restarting approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Codification and Initial Gene Pool: To combine the rule selection</head><p>with the global lateral tuning, a double coding scheme for both rule selection (C S ) and lateral tuning (C T ) is used: For the C S part, each chromosome is a binary vector that determines when a rule is selected or not (alleles '1' and '0' respectively). Considering the M rules contained in the candidate rule set (rules from the two hierarchical levels considered), the corresponding part C S = {c 1 , . . . , c M } represents a subset of rules composing the final rule base, so that, If c j = 1then (R j 2 RB) else (R j R RB), with R j being the corresponding jth rule in the candidate rule set and RB being the final RB.</p><p>For the C T part, a real coding is considered. This part is the joint of the a parameters of each fuzzy partition. Let us consider the following number of labels per variable: (ml 1 , ml 2 , . . . , ml n ) for low granularity rules and (mh </p><formula xml:id="formula_19">C = C S C T .</formula><p>To make use of the available information, all the candidate rules are included in the population as an initial solution. To do this, the initial pool is obtained with the first individual having all genes with value '1' in the C S part and all genes with value '0.0' in the C T part. The remaining individuals are generated at random.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Chromosome Evaluation:</head><p>To evaluate a determined chromosome we compute its accuracy over the training set. If two individuals obtain the same value, then the individual with the lower number of selected rules is preferred. 3. Crossover Operator: The crossover operator will depend on the chromosome part where it is applied:</p><p>In the C S part, the half uniform crossover scheme (HUX) is employed.</p><p>For the C T part, we consider the Parent Centric BLX (PCBLX) operator <ref type="bibr" target="#b34">[31]</ref>, which is based on BLX-a. 4. Restarting Approach: To get away from local optima, this algorithm uses a restart approach that is performed to improve the diversity of the population that may be reduced by the strong elitist pressure of the replacement scheme.</p><p>For details about the remainder features of the optimization process, please refer to Fernández et al. <ref type="bibr" target="#b24">[21]</ref> and Fernández et al. <ref type="bibr" target="#b25">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Summary of the GP-COACH-H algorithm</head><p>Once every step of the algorithm has been explained we briefly sum up how the GP-COACH-H algorithm works. Fig. <ref type="figure" target="#fig_7">8</ref> depicts a flowchart of the GP-COACH-H algorithm. There are three different steps in the building of the model:</p><p>1. Preprocessing stage: In this first step, GP-COACH-H preprocesses the original data-set to balance the class distribution. In order to do so, the SMOTE algorithm is used, as described in subSection 2.2. 2. Generation of the HKB: This stage is devoted to the generation of a two-layer HKB from the balanced data-set. This HKB is composed by two different DBs (each one with a different granularity level) and one RB that contains rules from the two hierarchies: (a) HDB Generation: The first layer DB is created with the same number of linguistic terms for all input variables, composed of symmetrical triangular-shaped and uniformly distributed membership functions. The second layer, is built preserving all the membership function modal points, corresponding to each linguistic term. (b) HRB Generation: In order to generate the HRB we use as a base the GP-COACH algorithm, which has been modified to incorporate in its internal way of running the creation of hierarchical rules. The adjustments reinforce the connection between the GP-COACH algorithm and the hierarchical methodology because they have been designed to get the greatest possible performance. Specifically, these modifications include: A step to identify good and bad rules, where bad rules are deleted and the examples covered by them are used to create new high granularity rules. Changes in the global fitness function considering the different granularities in the rule population. A variation on the conditions of the application of the crossover operator where only rules with the same granularity level are allowed to produce an offspring.</p><p>This HRB generation procedure uses the preprocessed data-set from the previous step and the membership functions defined by the HDB. 3. Refinement of the HKB: After the building of an initial HKB in the previous phase, another genetic procedure is applied to improve the final performance of this solution. In this step, rules that cooperate properly in the population are selected and the HDB is tuned with the 2-tuples linguistic representation. These optimizations are done in a single step to take advantage of the synergy that both techniques can achieve. The set of selected rules define the final HRB given as solution and the tuning parameters obtained modify the original HDB to create the final HDB which is the output of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental framework</head><p>In this section, we present the set up of the experimental framework used to develop the analysis of our proposal. First we introduce the algorithms selected for the comparison with the proposed approach and their configuration parameters (subSection 4.1). Next, we provide details of the problems chosen for the experimentation (subSection 4.2). Finally, we present the statistical tests applied to compare the results obtained with the different classifiers (subSection 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Algorithms selected for the study and parameters</head><p>In order to test the performance of our approach, GP-COACH-H, several classification methods have been selected to perform the experimental study. These methods are: GP-COACH <ref type="bibr" target="#b10">[7]</ref>: The original FRBCS that was used as base for our approach, a GP-based algorithm for the learning of compact and interpretable fuzzy rule bases that obtains good accuracy in high dimensional classification problems. HFRBCS(Chi) <ref type="bibr" target="#b24">[21]</ref>: This approach obtains a Hierarchical Fuzzy Rule Base Classification System (HFRBCS) using the Chi et al. algorithm <ref type="bibr" target="#b13">[10]</ref> as the linguistic rule generation method and has reported good results in imbalanced data-sets. C4.5 <ref type="bibr" target="#b50">[47]</ref>: A well-known decision tree which has shown a good behavior in the framework of imbalanced data-sets <ref type="bibr" target="#b9">[6]</ref>.</p><p>The configuration parameters used for these algorithms are shown in Table <ref type="table" target="#tab_2">2</ref>. All the methods were run using KEEL software<ref type="foot" target="#foot_3">2</ref>  <ref type="bibr" target="#b6">[3]</ref>, following the default parameter values given in the KEEL platform to configure the methods, which were selected according to the recommendation of the corresponding authors of each algorithm, assuming that the choice of the values of the parameters was optimal.</p><p>Regarding the use of the SMOTE <ref type="bibr" target="#b12">[9]</ref> and SMOTE+ENN <ref type="bibr" target="#b9">[6]</ref> preprocessing methods, we consider only the 1-nearest neighbor (using the euclidean distance) to generate the synthetic samples, and we balance the training data to the 50% distribution. We only use SMOTE + ENN for C4.5 because it shows a positive synergy when pruning the tree <ref type="bibr" target="#b19">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Data-sets and data partitions</head><p>In order to analyze the quality of our approach GP-COACH-H against the algorithms introduced in the previous section, we have  selected several highly imbalanced and borderline imbalanced data-sets. Specifically, as highly imbalanced data-sets, we have selected 44 data-sets from KEEL data-set repository<ref type="foot" target="#foot_4">3</ref>  <ref type="bibr" target="#b5">[2]</ref> with an imbalance ratio (IR) <ref type="bibr" target="#b49">[46]</ref> greater than 9. The data are summarized in Table <ref type="table" target="#tab_3">3</ref>, where we denote the number of examples (#Ex.), number of attributes (#Atts.), class name of each class (positive and negative), class attribute distribution and IR. This table is in ascending order according to the IR.</p><p>Inspired by Kubat and Matwin <ref type="bibr" target="#b43">[40]</ref>, Napierala et al. <ref type="bibr" target="#b47">[44]</ref> created several artificial data-sets that contain borderline examples in an imbalanced scenario to address the correct identification of those examples. These data-sets have three different shapes of the positive class: subclus (Fig. <ref type="figure" target="#fig_9">9</ref>), clover (Fig. <ref type="figure" target="#fig_10">10</ref>) and paw (Fig. <ref type="figure" target="#fig_11">11</ref>), all surrounded uniformly by the negative class. For each shape, we have data-sets from two different sizes and IR: data-sets with 600 examples with an IR of 5 and data-sets with 800 examples with an IR of 7. Each one of these data-sets is affected by different disturbance ratio levels (0%, 30%, 50%, 60% and 70%). The disturbance ratio is simulated increasing the ratio of borderline examples from the positive class subregions.</p><p>To develop the different experiments we consider a 5-fold crossvalidation model, i.e., five random partitions of data with a 20% and the combination of 4 of them (80%) as training and the remaining ones as test. For each data-set we consider the average results of the five partitions. The data-sets used in this study use the partitions provided by the KEEL data-set repository in the imbalanced classification data-set section. <ref type="foot" target="#foot_5">4</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Statistical tests for performance comparison</head><p>Statistical analysis needs to be carried out in order to find significant differences among the results obtained by the studied methods <ref type="bibr" target="#b27">[24]</ref>. We consider the use of non-parametric tests, according to the recommendations made in <ref type="bibr" target="#b28">[25,</ref><ref type="bibr" target="#b27">24]</ref> where a set of simple, safe and robust non-parametric tests for statistical comparisons of classifiers is presented. These tests are used due to the fact that the initial conditions that guarantee the reliability of the parametric tests may not be satisfied, causing the statistical analysis to lose credibility <ref type="bibr" target="#b53">[50]</ref>.</p><p>The Wilcoxon test <ref type="bibr" target="#b53">[50]</ref> will be used as a non-parametric statistical procedure in order to conduct pairwise comparisons between two algorithms. For multiple comparisons we use the Iman-Davenport test to detect statistical differences among a group of results, and the Holm post-hoc test in order to find which algorithms are distinctive among a 1 Â n comparison. The post-hoc procedure allows us to know whether a hypothesis of comparison of means could be rejected at a specified level of significance a. However, it is very interesting to compute the p-value associated with each comparison, which represents the lowest level of significance of a hypothesis that results in a rejection. It is the adjusted p-value. In this manner, we can know whether two algorithms are significantly different and how different they are.</p><p>Furthermore, we consider the average ranking of the algorithms, in order to show how good a method is with respect to its partners. This ranking is obtained by assigning a position to each algorithm depending on its performance for each data-set. The algorithm which achieves the best accuracy in a specific data-set will have the first ranking (value 1); then, the algorithm with the second best accuracy is assigned rank 2, and so forth. This task is carried out for all data-sets and finally an average ranking is computed as the mean value of all rankings.</p><p>These tests are suggested in the studies presented in <ref type="bibr" target="#b28">[25,</ref><ref type="bibr" target="#b27">24]</ref>, where their use in the field of machine learning is highly recommended. For a wider description of the use of these tests, please refer to the website on Statistical Inference in Computational Intelligence and Data Mining. <ref type="foot" target="#foot_6">5</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental study</head><p>In this section, we present a set of experiments to illustrate and demonstrate the behavior of GP-COACH-H. These experiments are designed towards two objectives: to exemplify how the GP-COACH-H algorithm works, and to determine its robustness for highly and borderline imbalanced data-sets.</p><p>We organize those experiments in the following way. First, Section 5.1 presents a case of study over one one of the highly imbalanced data-sets presented in the previous section. Next, Section 5.2 contains an analysis of the impact of the hierarchical step in the algorithm. Section 5.3 studies the the importance of the usage of a preprocessing step when dealing with highly imbalanced datasets. Later, Section 5.4 performs a global comparison among the fuzzy classification methods and C4.5 over the highly imbalanced data-sets. Finally, in Section 5.5, this global comparison is also carried out over the borderline imbalanced data-sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Sample procedure of the GP-COACH-H algorithm: a case of study</head><p>In order to illustrate how GP-COACH-H works we have selected the glass0146vs2 data-set. We will follow the algorithm operations and the results it provides. The glass0146vs2 data-set is a highly imbalanced data-set from the KEEL data-set repository, <ref type="foot" target="#foot_7">6</ref> with 9 input attributes, 205 instances and an IR equal to 11.06. We have selected this data-set as one with a small size whose results can be easily interpreted.</p><p>For this specific run, we have chosen the 3rd partition from the 5-fcv used in all the experiments. This partition uses 164 instances for training (14 positive and 150 negative) and 41 for test (3 positive and 38 negative), using the 9 input attributes of the whole data-set. The first step of the GP-COACH-H algorithm (see Fig. <ref type="figure" target="#fig_7">8</ref>) uses the SMOTE algorithm to balance the class distribution. Therefore, we apply the SMOTE algorithm and we obtain a new training set that contains 300 instances, 150 instances for each class.</p><p>The second step starts using the preprocessed data-set to generate the HKB. In order to generate the HKB, we first generate the HDB from the available data. The HDB is generated (as was explained in the previous sections) with the same number of linguistic terms for all input variables, composed of symmetrical triangular-shaped and uniformly distributed membership functions. The second layer, is built preserving all the membership function modal points, corresponding to each linguistic term. Figs. 12 and 13 show the linguistic variables generated for the Mg attribute, according to the given instructions.</p><p>Once we have generated the HDB, we start the GP procedure to generate the HRB. This procedure evolves a rule population through several generations, including the usage of genetic operators to generate new individuals, the token competition procedure to delete irrelevant rules and the hierarchical creation of new rules in each step. At the end of the iterations, a rule base with different granularity rules is obtained. In Fig. <ref type="figure" target="#fig_12">14</ref>, the rules generated using the generated HDB and the preprocessed training set are shown.</p><p>At this point, we start the last step of the algorithm which is the genetic rule selection and lateral tuning of the variables. To obtain the final solution, we use the preprocessed set from the first step and the HKB generated previously. The genetic search looks for a   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Analysis of the impact of the hierarchical levels over the imbalanced data-sets</head><p>This subsection is devoted to the impact of the usage of the HKB in the GP-COACH-H algorithm in relation to not using a HKB and use a traditional KB instead. In this manner, we will detect the influence of this component of the GP-COACH-H algorithm thus justifying its use.</p><p>We will compare the results of the GP-COACH-H algorithm according to the fuzzy HKB generated after the application of the GP procedure to the results of the basic GP-COACH algorithm with 5 and 9 labels, using SMOTE as preprocessing algorithm in both cases. The performance measures used are sensitivity and specificity to observe the impact for each class. Table <ref type="table">4</ref> shows the average results for each algorithm over the highly imbalanced data-sets. The complete table of results for all data-sets can be found in the appendix of this work.</p><p>Considering the sensitivity measure the best performing average algorithm is the basic GP-COACH with 5 labels, however, if we look at the specificity measure then the best performing algorithm is the basic GP-COACH with 9 labels. Therefore, we need to consider the effectiveness for each class separately.</p><p>Contemplating the positive class, we can observe that the best performance in training is higher for the hierarchical version, being   able to describe the training set more accurately than in the presence of low granularity rules only. Therefore, our initial intuition where the HKB was able to better describe difficult data spaces is confirmed. Comparing the training results in relation to the test results we notice a drop in performance for all the algorithms where GP-COACH-5 gets the best results, GP-COACH-H obtains similar results to GP-COACH-5 and GP-COACH-9 accomplishes lower results than the other two.</p><p>Analyzing the results associated to the negative class, we see an almost opposite situation. For training results the GP-COACH-9 algorithm is the algorithm that best describes the data, a situation where GP-COACH-H is supposed to be found. Nevertheless, GP-COACH-H is designed to specifically deal with imbalanced datasets concentrating on the positive class so is logical that it does not characterize the negative class as well as the previous case.</p><p>Confronting the training results with the test results we find a drop in the performance on equal levels for each approaches. Therefore, GP-COACH-9 is the best performing algorithm for the negative class, closely followed by GP-COACH-H where GP-COACH-5 performance falls behind those two approaches.</p><p>After checking the performance in each class, we discover that the basic GP-COACH is a powerful tool to describe one of our classes depending on the number of labels used. Nevertheless, if we choose a specific number of labels to focus on one class the final performance is degraded in the other one. Consequently, the GP-COACH-H approach that combines low granularity and high granularity rules is able to address the description of both classes accordingly. Its performance does not exceed the results of the basic algorithm, however, it goes closely after them in each class. Furthermore, there is not a high decrease in performance for the class  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>Average results for GP-COACH-5, GP-COACH-9 and GP-COACH-H for the highly imbalanced data-sets. as in the basic algorithm. In this manner, GP-COACH-H is able to profit from the descriptive power of each granularity level obtaining a good balance between the performance of both classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data-set</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Analysis of the suitability of the preprocessing step for imbalanced problems</head><p>In this part of the study, our aim is to show the suitability of the preprocessing step included in GP-COACH-H as the first step of the algorithm. We also check the performance of applying this preprocessing step to the basic GP-COACH algorithm in order to show the necessity of this procedure when dealing with imbalanced datasets, thus justifying the inclusion of this step in our proposal.</p><p>According to this objective, we show the average GM results in training and test in Table <ref type="table" target="#tab_4">5</ref>, together with the corresponding standard deviation, for the basic GP-COACH algorithm and for the hierarchical GP-COACH-H with and without SMOTE preprocessing over the highly imbalanced data-sets presented in Section 4.2. The complete table of results for all data-sets is shown in the appendix of this work. We observe that the best result in test (which is stressed in boldface) always corresponds to the one obtained when the SMOTE preprocessing is applied. Furthermore, there is an enormous difference between the usage or not usage of preprocessing. Therefore, we conclude that the usage of SMOTE as preprocessing clearly outperforms the usage of the original data-sets making the use of this methodology a necessity in the framework of imbalanced data-sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Analysis of GP-COACH-H on highly imbalanced data-sets</head><p>The following part of the study will consider the performance of the GP-COACH-H algorithm in contrast with other FRBCS learning proposals and with the C4.5 algorithm. Table <ref type="table" target="#tab_5">6</ref> shows the average GM results in training and test together with the corresponding standard deviation for the highly imbalanced data-sets considered. By rows, we can observe the results for the basic GP-COACH method with 5 and 9 labels (GP-COACH-5 and GP-COACH-9), the HFRBCS(Chi), the proposed GP-COACH-H and the C4.5 decision tree. The best average case in test is highlighted in bold. The complete table of results for all data-sets is also shown in the appendix of this work together with the results of the previous experiments. We remind that SMOTE is used for the FRBCS whereas SMOTE+ENN is applied in conjunction with C4.5 along all the experiments.</p><p>According to the average values shown in this table the best method in highly imbalanced data-sets is the GP-COACH-H. To carry out the statistical study we first check for significant differences among the algorithms using an Iman-Davenport test. The p-value (0.0779) is low enough to reject the null equality hypothesis with a high confidence level. Thus, we can conclude that significant differences do exist, proceeding by showing in Table <ref type="table" target="#tab_6">7</ref> the average  obtains the lower ranking which makes it the control method used for the post-hoc computation. As all the adjusted p-values are sufficiently low to reject the null-hypothesis in all cases, the assumption where GP-COACH-H is the best performing method considered for highly imbalanced data-sets is reinforced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Analysis of GP-COACH-H on borderline imbalanced data-sets</head><p>In the last part of our study, we want to analyze the behavior of the GP-COACH-H proposal in the scenario of imbalance borderline data-sets. We will take into account the same algorithms considered in the analysis for highly imbalanced data-sets, namely, the basic GP-COACH method with 5 and 9 labels (GP-COACH-5 and GP-COACH-9), HFRBCS <ref type="bibr">(Chi)</ref>, GP-COACH-H and the C4.5 decision tree. Table <ref type="table">8</ref> shows the average results in training and test together with the corresponding standard deviation for the algorithms used in the study over the borderline imbalanced data-sets. As in previous tables, the best average case in test is highlighted in bold and the complete table of results for the borderline imbalanced datasets is also shown in the appendix of this work.</p><p>Observing the average results table we detect GP-COACH-H as the method with the best average results. Similarly to the procedure used in the highly imbalanced data-sets comparison we start the statistical study for borderline imbalanced data-sets computing the Iman-Davenport test to discern if there are significant differences among the algorithms. The p-value computed is zero, implying that there are differences between the algorithms. Therefore, we perform the Holm test as post-hoc procedure. Table <ref type="table" target="#tab_7">9</ref> contains the ranks of the algorithms and the adjusted p-values computed using the Holm test.</p><p>According to Table <ref type="table" target="#tab_7">9</ref> the lowest ranking corresponds to GP-COACH-H turning it into the control method used in the Holm test as the best performing method for borderline data-sets. In this case, the adjusted p-values associated to the basic GP-COACH (with 5 and 9 labels) and to HFRBCS <ref type="bibr">(Chi)</ref> are low enough to reject the null-hypothesis with a high confidence level. This means, that our proposal GP-COACH-H is the best performing FRBCS in borderline imbalanced data-sets. In the remaining case (C4.5), we perform a Wilcoxon test (Table <ref type="table" target="#tab_8">10</ref>) in order to check if we find differences between both algorithms.</p><p>In this case, the p-value computed does not reject the null hypothesis. Nevertheless, GP-COACH-H achieves a higher sum of ranks, which means that GP-COACH-H has obtained a greater performance in a superior number of data-sets than C4.5, turning GP-COACH-H into a competitive method. Furthermore, the average performance of GP-COACH-H is better than the performance of C4.5 and the standard deviation is lower which causes GP-COACH-H to be a more robust method in each occasion.</p><p>To sum up, our experimental study has shown that GP-COACH-H is an algorithm that presents a good behavior in the framework of imbalanced data-sets, specifically, when dealing with high imbalanced data and borderline imbalanced data. The design of GP-COACH-H integrates different strategies to deal with the problem that help to overcome the difficulties when they appear. Specifically, the preprocessing step is used to counter the imbalance problem, the hierarchical procedure is added to the FRBCS used as base to obtain a better representation of the data-set in difficult areas such as small disjuncts or borderline samples and the rule selection combined with tuning refines the results obtained improving the overall results. These schemes combined together deal with the mentioned problems in conjunction generating good results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Concluding remarks</head><p>In this paper we have presented a FRBCS with different granulation levels that integrates rule selection and the 2-tuples tuning approach to improve the performance in imbalanced data-sets. The proposal integrates data sampling together with algorithm modifications to the basic approach and adapts its behavior to the different granulation levels considered, adding a post-processing step that helps the hierarchical fuzzy rule base classification system to have a better adaptation to the context of each problem and therefore to enhance its global behavior.</p><p>The proposed hierarchical fuzzy rule based classification was compared to the GP-COACH algorithm, HFRBCS algorithm and the C4.5 decision tree in order to demonstrate its good performance. The experimental study justifies the combination of SMOTE with the algorithmic modifications such as the usage of a hierarchical knowledge base in order to increase the performance in the imbalanced data-set scenario. Moreover, the results obtained when we deal with this scenario evidence the interest of this proposal. Specifically, this proposal outperforms the other approaches in the framework of highly imbalanced data-sets, which usually is an scenario where most algorithms have lots of difficulties to perform properly.</p><p>For borderline imbalanced data-sets our approach shows a better behavior than other FRBCSs used in the experimental studio and maintains a competitive performance when it is compared with C4.5. These results have been contrasted by several nonparametric statistical procedures that reinforce the extracted conclusions.</p><p>As future work, we consider several lines of work centered on the features of GP-COACH-H that can still be enhanced to obtain a better performance. One possibility includes the modification of the genetic operations to achieve a multi-objective procedure that enables a trade-off between interpretability and accuracy. Moreover, we want to study in depth the data intrinsic characteristics that hinder the performance in imbalanced data-sets and incorporate this knowledge into the model with a specialized strategy for each case. Another possibility focus on the balance level of the preprocessing step. If an equal balance is not needed and can be substituted by a lower number of instances then the run time of the algorithm will decrease. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>ples. Safe examples are placed in relatively homogeneous areas with respect to the class label. By noisy examples we understand individuals from one class occurring in safe areas of the other class. Finally, borderline examples are located in the area surrounding class boundaries, where the positive and negative classes overlap.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Data-set with low imbalance (IR = 2.23).</figDesc><graphic coords="3,111.29,468.57,113.88,112.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Data-set with high imbalance (IR = 9.15).</figDesc><graphic coords="3,111.29,627.65,113.88,111.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. An illustration of how to create the synthetic data points in the SMOTE algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Example of the SMOTE application.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4 SFig. 6 .</head><label>46</label><figDesc>Fig.6. Transition from a partition in DB(1, 3) to another one in DB(2, 5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Lateral displacement of a MF.</figDesc><graphic coords="7,337.83,614.78,198.71,123.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Flowchart of GP-COACH-H.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>, Initial Population Size = 200, a (raw fitness) = 0.7, Crossover Probability = 0.5, Mutation Probability = 0.2, Dropping Condition Probability = 0.15, Insertion Probability = 0.15, Tournament size = 2, w 1 = 0.8, w 2 = w 3 = 0.05, w 4 = 0.1 Hierarchical procedure parameters GP-COACH-H and HFRBCS(Chi) a (rule expansion) = 0.2, CHC Evaluations = 10,000, CHC Population Size = 61, CHC bits per gene (for GP-COACH-H) = 30 C4.5 parameters C4.5 Pruned=true, Confidence = 0.25 and Minimum number of item-sets per leaf = 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Subclus.</figDesc><graphic coords="11,111.29,67.92,113.88,137.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Clover.</figDesc><graphic coords="11,111.29,245.48,113.88,116.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Paw.</figDesc><graphic coords="11,111.29,406.21,113.88,114.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Rules generated after the Fuzzy HRB Generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Database Layer 2 with 9 labels, Mg attribute.</figDesc><graphic coords="12,109.25,239.81,365.56,134.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Database Layer 1 with 5 labels, Mg attribute.</figDesc><graphic coords="12,109.25,67.92,364.44,133.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Final database Layer 2 with 9 labels, Mg attribute.</figDesc><graphic coords="13,119.06,238.11,364.48,143.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Final database Layer 1 with 5 labels, Mg attribute.</figDesc><graphic coords="13,116.22,67.92,368.80,136.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. Final rules generated with GP-COACH-H algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Confusion matrix for a two-class problem.</figDesc><table><row><cell></cell><cell>Positive prediction</cell><cell>Negative prediction</cell></row><row><cell>Positive class</cell><cell>True Positive (TP)</cell><cell>False Negative (FN)</cell></row><row><cell>Negative class</cell><cell>False Positive (FP)</cell><cell>True Negative (TN)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Parameter specification for the algorithms tested in the experimentation. Maximum t-conorm, Rule Weight = Certainty Factor, Fuzzy Reasoning Method = Normalized Sum, Number of Fuzzy Labels (for basic GP-COACH) = 5 or 9, Number of Fuzzy Labels (for GP-COACH-H) = 5 for Low Granularity Rules and 9 for High Granularity</figDesc><table><row><cell>Algorithm</cell><cell>Parameters</cell></row><row><cell>FRBCS parameters</cell><cell></cell></row><row><cell>GP-COACH and GP-COACH-H</cell><cell>Minimum t-norm, Rules</cell></row><row><cell>HFRBCS(Chi)</cell><cell>Product t-norm, Rule Weight = Penalized Certainty Factor, Fuzzy Reasoning Method = Winning Rule, Number of Fuzzy Labels = 3 for Low</cell></row><row><cell></cell><cell>Granularity Rules and 5 for High Granularity Rules</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Summary of imbalanced data-sets.</figDesc><table><row><cell>Data-sets</cell><cell>#Ex.</cell><cell>#Atts.</cell><cell>Class (À; +)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>Average results for GP-COACH versions with and without SMOTE preprocessing for the highly imbalanced data-sets.</figDesc><table><row><cell></cell><cell>Sensitivity tr</cell><cell>Sensitivity tst</cell><cell>Specificity tr</cell><cell>Specificity tst</cell></row><row><cell>GP-COACH-5</cell><cell>.9097 ± .0307</cell><cell>.7809 ± .1212</cell><cell>.8643 ± .0307</cell><cell>.8531 ± .1212</cell></row><row><cell>GP-COACH-9</cell><cell>.8983 ± .0267</cell><cell>.7319 ± .1334</cell><cell>.9231 ± .0267</cell><cell>.9055 ± .1334</cell></row><row><cell>GP-COACH-H</cell><cell>.9398 ± .0204</cell><cell>.7797 ± .1233</cell><cell>.9025 ± .0204</cell><cell>.8855 ± .1233</cell></row><row><cell>Data-set</cell><cell>No preprocessing</cell><cell></cell><cell>SMOTE preprocessing</cell><cell></cell></row><row><cell></cell><cell>GM tr</cell><cell>GM tst</cell><cell>GM tr</cell><cell>GM tst</cell></row><row><cell>GP-COACH-5</cell><cell>.4789 ± .1017</cell><cell>.3677 ± .1922</cell><cell>.8763 ± .0307</cell><cell>.7897 ± .1212</cell></row><row><cell>GP-COACH-9</cell><cell>.5074 ± .0871</cell><cell>.3929 ± .1996</cell><cell>.9056 ± .0267</cell><cell>.7845 ± .1334</cell></row><row><cell>GP-COACH-H</cell><cell>.4536 ± .1216</cell><cell>.3439 ± .1697</cell><cell>.9576 ± .0121</cell><cell>.8175 ± .1193</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>Average results for FRBCS methods and C4.5 for the highly imbalanced data-sets. SMOTE preprocessing for FRBCS methods, SMOTE+ENN for C4.5.</figDesc><table><row><cell>Data-set</cell><cell>GM tr</cell><cell>GM tst</cell></row><row><cell>GP-COACH-5</cell><cell>.8763 ± .0307</cell><cell>.7897 ± .1212</cell></row><row><cell>GP-COACH-9</cell><cell>.9056 ± .0267</cell><cell>.7845 ± .1334</cell></row><row><cell>HFRBCS(Chi)</cell><cell>.9331 ± .0117</cell><cell>.7901 ± .1325</cell></row><row><cell>GP-COACH-H</cell><cell>.9576 ± .0121</cell><cell>.8175 ± .1193</cell></row><row><cell>C4.5</cell><cell>.9549 ± .0180</cell><cell>.7848 ± .1452</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7</head><label>7</label><figDesc>Average rankings and adjusted p-values using Holm's post-hoc procedure for FRBCS methods and C4.5 adopting the GM measure for the highly imbalanced data-sets.</figDesc><table><row><cell>Algorithm</cell><cell>Average ranking</cell><cell>Adjusted p-value (Holm's test)</cell></row><row><cell>GP-COACH-H</cell><cell>2.4091</cell><cell></cell></row><row><cell>GP-COACH-9</cell><cell>3.0227</cell><cell>0.0862</cell></row><row><cell>GP-COACH-5</cell><cell>3.0909</cell><cell>0.0862</cell></row><row><cell>C4.5</cell><cell>3.2045</cell><cell>0.0549</cell></row><row><cell>HFRBCS(Chi)</cell><cell>3.2727</cell><cell>0.0416</cell></row><row><cell>Table 8</cell><cell></cell><cell></cell></row><row><cell cols="3">Average results for FRBCS methods and C4.5 for the borderline imbalanced data-sets.</cell></row><row><cell cols="3">SMOTE preprocessing for FRBCS methods, SMOTE+ENN for C4.5.</cell></row><row><cell>Data-set</cell><cell>GM tr</cell><cell>GM tst</cell></row><row><cell>GP-COACH-5</cell><cell>.7899 ± .0218</cell><cell>.7630 ± .0578</cell></row><row><cell>GP-COACH-9</cell><cell>.8103 ± .0330</cell><cell>.7628 ± .0705</cell></row><row><cell>HFRBCS(Chi)</cell><cell>.8316 ± .0195</cell><cell>.7992 ± .0461</cell></row><row><cell>GP-COACH-H</cell><cell>.8674 ± .0157</cell><cell>.8234 ± .0428</cell></row><row><cell>C4.5</cell><cell>.8881 ± .0244</cell><cell>.8208 ± .0462</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9</head><label>9</label><figDesc>Average rankings and adjusted p-values using Holm's post-hoc procedure for FRBCS methods and C4.5 adopting the GM measure for the borderline imbalanced data-sets.</figDesc><table><row><cell>Algorithm</cell><cell>Average ranking</cell><cell>Adjusted p-value (Holm's test)</cell></row><row><cell>GP-COACH-H</cell><cell>1.7333</cell><cell></cell></row><row><cell>C4.5</cell><cell>1.9000</cell><cell>0.6831</cell></row><row><cell>HFRBCS(Chi)</cell><cell>3.0667</cell><cell>0.0022</cell></row><row><cell>GP-COACH-9</cell><cell>3.8667</cell><cell>0.0000</cell></row><row><cell>GP-COACH-5</cell><cell>4.4333</cell><cell>0.0000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10</head><label>10</label><figDesc>Wilcoxon test to compare GP-COACH-H against C4.5 in borderline imbalanced datasets. R + corresponds to the sum of the ranks for GP-COACH-H and R À to C4.5. of the algorithms and the adjusted p-values computed by the Holm test. Looking at this table we can notice that GP-COACH-H</figDesc><table><row><cell>Comparison</cell><cell>R +</cell><cell>R À</cell><cell>p-Value</cell></row><row><cell>GP-COACH-H vs C4.5</cell><cell>261.0</cell><cell>204.0</cell><cell>0.551</cell></row></table><note><p>ranks</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11</head><label>11</label><figDesc>Complete table of results for GP-COACH-5, GP-COACH-9 and GP-COACH-H after the GP procedure using the specificity and sensitivity measures.</figDesc><table><row><cell>Data-set</cell><cell>GP-COACH-5</cell><cell></cell><cell></cell><cell></cell><cell>GP-COACH-9</cell><cell></cell><cell></cell><cell></cell><cell>GP-COACH-H</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Sensitivity tr</cell><cell>Sensitivity tst</cell><cell>Specificity tr</cell><cell>Specificity tst</cell><cell>Sensitivity tr</cell><cell>Sensitivity tst</cell><cell>Specificity tr</cell><cell>Specificity tst</cell><cell>Sensitivity tr</cell><cell>Sensitivity tst</cell><cell>Specificity tr</cell><cell>Specificity tst</cell></row><row><cell>ecoli034vs5</cell><cell>.9750 ± .0165</cell><cell>.8500 ± .1282</cell><cell cols="2">.9764 ± .0165 .9722 ± .1282</cell><cell>.9875 ± .0158</cell><cell cols="3">.9000 ± .0709 .9653 ± .0158 .9556 ± .0709</cell><cell>.9375 ± .0567</cell><cell>.8500 ± .0759</cell><cell>.9806 ± .0567</cell><cell>.9667 ± .0759</cell></row><row><cell>yeast2vs4</cell><cell>.9316 ± .0092</cell><cell>.8818 ± .0412</cell><cell cols="2">.9196 ± .0092 .9179 ± .0412</cell><cell>.9265 ± .0044</cell><cell cols="3">.8818 ± .0381 .9319 ± .0044 .9288 ± .0381</cell><cell>.9365 ± .0166</cell><cell>.8636 ± .0471</cell><cell>.9352 ± .0166</cell><cell>.9308 ± .0471</cell></row><row><cell>ecoli067vs35</cell><cell>.9654 ± .0193</cell><cell>.8400 ± .2093</cell><cell cols="2">.9200 ± .0193 .8650 ± .2093</cell><cell>.9438 ± .0210</cell><cell cols="3">.8400 ± .2265 .9412 ± .0210 .9250 ± .2265</cell><cell>.9660 ± .0292</cell><cell>.8400 ± .2248</cell><cell>.9463 ± .0292</cell><cell>.9200 ± .2248</cell></row><row><cell>ecoli0234vs5</cell><cell>.9625 ± .0095</cell><cell>.7500 ± .1552</cell><cell cols="2">.9794 ± .0095 .9338 ± .1552</cell><cell>.9625 ± .0409</cell><cell cols="3">.8000 ± .1648 .9670 ± .0409 .9174 ± .1648</cell><cell>.9875 ± .0111</cell><cell>.8500 ± .1239</cell><cell>.9822 ± .0111</cell><cell>.9392 ± .1239</cell></row><row><cell>glass015vs2</cell><cell>.7429 ± .1337</cell><cell>.4833 ± .2183</cell><cell cols="2">.4774 ± .1337 .4968 ± .2183</cell><cell>.8077 ± .0511</cell><cell cols="3">.1833 ± .3032 .8581 ± .0511 .7677 ± .3032</cell><cell>.8978 ± .0440</cell><cell>.6000 ± .0739</cell><cell>.7742 ± .0440</cell><cell>.7677 ± .0739</cell></row><row><cell>yeast0359vs78</cell><cell>.7650 ± .0840</cell><cell>.6400 ± .1244</cell><cell cols="2">.5273 ± .0840 .5312 ± .1244</cell><cell>.3700 ± .0258</cell><cell cols="3">.3600 ± .0833 .8499 ± .0258 .8418 ± .0833</cell><cell>.8450 ± .0199</cell><cell>.7000 ± .0820</cell><cell>.8164 ± .0199</cell><cell>.8026 ± .0820</cell></row><row><cell>yeast02579vs368</cell><cell>.8763 ± .0093</cell><cell>.8700 ± .0376</cell><cell cols="2">.9577 ± .0093 .9514 ± .0376</cell><cell>.8864 ± .0109</cell><cell cols="3">.8900 ± .0395 .9279 ± .0109 .9204 ± .0395</cell><cell>.8788 ± .0093</cell><cell>.8600 ± .0488</cell><cell>.9577 ± .0093</cell><cell>.9547 ± .0488</cell></row><row><cell>yeast0256vs3789</cell><cell>.7096 ± .0136</cell><cell>.6858 ± .0676</cell><cell cols="2">.9251 ± .0136 .9271 ± .0676</cell><cell>.7322 ± .0149</cell><cell cols="3">.7063 ± .0563 .9022 ± .0149 .8994 ± .0563</cell><cell>.7247 ± .0154</cell><cell>.7063 ± .0598</cell><cell>.9191 ± .0154</cell><cell>.9182 ± .0598</cell></row><row><cell>ecoli046vs5</cell><cell>.9750 ± .0168</cell><cell>.9000 ± .1248</cell><cell cols="2">.9740 ± .0168 .9509 ± .1248</cell><cell>.9875 ± .0174</cell><cell cols="4">.8500 ± .2166 .9836 ± .0174 .9566 ± .2166 1.0000 ± .0073</cell><cell>.8500 ± .2117</cell><cell>.9727 ± .0073</cell><cell>.9401 ± .2117</cell></row><row><cell>ecoli01vs235</cell><cell>.9689 ± .0151</cell><cell>.8600 ± .1131</cell><cell cols="2">.9125 ± .0151 .8955 ± .1131</cell><cell>.9689 ± .0152</cell><cell cols="3">.9100 ± .0670 .9398 ± .0152 .9227 ± .0670</cell><cell>.9479 ± .0184</cell><cell>.7700 ± .1915</cell><cell>.9443 ± .0184</cell><cell>.9364 ± .1915</cell></row><row><cell>ecoli0267vs35</cell><cell>.9216 ± .0211</cell><cell>.8000 ± .1311</cell><cell cols="2">.9209 ± .0211 .9156 ± .1311</cell><cell>.9778 ± .0334</cell><cell cols="3">.8000 ± .1125 .9220 ± .0334 .8916 ± .1125</cell><cell>.9444 ± .0260</cell><cell>.8000 ± .0928</cell><cell>.9073 ± .0260</cell><cell>.8709 ± .0928</cell></row><row><cell>glass04vs5</cell><cell>1.0000 ± .0208</cell><cell>.9000 ± .1277</cell><cell cols="7">.9338 ± .0208 .9287 ± .1277 1.0000 ± .0247 1.0000 ± .0134 .9426 ± .0247 .9279 ± .0134 1.0000 ± .0365</cell><cell>.8000 ± .4020</cell><cell>.9190 ± .0365</cell><cell>.8412 ± .4020</cell></row><row><cell>ecoli0346vs5</cell><cell>1.0000 ± .0028</cell><cell>.8000 ± .1132</cell><cell cols="2">.9919 ± .0028 .9784 ± .1132</cell><cell>.9875 ± .0159</cell><cell cols="4">.8500 ± .0632 .9811 ± .0159 .9459 ± .0632 1.0000 ± .0107</cell><cell>.8500 ± .0608</cell><cell>.9770 ± .0107</cell><cell>.9622 ± .0608</cell></row><row><cell>ecoli0347vs56</cell><cell>.9700 ± .0104</cell><cell>.8000 ± .1039</cell><cell cols="2">.9461 ± .0104 .9224 ± .1039</cell><cell>.9700 ± .0166</cell><cell cols="3">.8000 ± .1459 .9590 ± .0166 .9397 ± .1459</cell><cell>.9800 ± .0069</cell><cell>.8400 ± .0923</cell><cell>.9483 ± .0069</cell><cell>.9309 ± .0923</cell></row><row><cell>yeast05679vs4</cell><cell>.7843 ± .0198</cell><cell>.7836 ± .0759</cell><cell cols="2">.8569 ± .0198 .8490 ± .0759</cell><cell>.7990 ± .0158</cell><cell cols="3">.7636 ± .0858 .8753 ± .0158 .8616 ± .0858</cell><cell>.8184 ± .0117</cell><cell>.7255 ± .0653</cell><cell>.8496 ± .0117</cell><cell>.8449 ± .0653</cell></row><row><cell>ecoli067vs5</cell><cell>1.0000 ± .0144</cell><cell>.8000 ± .1277</cell><cell cols="2">.9113 ± .0144 .8850 ± .1277</cell><cell>.9625 ± .0162</cell><cell cols="3">.8500 ± .0884 .9488 ± .0162 .9400 ± .0884</cell><cell>.9625 ± .0153</cell><cell>.8500 ± .0513</cell><cell>.9463 ± .0153</cell><cell>.9350 ± .0513</cell></row><row><cell>vowel0</cell><cell>.9861 ± .0116</cell><cell>.9667 ± .0154</cell><cell cols="2">.9527 ± .0116 .9532 ± .0154</cell><cell>.9778 ± .0075</cell><cell cols="3">.9444 ± .0093 .9510 ± .0075 .9365 ± .0093</cell><cell>.9611 ± .0160</cell><cell>.9333 ± .0232</cell><cell>.9630 ± .0160</cell><cell>.9588 ± .0232</cell></row><row><cell>glass016vs2</cell><cell>.9560 ± .0558</cell><cell>.5500 ± .1228</cell><cell cols="2">.6443 ± .0558 .5886 ± .1228</cell><cell>.8231 ± .0355</cell><cell cols="3">.3000 ± .2385 .8257 ± .0355 .7886 ± .2385</cell><cell>.9714 ± .0171</cell><cell>.5833 ± .1741</cell><cell>.7286 ± .0171</cell><cell>.7086 ± .1741</cell></row><row><cell>glass2</cell><cell>.9264 ± .0256</cell><cell>.7667 ± .0841</cell><cell cols="2">.5330 ± .0256 .5074 ± .0841</cell><cell>.7626 ± .0615</cell><cell cols="3">.3500 ± .3725 .8514 ± .0615 .8027 ± .3725</cell><cell>.9407 ± .0533</cell><cell>.4667 ± .1544</cell><cell>.8225 ± .0533</cell><cell>.7505 ± .1544</cell></row><row><cell>ecoli0147vs2356</cell><cell>.9228 ± .0241</cell><cell>.7200 ± .1060</cell><cell cols="2">.9267 ± .0241 .8990 ± .1060</cell><cell>.8808 ± .0521</cell><cell cols="3">.7133 ± .1467 .9218 ± .0521 .9056 ± .1467</cell><cell>.9221 ± .0232</cell><cell>.8333 ± .0477</cell><cell>.9120 ± .0232</cell><cell>.9154 ± .0477</cell></row><row><cell>led7digit02456789vs1</cell><cell>.8582 ± .0246</cell><cell>.8607 ± .0829</cell><cell cols="2">.9421 ± .0246 .9483 ± .0829</cell><cell>.8582 ± .0274</cell><cell cols="3">.8321 ± .0708 .9434 ± .0274 .9507 ± .0708</cell><cell>.8648 ± .0216</cell><cell>.8607 ± .0851</cell><cell>.9403 ± .0216</cell><cell>.9458 ± .0851</cell></row><row><cell>glass06vs5</cell><cell cols="2">1.0000 ± .0073 1.0000 ± .0113</cell><cell cols="3">.9798 ± .0073 .9900 ± .0113 1.0000 ± .0069</cell><cell cols="5">.9000 ± .1332 .9849 ± .0069 .9300 ± .1332 1.0000 ± .0141 1.0000 ± .0215</cell><cell>.9722 ± .0141</cell><cell>.9595 ± .0215</cell></row><row><cell>ecoli01vs5</cell><cell>1.0000 ± .0137</cell><cell>.8000 ± .1248</cell><cell cols="3">.9636 ± .0137 .9682 ± .1248 1.0000 ± .0075</cell><cell cols="4">.9000 ± .0908 .9784 ± .0075 .9409 ± .0908 1.0000 ± .0043</cell><cell>.8500 ± .0868</cell><cell>.9784 ± .0043</cell><cell>.9545 ± .0868</cell></row><row><cell>glass0146vs2</cell><cell>.9253 ± .0483</cell><cell>.7833 ± .0450</cell><cell cols="2">.6208 ± .0483 .5909 ± .0450</cell><cell>.8978 ± .0665</cell><cell cols="3">.5167 ± .3191 .8085 ± .0665 .8027 ± .3191</cell><cell>.8956 ± .0273</cell><cell>.5833 ± .0748</cell><cell>.7486 ± .0273</cell><cell>.7343 ± .0748</cell></row><row><cell>ecoli0147vs56</cell><cell>.9700 ± .0222</cell><cell>.8000 ± .0385</cell><cell cols="2">.9455 ± .0222 .8987 ± .0385</cell><cell>.9900 ± .0154</cell><cell cols="3">.8000 ± .0661 .9577 ± .0154 .9282 ± .0661</cell><cell>.9800 ± .0198</cell><cell>.8400 ± .0898</cell><cell>.9381 ± .0198</cell><cell>.9118 ± .0898</cell></row><row><cell>cleveland0vs4</cell><cell>.9800 ± .0194</cell><cell>.5667 ± .1710</cell><cell cols="2">.9687 ± .0194 .9697 ± .1710</cell><cell>.9418 ± .0266</cell><cell cols="3">.6333 ± .2114 .9781 ± .0266 .9634 ± .2114</cell><cell>.9600 ± .0378</cell><cell>.8000 ± .1632</cell><cell>.9439 ± .0378</cell><cell>.9146 ± .1632</cell></row><row><cell>ecoli0146vs5</cell><cell>1.0000 ± .0171</cell><cell>.8500 ± .1133</cell><cell cols="2">.9577 ± .0171 .9385 ± .1133</cell><cell>.9875 ± .0168</cell><cell cols="4">.8500 ± .1158 .9750 ± .0168 .9423 ± .1158 1.0000 ± .0111</cell><cell>.8500 ± .1162</cell><cell>.9712 ± .0111</cell><cell>.9462 ± .1162</cell></row><row><cell>ecoli4</cell><cell>.9750 ± .0151</cell><cell>.9000 ± .0717</cell><cell cols="2">.9755 ± .0151 .9811 ± .0717</cell><cell>.9625 ± .0201</cell><cell cols="3">.8500 ± .0806 .9723 ± .0201 .9588 ± .0806</cell><cell>.9750 ± .0143</cell><cell>.9000 ± .0724</cell><cell>.9723 ± .0143</cell><cell>.9684 ± .0724</cell></row><row><cell>yeast1vs7</cell><cell>.9333 ± .0568</cell><cell>.8333 ± .0539</cell><cell cols="2">.5640 ± .0568 .5644 ± .0539</cell><cell>.8667 ± .0832</cell><cell cols="3">.4667 ± .1465 .8736 ± .0832 .8483 ± .1465</cell><cell>.9000 ± .0314</cell><cell>.6667 ± .0899</cell><cell>.7506 ± .0314</cell><cell>.6945 ± .0899</cell></row><row><cell>shuttle0vs4</cell><cell cols="5">1.0000 ± .0000 1.0000 ± .0020 1.0000 ± .0000 .9982 ± .0020 1.0000 ± .0003</cell><cell cols="3">.9920 ± .0103 .9990 ± .0003 .9988 ± .0103</cell><cell>.9980 ± .0023</cell><cell cols="3">.9917 ± .0094 1.0000 ± .0023 1.0000 ± .0094</cell></row><row><cell>glass4</cell><cell>1.0000 ± .0168</cell><cell>.7333 ± .4090</cell><cell cols="2">.9615 ± .0168 .9200 ± .4090</cell><cell>.9800 ± .0212</cell><cell cols="4">.8333 ± .1306 .9739 ± .0212 .9500 ± .1306 1.0000 ± .0187</cell><cell>.6667 ± .3937</cell><cell>.9478 ± .0187</cell><cell>.9200 ± .3937</cell></row><row><cell>page-blocks13vs4</cell><cell>.9273 ± .0477</cell><cell>.9000 ± .0679</cell><cell cols="2">.9189 ± .0477 .9144 ± .0679</cell><cell>.9391 ± .0703</cell><cell cols="3">.8400 ± .1574 .9262 ± .0703 .9233 ± .1574</cell><cell>.9735 ± .0116</cell><cell>.7933 ± .1273</cell><cell>.9825 ± .0116</cell><cell>.9752 ± .1273</cell></row><row><cell>abalone9-18</cell><cell>.7439 ± .0355</cell><cell>.7306 ± .0996</cell><cell cols="2">.6589 ± .0355 .6705 ± .0996</cell><cell>.8275 ± .0149</cell><cell cols="3">.5889 ± .1103 .7885 ± .0149 .7851 ± .1103</cell><cell>.8446 ± .0191</cell><cell>.7778 ± .0917</cell><cell>.8004 ± .0191</cell><cell>.7823 ± .0917</cell></row><row><cell>glass016vs5</cell><cell cols="2">1.0000 ± .0106 1.0000 ± .0422</cell><cell cols="3">.9643 ± .0106 .9314 ± .0422 1.0000 ± .0058</cell><cell cols="4">.9000 ± .1312 .9643 ± .0058 .9314 ± .1312 1.0000 ± .0108</cell><cell>.8000 ± .1672</cell><cell>.9557 ± .0108</cell><cell>.9429 ± .1672</cell></row><row><cell>shuttle2vs4</cell><cell cols="2">1.0000 ± .0496 1.0000 ± .0667</cell><cell cols="8">.9310 ± .0496 .9190 ± .0667 1.0000 ± .0046 1.0000 ± .0090 .9959 ± .0046 .9920 ± .0090 1.0000 ± .0046 1.0000 ± .0000</cell><cell cols="2">.9939 ± .0046 1.0000 ± .0000</cell></row><row><cell>yeast1458vs7</cell><cell>.6917 ± .1104</cell><cell>.4333 ± .2086</cell><cell cols="2">.6012 ± .1104 .5829 ± .2086</cell><cell>.7750 ± .0498</cell><cell cols="3">.4000 ± .1118 .7492 ± .0498 .7451 ± .1118</cell><cell>.8583 ± .0314</cell><cell>.5000 ± .1284</cell><cell>.6923 ± .0314</cell><cell>.6756 ± .1284</cell></row><row><cell>glass5</cell><cell>.9714 ± .0317</cell><cell>.6000 ± .5297</cell><cell cols="2">.9720 ± .0317 .9561 ± .5297</cell><cell>.9714 ± .0340</cell><cell cols="4">.6000 ± .4085 .9854 ± .0340 .9805 ± .4085 1.0000 ± .0368</cell><cell>.8000 ± .4225</cell><cell>.9183 ± .0368</cell><cell>.9122 ± .4225</cell></row><row><cell>yeast2vs8</cell><cell>.5750 ± .0319</cell><cell>.5500 ± .1487</cell><cell cols="2">.9919 ± .0319 .9957 ± .1487</cell><cell>.6500 ± .0211</cell><cell cols="3">.6000 ± .1606 .9973 ± .0211 .9978 ± .1606</cell><cell>.9625 ± .0175</cell><cell>.5500 ± .1322</cell><cell>.9259 ± .0175</cell><cell>.9111 ± .1322</cell></row><row><cell>yeast4</cell><cell>.8434 ± .0095</cell><cell>.7236 ± .0527</cell><cell cols="2">.8789 ± .0095 .8772 ± .0527</cell><cell>.7988 ± .0140</cell><cell cols="3">.6873 ± .0469 .8939 ± .0140 .8905 ± .0469</cell><cell>.9216 ± .0137</cell><cell>.8018 ± .0438</cell><cell>.8238 ± .0137</cell><cell>.8248 ± .0438</cell></row><row><cell>yeast1289vs7</cell><cell>.7583 ± .1556</cell><cell>.5333 ± .1253</cell><cell cols="2">.6065 ± .1556 .6122 ± .1253</cell><cell>.7917 ± .0672</cell><cell cols="3">.4667 ± .1633 .8277 ± .0672 .8079 ± .1633</cell><cell>.9000 ± .0495</cell><cell>.7000 ± .0902</cell><cell>.7132 ± .0495</cell><cell>.6925 ± .0902</cell></row><row><cell>yeast5</cell><cell>.9208 ± .0262</cell><cell>.8611 ± .0478</cell><cell cols="2">.9493 ± .0262 .9479 ± .0478</cell><cell>.9324 ± .0289</cell><cell cols="3">.8833 ± .0434 .9642 ± .0289 .9667 ± .0434</cell><cell>.9487 ± .0131</cell><cell>.9083 ± .0477</cell><cell>.9488 ± .0131</cell><cell>.9479 ± .0477</cell></row><row><cell>ecoli0137vs26</cell><cell>.8867 ± .0418</cell><cell>.7000 ± .4202</cell><cell cols="2">.9516 ± .0418 .9490 ± .4202</cell><cell>.8533 ± .0354</cell><cell cols="4">.7000 ± .4200 .9562 ± .0354 .9562 ± .4200 1.0000 ± .0106</cell><cell>.8000 ± .4201</cell><cell>.9362 ± .0106</cell><cell>.9015 ± .4201</cell></row><row><cell>yeast6</cell><cell>.8786 ± .0128</cell><cell>.8571 ± .0907</cell><cell cols="2">.9208 ± .0128 .9248 ± .0907</cell><cell>.8571 ± .0187</cell><cell cols="3">.8000 ± .1348 .9367 ± .0187 .9399 ± .1348</cell><cell>.8857 ± .0130</cell><cell>.8286 ± .0988</cell><cell>.9294 ± .0130</cell><cell>.9310 ± .0988</cell></row><row><cell>abalone19</cell><cell>.8508 ± .0131</cell><cell>.6952 ± .0824</cell><cell cols="2">.6165 ± .0131 .6200 ± .0824</cell><cell>.9298 ± .0196</cell><cell cols="3">.4714 ± .0569 .7402 ± .0196 .7359 ± .0569</cell><cell>.8588 ± .0165</cell><cell>.4667 ± .1476</cell><cell>.7233 ± .0165</cell><cell>.7216 ± .1476</cell></row><row><cell>Mean</cell><cell>.9097 ± .0307</cell><cell>.7809 ± .1212</cell><cell cols="2">.8643 ± .0307 .8531 ± .1212</cell><cell>.8983 ± .0267</cell><cell cols="3">.7319 ± .1334 .9231 ± .0267 .9055 ± .1334</cell><cell>.9398 ± .0204</cell><cell>.7797 ± .1233</cell><cell>.9025 ± .0204</cell><cell>.8855 ± .1233</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 12</head><label>12</label><figDesc>Complete table of results for GP-COACH versions with and without SMOTE preprocessing.</figDesc><table><row><cell>Data-set</cell><cell>No preprocessing</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 13</head><label>13</label><figDesc>Complete table of results for FRBCS methods and C4.5 in highly imbalanced data-sets. SMOTE preprocessing for FRBCS methods, SMOTE + ENN for C4.5.</figDesc><table><row><cell>Data-set</cell><cell>GP-COACH-5</cell><cell></cell><cell>GP-COACH-9</cell><cell></cell><cell>HFRBCS(Chi)</cell><cell></cell><cell>GP-COACH-H</cell><cell></cell><cell>C4.5</cell></row><row><cell></cell><cell>GM tr</cell><cell>GM tst</cell><cell>GM tr</cell><cell>GM tst</cell><cell>GM tr</cell><cell>GM tst</cell><cell>GM tr</cell><cell>GM tst</cell><cell>GM tr</cell><cell>GM tst</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 14</head><label>14</label><figDesc>Complete table of results for FRBCS methods and C4.5 in borderline imbalanced data-sets. SMOTE preprocessing for FRBCS methods, SMOTE+ENN for C4.5.</figDesc><table><row><cell>Data-set</cell><cell>GP-COACH-5</cell><cell></cell><cell>GP-COACH-9</cell><cell></cell><cell>HFRBCS(Chi)</cell><cell></cell><cell>GP-COACH-H</cell><cell></cell><cell>C4.5</cell></row><row><cell></cell><cell>GM tr</cell><cell>GM tst</cell><cell>GM tr</cell><cell>GM tst</cell><cell>GM tr</cell><cell>GM tst</cell><cell>GM tr</cell><cell>GM tst</cell><cell>GM tr</cell><cell>GM tst</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www.keel.es/datasets.php.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>V. López et al. / Knowledge-Based Systems 38 (2013) 85-104</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>V.López  et al. / Knowledge-Based Systems 38 (2013) 85-104</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3"><p>http://www.keel.es/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_4"><p>http://www.keel.es/datasets.php.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_5"><p>http://www.keel.es/imbalanced.php.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_6"><p>http://sci2s.ugr.es/sicidm/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_7"><p>http://www.keel.es/imbalanced.php.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been supported by the Spanish Ministry of Science and Technology under Projects TIN2011-28488 and TIN2008-06681-C06-02, and the Andalusian Research Plan P10-TIC-6858 and TIC-3928. V. López holds a FPU scholarship from Spanish Ministry of Education.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Detailed results for the experimental study</head><p>In this appendix we present the complete results tables for all the algorithms used in this work. Thus, the reader can observe the full training and test results, with their associated standard deviation, in order to compare the performance of each approach. In Table <ref type="table">11</ref> we show the detailed results for the GP-COACH-5, GP-COACH-9 and GP-COACH-H versions with SMOTE preprocessing for the GP procedure using the specificity and sensitivity measures. Next, in Table <ref type="table">12</ref> we show the results for the basic GP-COACH method and the hierarchical GP-COACH-H with and without SMOTE preprocessing. Later, the results for each FRBCS method with SMOTE preprocessing and C4.5 with SMOTE+ENN preprocessing over the highly imbalanced data-sets are shown in Table <ref type="table">13</ref>. Finally, Table <ref type="table">14</ref> presents the results for the same algorithms as Table <ref type="table">13</ref> over the borderline data-sets considered.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">4404 yeast2vs8 .7401 ± .0348</title>
		<idno>1497 .7401 ± .0348 .7283 ± .1497 .7410 ± .0351 .7283 ± .1497 .7544 ± .0319 .7274 ± .1487 .8049 ± .0211 .7601 ± .1606 .9937 ± .0047 .7381 ± .1765 yeast4 .0000 ± .0000 .0000 ± .0000 .0000 ± .0000 .0000 ± .0000 .0853 ± .1236 .0000 ± .0000 .8602 ± .0095 .7923 ± .0527 .8443 ± .0140 .7807 ± .0469 .9001 ± .0156 .8175 ± .0391 yeast1289vs7 .0000 ± .0000 .0000 ± .0000 .0000 ± .0000 .0000 ± .0000 .0000 ± .0000 .0000 ± .0000 .6325 ± .1556 .5262 ± .1253 .7996 ± .0672 .5860 ± .1633 .8843 ± .0292 .6939 ± .1205 yeast5 .0000 ± .0000 .0000 ± .0000 .0000 ± .0000 .0000 ± .0000 .0333 ± .0745 .0000 ± .0000 .9344 ± .0262 .9020 ± .0478 .9477 ± .0289 .9229 ± .0434 .9724 ± .0066 .9428 ± .0526 ecoli0137vs26 .6472 ± .0986 .1414 ± .3162 .3344 ± .1877 .1414 ± .3162 .8430 ± .0583 .1401 ± .3133 .9167 ± .0418 .7215 ± .4202 .9021 ± .0354 .7203 ± .4200</idno>
		<imprint>
			<biblScope unit="volume">7877</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">V</forename><surname>López</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">/ Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="85" to="104" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">V</forename><surname>López</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">/ Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="85" to="104" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A proposal for the genetic lateral tuning of linguistic fuzzy systems and its interaction with rule selection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Alcalá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alcalá-Fdez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="616" to="635" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">KEEL data-mining software tool: data set repository, integration of algorithms and experimental analysis framework</title>
		<author>
			<persName><forename type="first">J</forename><surname>Alcalá-Fdez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luengo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Derrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multi-Valued Logic and Soft Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="255" to="287" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">KEEL: a software tool to assess evolutionary algorithms for data mining problems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Alcalá-Fdez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Del Jesus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Garrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Otero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bacardit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Rivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="307" to="318" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Strategies for learning in class imbalance problems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Barandela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rangel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="849" to="851" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">How to handle the flexibility of linguistic variables with applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bastian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="463" to="484" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A study of the behaviour of several methods for balancing machine learning training data</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E A P A</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Prati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Monard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explorations</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="20" to="29" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">GP-COACH: genetic programming-based learning of compact and accurate fuzzy rule-based classification systems for high-dimensional problems</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Berlanga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Rivera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Del Jesus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="page" from="1183" to="1200" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic recognition of complete palynomorphs in digital images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Vision and Applications</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="53" to="60" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SMOTE: synthetic minority over-sampling technique</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligent Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pham</surname></persName>
		</author>
		<title level="m">Fuzzy Algorithms with Applications to Image Processing and Pattern Recognition</title>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A proposal for improving the accuracy of linguistic modeling</title>
		<author>
			<persName><forename type="first">O</forename><surname>Cordón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="335" to="344" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Genetic fuzzy systems: evolutionary tuning and learning of fuzzy knowledge bases</title>
		<author>
			<persName><forename type="first">O</forename><surname>Cordón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Magdalena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Fuzzy Systems -Applications and Theory</title>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generating the knowledge base of a fuzzy rulebased system by the genetic learning of the data base</title>
		<author>
			<persName><forename type="first">O</forename><surname>Cordón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Villar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="667" to="674" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Linguistic modeling by hierarchical systems of linguistic rules</title>
		<author>
			<persName><forename type="first">O</forename><surname>Cordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Zwir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="2" to="20" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A proposal on reasoning methods in fuzzy rule-based classification systems</title>
		<author>
			<persName><forename type="first">O</forename><surname>Cordón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Del Jesus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="21" to="45" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">C4.5, class imbalance, and cost sensitivity: why under-sampling beats over-sampling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Holte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML 2003) Workshop on Learning from Imbalanced Data Sets II</title>
		<meeting>the International Conference on Machine Learning (ICML 2003) Workshop on Learning from Imbalanced Data Sets II</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The foundations of cost-sensitive learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th IEEE International Joint Conference on Artificial Intelligence (IJCAI&apos;01)</title>
		<meeting>the 17th IEEE International Joint Conference on Artificial Intelligence (IJCAI&apos;01)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="973" to="978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The CHC adaptive search algorithm: How to have safe search when engaging in nontraditional genetic recombination</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Eshelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foudations of Genetic Algorithms</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Addressing the classification with imbalanced data: open problems and new challenges on class distribution</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Hybrid Artificial Intelligence Systems (HAIS&apos;11)</title>
		<meeting>the 6th International Conference on Hybrid Artificial Intelligence Systems (HAIS&apos;11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A study of the behaviour of linguistic fuzzy rule based classification systems in the framework of imbalanced data-sets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Del Jesus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fuzzy Sets and Systems</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="2378" to="2398" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical fuzzy rule based classification systems with genetic rule selection for imbalanced data-sets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Del Jesus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="561" to="577" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the 2-tuples based genetic tuning performance for fuzzy rule based classification systems in imbalanced datasets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Del Jesus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="page" from="1268" to="1291" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Evolutionary-based selection of generalized instances for imbalanced classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Derrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Triguero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Carmona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="3" to="12" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A study of statistical techniques and performance measures for genetics-based machine learning: accuracy and interpretability</title>
		<author>
			<persName><forename type="first">S</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luengo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="959" to="977" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An extension on &apos;&apos;statistical comparisons of classifiers over multiple data sets&apos;&apos; for all pairwise comparisons</title>
		<author>
			<persName><forename type="first">S</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2607" to="2624" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the k-NN performance in a challenging scenario of imbalance and overlapping</title>
		<author>
			<persName><forename type="first">V</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mollineda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Sánchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis Applications</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="269" to="280" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the effectiveness of preprocessing methods when dealing with different levels of class imbalance</title>
		<author>
			<persName><forename type="first">V</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mollineda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="13" to="21" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Class imbalance methods for translation initiation site recognition in DNA sequences</title>
		<author>
			<persName><forename type="first">N</forename><surname>García-Pedrajas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pérez-Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>García-Pedrajas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ortiz-Boyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fyfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="22" to="34" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Genetic fuzzy systems: taxonomy, current research trends and prospects</title>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="27" to="46" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A taxonomy for the crossover operator for real-coded genetic algorithms: an experimental study</title>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lozano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Sánchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="309" to="338" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A 2-tuple fuzzy linguistic representation model for computing with words</title>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Martínez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="746" to="752" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Effect of rule weights in fuzzy rule-based classification systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ishibuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="506" to="515" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Ishibuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nii</surname></persName>
		</author>
		<title level="m">Classification and Modeling with Linguistic Information Granules: Advanced Approaches to Linguistic Data Mining</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Selecting fuzzy if-then rules from classification problems using genetic algorithms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ishibuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nozaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="260" to="270" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rule weight specification in fuzzy rule-based classification systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ishibuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yamamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="428" to="435" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The class imbalance problem: a systematic study</title>
		<author>
			<persName><forename type="first">N</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stephen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent Data Analysis Journal</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="429" to="450" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Iterative boolean combination of classifiers in the ROC space: an application to anomaly detection with HMMs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Khreich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Miri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sabourin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="2732" to="2752" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Genetic Programming: On the Programming of Computers by Means of Natural Selection</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Koza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Addressing the curse of imbalanced training sets: onesided selection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kubat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Matwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International Conference on Machine Learning(ICML97</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="179" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A comparative study on rough set based class imbalance learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="753" to="763" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Analysis of preprocessing vs. cost-sensitive learning for imbalanced classification. Open problems on intrinsic data characteristics</title>
		<author>
			<persName><forename type="first">V</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Moreno-Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="6585" to="6608" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A fuzzy logic controller with learning through the evolution of its knowledge base</title>
		<author>
			<persName><forename type="first">L</forename><surname>Magdalena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Monasterio-Huelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="335" to="358" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data in presence of noisy and borderline examples</title>
		<author>
			<persName><forename type="first">K</forename><surname>Napierala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stefanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wilk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th International Conference on Rough Sets and Current Trends in Computing (RSCTC2010)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="158" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Ensemble learning with active example selection for imbalanced biomedical data classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Computational Biology and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="316" to="325" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Evolutionary rule-based systems for imbalanced datasets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Orriols-Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bernadó-Mansilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="213" to="225" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Quinlan</surname></persName>
		</author>
		<title level="m">C4.5: Programs for Machine Learning</title>
		<meeting><address><addrLine>Morgan Kauffman</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">SVM: which one performs better in classification of MCCs in mammogram imaging</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Vs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="144" to="153" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">An empirical study of the classification performance of learners on imbalanced and noisy software quality data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Seiffert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Hulse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Folleco</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2010.12.016</idno>
		<ptr target="http://dx.doi.org/10.1016/j.ins.2010.12.016" />
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint/>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Handbook of Parametric and Nonparametric Statistical Procedures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sheskin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Chapman &amp; Hall/CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Classification of imbalanced data: a review</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K C</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kamel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="687" to="719" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Extraction of classification rules characterized by ellipsoidal regions using soft-computing techniques</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Dillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Systems Science</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="969" to="980" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Mining with rarity: a unifying framework</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explorations</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="7" to="19" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Asymptotic properties of nearest neighbor rules using edited data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="408" to="421" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Data Mining Using Grammar-Based Genetic Programming and Applications</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Leung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Kluwer Academic Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">10 challenging problems in data mining research</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Information Technology &amp; Decision Making</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="597" to="604" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning and making decisions when costs and probabilities are both unknown</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zadrozny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Knowledge Discovery and Data Mining (KDD&apos;01)</title>
		<meeting>the 7th International Conference on Knowledge Discovery and Data Mining (KDD&apos;01)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="204" to="213" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
