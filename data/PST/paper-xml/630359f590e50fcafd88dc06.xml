<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DICE: Domain-attack Invariant Causal Learning for Improved Data Privacy Protection and Adversarial Robustness</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qibing</forename><surname>Ren</surname></persName>
							<email>renqibing@sjtu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yiting</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yichuan</forename><surname>Mo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
							<email>yanjunchi@sjtu.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="laboratory" key="lab1">MoE Key Lab of Artifi-cial Intelligence</orgName>
								<orgName type="laboratory" key="lab2">and Shanghai AI Laboratory</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">KDD &apos;22</orgName>
								<address>
									<addrLine>August 14-18</addrLine>
									<postCode>2022</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DICE: Domain-attack Invariant Causal Learning for Improved Data Privacy Protection and Adversarial Robustness</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3534678.3539242</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Data Privacy</term>
					<term>Robustness</term>
					<term>Causal Inference</term>
					<term>Attack Transferability</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The adversarial attack reveals the vulnerability of deep models by incurring test domain shift, while delusive attack relieves the privacy concern about personal data by injecting malicious noise into the training domain to make data unexploitable. However, beyond their successful applications, the two attacks can be easily defended by adversarial training (AT). While AT is not the panacea, it suffers from poor generalization for robustness. For the limitations of attack and defense, we argue that to fit data well, DNNs can learn the spurious relations between inputs and outputs, which are consequently utilized by the attack and defense and degrade their effectiveness, and DNNs can not easily capture the causal relations like humans to make robust decisions under attacks. In this paper, to better understand and improve attack and defense, we first take a bottom-up perspective to describe the correlations between latent factors and observed data, then analyze the effect of domain shift on DNNs induced by attack and finally develop our causal graph, namely Domain-attack Invariant Causal Model (DICM). Based on DICM, we propose a coherent causal invariant principle, which guides our algorithm design to infer the human-like causal relations. We call our algorithm Domain-attack Invariant Causal Learning (DICE) 1 and the experimental results on two attacks and one defense task verify its effectiveness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Existing DNNs often rely on the IID assumption that the training and test data follow the same distribution. When there exist domain shifts, the performance of DNNs on the new test domain would suffer dramatic degradation, which has been demonstrated by the pervasive existence of adversarial examples generated via injecting an imperceptible yet malicious noise to test domain <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b42">43]</ref>. Meanwhile, poisoning train data with manual noise, also named delusive attack, recently shows its effectiveness on maximizing test error of DNNs <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b17">18]</ref>, and thus by poisoning personal data before releasing them online, we can protect data privacy against unauthorized or even illegal use because training DNNs on the poisoned data would greatly degrade their performance on clean test data. As shown in Fig. <ref type="figure" target="#fig_1">1</ref>, although the shift away from the original domain incurred by the above two attacks brings a huge performance gap of DNNs, it does NOT affect human decisions since perturbations resulting from these attacks are imperceptible to humans <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b42">43]</ref>. One popular hypothesis is that the human cognitive system is capable of capturing causal relations that are invariant to domain shifts <ref type="bibr" target="#b13">[14]</ref>, while DNNs tend to fit all types of correlations to fit data well and there can be spurious ones, i.e., not the cause of labels. Therefore, it is reasonable to assume that the attacker succeeds by exploiting such spurious factors, to shift data away from the natural distribution. However, the vulnerability of DNNs revealed by attacks can be greatly mitigated by adversarial training (AT) <ref type="bibr" target="#b24">[25]</ref>, a defense strategy that minimizes adversarial risk on malicious data, which shows that such spurious factors disturbed by the attacker can be easily recovered by the defender. Consequently, the attackers can improve their power if they can identify and perturb the casual relation instead of the spurious one. In terms of data privacy, the stronger delusive attack indicates better protection of personal data.</p><p>On the opposite side, AT itself suffers from poor generalization with relatively low robustness on test domain <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. Since we assume that attacks succeed by utilizing spurious factors, we argue that to defend against attacks, AT tends to fit such spurious correlation between outputs and latent factors, which is not necessarily invariant across domains. Therefore, when the spurious association changes, e.g., from training domain to test domain, the performance of robustness will change accordingly, resulting in a large generalization gap. Overall, our defense is aimed at better robustness generalization by focusing on causal factors, instead of the above spurious correlation. In this spirit, since causal reasoning can identify causal relation and remove the spurious bias by intervention <ref type="bibr" target="#b29">[30]</ref>, we initiate our study towards understanding and improving attack and defense using this powerful tool.</p><p>However, here comes two main challenges to our goals: (i) How to construct a causal graph to describe causal relationships between latent factors and observed variables in the context of attack and defense. (ii) Based on our causal graph, how to efficiently infer the unobserved causal relations from observed variables remains to be solved.</p><p>To address the first challenge, along with the perspective of latent data generating, we propose our simple yet principled bottom-up Structural Causal Model <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref>: namely the Domain-attack Invariant Causal Model (DICM), as illustrated in Fig. <ref type="figure" target="#fig_2">2</ref>. In DICM, following causal assumptions of <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b40">41]</ref>, we split the latent factors into output-causative factors 𝑆 (e.g., shape or contour of the object) and others 𝑉 (e.g., the style of the object), both of which constitute the inputs 𝑋 . We additionally assume that 𝑆 and 𝑉 are spuriously correlated, which forms a spurious and even harmful path from 𝑉 to 𝑌 . Different from <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b40">41]</ref>, we introduce an extra domain variable 𝐷 to explicitly model the effect of domain shifts induced by the attack. So far, we are capable of re-interpreting how the attack and defense work using a causal view: (i) change the mechanism from domain variable to latent factors, e.g., the attacker crafts malicious examples via 𝑃 (𝑋 |𝑆, 𝑉 ) by perturbing 𝑃 (𝑉 |𝐷) with interventions on 𝐷; (ii) manipulate the distribution of domain variable, e.g., some defense seek robust prediction through an ensemble of cross-domain models <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b45">46]</ref>; or more unlabeled data <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">27]</ref>, both of which extend single domain to multiple domains and hence make models fit domain-invariant 𝑆. Finally, based on the above assumptions embedded on DICM, we formulate our causal invariance principle over attack-induced domain shift, which guides our algorithm design for causal inference.</p><p>For the second challenge, based on DICM, we propose a causal inference pipeline, called Domain-attack Invariant Causal Learning (DICE), to remove the spurious bias by causal intervention. Specifically, we propose to use the backdoor adjustment method <ref type="bibr" target="#b30">[31]</ref> for intervention, i.e., blocking the spurious path from 𝑉 to 𝑌 by intervening on 𝑉 as Fig. <ref type="figure" target="#fig_2">2</ref> (b) shows. Instead of performing the expensive "physical" intervention, DICE performs a practical "virtual" one from only the observation data. Motivated by a line of interpretability works that adversarial training produces human-perception aligned gradients <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b46">47]</ref>, we utilize such a prior provided by the robust model to approximate the confounder 𝑉 . We also propose to improve the diversity and confounding effect of 𝑉 in an adversarial way and we provide a random-sampled confounder set for an effective approximation of backdoor adjustment, as shown in Fig. <ref type="figure" target="#fig_2">2 (c</ref>). Finally, DICE learns to infer 𝑆 against 𝑉 through minimizing the devised causal invariant risk. Experimentally, we verify the advantages of DICE through three downstream tasks on real-world dataset CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b21">[22]</ref>: (i) for delusive attack, we propose to facilitate this attack by attacking the learned domain-invariant factors 𝑆. Our attack outperforms the current state-of-the-art methods even under strong defense; (ii) for adversarial attack, adversarial examples crafted on the learned 𝑆 exhibit strong transferability across different unknown target models, even being comparable to white-box attacks; (iii) for defense, we verify the generality and effectiveness of DICE by integrating DICE with two popular defense baselines, PGD-AT <ref type="bibr" target="#b24">[25]</ref> and TRADES <ref type="bibr" target="#b58">[59]</ref>, as two defense variants. Both of variants can significantly improve adversarial robustness over baselines.</p><p>Our main contributions are: (i) Methodologically, from a bottomup data generating process, we build our causal graph, namely Domain-attack Invariant Causal Model (DICM), to provide a unified view of the attack and defense. With our specific and moderate assumptions embedded in DICM, we further derive causal invariance principle, pointing out the necessity of identifying the output-causative factors. (ii) Algorithmically, we propose a causal inference pipeline, namely Domain-attack Invariant Causal Learning (DICE) to infer domain-invariant features via an effective approximation of backdoor adjustment. (iii) Experimentally, we demonstrate our DICE outperforms baselines under two attack and one defense scenarios, with better transferability of adversarial attack, better data privacy protection by delusive attack and better robustness by defense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>Consider a classification task with data (𝑥, 𝑦) ∈ X × Y from an observed distribution 𝐷. 𝐷 𝑡𝑟𝑎𝑖𝑛 denotes the training domain with 𝐷 𝑡𝑒𝑠𝑡 as the test domain, while D is the manually perturbed domain based on 𝐷. We first discuss the vanilla structural casual model, followed by the basic ideas on attack and defense as well as privacy and robustness, which serve the preliminaries for our main approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Vanilla Structural Causal Model</head><p>2.1.1 Structural Causal Model (SCM).. The structural causal model (SCM) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref> is used to describe the causal relationships. As shown in Fig. <ref type="figure" target="#fig_2">2</ref>, each directed arrow indicates the causal relationship between two variables, e.g., 𝑆 → 𝑌 denotes that 𝑆 is the cause of 𝑌 . In a SCM, if a variable is the common cause of two variables, it is called the confounder, which induces spurious correlation between them, e.g., 𝑋 ← 𝑉 ↔ 𝑆 → 𝑌 , where 𝑉 is the confounder of 𝑋 and 𝑌 . Furthermore, the above path from 𝑋 to 𝑌 is called back-door path, defined as a path that ends with an arrow pointing to 𝑋 . At last, causes of variables should meet some requirement, as previous works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b37">38]</ref> express in Principle 2.1: Principle 2.1. ( <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b37">38]</ref>). Independent Causal Mechanisms (ICM) Principle: The conditional distribution of each variable given its causes, i.e., its mechanism, does NOT inform or influence the other mechanisms.</p><p>2.1.2 Label prediction with SCM.. For causal learning for output prediction, one line of work ascribes causes of observed variables to the latent factors: those unobserved abstractions constitute inputs and their outputs <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">41]</ref>. <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b40">41]</ref> further assume that the latent factors can be disentangled into output-causative factors and others while there exist domain-specific correlation between them. Our work adopts the above assumptions as our base SCM while under the scenario of attack and defense, we build our SCM with two main contributions: (i) while <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b40">41]</ref> focus on Out-of-Distribution (OOD) problem and only consider the natural sampling bias across multiple environments/domains as the source, we focus on the single domain shift incurred by the attack, and analyze the effect of manually injected bias such as adversarial noise, which is necessary and more realistic since it is very likely for practitioners to collect artificial malicious examples from untrusted sources; (ii) we introduce a domain variable to model the effect of domain shift on the latent factors, resulted by the attack and defense. Though <ref type="bibr" target="#b40">[41]</ref> also adds a domain variable, it acts as the domain index, which is unobserved and simple. However, we conceptually re-interpret attack and defense in terms of domain effect (cf. Sec. 3.1) and explicitly model domain effect in algorithm design (cf. Sec. 3.2 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Preliminaries for Attack and Defense</head><p>Without loss of generality, we use the image recognition task and its conventional learning paradigm is to train a classifier 𝑓 : X → Y, to minimize the loss 𝐿(•, •). In this regard, standard training (ST) works by empirical risk minimization (ERM) over the clean data, which is defined as:</p><formula xml:id="formula_0">𝑅 𝑛𝑎𝑡 (𝑓 , 𝐷) = E (𝑥,𝑦)∼𝐷 [𝐿(𝑓 (𝑥), 𝑦)]<label>(1)</label></formula><p>In terms of the attack and defense, a widely used constraint especially in vision is that the added perturbation is within a ball, B (𝑥, 𝜖) = {𝑥 ′ : 𝑑 𝑝 (𝑥, 𝑥 ′ ) ≤ 𝜖}, and 𝑑 𝑝 (𝑥, 𝑥 ′ )= ∥ 𝑥 ′ − 𝑥 ∥ 𝑝 is the similarity metric, and a common choice is ℓ-∞ norm. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Preserving Privacy via Train Data Poisoning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Lifting Robustness via Adversarial Training</head><p>For resistance of adversarial (delusive) attacks, adversarial training (AT) <ref type="bibr" target="#b24">[25]</ref> remains the most effective approach, by minimizing the adversarial risk 𝑅 𝑎𝑑𝑣 as follows:</p><formula xml:id="formula_2">𝑅 𝑎𝑑𝑣 (𝑓 , 𝐷 𝑡𝑟𝑎𝑖𝑛 ) = E (𝑥,𝑦)∼𝐷 𝑡𝑟𝑎𝑖𝑛 [ max 𝑥 ′ ∈B (𝑥,𝜖) 𝐿(𝑓 (𝑥 ′ ), 𝑦)]<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>We build our causal graph from the causal data-generating process to form a unified view of the attack and defense, depicted in our Domain-attack Invariant Causal Model (DICM) in Sec. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Domain-attack Invariant Causal Model</head><p>In this section, in line with the majority of literature, we study the attack and defense for classification and present a causal view of the data-generating process behind it. Specifically, as illustrated in Fig. <ref type="figure" target="#fig_2">2</ref>, we inspect the causalities among five variables: input data instance 𝑋 , instance-level label 𝑌 , causal factors 𝑆, non-causal factors 𝑉 , and the domain variable 𝐷, with a Structural Causal Model <ref type="bibr" target="#b11">[12]</ref>. Following the discussion in Sec. 2.1.2, we next introduce the three main causal assumptions in DICM.</p><p>• (𝑆, 𝑉 )→ 𝑋 , 𝑆 → 𝑌 (latent generating mechanism)<ref type="foot" target="#foot_1">1</ref> : We introduce two latent factors 𝑆, 𝑉 as the abstractions that determine the observed variables (𝑋 , 𝑌 ), which have been similarly assumed in existing works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b40">41]</ref>. To be specific, 𝑆 as the Y-causative factors has a direct causal link to 𝑌 , which for example refers to the shape or contour of an object, while the non-causal factors 𝑉 together with 𝑆, generate 𝑋 , and in OOD setting, the generation process usually correlates with natural bias during sampling process, e.g., light or view, across multiple domains, while in the context of attack and defense, we assume the domain shift is incurred by manually injected bias, e.g., malicious noise by attacks. Different from <ref type="bibr" target="#b40">[41]</ref>, we focus on 𝑉 related to manual bias. Recall that the key constraint on both adversarial and delusive attack is human-imperceptibility <ref type="bibr" target="#b12">[13]</ref>, which means in the manner of human perception, 𝑋 (the appearance of an object) and 𝑌 (the annotated label) remain unchanged. Thus we argue that the attacks do not disturb the label-causative factor 𝑆, and 𝑆 → 𝑌 is invariant to domain shifts. However, the counter-intuitive behavior of DNNs against attacked examples 𝑋 reveals that the attacker may exploit high-frequency components to fool DNNs <ref type="bibr" target="#b47">[48]</ref>, which are not perceivable to humans and spuriously correlated with labels, i.e., 𝑉 . Therefore, the generative mechanism (𝑆, 𝑉 ) → 𝑋 on DNNs is different from humans. Beyond such difference, there is a common property for both DNNs and humans based on the ICM Principle (Principle 2.1): the generative mechanism 𝑃 (𝑋 |𝑉 , 𝑆) is unaffected under the intervened domain shift, i.e., 𝑃 (𝑉 , 𝑆 |𝑑𝑜 (𝐷)), which constitutes our main Assump. 3.1: Assumption 3.1. (Causal Invariance over Attack-induced Domain Shifts) By ICM Principle <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b37">38]</ref>, for causal models in DICM, the 𝑃 (𝑋 |𝑆, 𝑉 ) and 𝑃 (𝑌 |𝑆) are invariant to attack-induced domain shifts.</p><p>We give further discussion based on the above assumption.</p><p>• 𝑆 ↔ 𝑉 (latent spurious correlation): We further assume that there exists a spurious correlation between 𝑆 and 𝑉 , marked as a bidirected arrow, which results from natural (manual) bias in dataset, e.g., the background (𝑉 ) variation in data instances by sampling (attacks). Such a correlation 𝑃 (𝑆, 𝑉 ) opens a backdoor path, i.e., 𝑉 ↔ 𝑆 → 𝑌 , between 𝑉 and 𝑌 , and we ascribe the vulnerability of DNNs under delusive or adversarial attacks to the learned spurious bias during fitting the dataset.</p><p>• 𝐷 → (𝑆, 𝑉 ): For the correlation 𝑃 (𝑆, 𝑉 ), we introduce an auxiliary variable 𝐷. The magnitude of correlation can be either due to the changing mechanism 𝐷 → (𝑆, 𝑉 ) or the mutable distribution of the confounder 𝐷. Next we take a causal view to understand the attack and defense with help of 𝐷.</p><p>Causal view on criterion of attack and defense: So far, we can utilize our causal tool to redefine the criterion of attack and defense: given a data instance 𝑥, the label 𝑦 and the target label 𝑦 𝑡 , the attacker fools the DNN-based classifier, outputting 𝑦 𝑡 instead of 𝑦, by injecting 𝑣 𝑡 , the target spurious features, into 𝑣, or in other words, disturbing 𝑃 (𝑆, 𝑉 |𝐷) close to 𝑃 (𝑆, 𝑉 𝑡 |𝐷). The only difference between delusive and adversarial attack is that the former intervenes on training domain 𝐷 𝑡𝑟𝑎𝑖𝑛 while the latter does on test domain 𝐷 𝑡𝑒𝑠𝑡 . For defense, AT improves robustness via minimizing adversarial risk on malicious data. Since we assume that malicious data is generated from 𝑃 (𝑆, 𝑉 𝑡 |𝐷), we further state that AT improves model robustness by proactively fitting malicious data from 𝑃 (𝑋 |𝑆, 𝑉 𝑡 ) to defend against future possible attacks. Moreover, for the recent defense methods that require more data <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">27]</ref> or an ensemble of cross-domain models <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b45">46]</ref>, we argue that they improve robustness by manipulating 𝑃 (𝐷), extending it from single domain to multiple domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Causal Intervention by Backdoor Adjustment</head><p>Assump. 3.1 motivates us to identify 𝑆 against 𝑉 to fundamentally endow models with robustness or protect data against abuse. However, it is difficult to directly discover the causal factors 𝑆 from the observed variables, since in our analysis DNNs also fit spurious bias 𝑉 for prediction. We propose to use causal intervention: 𝑃 (𝑌 |𝑑𝑜 (𝑋 )) to learn the invariant mechanism 𝑃 (𝑌 |𝑆). Since "physical" intervention on 𝑋 , i.e., collecting instances with all possible views or backgrounds e.g. for images, is impossible, we apply backdoor adjustment <ref type="bibr" target="#b30">[31]</ref> to do "virtual" intervention on 𝑋 in Eq. 5.</p><formula xml:id="formula_3">𝑃 (𝑌 |𝑑𝑜 (𝑋 ), 𝐷) = ∑︁ 𝑣 𝑃 (𝑌 |𝑋, 𝑉 = 𝑣)𝑃 (𝑣 |𝐷)<label>(5)</label></formula><p>Based on Assump. 3.1, 𝑃 (𝑌 |𝑋, 𝑉 = 𝑣) is domain-agnostic as 𝐷 is separated from 𝑋 and 𝑌 with given 𝑣. Thus 𝑃 (𝑌 |𝑋, 𝑉 ) generalizes to test-domain 𝐷 = 𝑡 even if trained within domain 𝐷 = 𝑠. Moreover, by stratifying different values of confounder set V = {𝑣 } in Eq. 5 and intervening 𝑉 as 𝑣, we remove the causal link 𝑉 → 𝑋 . Since 𝑉 is also not observable, to this end, we approximate confounder set V = {𝑣 1 , 𝑣 2 , . . . , 𝑣 𝑛 } using the class-wise instance-specific mask, where 𝑛 is class size in dataset and 𝑣 𝑖 ∈ R ℎ×𝑤 where ℎ × 𝑤 is the instance size. We design an additional module to generate each mask for class 𝑖 to approximate the non-causal part of this class. For 𝑃 (𝑣 |𝐷), to avoid dependence of causal intervention on the training domain, 𝑃 (𝑣 |𝐷) is set as the uniform 1/𝑛.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Domain-attack Invariant Causal Learning</head><p>Our framework consists of four components, as shown in Fig. <ref type="figure" target="#fig_3">3</ref> .</p><p>Confounder Generator. Motivated by the observation that loss gradients from robust models align better with salient data features and human perception, which well outlines the contour of an object in images <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b46">47]</ref> (see visualization in Fig. <ref type="figure" target="#fig_2">2</ref>), in the context of image classification, we specify to utilize magnitude of gradients as the importance of pixels and generate the mask. Specifically, given an instance 𝑥, the label 𝑦, the intervention generator first adopt a robust model with 𝑓 𝑝𝑟𝑖𝑜𝑟 to obtain mask prior 𝑀 𝑝𝑟𝑖𝑜𝑟 , to output the instance-specific confounder ṽ𝑥 :</p><formula xml:id="formula_4">𝛿 = ∇ 𝑥 𝐿(𝑓 𝑝𝑟𝑖𝑜𝑟 (𝑥), 𝑦), 𝑀 𝑖 𝑗 = 0, max 𝑘 𝛿 𝑖 𝑗𝑘 &gt; 𝑑 1, 𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒 , ṽ𝑥 = 𝑀 𝑝𝑟𝑖𝑜𝑟 ⊙𝑥<label>(6)</label></formula><p>where 𝛿 ∈ R ℎ×𝑤×3 is the loss gradient of robust models, 𝑑 is the threshold that determines the number of pixels as the confounder, and ⊙ is the element-wise multiplication. For simplicity, we select the value that excludes 50% pixels per image as 𝑑 in all experiments. Moreover, we propose to adaptively update masks based on prior from robust model and knowledge learned by our model during training in a running average way, i.e., 𝑀 = 𝛼 * 𝑀 𝑜𝑢𝑟𝑠 + (1 − 𝛼) * 𝑀 𝑝𝑟𝑖𝑜𝑟 , where 𝛼 increases monotonically with more epochs.</p><p>Confounder Replay Buffer. Unlike existing invariant learning works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b22">23]</ref> for OOD, which construct different domains by partitioning the training set to infer the invariant features, we try to identify invariant features on a single domain by causal intervention, as shown in Eq. 5. To block all spurious paths from 𝑋 to 𝑌 , we need intervene on all possible confounders 𝑉 , which is impractical. In this regard, we collect ṽ𝑥 of all the inputs computed previously into a replay buffer and next randomly sample a confounder set Ṽ = { ṽ1 , ṽ2 , . . . , ṽ𝑛 } to conduct the intervention on the inputs of the modules: encoder and classifiers.</p><p>Encoder &amp; Classifiers. To measure the prediction discrimination of 𝑆 against 𝑉 , we employ a shared encoder ℎ with two classifiers, 𝑔 𝑠 and 𝑔 𝑣 , where ℎ maps the intervened input 𝑥 s to representation z s ∈ R 𝑑 , and 𝑔 𝑠 projects z s into a probability distribution ŷ s over class labels, shown in Eq. 7. Analogously, the predictive power of confounder ṽ𝑥 , i.e., ŷ ṽ , can be measured via 𝑔 𝑣 .</p><formula xml:id="formula_5">z s = ℎ(𝑥 s ), ŷ s = 𝑔 𝑠 (z s )<label>(7)</label></formula><p>To calculate Eq. 5, with the confounder set Ṽ, it takes 𝑛 times forward pass, which is expensive. To address this challenge, we adopt the Normalized Weighted Geometric Mean (NWGM) approximation <ref type="bibr" target="#b51">[52]</ref> to move the outer sampling over 𝑣 into the feature level, i.e., 𝑣 𝑃 (𝑌 |𝑋, 𝑉 = 𝑣)𝑃 (𝑣) ≈ 𝑃 (𝑌 |𝑋, 𝑣 𝑃 (𝑣)𝑣), such that the forward cost is reduced to only once. Furthermore, since we obtain the pixel-level confounder ṽ𝑥 , we constitute 𝑥 s = 𝑥 + ṽ ∈ Ṽ 𝑃 ( ṽ) ṽ by feature addition.</p><p>Optimization. Having obtained the prediction ŷ s under causal intervention, we can build our causal invariant risk 𝑅 𝑠 as follows:</p><formula xml:id="formula_6">𝑅 𝑠 (𝑓 𝑠 , 𝐷) = E (𝑥,𝑦)∼D,𝑆=𝑥 s [𝐿(𝑓 𝑠 (𝑥 s ), 𝑦)]<label>(8)</label></formula><p>where 𝑓 𝑠 (•) denotes 𝑔 𝑠 •ℎ(•) for simplicity, D is the training domain.</p><p>Based on Eq. 8, we define the final risk 𝑅 𝑐𝑎𝑢𝑠𝑎𝑙 by combining 𝑅 𝑛𝑎𝑡 and 𝑅 𝑠 with a tunable hyper-parameters 𝛽 as:</p><formula xml:id="formula_7">𝑅 𝑐𝑎𝑢𝑠𝑎𝑙 (𝑓 𝑠 , 𝐷) = E (𝑥,𝑦)∼D,𝑆=𝑥 s [𝐿(𝑓 𝑠 (𝑥), 𝑦) + 𝛽 * 𝐿(𝑓 𝑠 (𝑥 s ), 𝑦)]<label>(9)</label></formula><p>For optimizing 𝑔 𝑣 , aiming at generating various and powerful confounders that may not exist in original domain, we propose to perform adversarial attack on the confounders ṽ𝑥 and add them into the replay buffer. The confounding adversarial risk 𝑅 𝑐𝑜𝑛𝑓 is as follows:</p><formula xml:id="formula_8">𝑅 𝑐𝑜𝑛𝑓 (𝑓 𝑣 , 𝐷) = E (𝑥,𝑦)∼D,𝑉 = ṽ𝑥 max ṽ′ 𝑥 ∈B ( ṽ𝑥 ,𝜖) 𝐿(𝑓 𝑣 ( ṽ ′ 𝑥 ), 𝑦)<label>(10)</label></formula><p>where 𝑓 𝑣 (•) denotes 𝑔 𝑣 • ℎ(•). Overall, we can jointly optimize the above two components:</p><formula xml:id="formula_9">min ℎ,𝑔 𝑠 𝑅 𝑐𝑎𝑢𝑠𝑎𝑙 (𝑓 𝑠 , 𝐷) + min 𝑔 𝑣 𝑅 𝑐𝑜𝑛𝑓 (𝑓 𝑣 , 𝐷)<label>(11)</label></formula><p>The training procedure and detailed implementations are summarized in Appendix 7.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Causality guided Attack and Defense</head><p>In this subsection, we propose our causal invariant criterion to guide the three mainstream tasks of trustworthy AI, aiming at boosting their performance by the learned causal invariant features.</p><p>Delusive Attack: For this attack, we adopt the setting of <ref type="bibr" target="#b9">[10]</ref> </p><p>Adversarial Defense: For adversarial defense, our causal invariant criterion can be seamlessly integrated with popular defense mechanisms such as PGD-AT <ref type="bibr" target="#b24">[25]</ref>, and TRADES <ref type="bibr" target="#b58">[59]</ref>. Formally, we introduce the adversarial risk in Eq. 4 into our causal invariant risk in Eq. 9 as: min</p><formula xml:id="formula_11">𝑓 𝑠 max D𝑡𝑟𝑎𝑖𝑛 ∼B (𝐷 𝑡𝑟𝑎𝑖𝑛 ,𝜖) 𝑅 𝑐𝑎𝑢𝑠𝑎𝑙 (𝑓 𝑠 , D)<label>(14)</label></formula><p>Since we focus on defending against adversarial attacks on 𝑥 via minimizing 𝑅 𝑐𝑎𝑢𝑠𝑎𝑙 , we propose to stop the gradient flow of 𝑅 𝑐𝑜𝑛𝑓 before the feature encoder to avoid the spurious bias resulted by ṽ ′ 𝑥 interferes with robust representation learning on 𝑥.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORKS</head><p>OOD Generalization with Causality Previous studies mostly seek to construct multiple domains/environments to infer causal invariant features by either partitioning the training data by prior knowledge <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">41]</ref>, or adversarial environment inference <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b50">51]</ref>, while our work focuses on single domain shift incurred by the attack and defense. <ref type="bibr" target="#b33">[34]</ref> proposes an adversarial domain augmentation to achieve domain generalization based on a single domain while it does not use causal assumptions. For causal assumptions, the old school methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32]</ref> assumes causal relations directly reside between observed inputs and outputs, which may not well suit with visual data which is the majority of benchmarks, since causality lies in conceptually latent space <ref type="bibr" target="#b23">[24]</ref>. For latent factors, we follow the assumption of <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b40">41]</ref>, i.e., the mechanism that maps latent factors to observational distribution is invariant to domain shift. However, we further propose our assumptions by (i) extending the cause of spurious relation from natural sampling bias to manually injected bias such as adversarial noise; (ii) introduce a domain variable to model the effect of attack and defense on latent factors; (iii) propose our domain-attack invariance principle. Moreover, <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b40">41]</ref> infer latent factors with latent generative models while we do by causal intervention <ref type="bibr" target="#b30">[31]</ref>. Another similar work is <ref type="bibr" target="#b25">[26]</ref> which augments images by intervening non-causal factors in self-supervised learning, while the causal factors actually depend on non-causal ones for given inputs based on its assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adversarial robustness with causality</head><p>The most relevant work is ADA <ref type="bibr" target="#b60">[61]</ref>, which analyzes adversarial attack through a causal lens. However, ADA assumes the independence between causal factors and non-causal factors, which is unrealistic. Methodologically, ADA uses a "soft" intervention by penalizing the adversarial distribution and the natural distribution while we approximate "hard" intervention by backdoor adjustment. More importantly, our work provides a unified causal view of adversarial(delusive) attack and defense together while ADA only considers the adversarial data generating process. Another relevant work is CAMA <ref type="bibr" target="#b55">[56]</ref>, which directly models the latent space with a generative model, being different from our causal inference from observed data. Moreover, CAMA assumes the output is the cause of inputs, which is impractical, e.g., intervening the label with noise by distracting the sampler does not change the image.</p><p>Causal Inference It is about causal-effect reasoning or counterfactual learning by "do-calculus" intervention <ref type="bibr" target="#b29">[30]</ref>. Recent works mostly follow the causal assumption that the observed inputs have a portion of causal relations with outputs <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b55">56]</ref>, while <ref type="bibr" target="#b50">[51]</ref> generalizes similar hypothesis to non-Euclidean graph data for capturing causal information among ego-graph features. However, these studies focus on guiding the representation or predictor model to leverage causality behind data, while our work designs an effective approximation of backdoor adjustment to directly infer causal factors for robust learning purpose. Similarly, CONTA <ref type="bibr" target="#b56">[57]</ref> and CaaM <ref type="bibr" target="#b48">[49]</ref> also utilize backdoor adjustment for causal inference, besides difference in basic causal assumptions, our work differs from them in two aspects: (i) we generate confounders based on human-perception aligned gradients from robust models while CONTA utilizes CAM and CaaM based on the attention module, which is technically differently; (ii) we design a confounder replay buffer with adversarial confounders added, bringing a more efficient approximation of backdoor adjustment while such designs are missing in CONTA and CaaM. Moreover, another related work CATT <ref type="bibr" target="#b52">[53]</ref> proposes to remove spurious bias on attention-based vision-language models via a frontdoor adjustment, instead of our backdoor adjustment.</p><p>Adversarial Attack Since the seminal works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b42">43]</ref> that reveal the vulnerability of DNNs, studying stronger adversarial attacks has become a trending direction, among which FGSM <ref type="bibr" target="#b12">[13]</ref>, PGD attack <ref type="bibr" target="#b24">[25]</ref>, C&amp;W attack <ref type="bibr" target="#b3">[4]</ref>, and AutoAttack <ref type="bibr" target="#b7">[8]</ref> become the popular attack baselines to evaluate robustness of DNNs.</p><p>Delusive Attack Besides adversarial attack on test data, poisoning train data, i.e., delusive attack, recently has become a heated topic for preserving data privacy. DeepConfuse <ref type="bibr" target="#b8">[9]</ref> first applies delusive attack to DNNs with auto-encoder. More delusive attacks have been devised by adopting new techniques and approximations, e.g., gradient alignment <ref type="bibr" target="#b10">[11]</ref>, computation graph unrolling <ref type="bibr" target="#b18">[19]</ref> and loss minimization objective <ref type="bibr" target="#b17">[18]</ref>, etc. Our work is based on loss maximization objective <ref type="bibr" target="#b9">[10]</ref>, and further mitigates its limitation against adversarial training by our learned causal features.</p><p>Adversarial Defense The discovery of adversarial examples promote the development of defense methods, among which adversarial training (AT) <ref type="bibr" target="#b24">[25]</ref> is considered a principled defense against adversarial attacks. Recent works also show the effectiveness of AT against delusive attacks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b43">44]</ref>. Moreover, many works analyze the limitations of AT in two aspects: (i) the trade-off between accuracy and robustness <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b58">59</ref>], (ii) robust overfitting <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b35">36]</ref>, for which we take a causal view to derive our insight and solution in this work. Many variants have also been proposed to improve AT by appearance-similar regularization <ref type="bibr" target="#b34">[35]</ref>, rethinking the misclassified examples <ref type="bibr" target="#b49">[50]</ref>, sample-wise importance reweighting <ref type="bibr" target="#b59">[60]</ref>, and adding more unlabeled data <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">27]</ref>. Based on our causal graph, all of them can be encapsulated into two types: (i) manipulating the distribution of domain variable; (ii) changing the mechanism from domain variable to latent factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We verify the efficacy of DICE on three downstream tasks. We first show that our delusive attack is stronger than the current state-of-the-art methods for better privacy protection (c.f. Sec. 5.1). Secondly, DICE also improves transferability of adversarial attack under the challenging black-box setting, even achieving comparable performance to white-box attacks (c.f. Sec. 5.2). Finally, DICE integrated with two defense paradigms exhibits better robustness over their baselines (c.f. Sec. 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Improving Data Privacy Protection</head><p>Evaluation metrics and training details. In Sec. 3.4, we introduce our delusive attack based on TAP <ref type="bibr" target="#b9">[10]</ref>. In line with TAP, we perform delusive attack on the base model, then train victim models on the poisoned data and compute its accuracy on clean test data to measure the attack's performance.</p><p>Attack Models. To fairly compare our method with EM and TAP, we use ResNet-18 (RN18) <ref type="bibr" target="#b14">[15]</ref> as the backbone of all methods to generate poisoned data. In line with TAP, we perform PGD-200 <ref type="bibr" target="#b24">[25]</ref> attack to craft poison data examples under 𝑙-∞ norm with 𝜖 = 8/255, and we adopt the differentiable data augmentation from TAP to further improve the potency of the generated poisons.</p><p>Defense Models To fully evaluate the attack performance, similar to TAP, we test two data augmentations not known to the attacker during generating poisons, Gaussian Smoothing (GS) and   <ref type="table" target="#tab_3">1</ref> shows the result of delusive attack. Our DICE generates more powerful poisoned data over the state-of-the-art EM and TAP attacks against all defenses. Even under AT, our poisoned data still remains its effectiveness. We report more results under AT with larger perturbation radii 𝜖 in Appendix 7.2.3, which our attack consistently shows its advantage. It is worth noting that AT might not be an ideal solution for delusive attack, since it is computationally expensive and degrades clean accuracy especially for large scale dataset, e.g., in Table <ref type="table" target="#tab_3">1</ref>, CIFAR100 under AT (𝜖=2/255) drops from 78.25% to 67.54% without being attacked.</p><p>Less Data. We then study a more challenging and realistic scenario, where only part of data is protected, i.e., being poisoned. Specifically, we randomly select a certain proportion of data from CIFAR10, poison them, and train models on the mixed data and the remaining clean data. Table <ref type="table" target="#tab_4">2</ref> shows that (i) models trained on mixed data perform worse than only clean data in all cases, verifying the effectiveness of poisons; (ii) as data protection proportion increases, i.e., more poisoned data is added, both of attacks become stronger while DICE is consistently stronger than TAP. Since EM <ref type="bibr" target="#b17">[18]</ref> are observed to be slightly helpful when combined with clean data, we do not report results of EM. All poisoned data are crafted by PGD-50.</p><p>Transferability. We further test the transferability of our delusive attack. Specifically, we select RN18 as the surrogate model on which poisoned data gets crafted by PGD-200, then train various target models on the poisoned data. For a thorough evaluation, we select VGG19 <ref type="bibr" target="#b38">[39]</ref>, GoogleNet <ref type="bibr" target="#b41">[42]</ref>, DenseNet-121 (DN121) <ref type="bibr" target="#b16">[17]</ref> and MobileNet-V2 (MOB-V2) <ref type="bibr" target="#b15">[16]</ref> as target models. Results of Table <ref type="table" target="#tab_5">3</ref> show that our DICE is stronger than TAP across all targets models. Ablation Study. We perform a sensitivity analysis about 𝛾 of Eq. 12 in Appendix 7.2.4. We also compare the effects of different attack steps during crafting poisoned data on model performance. See Appendix 7.2.5 for details. Finally, we present time complexity analysis in Appendix 7.2.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Improving Transferability of Adversarial Attack</head><p>Evaluation metrics and training details. To evaluate the transferability of attacks, we generate adversarial examples based on surrogate model, then transfer those examples to target models to compute the test accuracy as a measurement. For a thorough evaluation, we perform FGSM <ref type="bibr" target="#b12">[13]</ref> and PGD-100 attacks on CIFAR10 respectively, which get crafted under the 𝑙 ∞ norm with 𝜖 = 8/255. Baseline Models. To cover as many different deep architectures as possible, we select VGG16 <ref type="bibr" target="#b38">[39]</ref>, ResNet-18 (RN18), ResNet-50 (RN50) <ref type="bibr" target="#b14">[15]</ref>, and DenseNet-121 (DN121) <ref type="bibr" target="#b16">[17]</ref> as our target models. We reproduce these models based on a popular Github repository <ref type="foot" target="#foot_2">2</ref> . For a fair comparison, we select WRN-34-10 (WRN) <ref type="bibr" target="#b54">[55]</ref>, the backbone of our model, as the surrogate model baseline.</p><p>Transfer attack results. Table <ref type="table" target="#tab_6">4</ref> shows that our model consistently achieves stronger transferability over the target models than WRN by a huge margin, which is comparable to white-box attacks that have the full knowledge of target models.  <ref type="bibr" target="#b3">[4]</ref> attacks, and AutoAttack (AA.) <ref type="bibr" target="#b7">[8]</ref>. We choose WRN-34-10 (WRN) as the backbone for both CIFAR-10 and CIFAR-100. We adopt the hyper-parameters recipe for training suggested by <ref type="bibr" target="#b28">[29]</ref> and robustness is evaluated on the last checkpoint of all models. More training details is given in Appendix 7.3.</p><p>Baseline Models. We propose our causal guided adversarial defense objective in Eq. 14. We apply our DICE to two popular defense paradigms, PGD-AT <ref type="bibr" target="#b24">[25]</ref> and TRADES <ref type="bibr" target="#b58">[59]</ref> respectively, to build our robust model, named DICE-M and DICE-T.</p><p>Sensitivity of regularization hyperparameter 𝛽 on CIFAR10 As Eq. 9 shows, 𝛽 is an important hyper-parameter. In Table <ref type="table" target="#tab_7">5</ref>, we select WRN as the backbone and compare PGD-AT baseline to DICE-M with different 𝛽 values, and find that as 𝛽 ∈ [0, 4], larger 𝛽 brings better clean accuracy vs. robustness trade-off than baseline (𝛽=0). However, as 𝜆 further increases, e.g., 𝛽 = 5, our model starts to overfit the adversarial noise induced by the C&amp;W attack since its robustness begins to decrease under PGD attack. Therefore, we set 𝛽 as 4 for both CIFAR-10 and CIFAR-100.</p><p>Performance on CIFAR-100 Besides CIFAR-10, we further apply our method on CIFAR-100 and Table <ref type="table" target="#tab_8">6</ref> shows that our two variants DICE-M and DICE-T consistently exhibit better robustness than their baseline PGD-AT and TRADES respectively under various attacks including AutoAttack. Moreover, DICE helps robust baselines with a better trade-off between accuracy and robustness: all variants enjoy better test accuracy than their baselines.</p><p>Consideration of gradient obfuscation To exclude the possible effect of obfuscated gradients that give a false sense of security, we perform a series of sanity checks suggested by <ref type="bibr" target="#b1">[2]</ref>   than PGD-100 (48.57%). Therefore, according to the criterion given by <ref type="bibr" target="#b1">[2]</ref>, we believe that the robustness of our model does not result from obfuscated gradients.</p><p>Consideration of adaptive attack Based on the adaptive attack criterion <ref type="bibr" target="#b44">[45]</ref>, we generate attacks by maximizing our causal invariant risk 𝑅 𝑐𝑎𝑢𝑠𝑎𝑙 , i.e., Eq. 9 and DICE-M on CIFAR-10 achieves 49.04% robust accuracy under the adaptive PGD-20 attack compared to 49.28% under vanilla PGD-20 attack, showing our model consistently achieves strong robustness even under a specifically designed attack.</p><p>Mitigating robust overfitting Robust overfitting is a common limitation existing in current defense methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b35">36]</ref>, which means that model robustness on test data begins decreasing after the first learning rate decay while robustness on train data keeps increasing. The test accuracy plot in Fig. <ref type="figure" target="#fig_5">4</ref> shows that our model indeed mitigates this limitation with both better clean accuracy and robustness, which is aligned with our analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>In this paper, we take a bottom-up view to model the latent data generating process and understand the limitations of the attack and defense. In specific, we propose our causal graph and show that the spurious correlation between latent factors and outputs is exploited by attacks while the defense also learns to fit such spurious relations to defend against attacks, and such reliance may result in poor generalization defense. Therefore, we argue that inferring causal relations like humans is important for improving both attack and defense. Inspired by our causal graph, we propose a causal inference pipeline to learn domain-invariant features via an effective approximation of causal intervention. Experimental results verify the utility of our method, bringing better privacy protection on delusive attack, improved transferability on adversarial attack, and higher robustness on defense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">APPENDIXES 7.1 Domain-attack Invariant Causal Learning</head><p>7.1.1 training details. Models are trained on the SGM optimizer with momentum 0.9, weight decay 0.0005, and batch size 128. For both CIFAR10 and CIFAR100, we set the learning rate as 0.1, which gets decays at 100 and 105 epochs with total 110 epochs. 𝛽 in Eq. 9 is kept as 1.0 for both delusive attack and adversarial attack, and 4.0 for adversarial defense.</p><p>For generating confounders, we find that directly relying on the current model without prior model achieves comparable or even better performance than ensembling the two models together. Therefore, we only utilize our current model to generate confounders.</p><p>For confounder replay buffer, since our confounder gets updated during training, the newly added confounders are relatively more important than old ones. So when sampling confounder set, we assign each confounder with a timestamp, which gets decayed since being first added, thus those newly added confounders are more likely to be sampled during training. We set the maximum size of reply buffer as 10000 and the confounder set size is set as 20. We follow TAP <ref type="bibr" target="#b9">[10]</ref> to perform delusive attacks. For the standard trained classifier, it is trained for 40 epochs with a batch size of 128, a momentum factor of 0.9, a weight decay factor of 0.0005, an initial learning rate of 0.1, and a learning rate scheduler that decays learning rate by 0.1 after 15, 25 and 35 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Causality guided Delusive Attack</head><p>The regularization hyper-parameter 𝛾 is set to 0.5 for every experiment. For CIFAR10, attack iteration is set to 200. For CIFAR100, attack iteration is set to 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7.2.2</head><p>Hyper-parameters and training details for evaluatioin. We follow TAP <ref type="bibr" target="#b9">[10]</ref> to craft poisoned data on a fixed pretrained model. For pretraining, the classifier is trained for 200 epochs with a batch size of 128, a momentum factor of 0.9, a weight decay factor of 0.0005, an initial learning rate of 0.1, and a CosineAnnealing learning rate scheduler.</p><p>For the adversarial training paradigm used in defending delusive attacks, we follow the official setting of PGD-AT <ref type="bibr" target="#b24">[25]</ref> and build the PGD-7 attack adversary with a relative step size 0.25. Classifiers are trained for 100 epochs with a batch size of 128, a momentum factor of 0.9, a weight decay factor of 0.0005, an initial learning rate of 0.1, and a learning rate scheduler that decays learning rate by 0.1 after 40 and 80 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.3">More results under stronger AT.</head><p>To fully compare the effect of AT against delusive attack, we perform stronger AT with larger perturbation budget 𝜖=3/255 and 4/255 on DICE and TAP. Table <ref type="table" target="#tab_10">7</ref> shows that when 𝜖=3/255, DICE is stronger then TAP while with 𝜖=4/255, the protection of both DICE and TAP becomes worthless. However, as we argue in our paper, a large AT perturbation would induce lower natural accuracy, hindering its applicability, thus it may not be the ideal solution for delusive attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.4">Sensitivity analysis of the regularization hyper-parameter 𝛾.</head><p>Eq. 12 tells that 𝛾 is an important regularization hyper-parameter for our delusive attack. For this experiment, we employ PGD-50   <ref type="table" target="#tab_12">9</ref> shows that more attack steps induce stronger poison attacks. For a better trade-off between computation cost and performance, in our paper, we finally choose poison data by PGD-200 step attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.6">Time complexity analysis.</head><p>In line with TAP, our method enjoys the same advantage of the ease of crafting perturbations: we craft the poisoned data on a fixed pretrained model, without extra training cost. In contrast, DeepConfuse <ref type="bibr" target="#b8">[9]</ref> needs training an adversarial auto-encoder (5 7 GPU days on simple datasets) and EM <ref type="bibr" target="#b17">[18]</ref> needs iteratively updating model and poisons until the evaluation metric meets their threshold. Moreover, EM also needs grasp the whole training dataset at once, which is less realistic. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Causality guided Adversarial Training</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Vulnerability of DNNs under manual attacks.</figDesc><graphic url="image-8.png" coords="1,325.26,450.54,126.24,69.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) Our domain-attack invariant causal model (DICM), (b) The intervened DICM under causal intervention, (c) Realization of causal intervention by backdoor adjustment.(DICE), to remove the spurious bias by causal intervention. Specifically, we propose to use the backdoor adjustment method<ref type="bibr" target="#b30">[31]</ref> for intervention, i.e., blocking the spurious path from 𝑉 to 𝑌 by intervening on 𝑉 as Fig.2(b) shows. Instead of performing the expensive "physical" intervention, DICE performs a practical "virtual" one from only the observation data. Motivated by a line of interpretability works that adversarial training produces human-perception aligned gradients<ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b46">47]</ref>, we utilize such a prior provided by the robust model to approximate the confounder 𝑉 . We also propose to improve the diversity and confounding effect of 𝑉 in an adversarial way and we provide a random-sampled confounder set for an effective approximation of backdoor adjustment, as shown in Fig.2 (c). Finally, DICE learns to infer 𝑆 against 𝑉 through minimizing the devised causal invariant risk.Experimentally, we verify the advantages of DICE through three downstream tasks on real-world dataset CIFAR-10 and CIFAR-100<ref type="bibr" target="#b21">[22]</ref>: (i) for delusive attack, we propose to facilitate this attack by attacking the learned domain-invariant factors 𝑆. Our attack outperforms the current state-of-the-art methods even under strong defense; (ii) for adversarial attack, adversarial examples crafted on the learned 𝑆 exhibit strong transferability across different unknown target models, even being comparable to white-box attacks; (iii) for defense, we verify the generality and effectiveness of DICE by integrating DICE with two popular defense baselines, PGD-AT<ref type="bibr" target="#b24">[25]</ref> and TRADES<ref type="bibr" target="#b58">[59]</ref>, as two defense variants. Both of variants can significantly improve adversarial robustness over baselines.Our main contributions are: (i) Methodologically, from a bottomup data generating process, we build our causal graph, namely Domain-attack Invariant Causal Model (DICM), to provide a unified view of the attack and defense. With our specific and moderate assumptions embedded in DICM, we further derive causal invariance principle, pointing out the necessity of identifying the output-causative factors. (ii) Algorithmically, we propose a causal inference pipeline, namely Domain-attack Invariant Causal Learning (DICE) to infer domain-invariant features via an effective approximation of backdoor adjustment. (iii) Experimentally, we demonstrate our DICE outperforms baselines under two attack and one defense scenarios, with better transferability of adversarial attack, better data privacy protection by delusive attack and better robustness by defense.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Pipeline of Domain-attack Invariant Causal Learning. The green line corresponds to the data flow through the confounding classifier, where ṽ ′𝑥 is the adversarial confounder being attacked. On the causal data flow via the orange line, both the original input 𝑥 and intervened 𝑥 s get inferred to the shared encoder to obtain output ŷ𝑥 and ŷ s respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Natural (Robust) test accuracy of PGD-AT (red line) and DICE-M (blue line) trained using WRN-34-10 on CIFAR-10 under PGD-20 attack during training. PGD-AT-Nat and PGD-AT-Adv mean the accuracy of PGD-AT on natural and adversarial data respectively, which also applies to our model. To better verify the effectiveness of our model on mitigating robust overfitting, the learning rate gets decayed at 30 and 60 epochs during the whole 100 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>7. 2 . 1</head><label>21</label><figDesc>Hyper-parameters and training details for delusive attack.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>7. 3 . 1</head><label>31</label><figDesc>Training details. We craft PGD attacks under the 𝑙-∞ norm with 𝜖=8/255 in all defense experiments. In training, we use the PGD-10 attack adversary with a step size of 0.25. Models are trained on the SGM optimizer with momentum 0.9, weight decay 0.0005, and batch size 128. For CIFAR10/100, we set the learning rate as 0.1, which gets decays at 100 and 105 epochs with total 110 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>, which crafts attack examples based on standard training (ST) via maximizing classification loss. Based on our causal classifier 𝑓 𝑠 trained by Eq. 11, we propose to regularize the attack generating process to focus more on causal features: max D𝑡𝑟𝑎𝑖𝑛 ∼B (𝐷 𝑡𝑟𝑎𝑖𝑛 ,𝜖) [𝑅 𝑛𝑎𝑡 (𝑓 𝑏𝑎𝑠𝑒 , D𝑡𝑟𝑎𝑖𝑛 ) +𝛾 * 𝑅 𝑛𝑎𝑡 (𝑓 𝑠 , D𝑡𝑟𝑎𝑖𝑛 )] (12)where 𝑓 𝑏𝑎𝑠𝑒 is the standard trained classifier with 𝑓 𝑠 as ours, and 𝛾 is a tunable hyper-parameter.</figDesc><table><row><cell>Adversarial Attack: For adversarial attack, we directly attack</cell></row><row><cell>our model via the causal classifier 𝑔 𝑠 :</cell></row><row><cell>max D𝑡𝑒𝑠𝑡 ∼B (𝐷 𝑡𝑒𝑠𝑡 ,𝜖)</cell></row></table><note>𝑅 𝑛𝑎𝑡 (𝑓 𝑠 , D𝑡𝑒𝑠𝑡 )</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Test accuracy (%) trained on poisoned data protected by defensive noises including MIXUP, DGSGD, and adversarial training with different perturbation radii 𝜖. EM: errorminimizing noise. TAP: targeted adversarial poisoning noise.</figDesc><table><row><cell>Defender</cell><cell cols="3">Attacker Clean EM</cell><cell>TAP DICE (ours)</cell></row><row><cell cols="2">Standard Training</cell><cell cols="3">94.66 34.74 12.20</cell><cell>6.70</cell></row><row><cell cols="5">Gaussian Smoothing 95.09 40.49 31.13</cell><cell>9.88</cell></row><row><cell cols="2">MIXUP</cell><cell cols="3">95.77 46.87 19.88</cell><cell>14.35</cell></row><row><cell cols="2">AT (𝜖=1/255)</cell><cell cols="3">93.74 79.04 12.98</cell><cell>8.15</cell></row><row><cell cols="2">AT (𝜖=2/255)</cell><cell cols="3">92.37 91.84 23.25</cell><cell>15.79</cell></row><row><cell></cell><cell cols="4">(a) Evaluation results on CIFAR10.</cell></row><row><cell>Defender</cell><cell cols="2">Attacker Clean</cell><cell>EM</cell><cell>TAP DICE (ours)</cell></row><row><cell cols="2">Standard Training</cell><cell cols="3">78.25 15.99 32.75</cell><cell>24.57</cell></row><row><cell cols="5">Gaussian Smoothing 77.05 18.05 30.90</cell><cell>24.30</cell></row><row><cell cols="2">MIXUP</cell><cell cols="3">78.99 34.43 35.28</cell><cell>34.03</cell></row><row><cell cols="2">AT (𝜖=1/255)</cell><cell cols="3">70.12 66.95 53.71</cell><cell>52.11</cell></row><row><cell cols="2">AT (𝜖=2/255)</cell><cell cols="3">67.54 65.64 67.38</cell><cell>65.24</cell></row><row><cell></cell><cell cols="4">(b) Evaluation results on CIFAR100.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Test accuracy (%) of DICE vs. TAP with different data protection proportions on CIFAR10.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Data Protection Proportion</cell><cell></cell></row><row><cell>Model</cell><cell>20%</cell><cell>40%</cell><cell>60%</cell><cell>80%</cell></row><row><cell></cell><cell cols="4">mixed clean mixed clean mixed clean mixed clean</cell></row><row><cell cols="2">TAP DICE 85.95 90.47 94.81</cell><cell>86.52 93.90 74.16</cell><cell>78.80 92.12 65.90</cell><cell>56.37 86.60 56.25</cell></row><row><cell cols="5">Mixup [58], which mixes inputs and outputs during training. More-</cell></row><row><cell cols="5">over, we test adversarial training, a powerful defense against recent</cell></row><row><cell cols="5">delusive attacks [10, 18, 44], see training details in Appendix 7.2.2.</cell></row><row><cell cols="5">Baseline CIFAR10 &amp; CIFAR100 Results. Table</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Test accuracy (%) on CIFAR10 w/ different backbones as attack target in DICE and TAP-based transfer attacks.</figDesc><table><row><cell>SURROGATE</cell><cell cols="5">TARGET VGG19 GoogleNet DN121 MOB-V2</cell></row><row><cell>TAP</cell><cell></cell><cell>11.78</cell><cell>10.59</cell><cell>9.89</cell><cell>7.27</cell></row><row><cell cols="2">DICE (ours)</cell><cell>9.17</cell><cell>6.42</cell><cell>8.72</cell><cell>7.19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Test accuracy (%) on CIFAR10 of different target models in transfer-based FGSM and PGD-100 black box attacks, which are generated based on WRN (baseline) and DICE. We also report the results of white-box attacks directly on target models.</figDesc><table><row><cell>SURROGATE</cell><cell cols="5">TARGET VGG16 RN18 RN50 DN121</cell></row><row><cell>WRN</cell><cell></cell><cell>67.18</cell><cell cols="2">61.66 64.35</cell><cell>74.54</cell></row><row><cell cols="2">DICE (ours)</cell><cell cols="3">43.91 34.97 39.73</cell><cell>51.42</cell></row><row><cell cols="2">White-box</cell><cell>52.69</cell><cell cols="3">36.84 33.68 38.67</cell></row><row><cell cols="5">(a) Evaluation results under FGSM attack.</cell></row><row><cell>SURROGATE</cell><cell cols="5">TARGET VGG16 RN18 RN50 DN121</cell></row><row><cell>WRN</cell><cell></cell><cell>69.23</cell><cell cols="2">48.94 56.62</cell><cell>80.49</cell></row><row><cell cols="2">DICE (ours)</cell><cell>9.95</cell><cell>2.8</cell><cell>5.68</cell><cell>24.27</cell></row><row><cell>White-box</cell><cell></cell><cell>0.01</cell><cell>2.40</cell><cell>1.78</cell><cell>0.22</cell></row></table><note>(b) Evaluation results under PGD-100 attacks.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Test accuracy (%) of DICE-M with different regularization hyperparameter 𝛽 values on CIFAR10 Under WRN.</figDesc><table><row><cell cols="3">𝛽 Clean PGD-20 C&amp;W-20</cell></row><row><cell>0 82.88</cell><cell>46.59</cell><cell>49.19</cell></row><row><cell>1 82.81</cell><cell>47.04</cell><cell>50.97</cell></row><row><cell>2 81.98</cell><cell>47.73</cell><cell>50.36</cell></row><row><cell>3 82.26</cell><cell>48.51</cell><cell>51.24</cell></row><row><cell>4 83.18</cell><cell>49.28</cell><cell>51.96</cell></row><row><cell>5 82.67</cell><cell>48.54</cell><cell>52.52</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Test Accuracy (%) of our DICE-M and DICE-T vs. Baselines on CIFAR100 Under WRN.</figDesc><table><row><cell>Defender</cell><cell cols="6">Attacker Clean FGSM PGD-100 C&amp;W-100 AA.</cell></row><row><cell cols="2">PGD-AT</cell><cell cols="2">66.26 35.31</cell><cell>22.95</cell><cell>24.39</cell><cell>22.01</cell></row><row><cell cols="2">TRADES</cell><cell cols="2">58.64 37.12</cell><cell>26.63</cell><cell>24.48</cell><cell>23.2</cell></row><row><cell cols="2">DICE-M</cell><cell>67.15</cell><cell>37.4</cell><cell>24.35</cell><cell>26.99</cell><cell>23.61</cell></row><row><cell cols="2">DICE-T</cell><cell cols="2">59.84 39.88</cell><cell>28.42</cell><cell>26.37</cell><cell>25.73</cell></row><row><cell cols="7">5.3 Improving Robustness over Adversarial</cell></row><row><cell cols="2">Training</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Evaluation metrics and training details. For robustness evalua-</cell></row><row><cell cols="7">tion, we craft adversarial examples under 𝑙-∞ norm with 𝜖 = 8/255</cell></row><row><cell cols="3">by FGSM, PGD, C&amp;W</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Test accuracy (%) of DICE vs. TAP under AT with different perturbation radii 𝜖.</figDesc><table><row><cell>Defender</cell><cell cols="3">Attacker Clean TAP DICE(ours)</cell></row><row><cell cols="2">AT(𝜖 = 3/255)</cell><cell>89.23 85.34</cell><cell>78.56</cell></row><row><cell cols="2">AT(𝜖 = 4/255)</cell><cell>88.26 87.61</cell><cell>87.50</cell></row><row><cell cols="3">(a) Evaluation results on CIFAR10.</cell></row><row><cell>Defender</cell><cell cols="3">Attacker Clean TAP DICE(ours)</cell></row><row><cell cols="2">AT(𝜖 = 3/255)</cell><cell>66.47 64.04</cell><cell>62.52</cell></row><row><cell cols="2">AT(𝜖 = 4/255)</cell><cell>64.33 62.20</cell><cell>61.27</cell></row><row><cell cols="4">(b) Evaluation results on CIFAR100.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Test accuracy (%) of DICE with different regularization hyper-parameter 𝛾 on CIFAR10. FGSM denotes adversarial training under FGSM attack.</figDesc><table><row><cell>Defender</cell><cell>𝛾</cell><cell>0.1</cell><cell>0.5</cell><cell>1.0</cell></row><row><cell>ST</cell><cell></cell><cell cols="2">10.67 9.87</cell><cell>7.58</cell></row><row><cell>FGSM</cell><cell></cell><cell cols="3">21.00 19.35 19.07</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Test accuracy (%) of DICE with different PGD attack steps on CIFAR10. attack to perform delusive attack. In Table8, we compare the effect of different 𝛾 values and find that larger 𝑔𝑎𝑚𝑚𝑎 leads to more effective delusive attack.7.2.5 Sensitivity analysis of different attack steps.Another important hyper-parameter of delusive attacks is the attack steps. Table</figDesc><table><row><cell>Defender</cell><cell>Attack steps</cell><cell>50</cell><cell>100</cell><cell>200</cell></row><row><cell>ST</cell><cell></cell><cell cols="3">13.21 10.21 6.70</cell></row><row><cell cols="2">AT(𝜖=1/255)</cell><cell cols="3">34.55 17.66 8.15</cell></row><row><cell cols="2">AT(𝜖=2/255)</cell><cell cols="3">79.19 43.36 15.79</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Implementation available at https://github.com/Thinklab-SJTU/DICE.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">Note that we consider the scenario where 𝑋 and 𝑌 are generated concurrently, i.e., there is neither directed path from 𝑋 to 𝑌 nor 𝑌 to 𝑋 .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">https://github.com/kuangliu/pytorch-cifar</note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was partly supported by National Key Research and Development Program of China (2020AAA0107600), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), and NSFC (61972250, 72061127003).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<title level="m">Invariant risk minimization</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Invariance, causality and robustness</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statist. Sci</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unlabeled data improves adversarial robustness</title>
		<author>
			<persName><forename type="first">Yair</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Invariant rationalization</title>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Robust overfitting may be mitigated by properly learned smoothening</title>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to confuse: generating training time adversarial data with auto-encoder</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi-Zhi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adversarial examples make strong poisons</title>
		<author>
			<persName><forename type="first">Liam</forename><surname>Fowl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micah</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping-Yeh</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Czaja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Witches&apos; brew: Industrial scale data poisoning via gradient matching</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Fowl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronny</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Czaja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Moeller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Causal inference in statistics: A primer</title>
		<author>
			<persName><forename type="first">Madelyn</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">P</forename><surname>Jewell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>arxiv:1412.6572</idno>
		<title level="m">Explaining and Harnessing Adversarial Examples</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A theory of causal learning in children: causal maps and Bayes nets</title>
		<author>
			<persName><forename type="first">Alison</forename><surname>Gopnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Sobel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><forename type="middle">E</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamar</forename><surname>Kushnir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Danks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Andrew G Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unlearnable examples: Making personal data unexploitable</title>
		<author>
			<persName><forename type="first">Hanxun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">Monazam</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Metapoison: Practical general-purpose clean-label data poisoning</title>
		<author>
			<persName><forename type="first">Ronny</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Fowl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Variational autoencoders and nonlinear ica: A unifying framework</title>
		<author>
			<persName><forename type="first">Ilyes</forename><surname>Khemakhem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvarinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Beomsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junghoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taegyun</forename><surname>Jeon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11626</idno>
		<title level="m">Bridging adversarial robustness and gradient interpretability</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Out-ofdistribution generalization via risk extrapolation (rex)</title>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joern-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Binas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Le Priol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In ICML</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning causal semantic representation for out-ofdistribution prediction</title>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyue</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Representation learning via invariant causal mechanisms</title>
		<author>
			<persName><forename type="first">Jovana</forename><surname>Mitrovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.07922</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robustness to adversarial perturbations in learning from incomplete data</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin-Ichi</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving adversarial robustness via promoting ensemble diversity</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Bag of tricks for adversarial training</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00467</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Causality</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Interpretation and identification of causal mediation</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological methods</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">459</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Causal inference by using invariant prediction: identification and confidence intervals</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bühlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolai</forename><surname>Meinshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Elements of causal inference: foundations and learning algorithms</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to learn single domain generalization</title>
		<author>
			<persName><forename type="first">Fengchun</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Appearance and Structure Aware Robust Deep Visual Graph Matching: Attack, Defense and Beyond</title>
		<author>
			<persName><forename type="first">Qibing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingquan</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runzhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Overfitting in adversarially robust deep learning</title>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Adversarially robust generalization requires more data</title>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On causal and anticausal learning</title>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eleni</forename><surname>Sgouritsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joris</forename><surname>Mooij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Is Robustness the Cost of Accuracy?-A Comprehensive Study on the Robustness of 18 Deep Image Classification Models</title>
		<author>
			<persName><forename type="first">Dong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongge</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Recovering Latent Causal Factor for Generalization to Distributional Shifts</title>
		<author>
			<persName><forename type="first">Xinwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Botong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno>arxiv:1312.6199</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Better safe than sorry: Preventing delusive adversaries with adversarial training</title>
		<author>
			<persName><forename type="first">Lue</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng-Jun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songcan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">On adaptive attacks to adversarial example defenses</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07204</idno>
		<title level="m">Ensemble adversarial training: Attacks and defenses</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12152</idno>
		<title level="m">Robustness may be at odds with accuracy</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">High-frequency component helps explain the generalization of convolutional neural networks</title>
		<author>
			<persName><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xindi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Causal attention for unbiased visual recognition</title>
		<author>
			<persName><forename type="first">Tan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Improving adversarial robustness requires revisiting misclassified examples</title>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Difan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Handling Distribution Shifts on Graphs: An Invariance Perspective</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Causal attention for vision-language tasks</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guojun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A closer look at accuracy vs. robustness</title>
		<author>
			<persName><surname>Yao-Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyrus</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamalika</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">A causal view on robustness of neural networks</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingzhen</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Causal intervention for weakly-supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Xian-Sheng Hua, and Qianru Sun. In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><surname>Lopez-Paz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Theoretically principled trade-off between robustness and accuracy</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaodong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiantao</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Geometry-aware instance-reweighted adversarial training</title>
		<author>
			<persName><forename type="first">Jingfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohan</forename><surname>Kankanhalli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">Yonggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.06196</idno>
		<title level="m">Adversarial robustness through the lens of causality</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
