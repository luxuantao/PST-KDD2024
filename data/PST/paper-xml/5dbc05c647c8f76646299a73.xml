<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Multi-organ Nucleus Segmentation Challenge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Neeraj</forename><surname>Kumar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ruchika</forename><surname>Verma</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Deepak</forename><surname>Anand</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yanning</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName><roleName>Efstratios</roleName><forename type="first">Omer</forename><forename type="middle">Fahri</forename><surname>Onder</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pheng- Ann</forename><surname>Heng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiahui</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhiqiang</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName><roleName>Navid</roleName><forename type="first">Yunzhi</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alemi</forename><surname>Koohbanani</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mostafa</forename><surname>Jahanifar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Neda</forename><surname>Zamani</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ali</forename><surname>Gooya</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nasir</forename><surname>Rajpoot</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xuhua</forename><surname>Ren</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sihang</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qian</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Cheng-Kun</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chi- Hung</forename><surname>Weng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wei-Hsiang</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chao-Yuan</forename><surname>Yeh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shuang</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><roleName>Pak</roleName><forename type="first">Shuoyu</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hei</forename><surname>Yeung</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Amirreza</forename><surname>Mah- Bod</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gerald</forename><surname>Schaefer</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Isabella</forename><surname>Ellinger</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rupert</forename><surname>Ecker</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Orjan</forename><surname>Smedby</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chunliang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><roleName>That- Vinh</roleName><forename type="first">Benjamin</forename><surname>Chidester</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Minh-Triet</forename><surname>Ton</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Tran</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Minh</forename><forename type="middle">N</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><surname>Do</surname></persName>
						</author>
						<author>
							<persName><roleName>Quoc</roleName><forename type="first">Dang</forename><surname>Graham</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jin</forename><forename type="middle">Tae</forename><surname>Vu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Akshayku- Mar</forename><surname>Kwak</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Raviteja</forename><surname>Gunda</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Corey</forename><surname>Chunduri</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaoyang</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dariush</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Reza</forename><surname>Lotfi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Antanas</forename><surname>Safdari</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ali- Son</forename><surname>Kascenas</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dennis</forename><surname>O'neil</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Johannes</forename><surname>Eschweiler</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yanping</forename><surname>Stegmaier</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Baocai</forename><surname>Cui</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kailin</forename><surname>Yin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xin- Mei</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Philipp</forename><surname>Tian</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Erhardt</forename><surname>Gruening</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Elad</forename><surname>Barth</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Itay</forename><surname>Arbel</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Amir</forename><surname>Remer</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ekaterina</forename><surname>Ben-Dor</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Matthias</forename><surname>Sirazitdi- Nova</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stefan</forename><surname>Kohl</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuexiang</forename><surname>Braunewell</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xinpeng</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Linlin</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName><roleName>Krishanu</roleName><forename type="first">Das</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mo- Hammad</forename><forename type="middle">Azam</forename><surname>Baksi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jaegul</forename><surname>Khan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Adrián</forename><surname>Choo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Valery</forename><surname>Colomer</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Linmin</forename><surname>Naranjo</surname></persName>
						</author>
						<author>
							<persName><roleName>Khan</roleName><forename type="first">M</forename><surname>Pei</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kaushiki</forename><surname>Iftekharud- Din</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Debotosh</forename><surname>Roy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anibal</forename><surname>Bhattacharjee</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Maria</forename><forename type="middle">Gloria</forename><surname>Pedraza</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sabarinathan</forename><surname>Bueno</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sara- Vanan</forename><surname>Devanathan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Praveen</forename><surname>Radhakrishnan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zihan</forename><surname>Koduganty</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Guanyu</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaojie</forename><surname>Cai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuqin</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Amit</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><surname>Sethi</surname></persName>
						</author>
						<title level="a" type="main">A Multi-organ Nucleus Segmentation Challenge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7061215B87BEEF89AB1E5C9BA6D00D16</idno>
					<idno type="DOI">10.1109/TMI.2019.2947628</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMI.2019.2947628, IEEE Transactions on Medical Imaging</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multi-organ</term>
					<term>nucleus segmentation</term>
					<term>digital pathology</term>
					<term>instance segmentation</term>
					<term>aggregated Jaccard index</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generalized nucleus segmentation techniques can contribute greatly to reducing the time to develop and validate visual biomarkers for new digital pathology datasets. We summarize the results of MoNuSeg 2018 Challenge whose objective was to develop generalizable nuclei segmentation techniques in digital pathology. The challenge was an official satellite event of the MICCAI 2018 conference in which 32 teams with more than 80 participants from geographically diverse institutes participated. Contestants were given a training set with 30 images from seven organs with annotations of 21,623 individual nuclei. A test dataset with 14 images taken from seven organs, including two organs that did not appear in the training set was released without annotations. Entries were evaluated based on average aggregated Jaccard index (AJI) on the test set to prioritize accurate instance segmentation as opposed to mere semantic segmentation. More than half the teams that completed the challenge outperformed a previous baseline <ref type="bibr" target="#b11">[1]</ref>. Among the trends observed that contributed to increased accuracy were the use of color normalization as well as heavy data augmentation. Additionally, fully convolutional networks inspired by variants of U-Net [2], FCN [3], and Mask-RCNN [4] were popularly used, typically based on ResNet [5]  or VGG <ref type="bibr" target="#b16">[6]</ref> base architectures. Watershed segmentation on predicted semantic segmentation maps was a popular post-processing strategy. Several of the top techniques compared favorably to an individual human annotator and can be used with confidence for nuclear morphometrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Examination of H&amp;E stained tissue under a microscope remains the mainstay of pathology. The popularity of H&amp;E is due to its low cost and ability to reveal tissue structure N. Kumar, R. Verma, D. Anand and A. Sethi co-organized the challenge; all others contributed the results of their algorithms. Due to space constraints, funding information and author affiliations for this work appear in the acknowledgement section. Asterisk indicates the corresponding author. Address all correspondence to: neeraj.kumar.iitg@gmail.com, and asethi@iitb.ac.in and nuclear morphology, which is sufficient for primary diagnosis of several diseases including many cancers. Nuclear shapes and spatial arrangements often form the basis of the examination of H&amp;E stained tissue sections. For example, grading of various types of cancer and risk stratification of patients is usually done by examining different types of nuclei on a tissue slide <ref type="bibr" target="#b17">[7]</ref>, <ref type="bibr" target="#b18">[8]</ref>. Nuclear morphometric features and appearance including the color of their surrounding cytoplasm also helps in identifying various types of cells such as epithelial (glandular), stromal, or inflammatory, which in turn give an idea of the glandular structure and disease presentation at low power <ref type="bibr" target="#b17">[7]</ref>- <ref type="bibr" target="#b20">[10]</ref>. Segmentation of nuclei accurately in H&amp;E images therefore has high utility in digital pathology.</p><p>However, nucleus segmentation algorithms that work well on one dataset can perform poorly on a different dataset. There is far too much variation in the appearance of nuclei and their surroundings by organs, disease conditions, and even digital scanner brands or histology technicians. Examples of such variations are shown in Figure <ref type="figure">1</ref>, along with the problems of some common segmentation algorithms such as Otsu thresholding <ref type="bibr" target="#b21">[11]</ref>, marker controlled watershed segmentation <ref type="bibr" target="#b22">[12]</ref>- <ref type="bibr" target="#b24">[14]</ref> or open-source packages like Fiji <ref type="bibr" target="#b25">[15]</ref> and Cell Profiler <ref type="bibr" target="#b26">[16]</ref>. Segmentation based on machine learning should be able to do a better job, but that makes designing and refining nucleus segmentation algorithms for a new study a tedious task because annotations of thousands of nuclei are needed to train such segmentation models on datasets of interest. Algorithms that generalize to new datasets and organs that were not seen during training can reduce this effort substantially and contribute to rapid experimentation with new phenotypical (visual) biomarkers.</p><p>Until recently, one of the major challenges in training generalized nucleus segmentation models has been the unavailability of large multi-organ datasets with annotated nuclei. In 2017 Kumar et al. <ref type="bibr" target="#b11">[1]</ref> released a dataset with more than 21,000 H&amp;E Otsu Watershed Cell profiler Fiji Fig. <ref type="figure">1</ref>: Nucleus segmentation challenges: Original H&amp;E images show crowded and chromatin-sparse nuclei with color variation across tissue slides. Otsu thresholding <ref type="bibr" target="#b21">[11]</ref> and Cell Profiler <ref type="bibr" target="#b26">[16]</ref> gives merged nuclei (under-segmentation). Marker controlled watershed segmentation <ref type="bibr" target="#b22">[12]</ref> and Fiji <ref type="bibr" target="#b25">[15]</ref> produces fragmented nuclei (over-segmentation). Segmented nuclei instances are shown in different colors in rows 2-5.</p><p>hand-annotated nuclei in H&amp;E stained tissue images acquired at the commonly used 40x magnification, sourced from seven organs and multiple hospitals in The Cancer Genome Atlas (TCGA) <ref type="bibr" target="#b27">[17]</ref>. Kumar et al. also introduced a metric called Aggregated Jaccard Index (AJI) that is more appropriate to evaluate algorithms for this instance segmentation problem as opposed to other popular metrics such as Dice coefficient, which are more suited for semantic segmentation problems. This is because nucleus segmentation algorithms should not only tell the difference between nuclear and non-nuclear pixels, but they should also be able to tell pixels belonging to two nuclei apart that touch or overlap with each other. Additionally, they had released a trained model that performed reasonably well on unseen organs from the test subset of images.</p><p>We organized the Multi-organ nucleus segmentation (MoNuSeg) Challenge at MICCAI 2018 to build upon Kumar et al.'s work by enlarging the dataset and by encouraging others to introduce new techniques for generalized nucleus segmentation. The participation was wide and several of participants outperformed the previous benchmark <ref type="bibr" target="#b11">[1]</ref> by a significant margin. In this paper we describe in detail the objectives of the competition, the released dataset, and the emerging trends of techniques that performed well on the challenge task. We hope that the algorithms described on the challenge webpage <ref type="bibr" target="#b28">[18]</ref> will be of use to the computational pathology research community. The rest of the paper is organized as follows. We describe the prior work on nucleus segmentation and dataset creation in Section II. We describe the dataset and competition rules in Section III. We present an organized summary of the techniques used by the challenge participants in Section IV. Finally, we discuss emerging trends in nucleus segmentation techniques in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND AND PRIOR WORK</head><p>In this section we describe the importance of H&amp;E stained images in histopathology and provide details of some previous notable techniques and datasets for nucleus segmentation from H&amp;E stained images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Hematoxylin and eosin (H&amp;E) stained images</head><p>Pathologists usually observe tissue slides under a microscope at a specific resolution (ranging between 5x and 40x) to report their diagnoses including tumor grade, extent of spread, surgical margin, etc. Their assessment is primarily based on the appearance, size, shape, color and crowding of various nuclei (and glands) in epithelium and stroma. Stains are used to enhance the contrast between these tissue components to help a pathologist looking for specific nuclei and gland features. The combination of hematoxylin and eosin (H&amp;E) is a frequentlyused, universal, and inexpensive staining scheme for general contrast enhancement of histologic structures of a tissue. Hematoxylin renders the nuclei dark blueish purple and the epithelium light purple, while eosin renders the stroma pink. Compared to the general use of H&amp;E, immunohistochemical staining is more specialized as it targets proteins specific to certain disease states for visual identification.</p><p>With the advent of high resolution cameras mounted on microscopes, and more importantly, digital whole slide scanners, it is now possible to acquire whole slide images (WSIs) of the tissue sections for computer assisted diagnosis (CAD). However, the development of CAD systems requires automated extraction of rich information encoded in the pixels of WSIs. Recently, computer based assessment of tissue images has been used for tumor molecular sub-type detection <ref type="bibr" target="#b29">[19]</ref>, mortality or recurrence prediction <ref type="bibr" target="#b19">[9]</ref>, <ref type="bibr" target="#b30">[20]</ref>, and treatment effectiveness prediction <ref type="bibr" target="#b20">[10]</ref>. Notably, nucleus detection and segmentation is often a first step for several such CAD systems that rely on nuclear morphometrics for disease state stratification and predictive modelling. Therefore, MoNuSeg 2018 focused on crowdsourcing techniques for nucleus segmentation in H&amp;E stained images captured at 40x resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Nucleus segmentation techniques</head><p>Prior to the advent of deep learning, approaches to segment nuclei relied on watershed segmentation, morphological operations -such as erosion, dilation, opening and closingcolor-based thresholding, and variants of active contours <ref type="bibr" target="#b22">[12]</ref>, <ref type="bibr" target="#b23">[13]</ref>, <ref type="bibr" target="#b31">[21]</ref>- <ref type="bibr" target="#b33">[23]</ref>. These techniques were often complemented with a collection of pre-processing methods, such as contrast enhancement and deblurring to improve the 'image quality'. Additionally, several post-processing techniques, such as hole filling, noise removal, graph-cuts, etc., were also used to refine the outputs of the segmentation algorithms. However, these approaches do not generalize well across a wide spectrum of tissue images due to reasons such as (a) variations in nuclei morphologies of various organs and tissue types, (b) inter-and intra-nuclei color variations in crowded and chromatin-sparse nuclei, and (c) diversity in the quality of tissue images owing to the differences in image acquisition equipment and slide preparation protocols across hospitals and clinics.</p><p>There have been tremendous advances in the recent years to develop learning-based nucleus segmentation methods to advance the state-of-the-art. Instead of relying on pre-determined algorithms for segmentation, machine learning methods derive data driven algorithms that are trained in a supervised manner based on annotations of nuclear and non-nuclear pixels. This allows them to concentrate on relative differences between nuclear and non-nuclear pixels and their surrounding patches and overcome the aforementioned sources of intra-class variations for better generalized segmentation. The use of learning based approaches started with the extraction of hand-crafted local features based on color and spatial filtering that were fed to traditional learning-based models such as random forests, support vector machines, etc. to segment nuclei and non-nuclei regions <ref type="bibr" target="#b34">[24]</ref>- <ref type="bibr" target="#b36">[26]</ref>. The selection of features is dependent on domain knowledge and trial-and-error for improving nucleus segmentation performance, and yet it is difficult to detect all nuclei with diverse appearances and crowding patterns.</p><p>To circumvent the constraints of hand-crafted features, representation learning algorithms, popularly known as deep learning techniques, have recently emerged. These methods -specifically the ones using convolutional neural networks (CNNs) -have outperformed previous techniques in nucleus detection and segmentation tasks by significant margins <ref type="bibr" target="#b11">[1]</ref>, <ref type="bibr" target="#b37">[27]</ref>- <ref type="bibr" target="#b40">[30]</ref>. To use deep learning, the problem is often cast as one of semantic segmentation wherein a two-class probability map for nuclear and non-nuclear regions is usually computed. After semantic segmentation, sophisticated post-processing methods -such as graph partitioning <ref type="bibr" target="#b37">[27]</ref>, or the computation of distance transform of the nuclear map followed by Hminima transform and region growing <ref type="bibr" target="#b38">[28]</ref> -are often used to obtain final nuclei shapes with the desired separation of touching and overlapping nuclei. Semantic segmentation of third class of pixels -those on the nuclear boundaries including that between two touching nuclei -has also been proposed to exclusively refine the separation between the segmented touching and overlapping nuclei <ref type="bibr" target="#b11">[1]</ref>. Deep generative models have also been used for accurate nuclei segmentation <ref type="bibr" target="#b41">[31]</ref>. More recently, nucleus segmentation problem has been formulated as a regression task to predict a distance map with respect to centroids or boundaries of nuclei using fully convolutional networks (FCNs) to achieve both segmentation and computational performance gains over previous deep learning based approaches <ref type="bibr" target="#b40">[30]</ref>. More comprehensive reviews of state-of-theart nucleus segmentation algorithms can be found in <ref type="bibr" target="#b42">[32]</ref> and <ref type="bibr" target="#b43">[33]</ref>.</p><p>One of the major barriers in out of the box (without retraining) application of state-of-the art deep learning based nucleus segmentation algorithms was the lack of publicly available source codes and trained models by previously published techniques until Kumar et al. <ref type="bibr" target="#b11">[1]</ref> and Naylor et al. <ref type="bibr" target="#b40">[30]</ref> released their source codes. The other major barrier was the lack of publicly available annotated datasets for benchmarking, which we address next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Nucleus segmentation datasets</head><p>The success of machine learning and the development of state-of-the art deep learning algorithms in computer vision can be attributed to the healthy competition enabled by publicly available consumer photography datasets such as ImageNet <ref type="bibr" target="#b44">[34]</ref> and COCO <ref type="bibr" target="#b45">[35]</ref> for object recognition in images. Unfortunately, we do not see similar progress in digital pathology image analysis as there is dearth of labeled and annotated datasets for solving various tasks of pathologist's interest. For example, CAMELYON dataset <ref type="bibr" target="#b46">[36]</ref>, which is one of the largest histopathology classification dataset, has 1,399 images, while ImageNet <ref type="bibr" target="#b44">[34]</ref> has 14 million images. Similarly, CheXpert <ref type="bibr" target="#b47">[37]</ref>, which is one of the largest medical image segmentation datasets has only 224, 316 images. This is because labeling and annotating pathology images require expert knowledge and diligent work. However, there have been a few recent efforts dedicated to the release of hand-annotated H&amp;E stained tissue slide images for nucleus segmentation as summarized in Table <ref type="table" target="#tab_0">I</ref>. These datasets can also be downloaded from the challenge webpage <ref type="bibr" target="#b28">[18]</ref>. Please note that we have not included datasets where the nuclei were annotated for detection alone in Table I because these cannot be used for the segmentation task. We also excluded datasets annotated for other specific objectives such as gland segmentation, mitosis detection, epithelial segmentation, and tumor type classification, as opposed to generalized nucleus segmentation. Most of the datasets listed in Table <ref type="table" target="#tab_0">I</ref> focus on a specific organ with the exception of Kumar et al. <ref type="bibr" target="#b11">[1]</ref> and Wienert et al. <ref type="bibr" target="#b33">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DATASET AND COMPETITION RULES</head><p>The objective of MoNuSeg 2018 was to encourage the development of learning based generalized nucleus segmentation techniques that work right out of the box (without re-training) on a diverse set of H&amp;E-stained tissue images. The images therefore spanned a range of patients, organs, disease states, and sourcing hospitals with potentially different slide preparation and image acquisition methods. Training and testing datasets were carefully curated and the competition rules were crafted in accordance with these objectives.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training dataset</head><p>The training data of MoNuSeg 2018 was the same as that released previously by Kumar et al. <ref type="bibr" target="#b11">[1]</ref>, which comprised 30 tissues images, each of size 1000 × 1000, containing 21, 623 hand-annotated nuclear boundaries. Each 1000 × 1000 image in this dataset was extracted from a separate whole slide image (WSI) (scanned at 40x) of an individual patient downloaded from TCGA <ref type="bibr" target="#b27">[17]</ref>. The dataset represented 7 different organs viz., breast, liver, kidney, prostate, bladder, colon and stomach, and included both benign and diseased tissue samples to ensure diversity of nuclear appearances. Furthermore, the training images came from 18 different hospitals, which introduced another source of appearance variation due to the differences in the staining practices and image acquisition equipments (scanners) across labs. Representative 1000×1000 sub-images from regions dense in nuclei were extracted from patient WSIs to reduce the computational burden of processing WSIs and increase participation. Only one crop per WSI and patient was included in the dataset to ensure diversity. The distribution of training images across organs is shown in Table <ref type="table" target="#tab_1">II</ref> while patient and hospital details are available on the challenge webpage <ref type="bibr" target="#b28">[18]</ref>.</p><p>Both epithelial and stromal nuclei were manually annotated in the 1000 × 1000 sub-images using Aperio ImageScope R . Annotations were performed on a 25" monitor with a 200x digital magnification such that each image pixel occupied 5 × 5 screen pixels to ensure clear visibility for annotating nuclear boundaries with a laser mouse. For overlapping nuclei, each multi-nuclear pixel was assigned to the nucleus that appeared to be on top in the 3-D structure. The annotators were engineering students and the quality control was performed by an expert pathologist with years of experience in analyzing tissue sections. Specifically, each H&amp;E image was included in a PowerPoint R (Microsoft, Redmond WA, USA) slide at 300 dots per inch, along with the annotated boundaries overlaid in bright green. The slides were examined by a pathologist on 25" monitor to point out missed nuclei, false nuclei, and nuclei with wrong boundaries. For each image, the numbers of each type of error was summed up and divided by the number of annotated nuclei to assess the quality of annotations. As shown in Supplementary Table <ref type="table">S2</ref>, the error rate for each organ was smaller than 1%. The images and XML files containing pixel coordinates of the annotated nuclear boundaries were released for public use by <ref type="bibr" target="#b11">[1]</ref>. The reasons that make this dataset ideal for training a generalized nucleus segmentation model are as follows:</p><p>1 </p><formula xml:id="formula_0">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Testing dataset</head><p>A new testing set comprising 14 images, each of size 1000 × 1000 pixels, spanning 7 organs (viz. kidney, lung, colon, breast, bladder, prostate, brain), several disease states (benign and tumors at different stages), and approximately 7,223 annotated nuclei was prepared in the same manner as used for preparing the training data. As shown in Table <ref type="table" target="#tab_1">II</ref>, lung and brain tissue images were exclusive to the test set which made it more challenging. More details about the test set are available in the "supplementary material" tab of the challenge webpage <ref type="bibr" target="#b28">[18]</ref>. The annotations of the test set were not released to the participants. To formally conclude the challenge, with this paper, we are releasing the test annotations on the challenge webpage <ref type="bibr" target="#b28">[18]</ref> to facilitate future research in the development of generalized nucleus segmentation algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Competition metric and Results</head><p>Competitors were evaluated only once on the test set. Their latest submission before the deadline was considered as the final submission for evaluation. Average aggregated Jaccard Index (AJI) was used as the metric to evaluate nucleus segmentation performance of the competing algorithms because of its established advantages over other segmentation metrics <ref type="bibr" target="#b11">[1]</ref>, <ref type="bibr" target="#b40">[30]</ref>. The value of AJI ranges between 0 to 1 (higher is better). Computing AJI involves matching every ground truth nuclei to one detected nuclei by maximizing the Jaccard index. The AJI is then equal to the ratio of the sums of the cardinals of intersection and union of these matched ground truth and predicted nuclei. Additionally, all detected components that are not matched are added to the denominator. We reproduce Algorithm 1 detailing AJI computation from <ref type="bibr" target="#b11">[1]</ref> with permission. The code for computing AJI is available on the challenge webpage <ref type="bibr" target="#b28">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Aggregated Jaccard index (AJI)</head><p>Input: A set of images with a combined set of annotated nuclei Gi indexed by i, and a segmented set of nuclei S k indexed by k. Output: Aggregated Jaccard Index A.</p><p>1: Initialize overall correct and union pixel counts: C ← 0; U ← 0 2: for Each ground truth nucleus Gi do 3:</p><formula xml:id="formula_1">j ← arg max k (|Gi ∩ S k |/|Gi ∪ S k |) 4:</formula><p>Update pixel counts: C ← C +|Gi ∩Sj|; U ← U +|Gi ∪Sj| 5:</p><p>Mark Sj used 6: end for 7: for Each segmented nucleus Sj do 8:</p><formula xml:id="formula_2">If S k is not used then U ← U + |S k | 9: end for 10: A ← C/U</formula><p>Participants were asked to submit 14 segmentation output files (one for each of the 14 test images) to the challenge organizers. For each participant's submission, the organizers then computed 14 AJIs (one for each test image) as per Algorithm 1. If a participant did not submit the results for a particular testing image then AJI value of zero was assigned for that particular image to that participant. The organizers then computed the average AJI (a-AJI) for each participant by averaging image level AJIs across 14 test images. The participants were then ranked in the descending order of a-AJI to obtain the final leaderboard shown in Table <ref type="table" target="#tab_1">III</ref>.</p><p>Table III also includes the 95% confidence intervals (CIs) around each participant's a-AJI. It is evident that the confidence intervals of the top five techniques exclude a-AJI of the lower ranked techniques. To further assess the overall a-AJI based ranking scheme, we also computed organ level a-AJI (and confidence intervals), for each participant, by averaging image level AJIs across the number of images that belonged to a specific organ, as shown in Supplementary Table <ref type="table">S3</ref>. From Supplementary Table <ref type="table">S3</ref>, it is evident that (a) the top five techniques perform better than other techniques for each organ as well, (b) the organ is a larger contributor to the variability in performance among the top five techniques than the technique itself, and (c) techniques with a higher overall a-AJI perform better for more organs even among the top five techniques. Specifically, for instance, (a) no technique that is not among the top-five overall breaks into the top-five for more than two organs, (b) breast cancer images had AJIs that were lower by about 0.063 to 0.085 compared to those for bladder for the top-five techniques, and (c) the overall top-ranked technique is also the top-ranked one for all but one organ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SUMMARY OF SEGMENTATION TECHNIQUES</head><p>In this section we present a summary of the techniques used by 32 teams who successfully completed the challenge. We describe the trends observed in pre-processing, data augmentation, modeling, task specification, optimization, and postprocessing techniques used by the teams. Specific details of all algorithms are provided in the respective manuscripts submitted by participants as per challenge policies and are available at challenge webpage <ref type="bibr" target="#b28">[18]</ref> under "manuscripts" tab.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pre-processing and data augmentation</head><p>Pre-processing techniques reduce unwanted variations among input images -from both the training and testing sets -so that the test data distribution is not very different from the training data distribution, by projecting both to the same low-dimensional manifold. On the other hand, data augmentation techniques increase the training data set size by introducing controlled random variations with the hope of creating a training data distribution that covers most of the test data distribution. There are several ways in which the participants altered the given images and their ground truth masks before passing them to the segmentation learning systems in order to increase test accuracy. We summarize some of the interesting trends observed in this challenge. These results are also summarized in Table <ref type="table" target="#tab_1">III</ref>.</p><p>1) Color and intensity normalization: Among the data pre-processing techniques, color and intensity transformations were the most common. Approximately half the teams used color normalization techniques that were specifically developed for pathology images to reduce unwanted color variations between training and testing data. Structure Preserving Color Normalization (SPCN) by Vahadane et al. <ref type="bibr" target="#b51">[41]</ref> was used by ten teams due to its demonstrated performance and code availability. Another seven teams used Mecenko et al.'s color normalization scheme <ref type="bibr" target="#b52">[42]</ref>, out of which one used this technique in combination with another technique by Reinhard et al. <ref type="bibr" target="#b53">[43]</ref>.</p><p>Pixel intensity and RGB color transformations that are unspecific to pathology were also used by approximately half of the teams. Most popular among this class of techniques were channel-wise mean subtraction, variance normalization (unit variance), and pixel-value range standardization. Six teams also used either contrast enhancement (or histogram equalization), among which CLAHE <ref type="bibr" target="#b54">[44]</ref> was the most commonly used technique.</p><p>Among the unique techniques, one team used image sharpening to remove unwanted variations between training and testing data, one team concatenated HSV and L channels (of L, a*, b* color space) to the RGB channels, and one team used only the blue channel after color normalization of the RGB images.</p><p>2) Data augmentation: Among data augmentation techniques, geometric transformations of the image grid were the most common. For example, rigid transformations of the images -such as rotation (especially, by multiples of 90 degrees) and/or flipping -were used by all but four teams to increase the size of the training data. However, as can be seen in Table <ref type="table" target="#tab_1">III</ref>, all of the top twelve teams by a-AJI also augmented the training set using affine transformations, while only five teams below that used this type of augmentation. Another transformation used by the participants was elastic deformation, but it was not very popular among the contestants due to the marginal gain it might afford over an affine transform, while being more complicated to implement. Another geometric transformation is image scaling, which was used by nine contestants.</p><p>Another popular set of augmentation techniques involve changing the pixel values while leaving the geometric structure intact. The most popular among these techniques was the addition of white Gaussian noise, which was used by several of the top performing teams. Another popular technique is color jitter or random HSV shifts, which was used by nine of the top twelve teams. Color jitter is opposite in spirit to color normalization in that it is used to present more color variations of the same input geometric structure to the learning machine with the hope that it will learn to focus on the geometric structure as opposed to the color of nuclei, which may vary between training and testing data sets. Random intensity (brightness) shifts were used by fewer participants, as were blurring by isotropic Gaussian filters of random widths and random image sharpening.</p><p>One interesting data augmentation technique that was used by team CMU-UIUC involved extracting the nuclei, augment them in-place, filling the holes in the background, and then pasting the nuclei back on to the reconstructed background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Specification of the learning task</head><p>The challenge of nucleus segmentation can be split into two tasks: distinguishing between nuclear and non-nuclear pixels (semantic segmentation) and separating touching nuclei (instance segmentation). The following were three principal types of outputs that the contestants produced using deep learning to meet these two challenges:</p><p>1) Binary class probability maps distinguish between pixels that belong to the core of any nucleus versus those that do not. The process of not including the outer periphery of the nuclei into the foreground class helps separate touching nuclei. The lost nuclear territory can later be gained back during post-processing. 2) Ternary class probability map distinguishes between nuclear core, non-nuclear, and nuclear boundary pixels. Nuclear pixels that are on a shared boundary of two touching nuclei are considered to belong to the third class, which has been shown to be useful in separating touching nuclei <ref type="bibr" target="#b11">[1]</ref>. 3) Distance map estimates how far a nuclear pixel is from the centroid of a nucleus. Such a map can also distinguish between nuclear and non-nuclear pixels by assigning a fixed value to the latter, such as 0. This is a per-pixel regression problem while the previous two are classification problems. A variant of this distance map is to predict the distance from the boundary of the nucleus. Most teams trained their models to predict variants of one or more of the three types of maps described above. One interesting departure from these three tasks was by Canon Medical Research Europe who predicted a five-class probability map -one for nuclear pixels, and the other four for their probability of belonging to one of the four Cartesian quadrants of a nucleus in order to separate touching nuclei.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Model architectures</head><p>All participants used deep convolutional neural networks. Twenty one teams used variants of U-Net <ref type="bibr" target="#b12">[2]</ref>, of which the original U-Net architecture was used by 11 teams while six teams used base architectures inspired by VGGNet <ref type="bibr" target="#b16">[6]</ref>, and another 11 teams used architectures inspired by either MRCNN <ref type="bibr" target="#b14">[4]</ref>, FCN <ref type="bibr" target="#b13">[3]</ref>, DenseNet <ref type="bibr" target="#b55">[45]</ref>, or ResNet <ref type="bibr" target="#b15">[5]</ref> with different depths. Eight teams used Mask Region with CNN features (MRCNN) <ref type="bibr" target="#b14">[4]</ref> as the primary models (of which, two also used U-Net), and two used FCN <ref type="bibr" target="#b13">[3]</ref> (of which one also used U-Net). Among the remaining, four teams used their own custom models and architectures, and one each used VGGNet <ref type="bibr" target="#b16">[6]</ref>, Deep Layer Aggregation <ref type="bibr" target="#b56">[46]</ref>, PANet <ref type="bibr" target="#b57">[47]</ref>, and TernausNet <ref type="bibr" target="#b58">[48]</ref>. A few teams used multiple architectures for ensembling. Two teams used two architectures each for two different tasks, for example one for semantic segmentation (binary classification between foreground and background pixels) and another for distance map prediction to separate touching nuclei. Notable innovations in model architectures tried by some of the top teams are described in Section IV-G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Model optimization</head><p>The choice of loss function depends on the desired output being predicted. Among various choices for the loss function, pixel-wise cross entropy was used by 28 teams for predicting binary or ternary probability maps, and it was by far the most popular loss function. Ten teams used Dice loss <ref type="bibr" target="#b59">[49]</ref>, and two teams used its variant such as smooth Jaccard index loss or IOU (intersection over union) loss <ref type="bibr" target="#b60">[50]</ref>. For regression problems, seven teams used a smooth L 1 loss. Five teams used mean square error. In total, 16 teams used more than one loss function. Most teams trained their models end-to-end, except when an ensemble of more than one model was used, with the exception of team Yunzhi that used a cascade of two neural networks trained one after another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Post-processing</head><p>For post-processing, watershed segmentation (WS) was used by 17 teams. The most popular way to apply WS was on the nuclear probability pixel map. Additionally, to separate touching nuclei several teams used a neural network to predict the location of a marker for each nucleus, such as by using a nuclear-core probability map, a distance map, or a vector map pointing to the nearest nuclear center. Cleaning up small or weakly detected nuclei was also a common theme. Nonmaxima suppression and h-minima were commonly used along with a threshold to clean up false positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Training and testing time</head><p>Training times ranged from 2 hours and 17 minutes on using a single Nvidia 1080Ti GPU for team Junma to 42 hours for team Johannes Stegmaier on a similar hardware. Testing times also had a wide range from 1 second per 1000 × 1000 image for team Unblockabulls on an Amazon Web Services GPU instance powered by an Nvidia K80 GPU to 2 minutes 58 seconds per image on an Nvidia Titan X GPU by team CVBLab.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Description of the top-five techniques</head><p>We now describe the top-five techniques in more detail as examples of the innovations and diligence with which the participants tried to get robust generalization. Comparative results of the top-five techniques are shown in Figure <ref type="figure" target="#fig_0">2</ref>. Specific details about parameter settings of each algorithm can be found in their respective manuscripts available on the challenge webpage <ref type="bibr" target="#b28">[18]</ref> under the "manuscripts" tab.</p><p>1) CUHK &amp; IMSIGHT: Extensive data augmentation based on random affine transform, rotation, and color jitter was used. Nuclei segmentation task was split into that of nucleus and boundary segmentation. A contour information aggregation network (CIA-Net), inspired by FCN <ref type="bibr" target="#b13">[3]</ref> and U-net <ref type="bibr" target="#b12">[2]</ref>, to simultaneously segment nuclei and boundary was developed using Resnet50 <ref type="bibr" target="#b15">[5]</ref> as the backbone architecture. The binary cross-entropy loss function that combined nucleus and boundary annotation errors was used to train the network. This algorithm missed some of the smaller nuclei and oversegmented (incorrectly splitting a large nuclei into multiple smaller nuclei) some larger nuclei as shown in Figure <ref type="figure" target="#fig_0">2</ref>.</p><p>2) BUPT.J.LI: Images were color normalized and training data was augmented using random cropping, flipping, rotation, scaling, and noise addition. Deep layer aggregation <ref type="bibr" target="#b56">[46]</ref> architecture was used to perform three tasks -(1) detect insidenuclei pixels, (2) estimate the geometric center of the insidenuclei pixels and (3) estimate a center vector that pointed towards the estimated nuclei center for each inside-nuclei pixel. During inference, the detected nuclei centers and center vectors were used to assign inside-nuclei pixels to one of the overlapping or touching nuclei instances. Since, nuclei boundary information was not explicitly used by the network, this technique produced overly smooth nuclei boundaries (Figure <ref type="figure" target="#fig_0">2</ref>), especially for nuclei with high curvature boundaries.</p><p>3) pku.hzq: Extensive data augmentation was used such as flips, rotations, scaling, and noise addition. Then a U-Net <ref type="bibr" target="#b12">[2]</ref> was used to predict a ternary class map similar to Kumar et al. <ref type="bibr" target="#b11">[1]</ref>. Additionally, an MRCNN <ref type="bibr" target="#b14">[4]</ref> was used for top-down instance segmentation. Predictions from the two models were combined as an ensemble for both boundary and nucleus prediction. Then the ensembled nuclei center masks were calculated using morphological eroding of the predicted nuclei pixels. A random walker was used to obtain instance segmentation masks from the ensembled semantic masks and center masks. From Figure <ref type="figure" target="#fig_0">2</ref>, it is evident the boundaries for touching and overlapping nuclei were sometimes unnatural (and occasionally merged) due to pixel-level (semantic) ensembling of the boundary class predictions.</p><p>4) Yunzhi: For data preparation contrast-limited adaptive equalization (CLAHE) <ref type="bibr" target="#b54">[44]</ref> was used. Data augmentation was done using mirror flipping, rotations that were multiples of 90 degrees, color jitter, Gaussian noise addition, and elastic deformation. For each pixel, the probability of it belonging to a nucleus, or a nucleus boundary and unit vector to the center of the nuclei was computed using two cascaded U-nets <ref type="bibr" target="#b12">[2]</ref>. First U-net predicted the inside nuclei pixels and unit vector to the center of the nuclei, which were then used in the subsequent U-net to accurately predict nuclei boundaries. Delineation of touching and overlapping nuclei using this technique heavily relied on accurate estimation of the unit vector that pointed towards the center of a nuclei and due to inaccuracy in precisely estimating the unit vector, this technique produced some over-segmentation and undersegmentation (incorrectly merging two touching or overlapping nuclei) errors (see Figure <ref type="figure" target="#fig_0">2</ref>). 5) Navid Alemi: A neural network predicted both foreground (nuclear core) and background (nuclear boundary) markers. The neural network was a multi-scale feature-sharing network that used extensive skip connections, and was dubbed SpaghettiNet. For training the marker head prediction, the network used a combination of weighted Dice and binary cross entropy loss. For predicting the boundaries, it used smooth Jaccard loss and the boundary map was cleaned up using Frangi vesselness filter <ref type="bibr" target="#b61">[51]</ref>. Finally, marker-controlled watershed segmentation using predicted markers and boundaries was employed to obtain the instance segmentation maps. Figure <ref type="figure" target="#fig_0">2</ref> shows that this technique produced overly smooth boundaries with some over-segmentation and under-segmentation errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Ensemble of top-five techniques</head><p>Unlike ensembling of semantic segmentation, where class probabilities or decisions can be averaged for each pixel location, ensembling of instance segmentation results is far from trivial. Hence, we developed our own approach to generate the ensemble output of instances segmented by the top-five techniques because literature on this topic is thin and unconvincing. First, we looped over instances of the top-ranked technique and identified the corresponding nuclei instances from the other four techniques on the basis of maximum overlapping pixels. Once the matched instances from all techniques were identified, the corresponding ensemble instance was computed through pixel level majority voting as would be done for semantic segmentation of a single nucleus. Once we looped over all nuclei instances predicted by rank 1 technique, to incorporate the instances missed by rank 1 technique, we looped over all those instances of rank 2 technique that did not find an overlap with those of the rank 1 technique. The process was repeated for rank 3 technique, but not for the other two remaining techniques because the extra instances detected by those two would not have a majority vote from the top three techniques. This ensembling method gave an overall a-AJI of 0.693 (95%CI: 0.682-0.703), which is only marginally better than the individual results of top-five teams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Comparison to inter-human agreement</head><p>We re-annotated all 14 test images and computed their a-AJI with the previous annotations. The re-annotation protocol was identical to the one used for creating the training set of MoNuSeg 2018 and the annotator was blinded to the previous test set annotations. The a-AJI between new and old manual annotations across 14 test images was 0.653 (95%CI 0.639-0.667), to which the a-AJI of the top few techniques compares very favorably. This suggests that for nucleus segmentation in H&amp;E images, machine performance is at par with human performance if the image quality is as good as the one used in this challenge.</p><p>V. DISCUSSION AND CONCLUSION Some clear trends emerged from analyzing the top few techniques in Table <ref type="table" target="#tab_1">III</ref>. While based on a prior idea that color normalization can improve performance of segmentation tasks <ref type="bibr" target="#b11">[1]</ref>, <ref type="bibr" target="#b62">[52]</ref>, it is becoming apparent that color augmentation (jitter) trains more robust segmentation models <ref type="bibr" target="#b63">[53]</ref>. Most of the top techniques relied on heavy data augmentation including affine transformations, color jitter and noise addition. ResNet <ref type="bibr" target="#b15">[5]</ref> seems to be an architecture of choice for several top performers irrespective of how they formulated the learning task. This is because the residual skip connections in ResNet allow backpropagation of gradient deep into the network without dilution. Most of the highly successful networks stuck to predicting pixel-wise class probabilities or using MRCNN <ref type="bibr" target="#b14">[4]</ref> to predict instance maps. Watershed segmentation was among the most heavily utilized post-processing techniques. It was applied to the nuclear probability maps, most often coupled with a marker, where the marker was based on detecting the cores of individual nuclei. Some of the aforementioned general trends observed corroborated those found in instance segmentation challenges of general photography images such as Common Objects in Context (COCO) Challenge <ref type="bibr" target="#b45">[35]</ref>.</p><p>Although, the participating nuclei segmentation techniques reported significant improvement over the baseline method of <ref type="bibr" target="#b11">[1]</ref>, more improvements are possible and welcome. To further improve the nuclei segmentation quality, the ambiguity at the boundaries of touching and overlapping nuclei need to be better resolved. Additionally, new techniques should also produce more accurate nuclei boundaries without smoothing out high curvature boundaries. Another direction to be investigated is that of developing techniques that are tolerant of errors in the ground truth annotation itself. The role of generatative adverserial networks (GANs) to further improve nuclei segmentation performance should also be explored <ref type="bibr" target="#b41">[31]</ref>. Based on the fact that the top techniques submitted to the MoNuSeg challenge had a-AJIs that were at par with that of a human annotator, it seems that it is time to put some of these techniques to use in nuclear morphometry based disease assessment studies to develop morphometric biomarkers. Finally, the robustness of the dataset and the techniques that have emerged as a part of the MoNuSeg challenge should be assessed for segmenting nuclei under multi-resolution and multi-stain settings. This can be achieved by conducting future competitions on the datasets containing annotated nuclei from images obtained at multiple microscopic resolutions (e.g, 10×, 20×, 40×, etc.) and including annotated nuclei from images stained with different types of stains (e.g. multiple IHC stains).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Test sub-images taken from different organs exemplifying challenges of working with varied nuclear appearances and crowding patterns are shown in columns. Original H&amp;E images, nuclear boundary annotations and segmentation results from the top-five techniques are shown in rows.</figDesc><graphic coords="8,78.95,123.05,483.34,540.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,63.27,55.61,239.08,401.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Publicly available H&amp;E stained tissue image datasets annotated for nucleus segmentation</figDesc><table><row><cell>Dataset</cell><cell>Image Size</cell><cell>Images</cell><cell>Nuclei</cell><cell># Organs</cell><cell>Annotation type</cell></row><row><cell>Kumar et al. [1]</cell><cell>1000 × 1000</cell><cell>30</cell><cell>21,623</cell><cell>7</cell><cell>Individual nuclei boundary coordinates</cell></row><row><cell>Janowczyk et al. [38]</cell><cell>2000 × 2000</cell><cell>143</cell><cell>12,000</cell><cell>1 (breast)</cell><cell>Binary foreground mask</cell></row><row><cell>Wienert et al. [23]</cell><cell>600 × 600</cell><cell>36</cell><cell>7,931</cell><cell>5</cell><cell>Individual nuclei boundary coordinates</cell></row><row><cell>Naylor et al. [30]</cell><cell>512 × 512</cell><cell>50</cell><cell>4,022</cell><cell>1 (breast)</cell><cell>Binary foreground mask</cell></row><row><cell>Irshad et al. 1 [39]</cell><cell>400 × 400</cell><cell>63</cell><cell>2,532</cell><cell>1 (kidney)</cell><cell>Binary foreground mask</cell></row><row><cell>Gelasca et al. [40]</cell><cell>896 × 768 (768 × 512)</cell><cell>50</cell><cell>1,895</cell><cell>1 (breast)</cell><cell>Binary foreground mask</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>MoNuSeg 2018 training and testing datset composition.</figDesc><table><row><cell>Data subset ↓</cell><cell>Nuclei Total</cell><cell>Total</cell><cell>Breast</cell><cell>Liver</cell><cell>Kidney</cell><cell cols="2">Images Prostate Bladder</cell><cell>Colon</cell><cell>Stomach</cell><cell>Lung</cell><cell>Brain</cell></row><row><cell>Training set</cell><cell>21,623</cell><cell>30</cell><cell>6</cell><cell>6</cell><cell>6</cell><cell>6</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>-</cell><cell>-</cell></row><row><cell>Testing set</cell><cell>7,223</cell><cell>14</cell><cell>2</cell><cell>-</cell><cell>3</cell><cell>2</cell><cell>2</cell><cell>1</cell><cell>-</cell><cell>2</cell><cell>2</cell></row><row><cell>Total</cell><cell>28,846</cell><cell>44</cell><cell>8</cell><cell>6</cell><cell>9</cell><cell>8</cell><cell>4</cell><cell>3</cell><cell>2</cell><cell>2</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>It is the largest repository of hand annotated nuclei which aptly represents a miscellany of nuclei shapes, and sizes across multiple organs, disease states and patients. The inclusion of tissue sections from 18 hospitals further augments the richness of this dataset. From TableI, the only multi-organ alternative to it is Weinert et al.<ref type="bibr" target="#b33">[23]</ref>. However, Weinert et al.<ref type="bibr" target="#b33">[23]</ref> contains tissues from lesser number of organs captured in a single hospital with a single scanner.2) It extracted only one sub-image of 1000 × 1000 pixels per patient to maximize nuclear appearance variation.Other datasets mentioned in TableIextracted multiple sub-images from each patient and are thus limited in representing nuclear appearance diversity. For example, WSIs of only 10 and 11 patients were used in Irshad et al.<ref type="bibr" target="#b49">[39]</ref> and Naylor et al.<ref type="bibr" target="#b40">[30]</ref>, respectively.</figDesc><table><row><cell>3) It provided coordinates of annotated nuclear boundaries</cell></row><row><cell>in an XML format instead of binary masks. This is</cell></row><row><cell>crucial for learning to separate touching and overlapping</cell></row><row><cell>nuclei in any automatic nucleus segmentation algo-</cell></row><row><cell>rithm. This helped several participants of MoNuSeg</cell></row><row><cell>2018 whose nucleus segmentation algorithms explicitly</cell></row><row><cell>learned to recognize nuclear boundaries in addition to</cell></row><row><cell>the usual foreground (nuclei pixels) and background</cell></row><row><cell>classes (non-nuclei pixels).</cell></row><row><cell>4) It publicly released the source code of their generalized</cell></row><row><cell>nucleus segmentation algorithm to catalyze natural com-</cell></row><row><cell>petition among a newer generation of automatic nucleus</cell></row><row><cell>segmentation algorithms.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Only annotations verified by a pathologist were considered.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was supported by the NCI-NIH (USA) grant no. 5R25-CA057699 for Kumar, NRF (Korea) grant no. 2016R1C1B2012433 for Vu and Kwak, and NIBIB-NIH (USA) grant no. R01EB020683 for Iftekharuddin. We are thankful to Gaurav Patel, Yashodhan Ghadge, and Sanjay Kumar for annotating nuclei for the testing set and to NVIDIA for donating the GPUs.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">and Q. Wang are with the School of Biomedical Engineering</title>
		<author>
			<persName><forename type="first">*</forename><forename type="middle">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">F</forename><surname>Onder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tsougenis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><forename type="middle">P A</forename><surname>Kong</surname></persName>
		</author>
		<editor>N. A. Koohbanani, M. Jahanifar, N. Z. Tajeddin, A. Gooya, S. Graham and N</editor>
		<imprint>
			<pubPlace>Beijing, China; Oklahoma, USA; Warwick, United Kingdom. X. Ren; Shangai, China; Chapel Hill, North Carolina, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Pathology, University of Illinois at Chicago. R. Verma is with the Department of Biomedical Engineering, Case Western Reserve University. D. Anand is with the Department of Electrical Engineering, Indian Institute of Technology Bombay. Y. Zhou is with the Department of Computer Science and Engineering, Chinese University of Hong Kong ; Computer Science and Engineering, Chinese University of Hong Kong. J. Lui is with the School of Computer Science, Beijing University of Post and Telecommunications ; Hu is with the School of Electronics Engineering and Computer Science, Peking University, China. Y. Wang is with the Department of Electrical and Computer Engineering, University of Oklahoma ; Department of Computer Science, University of Warwick ; Shanghai Jiao Tong University</orgName>
		</respStmt>
	</monogr>
	<note>are with Imsight Medical Technology Inc. S. Zhou and D. Shen are with the Department of Radiology and BRIC, University of North Carolina at Chapel Hill</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">S. Xu is with Sun Yat-Sen University Cancer Center, Guangzhou, China and with Bio-totem Pte</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ltd</title>
		<imprint/>
		<respStmt>
			<orgName>Yang is with Zhejiang University ; Sun is with Sun Yat-Sen University Cancer Center ; Medical University of Viena, Viena, Austria. G. Schaefer is with the Department of Computer Science, Loughborough University</orgName>
		</respStmt>
	</monogr>
	<note>A. Mahbod and I. Ellinger are with the Institute for Pathophysiology and Allergy Research. United Kingdom. R. Ecker is with TissueGnostics GmbH</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">O</forename><surname>Smedby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<publisher>KTH Royal Institute of Technology</publisher>
			<pubPlace>Stockholm, Sweden</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Biomedical Engineering and Health Systems</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">B</forename><surname>Chidester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<pubPlace>Pennsylvania, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>School of Computer Science, Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Do are with the Department of Electrical</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">V</forename><surname>Ton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename></persName>
		</author>
		<imprint>
			<pubPlace>Urbana-Champaign, Illinois, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>and Computer Engineering, University of Illinois at</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename></persName>
		</author>
		<imprint>
			<pubPlace>Vietnam</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Tran is with University of Science, Vietnam National University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">B. Yin and K. Chen are with iFlytek AI Research</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">D</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><forename type="middle">D</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lofti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eschweiler and J. Stegmaier are with the Institute of Imaging and Computer Vision</title>
		<meeting><address><addrLine>Edinburgh, United Kingdom; Anhui, China; Guangzhou, China; Tel-Aviv, Israel; Munich, Germany</addrLine></address></meeting>
		<imprint/>
		<respStmt>
			<orgName>Sejong University, Seoul, South Korea. A. Gunda is with the Department of Mechanical Engineering, Indian Intitute of Technology Madras, Chennai, India. R. Chunduri is with the Department of Aerospace Engineering, Indian Institute of Technology Bombay, Mumbai, India. C. Hu. is with the Department of Computer Science, University of California Berkeley, California, USA. X. Zhou. is with the Hong Kong University of Science and Technology ; Safdari are with Tehran Science and Research and Qazvin Branches of the Islamic Azad University ; RWTH Aachen University, Aachen, Germany. Y. Cui and X. Tian are with the University of Science and Technology of China ; Barth are with the Department of Computer Science, Institute for Neuro-and Bioinformatics, University of Lübeck, Lübeck, Germany. E. Arbel, I. Remer and A. Ben-Dor are with Agilent Labs, Agilent Technologies Ltd ; Braunewell are with Konica Minolta Laboratory Europe</orgName>
		</respStmt>
	</monogr>
	<note>Kwak are with the College of Software Convergence</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">China. J. Ma is with the Department of Mathematics</title>
		<imprint>
			<publisher>TATA Consultancy Services Ltd</publisher>
			<pubPlace>Shenzhen; Nanjing, China; Pune, India</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Computer Vision Institute, Shenzhen University ; Nanjing University of Science and Technology</orgName>
		</respStmt>
	</monogr>
	<note>TCS Research</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bhattacharjee are with the Department of</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><surname>València</surname></persName>
		</author>
		<author>
			<persName><forename type="middle">L</forename><surname>Spain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M ; K</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
		<imprint>
			<pubPlace>Norfolk, Virginia; Kolkata, India; Xiamen, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science and Engineering, Korea University, Seoul, South Korea. A. Colomer and V. Naranjo are with Instituto de Investigación e Innovación en Bioingeníera, Universitat Politècnica de València ; Iftekharuddin are with the Department of Electrical &amp; Computer Engineering, Old Dominion University ; Computer Science and Engineering, Jadavpur University ; Visilab Research Group, University of Castilla -La Mancha, Ciudad Real, Spain. S. Devanathan, S. Radhakrishnan and P. Koduganty are with Cognizant Technology Solutions India Private Ltd, India. Z. Wu is with Xiamen University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<pubPlace>Shanghai, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Tongji University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sethi is with the Department of Electrical Engineering</title>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<imprint>
			<pubPlace>Mumbai, India. REFERENCES</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Indian Institute of Technology Bombay</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A dataset and a technique for generalized nuclear segmentation for computational pathology</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahadane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sethi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Invariant delineation of nuclear architecture in glioblastoma multiforme for clinical and molecular association</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Loss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Spellman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Parvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="670" to="682" />
			<date type="published" when="2013-04">April 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Computer-aided breast cancer diagnosis based on the analysis of cytological images of fine needle biopsies</title>
		<author>
			<persName><forename type="first">P</forename><surname>Filipczuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krzyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
			<date type="published" when="2013-12">Dec 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Systematic analysis of breast cancer morphology uncovers stromal features associated with survival</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Sangoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Marinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">O</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Van De Vijver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van De Rijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<ptr target="http://stm.sciencemag.org/content/3/108/108ra113" />
	</analytic>
	<monogr>
		<title level="j">Science Translational Medicine</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">108</biblScope>
			<biblScope unit="page" from="108" to="113" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>ra113</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Computer vision detects subtle histological effects of dutasteride on benign prostate</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Macias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Deaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Gann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BJU international</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="143" to="151" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">t -tests, f -tests and otsu&apos;s methods for image thresholding</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Titterington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2392" to="2396" />
			<date type="published" when="2011-08">Aug 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Nuclei segmentation using markercontrolled watershed, tracking using mean-shift, and kalman filter in time-lapse microscopy</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems I: Regular Papers</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2405" to="2414" />
			<date type="published" when="2006-11">Nov 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic nuclei segmentation in h&amp;e stained breast cancer histopathology images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Veta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Van Diest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kornegoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Pluim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>p. e70221</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards generalized nuclear segmentation in histological images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vahadane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sethi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 13th International Conference on</title>
		<imprint>
			<date type="published" when="2013-11">2013. Nov 2013</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fiji: an open-source platform for biological-image analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schindelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Arganda-Carreras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kaynig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Longair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pietzsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Preibisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rueden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saalfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Tinevez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hartenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Eliceiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tomancak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cardona</surname></persName>
		</author>
		<idno type="DOI">10.1038/nmeth.2019</idno>
		<ptr target="http://dx.doi.org/10.1038/nmeth.2019" />
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="676" to="682" />
			<date type="published" when="2012-07">Jul. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cellprofiler: image analysis software for identifying and quantifying cell phenotypes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lamprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Friman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Guertin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Lindquist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Golland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Sabatini</surname></persName>
		</author>
		<idno type="DOI">10.1186/gb-2006-7-10-r100</idno>
		<ptr target="http://dx.doi.org/10.1186/gb-2006-7-10-r100" />
	</analytic>
	<monogr>
		<title level="j">Genome Biology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The cancer genome atlas (tcga)</title>
		<ptr target="http://cancergenome.nih.gov/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Multi-organ nuclei segmentation challenge (MoNuSeg) 2018 [online</title>
		<ptr target="https://monuseg.grand-challenge.org/" />
		<imprint>
			<date type="published" when="2019-02-11">11 Feb. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Detecting multiple subtypes of breast cancer in a single patient</title>
		<author>
			<persName><forename type="first">R</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Gann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2016-09">Sept 2016</date>
			<biblScope unit="page" from="2648" to="2652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for prostate cancer recurrence prediction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Gann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Pathology</title>
		<imprint>
			<biblScope unit="volume">10140</biblScope>
			<biblScope unit="page">101400</biblScope>
			<date type="published" when="2017">2017. 2017</date>
			<publisher>International Society for Optics and Photonics</publisher>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An integrated region-, boundary-, shapebased active contour for multiple object overlap resolution in histological imagery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1448" to="1460" />
			<date type="published" when="2012-07">July 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improved automatic detection and segmentation of cell nuclei in histopathology images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Al-Kofahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lassoued</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Roysam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="841" to="852" />
			<date type="published" when="2010-04">April 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Detection and segmentation of cell nuclei in virtual microscopy images: a minimum-model approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wienert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stenzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Beil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hufnagl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dietel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Denkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Klauschen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">503</biblScope>
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Partitioning histopathological images: An integrated framework for supervised colortexture segmentation and cell splitting</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gurcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Belkacem-Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1661" to="1677" />
			<date type="published" when="2011-09">Sept 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Invariant delineation of nuclear architecture in glioblastoma multiforme for clinical and molecular association</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Loss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Spellman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Parvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="670" to="682" />
			<date type="published" when="2013-04">April 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Small blob identification in medical images using regional features from optimum scale</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1051" to="1062" />
			<date type="published" when="2015-04">April 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Accurate segmentation of cervical cytoplasm and nuclei based on multiscale convolutional network and graph partitioning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2421" to="2433" />
			<date type="published" when="2015-10">Oct 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An automatic learning-based framework for robust nucleus segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="550" to="566" />
			<date type="published" when="2016-02">Feb 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Locality sensitive deep learning for detection and classification of nuclei in routine colon cancer histology images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sirinukunwattana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E A R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1196" to="1206" />
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Segmentation of nuclei in histopathology images by deep regression of the distance map</title>
		<author>
			<persName><forename type="first">P</forename><surname>Naylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>La</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Reyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="448" to="459" />
			<date type="published" when="2019-02">Feb 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Mckay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Salimian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Durr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00236</idno>
		<title level="m">Deep adversarial training for multiorgan nuclei segmentation in histopathology images</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Methods for nuclei detection, segmentation, and classification in digital histopathology: A review, current status and future potential</title>
		<author>
			<persName><forename type="first">H</forename><surname>Irshad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Veillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Racoceanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Reviews in Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="97" to="114" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Robust nucleus/cell detection and segmentation in digital pathology and microscopy images: A comprehensive review</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Reviews in Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="234" to="263" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E E</forename><surname>Maire</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">1399 h&amp;e-stained sentinel lymph node sections of breast cancer patients: the camelyon dataset</title>
		<author>
			<persName><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Ehteshami</forename><surname>Bejnordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Geessink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Balkenhol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bult</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Halilovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hermsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van De Loo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vogels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">GigaScience</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">65</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ciurea-Ilcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chute</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haghgoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shpanskaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07031</idno>
		<title level="m">Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep learning for digital pathology image analysis: A comprehensive tutorial with selected use cases</title>
		<author>
			<persName><forename type="first">A</forename><surname>Janowczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Pathology Informatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2016-07">July 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Crowdsourcing image annotation for nucleus detection and segmentation in computational pathology: evaluating experts, automated methods, and the crowd</title>
		<author>
			<persName><forename type="first">H</forename><surname>Irshad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Montaser-Kouhsari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Waltz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bucur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W</forename><surname>Knoblauch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Beck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific Symposium on Biocomputing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="294" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A biosegmentation benchmark for evaluation of bioimage analysis methods</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Gelasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Obara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kvilekval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">368</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Structurepreserving color normalization and sparse stain separation for histological images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vahadane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Steiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Schlitter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Esposito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1962" to="1971" />
			<date type="published" when="2016-08">Aug 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A method for normalizing histology slides for quantitative analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Macenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Marron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Woosley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE International Symposium on Biomedical Imaging: From Nano to Macro</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1107" to="1110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Color transfer between images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Adhikhmin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shirley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer graphics and applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="34" to="41" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Adaptive histogram equalization and its variations</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Pizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Amburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cromartie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geselowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Greer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ter Haar Romeny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Zimmerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zuiderveld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision, graphics, and image processing</title>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="355" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Ternausnet: U-net with vgg11 encoder pre-trained on imagenet for image segmentation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Iglovikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shvets</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.05746</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Measures of the amount of ecologic association between species</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Dice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="302" />
			<date type="published" when="1945">1945</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Étude comparative de la distribution florale dans une portion des alpes et des jura</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jaccard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull Soc Vaudoise Sci Nat</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="547" to="579" />
			<date type="published" when="1901">1901</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Multiscale vessel enhancement filtering</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Niessen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Vincken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="130" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Empirical comparison of color normalization methods for epithelial-stromal classification in h and e images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Pathology Informatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Quantifying the effects of data augmentation and stain color normalization in convolutional neural networks for computational pathology</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tellez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bulten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Bokhorst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ciompi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Der Laak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06543</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">On stain normalization in deep learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Janowczyk</surname></persName>
		</author>
		<ptr target="http://www.andrewjanowczyk.com/on-stain-normalization-in-deep-learning/" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
