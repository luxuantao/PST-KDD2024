<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Optimal Coverage for Multiple Hovering Robots with Downward Facing Cameras</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mac</forename><surname>Schwager</surname></persName>
							<email>schwager@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Lab (CSAIL)</orgName>
								<orgName type="institution">MIT</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Brian</forename><forename type="middle">J</forename><surname>Julian</surname></persName>
							<email>bjulian@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Lab (CSAIL)</orgName>
								<orgName type="institution">MIT</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">MIT Lincoln Laboratory</orgName>
								<address>
									<addrLine>244 Wood Street</addrLine>
									<postCode>02420</postCode>
									<settlement>Lexington</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniela</forename><surname>Rus</surname></persName>
							<email>rus@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Lab (CSAIL)</orgName>
								<orgName type="institution">MIT</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Optimal Coverage for Multiple Hovering Robots with Downward Facing Cameras</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2D5D64C4E32E415E6F85FB92F2BEF877</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a distributed control strategy for deploying hovering robots with multiple downward facing cameras to collectively monitor an environment. Information per pixel is proposed as an optimization criterion for multicamera placement problems. This metric is used to derive a specific cost function for multiple downward facing cameras mounted on hovering robot platforms. The cost function leads to a gradient-based distributed controller for positioning the robots. A convergence proof using LaSalle's invariance principle is given to show that the robots converge to locally optimal positions. The controller is demonstrated in experiments with three flying quad-rotor robots.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Multiple collaborating robots with cameras are useful in a broad range of applications, from surveying disaster sites, to observing the health of coral reefs. However, an immediate and difficult question arises in such applications: how should one position the robots so as to maintain the best view of an environment? In this paper we offer an approach motivated by an information content principle: minimum information per pixel. Using information per pixel as a metric allows for the incorporation of physical, geometric, and optical parameters to give a cost function that represents how well a group of cameras covers an environment. We develop the approach in detail for the particular case of multiple downward facing cameras mounted to robots. The cost function leads to a gradient-based distributed controller for the robots to position themselves in three dimensions so as to best observe a planar environment over which they hover. We present simulation results in a Matlab environment. We also present experimental results with three AscTec Hummingbird quad-rotor robots. This paper is accompanied by a video of the quad-rotors using the control algorithm.</p><p>Our algorithm can be used in support of a higher-level computer vision task, such as object recognition or tracking. We address the problem of how to best position the robots The authors would like to thank an anonymous reviewer who gave particularly helpful and detailed comments.</p><p>This work was supported in part by the MURI SWARMS project grant number W911NF-05-1-0219, NSF grant numbers IIS-0513755, IIS-0426838, CNS-0520305, CNS-0707601, EFRI-0735953, MIT Lincoln Laboratory, the MAST project, and the Boeing Corporation.</p><p>This work is sponsored by the Department of the Air Force under Air Force contract number FA8721-05-C-0002. The opinions, interpretations, recommendations, and conclusions are those of the authors and are not necessarily endorsed by the United States Government.</p><p>given that the data from their cameras will be used by some computer vision algorithm. Our design principle can be readily adapted to a number of applications. For example, it could be used to control groups of autonomous underwater or air vehicles to do mosaicing <ref type="bibr" target="#b0">[1]</ref>, or to produce photometric stereo from multiple camera views <ref type="bibr" target="#b1">[2]</ref>, for inspection of underwater or land-based archaeological sites, biological environments such as coral reefs or forests, disaster sites, or any other large scale environment of interest. Our algorithm could also be used by autonomous flying robots to do surveillance <ref type="bibr" target="#b2">[3]</ref>, target tracking <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>, or to aid in navigation of agents on the ground <ref type="bibr" target="#b6">[7]</ref>. The main contributions of this work are three-fold: 1) we propose the minimum information per pixel principle as a cost function for camera placement and formulate it explicitly for the case of multiple hovering robots with downward facing cameras, 2) we use the cost function to design a controller to deploy multiple robots to their optimal positions in a distributed fashion, 3) we implement the proposed controller on three quad-rotor robots. The proposed robot coordination algorithm is fully decentralized, provably stable, adaptive to a changing number of flying agents and a changing environment, and will work with a broad class of environment geometries, including convex, non-convex, and disconnected spaces.</p><p>We are inspired by a recent body of work concerning the optimal deployment of robots for providing sensor coverage of an environment. Cortés et al. <ref type="bibr" target="#b7">[8]</ref> introduced a stable distributed controller for sensor coverage based on ideas from the optimal facility placement literature <ref type="bibr" target="#b8">[9]</ref>. This approach involves a Voronoi partition of the environment, and has seen several extensions ( [10]- <ref type="bibr" target="#b11">[12]</ref>). One recent extension described in <ref type="bibr" target="#b12">[13]</ref>, Figure <ref type="figure" target="#fig_0">14</ref>, proposed an algorithm for the placement of hovering sensors, similar to our scenario.</p><p>Our method in this paper is related to this body of work in that we propose a cost function and obtain a distributed controller by taking its gradient. However, the cost function we propose is different from previous ones in that it does not involve a Voronoi partition. To the contrary, it relies on the fields of view of multiple cameras to overlap with one another. Another distinction from previous works is that the agents we consider move in a space that is different from the one they cover. Previous coverage scenarios have considered agents constrained to move in the environment that they cover, which leads to a constraint that the environment must be convex (to prevent agents from trying to leave the environment). In contrast, we consider agents moving in a space R 3 , covering an arbitrary lower dimensional environment Q ⊂ R 2 . This eliminates the need for Q to be convex. Indeed, it need not even be connected. It must only be Lebesgue measurable (since the robots will calculate integrals over it), which is quite a broad specification.</p><p>There have also been other algorithms for camera placement, for example a probabilistic approach for general sensor deployment based on the Cramér-Rao bound was proposed in <ref type="bibr" target="#b13">[14]</ref>, and an application of the idea for cameras was given in <ref type="bibr" target="#b14">[15]</ref>. We choose to focus on the problem of positioning downward facing cameras, similarly to <ref type="bibr" target="#b15">[16]</ref>, as opposed to arbitrarily oriented cameras. Many geometrical aspects of the problem are significantly simplified in this setting, yet there are a number of practical applications that stand to benefit from controlling cameras in this way, as previously described. More generally, several other works have considered cooperative control with flying robots and UAV's. For an excellent review of cooperative UAV control please see <ref type="bibr" target="#b16">[17]</ref>, or <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b18">[19]</ref> for two recent examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. OPTIMAL CAMERA PLACEMENT</head><p>We motivate our approach with an informal justification of a cost function, then develop the problem formally for the single camera case followed by the multi-camera case. We desire to cover a bounded environment, Q ⊂ R 2 , with a number of cameras. We assume Q is planar, without topography, to avoid the complications of changing elevation or occlusions. Let p i ∈ P represent the state of camera i, where the state-space, P, will be characterized later. We want to control n cameras in a distributed fashion such that their placement minimizes the aggregate information per camera pixel over the environment, min (p1,...,pn)∈P n Q info pixel dq.</p><p>This metric makes sense because the pixel is the fundamental information capturing unit of the camera. Consider the patch of image that is exposed to a given pixel. The information in that patch is reduced by the camera to a lowdimensional representation (i.e. mean color and brightness over the patch). Therefore, the less information content the image patch contains, the less information will be lost in its low-dimensional representation by the pixel. Furthermore, we want to minimize the accumulated information loss due to pixelation over the whole environment Q, hence the integral.</p><p>In the next two sections we will formalize the notion of information per pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Single Camera</head><p>We develop the cost function for a single camera before generalizing to multiple cameras. It is convenient to consider the information per pixel as the product of two functions, f : P ×Q → (0, ∞], which gives the area in the environment seen by one pixel (the "area per pixel" function), and φ : Q → (0, ∞) which gives the information per area in the environment. The form of f (p i , q) will be derived from the optics of the camera and geometry of the environment. The function φ(q) is a positive weighting of importance over Q and should be specified beforehand (it can also be learned from sensor data, as in <ref type="bibr" target="#b10">[11]</ref>). For instance, if all points in the environment are equally important, φ(q) should be constant over Q. If some known area in Q requires more resolution, the value of φ(q) should be larger in that area than elsewhere in Q. This gives the cost function</p><formula xml:id="formula_0">min p Q f (p, q)φ(q) dq,<label>(1)</label></formula><p>which is of a general form common in the locational optimization and optimal sensor deployment literature <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b19">[20]</ref>.</p><p>We will introduce significant changes to this basic form with the addition of multiple cameras.</p><p>The state of the camera, p, consists of all parameters associated with the camera that effect the area per pixel function, f (p, q). In a general setting one might consider the camera's position in R 3 , its orientation in so(3) (the three rotational angles), and perhaps a lens zooming parameter in the interval (0, ∞), thus leading to an optimization in a rather complicated state-space (P = R 3 × so(3) × (0, ∞)) for only one camera. For this reason, we consider the special case in which the camera is downward facing (hovering over Q). Indeed, this case is of particular interest in many applications, as described in Section I. We define the field of view, B, to be the intersection of the cone whose vertex is the focal point of the camera lens with the subspace that contains the environment, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. In Section III-A we will consider a camera with a rectangular field of view, but initially consider a circular field of view, so the rotational orientation of the downward facing camera is irrelevant. In this case P = R 3 , and the state-space in which we do optimization is considerably simplified from that of the unconstrained camera. Decompose the camera position as p = [c T , z] T , with c ∈ R 2 the center point of the field of view, and z ∈ R the height of the camera over Q. We have</p><formula xml:id="formula_1">B = q | q -c z ≤ tan θ (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where θ is the half-angle of view of the camera. To find the area per pixel function, f (p, q), consider the geometry in Fig. <ref type="figure" target="#fig_1">2</ref>. Let b be the focal length of the lens. Inside B, the area/pixel is equal to the inverse of the area magnification factor (which is defined from classical optics to be b 2 /(bz) 2 ) times the area of one pixel <ref type="bibr" target="#b20">[21]</ref>. Define a to be the area of one pixel divided by the square of the focal length of the lens. We have,</p><formula xml:id="formula_3">f (p, q) = a(b -z) 2 for q ∈ B ∞ otherwise,<label>(3)</label></formula><p>Outside of the field of view, there are no pixels, therefore the area per pixel is infinite (we will avoid dealing with infinite quantities in the multi-camera case). One can see in this simple scenario that the optimal solution is for p to be such that the field of view is the smallest ball that contains Q. However, with multiple cameras, the problem becomes more challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multiple Cameras</head><p>To find optimal positions for multiple cameras, we have to determine how to account for the area of overlap of the images of the cameras, as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. Intuitively, an area of Q that is being observed by two different cameras is better covered than if it were being observed by only one camera, but it is not twice as well covered. Consider a point q that appears in the image of n different cameras. The number of pixels per area at that point is the sum of the pixels per area for each camera. Therefore (assuming the cameras are identical, so they use the same function f (p i , q)) the area per pixel at that point is given by the inverse of the sum of the inverse of the area per pixel for each camera, or</p><formula xml:id="formula_4">Q ŀ B i Q ŀ B i q P i Q P j B j</formula><formula xml:id="formula_5">area pixel = n i=1 f (p i , q) -1 -1 ,</formula><p>where p i is the position of the ith camera. We emphasize that it is the pixels per area that sum because of the multiple cameras, not the area per pixel because, in the overlap region, multiple pixels are observing the same area. Therefore the inverse of the sum of inverses is unavoidable. Incidentally, this is the same form one would use to combine the variances of multiple noisy measurements when doing sensor fusion. Finally, we introduce a prior area per pixel, w ∈ (0, ∞). The interpretation of the prior is that there is some preexisting photograph of the environment (e.g. an initial reconnaissance photograph), from which we can get a base-line area per pixel measurement. This is compatible with the rest of our scenario, since we will assume that the robots have knowledge of the geometry of the environment Q, and some notion of information content over it, φ(q), which could also be derived from a pre-existing photograph. This pre-existing information can be arbitrarily vague (w can be arbitrarily large) but it must exist. The prior also has the benefit of making the cost function finite for all robot positions. It is combined with the camera sensors as if it were another camera to get</p><formula xml:id="formula_6">area pixel = n i=1 f (p i , q) -1 + w -1 -1 ,</formula><p>Let N q be the set of indices of cameras for which f (p i , q) is bounded, N q = {i | q ∈ B i ). We can now write the area per pixel function as</p><formula xml:id="formula_7">h Nq (p 1 , . . . , p n , q) = i∈Nq f (p i , q) -1 + w -1 -1 . (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>to give the cost function</p><formula xml:id="formula_9">H(p 1 , . . . , p n ) = Q h Nq (p 1 , . . . , p n , q)φ(q) dq.<label>(5)</label></formula><p>We will often refer to h Nq and H without their arguments. Now we can pose the multi-camera optimization problem,</p><formula xml:id="formula_10">min (p1,...,pn)∈P n H. (<label>6</label></formula><formula xml:id="formula_11">)</formula><p>The cost function ( <ref type="formula" target="#formula_9">5</ref>) is of a general form valid for any area per pixel function f (p i , q), and for any camera state space P (including cameras that can can swivel on gimbels). We proceed with the special case of downward facing cameras, where P = R 3 and f (p i , q) is from (3) for the remainder of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DISTRIBUTED CONTROL</head><p>We will take the gradient of ( <ref type="formula" target="#formula_9">5</ref>) and find that it is distributed among agents. This will lead to a gradient-based controller. We will use the notation N q \{i} to mean the set of all indices in N q , except for i.</p><p>Theorem 1 (Gradient Component): The gradient of the cost function H(p 1 , . . . , p n ) with respect to a robot's position p i , using the area per pixel function in (3) is given by</p><formula xml:id="formula_12">∂H ∂c i = Q∩∂Bi (h Nq -h Nq\{i} ) (q -c i ) q -c i φ(q) dq,<label>(7)</label></formula><p>and</p><formula xml:id="formula_13">∂H ∂z i = Q∩∂Bi (h Nq -h Nq\{i} )φ(q) tan θ dq - Q∩Bi 2h 2 Nq a(b -z i ) 3 φ(q) dq.<label>(8)</label></formula><p>Please refer to the appendix for a proof. Remark 1 (Intuition): We will consider a controller that moves a robot in the opposite direction of its gradient component. In which case, the single integral for the lateral component <ref type="bibr" target="#b6">(7)</ref> causes the robot to move to increase the amount of the environment in its field of view, while also moving away from other robots j whose field of view overlaps with its own. The vertical component <ref type="bibr" target="#b7">(8)</ref> has two integrals with competing tendencies. The first integral causes the robot to move up to bring more of the environment into its field of view, while the second integral causes it to move down to get a better look at the environment already in its field of view.</p><p>Remark 2 (Requirements): Both the lateral ( <ref type="formula" target="#formula_12">7</ref>) and vertical (8) components can be computed by robot i with knowledge of 1) its own position, p i , 2) the extent of the environment Q, 3) the information per area function φ(q), and 4) the positions of all other robots whose fields of view intersect with its own (which can be found by communication or sensing).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 3 (Network Requirements):</head><p>The requirement that a robot can communicate with all other robots whose fields' of view intersect with its own describes a minimal network graph for our controller to be feasible. In particular, we require the network to be at least a proximity graph in which all agents i are connected to all other agents j ∈ N i , where</p><formula xml:id="formula_14">N i = {j | Q ∩B i ∩B j = ∅, i = j}.</formula><p>The controller can be run over a network that is a subgraph of the required proximity graph, in which case performance will degrade gracefully as the network becomes more sparse.</p><p>We propose to use a gradient control law in which every robot follows the negative of its own gradient component,</p><formula xml:id="formula_15">u i = -k∂H/∂p i ,<label>(9)</label></formula><p>where u i is the control input for robot i and k ∈ (0, ∞) is a control gain. Assuming integrator dynamics for the robots,</p><formula xml:id="formula_16">ṗi = u i ,<label>(10)</label></formula><p>we can prove the convergence of this controller to locally minimize the aggregate information per area. Theorem 2 (Convergence): For a network of n robots with the dynamics in <ref type="bibr" target="#b9">(10)</ref>, using the controller in <ref type="bibr" target="#b8">(9)</ref>,</p><formula xml:id="formula_17">lim t→∞ ∂H ∂p i = 0 ∀i ∈ {1, . . . , n}.<label>(11)</label></formula><p>Proof: The proof is an application of LaSalle's invariance principle ( <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b19">[20]</ref> Theorem 1.17 <ref type="foot" target="#foot_0">1</ref> ). Let H(p 1 , . . . , p n ) be a Lyapunov-type function candidate. The closed-loop dynamics ṗi = -∂H/∂p i do not depend on time, and ∂H/∂p i is a continuously differentiable function of p j for all j (i.e. all terms in the Hessian of H are continuous). Taking the time derivative of H along the trajectories of the system gives</p><formula xml:id="formula_18">Ḣ = n i=1 ∂H ∂p i T ṗi = - n i=1 ∂H ∂p i T ∂H ∂p i ≤ 0.<label>(12)</label></formula><p>Next we show that all evolutions of the system are bounded.</p><p>To see this, consider a robot at p i such that Q∩B i = ∅. Then ṗi = 0 for all time (if the field of view leaves Q, the robot stops for all time), so c i (t) is bounded. Given Q ∩B i = ∅, H is radially unbounded (i.e. coercive) in z i , therefore Ḣ ≤ 0 implies that z i is bounded for all time. Finally, consider the set of all (p 1 , . . . , p n ) for which Ḣ = 0. This is itself an invariant set, since Ḣ = 0 implies ∂H/∂p i = ṗi = 0 for all i. Therefore, all conditions of LaSalle's principle are satisfied and the trajectories of the system converge to this invariant set.</p><p>To be more precise, there may exist configurations at which ∂H ∂pi = 0 ∀i that are saddle points or local maxima of H. However, since the controller is a gradient controller, only the local minima of H are stable equilibria. A proof of this intuitively obvious fact about gradient systems can be found in <ref type="bibr" target="#b22">[23]</ref>, Chapter 9, Section 4.</p><p>This controller can be implemented in a discretized setting as Algorithm 1. In general, the integrals in the controller must be computed using a discretized approximation. Let Q ∩ ∂B i and Q ∩ B i be the discretized sets of gird points representing the sets Q ∩ ∂B i and Q ∩ B i , respectively. Let Δq be the length of an arc segment for the discretized set Q ∩ ∂B i , and the area of a grid square for the discretized set Q ∩ B i . A simple algorithm that approximates (9) is then given in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 4 (Time Complexity):</head><p>To determine the computational complexity of this algorithm, let us assume that there are m points in both sets Q ∩ ∂B i and Q ∩ B i . We can now calculate the time complexity as</p><formula xml:id="formula_19">T (n, m) ≤ m j=1 (O(1) + n k=1 O(1)) + m j=1 (O(1) + n k=1 O(1) + n-1 k=1 O(1)) ∈ O(nm).</formula><p>When calculating the controller for all robots on a centralized processor (as was done for the simulations in Section V), the time complexity becomes T (n, m) ∈ O(n 2 m).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 5 (Adaptivity):</head><p>The controller is adaptive in the sense that it will stably reconfigure if any number of robots Algorithm 1 Discretized Controller Require: Robot i knows its position p i , the extent environment Q, and the information per area function φ(q). Require: Robot i can communicate with all robots j whose field of view intersects with its own. loop Communicate with neighbors to get p j Compute and move to</p><formula xml:id="formula_20">c i (t + Δt) = c i (t) -k q∈ Q∩∂Bi (h Nq -h Nq\{i} ) (q-ci)</formula><p>q-ci φ(q)Δq Compute and move to</p><formula xml:id="formula_21">z i (t + Δt) = z i (t) -k q∈ Q∩∂Bi (h Nq -h Nq\{i} )φ(q) tan θΔq +k q∈ Q∩Bi 2h 2</formula><p>Nq a(b-zi) 3 φ(q)Δq end loop fail. It will also work with nonconvex environments, Q, including disconnected ones. In the case of a disconnected environment, the robots may (or may not, depending on the specific scenario) split into a number of sub-groups that are not in communication with one another. The controller can also track changing environments, Q, and changing information per area functions, φ(q), provided these quantities change slowly enough. This is not addressed by the proof, but has been shown to be the case in simulation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 6 (Control Gains and Robustness):</head><p>The proportional control gain, k, adjusts the aggressiveness of the controller. In a discretized implementation one should set this gain low enough to provide robustness to discretization errors and noise in the system. The prior area per pixel, w, adjusts how much of the area Q will remain uncovered in the final configuration. It should be chosen to be as large as possible, but as with k, should be small enough to provide robustness to discretization errors and noise in the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Rectangular Field of View</head><p>Until this point we have assumed that the camera's field of view, B i is a circle, which eliminates a rotational degree of freedom. Of course, actual cameras have a rectangular CCD array, and therefore a rectangular field of view. In this section we revisit the gradient component in Theorem 1 and calculate it for a rectangular field of view and a robot with a rotational degree of freedom.</p><p>Let the state space of p i = [c T i z i ψ i ] T be P = R 3 ×S, where ψ i is the rotation angle. Define a rotation matrix</p><formula xml:id="formula_22">R(ψ i ) = cos ψ i sin ψ i -sin ψ i cos ψ i ,<label>(13)</label></formula><p>where R(ψ i )q rotates a vector q expressed in the global coordinate frame, to a coordinate frame aligned with the axes of the rectangular field of view. As is true for all rotation matrices, R(</p><formula xml:id="formula_23">ψ i ) is orthogonal, meaning R(ψ i ) T = R(ψ i ) -1 .</formula><p>Using this matrix, define the field of view of robot i to be</p><formula xml:id="formula_24">B i = {q | |R(ψ i )(q -c i )| ≤ z i tan θ} ,<label>(14)</label></formula><p>where θ = [θ 1 , θ 2 ] T is a vector with two angles which are the two half-view angles associated with two perpendicular edges of the rectangle, as shown in Fig. <ref type="figure">4</ref>, and the ≤ symbol applies element-wise (all elements in the vector must satisfy ≤). We have to break up the boundary of the rectangle into each of its four edges. Let l k be the kth edge, and define four outward-facing normal vectors n k , one associated with each edge, where</p><formula xml:id="formula_25">n 1 = [1 0] T , n 2 = [0 1] T , n 3 = [-1 0],</formula><p>and</p><formula xml:id="formula_26">n 4 = [0 -1].</formula><p>The cost function, H(p 1 , . . . , p n ), is the</p><formula xml:id="formula_27">P i Q ș 2 ș 1</formula><p>Fig. <ref type="figure">4</ref>. The geometry of a camera with a rectangular field of view is shown in this figure . 

same as for the circular case, as is the area per pixel function f (p i , q). Theorem 3 (Rectangular Gradient): The gradient of the cost function H(p 1 , . . . , p n ) with respect to a robot's position p i using the area per pixel function in (3) and the rectangular field of view in ( <ref type="formula" target="#formula_24">14</ref>) is given by</p><formula xml:id="formula_28">∂H ∂c i = 4 k=1 Q∩l k (h Nq -h Nq\{i} )R(ψ i ) T n k φ(q) dq,<label>(15)</label></formula><formula xml:id="formula_29">∂H ∂z i = 4 k=1 Q∩l k (h Nq -h Nq\{i} ) tan θ T n k φ(q) dq - Q∩Bi 2h 2 Nq a(b -z i ) 3 φ(q) dq,<label>(16)</label></formula><p>and</p><formula xml:id="formula_30">∂H ∂ψ i = 4 k=1 Q∩l k (h Nq -h Nq \{i} ) •(q -c i ) T R(ψ i + π/2) T n k φ(q) dq. (<label>17</label></formula><formula xml:id="formula_31">)</formula><p>Remark 7 (Intuition): The terms in the gradient have interpretations similar to the ones for the circular field of view. The lateral component <ref type="bibr" target="#b14">(15)</ref> has one integral which tends to make the robots move away from neighbors with intersecting fields of view, while moving to put its entire field of view inside of the environment Q. The vertical component <ref type="bibr" target="#b15">(16)</ref> comprises two integrals. The first causes the robot to go up to take in a larger view, while the second causes it to go down to get a better view of what it already sees. The angular component <ref type="bibr" target="#b16">(17)</ref> rotate the robot to get more of its field of view into the environment, while also rotating away from other robots whose field of view intersects its own. Computation of the gradient component for the rectangular field of view is of the same complexity as the circular case, and carries the same constraint on the communication topology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We implemented Algorithm 1 on a group of three AscTec Hummingbird flying quad-rotor robots. Our experiments were performed at CSAIL, MIT in a laboratory equipped with a Vicon motion capture system. The robots' position coordinates (x, y, z, yaw) were broadcast wirelessly at 50Hz via a 2.4 Ghz xBee module. Each robot was equipped with a custom ARM microprocessor module running a PID position control loop at 33Hz. Pitch and roll were fully stabilized by the commercial controller described in <ref type="bibr" target="#b23">[24]</ref>. A schematic of the experimental setup is shown in Fig. <ref type="figure" target="#fig_3">5</ref>.</p><p>The coverage algorithm was implemented on the same onboard ARM modules, running asynchronously in a fully distributed fashion. The algorithm calculated way points (c i (t) and z i (t) from Algorithm 1) at 1Hz. This timescale separation between the coverage algorithm and the PID controller was required to approximate the integrator dynamics assumed in <ref type="bibr" target="#b9">(10)</ref>. The camera parameters were set to a = 10 -6 and b = 10 -2 m (which are typical for commercially available cameras), the field of view was θ = 35deg, the information per area was a constant φ(q) = 1, the prior area per pixel was w = 10 -6 m 2 , and the control gain was k = 10 -5 . The enviornment to be covered was a skewed rectangle, 3.7m across at its widest, shown in white in Fig. <ref type="figure" target="#fig_4">6</ref>.</p><p>To test the effectiveness of the algorithm and its robustness to robot failures, we conducted experiments as follows: 1) three robots moved to their optimal positions using the algorithm, 2) one robot was manually removed from the environment, and the remaining two were left to reconfigure automatically, 3) a second robot was removed from the environment and the last one was left to reconfigure automatically. Fig. <ref type="figure" target="#fig_4">6</ref> shows photographs of a typical experiment at the beginning (Fig. <ref type="figure" target="#fig_4">6</ref> is shown in Fig. <ref type="figure" target="#fig_4">6</ref>(e), where the error bars represent one standard deviation. Notice that when one robot is removed, the cost function momentarily increases, then decrease as the remaining robots find a new optimal configuration. The algorithm proved to be robust to the significant, highly nonlinear unmodeled aerodynamic effects of the robots, and to individual robot failures. This paper is accompanied by a video showing the experiments and numerical simulations. We repeated the above experiment a total of 20 times. Of these 19 were successful, while in one experiment two of the robots collided in mid air. The collision was caused by an unreliable gyroscopic sensor, not by a malfunction of the coverage algorithm. With appropriate control gain values, collisions are avoided by the algorithm's natural tendency for neighbors to repel one another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. SIMULATIONS</head><p>We conducted numerical simulations to investigate the scalability and robustness of the algorithm. Hovering robots with integrator dynamics (10) were simulated using Algorithm 1 on a centralized processor. The values of a, b, θ, φ, w, and k were the same as in the experiments. The simulations were over a non-convex environment, as shown in Fig. <ref type="figure" target="#fig_5">7</ref>. Communication constraints were modeled probabilistically. The probability of robot i communicating with robot j was calculated as a linear function of the distance between them decreasing from 1 at a distance of 0, to 0 at a distance of R = 1.5m, and the environment width was roughly 3m. Uncertainty in the robots' velocity was modeled as white Gaussian noise with covariance of I 3 × 10 -4 m 2 /s 2 (where I 3 is the 3 × 3 identity matrix). Fig. <ref type="figure" target="#fig_5">7</ref> shows the results of a typical simulation with ten robots. The robots start in an arbitrary configuration and spread out and up so that their fields of view cover the environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper we presented a distributed control algorithm to allow hovering robots with downward facing cameras to cover an environment. The controller is proven to locally minimize a cost function representing the aggregate information per pixel of the robots over the environment, and can be used in nonconvex and disconnected environments. We implemented the algorithm on a group of three autonomous quad-rotor robots, and experimentally demonstrated robustness to unforseen robot failures. We also investigated scalability and robustness to network failures in simulations with ten flying robots. This paper is accompanied by a video showing robot experiments and simulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. APPENDIX</head><p>Proof: [Theorem 1] We can break up the domain of integration into two parts as</p><formula xml:id="formula_32">H = Q∩Bi h Nq φ(q) dq + Q\Bi h Nq φ(q) dq.</formula><p>Only the integrand in the first integral is a function of p i since the condition i ∈ N q is true if and only if q ∈ B i (from the definition of N q ). However the boundaries of both terms are functions of p i , and will therefore appear in boundary terms in the derivative. Using the standard rule for differentiating an integral, with the symbol ∂• to mean boundary of a set, we have </p><formula xml:id="formula_33">∂H ∂p i = Q∩Bi ∂h Nq ∂p i φ(q) dq + ∂(Q∩Bi) h Nq φ(q) ∂q ∂(Q∩Bi) ∂p i T n ∂(Q∩Bi) dq + ∂(Q\Bi) h Nq\{i} φ(q) ∂q ∂(Q\Bi) ∂p i T n ∂(Q\Bi) dq,<label>(18)</label></formula><p>and</p><formula xml:id="formula_35">∂H ∂z i = Q∩∂Bi (h Nq -h Nq\{i} )φ(q) • ∂q (Q∩∂Bi) ∂z i T n (Q∩∂Bi) dq - Q∩Bi 2h<label>2</label></formula><p>Nq a(bz i ) 3 φ(q) dq, <ref type="bibr" target="#b19">(20)</ref> where we used the fact that ∂h Nq /∂c i = [0 0] T , and a straightforward calculation yields ∂h Nq /∂z i = -2h 2 Nq /(a(bz i ) 3 ). Now we solve for the boundary terms, ∂q (Q∩∂Bi) ∂c i T n (Q∩∂Bi) and ∂q (Q∩∂Bi) ∂z i T n (Q∩∂Bi) , which generally can be found by implicitly differentiating the constraint that describes the boundary. Henceforth we will drop the subscript on q, but it should be understood that we are referring to points, q, constrained to lie on the set Q ∩ ∂B i . A point q on the boundary set Q ∩ ∂B i will satisfy</p><formula xml:id="formula_36">q -c i = z i tan θ,<label>(21)</label></formula><p>and the outward facing normal on the set Q ∩ B i is given by n (Q∩∂Bi) = (qc i ) qc i .</p><p>Differentiate <ref type="bibr" target="#b20">(21)</ref> implicitly with respect to c i to get</p><formula xml:id="formula_37">∂q ∂c i T -I 2 (q -c i ) = 0,</formula><p>where I 2 is the 2 × 2 identity matrix, therefore ∂q ∂c i T (qc i )</p><p>qc i = (qc i ) qc i , which gives the boundary terms for <ref type="bibr" target="#b18">(19)</ref>. Now differentiate <ref type="bibr" target="#b20">(21)</ref> implicitly with respect to z i to get ∂q ∂z i T (qc i )</p><p>qc i = tan θ, which gives the boundary term for <ref type="bibr" target="#b19">(20)</ref>. The derivative of the cost function H with respect to p i can now be written as in Theorem 1</p><p>Proof: [Theorem 3] The proof is the same as that of Theorem 1 up to the point of evaluating the boundary terms. Equations ( <ref type="formula" target="#formula_34">19</ref>) and ( <ref type="formula">20</ref>) are true. Additionally the angular component is given by ∂H ∂c i = Q∩∂Bi (h Nqh Nq\{i} )φ(q)</p><p>• ∂q (Q∩∂Bi) ∂ψ i T n (Q∩∂Bi) dq.</p><p>The constraint for points on the kth leg of the rectangular boundary is <ref type="bibr" target="#b13">14)</ref>. Differentiate this constraint implicitly with respect to c i , z i , and ψ i and solve for the boundary terms to get</p><formula xml:id="formula_38">(q -c i ) T R(ψ i ) T n k = z i tan θ T n k , from<label>(</label></formula><formula xml:id="formula_39">∂q ∂c i T R(ψ i ) T n k = R(ψ i ) T n k , ∂q ∂z i T R(ψ i ) T n k = tan θ T n k ,<label>and</label></formula><formula xml:id="formula_40">∂q ∂ψ i T R(ψ i ) T n k = -(q -c i ) T R(ψ i + π/2) T n k ,</formula><p>where we have used the fact that ∂R(ψ i )</p><formula xml:id="formula_41">∂ψ i = -sin ψ i cos ψ i -cos ψ i -sin ψ i = R(ψ i + π/2).</formula><p>Break the boundary integrals into a sum of four integrals, one integral for each edge of the rectangle. The expression in Theorem 3 follows.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. This snapshot of an experiment shows three flying quad-rotor robots moving so that their cameras cover the environment represented by the white polygon.</figDesc><graphic coords="1,377.69,366.44,114.70,118.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The camera optics and the geometry of the environment are shown in this figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. This figure shows the relevant quantities involved in characterizing the intersecting fields of view of two cameras.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. This figure shows the experimental setup. The robots positions were captured with an Vicon motion capture system. The robots used their position information to run the coverage algorithm in a distributed fashion.</figDesc><graphic coords="6,322.37,176.48,107.95,102.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Frame shots from an experiment with three AscTec Hummingbird quad-rotor robots are shown. After launching from the ground (Fig.6(a)), the three robots stabilize in an optimal configuration (Fig.6(b)). Then one robot is manually removed to simulate a failure, and the remaining two move to a new optimal position (Fig.6(c)). Finally a second robot is removed and the last one stabilizes at an optimal position (Fig.6(d)). The robots move so that their fields of view (which cannot be seen in the snapshots) cover the environment, represented by the white polygon. The cost function during the three stages of the experiment, averaged over 19 successful experiments, is shown in Fig.6(e). The error bars denote one standard deviation. The experiments demonstrate the performance of the algorithm, and its ability to adapt to unforeseen robot failures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Results of a simulation with ten robots covering a nonconvex environment are shown. The ×'s mark the robot positions and the circles represent the fields of view of their cameras. Communication failures and noise on the robots' velocities are also modeled in the simulation. The initial, middle, and final configurations are shown in 7(a), 7(b), and 7(c), respectively.</figDesc><graphic coords="7,57.95,54.08,72.09,62.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>where q ∂• is a point on the boundary of a set expressed as a function of p i , and n ∂• is the outward pointing normal vector of the boundary of the set. Decomposing the boundary further, we find that ∂(Q∩ B i ) = (∂Q ∩ B i ) ∪ (Q ∩ ∂B i ) and ∂(Q\B i ) = (∂Q\B i ) ∪ (Q ∩ ∂B i ).But points on ∂Q do not change as a function of p i , therefore we have∂q (∂Q∩Bi) ∂p i = 0 ∀q ∈ ∂Q ∩ B i and ∂q (∂Q\Bi) ∂p i = 0 ∀q ∈ ∂Q\B i .Furthermore, everywhere in the setQ ∩ ∂B i the outward facing normal of ∂(Q\B i ) is the negative of the outward facing normal of ∂(Q ∩ B i ), n ∂(Q\Bi) = -n (∂(Q∩Bi) ∀q ∈ Q ∩ ∂B i .Simplifying (18) leads to∂H ∂c i =Q∩∂Bi (h Nqh Nq\{i} )φ(q)• ∂q (Q∩∂Bi) ∂c i T n (Q∩∂Bi) dq.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The invariance principle requires 1) autonomous, continuously differentiable dynamics</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>, 2) a continuously differentiable, non-increasing Lyapunov function, 3) all evolutions of the system remain bounded.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Toward large area mosaicing for underwater scientific applications</title>
		<author>
			<persName><forename type="first">O</forename><surname>Pizarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Oceanic Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="651" to="672" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiview photometric stereo</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hérnandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vogiatzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="548" to="554" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Algorithms for cooperative multisensor surveillance</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fujiyoshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1456" to="1477" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tracking human motion in structured environments using a distributed-camera system</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1241" to="1247" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Consistent labeling of tracked objects in multiple cameras with overlapping fields of view</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1355" to="1360" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi camera image tracking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1256" to="1267" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Planning and control of mobile robots in image space from overhead cameras</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">April 18-22 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Coverage control for mobile sensing networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cortés</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karatas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bullo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="243" to="255" />
			<date type="published" when="2004-04">April 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Facility Location: A Survey of Applications and Methods, ser</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Drezner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Springer Series in Operations Research</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Maximizing visibility in nonconvex polygons: nonsmooth analysis and gradient algorithm design</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cortés</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bullo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the American Control Conference</title>
		<meeting>the American Control Conference<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06">June 2005</date>
			<biblScope unit="page" from="792" to="797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Decentralized, adaptive coverage control for networked robots</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schwager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Slotine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="357" to="375" />
			<date type="published" when="2009-03">March 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sensing and coverage for a network of heterogeneous robots</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C A</forename><surname>Pimenta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Mesquita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A S</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Decision and Control</title>
		<meeting>the IEEE Conference on Decision and Control<address><addrLine>Cancun, Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-12">December 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Motion coordination with distributed information</title>
		<author>
			<persName><forename type="first">S</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cortés</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bullo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Control Systems Magazine</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="75" to="88" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multisensor resource deployment using posterior Cramér-Rao bounds</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kirubarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bar-Shalom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Aerospace and Electronic Systems</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="399" to="416" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimal positioning of multiple cameras for object recognition using Cramér-Rao lower bound</title>
		<author>
			<persName><forename type="first">F</forename><surname>Farshidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sirouspour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kirubarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Robotics and Automation</title>
		<meeting>of the IEEE International Conference on Robotics and Automation<address><addrLine>Orlando, Florida</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="934" to="939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Aerial video surveillance and exploration</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samarasekera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hirvonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Burt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1518" to="1539" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An overview of emerging results in cooperative UAV control</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zennaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Hedrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 43rd IEEE Conference on Decision and Control</title>
		<meeting>of the 43rd IEEE Conference on Decision and Control<address><addrLine>Nassau</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="602" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cooperative vision based estimation and tracking using multiple uav&apos;s</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bethke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valenti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>How</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Cooperative Control and Optimization, ser. Lecture Notes in Control and Information Sciences</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">369</biblScope>
			<biblScope unit="page" from="179" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cooperative surveillance with multiple UAV&apos;s</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Beard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Distributed Consensus in Multi-vehicle Cooperative Control, ser. Communications and Control Engineering</title>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="265" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Distributed Control of Robotic Networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bullo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cortés</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Martínez</surname></persName>
		</author>
		<ptr target="http://coordinationbook.info" />
		<imprint>
			<date type="published" when="2008-06">June 2008</date>
		</imprint>
	</monogr>
	<note>manuscript preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Hecht</surname></persName>
		</author>
		<title level="m">Optics</title>
		<meeting><address><addrLine>Reading, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>rd ed.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Some extensions of liapunov&apos;s second method</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lasalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE Transactions on Circuit Theory</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="520" to="527" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Differential Equations, Dynamical Systems, and Linear Algebra</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Smale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974">1974</date>
			<publisher>Academic Press, Inc</publisher>
			<pubPlace>Orlando, FL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Energy-efficient autonomous four-rotor flying robot controlled at 1kHz</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gurdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stumpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Achtelik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-M</forename><surname>Doth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hirzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2007 IEEE International Conference on Robotics and Automation</title>
		<meeting>of the 2007 IEEE International Conference on Robotics and Automation<address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-04">April 2007</date>
			<biblScope unit="page" from="361" to="366" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
