<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">M-SENA: An Integrated Platform for Multimodal Sentiment Analysis</title>
				<funder ref="#_ARVx9nk">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-23">23 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Huisheng</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology(BNRist)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziqi</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology(BNRist)</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Hua</forename><surname>Xu</surname></persName>
							<email>xuhua@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology(BNRist)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenmeng</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yihe</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology(BNRist)</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Hebei University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Hebei University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">M-SENA: An Integrated Platform for Multimodal Sentiment Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-23">23 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.12441v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Epoch-Based Sample-Based Feature Visualization Evaluation Module Live-Captured Data File Upload Trained Model</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>M-SENA is an open-sourced platform for Multimodal Sentiment Analysis. It aims to facilitate advanced research by providing flexible toolkits, reliable benchmarks, and intuitive demonstrations. The platform features a fully modular video sentiment analysis framework consisting of data management, feature extraction, model training, and result analysis modules. In this paper, we first illustrate the overall architecture of the M-SENA platform and then introduce features of the core modules. Reliable baseline results of different modality features and MSA benchmarks are also reported. Moreover, we use model evaluation and analysis tools provided by M-SENA to present intermediate representation visualization, on-the-fly instance test, and generalization ability test results. The source code of the platform is publicly available at https: //github.com/thuiar/M-SENA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multimodal Sentiment Analysis (MSA) aims to judge the speaker's sentiment from video segments <ref type="bibr" target="#b29">(Mihalcea, 2012;</ref><ref type="bibr" target="#b36">Soleymani et al., 2017;</ref><ref type="bibr" target="#b18">Guo et al., 2019)</ref>. It has attracted increasing attention due to the booming of user-generated online content. Although impressive improvements have been witnessed in recent MSA researches <ref type="bibr" target="#b38">(Tsai et al., 2019;</ref><ref type="bibr" target="#b33">Rahman et al., 2020;</ref><ref type="bibr" target="#b43">Yu et al., 2021)</ref>, building an end-to-end video sentiment analysis system for real-world scenarios is still full of challenges.</p><p>The first challenge lies in effective acoustic and visual feature extraction. Most previous approaches <ref type="bibr">(Zadeh et al., 2017a;</ref><ref type="bibr" target="#b21">Hazarika et al., 2020;</ref><ref type="bibr">Han et al., 2021a)</ref> are developed on the provided modality sequences from CMU-MultimodalSDK 1 . However, reproducing exact identical acoustic and visual feature extraction is almost impossible due to the the vague description of feature selection and backbone selection (both COVAREP<ref type="foot" target="#foot_0">2</ref> and Facet<ref type="foot" target="#foot_1">3</ref> can not be directly used in Python). Moreover, recent literature <ref type="bibr" target="#b38">(Tsai et al., 2019;</ref><ref type="bibr" target="#b17">Gkoumas et al., 2021;</ref><ref type="bibr">Han et al., 2021b)</ref> observe that the text modality stands in the predominant position while acoustic and visual modalities have few contributions to the final sentiment classification. Such results further arouse the attention on effective feature extraction of acoustic and visual modalities.</p><p>With the awareness of the importance of acoustic and visual feature extraction, researchers attempt to develop models based on customized modality sequences instead of provided features <ref type="bibr" target="#b10">(Dai et al., 2021;</ref><ref type="bibr" target="#b21">Hazarika et al., 2020)</ref>. However, performance comparison with different modality features is unfair. Therefore, the demand for reliable comparison of modality features and fusion methods is increasingly urgent.</p><p>Another factor that limits the application of existing MSA models in real scenarios is the lack of comprehensive model evaluation and analysis approaches. Models obtained outstanding performance on the given test set might degrade in realworld scenarios due to the distribution discrepancy or random modality perturbations <ref type="bibr" target="#b23">(Liang et al., 2019;</ref><ref type="bibr">Zhao et al., 2021;</ref><ref type="bibr" target="#b44">Yuan et al., 2021)</ref>. Besides, effective model analysis is also crucial for researchers to explain the improvements and perform model refinement.</p><p>The Multimodal SENtiment Analysis platform (M-SENA) is developed to address the above challenges. For acoustic and visual features, the platform integrates Librosa <ref type="bibr">(McFee et al., 2015)</ref>, OpenSmile <ref type="bibr" target="#b15">(Eyben et al., 2010)</ref>, OpenFace <ref type="bibr" target="#b3">(Baltrusaitis et al., 2018)</ref>, MediaPipe <ref type="bibr" target="#b26">(Lugaresi et al., 2019)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Utilities</head><p>Logger Metrics 1. By providing a highly customized feature extraction toolkit, the platform familiarizes researchers with the composition of modality features. Also, the platform bridges the gap between designing MSA models with provided, fixed modality features and building a real-world video sentiment analysis system.</p><p>2. The unified MSA pipeline guarantees fair comparison between different combinations of modality features and fusion models.</p><p>3.  It provides a graphical web interface as well as Python packages for researchers with all features above. The platform currently supports three popular MSA datasets across two languages, seven feature extraction backbones, and fourteen benchmark MSA models. Figure <ref type="figure" target="#fig_0">1</ref> illustrates the overall architecture of the M-SENA platform. In the remaining parts of this section, features of each module in Figure <ref type="figure" target="#fig_0">1</ref> will be described in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Management Module</head><p>The data management module is designed to ease the access of multimedia data on servers. Besides providing existing benchmark datasets, the module also enables researchers to build and manage their own datasets.</p><p>Benchmark Datasets. M-SENA currently supports three benchmark MSA datasets, including CMU-MOSI <ref type="bibr" target="#b49">(Zadeh et al., 2016)</ref>, CMU-MOSEI <ref type="bibr">(Zadeh et al., 2018c)</ref> in English, and CH-SIMS <ref type="bibr" target="#b42">(Yu et al., 2020)</ref>   <ref type="bibr" target="#b14">(Eyben et al., 2015)</ref> Static (LLDs) wav2vec2.0 <ref type="bibr" target="#b1">(Baevski et al., 2020)</ref> Learnable</p><p>Visual Feature Sets Facial Landmarks <ref type="bibr">(Zadeh et al., 2017b</ref>) Static Eyes Gaze <ref type="bibr" target="#b40">(Wood et al., 2015)</ref> Static Action Unit <ref type="bibr" target="#b2">(Baltru?aitis et al., 2015)</ref> Static</p><p>Textual Feature Sets GloVe6B <ref type="bibr" target="#b30">(Pennington et al., 2014)</ref> Static BERT <ref type="bibr" target="#b11">(Devlin et al., 2018)</ref> Learnable RoBerta <ref type="bibr" target="#b24">(Liu et al., 2019)</ref> Learnable</p><p>Table <ref type="table">1</ref>: Some of the supported features in M-SENA.</p><p>and view raw videos conveniently without downloading them to the local environment.</p><p>Building Private Datasets. The M-SENA platform also provides a graphical interface for researchers to construct their own datasets using uploaded videos. Following the literature <ref type="bibr" target="#b42">(Yu et al., 2020)</ref>, M-SENA supports unimodal sentiment labelling along with multimodal sentiment labelling.</p><p>The constructed datasets can be directly used for model training and evaluation on the platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature Extraction Module</head><p>Emotion-bearing modality feature extraction is still an open challenge for MSA tasks. To facilitate effective modality feature extraction for MSA, M-SENA integrates seven most commonly used feature extraction tools and provides a unified Python API as well as a graphical interface. Part of the supported features for each modality are listed in Table <ref type="table">1</ref> and described below: Acoustic Modality. Various acoustic features have been proven effective for emotion recognition (El <ref type="bibr" target="#b13">Ayadi et al., 2011;</ref><ref type="bibr" target="#b0">Ak?ay and Oguz, 2020)</ref>.</p><p>Hand-crafted acoustic features can be divided into two classes, low level descriptors (LLDs), and high level statistics functions (HSFs). LLDs features, including prosodies, spectral domain features and others, are calculated on a frame-basis, while HSFs features are calculated on an entire utterance level. In addition to the hand-crafted features, M-SENA also provides pretrained acoustic model wav2vec2.0 <ref type="bibr" target="#b1">(Baevski et al., 2020)</ref> as a learnable feature extractor. Researchers can also design and build their own customized acoustic features using the provided Librosa extractor. Visual Modality. In existing MSA research, facial Landmarks, eyes gaze, and facial action units are Types Scenarios Films(TV)</p><p>Variety Show Life(Vlog) Easy 10 (en:4 ch:6) 8 (en:4 ch:4) 8 (en:4 ch:4) Common 9 (en:4 ch:5) 11 (en:6 ch:5) 8 (en:4 ch:4) Difficult 9 (en:4 ch:5) 9 (en:5 ch:4) 8 (en:4 ch:4) Noise 9 (en:4 ch:5) 8 (en:4 ch:4) 7 (en:2 ch:5) Missing 9 (en:4 ch:5) 9 (en:5 ch:4) 7 (en:3 ch:4)</p><p>Table <ref type="table">2</ref>: Statistics of the generalization ability test dataset, where "en" represents "English", "ch" represents "Chinese".</p><p>commonly used visual features. The M-SENA platform enables researchers to extract visual feature combinations flexibly using OpenFace and Medi-aPipe extractors.</p><p>Text Modality. Compared with acoustic and visual features, semantic text embeddings are much more mature with the rapid development of pretrained language models <ref type="bibr" target="#b32">(Qiu et al., 2020)</ref>. Following previous works <ref type="bibr">(Zadeh et al., 2017a;</ref><ref type="bibr" target="#b33">Rahman et al., 2020;</ref><ref type="bibr" target="#b22">Lian et al., 2022)</ref>, M-SENA supports GloVe6B <ref type="bibr" target="#b30">(Pennington et al., 2014)</ref>, pretrained BERT <ref type="bibr" target="#b11">(Devlin et al., 2018)</ref>, and pretrained RoBerta <ref type="bibr" target="#b24">(Liu et al., 2019)</ref> as textual feature extractors. All feature extractors above are available through both Python API and Graphical User Interface(GUI). Listing 1 shows a simple example of default acoustic feature extraction using Python API. The process is similar for other modalities. Advanced usage and detailed documentation is available at Github Wiki<ref type="foot" target="#foot_2">4</ref> .</p><p>1 from MSA_FET import FeatureExtractionTool </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model Training Module</head><p>M-SENA provides a unified training module which currently integrates 14 MSA benchmarks, including tensor fusion methods, TFN <ref type="bibr">(Zadeh et al., 2017a)</ref>, LMF <ref type="bibr" target="#b25">(Liu et al., 2018)</ref>, modality factorization methods, MFM <ref type="bibr" target="#b39">(Tsai et al., 2018)</ref>, MISA <ref type="bibr" target="#b21">(Hazarika et al., 2020)</ref>, SELF-MM <ref type="bibr" target="#b43">(Yu et al., 2021)</ref>, word-level fusion methods, MulT <ref type="bibr" target="#b38">(Tsai et al., 2019)</ref>, BERT-MAG <ref type="bibr" target="#b33">(Rahman et al., 2020)</ref>, marks and action units [V3] for visual modality comparison. Besides, we also report the model performances using the modality features provided in CMU-MultimodalSDK.</p><formula xml:id="formula_0">Feature Combinations TFN GMFN MISA Bert-MAG Acc-2 (%) F1 (%) Acc-2 (%) F1 (%) Acc-2 (%) F1 (%) Acc-2 (%) F1 (%) CMU-</formula><p>Table <ref type="table" target="#tab_4">3</ref> shows the experiment results for feature selection. For Bert-MAG which is designed upon the Bert backbone, experiments are conducted only for Bert as text feature. It can be observed that, in most cases, using appropriate features instead of original features in CMU-MultimodalSDK helps to improve model performance. For textual modality, Roberta feature performs best for TFN and GMFN model, while Bert feature performs best for MISA model. For acoustic modality, wav2vec2.0 embeddings (without finetune) perform best for GMFN and Bert-MAG model. According to literature <ref type="bibr" target="#b9">(Chen and Rudnicky, 2021;</ref><ref type="bibr" target="#b31">Pepino et al., 2021)</ref>, finetuning wav2vec2.0 can further improve model performance which might provide more effective acoustic features for future MSA research. For Visual modality, the combination of facial landmarks and action units achieves the overall best result, revealing the effectiveness of both landmarks and action units for sentiment classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MSA Benchmark Comparison</head><p>Experiment results of benchmark MSA models are shown in Table <ref type="table" target="#tab_5">4</ref>. All models are improved using Bert as text embeddings while using original acoustic and visual features provided in CMU-MultimodalSDK. Besides recording reliable benchmark results, the M-SENA platform also provides researchers with a convenient approach to reproduce the benchmarks. Again, both GUI and Python API are available. We show an example of the proposed Python API in Listing 2. Detailed and Advanced usage is included in our documentation at Github<ref type="foot" target="#foot_3">5</ref> . We will continuously catch up on new MSA approaches and update their performances. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model Analysis Demonstration</head><p>This section demonstrates model analysis results using the M-SENA platform. Intermediate result analysis is presented in Section 4.1, on-the-fly instance analysis is shown in Section 4.2, and generalization ability analysis is illustrated in Section 4.3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Intermediate Result Analysis</head><p>The intermediate result analysis submodule is designed to monitor and visualize the training process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">On-the-fly Instance Analysis</head><p>M-SENA enables researchers to validate the proposed MSA approaches using uploaded or liverecorded instances. Figure <ref type="figure" target="#fig_2">3</ref> presents an example of the live demonstration. Besides model prediction results, the platform also provides feature visualization, including short-time Fourier transform (STFT) for acoustic modality and facial landmarks, eye gaze, head poses for visual modality. We will continuously update the demonstration to make it a even more intuitive and playable MSA model evaluation tool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Generalization Ability Analysis</head><p>We utilized the model trained on MOSI dataset with [T1]-[A1]-[V3] modality features in Section 3.1 for generalization ability test. Experimental results are reported in Table <ref type="table" target="#tab_6">5</ref>. It can be concluded that all models present a performance gap between Visual Modality: Landmarks, head pose.</p><p>Acoustic Modality: STFT.</p><p>Text Modality: Transcript.</p><p>this is great news, but i am not happy about it.  original test set and real-world scenarios, especially for the instances with noisy or missing modalities. Another observation is that the noisy instances are usually more challenging than modality missing for MSA models, revealing that noisy modality feature is worse than none at all. In the future, for the demand of real-world applications, MSA researchers may consider analyzing model robustness as well as performances on the test set, and design a more robust MSA model against random modality noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Predictions:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Works</head><p>To the best of our knowledge, there are two widely used open-source repositories from CMU team 6 and SUTD team 7 . Both of them provide tools to load well-known MSA datasets and implement several benchmarks methods. So far, their works have attracted considerable attention and facilitated the birth of new MSA models such as MulT <ref type="bibr" target="#b38">(Tsai et al., 2019)</ref> and MMIM <ref type="bibr">(Han et al., 2021b)</ref>.</p><p>In this paper, we propose M-SENA, compared to previous works, the M-SENA platform is novel from the following aspects. For data management, previous work directly loads the extracted features, while the M-SENA platform focuses on intuitive raw video demonstration, and provides user with a convenient means for private dataset construction. For modality features, M-SENA platform first provides user-customized feature extraction toolkit and a transparent feature extraction process. Following the tutorial, Users can easily reproduce the feature extraction steps and develop their research on designed feature set. For model training, the M-SENA platform first utilizes a unified MSA framework and provide an easy-to-reproduce model training API integrating fourteen MSA benchmarks on three popular MSA dataset. For model evaluation, the M-SENA is the first MSA platform consisting of comprehensive evaluation means stressing model robustness for real-world scenarios, which aims to bridge the gap between MSA research and applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this work, we introduce M-SENA, an integrated platform that contains step-by-step recipes for data management, feature extraction, model training, and model analysis for MSA researchers. The platform evaluates MSA model in an end-to-end manner and reports reliable benchmark results for future research. Moreover, we further investigate comprehensive model evaluation and analysis methods and provide a series of user-friendly visualization and demonstration tools including intermediate representation visualization, on-the-fly instance test, and generalization ability test. In the future, we will continuously catch up on advanced MSA research progress and update new benchmarks on the M-SENA platform. MTFN. The Multi-Task Tensor Fusion Network <ref type="bibr" target="#b42">(Yu et al., 2020)</ref> calculates a multi-dimensional tensor (based on outer product) to capture uni-, bi-, and tri-modal interactions through unimodal labels training.</p><p>MLMF. The Multi-Task Low-rank Multimodal Fusion <ref type="bibr" target="#b42">(Yu et al., 2020)</ref> is an improvement over MTFN, where low-rank multimodal tensors fusion technique is performed to improve efficiency through unimodal labels training.</p><p>Self_MM. The Self-Supervised Multi-Task Multimodal <ref type="bibr" target="#b43">(Yu et al., 2021)</ref> design a label generation module based on the self-supervised learning strategy to acquire independent unimodal supervisions, which can balance the learning progress among different sub-tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Generalization Ability Test Datasets</head><p>The examples of the proposed generalization ability test dataset are shown in Figure <ref type="figure" target="#fig_8">4</ref>.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The overall framework of the M-SENA platform contains four main modules: data management module, feature extraction module, model training module and model evaluation module.</figDesc><graphic url="image-10.png" coords="2,412.78,319.37,89.92,54.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>platform features convenient data access, customized feature extraction, unified model training pipeline, and comprehensive model evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 3 #</head><label>3</label><figDesc>Extract Audio Feature for MOSI. 4 fet = FeatureExtractionTool("librosa") 5 6 feature = fet.run_dataset( 7 dataset_dir='~/MOSI', 8 out_file='output/feature.pkl' 9 ) Listing 1: An example of acoustic feature extraction on the MOSI dataset using MMSA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>User Designed Hyper-parameter. 10 config['post_fusion_dim'] = 32 11 12 # Modality Feature Selection. 13 config['featurePath'] = 'feature.pkl' An example to train model with M-SENA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Intermediate Result Analysis for TFN model trained on MOSI dataset.</figDesc><graphic url="image-11.png" coords="6,70.87,70.86,218.27,188.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure 2 shows an example of training TFN model on MOSI dataset. Epoch results of binary accuracy, f1-score and loss value are plotted. Moreover, the learned multimodal fusion representations are illustrated in an interactive 3D figure with the aim of helping users gain a better intuition about the multimodal feature representations and the fusion process. Unimodal representations of text, acoustic, and visual are also shown for models containing explicit unimodal representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: On-the-fly instance test example. The M-SENA platform also provides real-time modality feature visualization along with the model prediction results.</figDesc><graphic url="image-20.png" coords="6,397.05,235.43,118.41,57.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>I</head><label></label><figDesc>really think she just wanted love and to be loved and ? Tag: Difficult ?Vlog?English?Female?Negative ????????????? Tag: Video Missing?Variety Show?Chinese?Female?Negative ---Video missing ---And we have to work through that and understand that what we're doing is something that. Tag: Video Missing ?Variety Show?English?Male?Neutral ---Video missing ---????????????????? Tag: Difficult?Vlog?Chinese?Male?Negative I already lost my family once! Tag: Environment Noise ?TV?English?Female?Negative Environmental noise ??????????????????? Tag: Background music Noise ?TV?Chinese?Male?Negative Background music noise</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Examples of the constructed generalization ability test dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>On-the-fly Instance Test MySQL DB Data Management Module Feature Extraction Module Benchmark Dataset Management</head><label></label><figDesc>and provides a highly customized feature extraction API in Python. With the modular MSA pipeline, fair comparison between different features</figDesc><table><row><cell>Multimedia Data</cell><cell>Multimodal Labels</cell></row><row><cell></cell><cell>Label_M: Positive</cell></row><row><cell></cell><cell>Label_T: Neutral</cell></row><row><cell></cell><cell>Label_A: Positive</cell></row><row><cell></cell><cell>Label_V: Positive</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Text Transcript End-to-End Video Sentiment Analysis Pipeline Post-Processing Model Class Trainer Class Generalization Ability Test</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>Pre-Processing</cell><cell></cell><cell>Config</cell><cell></cell><cell></cell></row><row><cell>Text</cell><cell cols="2">Word Tokenization</cell><cell></cell><cell></cell><cell>TFN</cell></row><row><cell>Audio Video &amp;</cell><cell cols="2">Sampling Format Conversion</cell><cell>Base Config</cell><cell>Wrapper Model</cell><cell>Bert-MAG MulT LMF LF_DNN</cell></row><row><cell>Video</cell><cell cols="2">Activate Speaker Detection</cell><cell>Model</cell><cell></cell><cell>MISA Self-MM</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Config</cell><cell></cell><cell>MTFN</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>???</cell></row><row><cell cols="3">Backbone Extractors</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Text</cell><cell>Audio</cell><cell>Video</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT RoBERTa</cell><cell>Librosa OpenSMILE</cell><cell>OpenFace Mediapipe</cell><cell>Training Config</cell><cell>Trainer Wrapper</cell><cell>Initialization Train Pipeline Test Pipeline</cell></row><row><cell>Glove</cell><cell>Wav2Vec2</cell><cell>VGGFace2</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Other Config</cell><cell></cell><cell>Trained</cell><cell>Stand-Alone Test Set</cell></row><row><cell>Truncate</cell><cell>Padding</cell><cell>Packaging</cell><cell></cell><cell></cell><cell>Model</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Private Dataset Construction</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Upload Videos</cell><cell>Labeling</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results for feature selection. For text, [T1] refers to BERT, [T2] refers to GloVe6B, [T3] refers to RoBerta. For acoustic, [A1] refers to eGeMAPS, [A2] refers to customized feature including 20-dim MFCC, 12-dim CQT, and f0, [A3] refers to wav2vec2.0. For visual, [V1] refers to action units, [V2] refers to landmarks, [V3] refers to both landmarks and action units. CMU-SDK ? refers to modified CMU-SDK features with BERT for text. Compared to the provided test set of benchmark MSA datasets, realworld scenarios are often more complicated. Future MSA models need to be robust against modality noise as well as effective on the test set. Driven by the demand from real-world applications and observations, the M-SENA platform provides a generalization ability test dataset (consists of 68 Chinese and 61 English samples), simulating as many complicated and diverse real-world scenarios as possible. The statistics of the proposed dataset is shown in Table2. In general, the dataset contains three scenarios and five instance types. Specifically, the three scenarios refers to films, variety shows, and user-uploaded vlogs, while the five instance types refer to easy samples, common samples, difficult samples, samples with modality noise, samples with modality missing. In addition, the dataset is balanced in terms of gender and scenario to avoid irrelevant factors. Examples of the generalization ability test dataset are shown in Appendix C.</figDesc><table><row><cell>SDK  ?</cell><cell>78.02</cell><cell>78.09</cell><cell>76.98</cell><cell>77.06</cell><cell>82.96</cell><cell>82.98</cell><cell>83.41</cell><cell>83.47</cell></row><row><cell>[T1]-[A1]-[V1]</cell><cell>77.41</cell><cell>77.47</cell><cell>77.77</cell><cell>77.84</cell><cell>83.78</cell><cell>83.80</cell><cell>83.38</cell><cell>83.43</cell></row><row><cell>[T2]-[A1]-[V1]</cell><cell>70.40</cell><cell>70.51</cell><cell>71.40</cell><cell>71.54</cell><cell>75.22</cell><cell>75.68</cell><cell>-</cell><cell>-</cell></row><row><cell>[T3]-[A1]-[V1]</cell><cell>80.85</cell><cell>80.79</cell><cell>80.21</cell><cell>80.15</cell><cell>79.57</cell><cell>79.67</cell><cell>-</cell><cell>-</cell></row><row><cell>[T1]-[A2]-[V1]</cell><cell>76.80</cell><cell>76.82</cell><cell>78.02</cell><cell>78.03</cell><cell>83.72</cell><cell>83.72</cell><cell>82.96</cell><cell>83.04</cell></row><row><cell>[T1]-[A3]-[V1]</cell><cell>77.19</cell><cell>77.23</cell><cell>78.44</cell><cell>78.45</cell><cell>82.16</cell><cell>82.23</cell><cell>83.57</cell><cell>83.58</cell></row><row><cell>[T1]-[A1]-[V2]</cell><cell>77.38</cell><cell>77.48</cell><cell>78.81</cell><cell>78.71</cell><cell>83.2</cell><cell>83.14</cell><cell>82.13</cell><cell>82.20</cell></row><row><cell>[T1]-[A1]-[V3]</cell><cell>76.74</cell><cell>76.81</cell><cell>78.23</cell><cell>78.24</cell><cell>84.06</cell><cell>84.08</cell><cell>83.69</cell><cell>83.75</cell></row><row><cell cols="2">2.4 Result Analysis Module</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">The proposed M-SENA platform provides compre-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">hensive model evaluation tools including interme-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">diate result visualization, on-the-fly instance test,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">and generalization ability test. A brief introduction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">of each component is given below, while a detailed</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">demonstration is shown in Section 4.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Intermediate Result Visualization. The discrim-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">ination of multimodal representations is one of the crucial metrics for the evaluation of different fusion methods. The M-SENA platform records the final multimodal fusion results and illustrates them after decomposition with Principal Component Analysis (PCA). Training loss, binary accuracy, F1 score curves are also provided in M-SENA for detailed analysis. Live Demo Module. In the hope of bridging the gap between MSA research and real-world video sentiment analysis scenarios, M-SENA provides a live demo module, which performs on-the-fly instance tests. Researchers can validate the effec-tiveness and robustness of the selected MSA model by uploading or live-feeding videos to the platform. Generalization Ability Test. 3 Experiments on M-SENA In this section, we report experiments conducted on the M-SENA platform. Comparison of different modality features are shown in Section 3.1, and comparison of different fusion models are shown in Section 3.2. All reported results are the mean performances of five different seeds. 3.1 Feature Selection Comparison In the following experiments, we take BERT [T1], eGeMAPS (LLDs) [A1], and Action Unit [V1] as default modality features, and compare them with the other six feature sets. Specifi-cally, we utilize GloVe6B [T2], RoBerta [T3] for text modality comparison; customized acous-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">tic feature[A2](including 20 dimensional MFCC,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">12 dimensional CQT, and 1 dimensional f0),</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">wav2vec2.0 features [A3] for acoustic modality</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">comparison; facial landmarks [V2], facial land-</cell></row></table><note><p><p><p><p><p>multi-view learning methods: MFN</p>(Zadeh et al.,  2018a)</p>, GMFN</p>(Zadeh et al., 2018c)</p>, and other MSA methods. Detailed introduction of the integrated baseline methods is provided in Appendix B. We will continue following advanced MSA benchmarks and put our best effort into providing reliable benchmark results for future MSA research.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Experiment results for MSA benchmark comparison. All models utilize the Bert embedding and the provided acoustic and visual features in CMU-MultimodalSDK. Due to the requirement of unimodal labels, multitask models, including MLF_DNN, MTFN, and MLMF, are tested on SIMS only.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Results for English generalization ability test. Binary accuracy and F1 scores are reported to show the effectiveness and robustness of the model.</figDesc><table><row><cell>Types</cell><cell>TFN Acc-2 / F1</cell><cell>GMFN Acc-2 / F1</cell><cell>MISA Acc-2 / F1</cell><cell>Bert-MAG Acc-2 / F1</cell></row><row><cell>Easy</cell><cell>83.3 / 84.4</cell><cell>75.0 / 76.1</cell><cell>75.0 / 76.7</cell><cell>66.7 / 66.7</cell></row><row><cell>Common</cell><cell>71.4 / 74.5</cell><cell>85.7 / 82.3</cell><cell>71.4 / 75.8</cell><cell>78.6 / 78.6</cell></row><row><cell>Difficult</cell><cell>69.2 / 69.2</cell><cell>61.5 / 60.5</cell><cell>53.9 / 54.4</cell><cell>84.6 / 84.6</cell></row><row><cell>Noise</cell><cell>60.0 / 50.5</cell><cell>50.0 / 44.9</cell><cell>50.0 / 35.7</cell><cell>60.0 / 51.7</cell></row><row><cell>Missing</cell><cell>63.6 / 60.6</cell><cell>81.8 / 77.8</cell><cell>63.6 / 60.6</cell><cell>63.6 / 61.5</cell></row><row><cell>Avg</cell><cell>70.0 / 68.4</cell><cell>71.7 / 69.3</cell><cell>63.3 / 62.4</cell><cell>71.7 / 69.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc><ref type="bibr" target="#b33">Rahman et al., 2020)</ref> is an improvement over RAVEN on aligned data with applying multimodal adaptation gate at different layers of the BERT backbone. MISA. The Modality-Invariant and -Specific Representations<ref type="bibr" target="#b21">(Hazarika et al., 2020)</ref> is made up of a combination of losses including similarity loss, orthogonal loss, reconstruction loss and prediction loss to learn modality-invariant and modalityspecific representation. MFM. The Multimodal Factorization Model<ref type="bibr" target="#b39">(Tsai et al., 2018</ref>) is a robust model, which can learn multimodal-discriminative and modality-specific generative factors, then reconstructs missing reconstruct missing modalities by adjusting for independent factors. MLF_DNN. The Multi-Task Late Fusion Deep Neural Network<ref type="bibr" target="#b42">(Yu et al., 2020)</ref> first extracts modality features separately and performs late fusion strategy for final predictions through unimodal labels training.</figDesc><table><row><cell>Jinming Zhao, Ruichen Li, and Qin Jin. 2021. Missing</cell><cell>MFN, which can change the fusion structure dy-</cell></row><row><cell>modality imagination network for emotion recogni-tion with uncertain missing modalities. In Proceed-ings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th Interna-</cell><cell>namically to obtain the interaction between the modalities and improve the interpretability. MulT. The Multimodal Transformer (MulT) (Tsai</cell></row><row><cell>tional Joint Conference on Natural Language Pro-</cell><cell>et al., 2019) extends multimodal transformer ar-</cell></row><row><cell>cessing (Volume 1: Long Papers), pages 2608-2618.</cell><cell>chitecture with directional pairwise cross-modal</cell></row><row><cell>A Integrated Datasets</cell><cell>attention which translates one modality to another</cell></row><row><cell>CMU-MOSI. The MOSI (Zadeh et al., 2016)</cell><cell>using directional pairwise cross-attention. BERT-MAG. The Multimodal Adaptation Gate</cell></row><row><cell>dataset is a widely-used dataset that consists of</cell><cell>for Bert (MAG-BERT) (</cell></row><row><cell>a collection of 2,199 video segments from 93</cell><cell></cell></row><row><cell>YouTube movie review videos.</cell><cell></cell></row><row><cell>CMU-MOSEI. The MOSEI (Zadeh et al., 2018c)</cell><cell></cell></row><row><cell>dataset expands the MOSI dataset by enlarging the</cell><cell></cell></row><row><cell>number of utterances and enriching the variety of</cell><cell></cell></row><row><cell>samples, speakers, and topics. For both MOSI</cell><cell></cell></row><row><cell>and MOSEI datasets, instances are annotated with</cell><cell></cell></row><row><cell>a sentiment intensity score ranging from -3 to 3</cell><cell></cell></row><row><cell>(strongly negative to strongly positive).</cell><cell></cell></row><row><cell>CH-SIMS. The SIMS dataset (Yu et al., 2020) is a</cell><cell></cell></row><row><cell>Chinese unimodal and multimodal sentiment anal-</cell><cell></cell></row><row><cell>ysis dataset. It contains 2,281 refined video seg-</cell><cell></cell></row><row><cell>ments in the wild with both multimodal and inde-</cell><cell></cell></row><row><cell>pendent unimodal annotations of a sentiment inten-</cell><cell></cell></row><row><cell>sity score ranging from -1 to 1 (negative to positive,</cell><cell></cell></row><row><cell>the score interval is 0.2).</cell><cell></cell></row><row><cell>B Integrated Benchmarks</cell><cell></cell></row><row><cell>LF-DNN. The Late Fusion Deep Neural Network</cell><cell></cell></row><row><cell>(Cambria et al., 2017) first extracts modality fea-</cell><cell></cell></row><row><cell>tures separately and performs late fusion strategy</cell><cell></cell></row><row><cell>for final predictions.</cell><cell></cell></row><row><cell>EF-LSTM. The Early Fusion Long-Short Term</cell><cell></cell></row><row><cell>Memory (Cambria et al., 2017) is based on input-</cell><cell></cell></row><row><cell>level feature fusion and conducts Long-Short Term</cell><cell></cell></row><row><cell>Memory (LSTM) to learn multimodal representa-</cell><cell></cell></row><row><cell>tions.</cell><cell></cell></row><row><cell>TFN. The Tensor Fusion Network (TFN) (Zadeh</cell><cell></cell></row><row><cell>et al., 2017a) calculates a multi-dimensional tensor</cell><cell></cell></row><row><cell>(based on outer product) to capture uni-, bi-, and</cell><cell></cell></row><row><cell>tri-modal interactions.</cell><cell></cell></row><row><cell>LMF. The Low-rank Multimodal Fusion (LMF)</cell><cell></cell></row><row><cell>(Liu et al., 2018) is an improvement over TFN,</cell><cell></cell></row><row><cell>where the low-rank multimodal tensors fusion tech-</cell><cell></cell></row><row><cell>nique is performed to improve efficiency.</cell><cell></cell></row><row><cell>MFN. The Memory Fusion Network (MFN)</cell><cell></cell></row><row><cell>(Zadeh et al., 2018a) accounts for continuously</cell><cell></cell></row><row><cell>modeling the view specific and cross-view interac-</cell><cell></cell></row><row><cell>tions and summarizing them through time with a</cell><cell></cell></row><row><cell>Multi-view Gated Memory.</cell><cell></cell></row><row><cell>Graph-MFN. The Graph Memory Fusion Net-</cell><cell></cell></row><row><cell>work (Zadeh et al., 2018c) is an improvement of</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://github.com/covarep/covarep</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://imotions.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://github.com/thuiar/MMSA-FET/wiki</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://github.com/thuiar/MMSA/wiki</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>https://github.com/A2Zadeh/CMU-MultimodalSDK</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>https://github.com/declare-lab/multimodal-deeplearning</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This paper is funded by The <rs type="funder">National Natural Science Foundation of China</rs> (Grant No. <rs type="grantNumber">62173195</rs>) and <rs type="projectName">Beijing Academy of Artificial Intelligence (BAAI</rs>). The authors thank the anonymous reviewers for their valuable suggestions.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_ARVx9nk">
					<idno type="grant-number">62173195</idno>
					<orgName type="project" subtype="full">Beijing Academy of Artificial Intelligence (BAAI</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers</title>
		<author>
			<persName><forename type="first">Mehmet</forename><surname>Berkehan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ak?ay</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Kaya</forename><surname>Oguz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="56" to="76" />
		</imprint>
	</monogr>
	<note>Speech Communication</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11477</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cross-dataset learning and personspecific normalisation for automatic action unit detection</title>
		<author>
			<persName><forename type="first">Tadas</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marwa</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Openface 2.0: Facial behavior analysis toolkit</title>
		<author>
			<persName><forename type="first">Tadas</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 13th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sentiment analysis in an affective intelligent tutoring system</title>
		<author>
			<persName><forename type="first">Mar?a</forename><surname>Luc?a Barr?n-Estrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramon</forename><surname>Zatarain-Cabada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ra?l</forename><surname>Oramas-Bustillos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Gonz?lez-Hern?ndez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 17th international conference on advanced learning technologies (ICALT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="394" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mixedemotions: An open-source toolbox for multimodal emotion analysis</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Buitelaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sapna</forename><surname>Negi</surname></persName>
		</author>
		<author>
			<persName><surname>Mihael Arcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrejs</forename><surname>John P Mccrae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cecile</forename><surname>Abele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Robin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Housam</forename><surname>Andryushechkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hesam</forename><surname>Ziad</surname></persName>
		</author>
		<author>
			<persName><surname>Sagha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2454" to="2465" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Benchmarking multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><surname>Subramanyam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics and Intelligent Text Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="166" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An emotion recognition system for monitoring shopping experience</title>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Ceccacci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Generosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Giraldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maura</forename><surname>Mengoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th PErvasive Technologies Related to Assistive Environments Conference</title>
		<meeting>the 11th PErvasive Technologies Related to Assistive Environments Conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="102" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Twitter vigilance: a multi-user platform for cross-domain twitter data analytics, nlp and sentiment analysis</title>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Cenni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Nesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianni</forename><surname>Pantaleo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imad</forename><surname>Zaza</surname></persName>
		</author>
		<ptr target="SmartWorld/S-CALCOM/UIC/ATC/CBDCom/IOP/SCI" />
	</analytic>
	<monogr>
		<title level="m">Scalable Computing &amp; Communications, Cloud &amp; Big Data Computing, Internet of People and Smart City Innovation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Exploring wav2vec 2.0 fine-tuning for improved speech emotion recognition</title>
		<author>
			<persName><forename type="first">Li-Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rudnicky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.06309</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Multimodal end-to-end sparse model for emotion recognition</title>
		<author>
			<persName><forename type="first">Wenliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Cahyawijaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.09666</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Principal components analysis</title>
		<author>
			<persName><surname>George H Dunteman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Sage</publisher>
			<biblScope unit="volume">69</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Survey on speech emotion recognition: Features, classification schemes, and databases. Pattern recognition</title>
		<author>
			<persName><forename type="first">Moataz</forename><forename type="middle">El</forename><surname>Ayadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">S</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fakhri</forename><surname>Karray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="572" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The geneva minimalistic acoustic parameter set (gemaps) voice research and affective computing</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bj?rn</forename><forename type="middle">W</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Sundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisabeth</forename><surname>Andr?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><forename type="middle">Y</forename><surname>Devillers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Epps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petri</forename><surname>Laukka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Et</forename><surname>Shrikanth S Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on affective computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="190" to="202" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Opensmile: the munich versatile and fast open-source audio feature extractor</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>W?llmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bj?rn</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM international conference on Multimedia</title>
		<meeting>the 18th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1459" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mazajak: An online arabic sentiment analyser</title>
		<author>
			<persName><forename type="first">Ibrahim</forename><surname>Abu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farha</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Walid</forename><surname>Magdy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Arabic Natural Language Processing Workshop</title>
		<meeting>the Fourth Arabic Natural Language Processing Workshop</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="192" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">What makes the difference? an empirical comparison of fusion strategies for multimodal language analysis</title>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Gkoumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiuchi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="184" to="197" />
		</imprint>
	</monogr>
	<note>Information Fusion</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep multimodal representation learning: A survey</title>
		<author>
			<persName><forename type="first">Wenzhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="63373" to="63394" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Louis-philippe Morency, and Soujanya Poria. 2021a. Bi-bimodal modality fusion for correlationcontrolled multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 International Conference on Multimodal Interaction</title>
		<meeting>the 2021 International Conference on Multimodal Interaction</meeting>
		<imprint>
			<biblScope unit="page" from="6" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">2021b. Improving multimodal fusion with hierarchical mutual information maximization for multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Misa: Modality-invariant andspecific representations for multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1122" to="1131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Smin: Semi-supervised multi-modal interaction network for conversational emotion recognition</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning representations from imperfect time series data via tensor rank regularization</title>
		<author>
			<persName><forename type="first">Paul Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01011</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient lowrank multimodal fusion with modality-specific factors</title>
		<author>
			<persName><forename type="first">Zhun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Bharadhwaj Lakshminarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirali</forename><surname>Bagher Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2247" to="2256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Camillo</forename><surname>Lugaresi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuqiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadon</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Mcclanahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esha</forename><surname>Uboweja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuo-Ling</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><forename type="middle">Guang</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhyun</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08172</idno>
		<title level="m">Mediapipe: A framework for building perception pipelines</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kael</forename><surname>Rowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piali</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Wolk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thuvan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Czerwinski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12133</idno>
		<title level="m">A multimodal emotion sensing platform for building emotion-aware applications</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Eric Battenberg, and Oriol Nieto. 2015. librosa: Audio and music signal analysis in python</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><surname>Mcvicar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th python in science conference</title>
		<meeting>the 14th python in science conference</meeting>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis</title>
		<meeting>the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Pepino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Riera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luciana</forename><surname>Ferrer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.03502</idno>
		<title level="m">Emotion recognition from speech using wav2vec 2.0 embeddings</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pre-trained models for natural language processing: A survey</title>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yige</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science China Technological Sciences</title>
		<imprint>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Integrating multimodal information in large pretrained transformers</title>
		<author>
			<persName><forename type="first">Wasifur</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Kamrul Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangwu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirali</forename><surname>Bagher Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengfeng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Hoque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2359" to="2369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The interspeech 2016 computational paralinguistics challenge: Deception, sincerity &amp; native language</title>
		<author>
			<persName><forename type="first">Bj?rn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Steidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Batliner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Judee K Burgoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Baird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Elkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduardo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keelan</forename><surname>Coutinho</surname></persName>
		</author>
		<author>
			<persName><surname>Evanini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17TH Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2001" to="2005" />
		</imprint>
	</monogr>
	<note>Interspeech 2016</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The interspeech 2016 computational paralinguistics challenge: Deception, sincerity &amp; native language</title>
		<author>
			<persName><forename type="first">Bj?rn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Steidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Batliner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Judee K Burgoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Baird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Elkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduardo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keelan</forename><surname>Coutinho</surname></persName>
		</author>
		<author>
			<persName><surname>Evanini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17TH Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2001" to="2005" />
		</imprint>
	</monogr>
	<note>Interspeech 2016</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A survey of multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bj?rn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="3" to="14" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multisense-context-aware nonverbal behavior analysis framework: A psychological distress use case</title>
		<author>
			<persName><forename type="first">Giota</forename><surname>Stratou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="190" to="203" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multimodal transformer for unaligned multimodal language sequences</title>
		<author>
			<persName><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zico Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference. Association for Computational Linguistics. Meeting</title>
		<meeting>the conference. Association for Computational Linguistics. Meeting</meeting>
		<imprint>
			<publisher>NIH Public Access</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page">6558</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.06176</idno>
		<title level="m">Learning factorized multimodal representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rendering of eyes for eye-shape registration and gaze estimation</title>
		<author>
			<persName><forename type="first">Erroll</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tadas</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xucong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3756" to="3764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Benchmarking commercial emotion detection systems using realistic distortions of facial image datasets</title>
		<author>
			<persName><forename type="first">Kangning</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaofan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanna</forename><surname>Sarsenbayeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tilman</forename><surname>Dingler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Wadley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Goncalves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Visual Computer</title>
		<imprint>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Ch-sims: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality</title>
		<author>
			<persName><forename type="first">Wenmeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanyang</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiele</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyun</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaicheng</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3718" to="3727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning modality-specific representations with selfsupervised multi-task learning for multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">Wenmeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Ziqi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wu</forename><surname>Jiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Transformer-based feature reconstruction network for robust multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenmeng</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4400" to="4407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Tensor fusion network for multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1103" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Convolutional experts constrained local model for 3d facial landmark detection</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tadas</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2519" to="2528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. 2018a. Memory fusion network for multi-view sequential learning</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navonil</forename><surname>Mazumder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Prateek Vij, Erik Cambria, and Louis-Philippe Morency. 2018b. Multi-attention recurrent network for human communication comprehension</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Pincus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="88" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph</title>
		<author>
			<persName><forename type="first">Amirali</forename><surname>Bagher Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2236" to="2246" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
