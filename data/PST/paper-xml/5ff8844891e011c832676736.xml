<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Leap-Of-Thought: Teaching Pre-Trained Models to Systematically Reason Over Implicit Knowledge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
							<email>alont@allenai.org</email>
							<affiliation key="aff0">
								<orgName type="institution">The Allen Institute for AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
							<email>oyvindt@allenai.org</email>
							<affiliation key="aff0">
								<orgName type="institution">The Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
							<email>peterc@allenai.org</email>
							<affiliation key="aff0">
								<orgName type="institution">The Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
							<email>yoavg@allenai.org</email>
							<affiliation key="aff0">
								<orgName type="institution">The Allen Institute for AI</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Bar-Ilan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
							<email>jonathan@allenai.org</email>
							<affiliation key="aff0">
								<orgName type="institution">The Allen Institute for AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Leap-Of-Thought: Teaching Pre-Trained Models to Systematically Reason Over Implicit Knowledge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To what extent can a neural network systematically reason over symbolic facts? Evidence suggests that large pre-trained language models (LMs) acquire some reasoning capacity, but this ability is difficult to control. Recently, it has been shown that Transformer-based models succeed in consistent reasoning over explicit symbolic facts, under a "closed-world" assumption. However, in an open-domain setup, it is desirable to tap into the vast reservoir of implicit knowledge already encoded in the parameters of pre-trained LMs. In this work, we provide a first demonstration that LMs can be trained to reliably perform systematic reasoning combining both implicit, pre-trained knowledge and explicit natural language statements. To do this, we describe a procedure for automatically generating datasets that teach a model new reasoning skills, and demonstrate that models learn to effectively perform inference which involves implicit taxonomic and world knowledge, chaining and counting. Finally, we show that "teaching" the models to reason generalizes beyond the training distribution: they successfully compose the usage of multiple reasoning skills in single examples. Our work paves a path towards open-domain systems that constantly improve by interacting with users who can instantly correct a model by adding simple natural language statements.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A longstanding goal of artificial intelligence is to develop systems that continuously accumulate knowledge by consuming facts and rules about the world and reasoning over them <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b3">3]</ref>. Recently <ref type="bibr" target="#b4">[4]</ref>, it has been shown that Transformers <ref type="bibr" target="#b5">[5]</ref> are an effective architecture for this goal, as they can be trained to reason over knowledge expressed as natural language statements. However, modern neural networks for language do not store knowledge symbolically. Instead, substantial amounts of knowledge are encoded in their parameters by pre-training on large corpora with a language modeling (LM) objective <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b9">9]</ref>. Moreover, although LM-based models do exhibit certain reasoning abilities <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11]</ref>, these abilities are not systematic and are difficult to control. Thus, for real-world open-domain applications, it is imperative that models reason consistently over both explicit input statements and implicit knowledge that is already encoded by the network.</p><p>In this work, we develop models that reason over implicit knowledge and explicit natural language statements in a systematic manner. Consider the example in Figure <ref type="figure">1</ref>. The model needs to determine whether "A whale has a belly button", but answers incorrectly since this particular knowledge nugget is unknown to the model. However, if a user tells the model explicitly that "A mammal has a belly button", the model can combine this statement with the its implicit knowledge that "A whale is a mammal", and make the right inference on the spot. Thus, we want the model to systematically handle such cases that combine implicit knowledge with input natural language statements. Figure <ref type="figure">1</ref>: The model is wrong when asked whether "A whale has a belly button". However, if a user tells the model the explicit rule "A mammal has a belly button", the model combines this on-thefly with its implicit knowledge that "A whale is a mammal", and arrives at the right conclusion (without re-training).</p><p>We train our models by automatically generating examples that illustrate the expected types of inference. Because the knowledge of pre-trained models comes from real world text, we test this knowledge by generating examples using true facts and rules from multiple information sources.</p><p>We focus on two types of high-level reasoning: (a) inference that combines implicit taxonomic knowledge (hypernymy, meronymy, etc.) with explicit natural language rules, and (b) inference that requires counting over both implicit and explicit facts, and checking whether a certain count was reached. In both cases, we observe that models can be trained to reason over implicit and explicit knowledge. Importantly, model performance can be explained by its prior knowledge: inference is successful when the necessary knowledge exists in the model, and fails when it is missing.</p><p>Last, we show that training the models to perform inference generalizes beyond their training distribution. Specifically, we endow a pre-trained LM with multiple inference capabilities independently, and show that it can handle examples that require composing multiple inference types, even when these do not appear at training time. Thus, one can gradually improve the inference capabilities of a model and expect generalization.</p><p>Our work paves a path towards systems that constantly improve by interacting with users: when a user spots an error, they can fix that error by providing a single statement in natural language that will allow the model to apply its acquired inference skills and reach the right conclusion. If the system can successfully retrieve this statement in future interactions, it will fix not only the current mistake, but also future ones. This can be viewed as a form of "one-shot learning" that improves the model on-the-fly without further training, unlike most current work that relies on data collection and re-training for fixing model errors. All our code and data is publicly available at http://github.com/alontalmor/LeapOfThought. We begin by demonstrating that combining reasoning over explicit input with reasoning over implicit LM knowledge can "emerge" by training a Transformer on a dataset that requires each skill individually. We fine-tune ROBERTA <ref type="bibr" target="#b8">[8]</ref>, on binary (yes/no) question answering tasks from two datasets (using standard multi-task training): (a) 50K examples from TWENTY QUESTIONS (20Q),<ref type="foot" target="#foot_0">1</ref> a question answering (QA) dataset which includes questions such as "Does an aircraft fly?" (true) and "Do everyone have an alarm?" (false). This teaches the model to retrieve real world facts from its internal implicit knowledge; and (b) 100K examples from the RULETAKER <ref type="bibr" target="#b4">[4]</ref> reasoning dataset, teaching the model to reason over a set of assertions explicitly provided as natural language statements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A Motivating Example</head><p>We evaluate this model on a task that requires combining implicit knowledge about the sizes of animals (known to exist in ROBERTA <ref type="bibr" target="#b11">[11]</ref>) and an animal taxonomy, with explicit reasoning over natural language statements. Figure <ref type="figure" target="#fig_0">2</ref> illustrates the setup. The model needs to determine if a hypothesis of the form "Chen would like to buy a [ANIMAL]" is true, where the slot ANIMAL is replaced with some object. The model also observes explicit knowledge that specifies the desired size of the animal and that it must not be a fish. Over 24 animal pairs, the model obtains 91.6% accuracy, successfully combining implicit knowledge with the given statements in ways that were not observed during fine-tuning. Variations of this task work equally well.</p><p>This experiment shows that although training examples did not require reasoning over explicit statements and implicit knowledge, the model learned to effectively do so. While exciting, the procedure is still an "alchemy". Can we make it more systematic and more controlled, and get a better handle on its capabilities and limitations? The rest of the paper explores these questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We describe a general procedure for creating models that can perform inference over explicit and implicit knowledge. The process includes automatic data generation from existing sources, followed by standard supervised training. §4.1 and §4.3 show two instantiations of this procedure.</p><p>Definitions Our goal is to endow a model with the ability to perform certain inference types.</p><p>An inference type describes how two or more natural language statements combine logically. For example, we define the HYPERNYMY inference type with the assertion 'if A is a type of B, and B has property P, then A has property P'. Similarly, we will later use APPROX-IMATE COUNTING, and MERONYMY as additional inference types. To this end, we automatically generate training examples, where each example includes (a) a hypothesis, i.e., a textual statement that is either true or false ("A whale has a belly button"), and (b) explicit knowledge, which is a list of textual statements. Statements can either be facts, that is, describe a property of a particular entity ("John Lennon is a member of The Beatles"), or rules, that is, describe a property of some class ("Mammals have belly buttons"). The explicit knowledge is constructed such that the truth value of the hypothesis cannot be inferred from the explicit knowledge alone, but also requires some knowledge to be encoded by the LM a-priori. For example, the explicit knowledge might not include the rule "A whale is a mammal", necessary for deducing that they have belly buttons (Figure <ref type="figure">1</ref>).</p><p>Data generation Our primary motivation is to develop models that work in an open domain environment with real world facts. Thus, we automatically generate data by sampling from existing knowledge sources: CONCEPTNET <ref type="bibr" target="#b12">[12]</ref>, WORDNET <ref type="bibr" target="#b13">[13]</ref> and WIKIDATA <ref type="bibr" target="#b14">[14]</ref>. We sample (subject, predicate, object) triples that are known to be either true or false. Pseudo-language statements are then generated using manually constructed templates for each predicate. <ref type="foot" target="#foot_1">2</ref> For example, for true statements such as (chicken, has, feathers) we generate "A chicken has feathers."; for false statements, we generate "A chicken does not have horns". In all examples, the order of the statements in the explicit knowledge is random and the number of true and false hypotheses is equal.</p><p>Training Once examples are generated, we fine-tune a pre-trained LM, specifically ROBERTA-LARGE <ref type="bibr" target="#b8">[8]</ref>, on this dataset. The inputs and outputs are modeled in the standard manner <ref type="bibr" target="#b7">[7]</ref>: The input is given as a list of tokens '[CLS] explicit knowledge [SEP] hypothesis [SEP]', and the contextualized representation of the [CLS] token is linearly projected down to two logits, and passed through a softmax layer to obtain the probabilities that the hypothesis is true or false. We train by minimizing the binary cross-entropy loss, and evaluate models using accuracy.</p><p>In addition, To investigate the importance of pre-trained contextualized representations, we use ESIM <ref type="bibr" target="#b15">[15]</ref> over non-contextualized GLOVE <ref type="bibr" target="#b16">[16]</ref> representations as a baseline architecture, as it is known to provide a strong model when the input is a pair of text fragments <ref type="bibr" target="#b15">[15]</ref>. We adapt the architecture to the mutli-choice setup using the procedure proposed by <ref type="bibr" target="#b17">[17]</ref>, with two choices for 'yes' and 'no'. Last, motivated by the results in §2, we evaluate whether the RULET.+20Q model, described in §2, can utilize implicit knowledge even without being directly trained for this goal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We instantiate our approach over two inference types. First, correcly applying implicit taxonomic knowledge ( §4.1). Second, counting over explicit and implicit facts ( §4.3). Additionally, we analyze our results and show that model success can be reliably predicted by probing the background knowledge encoded in the pre-trained LM ( §4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implicit Knowledge of Taxonomic Relations</head><p>Pre-trained LMs have been shown to capture substantial amounts of taxonomic knowledge such as hypernymy (A is a type of B) and meronymy (A is part of B) <ref type="bibr" target="#b10">[10]</ref>. To test whether this knowledge can be leveraged for reasoning, we create examples that can be solved only by integrating this implicit knowledge. Data Construction Figure <ref type="figure" target="#fig_1">3</ref> illustrates the main components of an example. We describe the generation process, based on triples from CONCEPTNET and WORDNET.</p><p>To generate a positive example (true label), we sample a relevant hypernym rule, e.g., ((whale, is a, mammal), true). We then find a relevant property of the hypernym object, e.g., ((mammal, has a, belly button), true). We apply the hypernymy inference type (downward monotonicity): (if A is a B and B has property C, then A has property C) to deduce the logical conclusion which will become the hypothesis, e.g., ((whale, has a, belly button), true).</p><p>To add distractors, we create similar but irrelevant statements by: (a) randomly replacing the subject of the relevant property to create a distractor property, e.g., ((fish, has a, belly button), false). Negative examples are created from positive examples by using the distractor property ("a fish does not have a belly button"). We sample a hypernym rule, such that the object matches the subject of the distractor property e.g. ((salmon, is a, fish), true). We then apply the hypernymy inference type to obtain the false hypothesis ((salmon, has a, belly button), false). In the negative example the roles of the relevant hypernym and property, and distractor hypernym and property are reversed. The distractor subject and predicate are sampled independently.</p><p>We Experiments We evaluate our model in three different setups:</p><p>HYPOTHESIS-ONLY: The model is given only the hypothesis without the explicit knowledge. This tests whether the model already knows the answer without the need for inference.</p><p>EXPLICIT REASONING: The model is given the hypothesis and explicit knowledge with hypernym rules, thus the model needs to perform inference over explicit natural language statements. IMPLICIT REASONING: The model is given the hypothesis and explicit knowledge without hypernym rules, and is forced to choose the correct answer based on its implicit knowledge.</p><p>Models We compare the following models: (a) ROBERTA fine-tuned on the generated dataset, (b)</p><p>The RULET.+20Q model (see §3), and (c) ESIM trained on the generated dataset without pre-training (testing the value of pre-trained contextualized representations).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We show the results in Table  We evaluate performance of RULET.+20Q to determine if combining implicit and explicit knowledge emerges even without direct training. <ref type="foot" target="#foot_3">4</ref> The model, although trained on a different distribution for explicit reasoning (the RULETAKER dataset), achieves 98.4% on EXPLICIT REASONING. Surprisingly, without being trained to perform implicit reasoning, accuracy improves from 65.4 → 79.1 when given the explicit knowledge without hypernym rules (but still lower than 88.8). ESIM, trained with GLOVE pre-trained word embeddings, also shows a higher accuracy of 76.1% in IMPLICIT REASON-ING compared with 61.3% in HYPOTHESIS-ONLY. This suggests leveraging implicit knowledge is possible in models with non-contextualized representations. However, ESIM did not achieve perfect accuracy in EXPLICIT REASONING, reaching 79.8%. Interestingly, performance on meronymy is similar to hypernymy although no meronyms were provided during training, suggesting the model already has some knowledge of this reasoning skill from pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analyzing systematicity</head><p>IMPLICIT REASONING achieves 88.8 accuracy, a high, but not perfect, result. A key question is whether the performance in IMPLICIT REASONING can be explained in terms of the implicit knowledge of the LM. To estimate a model's implicit knowledge "beliefs", we convert the required implicit knowledge (relevant and distractor hypernyms) to hypotheses without any explicit knowledge rules (i.e. only "A whale is a mammal" hypothesis), and run the model to check if prediction is correct. We hypothesize that model performance should be high for cases where the implicit beliefs are accurate. If we intervene for the small number of incorrect beliefs, adding their rules to the explicit knowledge, the overall scores increase accordingly.</p><p>A potential bias in the setup from §4.1, is that the model also has an initial belief about the correctness of the hypothesis itself, regardless of its belief of the hypernym rules. To neutralize this effect we create a controlled dataset IMAGINARY, where entities have imaginary properties, such as "group-1" in "A Toucan is a group-1", for which the model does not have any prior knowledge. The rest of the dataset construction is similar to §4.1. Thus, the performance of HYPOTHESIS-ONLY in this setup is 50% by construction. As before there is one relevant implicit rule ("Toucan often has part wing."), but now there can be up to 5 distractor implicit rules, as well as a mix of rule conditions (hypernyms, meronyms and size comparisons). In Table <ref type="table" target="#tab_3">2</ref>, we observe a similar consistency in results when predicting the outcome for the IMAGINARY-set, where accuracy is 95.0 when all beliefs are supporting the correct answer, 66.3 when there are conflicting beliefs, and 7.1 when all beliefs are in support of the wrong answer.</p><p>Intervening here improves overall performance from 76.9 → 94.1. More details on IMAGINARY are available in the supp. material. To conclude, we observe that model reasoning can be explained by its prior knowledge, and that one can correct it by intervening and correcting false beliefs, suggesting that the model is using implicit knowledge in a systematic fashion. Our next experiment focuses on whether our models can simulate counting over explicit and implicit facts. While LMs are known to hold taxonomic knowledge <ref type="bibr" target="#b10">[10]</ref>, there is evidence that skills such as counting are not acquired during pre-training <ref type="bibr" target="#b11">[11]</ref>. Here, we train a model for simulating counting, and check whether it can count over both text and its prior knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Counting over Implicit Facts</head><p>Data Construction To collect 'facts that require counting' (member facts), we use WIKIDATA relations such as "member of band", "capital of country", and "child of person". Quantity facts provide the total count of facts for a specific entity, e.g., "The Beatles has four members". Figure <ref type="figure" target="#fig_3">4</ref> outlines a counting example. For each entity ("Beatles"), there are 1-4 member facts ("members").</p><p>We create training data by constructing 10,852 counting sets, that is, sets that include a single quantity fact ("The Beatles has four members.") stating the total number of member facts and K member facts, ("Ringo Starr is a member of The Beatles"), etc. For each counting set, we create exactly 2K − 1 examples: For all integers k ∈ [0, K − 1], we create an example by adding k member facts and the quantity fact to the explicit knowledge, and generating one positive hypothesis and one negative hypothesis. The positive hypothesis is a true member fact not in the explicit knowledge, and the negative hypothesis is generated by randomly sampling a subject that appears with the relevant predicate, e.g. ((Mick Jagger, is member of, The Beatles), false). When k = K, we generate only a false hypothesis.</p><p>Distractors are added to the explicit knowledge by adding one quantity fact and one member fact that share a predicate with the hypothesis, but have a different subject ("The Carpenters"), and one random member fact and quantity fact ("Jeff Bezos is CEO of Amazon"). We find that distractors are important in the sense that if none are added, the model learns to count sentences, and ignores their content, failing to pay attention to the actual subjects of the member facts.</p><p>Overall 38,700/3,005/3,005 training/development/test examples were created. In 25% of the examples k = K and thus no implicit knowledge is needed. If the model can count the relevant member facts and verify the count reaches the number specified in the relevant quantity fact, then it can deduce that the answer is false. In all other examples, solving the task requires combining the member facts that appear in the explicit knowledge with implicit member facts that the model knows.</p><p>Experiments We evaluate our approach using ROBERTA and ESIM (the performance of RULET.+20Q is low since it cannot count without training).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HYPOTHESIS-ONLY:</head><p>The model is given only the hypothesis (similar to §4.1). COUNTING: Examples are sampled from the same distribution as the training set. In 25% of the examples the model can use explicit counting to predict false (k = K). In the rest, it must combine prior knowledge of member facts with member facts that are in the explicit knowledge.</p><formula xml:id="formula_0">Experimental setup Subset ROBERTA HYPOTHESIS-ONLY (1, K − 1) 64.1 COUNTING (1, K − 1) 73 K 99.7</formula><p>Table <ref type="table">3</ref>: Test set performance for counting. We show performance on two subsets of the test set:</p><p>(1, K − 1): where the number of member facts is in 1, . . . , K − 1, and K where the number of member facts is exactly K.</p><p>Results Table <ref type="table">3</ref> shows the accuracy for all models. We distinguish between the case where the total number of member facts has been reached (k = K), where the answer is false, and the rest of the examples. ROBERTA achieves a near perfect of 99.7% when k = K, illustrating it can be trained to count the relevant member facts. <ref type="foot" target="#foot_4">5</ref>When the number of member facts is in 1, . . . , K − 1, accuracy improves by 9 points given the explicit knowledge (64.1 → 73), implying that the model learned to combine implicit knowledge of member facts with the explicit knowledge. Both RULET.+20Q (not trained on the counting dataset) and ESIM, trained on the generated data, predicted false to almost all examples, indicating they are not well suited for this task. To further validate that the model is indeed counting, we dropped the relevant quantity fact from all test examples. The accuracy drops from 73 → 64.4 (Table <ref type="table">3</ref>, counting (1, K − 1)), which is similar to hypothesis-only 64.1, suggesting that the model is using the quantity fact to know how many member facts should be counted.</p><p>Performance shows that the model uses the explicit knowledge to improve prediction. However, because the required implicit knowledge involves long-tail facts that might be missing from the LM, learning to use this knowledge can be difficult. We perform a more fine-grained analysis to decouple prior knowledge from the counting task at hand. Analysis Our analysis setup is the following. We take member facts, and obtain a fact probability, which is the output of our model when given the member fact as a hypothesis without any explicit knowledge. Then, for each counting set we create 2K − 1 examples as before, but for each k ∈ [1 . . . K − 1] we take k facts not randomly, but according to one of two orders: (a) ascending fact probability, and (b) descending fact probability. We hypothesize that if member facts are sorted in ascending order, performance should be higher, because the model is given explicit evidence for facts it is less confident about.</p><p>Figure <ref type="figure" target="#fig_4">5</ref> shows the results. The x-axis represents c = k K , that is, the fraction of member facts given in the explicit knowledge. When c = 0 or c = 1, ascending and descending orders are equivalent, since either no facts or all facts are given. Green squares show the distribution over labels for each value of c, illustrating that guessing based on label leads to 50% accuracy, except when c = 1, in which all examples are false. Blue circles display accuracy when member facts are added in ascending order, and orange triangles in descending order.</p><p>When c = 0, model accuracy is slightly lower than 60%, showing that ROBERTA has non-trivial knowledge of the relevant background facts. When c = 1 performance is perfect since our models count the correct facts and reach K. For both ascending and descending orders, the accuracy of ROBERTA monotonically increases with the number of member facts, although distribution over labels is constant. This suggests that explicit member facts help the pre-trained LM leverage its prior knowledge. When c = 0.75 both models hover around an impressive 90% accuracy. We confirm our hypothesis that adding facts in ascending order substantially improves performance: when c = 0.25 performance improves by 5.5 points, and when c = 0.75 it improves by 2.7. This shows that adding member facts which the model does not "know" improves performance more than a fact it is already confident about.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Generalizing to New Skill Combinations</head><p>In §4.1 and §4.3, we isolated a single type in each experiment. However, our goal is to have a model that can seamlessly combine these skills. To test this, we train models on various subsets from RULETAKER, 20Q (described in §2), COUNTING ( §4.3) and HYPERNYMS ( §4.1). We then test whether these models can handle examples that require multiple skills simultaneously. Training is done by randomly mixing the datasets and performing standard multi-task training.</p><p>Ultimately, we want users to correct models on-the-fly by mixing natural language statements that demand different reasoning skills. To emulate this process, we created MULTI-SKILL-SET, composed of 185 hand-crafted hypotheses and explicit knowledge, <ref type="foot" target="#foot_5">6</ref> labeled with the set of skills needed for each example. Figure <ref type="figure" target="#fig_5">6</ref> shows one example, requiring age/year comparison + hypernymy skills. We specifically chose hypotheses ROBERTA answers incorrectly in HYPOTHESIS-ONLY.</p><p>Results Table <ref type="table" target="#tab_5">4</ref> shows results on MULTI-SKILL-SET.</p><p>The rows show the subset of datasets the model was trained on and the columns show the accuracy for examples that require a particular skill. A model trained on all 4 datasets obtains an accuracy of 85.4. All models perform poorly given only the hypothesis, with an average accuracy of 40.2. (Less than random, due to the adverserial manner in which we chose examples for MULTI-SKILL-SET). Models trained without COUNTING show low accuracy on the counting skill subset, suggesting that this skill was not acquired at pre-training time. Interestingly, models combining HY-PERNYMS or COUNTING with RULETAKER and 20Q, display higher accuracy than models trained only on one of these datasets. This suggests that models are able to successfully combine multiple reasoning skills, some newly acquired. Models show high accuracy on skills that they are not explicitly trained for, but have been shown to be acquired during pre-training, such as size comparison <ref type="bibr" target="#b11">[11]</ref>, and meronyms <ref type="bibr" target="#b18">[18]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The right side of</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>We have demonstrated that LMs can be taught to systematically combine both pre-trained knowledge and explicit natural language statements. This predictability is important: If the model is behaving rationally, we can teach it. This distinguishes our work from other research in recognizing textual entailment (RTE), e.g., <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b22">22]</ref>, where models learn to score a hypothesis but in an opaque and somewhat unpredictable way <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b24">24]</ref>. It similarly distinguishes our work from multi-hop QA, e.g., <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b29">29]</ref>, where again model behavior is often opaque <ref type="bibr" target="#b30">[30]</ref>.</p><p>There have been numerous earlier demonstrations that neural systems can learn systematic behavior, including for semantic parsing <ref type="bibr" target="#b31">[31]</ref>, symbolic integration <ref type="bibr" target="#b32">[32]</ref>, mathematics <ref type="bibr" target="#b33">[33]</ref>, knowledge prediction <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b35">35]</ref>, and rule-based reasoning <ref type="bibr" target="#b4">[4]</ref>. However, these are all largely self-contained tasks. We extend this to show how implicit knowledge can be directly harnessed in a systematic inference process.</p><p>Finally, although teaching a machine via general statements has long been a goal of AI <ref type="bibr" target="#b36">[36]</ref>, current neural methods typically require large numbers of examples to convey knowledge <ref type="bibr" target="#b37">[37]</ref>. Our work shows how pre-trained networks can instead be taught on-the-fly using a few general statements, in a "one-shot" manner that exploits pre-trained knowledge, without requiring re-training of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>In this work, we show that pre-trained LMs can be trained to consistently combine implicit knowledge encoded in their parameters with explicit rules and facts. Models are able to perform various types of reasoning in this setup including multi-hop reasoning, counting, number comparisons, and taxonomic knowledge. Moreover, we show that one can inject the ability to perform various types of inferences one at a time independently, and obtain generalization to cases that require combining these skills in a single example. Our work opens the door to models that learn through interaction with users. Users can teach the model facts and rules about the world through natural language statements, and the model will utilize this new information immediately, combining it with the knowledge encoded internally. Such an approach allows users to "teach the model" and correct its current and future errors without the need for data collection and re-training, in a one-shot manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Our work, if successful, paves a possible path towards models that learn in a one-shot manner by interacting with users. Users of Question Answering and other systems utilizing reasoning over natural language may benefit by constantly improving models that do not require re-training in an interactive manner. However, users teaching false rules and facts may lead to the spread of ungrounded and possibly "fake" information. Thus, the provided rules and facts must be constantly monitored and curated. Finally, users relying on the reasoning of such systems for mission critical tasks, such as medical advice, might be at risk from possible errors. At current level of accuracy of state-of-the-art models, this type of usage is not advised.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Our motivating task. The top box shows a single example. In the bottom we systematically replace arguments matching the color of the bold underlined words in the top example. The text in the table corresponds to model predictions, and the color and indicate a correct prediction.</figDesc><graphic url="image-2.png" coords="2,353.52,464.91,150.48,162.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Outline of a taxonomy example. The purpose of each relevant and distractor rule is in parenthesis. Underlined hypernym rules are removed in IMPLICIT REASONING.</figDesc><graphic url="image-3.png" coords="4,345.60,232.47,158.40,88.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(b) Create a distractor hypernym by combining the subject of the hypothesis and the subject of the relevant property: e.g., ((whale, is a, fish), false), (c) add a distractor subject by randomly sampling a different rule about the hypothesis subject, and (d) add a distractor predicate by randomly sampling a different rule with the hypothesis predicate. Thus, the explicit knowledge contains the relevant hypernym and property, and four distractors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Fact counting example outline. The purpose of each fact is in parenthesis.</figDesc><graphic url="image-4.png" coords="6,333.72,342.32,170.28,96.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: An analysis of ROBERTA accuracy (y-axis) vs c = k K (x-axis).</figDesc><graphic url="image-5.png" coords="7,294.12,408.03,209.87,133.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: MULTI-SKILL-SET example, requiring age/year + hypernymy skills.</figDesc><graphic url="image-6.png" coords="8,337.68,372.38,166.32,65.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>generate 30,906  training examples using this procedure. We create development and test sets, 1,289 examples each, where the subjects and objects are disjoint from the ones in the training set. In 50% of the training examples, we remove the relevant and distractor hypernyms to teach the model to use implicit knowledge. We find that without this step, the model always predicts false if a necessary hypernym is missing, performing well only on the EXPLICIT REASONING setup. In 20% of the training examples we remove all distractors.3 In the test set, we remove some of the statements in the explicit knowledge, based on the experimental setting, as explained below. For example, we remove all hypernyms to test the use of implicit knowledge.</figDesc><table><row><cell>Meronyms (zero-shot) To test whether the inference type learned by the model generalizes to other in-</cell></row><row><cell>ference types, we used the same procedure to create 2,528 test examples using a MERONYMY inference</cell></row><row><cell>type (meronym transitivity), i.e. 'if A has part B and B has part C, then A has part</cell></row></table><note>C' or 'if a hand has a cell and a cell has an atom, then a hand has an atom.'. There is no training set in this setup.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>. On HYPOTHESIS-ONLY evaluation examples, ROBERTA achieves 65.2% accuracy, substantially higher than random (50%), suggesting it knows some of the answers even without the explicit knowledge (if we train only on HYPOTHESIS-ONLY examples, accuracy on HYPOTHESIS-ONLY test examples is slightly higher at 69.7%). EXPLICIT REASONING shows that when the relevant hypernym and property are both present in the explicit knowledge, ROBERTA achieves near perfect performance, easily solving 2-hop reasoning. Finally, the accuracy of IMPLICIT REASONING is 88.8%, substantially higher than HYPOTHESIS-ONLY. This suggests the model is able to correctly apply its implicit knowledge of hypernyms to select the relevant property rule, effectively performing 2-hop reasoning with one hop done internally. Test set results for reasoning over hypernymy and meronymy relations. The models learn to reason with implicit rules, significantly improving on the hypothesis-only baseline, some in zero-shot.</figDesc><table><row><cell>Model →</cell><cell></cell><cell>ROBERTA</cell><cell></cell><cell>ESIM</cell></row><row><cell>Train-set →</cell><cell cols="4">Hyper. Hyper. RULET.+20Q Hyper.</cell></row><row><cell>Test-set →</cell><cell cols="2">Hyper. Mero.</cell><cell>Hyper.</cell><cell>Hyper.</cell></row><row><cell>HYPOTHESIS-ONLY</cell><cell>65.2</cell><cell>70.8</cell><cell>65.4</cell><cell>61.3</cell></row><row><cell>EXPLICIT REASONING</cell><cell>99.7</cell><cell>99.4</cell><cell>98.4</cell><cell>79.8</cell></row><row><cell>IMPLICIT REASONING</cell><cell>88.8</cell><cell>86.9</cell><cell>79.1</cell><cell>76.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>shows how scores depend on whether all the model's implicit beliefs are correct. For the hypernymy examples, the model reaches 99.7 accuracy when the implicit beliefs are both correct, while it is much lower otherwise. Using the knowledge of which beliefs are incorrect, we can</figDesc><table><row><cell>Implicit beliefs</cell><cell cols="2">Hypernymy IMAGINARY</cell></row><row><cell>All correct</cell><cell>99.7</cell><cell>95.0</cell></row><row><cell>Some incorrect</cell><cell>70.0</cell><cell>66.3</cell></row><row><cell>All incorrect</cell><cell>13.0</cell><cell>7.1</cell></row><row><cell>Overall</cell><cell>88.8</cell><cell>76.9</cell></row><row><cell>Overall (after intervention)</cell><cell>98.3</cell><cell>94.1</cell></row><row><cell>Fraction beliefs corrected</cell><cell>0.17</cell><cell>0.20</cell></row></table><note>intervene and explicitly add the correct relevant and distractor hypernyms for the small number of incorrect beliefs (17% in this case). With intervention, the score goes up from 88.8 → 98.3, a significant error reduction.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>When the model's implicit beliefs needed for a hypothesis are all correct, scores are high.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 ,</head><label>4</label><figDesc>presents the accuracy on examples requiring at least two skills. Strikingly, the model is able to accurately compose multiple skills, reaching a high accuracy of 88 on examples combining implicit hypernyms and counting. Size, age, and year comparison examples are not available in our training sets, nevertheless, models achieve an impressive score of 81.8 and 73.9 on examples combining implicit hypernyms with size and age/year, respectively.</figDesc><table><row><cell>test questions, skills →</cell><cell cols="4">overall hypothesis hypernyms counting sizes age/year hypernyms hypernyms hypernyms</cell></row><row><cell>training data ↓</cell><cell>only</cell><cell>+ counting</cell><cell>+ sizes</cell><cell>+ age/year</cell></row><row><cell>HYPERNYMS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/allenai/twentyquestions</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">The full list of predicates used is available in the supplementary material.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">The training set also includes 2,864 examples that contain only a hypothesis, half of which are true hypernym rules ("a whale is a mammal") and half are false hypernym rules ("a whale is a fish"). This is useful for analyzing what prior knowledge a LM has, as we show in §4.2.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"> Clark et al. [4]  tested whether ROBERTA can perform reasoning in the few-shot setup, without further fine-tuning on additional auxiliary datasets, and found that models perform poorly (figure4in their paper).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">We also conduct an experiment where we provide K member facts, but some of them are false (e.g., "Mick Jagger is a member of the Beatles") and find that the model simply counts member facts, ignoring whether they are factually correct or not, still predicting False in all such cases.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">All examples and best model prediction are available in the supplementary material.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>We thank our colleagues at The Allen Institute of AI and Tel-Aviv University, especially Kyle Richardson, Nicholas Lourie, Ashish Sabharwal, Elad Segal, Mor Geva and Tomer Wolfson. This research was partially supported by The Israel Science Foundation grant 942/16, The Blavatnik Computer Science Research Fund and The Yandex Initiative for Machine Learning, and the European Union's Seventh Framework Programme (FP7) under grant agreement no. 802774-ERC-iEXTRACT and no. 802800-DELPHI.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Accuracy for MULTI-SKILL-SET. Rows show training set composition for ROBERTA. Overall accuracy, hypothesis-only accuracy, single-skill and multiple-skill breakdown are displayed in columns</title>
	</analytic>
	<monogr>
		<title level="j">Table</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note>References</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Never-ending learning</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Estevam</forename><surname>Hruschka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhanava</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="103" to="115" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">True knowledge: Open-domain question answering using structured knowledge and inference</title>
		<author>
			<persName><forename type="first">William</forename><surname>Tunstall-Pedoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="80" to="92" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Interactive transfer of expertise: Acquisition of new inference rules</title>
		<author>
			<persName><forename type="first">Randall</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transformers as soft reasoners over language</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics (NAACL)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>American Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>OpenAI</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">What does my qa model know? devising controlled probes using expert knowledge</title>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.13337</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.13283</idno>
		<title level="m">olmpics-on what language model pre-training captures</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4444" to="4451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Wikidata: A free collaborative knowledgebase</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vrandečić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krőtzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Enhanced LSTM for natural language inference</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1657" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Swag: A large-scale adversarial dataset for grounded commonsense inference</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Probing natural language inference models through semantic fragments</title>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">S</forename><surname>Moss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07521</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Recognizing Textual Entailment: Models and Applications</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sammons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><forename type="middle">Massimo</forename><surname>Zanzotto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Morgan and Claypool</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><surname>Uszkoreit</surname></persName>
		</author>
		<idno>ArXiv, abs/1606.01933</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Compare, compress and propagate: Enhancing neural architectures with alignment factorization for natural language inference</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siu</forename><forename type="middle">Cheung</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Annotation artifacts in natural language inference data</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adversarial nli: A new benchmark for natural language understanding</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<editor>ACL</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">HotpotQA: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The web as knowledge-base for answering complex questions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics (NAACL)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Constructing datasets for multi-hop reading comprehension across documents</title>
		<author>
			<persName><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06481</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning to retrieve reasoning paths over wikipedia graph for question answering</title>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<idno>ArXiv, abs/1911.10470</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dynamically fused graph network for multi-hop reasoning</title>
		<author>
			<persName><forename type="first">Yunxuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Compositional questions do not necessitate multi-hop reasoning</title>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Compositional generalization for primitive substitutions</title>
		<author>
			<persName><forename type="first">Yuanpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep learning for symbolic mathematics</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franccois</forename><surname>Charton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Analysing mathematical reasoning abilities of neural models</title>
		<author>
			<persName><forename type="first">David</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">How context affects language models&apos; factual predictions</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04611</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Programs with common sense</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">W</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Tedding Conf. on the Mechanization of Thought Processes</title>
				<meeting>Tedding Conf. on the Mechanization of Thought esses</meeting>
		<imprint>
			<date type="published" when="1959">1959</date>
			<biblScope unit="page" from="75" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Harnessing deep neural networks with logic rules</title>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
