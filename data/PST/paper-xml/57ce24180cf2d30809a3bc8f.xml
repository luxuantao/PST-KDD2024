<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">r i p t Glaucoma Detection Using Entropy Sampling And Ensemble Learning For Automatic Optic Cup And Disc Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-08-05">August 5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Julian</forename><surname>Zilly</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mechanical Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joachim</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Dwarikanath</forename><surname>Mahapatra</surname></persName>
							<email>dwarikanath.mahapatra@inf.ethz.ch</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">IBM Research Australia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">r i p t Glaucoma Detection Using Entropy Sampling And Ensemble Learning For Automatic Optic Cup And Disc Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-08-05">August 5, 2016</date>
						</imprint>
					</monogr>
					<idno type="MD5">980734ABD52EFCFEEED5F4C97D792C30</idno>
					<idno type="DOI">10.1016/j.compmedimag.2016.07.012</idno>
					<note type="submission">Preprint submitted to Elsevier</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel method to segment retinal images using ensemble learning based convolutional neural network (CNN) architectures. An entropy sampling technique is used to select informative points thus reducing computational complexity while performing superior to uniform sampling. The sampled points are used to design a novel learning framework for convolutional filters based on boosting. Filters are learned in several layers with the output of previous layers serving as the input to the next layer. A softmax logistic classifier is subsequently trained on the output of all learned filters and applied on test images. The output of the classifier is subject to an unsupervised graph cut algorithm followed by a convex hull transformation to obtain the final segmentation. Our proposed algorithm for optic cup and disc segmentation outperforms existing methods on the public DRISHTI-GS data set on several metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Glaucoma is one of the leading causes of irreversible vision loss in the world, accounting for 12% of such cases. It is estimated that almost 80 million people globally may be affected with glaucoma by 2020 <ref type="bibr" target="#b0">[1]</ref>. Glaucoma is characterized by damage to the optic nerve through increasing degeneration of the nerve fibers. The progression of the disease is asymptomatic in the early stages but gradually leads to irreversible vision loss. Although there is no known cure, early treatment has been shown to decrease the rate of blindness by around 50% <ref type="bibr" target="#b1">[2]</ref>. Hence it is essential to have a reliable early detection system for glaucoma onset. This work proposes a computationally efficient method using a ensemble learning based convolutional neural network (CNN) architecture for accurate and robust segmentation of the optic cup (OC) and optic disc (OD) from retinal fundus images. The segmented OC and OD are used to calculate the cup-to-disc ratio (CDR) which is an important indicator of glaucoma progression.</p><p>Ophthalmologists use three principal methods to detect onset of glaucoma <ref type="bibr" target="#b2">[3]</ref>. The first approach is the assessment of increased intraocular pressure inside A c c e p t e d M a n u s c r i p t the eye. However, this is not sensitive enough for early detection and glaucoma can sometimes occur without increased eye pressure. The second approach identifies field of abnormal vision with specialized equipment which makes it unsuitable for a comprehensive screening of glaucoma except in sophisticated medical centers. The third approach is evaluation of damage to the optic nerve. This is most reliable but requires a trained professional, is time-consuming, expensive and highly subjective. Expert assessment may vary depending on experience and training <ref type="bibr" target="#b2">[3]</ref>. CDR values from segmented optic cup and disc are an important indicator of damage to the optic nerve.</p><p>The use of automated diagnostic tools is desirable to minimize subjectivity and make the diagnosis robust and consistent. Color fundus imaging (CFI) has emerged as the preferred procedure for comprehensive large-scale retinal disease screening due to their ease of acquisition and good visibility of retinal structures <ref type="bibr" target="#b3">[4]</ref>. Glaucoma screening methods apply computer algorithms on color fundus retinal images for OD and OC segmentation and calculation of CDR values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Prior Related work</head><p>Automatic CDR measurement involves: 1) optic disc localization and segmentation; and 2) optic cup segmentation. Current state-of-the-art methods for disc segmentation use morphological features <ref type="bibr" target="#b4">[5]</ref> and active contours <ref type="bibr" target="#b5">[6]</ref>. A OC segmentation method is proposed in <ref type="bibr" target="#b6">[7]</ref> using depth maps computed from relatively displaced sequentially acquired images. Finally, a confidence measure is used to determine the boundary localization. Performance of these methods depends upon initialization and ability to identify weak edges. Chakravarty et al. <ref type="bibr" target="#b7">[8]</ref> formulate a Markov Random Field on depth maps extracted from multiple shifted images of the same retina to model the relative depth and discontinuity at the cup boundary. This depth map is subsequently used for optic cup segmentation.</p><p>Of late machine learning methods have become popular as they provide a powerful tool for feature classification using learned models. Cheng et al. in <ref type="bibr" target="#b2">[3]</ref> formulate a superpixel based classification method to segment the OD and OC. Center surround statistics from the super pixel neighborhood improve performance and a self-assessment reliability score indicates when a given segmentation might be less reliable. Bock et al. <ref type="bibr" target="#b8">[9]</ref> apply glaucoma specific preprocessing (including blood vessel removal), followed by the extraction of different generic features which are compressed using principal component analysis (PCA). A probabilistic two-stage classification scheme then combines these features types into a proposed glaucoma risk index. Mahapatra et. al. use a field of experts model <ref type="bibr" target="#b9">[10]</ref> and consensus based methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> for segmenting the optic cup and disc. Xu et al. <ref type="bibr" target="#b12">[13]</ref> focus on localizing the optic cup in fundus images and state an unsupervised closed form solution. Their technique estimates optic cup parameters from a code book and estimates the optic cup parameters through a weighted reconstruction based on training images. A prominent limitation of supervised learning methods is the definition of hand crafted features thought to be most relevant for the particular task. Such approaches do not generalize well for different datasets or application domains. Therefore many recent works A c c e p t e d M a n u s c r i p t on segmentation focus on learning the most discriminative features using deep learning and neural networks <ref type="bibr" target="#b13">[14]</ref>. CNNs are a general approach for learning discriminative features from training data in the form of convolutional filters.</p><p>Since we use CNNs for OC and OD segmentation we present related work on image segmentation. Mayraz and Hinton <ref type="bibr" target="#b14">[15]</ref> proposed a hierarchical learning procedure based on a probabilistic learning framework called the product of experts <ref type="bibr" target="#b15">[16]</ref> where the probability of an image is described by the normalized product of learned individual distributions. Kiros et al. <ref type="bibr" target="#b16">[17]</ref> use a hierarchical CNN at multiple scales for lung vessel segmentation by optimizing a 2-norm orthogonal matching pursuit problem. Given learned filters, new feature maps are extracted by convolving with the original images, and serve as input to the next layer of filter learning. Ciresan et al. <ref type="bibr" target="#b17">[18]</ref> use a Deep Neural Network (DNN) to segment neuronal structures in electron microscopy (EM) images. Turaga et al. <ref type="bibr" target="#b18">[19]</ref> segment neuronal structures in EM images by learning an affinity graph using a CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Our Contribution</head><p>Previous approaches to OC and OD segmentation have used hand crafted features to segment the desired anatomy. However, it is not known whether the hand crafted features are optimal in their performance. As a result such methods do not perform equally well on a wide variety of datasets. An alternative approach is to use CNNs to learn the most discriminative features from the training data. However CNN training requires a large dataset as well as significant computing resources. Our work approaches the problem of learning feature representations from training data from a ensemble learning perspective. Our proposed method is inspired from CNNs with the learned output being a set of filters whose convolutional output provides the optimal representation of the training data. Hence there is no need to define hand crafted features since the CNN architecture learns the optimal representational features through the filters. An ensemble learning approach significantly improves the computational efficiency of training and can be used with limited training data.</p><p>The primary research contribution of our work is a hierarchical architecture of CNNs to segment the OC and OD from retinal fundus images. We introduce a novel learning procedure to construct a CNN architecture based on boosting and it shares characteristics with ensemble learning systems. Secondly, an entropy based sampling technique is presented to identify most informative samples from the training dataset and significantly reduce computational complexity. The entropy sampling method is shown to generally yield superior results when compared to uniform sampling. Overall, the proposed method is demonstrated to outperform several other state-of-the-art approaches on a public retinal image data set. Our proposed method differs from conventional CNNs in the following respects: 1) instead of backpropagation we adopt a greedy approach where each stage of filters is learned sequentially using boosting; 2) Each stage considers the final classification error to update itself and not the error backpropagated through the next stages; 3) Our method operates on patch level data instead of image level data used for traditional CNNs. In summary our proposed method A c c e p t e d M a n u s c r i p t is a ensemble learning system inspired from traditional CNNs and is an effective approach to learn convolutional filters in the absence of large numbers of training data. We describe different components of our method in Sections 2-6, present our results in Sec 7, and conclude with Sec 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Convolutional filters and networks</head><p>In this section we briefly describe the theory behind convolutional filters and networks. Hierarchical layers of convolutional filters that mimic the effects of visual receptive fields were inspired by Hubel and Wiesel's work on feedforward processing in the early visual cortex of cats <ref type="bibr" target="#b19">[20]</ref>. Inspired by this CNNs use local spatial correlations in images and also exhibit robustness to natural transformations such as changes of viewpoint or scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Convolutional filtering</head><p>Convolution between functions f and g can be written as</p><formula xml:id="formula_0">y(t) = f (t) * g(t) = ∞ -∞ f (t -τ ) * g(τ )dτ<label>(1)</label></formula><p>where * denotes the convolution operation. The equivalent discrete formulation is</p><formula xml:id="formula_1">y[i, j] = (I * K)[i, j] = m n I[m, n]K[i -m, j -n]<label>(2)</label></formula><p>where K is a two-dimensional matrix called kernel and I is a two-dimensional (gray-valued) image. Convolution can be extended to higher dimensional images and kernels. Generally kernels tend to be considerably smaller than the images. Convolutional networks exploit three key ideas <ref type="bibr" target="#b20">[21]</ref>: sparse interactions, parameter sharing and equivariant representations. Using convolutional filters as building blocks, complex convolutional networks can be constructed. Some of the important building blocks of CNNs are:</p><p>• Convolutional filter: Also called kernel, it is the fundamental building block of a CNN. Each filter is convolved with each point of an assigned input image and generates an output. Filters are generally of size n 1 × n 1 × n maps where n 1 is the specified filter size and n maps describes the dimension of the input image, i.e., n maps = 3 for an RGB image.</p><p>• Max pooling layer: Max pooling is a non-linear downsampling operation where the input image is partitioned into a set of rectangular patches and the maximum of each such patch is returned as the output.</p><formula xml:id="formula_2">a j = max N2×N2 (a n×n i u(n, n))<label>(3)</label></formula><p>By returning the maximum for a given N 2 × N 2 patch, max pooling introduces a considerable amount of robustness into the CNN since several configurations of maximal values in a given patch will yield the same output.</p><p>Page 6 of 31 CNNs tend to be wide (many filters per layer) and deep (many layers). For a large scale system this requires training millions of parameters. Traditionally, this training is done using the backpropagation algorithm as explained in <ref type="bibr" target="#b20">[21]</ref>. Large number of parameters need to be trained on a large dataset to avoid overfitting. However we follow a different approach. With our relatively smaller dataset fewer parameters can be reliably trained by our method using entropy sampling and boosting such that the learned patterns generalize better to unseen data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preprocessing</head><p>We employ a two stage approach where the optic disc is first segmented followed by the smaller optic cup. Contrary to other works such as <ref type="bibr" target="#b8">[9]</ref> we apply a domain independent preprocessing step to enhance the information content of the images. The optic disc is first localized by applying a circular Hough transform on the green channel image. Each image is first cropped so that the optic disc or cup is relatively central to the cropped image and a certain amount of "background" around the optic disc is retained. This allows the training procedure to capture the essential characteristics of the image while being able to focus more on the region of interest. Cropping also reduces the computational burden.</p><p>Since the retinal fundus images are in RGB color space they are converted to L * a * b color space using a nonlinear transformation which mimics the nonlinear perceptive response of the eye. Empirical evidence suggests that L * a * b yields better results compared to other color spaces <ref type="bibr" target="#b21">[22]</ref>. The mean image intensity is subtracted from all pixel intensities followed by division with the standard deviation. The intensities are scaled to lie in [0, 1]. All color channels are normalized individually. Figure <ref type="figure" target="#fig_0">1</ref> shows results of this pre-processing step. Clearly, preprocessing enhances the contrast between optic disc or cup region with respect to background.</p><p>A c c e p t e d M a n u s c r i p t</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Entropy sampling</head><p>Analyzing every pixel in the cropped images constitutes a significant computational burden. Additionally, information at the pixel level is highly redundant since neighboring pixels tend to give highly correlated information. This problem is addressed using a entropy based sampling scheme to select the most informative pixels from the image. Uniform sampling with equal probability passes up the opportunity to extract relevant information for the algorithm. In the worst case, the sampled points cover the image but fail to provide a comprehensive account of where "interesting" patterns are present. Subsequently, nonuniform sampling approaches have been used in other applications <ref type="bibr" target="#b17">[18]</ref>. Since the proposed method is based on convolutional filters, it is important to select points with informative surroundings. Otherwise, local patches around selected points will not yield sufficiently discriminative information.</p><p>A first order entropy estimate for a given point can be calculated by recording each gray value in a neighborhood N using a histogram with 256 bins. With this probability estimate, the entropy is calculated as</p><formula xml:id="formula_3">H(x) = xi∈N3 -p(x i ) • log(p(x i )) (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where N 3 is the neighborhood of pixel x i . The entropy map quantifies the informativeness for each pixel. However entropy maps over any single color channel are noisy as there are many pixels with high entropy, thus defeating the purpose of identifying informative points. Hence we calculate an additional quantity which we term as "total entropy". It is defined as</p><formula xml:id="formula_5">H total (x) := nmaps l=1 H l (x) 2 .</formula><p>(</p><formula xml:id="formula_6">)<label>5</label></formula><p>It is essentially the sum of squares of the individual maps. The square increases the difference between high and low entropy points, thus suppressing noise. For entropy sampling, points are sampled without replacement to ensure a greater coverage of different points. Figure <ref type="figure" target="#fig_1">2</ref> (a) shows the final ("total entropy") map of Figure <ref type="figure" target="#fig_0">1</ref> (b) using a N = 7 × 7 neighborhood. A 7 × 7 neighborhood was used because it gives the best tradeoff between computational complexity and accuracy (see Section 7.4). Figures <ref type="figure" target="#fig_1">2 (b),</ref><ref type="figure">(c</ref>) demonstrate the difference between entropy sampling and uniform sampling with 1000 sampled points. Entropy sampled points (Fig. <ref type="figure" target="#fig_1">2  (b</ref>)) tend to lie much more frequently at edge points and at the border of the optic disc and cup. This makes it more likely for an algorithm to learn discriminative features between the optic disc and cup, and the background. However for uniform sampling (Fig. <ref type="figure" target="#fig_1">2 (c</ref>)) there are many points that lie on regions that are not particularly informative.</p><p>An added advantage of using entropy sampling is that it identifies informative points on various essential landmarks such as edges, blood vessels, etc which provide discriminative information. This is advantageous when we aim to learn Page 8 of 31 filters from a small dataset, where it is desirable to extract as much information as possible from the limited number of data samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Convolutional feature learning</head><p>The proposed method exploits the fact that convolutional networks are essentially an ensemble learning technique. Many of the characteristics of classical ensemble learning approaches also extend to CNNs. Convolutional networks are composed of individual convolutional filters that can be regarded as classifiers which together form an ensemble. Generally, CNNs have filters that are initialized to small random values and are incrementally adapted to a desired state using backpropagation <ref type="bibr" target="#b22">[23]</ref>. The random initialization ensures sufficient initial diversity in the filters. We use boosting to learn the diverse filters successively in a supervised fashion, each trying to minimize a weighted classification error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Filter learning</head><p>3 × 3 patches are extracted around each sampled point. This patch size gives the best tradeoff between computational complexity and generalization ability (Section 7.4). The number of parameters increases with the square of the patch width. Hence more data is required to reliably estimate more parameters. Experiments with 5 × 5 or 7 × 7 patches do not show improved segmentation accuracy and do not generalize better than the smaller 3×3 convolutional filters. Additionally, convolutional filters are learned on five different scales. The same filter size is learned on images and their downsampled versions. The scaling of different images is illustrated in Figure <ref type="figure" target="#fig_2">3</ref> to provide a better understanding of the coverage of the filters for different scales. Each scaled image of Figure <ref type="figure" target="#fig_2">3</ref> has a central green square of size 3 × 3 to illustrate the size of the convolutional filters. Evidently, a filter of size 3 × 3 will capture different levels of image information depending on the scale. Furthermore, scaling the image instead of using larger and larger filters enables a smaller filter to coarsely emulate larger filters.</p><p>To ensure that the filters do not learn redundant patterns, such as similar patterns with different magnitude, all patches are subjected to Local Contrast Normalization as in <ref type="bibr" target="#b16">[17]</ref>. Each patch is reshaped into a vector x patch , divided by its L 2 norm and its mean value subtracted. That is, x patch ← x patch x patch (6)</p><formula xml:id="formula_7">x patch ← x patch -µ patch<label>(7)</label></formula><p>where µ patch denotes the mean of the normalized patch vector and x patch is the ℓ 2 norm of the original patch vector. The convolutional filters are learned from these normalized patches using boosting. This is the core novelty of our proposed filter learning algorithm. To accomplish this, the following weighted ℓ 1 -norm optimization problem is solved for each filter individually minimize</p><formula xml:id="formula_8">w,b N i=1 v i • |y i -x T i w -b| + λ||w|| 1<label>(8)</label></formula><p>where y i ∈ Y = {-1, +1} is the binary label of a given point i, x i ∈ X patch represents the corresponding patch around point i in vector form. w is the convolutional filter in vector form that is to be learned, b is a learned constant offset, | • | is the vector dot product, and v i are the positive weights on an individual data point. The optimization environment CVX <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, a package for specifying and solving convex programs, was used to determine the final parameter values.</p><p>The above equation can be re-written as minimize</p><formula xml:id="formula_9">w,b N i=1 v i • |y i -x T i w| + λ||w|| 1<label>(9)</label></formula><p>w and x include offset b and an extra 1, respectively, in the column vector. The ℓ 1 -norm constraint is imposed since the labels are -1 A c c e p t e d M a n u s c r i p t or +1. We are interested in ensuring that each individual filter tries to solve the segmentation problem whose label is the sign of the expression in Eqn. 8. The algorithm does not focus on finding a way to minimize large square errors and hence the ℓ 2 -norm would not serve the purpose. The ℓ 1 -norm imposes lower penalty to deviations from the actual labels {-1, +1}.</p><p>In contrast to <ref type="bibr" target="#b16">[17]</ref>, consecutively learned filters are not required to be orthogonal to each other. Rather "exploration" of different filters is done through reweighting of data points based on Gentle AdaBoost as described in <ref type="bibr" target="#b25">[26]</ref>. Gentle AdaBoost is a version of AdaBoost <ref type="bibr" target="#b26">[27]</ref> which places less weight on outlier points. This is meant to generalize better by avoiding overfitting. In the context of imperfect labels and preliminary tests with the proposed algorithm, Gentle AdaBoost seems to indeed generalize better than the standard AdaBoost approach. Henceforth boosting refers to filters learned using Gentle AdaBoost.</p><p>Gentle AdaBoost is used in the following manner. Initialize the weights as</p><formula xml:id="formula_10">v i = 1</formula><p>m , for i = 1, . . . , m. For n = 1, . . . N : 1. Estimate the "weak" hypothesis h n (x), i.e. learn filter w and bias b in the optimization problem Eqn. 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Update weights</head><formula xml:id="formula_11">v i ← v i • exp(-y i h n (x i )) Z n<label>(10)</label></formula><p>with Z n chosen so that m i=1 v i = 1. The difference with respect to standard AdaBoost lies in the fact that the term in the exponential of Eqn. 10 has a weighting factor α = 1  2 log 1-ǫ ǫ when using AdaBoost. For Gentle AdaBoost the factor α is always set to 1. The factor α is determined by the error ǫ of the individual classifier, where a highly accurate classifier yields a high α and an inaccurate classifier possesses low α.</p><p>In addition to Gentle AdaBoost reweighting, samples of each class are reweighted such that each class is equally weighted, i.e. the sum of the weights of samples of each class is equal. This has been empirically shown to improve exploration of different convolutional filters. Sample learned filters are shown in Figure <ref type="figure">4</ref> where the first row shows filters learned for the optic disc and second row shows filters learned for optic cup segmentation. The two first filters of each scale are presented. Each of the filters are scaled to lie in the range of [0, 1] to demonstrate the pattern of weights in more comparable setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Convolutional network architecture</head><p>Figure <ref type="figure">5</ref> illustrates the architecture of our proposed CNN. Some of its salient points are:</p><p>• The proposed network has two fully connected layers.</p><p>• Filters are learned for 5 scales in the first layer and 4 scales in the second layer.</p><p>A c c e p t e d M a n u s c r i p t • The input to the second layer is the processed and max-pooled output of the first layer.</p><p>• The second layer has access to all the processed output of the first layer which makes the CNN fully connected.</p><p>• 6 filters are learned for each scale in the first layer giving a total of 6 × 5 = 30 filters in the first layer.</p><p>• 1 filter is learned for each scale in the second layer giving 4 filters.</p><p>• In total 30 + 4 = 34 convolutional filters are learned.</p><p>Convolutional filters are learned for each scale individually, implying that weights are reset for each scale and boosting is only applied within the same scale. For images at different scales, sampled points at one scale do not necessarily offer the same kind of information as points sampled at another scale. Furthermore, points sampled at a certain scale need not correspond to points sampled at the same position at a downsampled scale. The "original" point does not generally exist in the downsampled image. Consequently, new points are sampled for each scale. Filters are learned through optimization and reweighting as discussed in Section 5.1. The weighting factors are reset for each scale, since different points are sampled.</p><p>We observe that the same amount of coverage is achieved with far fewer sampled points since the total amount of available points has decreased. As a Page 12 of 31 A c c e p t e d M a n u s c r i p t heuristic, the number of sampled points in a scale is set to</p><formula xml:id="formula_12">N samples = 500 2 (#scale-1)<label>(11)</label></formula><p>where #scale refers to the number of the scale. The original scale has number one, the next scale of half the width and height has scale number two and subsequent scales are numbered accordingly. This enables the algorithm to learn smaller downsampled scales with higher scale number faster since less data points are sampled while at the same time having more coverage compared to the original image scale.</p><p>Each learned convolutional filter is later convolved with each input image at the respective scales. The standard discrete convolution operation applied by each filter is</p><formula xml:id="formula_13">y[i, j] = (I * K)[i, j] = m n I[m, n]K[i -m, j -n] (<label>12</label></formula><formula xml:id="formula_14">)</formula><p>where K is the kernel representation of filter w i of Eqn. 8 and I is the m × n × n maps dimensional image. Here y denotes the output of the convolution. The convolved images are then postprocessed to standardize them. Since these will later serve as input to the second convolutional layer, greater structured data is highly desirable. The output of the convolution of each filter is passed through a hyperbolic tangent saturation function as it gave the best accuracy in comparison to Relu and sigmoid functions. As previously done for preprocessing, the mean of the image is subtracted and all values divided by the standard deviation. The intensity values are rescaled to lie in the range [0, 1].</p><formula xml:id="formula_15">X patch-layer2 ← tanh(X patch-layer1 w + b) X patch-layer2 ← X patch-layer2 -µ patch-layer2 X patch-layer2 ← X patch-layer2 σ patch-layer2</formula><p>where µ patch-layer2 is the column-wise mean and σ patch-layer2 the column-wise standard deviation of patch matrix X patch-layer2 of dimension n 4 × k with n 4 the number of data points and k is the dimension of an individual patch.</p><p>The output of convolving filters with the input images highlights the image characteristics learned by the filter. Figure <ref type="figure" target="#fig_4">6</ref> shows the processed output of the first convolutional filters at each scale for the sample image shown in Figure <ref type="figure" target="#fig_0">1</ref> (a). The output of the filters becomes increasingly coarse and at the smaller scales the output closely resembles the optic disc we aim to segment, although their edges are blurred. Each filter focuses on different aspects of a given image. The combination of these diverse features or viewpoints is expected to offer better discriminative features than individual features. Images at lower scales are later upsampled to the original scale while convolving the learned filters.</p><p>The convolved images are passed through a max-pooling operation of dimension 2× 2 to introduce further robustness into the system <ref type="bibr" target="#b27">[28]</ref> In the next step postprocessed and max-pooled convolved images are stacked on top of each other resulting in images of size m × n × n maps , where n maps is equal to the number of filters learned in the first layer. These images are the input images to the second layer of the convolutional network. In the second layer, filters are learned for four different scales. The learning procedure of the optimization problem in Eqn. 8 is applied and the new set of learned filters combine the output of the previously learned filters in the first layer. In boosting <ref type="bibr" target="#b26">[27]</ref>, individual classifiers are generally combined by a weighted average of the form</p><formula xml:id="formula_16">C(x) = K2 i=1 α i c i (x).<label>(13)</label></formula><p>C(x) denotes the ensemble classifier, K 2 is the total number of individual classifiers and α i is the accuracy based weighting of the individual classifier c i (x).</p><p>The bagging approach of <ref type="bibr" target="#b28">[29]</ref> differs from conventional bagging in that all α i equal 1. Bagging averages the individual classifiers while boosting performs a weighted average. The filters in the second layer exhibit similar characteristics as the ensemble classifier in Eqn. 13. These can be written as</p><formula xml:id="formula_17">C conv (x) = K i=1 p∈patch w i,p c i (p)<label>(14)</label></formula><p>where C conv (x) is a filter of the second layer (or any further layers), p denotes the positions of points in the patch around point x, w i,p are the learned weights for the convolutional filter in layer two and c i (p) describes the processed output of convolutional filter i of the previous layer. Filters in the second layer act as an extended ensemble classifier as it considers not only output of individual classifiers (convolutional filters of a previous layer) but also spatial information around each point. This approach constructs an hierarchical ensemble learning framework using convolutional filters. Similar Page 14 of 31  to the first layer, the output of all filters of the second layer on the sample image is illustrated in figure <ref type="figure" target="#fig_6">7</ref>.</p><p>In summary, the following specifications for the convolutional network were used:</p><p>• Filters of size 3 × 3 × n maps are trained, where n maps = 3 corresponds to the number of color channels of each input image.</p><p>• Filters are learned on five scales in the first layer and four scales in the second layer. This is meant to give the local algorithm ( 3 × 3 filters) greater global coverage. Multiple scales indicate that the first set of filters are learned on the original image followed by their downsampled versions.</p><p>• Filters are learned using reweighting by applying Gentle AdaBoost.</p><p>• The convolutional filters are learned in a hierarchical manner.</p><p>• A max-pooling operation is performed on the processed output of the first layer, which serves as input to the second layer.</p><p>• A total of 6 filters per scale for 5 scales are trained in the first layer. 1 filter per scale for 4 scales are trained in the second layer.</p><p>• All configurations are used for optic cup and disc equally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Obtaining The Final Segmentation</head><p>After learning the filters, the training images are convolved with each of them to produce a set of 34 maps (equal to the number of learned filters). Note that there are two sets of 34 filters corresponding to optic cup and optic disc. 2000 points each belonging to optic cup and disc are sampled from each image, and the values from the convolved images are used as features. Additionally, the L*a*b color values of each sampled point are also included as a feature, giving a total of 37 features.</p><p>The features are used to train a softmax logistic regression classifier as in <ref type="bibr" target="#b16">[17]</ref>. The softmax classifier calculates the probability of a given point belonging A c c e p t e d M a n u s c r i p t to the optic disc, cup or the background. This is written in the form</p><formula xml:id="formula_18">h θ (x) =      P (y = 1|x; θ) P (y = 2|x; θ) . . . P (y = K|x; θ)      = 1 K i=1 exp(θ (i)T x)      exp θ (1)T x exp θ (2)T x . . . exp θ (K)T x     <label>(15)</label></formula><p>where θ i ∈ R n are the learned parameters for class i, x is the feature vector supplied to the classifier and y ∈ Y = {1, . . . , K} is the label corresponding to a given feature vector. The fraction</p><formula xml:id="formula_19">1 K i=1 exp(θ (i)T x)</formula><p>ensures that all values sum to 1, giving a probability distribution over all classes. The softmax regressor outputs probability maps such as the one displayed in Figure <ref type="figure" target="#fig_8">8 (a)</ref>.</p><p>A unsupervised graph cut algorithm <ref type="bibr" target="#b29">[30]</ref> is then applied to the probability map to obtain an initial segmentation of the disc as shown in Figure <ref type="figure" target="#fig_8">8 (b)</ref>. Image segmentation using graph cuts is a label assignment problem where each pixel is assigned a label l. Graph cut optimization finds a set of labels that minimizes the following energy function:</p><formula xml:id="formula_20">F (λ) = D(λ) + αR(λ)<label>(16)</label></formula><p>where D(λ) is the data dependent term, R(λ) is the regularization or smoothing term and α determines their relative contributions. The applied method finds the average intensities of different segments using a k-means approach. Then the difference of each pixel from these average values is subjected to a radial-basis function (rbf) kernel mapping. Equation 16 can be written as</p><formula xml:id="formula_21">F ({µ}, λ) = l∈L p∈R l (φ(µ l ) -φ(I p )) 2 + α {p,q}∈C r(λ(p), λ(q))<label>(17)</label></formula><p>where φ is the non-linear kernel transformation and r is the regularization term.</p><p>Minimizing the energy term in Eqn. 17 using graph cuts <ref type="bibr" target="#b30">[31]</ref> yields the desired unsupervised segmentation.</p><p>A convex hull transform is applied to the graph cut segmentation output. Given the oval shape of both the optic disc and cup it is apriori known that the desired shape is convex. Taking the convex hull of the output of the graph cut algorithm can combine previously disjoint regions into the optic disc or cup. Thus a better segmentation is achieved and the final segmentation is illustrated in figure <ref type="figure" target="#fig_8">8 (c</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">The DRISHTI-GS data set</head><p>Our method is validated on the DRISHTI-GS dataset <ref type="bibr" target="#b31">[32]</ref> which consists of 50 patient images obtained using 30 degree FOV at a resolution of 2896 × 1944. We use a 5 fold cross validation scheme with 40 training images and 10 test images in each fold. The ground truth disc and cup segmentation masks were obtained by a majority voting of manual markings by 4 ophthalmologists. Quantitative evaluation is based on F-score (F = 2 P × R/(P + R)) to measure the extent of region overlap and absolute pointwise localization error B in pixels (measured in the radial direction); P is precision and R is recall. Additionally we report the overlap measure S = Area(M ∩ A)/Area(M ∪ A). M is the manual segmentation while A is the algorithm segmentation.</p><p>To generate the "ground-truth" images four human experts segment the optic disc and cup in each image resulting in a softmap for the segmentation with values denoting the fraction of annotators who agree. These values vary from 0 to 1 in 0.25 increments, where zero corresponds to no experts segmenting a given pixel as cup or disc and 1 corresponds to all experts labeling the region as optic up or disc. A value of zero corresponds to background, while a value of 1 corresponds to the optic disc or cup. To further illustrate this point, the ground truth softmaps for optic disc and cup segmentation are displayed in figure <ref type="figure" target="#fig_9">9</ref>.</p><p>As is obvious from the figures, all experts do not always agree on the exact position of a segment, especially for optic cup segmentation. There is no "perfect" ground truth available. Only a "gold standard" is used which is defined to A c c e p t e d M a n u s c r i p t be the agreement of at least three of the four experts as specified by the authors of the data set <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Error measures</head><p>Similar to the error measures in <ref type="bibr" target="#b31">[32]</ref>, precision and recall values are calculated for each segmentation as follows</p><formula xml:id="formula_22">Precision = tp tp + f p , Recall = tp tp + f n (<label>18</label></formula><formula xml:id="formula_23">)</formula><p>where tp = count of true positive, f p =count of false positive and f n = count of false negative pixels. The F-score (F) is computed as the harmonic mean of precision and recall defined as:</p><formula xml:id="formula_24">F = 2 • Precision • Recall Precision + Recall .<label>(19)</label></formula><p>F-score values are in the range of [0, 1] with higher values indicating better performance.</p><p>Boundary localization Error: The distance between computed region boundary and ground truth is a metric for boundary localization. This error measurement is meant to indicate how well a given algorithm can find the object boundary when compared to the actual object boundary. Let C g and C o be the ground truth and computed boundary by a method, respectively. The distance (D, in pixels) between two curves is then defined as:</p><formula xml:id="formula_25">D = 1 N N -1 i=1 |(d n g ) -(d n o )| 2<label>(20)</label></formula><p>where, d n g and d n o are the distance from disk center to points on C g and C o , respectively in the angular direction indexed by n. N is set to 24 in this work to be comparable to <ref type="bibr" target="#b31">[32]</ref>.</p><p>Cup-to-disc ratio (CDR): After segmentation of optic disc and optic cup, the cup-to-disc ratio (CDR) is computed as the ratio of the maximal diameter of the optic cup to the maximal diameter of the optic disc</p><formula xml:id="formula_26">CDR = Cup diameter Disc diameter . (<label>21</label></formula><formula xml:id="formula_27">)</formula><p>To compare different methods their mean error of the CDR estimation and the standard deviation of CDR estimation errors are evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Entropy sampling</head><p>We analyze the effects of different neighborhood sizes on the output of entropy sampling. For different sizes of the entropy filter we calculate the F-score for optic cup segmentation, which is more challenging than disc segmentation. The results are shown in Table <ref type="table" target="#tab_0">1</ref>. All results were obtained using 5-fold crossvalidation. Since there is the chance that different points may be selected for Page 18 of 31   entropy and uniform sampling, we perform 10 runs of the entire pipeline in every fold to reduce any possible bias.</p><formula xml:id="formula_28">(a) (b) (c) (d) (e)</formula><p>For some window sizes entropy sampling performs better than uniform sampling. For very small neighborhoods (3 × 3, 5 × 5) the entropy map assigns high weights to many points and is noisy as a small neighborhood finds it difficult to identify informative samples. In contrast, larger neighborhoods are more robust in identifying informative points. However too big neighborhoods <ref type="bibr">(11 × 11)</ref> include a lot of superficial information from uninformative pixels. Figure <ref type="figure" target="#fig_11">10</ref> illustrates the results of entropy filtering using different neighborhoods. Since a 7 × 7 neighborhood gives the best results we use it for our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Kernel Size</head><p>We analyze the effects of different kernel sizes on the segmentation accuracy in terms of F-score for the optic cup. Table <ref type="table" target="#tab_1">2</ref> summarizes the results obtained using 5-fold cross-validation. The best results are obtained for 3 × 3 patches which is the motivation for using this patch size during learning.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.">Effect of Boosting vs. Bagging</head><p>In bagging variability is achieved by providing each individual classifier with a random subset of the training data. Boosting uses a more direct approach where previously incorrectly classified points receive higher weight than correctly classified samples in learning individual classifiers. Finding diverse and  accurate classifiers is essential for creating an accurate ensemble classifier. Table 3 highlights the difference in performance between Bagging and Boosting using 5-fold cross-validation and entropy sampling with a 7 × 7 neighborhood. Bagging in this case amounts to providing each convolutional filter with a different random subset of data points to learn from, where consecutively learned filters are required to be orthogonal to each other. Boosting on the other hand uses a reweighting policy based on Gentle AdaBoost to arrive at accurate yet distinct convolutional filters.</p><p>A closer look at the learning framework reveals that the most discriminative convolutional filter in each scale is the first one. Subsequently learned filters do not contribute nearly as much discriminative information. The difference in performance of Bagging and Boosting is that filters learned through Bagging tend to focus on very similar aspects of an image whereas Boosting allows consecutively learned filters to explore very different facets of the images. The sampling of different subsets in Bagging is not able to provide the same variability as reweighting does for Boosting. This difference in information gives Boosting a significant advantage over Bagging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6.">Optic disc and cup segmentation</head><p>The same algorithmic setup was used for both optic cup and optic disc segmentation and the results are compared to the benchmark methods on the used data set presented in <ref type="bibr" target="#b32">[33]</ref>. As mentioned before, our method is inspired from CNNs but is different from conventional CNN architectures. Hence we also compare the results of our method with a standard CNN architecture, referred to henceforth as CN N . The CN N is trained on the patches using 3 hidden layers having respectively 3, 4, 3 filters (or kernels) of size 3 × 3. Each convolutional layer is followed by a max-pooling operation which reduces the image dimension by a factor of 2 along both axes. The output of the third layer is the input to a fully connected layer whose output is the input to a soft-max classifier like our proposed method. A test image is subjected to the same sequence of operations and is classified using the soft-max classifier. The probability maps generated by the softmax classifier are processed using unsupervised graph cuts followed by a convex hull transform to get the final segmentation. Thus we see that the only difference between CN N and our P roposed method is the architecture.</p><p>Optic Disc: Optic disc segmentation is less challenging than optic cup segmentation and existing methods have already achieved high accuracy values. Table <ref type="table" target="#tab_4">4</ref> summarizes the segmentation performance of different methods. The competing methods are categorized as those which segment only the optic disk A c c e p t e d M a n u s c r i p t (D), optic cup (C) or both (D, C). Our method(P roposed) with F-score of 0.973 outperforms all the competing methods (which use hand crafted features) as is evident from the higher F and S values, and lower B values. The difference in F and S score values is also statistically significant since p &lt; 0.01 (from Student-t tests) for all methods compared to P roposed. CN N also performs better than previously proposed methods. The average performance measures of P roposed is higher than CN N , although statistical tests suggest a very small significance between two sets of results (p = 0.042). This quite clearly indicates that our proposed method compares well with standard CNN architectures and also outperforms them. The superior performance of our method can be attributed to the fact that Boosting allows the algorithm to explore the feature space and learn distinct representations of the training data.</p><p>Since <ref type="bibr" target="#b2">[3]</ref> is a superpixel based approach, pixels from different classes may be grouped in one superpixel which affects its performance. <ref type="bibr" target="#b5">[6]</ref> uses a modified Chan-Vese model, which finds it challenging to segment the optic disc using only intensity information. <ref type="bibr" target="#b4">[5]</ref> uses only morphological features which is good enough for disc segmentation, but does not perform as well for cup segmentation. <ref type="bibr" target="#b12">[13]</ref> was designed specifically for cup segmentation and hence performs well. However P roposed outperforms all these methods.</p><p>Figure <ref type="figure" target="#fig_15">11</ref> shows the comparative results of P roposed and selected other methods listed in Table <ref type="table" target="#tab_4">4</ref>. The example image is a typical case of glaucoma where the optic cup is enlarged and is almost as big as the disc. This example has a high CDR and has regions affected with PPA surrounding the optic disc. Our P roposed model outperforms even CN N and <ref type="bibr" target="#b5">[6]</ref> which are the best among the competing methods.</p><p>Certain images tend to throw the algorithm off track when evaluating the images as illustrated in Figure <ref type="figure" target="#fig_17">12</ref>. This could be due to somewhat local understanding of the algorithm. Additionally, the difficult cases for optic disc segmentation are mostly caused by peripapillary atrophy of the eye. This condition manifests in the images as a region around the optic disc that shares many of the characteristics of the optic disc such as color and texture. Due to this similarity, the convolutional filter algorithm is led astray by eliciting a similar response as for the correct optic disc region. A more global understanding of the image such as exhibited by a human observer would help to remedy this problem.</p><p>Figure <ref type="figure" target="#fig_17">12</ref> highlights the best case and worst case scenario for optic disc segmentation using CN N . In the best case a nearly perfect segmentation can be achieved. In the worst case the algorithm misses part of the optic disc region, possibly due to PPA.</p><p>Optic Cup: Segmentation of the optic cup is more challenging than optic disc. This is especially marked by the disagreement in segmentation of clinical experts. P roposed gives the best results for cup segmentation, followed by CN N . The next best results are obtained by <ref type="bibr" target="#b5">[6]</ref> using their r-bends technique that identifies the region of bending of blood vessels. However vessel bends can sometimes throw up erroneous candidates for cup boundary. Our convolutional filter based scheme using Boosting seems more reliable than other algorithms  for detecting the optic cup and hence outperforms CN N and <ref type="bibr" target="#b5">[6]</ref> by a significant margin. Figure <ref type="figure" target="#fig_17">13</ref> shows the best case and worst case result of our method for the optic cup. In the worst case scenario the algorithm experiences difficulties with images having a particularly small optic cup region. The size of small optic cup regions tend to be overestimated. However, from the overall results we can conclude that learning features through convolutional networks is of advantage in the challenging task of optic cup segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.7.">Cup-to-disc ratio</head><p>Cup-to-disc ratio (CDR) is calculated from the optic disc and cup segmentations by determining the ratio of the maximal diameter of the optic cup and the optic disc. The obtained CDR estimation results are compared to the performance obtained in <ref type="bibr" target="#b32">[33]</ref>. All results were obtained using 5-fold cross-validation. The performance values are displayed in table 5. As can be seen, the proposed method outperforms the combination of the best method for optic disc and optic cup segmentation used in <ref type="bibr" target="#b32">[33]</ref>. Higher accuracy for optic cup and disc segmentation makes the overall CDR estimation of our method more accurate than other methods, including CN N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CN N P roposed</head><p>[33] Expert-1 0.131±0.12 0.112 ± 0.08 0.15±0.12 Expert-2 0.11±0.09 0.098±0.074 0.13 ± 0.10 Expert-3 0.095±0.08 0.081 ± 0.062 0.10 ± 0.     <ref type="bibr" target="#b12">[13]</ref>. Note that <ref type="bibr" target="#b4">[5]</ref> segments only the disc while <ref type="bibr" target="#b12">[13]</ref> segments only the cup. Green contour is ground truth while blue contour is algorithm segmentation.</p><p>disc from the images. We then fit ellipses through the OC and OD, and calculate the CDR as the ratio of the vertical diameters. A 5-fold cross validation was then used to determine classification accuracy. We include all 85 normal subject images and 70 glaucoma cases for our evaluation. For every test fold we pick one-fifth of the dataset (17 normal and 14 glaucoma) for testing and the remaining for training. The average classification and segmentation results are reported in Table <ref type="table" target="#tab_9">7</ref> and Table <ref type="table" target="#tab_8">6</ref>. We observe that similar to Table <ref type="table" target="#tab_4">4</ref>, our proposed method performs better than competing methods. Table <ref type="table" target="#tab_9">7</ref> also highlights that our method had the best performance in terms of sensitivity (fraction of correctly identified glaucoma cases), specificity (fraction of correctly identified normal cases) and overall accuracy. This clearly indicates that by using our proposed method we can accurately identify normal and glaucoma cases. The median CDR for normal cases was 0.63 and 0.76 for glaucoma patients. Note that in Table <ref type="table" target="#tab_9">7</ref> we only present results for the methods common to both OC and OD segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.8.">Optic Disc Segmentation On MESSIDOR Dataset</head><p>We apply our method to the MESSIDOR dataset for optic disc segmentation, and the results are summarized in Table <ref type="table" target="#tab_10">8</ref>. Results are shown in terms of Jaccard index which has been used to validate the performance of different algorithms on the dataset. Our method outperforms three competing methods and hence justifies our approach to learn convolutional filters.   architecture between the two can be chosen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A c c</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.10.">Computation Time</head><p>Our whole pipeline was implemented in MATLAB on a 2.66 GHz quad core CPU running Windows 7. Training our algorithm takes around 15 to 20 minutes for 40 images (average 17.1 minutes per cross validation cycle). However segmenting a test image is very fast with an average of 5.3 seconds per image. Further increases can be expected when applying parallelization to processes that need not be calculated consecutively. Correspondingly, for CN N the training time is 37 -42 minutes for 40 images (an average of 39.4 minutes per cross validation cycle), and the segmentation time for a test image is 8.1 seconds per image. These numbers clearly highlight the computational efficiency of our method, even on a small dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.11.">Algorithm Limitations</head><p>Some of the limitations of our algorithm are</p><p>• Data: Working with only 50 images for training does not permit a CNN inspired algorithm to develop its full potential. Very few filters were trained when compared to other CNN methods. Additionally, it is not clear that the provided images capture all the variability of retinal images. Abnormal exception cases might not be represented in the data set.</p><p>• Algorithm: The proposed method might prove to be not as directed as CNNs trained using backpropagation. Furthermore due to the sampling  of points of the provided images there is a stochastic element in the algorithm. This means that the same performance cannot always be repeated. Interestingly, sampling more points does not in fact improve results but might simply reduce the fluctuation in performance. More research needs to be conducted into entropy sampling and how to ensure that sampling "interesting" points also translates into a better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Disussion and conclusion</head><p>In this paper we have proposed a general framework for learning most discriminative representations of the training data in the form of convolutional filters. This eliminates the need for designing hand crafted features which are not always robust for different tasks. Our proposed CNN inspired ensemble learning architecture has been shown to better the state-of-the-art on the public DRISHTI-GS data set. From a research point of view our work makes two main contributions. First, a novel entropy sampling method is proposed that allows the algorithm to considerably reduce its computational effort while performing better than the simple uniform sampling approach. Secondly, building upon this technique, an original framework for learning convolutional filters in a principled manner using reweighted boosting was described. The aim behind this learning framework is twofold. First, a novel way of training convolutional filters in a network architecture was explored. Second, we expect the proposed method will be more amenable to theoretical insights into the fundamental principles governing CNNs or even deep neural networks (DNN) in general. These insights might be based on the realization that many of the characteristics of the proposed method are equivalent to ensemble learning systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A c c e p t e d M a n u s c r i p t</head><p>Although a deep CNN network trained on numerous patches from the same dataset outperforms our method, it is not possible to reliably train a deep CNN from 50 images. However it is not possible to reliably train a deep CNN using a few images. Hence our proposed method can be used to learn convolutional filters when a small dataset is available.</p><p>Experimental results show that for the same number of samples, entropy sampling achieves superior results to uniform sampling. Similarly, boosting filters has shown to yield improved results when compared to bagged filters. This clearly indicates that boosting produces more discriminative filters than bagging. For convolutional network standards, the used DRISHTI-GS data set is rather small. In this particular case, it was essential to have an effective learning procedure that focused on the most important information. Having less data meant on the one hand that there was a smaller computational burden placed on any algorithm. On the other hand, less data means that less parameters can be reliably trained. Consequently, the proposed convolutional network is rather small compared to traditional CNN architectures. Nevertheless, the learned network has proved to be effective by out-competing several other methods that use hand crafted features. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) Image before pre-processing; and (b) image after preprocessing</figDesc><graphic coords="7,139.74,124.80,93.60,93.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) Output of "total entropy" map using N = 7 × 7; (b) entropy sampled points; and (c) uniformly sampled points.</figDesc><graphic coords="9,275.78,126.73,56.34,56.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) Scale 1; (b) Scale 2; (c) Scale 3; (d) Scale 4; and (e) Scale 5. The green square illustrates the coverage of the same 3 × 3 filter on images of different scale.</figDesc><graphic coords="10,161.84,195.92,53.80,56.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Illustration of the learned convolutional filters for optic disc and cup</figDesc><graphic coords="12,182.75,245.68,218.21,87.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Illustration of sample image convolved with the first learned filters of each scale (total of 5 scales): a) Filter 1 at scale 1; b) Filter 1 at scale 2; c) Filter 1 at scale 3 ;d) Filter 1 at scale 4; e) Filter 1 at scale 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Illustration of the second layer output for sample image with one filter learned per scale: (a) Filter learned at scale 1; (b) Filter learned at scale 2; (c) Filter learned at scale 3; (d) Filter learned at scale 4.</figDesc><graphic coords="15,146.37,124.83,53.80,59.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: a) Output of softmax classifier for sample image; b) Graph cut partitioning of probability classification map for sample image; c) Final segmentation of sample image after applying a convex hull transformation on graph cut output.</figDesc><graphic coords="17,153.34,249.54,70.80,59.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Illustration of softmap segmentations. a) Original image from the DRISHTI-GS data set [32]; b) ground truth of optic disc; c) ground truth of optic cup</figDesc><graphic coords="17,268.54,249.54,70.80,59.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>.11 0.84±0.09 0.87±0.1 0.85±0.07 0.82 ±0.<ref type="bibr" target="#b12">13</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Illustration of total entropy for different neighborhoods; (a) 3 × 3; (b) 5 × 5; (c) 7 × 7; (d) 9 × 9; (e) 11×11.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>.18 0.85±0.14 0.84±0.11 0.84±0.15 0.82 ±0.<ref type="bibr" target="#b22">23</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Segmentation results for different methods: (a) our P roposed model; (b)<ref type="bibr" target="#b4">[5]</ref>; (c)<ref type="bibr" target="#b5">[6]</ref> ; (d)<ref type="bibr" target="#b2">[3]</ref>; (e) CN N ; (f)<ref type="bibr" target="#b12">[13]</ref>. Note that<ref type="bibr" target="#b4">[5]</ref> segments only the disc while<ref type="bibr" target="#b12">[13]</ref> segments only the cup. Green contour is ground truth while blue contour is algorithm segmentation.</figDesc><graphic coords="23,152.02,236.28,73.45,83.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>1 .</head><label>1</label><figDesc>An ensemble learning based architecture to learn convolutional filters 2. Use of boosting as a computationally efficient learning framework 3. Accurate networks learned from few sample data Highlights (for review)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of F-score for different sizes of entropy filter, and its comparison with uniform sampling</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of F-score for different kernel sizes used in optic cup segmentation.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of performance of Bagging and Boosting</figDesc><table><row><cell>F-score</cell><cell>0.830 ± 0.12</cell><cell>0.86 ± 0.09</cell></row><row><cell>Precision</cell><cell>0.81 ± 0.20</cell><cell>0.84 ± 0.1</cell></row><row><cell>Recall</cell><cell>0.88 ± 0.1</cell><cell>0.92 ± 0.07</cell></row><row><cell cols="3">Boundary loc. (px) 18.05 ± 13.62 16.53 ± 11.807</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Segmentation accuracy in terms of F score, overlap and boundary distance for different methods. D, C indicate if the method segments the optic disc or optic cup or both. B is in pixels; F -F score; S-overlap measure; B-boundary error.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>CDR errors when compared to each individual expert and the average of all experts.Results on RIM-ONE v3 Dataset:The RIM ONE dataset<ref type="bibr" target="#b34">[35]</ref> consists of 159 stereo retinal fundus images with optic disc and cup ground truth. The reference segmentations have been provided by two expert ophthalmologists. The dataset has 85 normal cases (no presence of glaucoma) and 74 confirmed glaucoma cases. We used our proposed method to segment the optic cup and</figDesc><table><row><cell>Page 22 of 31</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Segmentation accuracy of RIM-ONE in terms of F score, overlap and boundary distance for different methods. D, C indicate if the method segments the optic disc or optic cup or both. B is in pixels; F -F score; S-overlap measure; B-boundary error.</figDesc><table><row><cell cols="3">P roposed CN N [3]</cell><cell>[34] [6]</cell></row><row><cell>Sen 92.3</cell><cell>90.1</cell><cell cols="2">87.4 86.4 89.8</cell></row><row><cell>Spe 95.6</cell><cell>94.3</cell><cell cols="2">92.5 92.0 94.0</cell></row><row><cell>Acc 94.1</cell><cell>92.4</cell><cell cols="2">90.2 89.4 92.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Glaucoma classification accuracy for RIM-ONE using CDR values in terms of Sen, Spe and Acc.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Segmentation accuracy for MESSIDOR dataset in terms of Jaccard index for optic disc.</figDesc><table><row><cell></cell><cell cols="2">Optic Disc</cell><cell cols="2">Optic Cup</cell></row><row><cell></cell><cell cols="2">P roposed [39]</cell><cell cols="2">P roposed [39]</cell></row><row><cell>F</cell><cell>97.3</cell><cell>97.6</cell><cell>87.1</cell><cell>87.5</cell></row><row><cell>S</cell><cell>91.4</cell><cell>91.9</cell><cell>85.0</cell><cell>85.7</cell></row><row><cell>B</cell><cell>9.9</cell><cell>9.4</cell><cell>10.2</cell><cell>9.8</cell></row><row><cell cols="2">HD 10.1</cell><cell>9.5</cell><cell>10.5</cell><cell>9.8</cell></row><row><cell>p</cell><cell>-</cell><cell cols="2">0.042 -</cell><cell>0.039</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Segmentation accuracy in terms of F score, overlap and boundary distance for different methods. D, C indicate if the method segments the optic disc or optic cup or both. B is in pixels; F -F score; S-overlap measure; B-boundary error; HD-95 percentile Hausdorff distance; p-result of paired t-test (between the F-values of our proposed method and the given method).</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Best: Ground Truth P roposed</p><p>Worst: Ground Truth P roposed </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.9.">Comparision with Deep CNNs</head><p>The primary novelty of our approach is a method which can be used to learn convolutional filters when a very small dataset is available that cannot be used for reliably training deep CNNs. However, we train deep CNNs using patches from the original images and compare their performance with our method. We extract 3, 000, 000 patches to train the architecture proposed in <ref type="bibr" target="#b38">[39]</ref>. It proposes a novel skip architecture for pixel wise segmentation of a given image. The results are summarized in Table <ref type="table">9</ref>. The deep CNN outperforms our method by a small degree, which is not surprising given that this architecture is considerably deeper than our proposed architecture. This clearly illustrates that our method may not perform as well as deep networks when a large image database is available. However, it is not possible to train the skip architecture of <ref type="bibr" target="#b38">[39]</ref> using only 50 images. Therefore, depending upon the size of the database, an appropriate </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Julian Zilly</head><p>Julian is a M.Sc student in the department of Mechanical Engineering at ETH Zurich. His interests are in the field of convolutional neural networks and applying them to different problems such as medical image analysis and robotics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dwarikanath Mahapatra</head><p>Dwarikanath is currently a research scientist at IBM Research Melbourne. He was a post-doctoral research scholar at the Department of Computer Science, ETH Zurich. His interests are in applying machine learning for improving healthcare systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joachim Buhmann</head><p>Joachim M. Buhmann is full Professor for Computer Science at ETH Zurich since October 2003. His research interests cover the area of pattern recognition and data analysis, computer vision and image analysis, remote sensing and bioinformatics. He also serves on the board of IEEE Transactions on Neural Networks and of IEEE Transactions on Image Processing.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Vision 2020. the right to sight. global initiative for the elimination of avoidable blindness</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>WHO Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The papilla as screening parameter for early diagnosis of glaucoma</title>
		<author>
			<persName><forename type="first">G</forename><surname>Michelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wärntges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deutsches Ärzteblatt International</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="34" to="35" />
			<date type="published" when="2008-08">August 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Superpixel classification based optic disc and optic cup segmentation for glaucoma screening</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1019" to="1032" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Screening for diabetic retinopathy</title>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">E</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Nathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><forename type="middle">A</forename><surname>Fogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Schachat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Internal Medicine</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="660" to="671" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detecting the optic disc boundary in digital fundus images using morphological edge detection and feature extraction techniques</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aquino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gegundez-Arias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1860" to="1869" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optic disk and cup segmentation from monocular color retinal images for glaucoma assessment</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Krishnadas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1192" to="1205" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Depth discontinuity-based cup segmentation from multiview color retinal images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Krishnadas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1523" to="1531" />
			<date type="published" when="2012-06">June 2012</date>
		</imprint>
	</monogr>
	<note>Biomedical Engineering</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Coupled sparse dictionary for depth-based cup segmentation from single color fundus image</title>
		<author>
			<persName><forename type="first">Arunava</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayanthi</forename><surname>Sivaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention MICCAI 2014</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Polina</forename><surname>Golland</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nobuhiko</forename><surname>Hata</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christian</forename><surname>Barillot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joachim</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Robert</forename><surname>Howe</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8673</biblScope>
			<biblScope unit="page" from="747" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Glaucoma risk index: Automated glaucoma detection from color fundus images</title>
		<author>
			<persName><forename type="first">Rüdiger</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörg</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lazlo</forename><forename type="middle">G</forename><surname>Nyul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joachim</forename><surname>Hornegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Michelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="471" to="481" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A field of experts model for optic cup and disc segmentation from retinal fundus images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mahapatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ISBI</title>
		<meeting>IEEE ISBI</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="218" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Obtaining consensus annotations for retinal image segmentation using random forest and graph cuts</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mahapatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICCAI-OMIA</title>
		<meeting>MICCAI-OMIA</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semi-supervised learning and graph cuts for medical image segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mahapatra</surname></persName>
		</author>
		<imprint>
			<publisher>In Press. Comp. Vis. Imag. Und</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Optic cup segmentation for glaucoma detection using low rank superpixel representation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICCAI Part 1</title>
		<meeting>MICCAI Part 1</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="788" to="795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Representation learning: A unified deep learning framework for automatic prostate MR segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICCAI, part 2</title>
		<meeting>MICCAI, part 2</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="254" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recognizing handwritten digits using hierarchical products of experts</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mayraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2002-02">Feb 2002</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="189" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Products of hidden markov models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>GCNU TR 2000-008</idno>
		<imprint>
			<date type="published" when="2000-11">November 2000</date>
		</imprint>
		<respStmt>
			<orgName>Gatsby Computational Neuroscience Unit, University College London</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stacked multiscale feature learning for domain independent medical image segmentation</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karteek</forename><surname>Popuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Cobzas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning in Medical Imaging</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Guorong</forename><surname>Wu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Daoqiang</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Luping</forename><surname>Zhou</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8679</biblScope>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep neural networks segment neuronal membranes in electron microscopy images</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2843" to="2851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional networks can learn to generate affinity graphs for image segmentation</title>
		<author>
			<persName><forename type="first">Srinivas</forename><forename type="middle">C</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">F</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viren</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Helmstaedter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Briggman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Winfried</forename><surname>Denk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Sebastian</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="511" to="538" />
			<date type="published" when="2009">2015/05/17 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shape and arrangement of columns in cat&apos;s striate cortex</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Physiology</title>
		<imprint>
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Book in preparation for MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Comparison of different color spaces for image segmentation using graph-cut</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronny</forename><surname>Hänsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Hellwich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Computer Vision Theory and Applications</title>
		<imprint>
			<publisher>SCITEPRESS Digital Library</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Flexible, high performance convolutional neural networks for image classification</title>
		<author>
			<persName><forename type="first">Dan</forename><forename type="middle">C</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ueli</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Second International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1237" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">CVX: Matlab software for disciplined convex programming, version 2.1</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-03">Mar. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graph implementations for nonsmooth convex programs</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Learning and Control</title>
		<title level="s">Lecture Notes in Control and Information Sciences</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Blondel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Kimura</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer-Verlag Limited</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="95" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Using adaboost classifiers in a hierarchical framework for classifying surface images of marble slabs</title>
		<author>
			<persName><forename type="first">Hatice</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olcay</forename><surname>Akay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="8814" to="8821" />
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A short introduction to boosting</title>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Evaluation of pooling operations in convolutional architectures for object recognition</title>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Artificial Neural Networks: Part III</title>
		<meeting>the 20th International Conference on Artificial Neural Networks: Part III<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="92" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="123" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multiregion image segmentation by parametric kernel graph cuts</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Salah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mitiche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">B</forename><surname>Ayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="545" to="557" />
			<date type="published" when="2011-02">Feb 2011</date>
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast approximate energy minimization via graph cuts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1222" to="1239" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Drishti-gs: Retinal image dataset for optic nerve head(onh) segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="53" to="56" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A comprehensive retinal image dataset for the assessment of glaucoma from the optic nerve head analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnadas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Syed Tabish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JSM BIOMEDICAL IMAGING DATA PAPERS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1004</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Level set based automatic cup to disc ratio determination using retinal fundus images in argali</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE EMBC</title>
		<meeting>IEEE EMBC</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="2266" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Estimation of the relative amount of hemoglobin in the cup and neuro-retinal rim using stereoscopic color fundus images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Pena-Betancor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gonzalez-Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fumero-Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sigut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alayon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>De La Rosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Invest Ophthalmol Vis Sci</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1562" to="1568" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast localization and segmentation of optic disk in retinal images using directional matched filtering and level sets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Barriga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Agurto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Echegaray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Pattichis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Soliz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Tech. in Biomed</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="644" to="657" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Detecting the optic disc boundary in digital fundus images using morphological, edge detection, and feature extraction techniques</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aquino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Gegundez-Arias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1860" to="1869" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Accurate and reliable segmentation of the optic disc in digital fundus images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Giachetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ballerini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Trucco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">24001</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
