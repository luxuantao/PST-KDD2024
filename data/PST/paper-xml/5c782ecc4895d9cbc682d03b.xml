<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Texture Mixing and Texture Movie Synthesis using Statistical Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ziv</forename><surname>Bar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science</orgName>
								<orgName type="institution">The Hebrew University</orgName>
								<address>
									<postCode>91904</postCode>
									<settlement>Jerusalem</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joseph</forename><surname>Ran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science</orgName>
								<orgName type="institution">The Hebrew University</orgName>
								<address>
									<postCode>91904</postCode>
									<settlement>Jerusalem</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">El</forename><forename type="middle">-</forename><surname>Yaniv</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science</orgName>
								<orgName type="institution">The Hebrew University</orgName>
								<address>
									<postCode>91904</postCode>
									<settlement>Jerusalem</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science</orgName>
								<orgName type="institution">The Hebrew University</orgName>
								<address>
									<postCode>91904</postCode>
									<settlement>Jerusalem</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Michael</forename><surname>Werman</surname></persName>
							<email>werman@cs.huji.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science</orgName>
								<orgName type="institution">The Hebrew University</orgName>
								<address>
									<postCode>91904</postCode>
									<settlement>Jerusalem</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Texture Mixing and Texture Movie Synthesis using Statistical Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0634FDF8F0D8A36863690E97AF5D895F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an algorithm based on statistical learning for synthesizing static and time-varying textures matching the appearance of an input texture. Our algorithm is general and automatic, and it works well on various types of textures including 1D sound textures, 2D texture images and 3D texture movies. The same method is also used to generate 2D texture mixtures that simultaneously capture the appearance of a number of different input textures. In our approach, input textures are treated as sample signals generated by a stochastic process. We first construct a tree representing a hierarchical multi-scale transform of the signal using wavelets. From this tree, new random trees are generated by learning and sampling the conditional probabilities of the paths in the original tree. Transformation of these random trees back into signals results in new random textures. In the case of 2D texture synthesis our algorithm produces results that are generally as good or better than those produced by previously described methods in this field. For texture mixtures our results are better and more general than those produced by earlier methods. For texture movies, we present the first algorithm that is able to automatically generate movie clips of dynamic phenomena such as waterfalls, fire flames, a school of jellyfish, a crowd of people, etc. Our results indicate that the proposed technique is effective and robust.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Texture synthesis is an interesting and important problem in the field of computer graphics. Recently, several techniques have emerged in the computer graphics literature that are able to analyze an input texture sample and synthesize many new random similar-looking textures <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34]</ref>. This work extends these techniques in several important ways. First, we describe a new texture mixing algorithm -a statistical learning algorithm that operates on several different input texture samples to synthesize a new texture. This new texture is statistically similar to all of the input samples and it exhibits a mixture of their features. Second, we extend our approach from the domain of static textures to the domain of texture movies: dynamic, time-varying textures, or TVTs for short. More specifically, we present a technique capable of generating a sequence of frames, corresponding to temporal evolution of a natural texture or pattern, that appears similar to an input frame sequence. For example, using this technique we generated short movies of dynamic phenomena, such as waterfalls, fire flames, a school of jellyfish, turbulent clouds, an erupting volcano, and a crowd of people. The generated sequences are distinguishable from the original input sample, yet they manage to capture the essential perceptual characteristics and the global temporal behavior observed in the input sequence. A specialized version of this method, described in a separate paper <ref type="bibr" target="#b0">[1]</ref>, is able to synthesize 1D sound textures, such as sounds of traffic, water, etc.</p><p>The natural applications of texture movie synthesis are in special effects for motion pictures and television, computer-generated animation, computer games, and computer art. Our method allows its user to produce many different movie clips from the same input example. Thus, a special effects technical director could fill an entire stadium with ecstatic fans from a movie of a small group, or populate an underwater shot with schools of fish. Designers of 3D virtual worlds will be able to insert animations of clouds, smoke, and water from a small number of input samples, without ever repeating the same animation in different places. However, these are by no means the only applications of such a technique. For example, methods for statistical learning of 2D texture images have been successfully applied not only to texture synthesis, but also to texture recognition and image denoising <ref type="bibr" target="#b9">[10]</ref>. These applications are made possible by realizing that statistical learning of 2D textures implicitly constructs a statistical model describing images of a particular class. Similarly, our approach for learning TVTs can be used as a statistical model suitable for describing TVTs. Therefore, it should be possible to apply this statistical model for tasks such as classification and recognition of such movie segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>Most previous work in texture synthesis has focused on the development of procedural textures, where complex interesting patterns are produced by a program executed before or during the shading process <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>. Although some of the most compelling synthetic imagery has been produced with the aid of procedural textures, the disadvantage of this approach is that it can be difficult to control and/or predict the outcome of such shaders. Furthermore, there is no general systematic way of generating textures matching the appearance of a particular texture. The latter difficulty gave rise to several algorithms that analyze an input texture and synthesize new random similar-looking textures <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34]</ref>. Our work can be viewed as extensions of De Bonet's approach <ref type="bibr" target="#b7">[8]</ref> to multiple input samples and to time-varying textures.</p><p>Texture mixing, a process of generating a new texture that contains features from several different input textures, has been addressed in several previous works. Burt and Adelson <ref type="bibr" target="#b4">[5]</ref> produce smooth transitions between different textures by weighted averaging of the Laplacian pyramid coefficients of the textures. This technique is very effective for seamless image mosaicing, but is less suitable for producing a mix of textures across the entire image, as will be demonstrated in section 5.2. Heeger and Bergen <ref type="bibr" target="#b17">[18]</ref> use their histogram-based texture synthesis algorithm to generate texture mixtures in which the color comes from one texture, while the frequency content comes from another. In this work we produce different kinds of mixtures, in which both colors and frequencies are mixed together.</p><p>To our knowledge, there have not been any previous attempts towards statistical learning of TVTs from input samples. So far, synthesis of dynamic natural phenomena has mostly been possible only via computationally intensive physically based simulations. For example, steam, fog, smoke, and fire have been simulated in this manner <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>. Explosions, fire, and waterfalls have been successfully simulated by animated particle systems <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref>. Simplified physically-based models have also been used to produce synthetic waves and surf <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21]</ref>. While the techniques mentioned above have been able to generate impressive results of compelling realism, a custom-tailored model must be developed for each type of simulated phenomena. Furthermore, such simulations are for the most part expensive and, more importantly, the resulting animations can be difficult to control. In contrast, as we shall see, statistical learning of TVTs is an extremely general, automatic, and fast alternative, provided that an example of the desired result is available.</p><p>Heeger and Bergman <ref type="bibr" target="#b17">[18]</ref> applied their texture synthesis technique to the generation of 3D solid textures (in addition to 2D texture images). However, in a solid texture all three dimensions are treated in the same manner, whereas in a 3D TVT the temporal dimension must be treated differently from the two spatial dimensions, as shall be explained in the next section. Another difference between their work and ours is that their method learns the statistical properties of a 2D texture and then generates a 3D texture with the same properties, whereas our method analyzes a 3D signal (TVT) directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Overview of Approach</head><p>Texture images are examples of approximately stationary 2D signals. We assume such signals to be generated by stochastic processes. This paper introduces a new approach for the statistical learning of such signals:</p><p>1. Obtain one or more training samples of the input signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Construct a hierarchical multi-resolution analysis (MRA) of each signal sample.</head><p>Each MRA is represented as a tree, assumed to have emerged from an unknown stochastic source.</p><p>3. Generate a new random MRA tree by statistically merging the MRA trees of the input samples.</p><p>4. Transform the newly generated MRA back into a signal, yielding a new texture that is statistically and perceptually similar to each of the inputs, but at the same time different from them.</p><p>Note that the procedure outlined above is applicable to general n-dimensional signals, although it is practical only for small values of n (since its time and space complexities are exponential in n).</p><p>Since the tree merging algorithm in stage 3 above involves random sampling, each invocation of the algorithm results in a different output texture. Thus, many different textures can be produced from the same input. If all of the input samples are taken from the same texture, we obtain a 2D texture synthesis algorithm similar to that of De Bonet <ref type="bibr" target="#b7">[8]</ref>. If the input samples come from different textures, the result is a mixed texture.</p><p>A naive extension of the above approach to generation of TVTs would be to independently synthesize a new frame from each frame in the input sequence. However, this method fails to capture the temporal continuity and features of the input segment. In contrast, the approach presented in this paper is to synthesize all three dimensions of the TVT simultaneously.</p><p>As in the 2D texture case, we assume that a time-varying texture is generated by a stochastic process. It is a 3D signal S(x, y, t), where x and y are the spatial coordinates of the signal (pixel coordinates in each frame), and t is the temporal coordinate (frame number). By applying the approach outlined above to 3D signals, with an appropriately chosen MRA scheme, we obtain a statistical learning algorithm for TVTs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Discussion</head><p>The current work extends previous methods for 2D texture synthesis to the case of texture movies. This extension is by no means straightforward, as several important issues must be dealt with.</p><p>The most significant difference between the two problems stems from the fundamental difference between the temporal dimension of TVTs and the two spatial dimensions. In a 2D texture, one cannot define a single natural ordering between different pixels in the image: a human observer looks at the entire image, rather than scanning the image from left to right, or from top to bottom. Thus, the x and y dimensions of the image are treated in the same way by 2D texture synthesis methods. In contrast, there is a clear and natural ordering of events in a texture movie, and a human observer watches the sequence in that order (from the first frame to the last). This indicates that the temporal dimension should be analyzed differently from the spatial ones.</p><p>Another practical difficulty with TVT synthesis stems from the higher dimensionality of the texture. A naive extension of the 2D analysis filters into 3D drastically increases the MRA construction time. Moreover, a naive extension of the 2D synthesis technique into 3D results in prohibitive synthesis times. Therefore, we have introduced various modifications both in the analysis and the synthesis stages of TVTs.</p><p>Finally, the methods that deal with 2D texture synthesis usually operate on an n ¢ n image.</p><p>However, in the texture movies case the input movie usually has dimensions n¢n¢r where r n. This must be properly accounted for in the analysis and synthesis stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview of the paper</head><p>The rest of this paper is organized as follows. In the next two sections (2 and 3) we review the mathematical background necessary for the detailed exposition of our approach. Section 4 presents the multiple source statistical learning algorithm. In Section 5 we demonstrate the application of our algorithm to 2D texture synthesis and texture mixing. In Section 6 we describe the extension of our algorithm to the synthesis of TVTs. Section 7 concludes this work and offers directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Statistical Learning</head><p>We first informally define and explain a few terms needed for our discussion.  <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">33]</ref>). Basseville et al. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref> provide a theoretical formulation of statistical modeling of signals by multi-resolution structures. In particular, these results consider a generative stochastic source (that can randomly generate multi-resolution structures), and study their statistical properties. They show that stationary signals (informally, a signal is stationary if the statistics it exhibits in a region is invariant to the region's location) can be represented and generated in a hierarchical order, where first the coarse, low resolution details are generated, and then the finer details are generated with probability that only depends on the already given lower resolution details. In particular, hierarchical representation was successfully applied to modeling the statistics of two-dimensional textures <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>The assumption that the signal is emitted from a stationary source entails that its pyramidal (tree) representation exhibits the following property. All the paths from the root to all nodes of the same level have the same distribution and any such path can be effectively modeled via a finite order stochastic process. We adopt this view and develop an algorithm that learns the conditional distributions of a source. We transform the sample to its multi-resolution, tree representation and learn the conditional probabilities along paths of the tree, using an estimation method for linear sequences. Thus, the problem is reduced to one of learning statistical sources of sequences, which are simply paths in the representing tree.</p><p>The particular estimation algorithm for sequences we chose to use is an extension of the algorithm due to El-Yaniv et al. <ref type="bibr" target="#b15">[16]</ref>, which operates on sequences over a finite alphabet, rather than on real numbers. Given a sample sequence S, this algorithm generates new random sequences which could have been generated from the source of S. In other words, based only on the evidence of S, each new random sequence is statistically similar to S. The algorithm generates the new sequence without explicitly constructing a statistical model for the source of S. This is done as follows: suppose we have generated s i , the first i symbols of the new sequence. In order to choose the next, (i + 1)-st symbol, the algorithm searches for the longest suffix of s i in S. Among all the occurrences of this suffix in S, the algorithm chooses one such occurrence x uniformly at random and chooses the next symbol of s i to be the symbol appearing immediately after x in S. This algorithm has been adapted to work on paths of tree representations, i.e., sequences of vectors of real numbers, as described in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Wavelets and Steerable Pyramids</head><p>Wavelets have become the tool of choice in analyzing single and multi-dimensional signals, especially if the signal has information both at different scales and localizations. The fundamental idea behind wavelets is to analyze the signal's local frequency at all scales and locations, using a fast invertible hierarchical transform. Wavelets have been effectively utilized in many different fields. A comprehensive review of wavelets applied to computer graphics can be found in a book by Stollnitz, Salesin, and DeRose <ref type="bibr" target="#b28">[29]</ref>.</p><p>A wavelet representation is a multi-scale decomposition of the signal and can be viewed as a complete tree, where each level stores the projections of the signal, with the wavelet basis functions of a certain resolution (at all possible translations of the basis functions).</p><p>In this work we use two different types of wavelets: the Daubechies wavelets <ref type="bibr" target="#b6">[7]</ref> and the steerable pyramid <ref type="bibr" target="#b24">[25]</ref>. The steerable pyramid transform is used to analyze 2D texture images and the spatial dimensions of texture movies. Daubechies wavelets are used to analyze the TVT's temporal dimension. Thus, for sound textures we use only the Daubechies wavelets, and for 2D textures we use only the steerable pyramid (as described in Section 5). For texture movies we use a combination of both transforms as described in Section 6.</p><p>The Daubechies wavelet is a one-dimensional wavelet that produces a series of coefficients that describe the behavior of the signal at dyadic scales and locations. The Daubechies wavelet transform is computed as follows: Initially, the signal is split into lowpass/scaling coefficients by convolving the original signal with a lowpass/scaling filter (denoted in this paper by ¨) and the wavelet/detail coefficients are computed by convolving the signal using a Daubechies wavelet filter (denoted ©). Both responses are subsampled by a factor of 2, and the same filters are applied again on the scaling coefficients, and so forth.</p><p>The steerable pyramid is a multi-scale, multi-orientation linear signal decomposition. This wavelet has several superior properties compared to traditional orthonormal wavelets, especially with respect to translation and rotation invariance, aliasing and robustness due to its nonorthogonality and redundancy. The steerable filter is produced as follows: Initially, the signal is split into low and highpass subbands. The lowpass subband is then split into a set of k oriented bandpass subbands using k oriented filters (denoted in this paper by ª i ) and a new lowpass subband is computed by convolving with a lowpass filter (denoted ¢). This lowpass subband is subsampled by a factor of 2 in each direction and the resulting subsampled signal is processed recursively. Simoncelli et al. <ref type="bibr" target="#b24">[25]</ref> provide a detailed exposition on steerable pyramids and further references.</p><p>A wavelet representation can be transformed back into the original signal using a fast hierarchical inverse transform. The computation proceeds from the root of the tree down to the leafs, using filters that are complementary to those used in the wavelet transform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Statistical Learning Algorithm</head><p>In this section we describe a general algorithm for sampling the most likely mutual source of stationary n-dimensional signals. Section 5 describes the specialization of this algorithm to the tasks of synthesizing and mixing 2D textures. Section 6 describes the extensions necessary for applying this algorithm to TVT synthesis.</p><p>The outline of our approach is as follows: given k n-dimensional signals as input we construct a 2 n -ary tree representing the wavelet-based multi-resolution analysis (MRA) of each signal. From the point of view of our synthesis algorithm, each signal is now encoded as a collection of paths from the root of the tree towards the leaves. It is assumed that all the paths in a particular tree are realizations of the same stochastic process. The task of the algorithm is to generate a tree whose paths are typical sequences generated by the most likely mutual source of the input trees. From the resulting tree, a new n-dimensional signal is reconstructed by applying a process inverse to the MRA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tree merging.</head><p>Given k source signals represented by their MRA trees T 1 , , T k with corresponding priors (weights) 1 , , k , such that È i = 1, our algorithm generates a new tree by merging together paths present in the source trees. The algorithm is described in pseudocode in Figure <ref type="figure">1</ref>. The generation of the new tree proceeds in breadth-first order (i.e., level by level). First, we randomly select one of the input trees T i (according to the given priors). The root value of T i along with the values of its children are copied into T. Now, let us assume that we have already generated the first i levels of the tree. In order to generate the (i + 1)-st level we need to assign values to 2 n children nodes of each node in level i. Let x i be a value of such a node, and denote by x i 1 , x i 2 , , x 1 the values of that node's ancestors along the path towards the root of the tree. The algorithm searches the i-th level in each of the source trees for nodes y i with the maximal length ¯-similar path suffixes y i , y i 1 , , y j , where ¯is a user-specified threshold and i j 1. Two paths are considered ¯-similar when the differences between their corresponding values are below a certain threshold. This computation occurs in the routine CandidateSet in Figure <ref type="figure">1</ref>. One of these candidate nodes is then chosen and the values of its children are assigned to the children of node x i . In this way a complete new tree is formed.</p><p>Improvements. The tree merging algorithm described above requires the examination of k2 ni paths in order to find the maximal ¯-similar paths, for each of the 2 ni nodes x i on level i. However, most of the computation can be avoided by inheriting the candidate sets from parents to their children in the tree. Thus, while searching for maximal ¯-similar paths of node x i the algorithm only examines the children of the nodes in the candidate sets that were found for x i 1 while constructing the previous level. This improvement is especially important in the case of the 3D texture movies, as described in more detail in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Synthesizing Textures of Arbitrary Dimensions</head><p>The algorithm described above constructs an output tree T of the same size as the input trees T 1 , , T k . It is easy to construct a larger (deeper) output tree as follows. Let d be the depth of the input trees. In order to construct an output tree of depth d + 1 we first use our merging algorithm 2 n times to compute 2 n new trees. The roots of those trees are then fed as a low resolution version of the new signal to the MRA construction routine, which uses those values to construct the new root and the first level of the new MRA hierarchy.</p><p>In this manner it is possible to generate 1D sound textures of arbitrary length from a short Input: Source trees T 1 , , T k , priors 1 , , k , threshold Ōutput:</p><p>A tree T generated by a mutual source of T 1 , , T k</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initialization:</head><p>Randomly choose T i according to the priors 1 , , k Root(T) := Root(T i ) Child j (T) := Child j (T i ) for j = 1, , 2 n</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Breadth-First Construction:</head><p>for i = 1 to d 1 (where d is the depth of the source trees):</p><formula xml:id="formula_0">foreach node x i on level i of T foreach j = 1 to k C j := CandidateSet(T j , i, x i , ¯)</formula><p>Randomly choose a node y ij from set C j endfor foreach j = 1 to k</p><formula xml:id="formula_1">£ j := j C j È k =1 C ( C j is the size of the set C j ) endfor Choose j according to the distribution £ = £ j</formula><p>Copy the values of the children of y ij to those of x i endfor endfor procedure CandidateSet(T j , i, x i ,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>¯)</head><p>Let x 1 , x 2 , ¡ ¡ ¡ , x i 1 be the ancestors of x i foreach node y i on level i of T j Let y 1 , y 2 , ¡ ¡ ¡ , y i 1 be the ancestors of y i L[y i ] := 0, sum := 0</p><formula xml:id="formula_2">for = i to 1 sum + = (x y ) 2 if sum i +1 then L[y i ]++ else break endfor endfor M := max y i L[y i ] return the set of all nodes y i such that L[y i ] == M Figure 1</formula><p>The n-dimensional tree-merging algorithm input sample of the sound. The same is true for our 2D texture synthesis algorithm. From a small input 2D texture we can synthesize a much larger texture (as will be demonstrated in Section 5). However, in the case of 3D TVTs, this approach often results in a noticeable temporal discontinuity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Threshold Selection</head><p>The threshold ¯is used in our algorithm as a measure of similarity. Recall that in the original suffix learning algorithm <ref type="bibr" target="#b15">[16]</ref>, the linear sequences contain discrete values. In order to extend the algorithm to sequences of continuous values we use the following similarity criterion. Two paths from nodes x and y to the root of the MRA tree are considered similar if the difference between their values (at each corresponding node along the suffix of length m of the path) are below a certain threshold. If two paths are similar, we can continue one with values from the other, while still preserving the fact that they emerged from the same stochastic source as shown in <ref type="bibr" target="#b15">[16]</ref>. In our implementation of this algorithm we use leveldependent similarity criteria for tree paths. Specifically, lower resolution levels of the tree have looser similarity criteria than higher resolution levels, and therefore a larger threshold is used at lower levels. This adaptive measure was chosen because the human visual system is more sensitive to high frequency information.</p><p>The selection of the threshold has a big impact on the outcome of the algorithm. Selecting a larger threshold causes the outcome to differ more strongly from the input (the actual difference depends on the type of the input sample, as well as on the value of the threshold). On the other hand, a small threshold can cause the outcome to be a copy of the input. Thus, by leaving the threshold selection to the user, the user is supplied with a powerful tool to achieve the desired outcome. Usually, the thresholds used for synthesizing structured 2D textures are lower than the ones used for synthesizing unstructured textures. This is due to the fact that in the resulting texture we would typically like to retain the large features of the structured texture. Allowing for too large a threshold in such a texture can "break" such features by incorporating random values in wrong places. In the case of texture movies however, selecting the threshold can prove to be a difficult task, since it is harder for the user to assess the scale of the structure present in the temporal dimension of the sequence. We address this problem in Section 6, and show how to automatically choose the threshold in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">2D Texture Synthesis and Mixing</head><p>In this section we describe the application of our statistical learning algorithm to the task of synthesizing and mixing 2D textures and discuss the differences between our algorithm and that of De Bonet <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10]</ref>. input texture synthesized texture Figure <ref type="figure">2</ref> Texture synthesis examples. The synthesized textures are four times larger than the input ones. As described in Section 4, these larger textures were generated by applying our algorithm four times on the same input. Because our algorithm generates a different texture each time, the enlarged texture does not appear tiled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Texture Synthesis</head><p>As explained in the previous section, our statistical learning algorithm begins by constructing an MRA representation for each input sample of the signal. The MRA representation we use to analyze 2D textures is the steerable pyramid <ref type="bibr" target="#b24">[25]</ref> described in Section 3 with four subband filter orientations (0, 45, 90, and 135 degrees). Color is handled by treating each of the three (red, green, and blue) channels separately. Thus, each node in the MRA tree contains a vector of length 12 (4 filter responses for each of 3 color channels). To obtain the k input trees for our tree merging algorithm, we select k large regions in the input texture (overlapping and slightly shifted with respect to each other). A steerable pyramid is then constructed for each region, yielding k MRA trees. The trees are assigned equal priors of i = 1 k. In practice, we found that two or three regions are sufficient for satisfactory results. Our learning algorithm uses these trees to generate a new random MRA tree, which is then transformed back into a 2D image.</p><p>Three different examples of textures synthesized by our algorithm are shown in Figure <ref type="figure">2</ref>. Each row shows a pair of images: the left image is the original texture from which the source trees were generated, and the right image is a synthetic texture, larger than the source by a factor of two in each dimension. Additional examples can be found at: http://www.cs.huji.ac.il/˜zivbj/textures/textures.html</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Texture Mixing</head><p>Texture mixing is a process of generating a new texture that contains features present in several different input textures. Our statistical learning algorithm can be used to mix textures in new creative and interesting ways. Instead of feeding the algorithm with several samples of the same texture, we provide it with samples of several different textures. Since our algorithm produces a new MRA tree by sampling the mutual source of all the input trees, the resulting texture exhibits features from all the input textures. Note that this method is different from simply averaging the transform coefficients, since each value comes from exactly one input tree. This allows the algorithm to produce a texture that has features from all input textures, while the merging still looks natural (due to the constraints imposed by the statistical learning algorithm). Figure <ref type="figure">3</ref> demonstrates the differences between simple texture blending (3a), blending of MRA coefficients a la Burt and Adelson <ref type="bibr" target="#b4">[5]</ref> (3b), and our technique (3c). Two additional texture mixing examples are shown in Figure <ref type="figure">4</ref>.</p><p>The following problem may arise during texture mixing. When the input textures are very different from each other, the algorithm tends to "lock on" to one of the input trees very early. As a result, starting from a high level node in the generated MRA its entire subtree comes from only one input texture. The result is a large non-mixed area in the output image. A possible solution to this problem is to increase the threshold, relaxing the similarity constraints on the path continuations. However, this solution often results in a blurry outcome.</p><p>In order to solve the locking problem we try to allow at least one candidate from each input tree to participate when selecting the continuation of a path in the synthesized tree high levels. In order to still be able to preserve strong large features we compute and store in each node of the input trees the cumulative sum of absolute values along the path from the root to that node. This value tends to be large in areas where a strong edge feature is found. When choosing among different candidates, we increase the probability of a candidate according to the magnitude of its cumulative sum.</p><p>More specifically, we modify our algorithm as follows. When looking for candidates to continue a node x we make sure that there is a non-empty candidate set for each of the input trees (the threshold is increased until the candidate set becomes non-empty). A single candidate is uniformly chosen from each candidate set, as before. Now, instead of choosing among these candidates based on the distribution £ = £ j , we choose the candidate with the highest cumulative sum.</p><p>Note that this modified method prefers learning from input tree nodes supporting regions in the texture that exhibit a strong global structure. This is so, because in such regions the steerable filter has a strong response onemphall levels. In smoother regions any of the input textures can be learned. For example, in a brick wall the edges between the bricks will be learned from the brick texture, while the texture inside the bricks might come from other less structured textures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison With De Bonet's Algorithm</head><p>As mentioned earlier the application of our algorithm to the task of 2D texture synthesis is similar to De Bonet's method <ref type="bibr" target="#b8">[9]</ref>. However, there are in fact several important differences between the two approaches. After explicitly stating our algorithm, we can now explain those differences in detail.</p><p>The most important distinguishing feature of our algorithm is that our tree merging routine takes several texture samples as input and constructs a texture that could have been produced by a mutual source of the input samples. In contrast, De Bonet's algorithm operates on a single texture sample. Using several input samples makes our algorithm more robust and less sensitive to the position of specific features in the input texture. Another practical implication of this difference is that our approach enables us not only to generate textures similar to the input one, but also to synthesize mixtures of different textures, as illustrated in Section 5.2. It should be noted that De Bonet and Viola briefly mention the possibility of extending their approach to multiple input examples <ref type="bibr" target="#b10">[11]</ref>.</p><p>A second difference is that when our algorithm generates level i tree nodes, we are actually looking at nodes in level i 1. For each such node x we are looking for nodes in the analysis pyramids (in the same level i 1) that have paths similar to x. Once we choose a candidate node x ¼ from this set, we copy the values of all the children nodes of x ¼ to the children nodes of x. Since, according to our algorithm, x and x ¼ have the same stochastic source, we would like to imitate this source when generating level i values, thus generating all the children values together. In contrast, when De Bonet's algorithm generates level i nodes, each node in that level is generated separately and independently of its siblings. This can result in more discontinuities in the synthesized texture.</p><p>A third difference between our approach and De Bonet's is in the selection of the candidates from which a continuation of a node x is chosen. In both methods, candidates are chosen based on similarity between paths in the tree. De Bonet's method always considers the entire path from the root of the tree to the candidate node. In contrast, our method looks for the ¯-similar paths of maximal length, including those that do not reach all the way up to the root. Thus, it is possible to choose nodes with paths that do not have similar values in the top levels, but only in the lower levels. As a result, we consider more candidates, allowing for a more varied texture.</p><p>In practice, the results produced by our synthesis algorithms are of comparable quality to those provided by De Bonet on his web pages<ref type="foot" target="#foot_0">1</ref> .</p><p>6 Synthesis of Time-Varying Textures</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">MRA Construction</head><p>As explained in Section 4, the first step in our approach is the construction of an MRA tree of the input signal. In the case of texture movies, the input signal S(x, y, t) is threedimensional and hence we construct the MRA by applying a 3D wavelet transform to the signal. The goal of this transform is to capture both spatial and temporal characteristic features of the signal at multiple scales. Since the steerable pyramid transform was found very well-suited for the analysis of 2D textures, our first inclination was to use a 3D variant of the steerable pyramid. However, steerable filters are non-separable and have a wide support (the 2D steerable sub-band filters we used were 9 ¢ 9). Repeated convolution of a 3D signal with multiple 9 ¢ 9 ¢ 9 3D filters is quite expensive: it requires 2 ¡ 9 3 = 1458 floating point operations for each pixel in each frame. Also, designing a set of properly constrained filters for the construction of a steerable pyramid is not a trivial task even in 2D <ref type="bibr" target="#b18">[19]</ref>. Finally, in the case of TVTs the signal has different characteristics along the temporal dimension from those it exhibits in the spatial dimensions. While in the temporal dimension there is a clear natural ordering of the frames, there is no prominent natural ordering of the pixels in a single frame. Thus, it does not necessarily make sense to use filters that are symmetric in all three dimensions.</p><p>We have also experimented with separable 3D wavelet transforms defined as a cartesian product of three 1D transforms, but they failed to adequately capture the spatial constraints between features in the TVT. The resulting sequences often exhibited strong discontinuities and blocky appearance.</p><p>Because of the above considerations, we decided to use a specially designed 3D transform defined by a cartesian product between the 2D steerable transform (applied to the spatial dimensions) and an orthonormal 1D wavelet transform (applied to the temporal dimension). Thus, our transform is semi-separable: it is non-separable in 2D, but the time dimension is separable from the other two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Building the Pyramid</head><p>Assume for now that the input signal S is given as a cubic 3D array of size n ¢ n ¢ n (we shall lift this restriction in Section 6.2). One way to think of this 3D array is as a stack of 2D slices, where each slice is a frame in the sequence. An alternative way to think about it is as a bundle of n 2 1D arrays, where each such array is a temporal sequence of values corresponding to a particular location (x, y).</p><p>In order to describe the MRA construction procedure we use the notation introduced in Section 3. Let ¨and © denote the scaling function and the wavelet analysis filters of the 1D wavelet transform, respectively. Each of these filters is convolved with a sequence of length N to produce N 2 coefficients. Let ª i denote the i-th sub-band steerable filter, which is convolved with an N ¢N image to produce an N ¢N response. Let ¢ denote the steerable low-pass filter, which is convolved with an N ¢ N image to produce an N 2 ¢ N 2 lowpassed image (the convolution is followed by downsampling by a factor of 2).</p><p>The construction of the MRA pyramid proceeds as follows (see Figure <ref type="figure" target="#fig_1">5</ref>). Given a 3D TVT we first apply once the analysis filters ¨and © on the n 2 temporal sequences S(x, y, £). The asterisk symbol £ is used here to denote the full range of values between 1 and n. Thus, S(£, £, t) stands for the entire t-th frame (time slice) of the signal, while S(x, y, £) denotes the vector of values of the (x, y) pixel in all of the different time frames. Each temporal sequence of length n is thus decomposed to n 2 scaling coefficients S ¨and n 2 detail coefficients S © . Viewing the n 2 ¢ n 2 scaling coefficients as a stack of n 2 slices, the 2D steerable transform is now applied n 2 times, once on each slice. Each steerable transform results in k sub-band responses S ª i and in a single downsampled low pass response S ¢ . All of the detail coefficients S © and the sub-band response values S ª i are stored as the values of the nodes at the bottom level of the pyramid. The same procedure is then repeated again on the downsampled low pass responses in order to compute the next level of the pyramid. The pseudocode for this procedure is given in Figure <ref type="figure">6</ref>. After constructing the -th level of the MRA we are left with n¢n¢n 2 nodes for this level and a downsampled low pass version of the signal S ¢ (£, £, £), which is then fed again to the same procedure to compute level 1, and so forth. Since S ¢ is downsampled by a factor of two in each of the three dimensions, each node in level 1 is considered a parent for eight nodes in level . Eventually, we construct level = 1, where we have four nodes and a single value representing the low pass average of the entire sequence. This value is stored at the root (level zero) of the tree. Thus, we obtain a tree whose root has four children, but otherwise it has a branching factor of eight. Each internal node of the tree contains a vector of 3(k + 1) values: k subband responses and one detail coefficient for each of the three color channels.</p><formula xml:id="formula_3">Input: A 3D signal S(£, £, £) of size n ¢ n ¢ n Output: a) Level of the MRA ( = log n) b) A low-passed 3D signal S ¢ (£, £, £) of size n 2 ¢ n 2 ¢ n</formula><p>In order for us to be able to learn the new TVT based on the MRA representation of the input TVT, it is imperative that the 3(k + 1) values stored in each tree node are responses corresponding to the same location in the input signal. This is indeed the case in our construction. We associate S © (x, y, t) and S ª i (x, y, t) with the same node in the MRA. Note that S © (x, y, t) represents the responses of the pixels in the original frames 2t 1 and 2t at location x, y to the temporal filter (since we subsample by a factor of two). S ª i (x, y, t) represents the responses of the same pixels to the steerable filter, since it was applied to the scaling coefficients corresponding to the same pixels.</p><p>Since both the 1D orthonormal wavelet transform and the steerable transform are invertible, so is our 3D transform. More precisely, given a TVT of dimensions n 2 ¢ n 2 ¢ n 2 that was reconstructed at level 1 we reconstruct level in the following way: First we apply the inverse steerable transform on each of the n 2 slices of size n 2 ¢ n 2 using the values of the steerable subband responses that are stored in the nodes of level . This results in n 2 slices each of size n ¢ n. We now apply the inverse temporal filter using the values of the highpass temporal filter that are stored in the nodes of this level. This results in an n ¢n¢n TVT. We repeat this process until we obtain a TVT of the same size as the input one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Specifics</head><p>The 1D orthonormal wavelet we use for the analysis along the temporal dimension is a Daubechies filter of length 10 <ref type="bibr" target="#b6">[7]</ref>. For the spatial domain analysis we use the steerable pyramid <ref type="bibr" target="#b24">[25]</ref> with four subband filter orientations (0, 45, 90, and 135 degrees). Color is handled by treating the red, green, and blue components separately. Thus, the values at each node of the MRA hierarchy are vectors of length 15 in its high levels (where we analyze the TVT as a cube) and of length 12 in its lower levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Handling non-cubic TVTs</head><p>The MRA construction algorithm described above assumes that the input signal has dimensions n ¢ n ¢ n, where n = 2 m . In practice, the input TVT is typically of dimensions n¢n¢r = 2 m ¢2 m ¢2 q , where q m. For example, most of the sequences we experimented with were 256 ¢ 256 ¢ 32.</p><p>There are many possible strategies to handle non-cubic TVTs. We have experimented with the two strategies described below.</p><p>1. Apply the 3D transform from the previous section q times, until S ¢ becomes a 2D signal of dimensions 2 m q ¢2 m q . The remainder of the pyramid is constructed using only the 2D steerable transform.</p><p>2. Apply the 2D steerable filters m q times to each frame, generating m q levels of the steerable pyramid for each image. We are now left with a 2 q ¢2 q ¢2 q signal, and apply our 3D transform to it. Thus, the resulting pyramid has branching degree 4 in its m q bottom levels, and branching degree 8 in the remaining levels.</p><p>Based on the results of our experiments, we chose the second strategy. We believe that this strategy produces better results because in the resulting tree the nodes containing both temporal and spatial response values are located closer to the root. Thus, all three dimensions of the sequence are taken into account at the early stages of the learning process, i.e., when the overall structure of the output TVT is being formed. The finer spatial details of each frame are filled in later, without any further temporal constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Synthesis Algorithm</head><p>Once the MRA of an input texture movie has been constructed, we use the statistical learning algorithm described in Section 4 (specialized to the case of a single input sample) to generate a new random MRA tree from which a new movie is reconstructed. Below we describe the automatic threshold selection algorithm we developed for TVT synthesis and some important optimizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Threshold Selection</head><p>It was already explained in Section 4.2 that we cannot expect the user to select an appropriate threshold for the temporal dimension of 3D TVTs, because it is difficult to assess the size of the temporal features in the sequence simply by observing it. Our technique for choosing a threshold for the temporal dimension is inspired by wavelet compression methods for images <ref type="bibr" target="#b11">[12]</ref>. The idea behind wavelet compression is to zero out coefficients with L 1 norm less than some small number a. This decimation of the coefficients results in little perceptual effect on subjective image quality. By the same token, we assume that switching is permitted between coefficients whose values are no more than 2a apart. Thus, we let the user specify a percentage p. We then compute the interval [ a, a] containing p percent of the TVTs temporal coefficients. The temporal threshold value is then set to 2a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reducing the Number of Candidates</head><p>A naive implementation of the tree synthesis algorithm requires the examination of all the nodes at level i in the original tree in order to find the maximal ¯-similar paths for every node v i on level i in the new tree. Given an 2 m ¢ 2 m ¢ 2 q input TVT, in the bottom level our algorithm has to check 2 m 1 ¢ 2 m 1 ¢ 2 q nodes in the new tree, so applying the naive algorithm results in a number of checks that is quadratic in this number. Since each node has 3k values, and for each such value we check a path of length m 1, this exhaustive search makes the synthesis of high-dimensional signals impractically slow. However, as briefly mentioned in Section 4, much of the search can be avoided by inheriting the candidate sets from parents to their children in the tree. Thus, while searching for maximal ¯-similar paths of node v i the algorithm must only examine the children of the nodes in the candidate sets that were found for v i 1 while constructing the previous level. The result is a drastic reduction in the number of candidates. The actual number of candidates depends of course on the threshold, but in almost all cases we found that the number is very small (between 4 and 16). In the case of our 3D TVTs we found that this improvement reduced the synthesis time from weeks to just a few minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Results</head><p>We tested the TVT synthesis algorithm described in this paper on many different examples of texture movies, including both natural and synthetic video sequences. The threshold for the temporal responses was obtained as described in Section 6.3, with p usually between 70 and 80 percent. All input and output texture movies were of size 256 ¢ 256 ¢ 32, and were generated on a Pentium II 450MHz with 1GB of RAM. Each movie clip took about 10 minutes to generate.</p><p>Figures <ref type="figure" target="#fig_4">7</ref> and<ref type="figure" target="#fig_5">8</ref> show four frames from each of six original and seven synthesized texture movies:</p><p>Waterfall See rows 1 and 2 in Figure <ref type="figure" target="#fig_4">7</ref>. The differences between the original and synthesized clips are noticeable both in the static structure of the waterfall, and in the water flow. These differences are particularly apparent in the left hand side and at the top of each frame.</p><p>Crowd See rows 3 and 4 in Figure <ref type="figure" target="#fig_4">7</ref>.</p><p>Volcano See rows 5, 6, and 7 in Figure <ref type="figure" target="#fig_4">7</ref>. Here we show two different synthesized clips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clouds</head><p>The original sequence in this case is also synthetic. See rows 1 and 2 in Figure <ref type="figure" target="#fig_5">8</ref>.</p><p>Fire See rows 3 and 4 in Figure <ref type="figure" target="#fig_5">8</ref>.</p><p>Jellyfish See rows 5 and 6 in Figure <ref type="figure" target="#fig_5">8</ref>.</p><p>In all of these cases, our synthesis algorithm has succeeded in producing texture movies which closely resemble the original sequences, yet exhibit various readily apparent differences.</p><p>We have also extensively experimented with the specialization of our algorithm to the task of sound texture synthesis (1D signals). The resulting sound synthesis algorithm is described in another paper <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Our algorithm stores in main memory both the MRA constructed from the input signal and the MRA that is being generated. The resulting memory requirements are quite substantial. Specifically, on a workstation with 1GB of RAM our implementation currently generates short movies (32 frames at 256 ¢ 256 resolution, and 128 frames at 128 ¢ 128). Although longer frame sequences can be generated by creating several short ones and concatenating them while blending their boundary frames, this approach often introduces excessive blur in the blended frames and does not result in a desired "typical" TVT.</p><p>In several synthesized texture movies, occasional spatial and temporal discontinuities can be seen. This results from the tree-based nature of the synthesis algorithm. Neighboring spatio-temporal regions in the movie can sometimes be far apart in the MRA tree structure.</p><p>In those cases the constraints between such regions are weaker than they should be.</p><p>Our approach assumes that the frames are filled with texture in a relatively homogeneous manner; the method does not respond well to large changes in the size of texture features across the frames, which can occur for example due to perspective foreshortening. Large static objects in the field of view also interfere with successful synthesis. The method works best when the camera appears to be stationary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>We have described a method, based on statistical learning and multi-resolution analysis, for generating new instances of textures from input samples. While most of the previous work in this area has focused on the synthesis of 2D textures <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34]</ref>, our technique also enables the synthesis of mixed 2D textures that simultaneously capture the appearance of a number of different input textures. The ability to produce such mixes will undoubtedly enhance the creative abilities of artists and graphics designers. We have also extended texture synthesis from the domain of static textures to time-varying textures: 1D sound textures and 3D texture movies. Our experiments demonstrate that our techniques are robust, and work on a large variety of textures.</p><p>The work described in this paper is just the first step towards building a complete system for automatic generation of special effects from examples. There are many ways to further enhance and extend our approach.</p><p>Longer movies. At present, our algorithm produces movie clips of the same length as the input clip. Longer clips can be generated by concatenating their MRA trees, but this often results in a temporal discontinuity. Thus, a more drastic change in the algorithm is needed in order to be able to generate arbitrarily long frame sequences. We would like to develop   an algorithm capable of adding more frames to a prefix frame sequence that has already been computed, without having to construct the entire MRA tree of the longer sequence.</p><p>Full integration of sound and picture. Currently, the synthesis of the movie and its soundtrack are completely independent. We would like to extend our algorithms to take into account constraints between these two modalities, and to synthesize them in a synchronized fashion.</p><p>Movie mixing. It should be possible to extend the technique for 2D texture mixing described in Section 5.2 to generation of "movie mixtures".</p><p>Classification. Methods for statistical learning of 2D texture images have been successfully applied not only to texture generation, but also to texture recognition and image denoising <ref type="bibr" target="#b9">[10]</ref>. These applications are made possible by realizing that the statistical learning of 2D textures implicitly constructs a statistical model describing images of a particular class. Similarly, our approach for TVT generation can be used as a statistical model suitable for describing TVTs. Therefore, it should be possible to apply this statistical model for tasks such as classification and recognition of such movie segments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 Figure 4</head><label>34</label><figDesc>Figure 3 Several different ways of mixing texture A (bottom left) with texture B (bottom right). The top row shows two different blends of A and B: image (a) was obtained by a simple blend of the two textures; image (b) was obtained by blending the coefficients of the multi-resolution transforms of A and B. Compare these blends with the mixed texture (c) produced by our algorithm. In (c) one can discern individual features from both input textures, yet the merging appears natural.</figDesc><graphic coords="13,219.60,338.98,172.80,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5</head><label>5</label><figDesc>Figure 5 Construction of one level of the MRA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 Stage 1 :Figure 6</head><label>216</label><figDesc>Figure 6Constructing the -th level of the MRA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7</head><label>7</label><figDesc>Figure 7 Texture movie synthesis examples. In each of the examples above, the first row shows four frames from the original movie clip (frames 0, 7, 14, and 21), and the following row(s) shows the corresponding frames in the synthesized clip(s). The examples are: waterfall (rows 1-2), crowd (rows 3-4), and volcano (rows 5-7, two different synthesized clips). While the synthesized frames are very similar to the original in their overall appearance, pairwise comparison reveals many differences.</figDesc><graphic coords="23,179.64,513.94,72.00,72.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8</head><label>8</label><figDesc>Figure 8 More texture movie synthesis examples: clouds (rows 1-2), fire (rows 3-4), and jellyfish (rows 5-6).</figDesc><graphic coords="24,179.64,524.14,72.00,72.00" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www.ai.mit.edu/˜jsd/Research/Synthesis/SPSynth</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>The mutual source. Let P and Q be two distributions. Their mutual source Z is defined as the distribution that minimizes the Kullbak-Leibler divergence <ref type="bibr" target="#b5">[6]</ref> to both P and Q. The KL-divergence from a distribution Z to a distribution P, where both P and Z are defined over a support A, is defined to be D KL (P Z) = È a¾A P(a) log P(a) Z(a) . Specifically,</p><p>The parameter should reflect the prior importance of P relative to Q. (When no such prior exists one can take = 1 2). The expression min</p><p>is known as the Jensen-Shannon dissimilarity. Using convexity arguments it can be shown (see e.g. <ref type="bibr" target="#b15">[16]</ref>) that the mutual source is unique, and therefore, the Jensen-Shannon measure is well defined. This Jensen-Shannon dissimilarity measure has appealing statistical properties and interpretation. For example, it provides an optimal solution to the two sample problem where one tests the hypothesis that that two given samples emerged from the same statistical source.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ran El-Yaniv, Dani Lischinski, and Michael Werman. Statistical learning of granular synthesis parameters with applications for sound texture synthesis</title>
		<author>
			<persName><forename type="first">Ziv</forename><surname>Bar-Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shlomo</forename><surname>Dubnov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Computer Music Conference (ICMC99)</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeling and estimation of multiresolution stochastic processes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Basseville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Benveniste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Golden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nikoukhah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="766" to="784" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiscale autoregressive processes, part II: Lattice structures for whitening and modeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Basseville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Benveniste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1935" to="1954" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E P</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Reinsel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jenkins</surname></persName>
		</author>
		<title level="m">Time Series Analysis : Forecasting and Control</title>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A multiresolution spline with application to image mosaics</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="217" to="236" />
			<date type="published" when="1983-10">October 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Elements of Information Theory</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joy</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wiley Series in Telecommunications</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Orhtonormal bases of compactly supported wavelets</title>
		<author>
			<persName><forename type="first">Ingrid</forename><surname>Daubechies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="909" to="996" />
			<date type="published" when="1988-10">October 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multiresolution sampling procedure for analysis and synthesis of texture images</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><surname>Bonet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics</title>
		<imprint>
			<publisher>ACM SIGGRAPH</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="361" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Novel statistical multiresolution techniques for image synthesis, discrimination, and recognition. Master&apos;s thesis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><surname>Bonet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997-05">May 1997</date>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A non-parametric multi-scale statistical model for natural images</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>De Bonet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Texture recognition using a non-parametric multi-scale statistical model</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>De Bonet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image compression through wavelet transform coding</title>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">A</forename><surname>Devore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Jawerth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradley</forename><forename type="middle">J</forename><surname>Lucier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="719" to="746" />
			<date type="published" when="1992">1992</date>
			<pubPlace>Part II</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<author>
			<persName><forename type="first">David</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Parent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Solid Spaces and Inverse Particle Systems for Controlling the Animation of Gases and Fluids</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="179" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kent</forename><surname>Musgrave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darwyn</forename><surname>Peachey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Perlin</surname></persName>
		</author>
		<author>
			<persName><surname>Worley</surname></persName>
		</author>
		<title level="m">Texturing and Modeling: A Procedural Approach</title>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1994-10">October 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rendering and animation of gaseous phenomena by combining fast volume and scanline A-buffer techniques</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">S</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Parent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (SIGGRAPH &apos;90 Proceedings)</title>
		<editor>
			<persName><forename type="first">Forest</forename><surname>Baskett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1990-08">August 1990</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="357" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Agnostic classification of Markovian sequences</title>
		<author>
			<persName><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Michael</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Jordan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sara</forename><forename type="middle">A</forename><surname>Kearns</surname></persName>
		</editor>
		<editor>
			<persName><surname>Solla</surname></persName>
		</editor>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A simple model of ocean waves</title>
		<author>
			<persName><forename type="first">Alain</forename><surname>Fournier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Reeves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (SIGGRAPH &apos;86 Proceedings)</title>
		<editor>
			<persName><forename type="first">David</forename><forename type="middle">C</forename><surname>Evans</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Russell</forename><forename type="middle">J</forename><surname>Athay</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1986-08">August 1986</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="75" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pyramid-based texture analysis/synthesis</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Heeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">R</forename><surname>Bergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 95 Conference Proceedings, Annual Conference Series</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Robert</surname></persName>
		</editor>
		<editor>
			<persName><surname>Cook</surname></persName>
		</editor>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1995-08">August 1995</date>
			<biblScope unit="page" from="229" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A filter design technique for steerable pyramid image transforms</title>
		<author>
			<persName><forename type="first">Anestis</forename><surname>Karasaridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eero</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP-96</title>
		<meeting>ICASSP-96<address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-10">May 7-10. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Universal prediction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Merhav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2124" to="2147" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling waves and surf</title>
		<author>
			<persName><forename type="first">R</forename><surname>Darwyn</surname></persName>
		</author>
		<author>
			<persName><surname>Peachey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (SIGGRAPH &apos;86 Proceedings)</title>
		<editor>
			<persName><forename type="first">David</forename><forename type="middle">C</forename><surname>Evans</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Russell</forename><forename type="middle">J</forename><surname>Athay</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1986-08">August 1986</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Computer Graphics (SIG-GRAPH &apos;85 Proceedings)</title>
		<author>
			<persName><forename type="first">Ken</forename><surname>Perlin</surname></persName>
		</author>
		<editor>B. A. Barsky</editor>
		<imprint>
			<date type="published" when="1985-07">July 1985</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
	<note>An image synthesizer</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Particle systems -a technique for modeling a class of fuzzy objects</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Reeves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="91" to="108" />
			<date type="published" when="1983-04">April 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Approximate and probabilistic algorithms for shading and rendering structured particle systems</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Reeves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricki</forename><surname>Blau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics (SIGGRAPH &apos;85 Proceedings)</title>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Barsky</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="313" to="322" />
			<date type="published" when="1985-07">July 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Shiftable multi-scale transforms</title>
		<author>
			<persName><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">H</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName><surname>Heeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="587" to="607" />
			<date type="published" when="1992-03">March 1992</date>
		</imprint>
	</monogr>
	<note>Special Issue on Wavelets</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Particle animation and rendering using data parallel computation</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Sims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (SIGGRAPH &apos;90 Proceedings)</title>
		<editor>
			<persName><forename type="first">Forest</forename><surname>Baskett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1990-08">August 1990</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="405" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Turbulent wind fields for gaseous phenomena</title>
		<author>
			<persName><forename type="first">Jos</forename><surname>Stam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Fiume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (SIGGRAPH &apos;93 Proceedings)</title>
		<editor>
			<persName><forename type="first">James</forename><forename type="middle">T</forename><surname>Kajiya</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1993-08">August 1993</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Depicting fire and other gaseous phenomena using diffusion processes</title>
		<author>
			<persName><forename type="first">Jos</forename><surname>Stam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Fiume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 95 Conference Proceedings, Annual Conference Series</title>
		<editor>
			<persName><forename type="first">Robert</forename><surname>Cook</surname></persName>
		</editor>
		<meeting><address><addrLine>Los Angeles, California</addrLine></address></meeting>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1995-08">1995. August 1995</date>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Wavelets for Computer Graphics: Theory and Applications</title>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">J</forename><surname>Stollnitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><forename type="middle">D</forename><surname>Derose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Morgan Kaufmann Publishers, Inc</publisher>
			<pubPlace>San Francisco, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generating textures for arbitrary surfaces using reaction-diffusion</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (SIGGRAPH &apos;91 Proceedings)</title>
		<editor>
			<persName><forename type="first">Thomas</forename><forename type="middle">W</forename><surname>Sederberg</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1991-07">July 1991</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="289" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reaction-diffusion textures</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (SIGGRAPH &apos;91 Proceedings)</title>
		<editor>
			<persName><forename type="first">Thomas</forename><forename type="middle">W</forename><surname>Sederberg</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1991-07">July 1991</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="299" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A cellular texture basis function</title>
		<author>
			<persName><forename type="first">P</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><surname>Worley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 96 Conference Proceedings, Annual Conference Series</title>
		<editor>
			<persName><forename type="first">Holly</forename><surname>Rushmeier</surname></persName>
		</editor>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1996-08">August 1996</date>
			<biblScope unit="page" from="291" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Wavelet-based representations for a class of self-similar signals with application to fractal modulation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Wornell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Oppenheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="785" to="800" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Filters random fields and maximum entropy(frame) -towards a unified theory for texture modeling</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="126" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
