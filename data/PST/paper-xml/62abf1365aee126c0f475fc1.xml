<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ResNorm: Tackling Long-tailed Degree Distribution Issue in Graph Neural Networks via Normalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-06-16">16 Jun 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Langzhang</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zenglin</forename><surname>Xu</surname></persName>
							<email>zenglin@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Zixing</forename><surname>Song</surname></persName>
							<email>zxsong@cse.cuhk.edu.hk</email>
						</author>
						<author>
							<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
							<email>king@cse.cuhk.edu.hk</email>
						</author>
						<author>
							<persName><forename type="first">Jieping</forename><surname>Ye</surname></persName>
							<email>jieping@gmail.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ResNorm: Tackling Long-tailed Degree Distribution Issue in Graph Neural Networks via Normalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-06-16">16 Jun 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2206.08181v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph Neural Networks</term>
					<term>Long-tailed Distribution</term>
					<term>Normalization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) have attracted much attention due to their ability in learning representations from graph-structured data. Despite the successful applications of GNNs in many domains, the optimization of GNNs is less well studied, and the performance on node classification heavily suffers from the long-tailed node degree distribution. This paper focuses on improving the performance of GNNs via normalization. In detail, by studying the long-tailed distribution of node degrees in the graph, we propose a novel normalization method for GNNs, which is termed ResNorm (Reshaping the long-tailed distribution into a normal-like distribution via normalization). The scale operation of ResNorm reshapes the node-wise standard deviation (NStd) distribution so as to improve the accuracy of tail nodes (i.e., low-degree nodes). We provide a theoretical interpretation and empirical evidence for understanding the mechanism of the above scale. In addition to the long-tailed distribution issue, over-smoothing is also a fundamental issue plaguing the community. To this end, we analyze the behavior of the standard shift and prove that the standard shift serves as a preconditioner on the weight matrix, increasing the risk of oversmoothing. With the over-smoothing issue in mind, we design a shif t operation for ResNorm that simulates the degree-specific parameter strategy in a low-cost manner. Extensive experiments have validated the effectiveness of ResNorm on several node classification benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Graph Neural Networks (GNNs) <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref> have been widely adopted to learn from graph-structured data. GNNs learn node representations by recursively aggregating the representations from neighbors. Recently, GNNs have achieved remarkable success in many fields, such as recommender systems <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>, information retrieval <ref type="bibr" target="#b7">[8]</ref>, and drug discovery <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b1">[2]</ref> are one of the representative GNN architectures that utilize the first-order Chebyshev approximation of spectral graph convolutions. Throughout this paper, we take GCNs as the main study framework. However, our empirical results in Section V validate that ResNorm is also an effective method to other representative GNN framework. Massive efforts have been devoted into studying the limitations of GCNs such as slow convergence <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> and long-tailed issue <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b14">[15]</ref>. To alleviate the limitations, we focus on normalization techniques for GCNs in this paper. Normalization <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> is commonly adopted to boost the training of deep learning models, such as LayerNorm <ref type="bibr" target="#b16">[17]</ref> for natural language understanding models and BatchNorm <ref type="bibr" target="#b15">[16]</ref> for image processing models. Although some studies have explored the normalization technique for GCNs, they do not consider the long-tailed node degree distribution issue, which substantially deteriorates the performance of tail nodes.</p><p>Node degrees typically follow long-tailed distributions, i.e., a small number of nodes (head nodes) possess a large number of neighbors, while the remaining majority (tail nodes) have only a few neighbors <ref type="bibr" target="#b17">[18]</ref>. As a result, the tail nodes commonly have inferior classification performance. This motivates us to leverage the long-tailed distribution prevalent in realworld graphs to design normalization methods. Our goal is to enhance the overall performance by improving the accuracy of tail nodes.</p><p>However, how to design a normalization method that fits the long-tailed distribution of graph data remains unclear. Since normalization is applied to node representations rather than the original input graph, we cannot modify the distribution of node degrees and further enhance the representation ability of tail nodes via normalization directly. Therefore, we seek a proxy statistical measure, namely the node-wise standard deviation (NStd given in Equation ( <ref type="formula" target="#formula_2">3</ref>)) calculated from the node representation in a particular layer in GCNs. To better understand the relationship between classification accuracy and NStd values, we select four groups of nodes (i.e., those with top 5%, top 20%, lowest 5%, and lowest 20% of the NStd values, respectively) for three node classification tasks, and plot the classification accuracy with different model depths in Figure <ref type="figure" target="#fig_0">1</ref>. It can be observed that nodes with higher NStd values can be consistently and correctly classified. The fact that the NStd distribution is similar to its corresponding node degree distribution (Figure <ref type="figure" target="#fig_3">3</ref> and<ref type="figure" target="#fig_9">7</ref>) inspires us to reshape the NStd distribution of nodes such that the node degree distribution can be implicitly reshaped, i.e., uniforming the node degrees among head and tail nodes. Although there may be some loss in the accuracy of head nodes, the improvement of the major tail nodes will ultimately benefit the overall performance.</p><p>In this paper, we take an initial step towards improving tail node classification performance via normalization which can be used as a generic plug-in for various GNNs. We start by observing a unique phenomenon in GCNs that the NStd is closely related to classification accuracy and link quality (i.e., node degree and intra-class rate). We further investigate the relationship between the NStd distribution and data distribution. Empirical results show that when training with long-tailed graph data, the NStd distribution will also exhibit in a long-tailed form. The scale operation of ResNorm reshapes the NStd distribution and thus significantly improves the performance of tail nodes. We provide a theoretical explanation for understanding how it works based on the influence distribution <ref type="bibr" target="#b18">[19]</ref>. For the design of the shif t operation of ResNorm, we mainly focus on the over-smoothing issue, which is a basic flaw of most popular GNN frameworks. Finally, we validate the effectiveness of ResNorm on seven popular node classification benchmarks. Experimental results show that ResNorm effectively improves the performance of GNNs on node-classification tasks, especially the accuracy of the tail nodes. More importantly, as a plug-and-play light module, ResNorm can be incorporated into many deep GNN models or techniques, resulting in a further enhancement.</p><p>The rest of this paper is organized as follows. Section II discusses the related work. By presenting the motivating observations and analyzing the underlying reasons, we describe our main ideas and motivations in Section III, followed by the principles of normalization for GCNs with the ResNorm in Section IV. Finally Section V presents the experimental results, followed by the conclusion in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PRELIMINARIES AND RELATED WORK</head><p>In this section, we present preliminaries on graph convolutional networks and normalization techniques, as well as related work on long-tailed classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Graph Convolutional Networks</head><p>Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b1">[2]</ref>, emerging as the most popular GNN architecture, have achieved high performance in the node classification task on various benchmarks <ref type="bibr" target="#b19">[20]</ref>. GCNs perform a first-order approximated graph convolution in the graph spectral domain per layer. Mathematically, the layer of GCN is defined as</p><formula xml:id="formula_0">H (l+1) = ?( D-1 2 ? D-1 2 H (l) W (l) ).<label>(1)</label></formula><p>Here, D-1 2 ? D-1 2 is a propagation matrix with ? = A + I n , where I n is an identity matrix and Dii = j ?ij is the degree matrix of ?, ?(?) denotes a non-linear function such as ReLU, A ? R n?n is the adjacency matrix, W (l) is a weight matrix at the l-th layer, H (l) is the node representations matrix at the l-th layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Normalization Techniques for Graph Representation Learning</head><p>Normalization is believed to reduce the training time substantially <ref type="bibr" target="#b16">[17]</ref>. Scale and shift are the main components of normalization. Scale operation scales the given set of features down by its standard deviation. Shift operation subtracts the mean from the given set of features. Formally, given a feature vector x ? R n , and its mean and variance ? =</p><formula xml:id="formula_1">1 n n i=1 x i , ? 2 = 1 n n i=1 (x i -?) 2 , a normalization step can be formulated as Norm(x) = ? x -? ? + ?,<label>(2)</label></formula><p>where ? and ? are learnable parameters. Different normalization methods are applied to different feature vectors. Batch-Norm <ref type="bibr" target="#b15">[16]</ref> normalizes the features within the same channel across different samples in a batch, while LayerNorm <ref type="bibr" target="#b16">[17]</ref> normalizes the features within the same sample across different channels.</p><p>How to appropriately leverage normalization to boost graph representation learning is less studied. A few studies focusing on that. GraphNorm <ref type="bibr" target="#b20">[21]</ref> is adapted from InstanceNorm <ref type="bibr" target="#b21">[22]</ref> and introduces a learnable parameter into the shift operation. NodeNorm removes the shift operation from normalization steps and scales a given node representation by its root of standard deviation. GraphNorm <ref type="bibr" target="#b20">[21]</ref> aims to adaptively achieve a trade-off between preserving the information contained in the mean and accelerating the training process. NodeNorm <ref type="bibr" target="#b22">[23]</ref> suggests that tackling the so-called variance inflammation issue via normalization. Besides, some normalization methods that tackles the over-smoothing issue have been proposed. For instance, PairNorm <ref type="bibr" target="#b23">[24]</ref> tackles over-smoothing by maintaining the node pair distance, while Differentiable Group Normalization (DGN) <ref type="bibr" target="#b24">[25]</ref> clusters nodes into multiple groups and then normalize them independently. Different from the above methods, our proposed ResNorm focuses on improving the classification accuracy of tail nodes. Besides, we validate by experiments that ResNorm can further improve other deep GNN frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Long-tailed Classification on Graph Data</head><p>Long-tailed data classification is a hot topic in image classification and is also known as the class imbalance problem that a small number of classes have a large number of samples while a large number of classes have only a few samples. Existing solutions usually include class-balancing strategies, e.g., re-designing losses <ref type="bibr" target="#b25">[26]</ref>, re-sampling data <ref type="bibr" target="#b26">[27]</ref>, or transfer learning from head-classes to tail-classes <ref type="bibr" target="#b27">[28]</ref>. However, longtailed data classification is less-studied in graph representation learning (GRL) where long-tailed distribution could be present as another form. We emphasize that our definition of longtailed data for graphs is different from that for images. Formally, a graph follows a long-tailed distribution if a small fraction of nodes have lots of neighbors while a large number of nodes have a few neighbors. Such long-tailed data poses a huge challenge on GRL: the major tail nodes are hard to learn good representations for downstream tasks due to their sparse edges, and also the bias and misleading issues. Most existing methods are designed for recommender system <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> or large-scale social networks <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b32">[33]</ref>, where the longtailed distribution issue is especially severe. Previous solutions utilize transfer learning to transfer knowledge from head items to tail items <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Others suggest that adding different weights to user-item pairs <ref type="bibr" target="#b33">[34]</ref> and clustering <ref type="bibr" target="#b34">[35]</ref>, as well as incorporating meta-learning <ref type="bibr" target="#b35">[36]</ref> and active learning <ref type="bibr" target="#b36">[37]</ref>. Besides, SL-DSGCN <ref type="bibr" target="#b37">[38]</ref> employs different parameters to train nodes' representations with different degrees. DEMO-Net <ref type="bibr" target="#b38">[39]</ref> is another graph convolution networks that map different subtrees to different feature vectors. We note our normalization method is orthogonal to the existing methods as they are model architectures, while the normalization can be used as a light and generic plug-in for GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MOTIVATING OBSERVATIONS AND ANALYSIS</head><p>Here, we present findings on the NStd. For a d-dimensional node representation, its NStd is computed across d dimension   features. Mathematically, for node v i with d-dimensional representation x i , its NStd is calculated as</p><formula xml:id="formula_2">? i = 1 d d j=1 (x i,j - 1 d d j=1 x i,j ) 2 .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Observations</head><p>We are interested in 4 types of nodes: a) nodes with the largest 5% NStds; b) nodes with the largest 20% NStds; c) nodes with the smallest 5% NStds; d) nodes with the smallest 20% NStds. Figure <ref type="figure" target="#fig_0">1</ref> reports the mean classification accuracy of these 4 types of nodes. Surprisingly, all of the results show the same pattern that those nodes with larger NStds consistently have higher classification accuracy. Such observations imply that NStd is the right factor that essentially has a strong positive correlation with node classification accuracy.</p><p>Since a larger NStd is generally associated with higher accuracy, we speculate that the nodes with the largest NStd are the easiest to classify, i.e., have many high-quality edges. We plot the intra-class neighbor rate and node degree of the nodes with the largest 2% and the smallest 2% NStds. As shown in Figure <ref type="figure" target="#fig_8">2</ref>, top 2% nodes consistently have a great number of high-quality neighbors, while the neighbors of the lowest 2% are of poor quality from the perspectives of degree and intraclass rate. Nodes surrounded by more intra-class neighbors will receive more useful information and thus are easier to classify under the mechanism of GCN <ref type="bibr" target="#b39">[40]</ref>. Therefore, we term larger-NStd nodes simple nodes, and term smaller-NStd nodes hard nodes. The phenomenon that the gap of NStd exists between simple and hard nodes is termed variance expressing (variance expresses the difficulty of node classification).</p><p>Figure <ref type="figure" target="#fig_3">3</ref> reports two cases of NStd distributions. They all exhibit the characteristics of long-tailed distribution similar to their node degree distributions. The similarity between NStd and data distribution implies that the NStd distribution may be the mapping of data distribution and thus we can implicitly uniformize the node degrees by reshaping the NStd distribution.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Analysis on the Variance Expressing</head><p>As shown in Figure <ref type="figure" target="#fig_8">2</ref>, the main difference among simple and hard nodes is the neighbor quality, i.e., node degree and intra-class rate. Hence, the feature propagation operation in GCNs is naturally a main reason for the variance expressing. Conjecture 1. Nodes with higher intra-class rates or larger degrees tend to have larger NStd after multiple propagation operations.</p><p>We provide theoretical analysis of Conjecture 1 in Appendix. We further verify this conjecture in real-world datasets. To avoid the influence of transformation, we use deparameterized 32-layer GCNs that only perform propagation at each layer. For the analysis of asymptotic behavior of such linear GCNs, the interested reader is referred to <ref type="bibr" target="#b40">[41]</ref>. Concretely, we perform 32 times of propagation operations to the raw input features, then report the intra-class rate and degree of different node sets in Table <ref type="table" target="#tab_1">I</ref>. When the difference of intra-class rate is relatively large, NStd monotonically increases as intra-class rate increases (Cora and Citeseer). On the other hand, NStd monotonically increases as node degree increases (Pubmed, because the mean node degree of Pubmed is significantly larger than that of the other two datasets.). Such observation suggests that the propagation tends to make the NStd of nodes with larger intra-class rates or degrees relatively larger than other nodes in a graph.</p><p>Transformation also contributes to the variance expressing. We plot the test accuracy of pure MLP with different model depths in Figure <ref type="figure" target="#fig_4">4</ref>. Transformation show the tendency to optimize models' parameters to a point that simple nodes' representations have relatively large NStd (in the context of pure MLP, simple nodes are the ones with better raw features or belong to head classes). A previous work <ref type="bibr" target="#b41">[42]</ref> also reports a similar fact that the mean magnitudes of feature vectors for each class are ranked from head to tail. They attribute this phenomenon to the momentum used in the optimizer. We argue that the reasons for variance expressing are two-folds:</p><p>(1) The propagation step gives rise to a phenomenon that larger intra-class rates or degrees are usually associated with larger NStd; and (2) Simpler nodes tend to have larger NStd after optimization. Under the mechanism of graph convolution, classification is easier for nodes with high-quality neighbors.</p><p>To summarize, propagation and transformation simultaneously contribute to the variance expressing phenomenon by different manners, i.e., graph convolution and optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Our Remedy: Distribution Reshaping</head><p>Motivated by the similarity between NStd distribution and data distribution, and the positive correlation between NStd and classification accuracy, we propose to leverage the NStd distribution to resolve the long-tailed data classification problem. Our main idea is to improve the overall and tail classification accuracy by incorporating a normalization into each graph convolutional layer. Concretely, our proposed normalization technique can reshape the NStd distribution from long-tailed into normal-like. Our motivations for this reshaping are mainly as follows.</p><p>? Training with long-tailed data is proved to cause a harmful effect that misleads the tail prediction biased towards the head <ref type="bibr" target="#b41">[42]</ref>. We further find that GCNs are optimized to a point that simple nodes have larger NStd. As a result, the NStd also shows as a long-tailed distribution similar to the data distribution. However, such distribution may be sub-optimal as it implies that the trained model is biased towards the head. Reshaping the distribution helps reduce bias by shrinking the magnitudes of feature vectors of head nodes.</p><p>? Techniques of reducing unstable extreme features is widely used in deep learning. Prior work <ref type="bibr" target="#b42">[43]</ref> proposes to learn discriminative features for face recognition by learning a center for each class and penalizing the distance between features and their corresponding class centers. Range loss <ref type="bibr" target="#b25">[26]</ref> penalizes variations by reducing the kurtosis (infrequent extreme deviation). Our distribution reshaping technique also effectively reduces the kurtosis of NStd distribution.</p><p>? Transforming distribution into more normal-like distribution is a widely used tool to stabilize variance and reduce skewness <ref type="bibr" target="#b43">[44]</ref>. Similar to zero-mean, unit-variance normalization methods <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b44">[45]</ref>, stabilizing the output of hidden layer will boost the training process. Moreover, distribution reshaping can preserve the important NStd information for each node, rather than simply resetting NStd by 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. THE PROPOSED RESNORM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Techniques to Improve Normality</head><p>Power transformation is a classic technique to improve normality <ref type="bibr" target="#b45">[46]</ref>, typically including square root transformation, reciprocal (negative exponent) transformation and identity transformation. Among them, the reciprocal transformation will reverse the order of the original data. We study the effects of several transformations from the perspectives of performance and normality. Specifically, we apply power transformation to node representations at each layer and record the mean accuracy of nodes with different NStds. Results are shown in Figure <ref type="figure" target="#fig_5">5</ref>. We can observe the inferior performance of tail nodes when no transformation is applied (Figure <ref type="figure" target="#fig_5">5(a)</ref>). Both square root and reciprocal square root transformations can substantially improve the overall and tail performance. We may understand the mechanism of power transformation by observing the performance change of the head and tail. Both square root and reciprocal square root transformation increase the relative magnitudes of the NStds of the tails and reduce that of the heads, which gives rise to the performance improvement and degradation for tail and head, respectively. Thanks to the long-tailed distribution, although we lose some accuracy of the head, we still achieve overall performance improvement. Reciprocal transformation is the best to uniformize the performance across heads and tails. However, square root transformation improves the overall performance the most. An intuitive explanation for that is reciprocal transformation reverses the order of NStd across the head and tail nodes, making the NStds of tail nodes larger than that of head nodes. Hence, the models are forced to pay more attention to tail nodes and reduce the bias towards head nodes. However, such violent transformation leads to huge instability. We can see the huge performance drop of head nodes in Figure <ref type="figure" target="#fig_5">5(d)</ref>. Therefore, square root transformation is more suitable. This inspires us to introduce positive power transformation as a component of the scale operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. An Interpretation from the Perspective of Influence Distribution</head><p>We here explore the mechanism of scaling from the perspective of influence distribution. Since the scaling is applied node-wisely, the magnitudes of node representations are scaled differently and individually, thus the influence distribution is modified. The idea of the distribution of influence score is initially introduced in <ref type="bibr" target="#b18">[19]</ref>, which captures the relative influences of all other nodes for any node x. For completeness, we also give the definition of influence distribution. is the input feature of node y. The influence distribution I x (y) of x is calculated as I x (y) = I(x,y) z I(x,z) . In expectation, the influence distribution of x is equivalent to the k-step random walk starting at x <ref type="bibr" target="#b18">[19]</ref>, which is static and only correlated to the graph topology. Here, static means that the expected influence distribution remains unchanged during the training process. The lack of adaptability and the irrelevance with node features harms the performance. For instance, some nodes are far away from each other but their features are similar, thus the influence on each other should be improved. The idea is that the relative influence of any node y on any node x should be learnable and also exploits the node features, which we achieve by incorporating the node-wise scale.</p><p>Theorem 1. The node-wise scale based on power transformation endows the influence distribution with dynamic and adaptability.</p><p>Proof. We start by deriving the derivative between hx ? e x and h x , where ? x denotes the std of d-dimensional vector h x .</p><p>Assuming ? x &gt; 0, we have</p><formula xml:id="formula_3">d(h x ? 1 ? e x ) dh x = d(h x ? ( 1 d h T x h x -1 d 2 1 T h x 1 T h x ) 0.5e ) dh x = 1 ? e x I d - e d? 1+0.5e x h x (h T x -?1 T ),<label>(4)</label></formula><p>where 1 is an all-ones vector and ? is the mean of h x . Assume we apply the scale at each layer right after the output of convolution layer, i.e., H (l) = Relu(Scale(GCN (H (l-1) ))) for any 1 &lt; l ? k. The output of CGN layer at the l-th layer is denoted by f</p><formula xml:id="formula_4">(l) x = 1 ? degx z?N (x) 1 ? degz W (l) h (l-1) z</formula><p>, and the output of scale is denoted by</p><formula xml:id="formula_5">B (l) x = f (l) x ? ( 1 ? (l) x</formula><p>) e . Then the partial derivative between h</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(l)</head><p>x and h (0) y is derived as</p><formula xml:id="formula_6">?h (l) x ?h (0) y = ?h (l) x ?B (l) x ?B (l) x ?f (l) x ?f (l) x ?h (0) y = diag(1 B (l) x &gt;0 )((<label>1</label></formula><formula xml:id="formula_7">? (l) x ) e I d - e d(? (l) x ) (1+0.5e) ? f (l) x (f (l) T x -? (l) x 1 T )) 1 ? deg x W (l) z?N (x) 1 ? deg z ?h (l-1) z ?h (0) y ,<label>(5)</label></formula><p>where 1 B (l)</p><p>x &gt;0 is an indicator vector indicates whether each element of B (l)</p><p>x is activated or not. By chain rule, we have</p><formula xml:id="formula_8">?h (k) x ?h (0) y = ? deg x deg y ? p=1 1 l=k 1 deg(v l p ) diag(1 B v l p &gt;0 )? v l p W (l) ,<label>(6)</label></formula><p>where ? is the number of paths v k p v k-1 p , ..., v 0 p of length k+1 starting from node x ending with y, v l-1 p ? N (v l p ) and ? v l p represents the Jacobian matrix produced by scale in Eq.4. Similar to <ref type="bibr" target="#b18">[19]</ref>, the expectation of ?h (l)   x ?h (0) y can be estimated as E[</p><formula xml:id="formula_9">?h (l) x ?h (0) y ] = ? degx ? degy ? 1 l=k ? v l p W (l) ( ? p=1 1 l=k 1 deg(v l p )</formula><p>), where 0 &lt; ? &lt; 1 is a probability. Consider when we do not apply the scale, i.e., E[</p><formula xml:id="formula_10">?h (l) x ?h (0) y ] = ? degx ? degy ? 1 l=k W (l) ( ? p=1 1 l=k 1 deg(v l p ) ). Note that ( ? p=1 1 l=k 1 deg(v l p )</formula><p>) is exactly the probability that a k-step random walk stopping at y when starting from x, the key point is that 1 l=k W (l) is irrelevant to the starting point and ending point, thus the sum of entries of</p><formula xml:id="formula_11">1 l=k W (l)</formula><p>is the same for all paths. Since multiplied by a same term for all paths does not change the probability (ratio), the expectation of influence distribution is still equivalent to the kstep random walk distribution. Although <ref type="bibr">GCN</ref>  that are inversely proportional to the NStds. This manifests that if y touch x via hub nodes (large-std nodes), then the relative influence of y on x will be reduced, which in turn reduces the relative influence of the hub nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Learning from Long-Tailed Graphs with Distribution-Reshaping Scaling</head><p>In this part, we formula the scale operation of ResNorm. We first include power transformation, i.e., x i = x i ? e i , where 0 &lt; e &lt; 1 is a hyper-parameter. Note that after power transformation, the residual NStd for node v i is ? 1-e i . We take the mean of the residual NStd to further transform the distribution into more normal-like by</p><formula xml:id="formula_12">x i ?? ? ? ? i</formula><p>, where 0 &lt; ? &lt; 1 is a hyperparameter. With this setting, e controls to what extent we normalize the magnitudes of NStd, while ? controls the extent we reshape the distribution of NStd into normal-like. This separation makes the scale operation flexible in determining the contributions of these two main effects. Formally, the scale operation of ResNorm is defined as</p><formula xml:id="formula_13">Scale(x i ) = x i ? ? ? ? e+? i .<label>(7)</label></formula><p>After scaling, the NStd of node v i becomes ?</p><p>(1-e-?) i</p><p>? ? ? , and the magnitude is similar to ?</p><formula xml:id="formula_14">(1-e) i</formula><p>, which is partially normalized. One of the most desirable effects of normalization is the "scale-invariant" property: the output remains unchanged even if the weights are scaled. As a result, normalization implicitly tunes the learning rate <ref type="bibr" target="#b15">[16]</ref> and decouples the optimization of direction and length of the parameters <ref type="bibr" target="#b46">[47]</ref>.</p><p>Proposition 1. Applying ResNorm after a linear layer makes the output more stable as the weights are scaled.</p><p>Proof. Let W be a weight matrix in a linear layer, W = ?W is the scaled weight matrix, x i is a column feature vector, h denotes the original output, and h denotes the output after the weight matrix is scaled by ?. We have</p><formula xml:id="formula_15">h = f (Scale(W x i )) = f ( W x i ? ? ? i ?+e ), = f ( ?W x i (??) ? (??) ?+e ) = ? (1-e) f ( W x i ? ? ? (e+?) i ), = ? (1-e) f (Scale(W x i )) = ? (1-e) h.<label>(8)</label></formula><p>Since 0 &lt; e &lt; 1, the scaling factor ? (1-e) for h is closer to 1 than the scaling factor ? for weight matrix W . Therefore, the fluctuation of output caused by weight scaling is alleviated. </p><formula xml:id="formula_16">? i = 1 d d j=1</formula><p>x i,j . The mean statistics contains information on node degree and neighboring relationship. We visualize an example in Figure <ref type="figure" target="#fig_7">6</ref>. The standard shift (shifting by subtracting the mean) may cause structural information loss. However, a very recent work <ref type="bibr" target="#b20">[21]</ref> suggests that subtracting mean makes the optimization curvature of W smoother. They introduce a learnable parameter to achieve a trade-off between retaining structural information and boosting training. In this paper, we mainly focus on the relationship between over-smoothing and shift. To reveal the effects caused by shifting, we adopt the widely used metric of smoothness.</p><p>Definition 2. The smoothness score of the node embedding matrix X ? R d?n is defined as</p><formula xml:id="formula_17">Score(X) = tr(XLX T ) = 1 2 n i=1 j?N (i)?i x i ? 1 + d i - x j 1 + d j 2 2 ,<label>(9)</label></formula><p>where L = I -P = I -D-1 2 ? D-1 2 is the graph laplacian.</p><p>A small value of smoothness score implies that the node representations may suffer from over-smoothing. We remove the non-linear activation for simplifying the analysis and give the following result. Lemma 1. (Lemma 1 in <ref type="bibr" target="#b47">[48]</ref>) The Smoothness score of the output of a GCN layer is bounded as S min C ? Score(W XP ) ? S max C, where C = tr(XP LP T X T ), S min and S max denote the minimum and maximum singular values of W , respectively.</p><p>If we formulate the shift in a matrix form, then applying shift operation is identical to multiply S in a form of S(W x i ), where S = I d -1 d 11 T and 1 is a d-dimensional vector full of 1. We further check how this operation affects the spectrum of W and establish the following result. As singular values of a matrix P are equal to the square root of the eigenvalues of P T P , we only need to prove that ? 1 ? ? 1 and ? d ? ? d . Note that S T S is a real symmetric matrix and the eigenvalues of S T S are 0, 1, 1, ..., 1, there exists an orthogonal matrix U that satisfies S T S = U T diag(0, 1, ..., 1)U . We begin with</p><formula xml:id="formula_18">W T W -W T S T SW = W T (I d -S T S)W , = W T U T (I d -diag(0, 1, ..., 1))U W , = W T U T diag(1, 0, ..., 0)U W , = W T U T (diag(1, 0, ..., 0)) T diag(1, 0, ..., 0)U W , = D T D,<label>(10)</label></formula><p>where D = diag(1, 0, ..., 0)U W . Let ? be the eigenvector of ? 1 such that ? T ? = 1 and W T W ? = ??. Then, we have</p><formula xml:id="formula_19">0 ? ? T D T D? = ? T W T W ? -?W T S T SW ?, = ? 1 ? T ? -?W T S T SW ? = ? 1 -?W T S T SW ?.<label>(11)</label></formula><p>Since W T S T SW is a real symmetric matrix, W T S T SW can be decomposed as</p><formula xml:id="formula_20">Q T diag(? 1 , ? 2 , ..., ? d )Q, where Q is an orthogonal matrix. Then ? 1 -?W T S T SW ? = ? 1 -? T Q T diag(? 1 , ? 2 , ..., ? d )Q?. We denote Q? = ? = [? 1 , ? 2 , ..., ? d ]. Note that (Q?) T Q? = ? T Q T Q? = ? T ? = 1, ? is also an unit vector such that ? 2 i = 1. We derive ? 1 &gt; ? 1 as below. 0 ? ? 1 -? T Q T diag(? 1 , ? 2 , ..., ? d )Q? = ? 1 -? T diag(? 1 , ? 2 , ..., ? d )?, = ? 1 -? 1 ? 2 1 + ? 2 ? 2 2 + ... + ? d ? 2 d , ? ? 1 -? 1 (? 2 1 + ? 2 2 + ... + ? 2 d ) = ? 1 -? 1 .<label>(12)</label></formula><p>Similarly, let ? be the unit eigenvector of ? d . Then we can immediately obtain ?W T W ? ? ? d similar to inequality <ref type="bibr" target="#b11">(12)</ref>. Finally, we derive ? d &gt; ? d as follows,</p><formula xml:id="formula_21">0 ?= ?W T W ? -? T W T S T SW ? = ?W T W ? -? d ? ? d -? d .<label>(13)</label></formula><p>With</p><formula xml:id="formula_22">? 1 ? ? 1 ? 0 and ? d ? ? d ? 0, we can obtain ? 1 = ? ? 1 ? ? ? 1 = ? 1 and ? d = ? ? d ? ? ? d = ? d , which</formula><p>completes the proof.</p><p>The above results validate that the standard shift serves as a preconditioner of W that shrinks the maximum and maximum singular values, leading to a smaller lower and upper bound of smoothness score. Hence, the standard shift potentially increases the risk of over-smoothing.</p><p>Based on the above empirical and theoretical analysis, the standard shift induces structural information loss and accelerates over-smoothing. We can conclude that the standard shift operation is not suitable for GRL. The importance of the mean statistics is different for each node, thus it is reasonable to retain a specific fraction of mean for each node representation. On the other hand, using degree-specific parameters for nodes with different degrees is considered a  feasible way in prior works <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>. However, the degreespecific parameter strategy is too expensive due to the longtailed distribution of node degree. Although we can utilize hashing-based approach to map some degrees to same entry of a hash table so as to reduce the parameters, it is still unaffordable for a normalization layer which is considered a light module. We here utilize the shift operation to simulate the degree-specific parameter strategy. Concretely, we assign a weight matrix W i = S i W = (I d -ki d 11 T )W to node v i , where 0 &lt; k i &lt; 1 and 1 -k i is the preservation ratio of mean for v i .</p><p>We now discuss what the matrix W i we assign to node v i should satisfy. Note that large-NStd nodes generally have better and more neighbors, it is natural their structural information is more useful. Besides, the over-smoothing issue should be well considered since the standard shift is proved to increase the risk of over-smoothing. Hence, the principles of the design of k i are (1) keeping more structural information for large-NStd nodes. <ref type="bibr" target="#b1">(2)</ref> Maintaining the Euclidean distance between node pairs. For the change of smoothness score, we have Theorem 3. Consider two node representations x i and x j , assume that the k i = k j , i.e., using the same preservation ratio for node pair v i and v j in the shift operation, then the smoothness score becomes smaller after shifting, i.e.,</p><formula xml:id="formula_23">Shift(xi) ? di+1 - Shift(xj ) ? dj +1 2 2 ? xi ? di+1 - xj ? dj +1 2 2 .</formula><p>Proof. We have Shift(x i ) = S i x i = (I d -ki d 11 T )x i , where d is the dimension of x i . Note that</p><formula xml:id="formula_24">S i x i ? d i - S j x j d j 2 2 = ( S i x i ? d i - S j x j d j ) T ( S i x i ? d i - S j x j d j ) = 1 d i d j ( d j S i x i -d i S j x j ) T ( d j S i x i -d i S j x j ) = 1 d i d j (d j x T i S i S i x i + d i x T j S j S j x j -2 d i d j x T i S i S j x j ),<label>(14) and xi</label></formula><formula xml:id="formula_25">? di - xj ? dj 2 2 = 1 didj (d j x T i x i +d i x T j x j -2 d i d j x T i x j ).</formula><p>Then we derive the result as follows,</p><formula xml:id="formula_26">S i x i ? d i - S j x j d j 2 2 - x i ? d i - x j d j 2 2 = 1 d i d j (d j x T i (S i S i -I d )x i + d i x T j (S j S j -I d )x j -2 d i d j x T i (S i S j -I d )x j ) = 1 d i d j d (d j x T i 1(k 2 i -2k i )1 T x i + d i x T j 1(k 2 j -2k j )1 T x j + 2 d i d j x T i 1(k i + k j -k i k j )1 T x j ) = 1 d i d j d (a 2 d j (k 2 i -2k i ) + b 2 d i (k 2 j -2k j ) + 2ab d i d j (k i + k j -k i k j )) = 1 d i d j d (2 -k)k(b d i -a d j )(a d j -b d i ) ? 0,<label>(15)</label></formula><p>where</p><formula xml:id="formula_27">0 &lt; k i = k j = k &lt; 1, a = x T i 1, and b = x T j 1.</formula><p>motivated by the fact larger degrees are generally associated with larger NStds, we set k i = 1 -?i ?max , where ? max is the maximum NStd of all nodes. The preservation ratio of mean for v i is ?i ?max , which is proportional to the NStd. Meanwhile, the diversity of preservation ratios avoids the continuous decay of Euclidean distance when multiple normalization layers are stacked. Mathematically, the shift component of ResNorm is defined as</p><formula xml:id="formula_28">Shift(x i ) = x i -(1 - ? i ? max )? i<label>(16)</label></formula><p>Combining the two components, ResNorm can be formulated as</p><formula xml:id="formula_29">ResNorm(x i ) = (x i -(1 -?i ?max )? i ) ? ? ? ? e+? i .<label>(17)</label></formula><p>Unlike the common setting, we do not introduce the learnable affine parameters that may lead to over-fitting since the shift and scale of ResNorm are applied node-wisely rather than channel-wisely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In this section, we evaluate performance of ResNorm on node classification benchmarks. In particular, we focus on answering the following questions: (Q1) How does ResNorm perform on node classification task compared to other graphbased normalization methods? (Q2) Can ResNorm further enhance other techniques that are designed for improving deep GNN models. (Q3) Can ResNorm significantly improve the accuracy of tail nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Settings</head><p>Following literature of GNNs, we adopt four popularly used datasets, including three citation networks Cora, Citeseer and Pubmed <ref type="bibr" target="#b48">[49]</ref>, a co-author network Coauthor Physics <ref type="bibr" target="#b49">[50]</ref>. For the three citation networks, we follow the standard split as previous work <ref type="bibr" target="#b50">[51]</ref>. For the coauthor network, we randomly sample 20 nodes per class for training, 30 nodes per class for validation, and the rest for testing. We compare ResNorm with 4 graph-based normalization methods, including Node-Norm <ref type="bibr" target="#b22">[23]</ref>, GraphNorm <ref type="bibr" target="#b20">[21]</ref>, PairNorm <ref type="bibr" target="#b23">[24]</ref>, and DGN <ref type="bibr" target="#b24">[25]</ref>. For comprehensive evaluation, all the methods are evaluated over three representative GNN backbones, i.e., GCN <ref type="bibr" target="#b1">[2]</ref>, GATv2 <ref type="bibr" target="#b51">[52]</ref>, and GraphSage <ref type="bibr" target="#b0">[1]</ref>. We use shallow (2-, 4-, 8-layer) GNN models for experiment since deep models will severely suffer from the over-smoothing issue. We will empirically show later that ResNorm can further enhance other deep GNN techniques. We implement all the methods and GNN backbones with Pytorch Geometric Library. Experiments are conducted using a single 24G NVIDIA GEFORCE RTX 3090. We set the learning rate to 0.01, weight decay to 5e-4 for the three citation networks and 0 for the coauthor network, the number of hidden units to 64, and early stopping patience to 100. We perform a grid search to tune other hyper-parameters. The search space for each hyper-parameter is as follows: dropout ratio ? {0.7, 0.5, 0.3}, ? ? {0.1, 0.3, 0.5, 0.7}, ? ? {0.1, 0.3, 0.5, 0.7}. For training, we select the Adam optimizer and the best model checkpoint with the minimum validation loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results on Node Classification</head><p>We evaluate the performance and report the test accuracy in Table <ref type="table" target="#tab_3">II</ref>. The results show that ResNorm outperforms other normalization methods in most settings, which demonstrates the effectiveness of ResNorm on node classification tasks. Although the preliminary analysis (e.g., Sec.IV) is mostly based on GCN, the consistent performance improvement validates the effectiveness of ResNorm in working on prevalent GNN frameworks. We argue that the accuracy improvement of the major tail nodes contributes to the higher test accuracy of ResNorm. Especially, when the models are relatively shallow (2, 4 layers), ResNorm has the best results over all the backbones on all the datasets. It is worthwhile to note that PairNorm and DGN are normalization layers that aim to tackle the over-smoothing issue. The competitive performance of ResNorm under the settings of 8 layers verifies the previous discuss and analysis on the shift operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Combining ResNorm with Other Deep GNN Techniques.</head><p>In this subsection, we study whether ResNorm can further enhance other deep GNN models and techniques. We utilize a representative deep model APPNP <ref type="bibr" target="#b52">[53]</ref> and GCN+PairNorm as backbones. For APPNP, ResNorm is only applied at the final output since the propagate process has no parameter. We tune the strength of initial residual ? ? {0.15, 0.2}. For GCN+PairNorm, ResNorm is applied at each layer right after PairNorm. We report the average test accuracy of 10 runs in Table <ref type="table" target="#tab_4">III</ref>. The results demonstrate that ResNorm, serving as a plug-and-play light module, can significantly improve deep models. We attribute the improvement to the ability of ResNorm in learning with long-tailed data while most current deep models only focus on the over-smoothing issue.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ability to Improve the Performance of Tail Nodes</head><p>To answer Q3, we report the test accuracy of node groups according to their NStds when trained with and without ResNorm. We use 8-layer GCN modes and Cora dataset. The node groups are ranked from smaller to larger NStd. Figure <ref type="figure" target="#fig_10">8</ref> depicts the test accuracy of tail and head nodes. The gap of accuracy among different node groups is significantly smaller when ResNorm is equipped to GCNs. Particularly, the lowest accuracy bars of GCN+ResNorm are significantly higher than that of vanilla GCN. The results validate that ResNorm can improve the performance of the tail nodes, which compensates for the declines of the highest accuracy bars. Consequently, the overall performance is improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we study the normalization technique for GCNs. We observe the variance expressing phenomenon and give explanations for why the head nodes commonly have larger NStd. We propose to leverage the distribution reshaping technique to improve the accuracy of tail nodes. The distribution reshaping operation constitutes the scale of ResNorm, aiming to implicitly reshape the distribution of node degree. We further theoretically prove that standard shift serves as a preconditioner of weight matrix that shrinks the singular values. Consequently, the standard shift increases the risk of over-smoothing. Empirically, the standard shift also induces structural information loss. Therefore, we propose a shift operation that can simultaneously preserve the structural information and boost the training. Experimental results confirm the effectiveness of ResNorm on improving classification performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Classification accuracy statistics w.r.t. nodes with different Node-wise Standard Deviation (NStd) on the three datasets (i.e., Cora, Citeseer, and Pubmed). The X-axis denotes model depth (i.e., the number of layers of GCNs, while the Y-axis denotes the classification accuracy. To sidestep the gradient vanishing problem, all models are equipped with the residual connection. Each NStd value is computed based on the output of the corresponding previous hidden layer, respectively. The results of different node groups (ranked according to the NStd values) are plotted in different colors.</figDesc><graphic url="image-3.png" coords="2,377.16,61.47,129.56,107.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig</head><label></label><figDesc>Fig.2. Mean intra-class neighbor rate and node degree statistics for nodes with the largest or smallest 2% NStds. Results are reported based on 10 independent runs. +2% corresponds to the nodes with the largest 2% NStds.-2% corresponds to the nodes with the smallest 2% NStds. Cr, CS, and PM denote Cora, Citeseer and Pubmed datasets, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. NStd distribution of the output of the last layer. We use 32-layer GCNs equipped with residual connection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Pure MLP models without feature propagation.</figDesc><graphic url="image-10.png" coords="4,396.82,56.80,79.50,66.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Accuracy for different node sets, where the indices of node set are ranked from smaller to larger NStd. Blue bars correspond to the overall accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Definition 1 .</head><label>1</label><figDesc>The influence score I(x, y) that measures the influence of node y on x is the sum of the absolute values of the entries of the Jacobian matrix [ representation of node x at the k-th (last) layer and h (0) y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. An illustration for showing that mean statistics contains topology information using KarateClub dataset. We can identify the nodes within the subgraph in the bottom right only using their means.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Theorem 2 .</head><label>2</label><figDesc>Let ? 1 and ? d respectively be the minimum and maximum singular values of matrix W , ? 1 and ? d respectively be the minimum and maximum singular values of matrix SW . Then we have ? 1 ? ? 1 and ? d ? ? d . Proof. Let ? 1 ? ? 2 ? ... ? ? d be the eigenvalues of W T W and ? 1 ? ? 2 ? ... ? ? d be the eigenvalues of W T S T SW .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Node degree distributions of the datasets used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Accuracy for different node groups on different datasets, where indices of node groups are ranked from smaller NStd to larger NStd.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I MEAN</head><label>I</label><figDesc>NODE DEGREE AND INTRA-CLASS RATE (ICR) STATISTICS FOR THE DIFFERENT NODE SETS. +k% CORRESPONDS TO THE NODES WITH THE LARGEST k% NSTDS AFTER 32 TIMES OF DE-PARAMETERIZED PROPAGATION.</figDesc><table><row><cell cols="2">Metric</cell><cell>Cora</cell><cell cols="2">Citeseer Pubmed</cell></row><row><cell></cell><cell>+1%</cell><cell>0.925</cell><cell>1.0</cell><cell>0.783</cell></row><row><cell>ICR</cell><cell cols="2">+5% +30% 0.821 0.892</cell><cell>0.856 0.761</cell><cell>0.804 0.803</cell></row><row><cell></cell><cell cols="2">+80% 0.814</cell><cell>0.745</cell><cell>0.804</cell></row><row><cell></cell><cell>+1%</cell><cell>7.44</cell><cell>0.76</cell><cell>37.0</cell></row><row><cell>Degree</cell><cell>+5% +30%</cell><cell>4.23 6.40</cell><cell>1.51 1.73</cell><cell>24.60 10.87</cell></row><row><cell></cell><cell>+80%</cell><cell>4.50</cell><cell>3.01</cell><cell>5.36</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>has a bias term</figDesc><table><row><cell>and adaptive. The norm of ? v l p is mainly determined by 1 ? e v l p since that of e d? 1+0.5e f v l p (f T v l -?1 T ) is normally small. Hence, I, p v l p the sum of the absolute values of the entries of 1 l=k ? v l p W (l) largely depends on the magnitudes of 1 ? e v l p</cell></row><row><cell>? ? degx degy distribution is still static since that punishes the nodes with high degrees, the influence ? degx ? degy cannot be modified</cell></row><row><cell>during training. However, the scale introduces an additional</cell></row><row><cell>derivative ? v l p that is trainable and varies with the current node</cell></row><row><cell>representation f v l p , making the influence distribution dynamic</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II SUMMARY</head><label>II</label><figDesc>OF CLASSIFICATION ACCURACY RESULTS. THE RESULTS WE REPORT ARE THE AVERAGE ACCURACY AFTER 10 INDEPENDENT RUNS. LAYER #K INDICATES THE MODELS HAVE K CONVOLUTION LAYERS STACKED.</figDesc><table><row><cell></cell><cell>Dataset</cell><cell>Model</cell><cell>NN</cell><cell>GN</cell><cell>Layer 2 PN</cell><cell>DGN</cell><cell>RN</cell><cell>NN</cell><cell>GN</cell><cell>Layer 4 PN</cell><cell>DGN</cell><cell>RN</cell><cell>NN</cell><cell>GN</cell><cell>Layer 8 PN</cell><cell>DGN</cell><cell>RN</cell></row><row><cell></cell><cell></cell><cell>GCN</cell><cell cols="3">82.2 76.6 75.1</cell><cell>81.5</cell><cell>83.1</cell><cell cols="3">80.8 78.5 73.4</cell><cell>79.7</cell><cell>82.6</cell><cell>78.7</cell><cell cols="2">72.8 72.2</cell><cell>73.5</cell><cell>78.6</cell></row><row><cell></cell><cell>Cora</cell><cell cols="4">GATv2 82.3 77.8 75.3</cell><cell>80.9</cell><cell>83.6</cell><cell cols="3">80.7 77.4 75.0</cell><cell>78.9</cell><cell>80.8</cell><cell>75.4</cell><cell cols="2">74.6 71.7</cell><cell>71.3</cell><cell>75.1</cell></row><row><cell></cell><cell></cell><cell>SAGE</cell><cell cols="3">81.9 76.1 74.0</cell><cell>80.6</cell><cell>82.6</cell><cell cols="3">81.2 76.8 73.2</cell><cell>80.2</cell><cell>82.2</cell><cell>46.5</cell><cell>72.2</cell><cell>65.3</cell><cell>68.3</cell><cell>67.2</cell></row><row><cell></cell><cell></cell><cell>GCN</cell><cell cols="3">70.1 63.1 63.6</cell><cell>69.3</cell><cell>73.2</cell><cell cols="3">68.5 63.6 65.0</cell><cell>62.1</cell><cell>70.0</cell><cell cols="3">59.8 58.1 58.9</cell><cell>46.0</cell><cell>63.7</cell></row><row><cell></cell><cell>Citeseer</cell><cell cols="4">GATv2 70.0 63.7 64.5</cell><cell>68.0</cell><cell>72.4</cell><cell cols="3">68.2 65.4 66.5</cell><cell>62.8</cell><cell>69.4</cell><cell cols="2">40.5 58.6</cell><cell>60.4</cell><cell>48.5</cell><cell>57.1</cell></row><row><cell></cell><cell></cell><cell>SAGE</cell><cell cols="3">70.8 62.4 61.9</cell><cell>68.2</cell><cell>72.9</cell><cell cols="3">67.2 61.2 62.9</cell><cell>63.0</cell><cell>69.0</cell><cell>38.7</cell><cell>55.1</cell><cell>52.2</cell><cell>46.5</cell><cell>39.8</cell></row><row><cell></cell><cell></cell><cell>GCN</cell><cell cols="3">78.1 76.7 76.0</cell><cell>78.0</cell><cell>80.0</cell><cell cols="3">77.3 77.2 76.4</cell><cell>76.9</cell><cell>78.3</cell><cell cols="2">75.1 75.2</cell><cell>75.5</cell><cell>75.2</cell><cell>75.3</cell></row><row><cell></cell><cell>Pubmed</cell><cell cols="4">GATv2 78.3 76.4 75.4</cell><cell>76.4</cell><cell>78.9</cell><cell cols="3">78.0 76.6 76.0</cell><cell>75.4</cell><cell>78.1</cell><cell>70.9</cell><cell>76.1</cell><cell>75.6</cell><cell>75.1</cell><cell>74.5</cell></row><row><cell></cell><cell></cell><cell>SAGE</cell><cell cols="3">78.3 75.6 74.4</cell><cell>76.9</cell><cell>79.3</cell><cell cols="3">76.8 76.2 74.4</cell><cell>75.0</cell><cell>78.0</cell><cell>61.9</cell><cell>74.5</cell><cell>74.3</cell><cell>72.1</cell><cell>68.0</cell></row><row><cell></cell><cell></cell><cell>GCN</cell><cell cols="3">93.8 87.1 83.4</cell><cell>94.0</cell><cell>94.2</cell><cell cols="3">93.5 90.1 90.0</cell><cell>93.2</cell><cell>94.5</cell><cell cols="3">93.4 90.2 89.0</cell><cell>92.1</cell><cell>93.7</cell></row><row><cell cols="2">Coauthors</cell><cell cols="4">GATv2 91.3 87.8 86.6</cell><cell>91.4</cell><cell>92.3</cell><cell cols="3">92.0 90.1 88.6</cell><cell>91.3</cell><cell>92.5</cell><cell cols="3">91.5 89.1 88.1</cell><cell>92.1</cell><cell>92.6</cell></row><row><cell></cell><cell></cell><cell>SAGE</cell><cell cols="3">93.2 84.7 82.0</cell><cell>93.4</cell><cell>93.3</cell><cell cols="3">92.8 89.6 86.4</cell><cell>92.2</cell><cell>93.9</cell><cell cols="3">92.5 89.6 84.5</cell><cell>85.8</cell><cell>93.1</cell></row><row><cell>0</cell><cell cols="2">50 Node Degree 100</cell><cell>150</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III TEST</head><label>III</label><figDesc>ACCURACY ON DEEP GNN MODELS WITH AND WITHOUT RESNORM. WE CALCULATE THE AVERAGE IMPROVEMENT ACHIEVED BY RESNORM OVER EACH BACKBONE.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell cols="3">Layer 16 Layer 32 Improvement(%)</cell></row><row><cell>Cora</cell><cell>PN PN+RN APPNP APPNP+RN</cell><cell>62.5 77.5 82.4 84.9</cell><cell>42.1 59.3 82.4 85.1</cell><cell>32.5 3.1</cell></row><row><cell>Citeseer</cell><cell>PN PN+RN APPNP APPNP+RN</cell><cell>39.0 63.5 69.6 74.3</cell><cell>18.1 39.3 69.7 74.8</cell><cell>90.0 7.0</cell></row><row><cell>Pubmed</cell><cell>PN PN+RN APPNP APPNP+RN</cell><cell>73.2 77.9 79.2 80.3</cell><cell>67.8 77.4 79.2 80.5</cell><cell>10.2 1.5</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>The preferred spelling of the word "acknowledgment" in America is without an "e" after the "g". Avoid the stilted expression "one of us (R. B. G.) thanks . . .". Instead, try "R. B. G. thanks. . .". Put sponsor acknowledgments in the unnumbered footnote on the first page.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adagnn: Graph neural networks with adaptive frequency response filter</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jalaian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="392" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-view denoising graph auto-encoders on heterogeneous information networks for cold-start recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<biblScope unit="page" from="2338" to="2348" />
			<date type="published" when="2021">2021</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Social recommendation: a review</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Network Analysis and Mining</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1113" to="1133" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Graph neural networks for social recommendation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">E</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">MIRA: leveraging multi-intention co-click information in web-scale document retrieval using deep neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="227" to="238" />
		</imprint>
	</monogr>
	<note>in WWW. ACM / IW3C2, 2021</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Few-shot graph learning for molecular property prediction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Herr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wiest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="2559" to="2567" />
		</imprint>
	</monogr>
	<note>in WWW. ACM / IW3C2, 2021</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graphdf: A discrete flow model for molecular graph generation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, ser. Proceedings of Machine Learning Research</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="7192" to="7203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>in ICLR. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Stochastic training of graph convolutional networks with variance reduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, ser. Proceedings of Machine Learning Research</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="941" to="949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">My head is your tail: applying link analysis on longtailed music listening behavior for music recommendation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="213" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recommending the long tail items through personalized diversification</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Hamedani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="page" from="348" to="357" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Trading-off among accuracy, similarity, diversity, and long-tail: a graph-based recommendation approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RecSys</title>
		<imprint>
			<biblScope unit="page" from="57" to="64" />
			<date type="published" when="2013">2013</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, ser. JMLR Workshop and Conference Proceedings</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Layer normalization</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tail-gnn: Tail-node graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<biblScope unit="page" from="1109" to="1119" />
			<date type="published" when="2021">2021</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, ser. Proceedings of Machine Learning Research</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="5449" to="5458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Graphnorm: A principled approach to accelerating graph neural network training</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, ser. Proceedings of Machine Learning Research</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="1204" to="1215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>abs/1607.08022</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Understanding and resolving performance degradation in deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2728" to="2737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pairnorm: Tackling oversmoothing in gnns</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Towards deeper graph neural networks with differentiable group normalization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Range loss for deep face recognition with long-tailed training data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5419" to="5428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Factors in finetuning deep model for object detection with long-tail distribution</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="864" to="873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7029" to="7039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Improving recommendation for long-tail queries via templates</title>
		<author>
			<persName><forename type="first">I</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Maarek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A model of two tales: Dual transfer learning framework for improved long-tail item recommendation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="2220" to="2231" />
		</imprint>
	</monogr>
	<note>in WWW. ACM / IW3C2, 2021</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The long tail of social networking.: Revenue models of social networking sites</title>
		<author>
			<persName><forename type="first">A</forename><surname>Enders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hungenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mauch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Management Journal</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="199" to="211" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph neural networks for friend ranking in large-scale social platforms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2535" to="2546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Tailgate: handling long-tail content with a little help from friends</title>
		<author>
			<persName><forename type="first">S</forename><surname>Traverso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huguenin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Trestian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Erramilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Laoutaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Papagiannaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Challenging the long tail recommendation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="896" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The long tail of recommender systems and how to leverage it</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tuzhilin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RecSys</title>
		<imprint>
			<biblScope unit="page" from="11" to="18" />
			<date type="published" when="2008">2008</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Melu: Meta-learned user preference estimator for cold-start recommendation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<biblScope unit="page" from="1073" to="1082" />
			<date type="published" when="2019">2019</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Addressing the item cold-start problem by attribute-driven active learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="631" to="644" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Investigating and mitigating degree-related biases in graph convoltuional networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1435" to="1444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Demo-net: Degree-specific graph neural networks for node and graph classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<biblScope unit="page" from="406" to="415" />
			<date type="published" when="2019">2019</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Measuring and relieving the over-smoothing problem for graph neural networks from the topological view</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3438" to="3445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Dissecting the diffusion process in linear graph convolutional networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5758" to="5769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Long-tailed classification by keeping the good and removing the bad momentum causal effect</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (7), ser</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9911</biblScope>
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The box-cox transformation technique: a review</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Sakia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series D (The Statistician)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="169" to="178" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="742" to="755" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On the comparative anatomy of transformations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Tukey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Annals of Mathematical Statistics</title>
		<imprint>
			<date type="published" when="1957">1957</date>
			<biblScope unit="page" from="602" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Exponential convergence rates for batch normalization: The power of length-direction decoupling in non-convex optimization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daneshmand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Neymeyr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS, ser. Proceedings of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="806" to="815" />
			<date type="published" when="2019">2019</date>
			<publisher>PMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Dirichlet energy constrained learning for deep graph neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="21" to="834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Revisiting semisupervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, ser. JMLR Workshop and Conference Proceedings</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">How attentive are graph attention networks?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yahav</surname></persName>
		</author>
		<idno>abs/2105.14491</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
