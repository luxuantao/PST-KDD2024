<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Segment Anything</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-04-05">5 Apr 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><surname>Mintun</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nikhila</forename><surname>Ravi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chloe</forename><surname>Rolland</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Laura</forename><surname>Gustafson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Spencer</forename><surname>Whitehead</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
						</author>
						<title level="a" type="main">Segment Anything</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-04-05">5 Apr 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2304.02643v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Meta AI Research, FAIR (b) Model: Segment Anything Model (SAM) prompt image valid mask image encoder prompt encoder lightweight mask decoder (a) Task: promptable segmentation segmentation prompt image model cat with black ears</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We aim to build a foundation model for segmentation by introducing three interconnected components: a promptable segmentation task, a segmentation model (SAM) that powers data annotation and enables zero-shot transfer to a range of tasks via prompt engineering, and a data engine for collecting SA-1B, our dataset of over 1 billion masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Large language models pre-trained on web-scale datasets are revolutionizing NLP with strong zero-shot and few-shot generalization <ref type="bibr" target="#b9">[10]</ref>. These "foundation models" <ref type="bibr" target="#b7">[8]</ref> can generalize to tasks and data distributions beyond those seen during training. This capability is often implemented with prompt engineering in which hand-crafted text is used to prompt the language model to generate a valid textual response for the task at hand. When scaled and trained with abundant text corpora from the web, these models' zero and few-shot performance compares surprisingly well to (even matching in some cases) fine-tuned models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21]</ref>. Empirical trends show this behavior improving with model scale, dataset size, and total training compute <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>Foundation models have also been explored in computer vision, albeit to a lesser extent. Perhaps the most prominent illustration aligns paired text and images from the web. For example, CLIP <ref type="bibr" target="#b81">[82]</ref> and ALIGN <ref type="bibr" target="#b54">[55]</ref> use contrastive learning to train text and image encoders that align the two modalities. Once trained, engineered text prompts enable zero-shot generalization to novel visual concepts and data distributions. Such encoders also compose effectively with other modules to enable downstream tasks, such as image generation (e.g., DALL?E <ref type="bibr" target="#b82">[83]</ref>). While much progress has been made on vision and language encoders, computer vision includes a wide range of problems beyond this scope, and for many of these, abundant training data does not exist.</p><p>In this work, our goal is to build a foundation model for image segmentation. That is, we seek to develop a promptable model and pre-train it on a broad dataset using a task that enables powerful generalization. With this model, we aim to solve a range of downstream segmentation problems on new data distributions using prompt engineering.</p><p>The success of this plan hinges on three components: task, model, and data. To develop them, we address the following questions about image segmentation:</p><p>1. What task will enable zero-shot generalization? 2. What is the corresponding model architecture? 3. What data can power this task and model? These questions are entangled and require a comprehensive solution. We start by defining a promptable segmentation task that is general enough to provide a powerful pretraining objective and to enable a wide range of downstream applications. This task requires a model that supports flexible prompting and can output segmentation masks in realtime when prompted to allow for interactive use. To train our model, we need a diverse, large-scale source of data. Unfortunately, there is no web-scale data source for segmentation; to address this, we build a "data engine", i.e., we iterate between using our efficient model to assist in data collection and using the newly collected data to improve the model. We introduce each interconnected component next, followed by the dataset we created and the experiments that demonstrate the effectiveness of our approach.</p><p>Task ( ?2). In NLP and more recently computer vision, foundation models are a promising development that can perform zero-shot and few-shot learning for new datasets and tasks often by using "prompting" techniques. Inspired by this line of work, we propose the promptable segmentation task, where the goal is to return a valid segmentation mask given any segmentation prompt (see Fig. <ref type="figure" target="#fig_1">1a</ref>). A prompt simply specifies what to segment in an image, e.g., a prompt can include spatial or text information identifying an object. The requirement of a valid output mask means that even when a prompt is ambiguous and could refer to multiple objects (for example, a point on a shirt may indicate either the shirt or the person wearing it), the output should be a reasonable mask for at least one of those objects. We use the promptable segmentation task as both a pre-training objective and to solve general downstream segmentation tasks via prompt engineering.</p><p>Model ( ?3). The promptable segmentation task and the goal of real-world use impose constraints on the model architecture. In particular, the model must support flexible prompts, needs to compute masks in amortized real-time to allow interactive use, and must be ambiguity-aware. Surprisingly, we find that a simple design satisfies all three constraints: a powerful image encoder computes an image embedding, a prompt encoder embeds prompts, and then the two information sources are combined in a lightweight mask decoder that predicts segmentation masks. We refer to this model as the Segment Anything Model, or SAM (see Fig. <ref type="figure" target="#fig_1">1b</ref>). By separating SAM into an image encoder and a fast prompt encoder / mask decoder, the same image embedding can be reused (and its cost amortized) with different prompts. Given an image embedding, the prompt encoder and mask decoder predict a mask from a prompt in ?50ms in a web browser. We focus on point, box, and mask prompts, and also present initial results with free-form text prompts. To make SAM ambiguity-aware, we design it to predict multiple masks for a single prompt allowing SAM to naturally handle ambiguity, such as the shirt vs. person example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data engine ( ?4).</head><p>To achieve strong generalization to new data distributions, we found it necessary to train SAM on a large and diverse set of masks, beyond any segmentation dataset that already exists. While a typical approach for foundation models is to obtain data online <ref type="bibr" target="#b81">[82]</ref>, masks are not naturally abundant and thus we need an alternative strategy. Our solution is to build a "data engine", i.e., we co-develop our model with model-in-the-loop dataset annotation (see Fig. <ref type="figure" target="#fig_1">1c</ref>). Our data engine has three stages: assisted-manual, semi-automatic, and fully automatic. In the first stage, SAM assists annotators in annotating masks, similar to a classic interactive segmentation setup. In the second stage, SAM can automatically generate masks for a subset of objects by prompting it with likely object locations and annotators focus on annotating the remaining objects, helping increase mask diversity. In the final stage, we prompt SAM with a regular grid of foreground points, yielding on average ?100 high-quality masks per image.</p><p>Dataset ( ?5). Our final dataset, SA-1B, includes more than 1B masks from 11M licensed and privacy-preserving images (see Fig. <ref type="figure">2</ref>). SA-1B, collected fully automatically using the final stage of our data engine, has 400? more masks than any existing segmentation dataset <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b115">117,</ref><ref type="bibr" target="#b59">60]</ref>, and as we verify extensively, the masks are of high quality and diversity. Beyond its use in training SAM to be robust and general, we hope SA-1B becomes a valuable resource for research aiming to build new foundation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Responsible AI ( ?6).</head><p>We study and report on potential fairness concerns and biases when using SA-1B and SAM. Images in SA-1B span a geographically and economically diverse set of countries and we found that SAM performs similarly across different groups of people. Together, we hope this will make our work more equitable for real-world use cases. We provide model and dataset cards in the appendix.</p><p>Experiments ( ?7). We extensively evaluate SAM. First, using a diverse new suite of 23 segmentation datasets, we find that SAM produces high-quality masks from a single foreground point, often only slightly below that of the manually annotated ground truth. Second, we find consistently strong quantitative and qualitative results on a variety of downstream tasks under a zero-shot transfer protocol using prompt engineering, including edge detection, object proposal generation, instance segmentation, and a preliminary exploration of text-to-mask prediction. These results suggest that SAM can be used out-of-the-box with prompt engineering to solve a variety of tasks involving object and image distributions beyond SAM's training data. Nevertheless, room for improvement remains, as we discuss in ?8.</p><p>Release. We are releasing the SA-1B dataset for research purposes and making SAM available under a permissive open license (Apache 2.0) at https://segment-anything.com. We also showcase SAM's capabilities with an online demo. Example images with overlaid masks from our newly introduced dataset, SA-1B. SA-1B contains 11M diverse, high-resolution, licensed, and privacy protecting images and 1.1B high-quality segmentation masks. These masks were annotated fully automatically by SAM, and as we verify by human ratings and numerous experiments, are of high quality and diversity. We group images by number of masks per image for visualization (there are ?100 masks per image on average).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Segment Anything Task</head><p>We take inspiration from NLP, where the next token prediction task is used for foundation model pre-training and to solve diverse downstream tasks via prompt engineering <ref type="bibr" target="#b9">[10]</ref>. To build a foundation model for segmentation, we aim to define a task with analogous capabilities.</p><p>Task. We start by translating the idea of a prompt from NLP to segmentation, where a prompt can be a set of foreground / background points, a rough box or mask, free-form text, or, in general, any information indicating what to segment in an image. The promptable segmentation task, then, is to return a valid segmentation mask given any prompt. The requirement of a "valid" mask simply means that even when a prompt is ambiguous and could refer to multiple objects (e.g., recall the shirt vs. person example, and see Fig. <ref type="figure" target="#fig_3">3</ref>), the output should be a reasonable mask for at least one of those objects. This requirement is similar to expecting a language model to output a coherent response to an ambiguous prompt. We choose this task because it leads to a natural pre-training algorithm and a general method for zero-shot transfer to downstream segmentation tasks via prompting.</p><p>Pre-training. The promptable segmentation task suggests a natural pre-training algorithm that simulates a sequence of prompts (e.g., points, boxes, masks) for each training sample and compares the model's mask predictions against the ground truth. We adapt this method from interactive segmentation <ref type="bibr" target="#b107">[109,</ref><ref type="bibr" target="#b69">70]</ref>, although unlike interactive segmentation whose aim is to eventually predict a valid mask after enough user input, our aim is to always predict a valid mask for any prompt even when the prompt is ambiguous. This ensures that a pre-trained model is effective in use cases that involve ambiguity, including automatic annotation as required by our data engine ?4. We note that performing well at this task is challenging and requires specialized modeling and training loss choices, which we discuss in ?3.</p><p>Zero-shot transfer. Intuitively, our pre-training task endows the model with the ability to respond appropriately to any prompt at inference time, and thus downstream tasks can be solved by engineering appropriate prompts. For example, if one has a bounding box detector for cats, cat instance segmentation can be solved by providing the detector's box output as a prompt to our model. In general, a wide array of practical segmentation tasks can be cast as prompting. In addition to automatic dataset labeling, we explore five diverse example tasks in our experiments in ?7.</p><p>Related tasks. Segmentation is a broad field: there's interactive segmentation <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b107">109]</ref>, edge detection <ref type="bibr" target="#b2">[3]</ref>, super pixelization <ref type="bibr" target="#b84">[85]</ref>, object proposal generation <ref type="bibr" target="#b1">[2]</ref>, foreground segmentation <ref type="bibr" target="#b93">[94]</ref>, semantic segmentation <ref type="bibr" target="#b89">[90]</ref>, instance segmentation <ref type="bibr" target="#b65">[66]</ref>, panoptic segmentation <ref type="bibr" target="#b58">[59]</ref>, etc. The goal of our promptable segmentation task is to produce a broadly capable model that can adapt to many (though not all) existing and new segmentation tasks via prompt engineering. This capability is a form of task generalization <ref type="bibr" target="#b25">[26]</ref>. Note that this is different than previous work on multi-task segmentation systems. In a multi-task system, a single model performs a fixed set of tasks, e.g., joint semantic, instance, and panoptic segmentation <ref type="bibr" target="#b112">[114,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b53">54]</ref>, but the training and test tasks are the same. An important distinction in our work is that a model trained for promptable segmentation can perform a new, different task at inference time by acting as a component in a larger system, e.g., to perform instance segmentation, a promptable segmentation model is combined with an existing object detector.</p><p>Discussion. Prompting and composition are powerful tools that enable a single model to be used in extensible ways, potentially to accomplish tasks unknown at the time of model design. This approach is analogous to how other foundation models are used, e.g., how CLIP <ref type="bibr" target="#b81">[82]</ref> is the text-image alignment component of the DALL?E <ref type="bibr" target="#b82">[83]</ref> image generation system. We anticipate that composable system design, powered by techniques such as prompt engineering, will enable a wider variety of applications than systems trained specifically for a fixed set of tasks. It's also interesting to compare promptable and interactive segmentation through the lens of composition: while interactive segmentation models are designed with human users in mind, a model trained for promptable segmentation can also be composed into a larger algorithmic system as we will demonstrate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Segment Anything Model</head><p>We next describe the Segment Anything Model (SAM) for promptable segmentation. SAM has three components, illustrated in Fig. <ref type="figure" target="#fig_4">4</ref>: an image encoder, a flexible prompt encoder, and a fast mask decoder. We build on Transformer vision models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b61">62]</ref> with specific tradeoffs for (amortized) real-time performance. We describe these components at a high-level here, with details in ?A.</p><p>Image encoder. Motivated by scalability and powerful pretraining methods, we use an MAE <ref type="bibr" target="#b46">[47]</ref> pre-trained Vision Transformer (ViT) <ref type="bibr" target="#b32">[33]</ref> minimally adapted to process high resolution inputs <ref type="bibr" target="#b61">[62]</ref>. The image encoder runs once per image and can be applied prior to prompting the model. Prompt encoder. We consider two sets of prompts: sparse (points, boxes, text) and dense (masks). We represent points and boxes by positional encodings <ref type="bibr" target="#b94">[95]</ref> summed with learned embeddings for each prompt type and free-form text with an off-the-shelf text encoder from CLIP <ref type="bibr" target="#b81">[82]</ref>. Dense prompts (i.e., masks) are embedded using convolutions and summed element-wise with the image embedding.</p><p>Mask decoder. The mask decoder efficiently maps the image embedding, prompt embeddings, and an output token to a mask. This design, inspired by <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20]</ref>, employs a modification of a Transformer decoder block <ref type="bibr" target="#b101">[103]</ref> followed by a dynamic mask prediction head. Our modified decoder block uses prompt self-attention and cross-attention in two directions (prompt-to-image embedding and vice-versa) to update all embeddings. After running two blocks, we upsample the image embedding and an MLP maps the output token to a dynamic linear classifier, which then computes the mask foreground probability at each image location.</p><p>Resolving ambiguity. With one output, the model will average multiple valid masks if given an ambiguous prompt. To address this, we modify the model to predict multiple output masks for a single prompt (see Fig. <ref type="figure" target="#fig_3">3</ref>). We found 3 mask outputs is sufficient to address most common cases (nested masks are often at most three deep: whole, part, and subpart). During training, we backprop only the minimum loss <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b63">64]</ref> over masks. To rank masks, the model predicts a confidence score (i.e., estimated IoU) for each mask.</p><p>Efficiency. The overall model design is largely motivated by efficiency. Given a precomputed image embedding, the prompt encoder and mask decoder run in a web browser, on CPU, in ?50ms. This runtime performance enables seamless, real-time interactive prompting of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Losses and training.</head><p>We supervise mask prediction with the linear combination of focal loss <ref type="bibr" target="#b64">[65]</ref> and dice loss <ref type="bibr" target="#b72">[73]</ref> used in <ref type="bibr" target="#b13">[14]</ref>. We train for the promptable segmentation task using a mixture of geometric prompts (for text prompts see ?7.5). Following <ref type="bibr" target="#b91">[92,</ref><ref type="bibr" target="#b36">37]</ref>, we simulate an interactive setup by randomly sampling prompts in 11 rounds per mask, allowing SAM to integrate seamlessly into our data engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Segment Anything Data Engine</head><p>As segmentation masks are not abundant on the internet, we built a data engine to enable the collection of our 1.1B mask dataset, SA-1B. The data engine has three stages: (1) a model-assisted manual annotation stage, (2) a semi-automatic stage with a mix of automatically predicted masks and model-assisted annotation, and (3) a fully automatic stage in which our model generates masks without annotator input. We go into details of each next.</p><p>Assisted-manual stage. In the first stage, resembling classic interactive segmentation, a team of professional annotators labeled masks by clicking foreground / background object points using a browser-based interactive segmentation tool powered by SAM. Masks could be refined using pixelprecise "brush" and "eraser" tools. Our model-assisted annotation runs in real-time directly inside a browser (using precomputed image embeddings) enabling a truly interactive experience. We did not impose semantic constraints for labeling objects, and annotators freely labeled both "stuff" and "things" <ref type="bibr" target="#b0">[1]</ref>. We suggested annotators label objects they could name or describe, but did not collect these names or descriptions. Annotators were asked to label objects in order of prominence and were encouraged to proceed to the next image once a mask took over 30 seconds to annotate.</p><p>At the start of this stage, SAM was trained using common public segmentation datasets. After sufficient data annotation, SAM was retrained using only newly annotated masks. As more masks were collected, the image encoder was scaled from ViT-B to ViT-H and other architectural details evolved; in total we retrained our model 6 times. Average annotation time per mask decreased from 34 to 14 seconds as the model improved. We note that 14 seconds is 6.5? faster than mask annotation for COCO <ref type="bibr" target="#b65">[66]</ref> and only 2? slower than bounding-box labeling with extreme points <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b70">71]</ref>. As SAM improved, the average number of masks per image increased from 20 to 44 masks. Overall, we collected 4.3M masks from 120k images in this stage.</p><p>Semi-automatic stage. In this stage, we aimed to increase the diversity of masks in order to improve our model's ability to segment anything. To focus annotators on less prominent objects, we first automatically detected confident masks. Then we presented annotators with images prefilled with these masks and asked them to annotate any additional unannotated objects. To detect confident masks, we trained a bounding box detector <ref type="bibr" target="#b83">[84]</ref> on all first stage masks using a generic "object" category. During this stage we collected an additional 5.9M masks in 180k images (for a total of 10.2M masks). As in the first stage, we periodically retrained our model on newly collected data (5 times). Average annotation time per mask went back up to 34 seconds (excluding the automatic masks) as these objects were more challenging to label. The average number of masks per image went from 44 to 72 masks (including the automatic masks).</p><p>Fully automatic stage. In the final stage, annotation was fully automatic. This was feasible due to two major enhancements to our model. First, at the start of this stage, we had collected enough masks to greatly improve the model, including the diverse masks from the previous stage. Second, by this stage we had developed the ambiguity-aware model, which allowed us to predict valid masks even in ambiguous cases. Specifically, we prompted the model with a 32?32 regular grid of points and for each point predicted a set of masks that may correspond to valid objects. With the ambiguity-aware model, if a point lies on a part or subpart, our model will return the subpart, part, and whole object. The IoU prediction module of our model is used to select confident masks; moreover, we identified and selected only stable masks (we consider a mask stable if thresholding the probability map at 0.5 -? and 0.5 + ? results in similar masks). Finally, after selecting the confident and stable masks, we applied non-maximal suppression (NMS) to filter duplicates. To further improve the quality of smaller masks, we also processed multiple overlapping zoomed-in image crops. For further details of this stage, see ?B. We applied fully automatic mask generation to all 11M images in our dataset, producing a total of 1.1B high-quality masks. We describe and analyze the resulting dataset, SA-1B, next. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Segment Anything Dataset</head><p>Our dataset, SA-1B, consists of 11M diverse, highresolution, licensed, and privacy protecting images and 1.1B high-quality segmentation masks collected with our data engine. We compare SA-1B with existing datasets and analyze mask quality and properties. We are releasing SA-1B to aid future development of foundation models for computer vision. We note that SA-1B will be released under a favorable license agreement for certain research uses and with protections for researchers.</p><p>Images. We licensed a new set of 11M images from a provider that works directly with photographers. These images are high resolution (3300?4950 pixels on average), and the resulting data size can present accessibility and storage challenges. Therefore, we are releasing downsampled images with their shortest side set to 1500 pixels. Even after downsampling, our images are significantly higher resolution than many existing vision datasets (e.g., COCO <ref type="bibr" target="#b65">[66]</ref> images are ?480?640 pixels). Note that most models today operate on much lower resolution inputs. Faces and vehicle license plates have been blurred in the released images.</p><p>Masks. Our data engine produced 1.1B masks, 99.1% of which were generated fully automatically. Therefore, the quality of the automatic masks is centrally important. We compare them directly to professional annotations and look at how various mask properties compare to prominent segmentation datasets. Our main conclusion, as borne out in the analysis below and the experiments in ?7, is that our automatic masks are high quality and effective for training models. Motivated by these findings, SA-1B only includes automatically generated masks.</p><p>Mask quality. To estimate mask quality, we randomly sampled 500 images (?50k masks) and asked our professional annotators to improve the quality of all masks in these images. Annotators did so using our model and pixel-precise "brush" and "eraser" editing tools. This procedure resulted in pairs of automatically predicted and professionally corrected masks. We computed IoU between each pair and found that 94% of pairs have greater than 90% IoU (and 97% of pairs have greater than 75% IoU). For comparison, prior work estimates inter-annotator consistency at 85-91% IoU <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b59">60]</ref>. Our experiments in ?7 confirm by human ratings that mask quality is high relative to a variety of datasets and that training our model on automatic masks is nearly as good as using all masks produced by the data engine.  Mask properties. In Fig. <ref type="figure" target="#fig_5">5</ref> we plot the spatial distribution of object centers in SA-1B compared to the largest existing segmentation datasets. Common photographer biases are present in all datasets. We observe that SA-1B has greater coverage of image corners compared to LVIS v1 <ref type="bibr" target="#b43">[44]</ref> and ADE20K <ref type="bibr" target="#b115">[117]</ref>, the two most similarly distributed datasets, while COCO <ref type="bibr" target="#b65">[66]</ref> and Open Images V5 <ref type="bibr" target="#b59">[60]</ref> have a more prominent center bias. In Fig. <ref type="figure">6</ref> (legend) we compare these datasets by size. SA-1B has 11? more images and 400? more masks than the second largest, Open Images. On average, it has 36? more masks per image than Open Images. The closest dataset in this respect, ADE20K, still has 3.5? fewer masks per image. Fig. <ref type="figure">6</ref> (left) plots the masks-perimage distribution. Next, we look at image-relative mask size (square root of the mask area divided by image area) in Fig. <ref type="figure">6</ref> (middle). As expected, since our dataset has more masks per image, it also tends to include a greater percentage of small and medium relative-size masks. Finally, to analyze shape complexity, we look at mask concavity (1 minus mask area divided by area of mask's convex hull) in Fig. <ref type="figure">6</ref> (right). Since shape complexity is correlated with mask size, we control for the datasets' mask size distributions by first performing stratified sampling from binned mask sizes. We observe that the concavity distribution of our masks is broadly similar to that of other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Segment Anything RAI Analysis</head><p>We next perform a Responsible AI (RAI) analysis of our work by investigating potential fairness concerns and biases when using SA-1B and SAM. We focus on the geographic and income distribution of SA-1B and fairness of SAM across protected attributes of people. We also provide dataset, data annotation, and model cards in ?F. Geographic and income representation. We infer the country images were photographed in using standard methods (see ?C). In Fig. <ref type="figure" target="#fig_6">7</ref> we visualize the per-country image counts in SA-1B (left) and the 50 countries with the most images (right). We note that the top-three countries are from different parts of the world. Next, in Table <ref type="table">1</ref> we compare the geographic and income representation of SA-1B, COCO <ref type="bibr" target="#b65">[66]</ref>, and Open Images <ref type="bibr" target="#b59">[60]</ref> Fairness in segmenting people. We investigate potential fairness concerns across perceived gender presentation, perceived age group, and perceived skin tone by measuring the performance discrepancy of SAM between groups. We use the More Inclusive Annotations for People (MIAP) <ref type="bibr" target="#b86">[87]</ref> dataset for gender presentation and age and a proprietary dataset for skin tone (see ?C). Our evaluation uses simulated interactive segmentation with random sampling of 1 and 3 points (see ?D). Table <ref type="table">2</ref> (top left) shows results for perceived gender presentation. We note that females have been shown to be underrepresented in detection and segmentation datasets <ref type="bibr" target="#b113">[115]</ref>, but observe that SAM performs similarly across groups. We repeat the analysis for perceived age in Table <ref type="table">2</ref> (bottom left), noting that those who are perceived to be younger and older have been shown to be underrepresented in large-scale datasets <ref type="bibr" target="#b108">[110]</ref>. SAM performs best on those who are perceived older (although the confidence interval is large). Finally, we repeat the analysis for perceived skin tone in Table <ref type="table">2</ref> (right), noting that those with lighter apparent skin tones have been shown to be overrepresented and those with darker skin tones underrepresented in large-scale datasets <ref type="bibr" target="#b108">[110]</ref>. As MIAP does not contain perceived skin tone annotations, we use a proprietary dataset that contains annotations for the perceived Fitzpatrick skin type <ref type="bibr" target="#b35">[36]</ref>, which ranges from 1 (lightest skin tone) to 6 (darkest skin tone). While the means vary somewhat, we do not find a significant difference across groups. We believe our findings stem from the nature of the task, and acknowledge biases may arise when SAM is used as a component in larger systems. Finally, in ?C we extend the analysis to segmenting clothing where we find an indication of bias across perceived gender presentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Zero-Shot Transfer Experiments</head><p>In this section, we present zero-shot transfer experiments with SAM, the Segment Anything Model. We consider five tasks, four of which differ significantly from the promptable segmentation task used to train SAM. These experiments evaluate SAM on datasets and tasks that were not seen dur-ing training (our usage of "zero-shot transfer" follows its usage in CLIP <ref type="bibr" target="#b81">[82]</ref>). The datasets may include novel image distributions, such as underwater or ego-centric images (e.g. Fig. <ref type="figure">8</ref>) that, to our knowledge, do not appear in SA-1B.</p><p>Our experiments begin by testing the core goal of promptable segmentation: producing a valid mask from any prompt. We emphasize the challenging scenario of a single foreground point prompt, since it is more likely to be ambiguous than other more specific prompts. Next, we present a sequence of experiments that traverse low, mid, and highlevel image understanding and roughly parallel the historical development of the field. Specifically, we prompt SAM to (1) perform edge detection, (2) segment everything, i.e. object proposal generation, (3) segment detected objects, i.e. instance segmentation, and (4), as a proof-of-concept, to segment objects from free-form text. These four tasks differ significantly from the promptable segmentation task that SAM was trained on and are implemented via prompt engineering. Our experiments conclude with an ablation study.</p><p>Implementation. Unless otherwise specified: (1) SAM uses an MAE <ref type="bibr" target="#b46">[47]</ref> pre-trained ViT-H <ref type="bibr" target="#b32">[33]</ref> image encoder and (2) SAM was trained on SA-1B, noting that this dataset includes only automatically generated masks from the final stage of our data engine. For all other model and training details, such as hyperparameters, refer to ?A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Zero-Shot Single Point Valid Mask Evaluation</head><p>Task. We evaluate segmenting an object from a single foreground point. This task is ill-posed as one point can refer to multiple objects. Ground truth masks in most datasets do not enumerate all possible masks, which can make automatic metrics unreliable. Therefore, we supplement the standard mIoU metric (i.e., the mean of all IoUs between predicted and ground truth masks) with a human study in which annotators rate mask quality from 1 (nonsense) to 10 (pixel-perfect). See ?D.1, ?E, and ?G for additional details.</p><p>By default, we sample points from the "center" of ground truth masks (at a maximal value of the mask's interior distance transform), following the standard evaluation protocol in interactive segmentation <ref type="bibr" target="#b91">[92]</ref>. Since SAM is capable of predicting multiple masks, we evaluate only the model's most confident mask by default. The baselines are all single-mask methods. We compare mainly to RITM <ref type="bibr" target="#b91">[92]</ref>, a strong interactive segmenter that performs best on our benchmark compared to other strong baselines <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>Datasets. We use a newly compiled suite of 23 datasets with diverse image distributions. Fig. <ref type="figure">8</ref> lists the datasets and shows a sample from each one (see appendix Table <ref type="table">7</ref> for more details). We use all 23 datasets for mIoU evaluation. For the human study, we use the subset listed in Fig. <ref type="figure" target="#fig_8">9b</ref> (due to the resource requirements of such studies). This subset includes both datasets for which SAM outperforms and underperforms RITM according to automatic metrics. ADE20K <ref type="bibr" target="#b115">[117]</ref> BBBC038v1 <ref type="bibr" target="#b11">[12]</ref> Cityscapes <ref type="bibr" target="#b24">[25]</ref> DOORS <ref type="bibr" target="#b79">[80]</ref> DRAM <ref type="bibr" target="#b23">[24]</ref> EgoHOS <ref type="bibr" target="#b111">[113]</ref> GTEA <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b62">63]</ref> Hypersim <ref type="bibr" target="#b85">[86]</ref> IBD <ref type="bibr" target="#b16">[17]</ref> iShape <ref type="bibr" target="#b109">[111]</ref> LVIS <ref type="bibr" target="#b43">[44]</ref> NDD20 <ref type="bibr" target="#b98">[100]</ref> NDISPark <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> OVIS <ref type="bibr" target="#b80">[81]</ref> PPDLS <ref type="bibr" target="#b73">[74]</ref> Plittersdorf <ref type="bibr" target="#b45">[46]</ref> STREETS <ref type="bibr" target="#b90">[91]</ref> TimberSeg <ref type="bibr" target="#b37">[38]</ref> TrashCan <ref type="bibr" target="#b51">[52]</ref> VISOR <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27]</ref> WoodScape <ref type="bibr" target="#b110">[112]</ref> PIDRay <ref type="bibr" target="#b102">[104]</ref> ZeroWaste-f <ref type="bibr" target="#b5">[6]</ref> Figure <ref type="figure">8</ref>: Samples from the 23 diverse segmentation datasets used to evaluate SAM's zero-shot transfer capabilities.</p><p>-20 0 +20 +40 IoU delta at 1 center point GTEA <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b62">63]</ref> TrashCan <ref type="bibr" target="#b51">[52]</ref> DRAM <ref type="bibr" target="#b23">[24]</ref> PIDRay <ref type="bibr" target="#b102">[104]</ref> Cityscapes <ref type="bibr" target="#b24">[25]</ref> WoodScape <ref type="bibr" target="#b110">[112]</ref> IBD <ref type="bibr" target="#b16">[17]</ref> EgoHOS <ref type="bibr" target="#b111">[113]</ref> Plittersdorf <ref type="bibr" target="#b45">[46]</ref> VISOR <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27]</ref> NDISPark <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> Hypersim <ref type="bibr" target="#b85">[86]</ref> OVIS <ref type="bibr" target="#b80">[81]</ref> ADE20K <ref type="bibr" target="#b115">[117]</ref> iShape <ref type="bibr" target="#b109">[111]</ref> ZeroWaste-f <ref type="bibr" target="#b5">[6]</ref> STREETS <ref type="bibr" target="#b90">[91]</ref> LVIS <ref type="bibr" target="#b43">[44]</ref> NDD20 <ref type="bibr" target="#b98">[100]</ref> TimberSeg <ref type="bibr" target="#b37">[38]</ref> DOORS <ref type="bibr" target="#b79">[80]</ref> BBBC038v1 <ref type="bibr" target="#b11">[12]</ref> PPDLS <ref type="bibr" target="#b73">[74]</ref> -21.4  Results. First, we look at automatic evaluation on the full suite of 23 datasets using mIoU. We compare per-dataset results in Fig. <ref type="figure" target="#fig_8">9a</ref> against RITM. SAM yields higher results on 16 of the 23 datasets, by as much as ?47 IoU. We also present an "oracle" result, in which the most relevant of SAM's 3 masks is selected by comparing them to the ground truth, rather than selecting the most confident mask. This reveals the impact of ambiguity on automatic evaluation. In particular, with the oracle to perform ambiguity resolution, SAM outperforms RITM on all datasets.</p><p>Results of the human study are presented in Fig. <ref type="figure" target="#fig_8">9b</ref>. Error bars are 95% confidence intervals for mean mask ratings (all differences are significant; see ?E for details). We observe that the annotators consistently rate the quality of SAM's masks substantially higher than the strongest baseline, RITM. An ablated, "ambiguity-unaware" version of SAM with a single output mask has consistently lower ratings, though still higher than RITM. SAM's mean ratings fall between 7 and 9, which corresponds to the qualitative rating guideline: "A high score (7-9): The object is identifiable and errors are small and rare (e.g., missing a small, heavily obscured disconnected component, ...)." These results indicate that SAM has learned to segment valid masks from a single point. Note that for datasets like DRAM and IBD, where SAM is worse on automatic metrics, it receives consistently higher ratings in the human study.</p><p>Fig. <ref type="figure" target="#fig_8">9c</ref> shows additional baselines, SimpleClick <ref type="bibr" target="#b66">[67]</ref> and FocalClick <ref type="bibr" target="#b17">[18]</ref>, which obtain lower single point performance than RITM and SAM. As the number of points increases from 1 to 9, we observe that the gap between methods decreases. This is expected as the task becomes easier; also, SAM is not optimized for the very high IoU regime. Finally, in Fig. <ref type="figure" target="#fig_8">9d</ref> we replace the default center point sampling with random point sampling. We observe that the gap between SAM and the baselines grows and SAM is able to achieve comparable results under either sampling method.  Table <ref type="table">3</ref>: Zero-shot transfer to edge detection on BSDS500.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Zero-Shot Edge Detection</head><p>Approach. We evaluate SAM on the classic low-level task of edge detection using BSDS500 <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b2">3]</ref>. We use a simplified version of our automatic mask generation pipeline. Specifically, we prompt SAM with a 16?16 regular grid of foreground points resulting in 768 predicted masks (3 per point). Redundant masks are removed by NMS. Then, edge maps are computed using Sobel filtering of unthresholded mask probability maps and standard lightweight postprocessing, including edge NMS (see ?D.2 for details).</p><p>Results. We visualize representative edge maps in Fig. <ref type="figure" target="#fig_9">10</ref> (see Fig. <ref type="figure" target="#fig_16">15</ref> for more). Qualitatively, we observe that even though SAM was not trained for edge detection, it produces reasonable edge maps. Compared to the ground truth, SAM predicts more edges, including sensible ones that are not annotated in BSDS500. This bias is reflected quantitatively in Table <ref type="table">3</ref>: recall at 50% precision (R50) is high, at the cost of precision. SAM naturally lags behind state-of-the-art methods that learn the biases of BSDS500, i.e., which edges to suppress. Nevertheless, SAM performs well compared to pioneering deep learning methods such as HED <ref type="bibr" target="#b106">[108]</ref> (also trained on BSDS500) and significantly better than prior, though admittedly outdated, zero-shot transfer methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Zero-Shot Object Proposals</head><p>Approach. Next, we evaluate SAM on the mid-level task of object proposal generation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b100">102]</ref>. This task has played an important role in object detection research, serving as an mask AR@1000 method all small med. large freq. com. rare ViTDet-H <ref type="bibr" target="#b61">[62]</ref> 63.0 51.7 intermediate step in pioneering systems (e.g., <ref type="bibr" target="#b100">[102,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b83">84]</ref>).</p><p>To generate object proposals, we run a slightly modified version of our automatic mask generation pipeline and output the masks as proposals (see ?D.3 for details).</p><p>We compute the standard average recall (AR) metric on LVIS v1 <ref type="bibr" target="#b43">[44]</ref>. We focus on LVIS because its large number of categories presents a challenging test. We compare to a strong baseline implemented as a ViTDet <ref type="bibr" target="#b61">[62]</ref> detector (with cascade Mask R-CNN <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b10">11]</ref> ViT-H). We note that this "baseline" corresponds to the "Detector Masquerading as Proposal generator" (DMP) method <ref type="bibr" target="#b15">[16]</ref> that was shown to game AR, making it a truly demanding comparison.</p><p>Results. In Table <ref type="table" target="#tab_4">4</ref> we see unsurprisingly that using the detections from ViTDet-H as object proposals (i.e., the DMP method <ref type="bibr" target="#b15">[16]</ref> that games AR) performs the best overall. However, SAM does remarkably well on several metrics. Notably, it outperforms ViTDet-H on medium and large objects, as well as rare and common objects. In fact, SAM only underperforms ViTDet-H on small objects and frequent objects, where ViTDet-H can easily learn LVISspecific annotation biases since it was trained on LVIS, unlike SAM. We also compare against an ablated ambiguityunaware version of SAM ("single out."), which performs significantly worse than SAM on all AR metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Zero-Shot Instance Segmentation</head><p>Approach. Moving to higher-level vision, we use SAM as the segmentation module of an instance segmenter. The implementation is simple: we run a object detector (the ViTDet used before) and prompt SAM with its output boxes. This illustrates composing SAM in a larger system.</p><p>Results. We compare the masks predicted by SAM and ViTDet on COCO and LVIS in Table <ref type="table">5</ref>. Looking at the mask AP metric we observe gaps on both datasets, where SAM is reasonably close, though certainly behind ViTDet. By visualizing outputs, we observed that SAM masks are often qualitatively better than those of ViTDet, with crisper boundaries (see ?D.4 and Fig. <ref type="figure" target="#fig_17">16</ref>). To investigate this observation, we conducted an additional human study asking annotators to rate the ViTDet masks and SAM masks on the 1 to 10 quality scale used before. In Fig. <ref type="figure" target="#fig_12">11</ref> we observe that SAM consistently outperforms ViTDet in the human study. Table <ref type="table">5</ref>: Instance segmentation results. SAM is prompted with ViTDet boxes to do zero-shot segmentation. The fullysupervised ViTDet outperforms SAM, but the gap shrinks on the higher-quality LVIS masks. Interestingly, SAM outperforms ViTDet according to human ratings (see Fig. <ref type="figure" target="#fig_12">11</ref>).   <ref type="table">5</ref>), SAM has higher ratings than ViTDet, suggesting that ViTDet exploits biases in the COCO and LVIS training data.</p><p>We hypothesize that on COCO, where the mask AP gap is larger and the ground truth quality is relatively low (as borne out by the human study), ViTDet learns the specific biases of COCO masks. SAM, being a zero-shot method, is unable to exploit these (generally undesirable) biases. The LVIS dataset has higher quality ground truth, but there are still specific idiosyncrasies (e.g., masks do not contain holes, they are simple polygons by construction) and biases for modal vs. amodal masks. Again, SAM is not trained to learn these biases, while ViTDet can exploit them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.">Zero-Shot Text-to-Mask</head><p>Approach. Finally, we consider an even higher-level task: segmenting objects from free-form text. This experiment is a proof-of-concept of SAM's ability to process text prompts. While we used the exact same SAM in all prior experiments, for this one SAM's training procedure is modified to make it text-aware, but in a way that does not require new text annotations. Specifically, for each manually collected mask with area larger than 100 2 we extract the CLIP image embedding. Then, during training, we prompt SAM with the extracted CLIP image embeddings as its first interaction. The key observation here is that because CLIP's image embeddings are trained to align with its text embeddings, we can train with image embeddings, but use text embeddings for inference. That is, at inference time we run text through CLIP's text encoder and then give the resulting text embedding as a prompt to SAM (see ?D.5 for details).</p><p>"a wheel" "beaver tooth grille" "a wiper" "a wiper" + point "wipers" "wipers" + point Results. We show qualitative results in Fig. <ref type="figure" target="#fig_13">12</ref>. SAM can segment objects based on simple text prompts like "a wheel" as well as phrases like "beaver tooth grille". When SAM fails to pick the right object from a text prompt only, an additional point often fixes the prediction, similar to <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6.">Ablations</head><p>We perform several ablations on our 23 dataset suite with the single center point prompt protocol. Recall that a single point may be ambiguous and that ambiguity may not be represented in the ground truth, which contains only a single mask per point. Since SAM is operating in a zeroshot transfer setting there can be systematic biases between SAM's top-ranked mask vs. the masks resulting from data annotation guidelines. We therefore additionally report the best mask with respect to the ground truth ("oracle").</p><p>Fig. <ref type="figure" target="#fig_14">13</ref> (left) plots SAM's performance when trained on cumulative data from the data engine stages. We observe that each stage increases mIoU. When training with all three stages, the automatic masks vastly outnumber the manual and semi-automatic masks. To address this, we found that oversampling the manual and semi-automatic masks during training by 10? gave best results. This setup complicates training. We therefore tested a fourth setup that uses only the automatically generated masks. With this data, SAM performs only marginally lower than using all data (?0.5 mIoU). Therefore, by default we use only the automatically generated masks to simplify the training setup.</p><p>In Fig. <ref type="figure" target="#fig_14">13</ref> (middle) we look at the impact of data volume. The full SA-1B contains 11M images, which we uniformly subsample to 1M and 0.1M for this ablation. At 0.1M images, we observe a large mIoU decline under all settings. However, with 1M images, about 10% of the full dataset, we observe results comparable to using the full dataset. This data regime, which still includes approximately 100M masks, may be a practical setting for many use cases. We train with all 11M images by default, but using 1M images is a reasonable practical setting. (Right) Scaling SAM's image encoder shows meaningful, yet saturating gains. Nevertheless, smaller image encoders may be preferred in certain settings.</p><p>Finally, Fig. <ref type="figure" target="#fig_14">13</ref> (right) shows results with ViT-B, ViT-L, and ViT-H image encoders. ViT-H improves substantially over ViT-B, but has only marginal gains over ViT-L. Further image encoder scaling does not appear fruitful at this time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Discussion</head><p>Foundation models. Pre-trained models have been adapted to downstream tasks since the early days of machine learning <ref type="bibr" target="#b97">[99]</ref>. This paradigm has become increasingly important in recent years with a growing emphasis on scale, and such models have recently been (re-)branded as "foundation models": i.e. models that are "trained on broad data at scale and are adaptable to a wide range of downstream tasks" <ref type="bibr" target="#b7">[8]</ref>. Our work correlates well with this definition, though we note that a foundation model for image segmentation is an inherently limited scope, since it represents an important, yet fractional, subset of computer vision. We also contrast one aspect of our approach with <ref type="bibr" target="#b7">[8]</ref>, which emphasizes the role of self-supervised learning in foundation models. While our model is initialized with a selfsupervised technique (MAE <ref type="bibr" target="#b46">[47]</ref>), the vast majority of its capabilities come from large-scale supervised training. In cases where data engines can scale available annotations, like ours, supervised training provides an effective solution.</p><p>Compositionality. Pre-trained models can power new capabilities even beyond ones imagined at the moment of training. One prominent example is how CLIP <ref type="bibr" target="#b81">[82]</ref> is used as a component in larger systems, such as DALL?E <ref type="bibr" target="#b82">[83]</ref>. Our goal is to make this kind of composition straightforward with SAM. We aim to achieve this by requiring SAM to predict a valid mask for a wide range of segmentation prompts. The effect is to create a reliable interface between SAM and other components. For example, MCC <ref type="bibr" target="#b104">[106]</ref> can easily use SAM to segment an object of interest and achieve strong generalization to unseen objects for 3D reconstruction from a single RGB-D image. In another example, SAM can be prompted with gaze points detected by a wearable device, enabling new applications. Thanks to SAM's ability to generalize to new domains like ego-centric images, such systems work without need for additional training.</p><p>Limitations. While SAM performs well in general, it is not perfect. It can miss fine structures, hallucinates small disconnected components at times, and does not produce boundaries as crisply as more computationally intensive methods that "zoom-in", e.g. <ref type="bibr" target="#b17">[18]</ref>. In general, we expect dedicated interactive segmentation methods to outperform SAM when many points are provided, e.g. <ref type="bibr" target="#b66">[67]</ref>. Unlike these methods, SAM is designed for generality and breadth of use rather than high IoU interactive segmentation. Moreover, SAM can process prompts in real-time, but nevertheless SAM's overall performance is not real-time when using a heavy image encoder. Our foray into the text-to-mask task is exploratory and not entirely robust, although we believe it can be improved with more effort. While SAM can perform many tasks, it is unclear how to design simple prompts that implement semantic and panoptic segmentation. Finally, there are domain-specific tools, such as <ref type="bibr" target="#b6">[7]</ref>, that we expect to outperform SAM in their respective domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion.</head><p>The Segment Anything project is an attempt to lift image segmentation into the era of foundation models. Our principal contributions are a new task (promptable segmentation), model (SAM), and dataset (SA-1B) that make this leap possible. Whether SAM achieves the status of a foundation model remains to be seen by how it is used in the community, but regardless we expect the perspective of this work, the release of over 1B masks, and our promptable segmentation model will help pave the path ahead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Segment Anything Model and Task Details</head><p>Image encoder. In general, the image encoder can be any network that outputs a C?H?W image embedding. Motivated by scalability and access to strong pre-training, we use an MAE <ref type="bibr" target="#b46">[47]</ref> pre-trained Vision Transformer (ViT) <ref type="bibr" target="#b32">[33]</ref> with minimal adaptations to process high resolution inputs, specifically a ViT-H/16 with 14?14 windowed attention and four equally-spaced global attention blocks, following <ref type="bibr" target="#b61">[62]</ref>. The image encoder's output is a 16? downscaled embedding of the input image. Since our runtime goal is to process each prompt in real-time, we can afford a high number of image encoder FLOPs because they are computed only once per image, not per prompt.</p><p>Following standard practices (e.g., <ref type="bibr" target="#b39">[40]</ref>), we use an input resolution of 1024?1024 obtained by rescaling the image and padding the shorter side. The image embedding is therefore 64?64. To reduce the channel dimension, following <ref type="bibr" target="#b61">[62]</ref>, we use a 1?1 convolution to get to 256 channels, followed by a 3?3 convolution also with 256 channels. Each convolution is followed by a layer normalization <ref type="bibr" target="#b3">[4]</ref>.</p><p>Prompt encoder. Sparse prompts are mapped to 256dimensional vectorial embeddings as follows. A point is represented as the sum of a positional encoding <ref type="bibr" target="#b94">[95]</ref> of the point's location and one of two learned embeddings that indicate if the point is either in the foreground or background. A box is represented by an embedding pair: (1) the positional encoding of its top-left corner summed with a learned embedding representing "top-left corner" and (2) the same structure but using a learned embedding indicating "bottomright corner". Finally, to represent free-form text we use the text encoder from CLIP <ref type="bibr" target="#b81">[82]</ref> (any text encoder is possible in general). We focus on geometric prompts for the remainder of this section and discuss text prompts in depth in ?D.5.</p><p>Dense prompts (i.e., masks) have a spatial correspondence with the image. We input masks at a 4? lower resolution than the input image, then downscale an additional 4? using two 2?2, stride-2 convolutions with output channels 4 and 16, respectively. A final 1?1 convolution maps the channel dimension to 256. Each layer is separated by GELU activations <ref type="bibr" target="#b49">[50]</ref>   and image embedding are then added element-wise. If there is no mask prompt, a learned embedding representing "no mask" is added to each image embedding location.</p><p>Lightweight mask decoder. This module efficiently maps the image embedding and a set of prompt embeddings to an output mask. To combine these inputs, we take inspiration from Transformer segmentation models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20]</ref> and modify a standard Transformer decoder <ref type="bibr" target="#b101">[103]</ref>. Before applying our decoder, we first insert into the set of prompt embeddings a learned output token embedding that will be used at the decoder's output, analogous to the [class] token in <ref type="bibr" target="#b32">[33]</ref>. For simplicity, we refer to these embeddings (not including the image embedding) collectively as "tokens".</p><p>Our decoder design is shown in Fig. <ref type="figure" target="#fig_15">14</ref>. Each decoder layer performs 4 steps: (1) self-attention on the tokens, (2) cross-attention from tokens (as queries) to the image embedding, (3) a point-wise MLP updates each token, and (4) cross-attention from the image embedding (as queries) to tokens. This last step updates the image embedding with prompt information. During cross-attention, the image embedding is treated as a set of 64 2 256-dimensional vectors. Each self/cross-attention and MLP has a residual connection <ref type="bibr" target="#b48">[49]</ref>, layer normalization, and a dropout <ref type="bibr" target="#b92">[93]</ref> of 0.1 at training. The next decoder layer takes the updated tokens and the updated image embedding from the previous layer. We use a two-layer decoder.</p><p>To ensure the decoder has access to critical geometric information the positional encodings are added to the image embedding whenever they participate in an attention layer. Additionally, the entire original prompt tokens (including their positional encodings) are re-added to the updated tokens whenever they participate in an attention layer. This allows for a strong dependence on both the prompt token's geometric location and type.</p><p>After running the decoder, we upsample the updated image embedding by 4? with two transposed convolutional layers (now it's downscaled 4? relative to the input image). Then, the tokens attend once more to the image embedding and we pass the updated output token embedding to a small 3-layer MLP that outputs a vector matching the channel dimension of the upscaled image embedding. Finally, we predict a mask with a spatially point-wise product between the upscaled image embedding and the MLP's output.</p><p>The transformer uses an embedding dimension of 256. The transformer MLP blocks have a large internal dimension of 2048, but the MLP is applied only to the prompt tokens for which there are relatively few (rarely greater than 20). However, in cross-attention layers where we have a 64?64 image embedding, we reduce the channel dimension of the queries, keys, and values by 2? to 128 for computational efficiency. All attention layers use 8 heads.</p><p>The transposed convolutions used to upscale the output image embedding are 2?2, stride 2 with output channel dimensions of 64 and 32 and have GELU activations. They are separated by layer normalization.</p><p>Making the model ambiguity-aware. As described, a single input prompt may be ambiguous in the sense that it corresponds to multiple valid masks, and the model will learn to average over these masks. We eliminate this problem with a simple modification: instead of predicting a single mask, we use a small number of output tokens and predict multiple masks simultaneously. By default we predict three masks, since we observe that three layers (whole, part, and subpart) are often enough to describe nested masks. During training, we compute the loss (described shortly) between the ground truth and each of the predicted masks, but only backpropagate from the lowest loss. This is a common technique used for models with multiple outputs <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b63">64]</ref>. For use in applications, we'd like to rank predicted masks, so we add a small head (operating on an additional output token) that estimates the IoU between each predicted mask and the object it covers.</p><p>Ambiguity is much rarer with multiple prompts and the three output masks will usually become similar. To minimize computation of degenerate losses at training and ensure the single unambiguous mask receives a regular gradient signal, we only predict a single mask when more than one prompt is given. This is accomplished by adding a fourth output token for an additional mask prediction. This fourth mask is never returned for a single prompt and is the only mask returned for multiple prompts.</p><p>Losses. We supervise mask prediction with a linear combination of focal loss <ref type="bibr" target="#b64">[65]</ref> and dice loss <ref type="bibr" target="#b72">[73]</ref> in a 20:1 ratio of focal loss to dice loss, following <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b13">14]</ref>. Unlike <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b13">14]</ref>, we observe that auxiliary deep supervision after each decoder layer is unhelpful. The IoU prediction head is trained with mean-square-error loss between the IoU prediction and the predicted mask's IoU with the ground truth mask. It is added to the mask loss with a constant scaling factor of 1.0.</p><p>Training algorithm. Following recent approaches <ref type="bibr" target="#b91">[92,</ref><ref type="bibr" target="#b36">37]</ref>, we simulate an interactive segmentation setup during training. First, with equal probability either a foreground point or bounding box is selected randomly for the target mask. Points are sampled uniformly from the ground truth mask. Boxes are taken as the ground truth mask's bounding box, with random noise added in each coordinate with standard deviation equal to 10% of the box sidelength, to a maximum of 20 pixels. This noise profile is a reasonable compromise between applications like instance segmentation, which produce a tight box around the target object, and interactive segmentation, where a user may draw a loose box.</p><p>After making a prediction from this first prompt, subsequent points are selected uniformly from the error region between the previous mask prediction and the ground truth mask. Each new point is foreground or background if the error region is a false negative or false positive, respectively. We also supply the mask prediction from the previous iteration as an additional prompt to our model. To provide the next iteration with maximal information, we supply the unthresholded mask logits instead of the binarized mask. When multiple masks are returned, the mask passed to the next iteration and used to sample the next point is the one with the highest predicted IoU.</p><p>We find diminishing returns after 8 iteratively sampled points (we have tested up to 16). Additionally, to encourage the model to benefit from the supplied mask, we also use two more iterations where no additional points are sampled. One of these iterations is randomly inserted among the 8 iteratively sampled points, and the other is always at the end. This gives 11 total iterations: one sampled initial input prompt, 8 iteratively sampled points, and two iterations where no new external information is supplied to the model so it can learn to refine its own mask predictions. We note that using a relatively large number of iterations is possible because our lightweight mask decoder requires less than 1% of the image encoder's compute and, therefore, each iteration adds only a small overhead. This is unlike previous interactive methods that perform only one or a few interactive steps per optimizer update <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b91">92]</ref>.</p><p>Training recipe. We use the AdamW <ref type="bibr" target="#b67">[68]</ref> optimizer (? 1 = 0.9, ? 2 = 0.999) and a linear learning rate warmup <ref type="bibr" target="#b41">[42]</ref> for 250 iterations and a step-wise learning rate decay schedule. The initial learning rate (lr), after warmup, is 8e -4 . We train for 90k iterations (?2 SA-1B epochs) and decrease the lr by a factor of 10 at 60k iterations and again at 86666 iterations. The batch size is 256 images. To regularize SAM, we set weight decay (wd) to 0.1 and apply drop path <ref type="bibr" target="#b52">[53]</ref> (dp) with a rate of 0.4. We use a layer-wise learning rate decay <ref type="bibr" target="#b4">[5]</ref> (ld) of 0.8. No data augmentation is applied. We initialize SAM from an MAE <ref type="bibr" target="#b46">[47]</ref> pre-trained ViT-H. We distribute training across 256 GPUs, due to the large image encoder and 1024?1024 input size. To limit GPU mem-ory usage, we train with up to 64 randomly sampled masks per GPU. Additionally, we find that lightly filtering SA-1B masks to discard any that cover more than 90% of the image qualitatively improves results.</p><p>For ablations and others variations on training (e.g., textto-mask ?D.5), we deviate from the default recipe above as follows. When training with data from the first and second data engine stages only, we augment the input with large-scale jitter <ref type="bibr" target="#b39">[40]</ref> with a scale range of [0.1, 2.0]. Intuitively, data augmentation may be helpful when training data is more limited. To train ViT-B and ViT-L, we use 180k iterations with batch size 128 distributed across 128 GPUs. We set lr = 8e -4 /4e -4 , ld = 0.6/0.8, wd = 0.1, and dp = 0.6/0.4 for ViT-B/L, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Automatic Mask Generation Details</head><p>Here we discuss details of the data engine's fully automatic stage that was used to generate the released SA-1B.</p><p>Cropping. Masks were generated from a regular grid of 32?32 points on the full image and 20 additional zoomedin image crops arising from 2?2 and 4?4 partially overlapping windows using 16?16 and 8?8 regular point grids, respectively. The original high-resolution images were used for cropping (this was the only time we used them). We removed masks that touch the inner boundaries of the crops. We applied standard greedy box-based NMS (boxes were used for efficiency) in two phases: first within each crop and second across crops. When applying NMS within a crop, we used the model's predicted IoU to rank masks. When applying NMS across crops, we ranked masks from most zoomed-in (i.e., from a 4?4 crop) to least zoomed-in (i.e., the original image), based on their source crop. In both cases, we used an NMS threshold of 0.7.</p><p>Filtering. We used three filters to increase mask quality. First, to keep only confident masks we filtered by the model's predicted IoU score at a threshold of 88.0. Second, to keep only stable masks we compared two binary masks resulting from the same underlying soft mask by thresholding it at different values. We kept the prediction (i.e., the binary mask resulting from thresholding logits at 0) only if the IoU between its pair of -1 and +1 thresholded masks was equal to or greater than 95.0. Third, we noticed that occasionally an automatic mask would cover the entire image. These masks were generally uninteresting, and we filtered them by removing masks that covered 95% or more of an image. All filtering thresholds were selected to achieve both a large number of masks and high mask quality as judged by professional annotators using the method described in ?5.</p><p>Postprocessing. We observed two error types that are easily mitigated with postprocessing. First, an estimated 4% of masks include small, spurious components. To address these, we removed connected components with area less than 100 pixels (including removing entire masks if the largest component is below this threshold). Second, another estimated 4% of masks include small, spurious holes. To address these, we filled holes with area less than 100 pixels. Holes were identified as components of inverted masks.</p><p>Automatic mask generation model. We trained a special version of SAM for fully automatic mask generation that sacrifices some inference speed for improved mask generation properties. We note the differences between our default SAM and the one used for data generation here: it was trained on manual and semi-automatic data only, it was trained for longer (177656 iterations instead of 90k) with large-scale jitter data augmentation <ref type="bibr" target="#b39">[40]</ref>, simulated interactive training used only point and mask prompts (no boxes) and sampled only 4 points per mask during training (reducing from our default of 9 to 4 sped up training iterations and had no impact on 1-point performance, though it would harm mIoU if evaluating with more points), and finally the mask decoder used 3 layers instead of 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SA-1B examples.</head><p>We show SA-1B samples in Fig. <ref type="figure">2</ref>. For more examples, please see our dataset explorer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. RAI Additional Details</head><p>Inferring geographic information for SA-1B. While the images in SA-1B are not geo-tagged, each image has a caption describing its contents and where it was taken. We infer approximate image geo-locations from these captions using an Elmo-based named entity recognition model <ref type="bibr" target="#b77">[78]</ref>. Each extracted location entity is mapped to every matching country, province, and city. Captions are mapped to a single country by first considering the matching countries, then provinces, and finally cities. We note that there are ambiguities and potential for biases with this method (e.g., "Georgia" may refer to the country or the US state). As such, we use the extracted locations to analyze the dataset as a whole, but do not release the inferred locations. The captions will not be released publicly as required by the image provider.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inferring geographic information for COCO and Open</head><p>Images. The COCO <ref type="bibr" target="#b65">[66]</ref> and Open Images <ref type="bibr" target="#b59">[60]</ref> datasets do not provide geo-locations. Following <ref type="bibr" target="#b28">[29]</ref>, we retrieve geographic metadata using the Flickr API. We retrieved locations for 24% of the COCO training set (19,562 images) and for Open Images we retrieved 18% of the training set (493,517 images, after only considering images with masks). We note that the geographic information is approximate, and the sample of images with this information may not fully match the full dataset distribution.</p><p>Inferring income information. We use each image's inferred country to look up its income level using the levels defined by The World Bank <ref type="bibr" target="#b96">[98]</ref>. We collapse the uppermiddle and lower-middle levels into a single middle level. Fairness in segmenting people. To investigate SAM's fairness at segmenting people we use the More Inclusive Annotations for People (MIAP) <ref type="bibr" target="#b86">[87]</ref> test set annotations for Open Images <ref type="bibr" target="#b59">[60]</ref>, which allows us to compare SAM's performance across perceived gender presentation and perceived age group. MIAP provides box annotations, while we need ground truth masks for this analysis. To get ground truth masks, we select each person-category mask from Open Images if its corresponding bounding box is within a 1% margin (based on relative box side lengths) of an annotated bounding box in MIAP, resulting in 3.9k masks.</p><p>Fairness in segmenting clothing. We extend our analysis from ?6 to clothing segmentation. We look at SAM's performance on clothing relative to the attributes of those wearing the clothes. We use all 6.5k ground truth masks from Open Images that have a category under the clothing superclass and reside within a person box from MIAP. In Table <ref type="table" target="#tab_6">6</ref> we compare performance across perceived gender presentation and age group. We find that SAM is better at segmenting clothing on those who present predominantly masculine, with disjoint 95% confidence intervals. The gap closes when moving from 1 to 3 point evaluation. Differences for perceived age group are not significant. Our results indicate there is a bias when segmenting clothing across perceived gender presentation with a one point prompt, and we encourage users of SAM to be mindful of this limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiment Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Zero-Shot Single Point Valid Mask Evaluation</head><p>Datasets. We built a new segmentation benchmark to evaluate the zero-shot transfer capabilities of our model using a suite of 23 diverse segmentation datasets from prior work. A description of each dataset is given in Table <ref type="table">7</ref>. For examples, see main text Fig. <ref type="figure">8</ref>. This suite covers a range of domains including egocentric <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b111">113]</ref>, microscopy <ref type="bibr" target="#b11">[12]</ref>, X-ray <ref type="bibr" target="#b102">[104]</ref>, underwater <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b98">100]</ref>, aerial <ref type="bibr" target="#b16">[17]</ref>, simulation <ref type="bibr" target="#b85">[86]</ref>, driving <ref type="bibr" target="#b24">[25]</ref>, and painting <ref type="bibr" target="#b23">[24]</ref> images. For efficient evaluation we subsampled datasets with more than 15k masks. Specifically, we randomly picked images so that the total number of masks in the sampled images was ?10k. We blurred faces of people in all the datasets. Point sampling. Our default point sampling follows standard practice in interactive segmentation <ref type="bibr" target="#b107">[109,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b91">92]</ref>. The first point is chosen deterministically as the point farthest from the object boundary. Each subsequent point is the farthest from the boundary of the error region between ground truth and the previous prediction. Some experiments (where specified) use a more challenging sampling strategy in which the first point is a random point, rather than a deterministically selected "center" point. Each subsequent point is selected as described above. This setting better reflects use cases in which the first point is not reliably near the center of the mask, such as prompting from eye gaze.</p><p>Evaluation. We measure IoU between a prediction after N point prompts and a ground truth mask, where N = {1, 2, 3, 5, 9} and points are sampled iteratively with either of the strategies described above. The per-dataset mIoU is the per-mask IoU averaged across all objects in the dataset. Finally, we report the top-line metric by averaging the perdataset mIoUs across all 23 datasets. Our evaluation differs from the standard interactive segmentation evaluation protocol which measures the average number of points needed to achieve X% IoU, with up to 20 points. We focus on predictions after just one, or possibly a few points, since many of our use cases involve a single or very few prompts. Given our application focus, which requires real-time prompt processing, we expect the best interactive segmentation models to outperform SAM when using a large number of points.</p><p>Baselines. We use three recent strong interactive baselines: RITM <ref type="bibr" target="#b91">[92]</ref>, FocalClick <ref type="bibr" target="#b17">[18]</ref>, and SimpleClick <ref type="bibr" target="#b66">[67]</ref>. For each, we use the largest models trained on the broadest datasets publicly released by the authors. For RITM, we use HRNet32 IT-M trained on the combination of COCO <ref type="bibr" target="#b65">[66]</ref> and LVIS <ref type="bibr" target="#b43">[44]</ref> introduced by the authors. For FocalClick, we use SegFormerB3-S2 trained on a "combined dataset" that includes 8 different segmentation datasets <ref type="bibr" target="#b17">[18]</ref>. For SimpleClick, we use ViT-H448 trained on a combination of COCO and LVIS. We follow the suggested default strategies for data pre-processing (i.e., data augmentations or image resizing) and do not change or adapt any parameters for our evaluation. In our experiments, we observe that RITM outperforms other baselines on our 23 dataset suite with 1 point evaluation. Therefore, we use RITM as the default baseline. When evaluating with more points we report results for all baselines.</p><p>Single point ambiguity and oracle evaluation. In addition to IoU after N points prompts, we report SAM's "oracle" performance at 1 point by evaluating the predicted mask that best matches ground truth from amongst SAM's three predictions (rather than using the one that SAM itself ranks first, as we do by default). This protocol addresses possible single point prompt ambiguity by relaxing the requirement to guess the one right mask among several valid objects. Table <ref type="table">7</ref>: Segmentation datasets used to evaluate zero-shot segmentation with point prompts. The 23 datasets cover a broad range of domains; see column "image type". To make our evaluation efficient, we subsample datasets that have more than 15k masks. Specifically, we randomly sampled images so that the total number of masks in the images is ?10k. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Zero-Shot Edge Detection</head><p>Dataset and metrics. We perform zero-shot edge detection experiments on BSDS500 <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b2">3]</ref>. The ground truth for each image comes from the manual annotations of five different subjects. We report results on the 200 image test subset using the four standard metrics for edge detection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32]</ref>: optimal dataset scale (ODS), optimal image scale (OIS), average precision (AP), and recall at 50% precision (R50).</p><p>Method. For zero-shot transfer, we use a simplified version of our automatic mask generation pipeline. We prompt SAM with a 16?16 regular grid of foreground points, which yields 768 predicted masks (three per point). We do not filter by predicted IoU or stability. Redundant masks are removed by NMS. Then we apply a Sobel filter to the remaining masks' unthresholded probability maps and set values to zero if they do not intersect with the outer boundary pixels of a mask. Finally, we take a pixel-wise max over all the predictions, linearly normalize the result to [0,1], and apply edge NMS <ref type="bibr" target="#b12">[13]</ref> to thin the edges.</p><p>Visualizations. In Fig. <ref type="figure" target="#fig_16">15</ref>, we show additional examples of zero-shot edge predictions from SAM. These qualitative examples further illustrate how SAM tends to output sensible edge maps, despite not being trained for edge detection.</p><p>We see that the edges can align well with the human annotations. Although, as previously mentioned, since SAM is not trained for edge detection it does not learn the biases of the BSDS500 dataset and often outputs more edges than are present in the ground truth annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Zero-Shot Object Proposals</head><p>Dataset and metrics. We report the standard average recall (AR) metric for masks at 1000 proposals on the LVIS v1 validation set <ref type="bibr" target="#b43">[44]</ref>. Since LVIS has high-quality masks for 1203 object classes, it provides a challenging test for object proposal generation. We focus on AR@1000 due to the open-world nature of our model, which will likely produce many valid masks outside even the 1203 classes in LVIS. To measure performance on frequent, common, and rare cate-gories, we use AR@1000 but measured against a ground truth set containing just the corresponding LVIS categories.</p><p>Baseline. We use cascade ViTDet-H as a baseline, the strongest model from <ref type="bibr" target="#b61">[62]</ref> by AP on LVIS. As noted in the main text, an object detector trained in-domain can "game" AR <ref type="bibr" target="#b15">[16]</ref> and is expected to be a stronger baseline than other models that focus on open-world proposals or segmentation <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b103">105]</ref>. To produce 1000 proposals, we disable score thresholding in the three cascade stages and as raise the maximum number of predictions per stage to 1000.</p><p>Method. We use a modified version of SAM's automatic mask generation pipeline for zero-shot transfer. First, to make inference time comparable to that of ViTDet we do not process image crops. Second, we remove filtering by predicted IoU and stability. This leaves two tunable parameters to get ?1000 masks per image: the input point grid and the NMS threshold duplicate mask suppression. We choose a 64?64 point grid and an NMS threshold of 0.9, which produces ?900 masks per image on average. At evaluation, if greater than 1000 masks have been proposed in an image, they are ranked by the average of their confidence and stability scores, then truncated to the top 1000 proposals.</p><p>We hypothesize that SAM's ability to output multiple masks is especially valuable for this task, since recall should benefit from proposals generated at multiple scales from a single input point. To test this, we compare to an ablated version SAM that only outputs a single mask instead of three (SAM -single-output). Since this model produces fewer masks, we further increase the number of points sampled and NMS threshold to 128?128 and 0.95, respectively, obtaining ?950 masks per image on average. Additionally, single-output SAM does not produce the IoU score used to rank masks for NMS in the automatic mask generation pipeline, so instead masks are ranked randomly. Testing suggests this has similar performance to more sophisticated methods of ranking masks, such as using the max logit value of the mask as a proxy for model confidence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. Zero-Shot Instance Segmentation</head><p>Method. For zero-shot instance segmentation, we prompt SAM with the boxes output by a fully-supervised ViTDet-H on COCO and LVIS v1 validation splits. We apply an additional mask refinement iteration by feeding the most confident predicted mask, together with the box prompt, back to the mask decoder to produce the final prediction. We show zero-shot instance segmentations predicted on LVIS in Fig. <ref type="figure" target="#fig_17">16</ref>. Compared to ViTDet, SAM tends to produce higher quality masks with cleaner boundaries. We confirm this observation with human studies in ?7.4. Note that as a zero-shot model, SAM is not able to learn annotation biases in a dataset. For instance, we see that SAM makes a valid modal prediction for the plate, whereas LVIS masks cannot contain holes by design so the plate is annotated amodally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5. Zero-Shot Text-to-Mask</head><p>Model and training. We use the largest publicly available CLIP model <ref type="bibr" target="#b81">[82]</ref> (ViT-L/14@336px) to compute text and image embeddings, which we 2 normalize prior to use.</p><p>To train SAM, we use masks from the first two stages of our data engine. Moreover, we discard all masks with an area smaller than 100 2 pixels. We train this model with largescale jitter <ref type="bibr" target="#b39">[40]</ref> for 120k iterations with batch size 128. All other training parameters follow our default settings.</p><p>Generating training prompts. To extract an input prompt we first expand the bounding box around each mask by a random factor from 1? to 2?, square-crop the expanded box to maintain its aspect ratio, and resize it to 336?336 pixels. Before feeding the crop to the CLIP image encoder, with 50% probability we zero-out pixels outside the mask.</p><p>To ensure the embedding focuses on the object, we use masked attention in the last layer to restrict attention from the output token to the image positions inside the mask. Finally, our prompt is the output token embedding. For training we supply the CLIP-based prompt first, followed by additional iterative point prompts to refine the prediction.</p><p>Figure <ref type="figure" target="#fig_6">17</ref>: Visualization of thresholding the similarities of mask embeddings from SAM's latent space. A query is indicated by the magenta box; top row shows matches at a low threshold, bottom row at a high threshold. The most similar mask embeddings in the same image can often be semantically similar to the query mask embedding, even though SAM is not trained with explicit semantic supervision.</p><p>Inference. During inference we use the CLIP text encoder without any modifications to create a prompt for SAM. We rely on the fact that text and image embeddings are aligned by CLIP, which allows us to train without any explicit text supervision while using text-based prompts for inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.6. Probing the Latent Space of SAM</head><p>Finally, we perform an initial investigation to qualitatively probe the latent space learned by SAM. In particular, we are interested in whether SAM is able to capture any semantics in its representation even though is not trained with explicit semantic supervision. To do so, we compute mask embeddings by extracting an image embedding from SAM from an image crop around a mask and its horizontally flipped version, multiplying the image embedding by the binary mask, and averaging over spatial locations. In Fig. <ref type="figure" target="#fig_6">17</ref>, we show 3 examples of a query mask and similar masks (in the latent space) in the same image. We observe that the nearest neighbors for each query show some, albeit imperfect, shape and semantic similarity. Although these results are preliminary, they indicate that the representations from SAM may be useful for a variety of purposes, such as further data labeling, understanding the contents of datasets, or as features for downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Human Study Experimental Design</head><p>Here we describe details of the human study used to evaluate mask quality in ?7.1 and ?7.4. The purpose of the human study is to address two limitations of using IoU to ground truth as a measure of predicted mask quality. The first limitation is that, for ambiguous inputs such as a single point, the model may be strongly penalized for returning a valid mask of a different object than the ground truth. The second limitation is that ground truth masks may include various biases, such as systematic errors in the edge quality or decisions to modally or amodally segment occluding objects. A model trained in-domain can learn these biases and obtain a higher IoU without necessarily producing better masks. Human review can obtain a measure of mask quality independent of an underlying ground truth mask in order to alleviate these issues.</p><p>Models. For single-point evaluation, we use RITM <ref type="bibr" target="#b91">[92]</ref>, single-output SAM, and SAM to test two hypotheses. First, we hypothesize that SAM produces visually higher quality masks than baseline interactive segmentation models when given a single point, even when metrics such as IoU with ground truth do not reveal this. Second, we hypothesize that SAM's ability to disambiguate masks improves mask quality for single point inputs, since single output SAM may return masks that average over ambiguous masks.</p><p>For instance segmentation experiments, we evaluate cascade ViTDet-H <ref type="bibr" target="#b61">[62]</ref> and SAM in order to test the hypothesis that SAM produces visually higher quality masks, even if it obtains a lower AP due to the inability to learn specific annotation biases of the validation dataset.</p><p>Datasets. For single-point experiments, we select 7 datasets from our set of 23 datasets, since the full suite is too large for human review. We choose LVIS v0.5 <ref type="bibr" target="#b16">[17]</ref>, VISOR <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27]</ref>, DRAM <ref type="bibr" target="#b23">[24]</ref>, IBD <ref type="bibr" target="#b16">[17]</ref>, NDD20 <ref type="bibr" target="#b98">[100]</ref>, OVIS <ref type="bibr" target="#b80">[81]</ref>, and iShape <ref type="bibr" target="#b109">[111]</ref>, which provide a diverse collection of images, including scene-level, ego-centric, drawn, overhead, underwater, and synthetic imagery. Additionally, this set includes datasets both where SAM outperforms RITM with IoU metrics and vice-versa. For instance segmentation experiments, we use the LVIS v1 validation set, allowing for direct comparison to ViTDet, which was trained on LVIS.</p><p>Methodology. We presented masks generated by the models to professional annotators and asked them to rate each mask using provided guidelines (see ?G for the complete guidelines). Annotators were sourced from the same com-pany that collected manually annotated masks for the data engine. An annotator was provided access to an image, the predicted mask of a single model, and the input to the model (either a single point or single box) and asked to judge the mask on three criterion: Does the mask correspond to a valid object? Does the mask have a clean boundary? and Does the mask correspond to the input? They then submitted a rating from 1-10 indicating the overall mask quality.</p><p>A score of 1 indicates a mask that corresponds to no object at all; a low score <ref type="bibr" target="#b1">(2)</ref><ref type="bibr" target="#b2">(3)</ref><ref type="bibr" target="#b3">(4)</ref> indicates that the mask has huge errors, such including huge regions of other objects or having large areas of nonsensical boundaries; a middle score <ref type="bibr" target="#b4">(5)</ref><ref type="bibr" target="#b5">(6)</ref> indicates masks that are mostly sensible but still have significant semantic or boundary errors; a high score <ref type="bibr" target="#b6">(7)</ref><ref type="bibr" target="#b7">(8)</ref><ref type="bibr" target="#b8">(9)</ref> indicates masks with only minor boundary errors; and a score of 10 is for masks with no visible errors. Annotators were provided with five different views, each designed to help identify different error types.</p><p>For single point experiments, 1000 masks per dataset were selected randomly from the same subsets used for benchmarking zero-shot interactive segmentation (see ?D.1 for details on these subsets). The model input was the centermost point, calculated as the largest value of the distance transform from the edge of the mask. For instance segmentation experiments, 1000 masks were selected from the LVIS v1 validation set, and the model input was the LVIS ground truth box. In all experiments, masks with a size smaller than 24 2 pixels were excluded from sampling, to prevent showing raters a mask that was too small to judge accurately. For both memory and display reasons, large images were rescaled to have a max side-length of 2000 before predicting a mask. In all experiments, the same inputs were fed to each model to produce a predicted mask.</p><p>For comparison, the ground truth masks from each dataset were also submitted for rating. For single-point experiments, this gave 4000 total rating jobs per dataset (1000 masks each for RITM, SAM single-output, SAM, and ground truth); for instance segmentation experiments, it gave 3000 total jobs (ViTDet, SAM, and ground truth).</p><p>For each dataset, these jobs were inserted with random ordering into a queue from which 30 annotators drew jobs. In initial testing of the review study, we provided each job to five different annotators and found reasonable consistency in scores: the average standard deviation in score over the five annotators was 0.83. Additionally, the annotation company deployed quality assurance testers who spot checked a fraction of results for extreme departures from the guidelines. Thus for our experiments each job (i.e., rating one mask in one image) was completed by only a single annotator. Average time spent per annotator per job was 90 seconds, longer than our initial target of 30 seconds, but still sufficiently fast to collect a large number of ratings on each of the 7 selected datasets.       Percent of ratings 4.9 ? 0.16, RITM 6.2 ? 0.17, SAM -single output 7.1 ? 0.15, SAM 9.3 ? 0.06, GT (g) iShape <ref type="bibr" target="#b109">[111]</ref> Figure <ref type="figure" target="#fig_1">18</ref>: Mask quality rating distributions by dataset from our human evaluation study.  Results. Fig. <ref type="figure" target="#fig_1">18</ref> shows histograms over ratings for each dataset in the single-point experiments. We run statistical tests for two hypotheses: (1) that SAM gets higher scores than the baseline model (RITM or ViTDet) and (2) that SAM gets higher scores than single-output SAM. P-values are calculated via a paired t-test on the means of the model scores, which we supplement with a paired bootstrap test on 10k samples to find the 99% confidence interval for the difference of means. Table <ref type="table" target="#tab_9">8</ref> shows p-values and confidence intervals for these tests. All statistical tests are strongly significant, and all confidence intervals exclude zero.</p><p>For instance segmentation, Fig. <ref type="figure" target="#fig_12">11</ref> of the main text shows the histogram for ratings. To compare to COCO ground truth, we additionally include 794 ratings of COCO ground truth masks that were collected during our testing of the human review process. These masks were presented to raters using an identical setup as the LVIS results. For fair comparison, results for LVIS in Fig. <ref type="figure" target="#fig_12">11</ref> were subsampled to the same 794 inputs for each model and ground truth. For Table <ref type="table" target="#tab_9">8</ref>, the full 1000 ratings are used to run statistical tests, which show that SAM's mask quality improvement over ViTDet is statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Dataset, Annotation, and Model Cards</head><p>In ?F.1 we provide a Dataset Card for SA-1B, following <ref type="bibr" target="#b38">[39]</ref>, in a list of questions and answers. Next, we provide a Data Annotation Card in ?F.2 for the first two stages of our data engine described in ?4, following CrowdWork-Sheets <ref type="bibr" target="#b29">[30]</ref>, again as a list of questions and answers. We provide a Model Card following <ref type="bibr" target="#b74">[75]</ref>   <ref type="formula">4</ref>) The data is more geographically diverse than its predecessors, and we hope it will bring the community one step closer to creating fairer and more equitable models.</p><p>2. Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)? The dataset was created by the FAIR team of Meta AI. The underlying images were collected and licensed from a third party photo company.</p><p>3. Who funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number.</p><p>Meta AI funded the creation of the dataset.</p><p>4. Any other comments? No.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Composition</head><p>1. What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description. All of the instances in the dataset are photos. The photos vary in subject matter; common themes of the photo include: locations, objects, scenes. All of the photos are distinct, however there are some sets of photos that were taken of the same subject matter.</p><p>2. How many instances are there in total (of each type, if appropriate)? There are 11 million images.</p><p>3. Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable). The dataset is composed of images licensed from a photo provider. The dataset contains all instances licensed. The images are photos, i.e. not artwork, although there are a few exceptions. The dataset includes all generated masks for each image in the dataset. We withheld ?2k randomly selected images for testing purposes.</p><p>4. What data does each instance consist of? "Raw" data (e.g., unprocessed text or images) or features? In either case, please provide a description. Each instance in the dataset is an image. The images were processed to blur faces and license plates to protect the identities of those in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Is there a label or target associated with each instance? If so, please provide a description. Each image is annotated with masks. There are no categories or text associated with the masks. The average image has ?100 masks, and there are ?1.1B masks in total.</p><p>6. Is any information missing from individual instances? If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text. Yes. Each image is accompanied by a short caption that describes the content and place of the photo in a free form text. Per our agreement with the photo provider we are not allowed to release these captions. However, we use them in our paper to analyze the geographical distribution of the dataset.</p><p>7. Are relationships between individual instances made explicit (e.g., users' movie ratings, social network links)? If so, please describe how these relationships are made explicit. No, there are no known relationships between instances in the dataset.</p><p>8. Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description. Errors: The masks are generated by a segmentation model, so there may be errors or inconsistencies in the masks.</p><p>Redundancies: While no two images are the same, there are instances of images of the same subject taken close together in time.</p><p>9. Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)? If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there official archival versions of the complete dataset (i.e., including the external resources as they existed at the time the dataset was created); c) are there any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to a dataset consumer? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate. The dataset is self-contained.</p><p>10. Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals' non-public communications)? If so, please provide a description. No.</p><p>11. Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why. We have two safety measures to prevent objectionable content:</p><p>(1) Photos are licensed from a photo provider and had to meet the terms of service of the photo provider. We requested that all objectionable content be filtered from the images we licensed. (2) If a user observes objectionable image(s) in the dataset, we invite them to report the image(s) at segmentanything@meta.com for removal. Despite the measures taken, we observe that a small portion of images contains scenes of protests or other gatherings that focus on a diverse spectrum of religious beliefs or political opinions that may be offensive. We were not able to produce a filtering strategy that removes all such images and rely on users to report this type of content.</p><p>12. Does the dataset identify any subpopulations (e.g., by age, gender)? If so, please describe how these subpopulations are identified and provide a description of their respective distributions within the dataset. The dataset does not identify any subpopulations of the people in the photos.</p><p>13. Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset? If so, please describe how. No. Images were subjected to a face blurring model to remove any personally identifiable information. If a user observes any anonymization issue, we invite them to report the issue and the image id(s) at segment-anything@meta.com.</p><p>14. Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals race or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)? If so, please provide a description. The dataset contains scenes of protests, or other gatherings that may suggest religious beliefs, political opinions or union memberships. However, the faces of all people in the dataset have been anonymized via facial blurring, so it is not possible to identify any person in the dataset.</p><p>15. Any other comments? No.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Collection Process</head><p>1. How was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., partof-speech tags, model-based guesses for age or language)? If the data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how. The released masks associated with each image were automatically inferred by our segmentation model, SAM. The masks that were collected using model-assisted manual annotation will not be released. Quality was validated as described in ?5.</p><p>2. What mechanisms or procedures were used to collect the data (e.g., hardware apparatuses or sensors, manual human curation, software programs, software APIs)? How were these mechanisms or procedures validated? The images in the dataset are licensed from an image provider. They are all photos taken by photographers with different cameras.</p><p>3. If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)? We withheld ?2k randomly selected images for testing purposes. The rest of the licensed images are included in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)? The released masks were automatically inferred by SAM. For details on our model-assisted manual annotation process see our Data Annotation Card in ?F.2. Note these masks will not be released.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created. The licensed photos vary in their date taken over a wide range of years up to 2022.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.</head><p>Were any ethical review processes conducted (e.g., by an institutional review board)? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation. If the dataset does not relate to people, you may skip the remaining questions in this section. We underwent an internal privacy review to evaluate and determine how to mitigate any potential risks with respect to the privacy of people in the photos. Blurring faces and license plates protects the privacy of the people in the photos.</p><p>7. Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)? We licensed the data from a third party photo provider.</p><p>8. Were the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself. The images are licensed from a third party who provided appropriate representations regarding the collection of any notices and consents as required from individuals. In addition, all identifiable information (e.g. faces, license plates) was blurred. Under the terms of the dataset license it is prohibited to attempt to identify or associate an image with a particular individual. 9. Did the individuals in question consent to the collection and use of their data? If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented. The images are licensed from a third party who provided appropriate representations regarding the collection of any notices and consents as required from individuals. In addition, all identifiable information (e.g. faces, license plates) was blurred from all images. For avoidance of doubt, under the terms of the dataset license it is prohibited to attempt to identify or associate an image with a particular individual.</p><p>10. If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses? If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate). We invite users to report at segmentanything@meta.com for image(s) removal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11.</head><p>Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted? If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation. To eliminate any potential impact on people whose photos are included in the dataset, identifiable information (faces, license plates) has been blurred.</p><p>12. Any other comments? No.</p><p>Preprocessing / Cleaning / Labeling 1. Was any preprocessing / cleaning / labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? If so, please provide a description. If not, you may skip the remaining questions in this section. We resized the high-resolution licensed images such that the shorter side is 1500 pixels and only processed the images to remove any identifiable and personal information from the photos (faces, license plates).</p><p>2. Was the "raw" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)? If so, please provide a link or other access point to the "raw" data. No, as we removed the data for safety reasons and to respect privacy, we do not release the unaltered photos.</p><p>3. Is the software that was used to preprocess/clean/label the data available? If so, please provide a link or other access point. We used the RetinaFace <ref type="bibr" target="#b87">[88,</ref><ref type="bibr" target="#b88">89]</ref> model (https://github.com/serengil/retinaface) to detect faces. The model used to blur license plates has not been made public.</p><p>Uses 1. Has the dataset been used for any tasks already? If so, please provide a description. The dataset was used to train our segmentation model, SAM.</p><p>2. Is there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point. No. However, all users of the dataset must cite it, so its use is trackable via citation explorers.</p><p>3. What (other) tasks could the dataset be used for? We intend the dataset to be a large-scale segmentation dataset. However, we invite the research community to gather additional annotations for the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a dataset consumer might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other risks or harms (e.g., legal risks, financial harms)? If so, please provide a description. Is there anything a dataset consumer could do to mitigate these risks or harms? We have an analysis of the approximate geographic and income level coverage of our dataset in ?6. While we believe our dataset to be more representative than most of the publicly existing datasets at this time, we acknowledge that we do not have parity across all groups, and we encourage users to be mindful of potential biases their models have learned using this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Are there tasks for which the dataset should not be used? If so, please provide a description. Full terms of use for the dataset including prohibited use cases can be found at https://ai.facebook.com/datasets/segment-anything.</p><p>6. Any other comments? No.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distribution</head><p>1. Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? If so, please provide a description. The dataset will be available for the research community.</p><p>2. How will the dataset will be distributed (e.g., tarball on website, API, GitHub)? Does the dataset have a digital object identifier (DOI)? The dataset is available at https://ai.facebook.com/datasets/segment-anything.</p><p>3. When will the dataset be distributed? The dataset will be released in 2023.</p><p>4. Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions. Yes. The license agreement and terms of use for the dataset can be found at https://ai.facebook.com/datasets/segment-anything. Users must agree to the terms of use before downloading or using the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions. Full terms of use and restrictions on use of the SA-1B dataset can be found at https://ai.facebook.com/datasets/segment-anything.</p><p>6. Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation. The license and restrictions on use of the SA-1B dataset can be found at https://ai.facebook.com/datasets/segment-anything.</p><p>7. Any other comments? No.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Maintenance</head><p>1. Who will be supporting/hosting/maintaining the dataset? The dataset will be hosted at https://ai.facebook.com/datasets/segment-anything and maintained by Meta AI.</p><p>2. How can the owner/curator/manager of the dataset be contacted (e.g., email address)? Please email segment-anything@meta.com.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>Is there an erratum? If so, please provide a link or other access point. No.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to dataset consumers (e.g., mailing list, GitHub)? To aid reproducibility of research using SA-1B, the only updates will be to remove reported images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were the individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced. There are no limits on data retention. We took measures to remove personally identifiable information from any images of people. Users may report content for potential removal here: segment-anything@meta.com.</p><p>6. Will older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how. If not, please describe how its obsolescence will be communicated to dataset consumers.</p><p>No, as the only updates will be to remove potentially harmful content, we will not keep older versions with the content.</p><p>7. If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to dataset consumers? If so, please provide a description. We encourage users to gather further annotations for SA-1B. Any users who generate annotations will be liable for hosting and distributing their annotations.</p><p>8. Any other comments? No.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2. Data Annotation Card</head><p>Task Formulation 1. At a high level, what are the subjective aspects of your task? Segmenting objects present in an image is inherently a subjective task. For instance, one annotator may segment two boots as one mask, whereas another may segment each boot separately. Depending on annotators's skills, the quality of the mask and the number of masks per image are different between annotators. Despite these subjective aspects of the task, we believed efficient annotation was possible as the data was annotated in a per-mask fashion with the main focus on the diversity of the data rather than completeness.</p><p>2. What assumptions do you make about annotators? Our annotators worked full time on our annotation task with very small attrition rate. This made it possible to train the annotators providing feedback and answering their questions on a regular basis. Specifically: (1) By giving a clear understanding of the goals of this work and providing clear guidelines, including visuals and video recordings of the tasks, annotators had enough context to understand and perform the tasks reasonably. (2) Sharing objectives and key results and meeting weekly with annotators increased the likelihood that annotators improved annotation quality and quantity over time.</p><p>3. How did you choose the specific wording of your task instructions? What steps, if any, were taken to verify the clarity of task instructions and wording for annotators? As our task was annotating images, the annotation guidelines included visual examples. Our research team completed 30 annotation tasks to identify any obvious challenges using the annotation tool, collectively decide how to handle complex cases, and refine the guidelines. The research team met with the annotators weekly for feedback sessions. Videos of the research team performing the task were shared live with the annotators, followed by Q&amp;A sessions. Annotators were able to give feedback on unclear aspects, both during the feedback session and asynchronously.</p><p>4. What, if any, risks did your task pose for annotators and were they informed of the risks prior to engagement with the task? No identified risks. Images were filtered for objectionable content prior to the annotation phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>What are the precise instructions that were provided to annotators? We provide only high-level instructions: Given an image, we aim at segmenting every possible object. Annotators generate a mask for every potential object they can identify. An object can be segmented using our interactive segmentation tool either by using corrective foreground/background clicks to add/remove parts of the mask or by drawing a bounding box around the object. Masks can be refined using pixel-precise tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selecting Annotations</head><p>1. Are there certain perspectives that should be privileged? If so, how did you seek these perspectives out? We chose to work with annotators that have worked on other vision annotation tasks before.</p><p>2. Are there certain perspectives that would be harmful to include? If so, how did you screen these perspectives out? No.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>Were sociodemographic characteristics used to select annotators for your task? If so, please detail the process. No.</p><p>4. If you have any aggregated socio-demographic statistics about your annotator pool, please describe. Do you have reason to believe that sociodemographic characteristics of annotators may have impacted how they annotated the data? Why or why not? We worked with 130 annotators. The annotators were all based in Kenya. We do not believe sociodemographic characteristics of annotators meaningfully impacted the annotated data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Consider the intended context of use of the dataset and the individuals and communities that may be impacted by a model trained on this dataset. Are these communities represented in your annotator pool? The Segment Anything 1B (SA-1B) dataset is to be used for research purposes only.</p><p>The SA-1B dataset is one of the most geographically diverse segmentation dataset, as discussed in ?6. In addition, we analyze the responsible AI axes of a model trained on the dataset in ?6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Platform and Infrastructure Choices</head><p>1. What annotation platform did you utilize? At a high level, what considerations informed your decision to choose this platform? Did the chosen platform sufficiently meet the requirements you outlined for annotator pools? Are any aspects not covered? We used a proprietary annotation platform.</p><p>2. What, if any, communication channels did your chosen platform offer to facilitate communication with annotators? How did this channel of communication influence the annotation process and/or resulting annotations?</p><p>We manually reviewed annotations and shared feedback with the annotators on a weekly basis. We communicated common mistakes or inconsistencies and the corresponding corrections. In addition, the annotators were given feedback for improvements daily by the annotation QA team. Outside the weekly feedback sessions, annotators had access to a spreadsheet and chat group to facilitate communication with the research team. This process greatly improved the average speed and quality of the annotations. 2. Are there any conditions or definitions that, if changed, could impact the utility of your dataset? We do not believe so.</p><p>3. Will you attempt to track, impose limitations on, or otherwise influence how your dataset is used? If so, how? The SA-1B dataset will be released under a license agreement allowing use for certain research purposes and protections for researchers. Researchers must agree to the terms of the license agreement to access the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Were annotators informed about how the data is externalized? If changes to the dataset are made, will they be informed? No, we do not plan to release the manual annotations at the moment.</p><p>5. Is there a process by which annotators can later choose to withdraw their data from the dataset? If so, please detail. No.</p><p>Example for 'Combine two unrelated things': The point indicates the lizard, but the mask covers both the lizard and a bird. This is a mask error.</p><p>Example error for 'Failure to consistently handle obscuring foreground objects': The pole on the right (blue arrow) is excluded from the mask, while the pole on the left is included in the object (black arrow). The mask should either include or exclude both of these.</p><p>Example of 'Pixelation of a small mask': this mask has an imperfect boundary, since it extends beyond the object at the black arrow. However, the 'blocky' pattern of the mask is not an error, since, when zoomed in this much, the image is also blocky the same way.</p><p>Example error for consistency with the provided point: The mask does not agree with the blue point, so this is a mask error.</p><p>Example for consistency with the provided point: For this input point, but the logo (left) and the container (right) are valid objects, since the blue point lies on both of them. Neither mask has a mask error.</p><p>Example for consistency with a box: The box surrounds the bowl of oranges, but the mask is only of a single orange. This is a mask error.</p><p>Example for consistency with a box: The box's shape fits the zebra. Even though the mask extends slightly outside the box to include the zebra's left leg, this is not an error.</p><p>Overall mask quality is subjective, each of the above errors may hurt mask quality only a little or a lot, depending on how large the error is. Please use your best judgment when choosing mask scores, and try to stay consistent from mask-to-mask. Here are some general guidelines for what different scores should correspond to:</p><p>? A score of 1: It is not possible to tell what object this mask corresponds to. This includes the case that there is no mask visible at all.</p><p>? A low score (2-4): The object is mostly identifiable, but the mask quality is extremely poor (e.g. large regions of the mask cover other objects; large regions of the object missing; extremely splotchy mask boundaries that cut through the middle of the object).</p><p>? A mid score (5-6): The object is identifiable and the boundary is mostly correct, but there are major errors (missing a significant disconnected part of the object; containing a significant part of another object; very poor boundary quality in one area of the object but not the entire object).</p><p>? A high score (7-9): The object is identifiable and errors are small and rare (missing a small, heavily obscured disconnected component, having small regions where the mask boundary does not quite match the object boundary).</p><p>? A score of 10: The mask is pixel-perfect; it has no identifiable errors at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mask Scoring</head><p>Example of a mask with a score of 1: It is not clear what object this mask corresponds to.</p><p>Example of a mask with a low score (2-4): The main object is identifiable, but the mask includes a large, incorrect portion of another object.</p><p>Example of a mask with a low score (2-4): The main object is identifiable, but a large, random part of the object is missing.</p><p>Example of a mask with a low-to-medium score (4-5): The object is identifiable and the edges are all correct, but the mask incorrectly includes the hand of the person on the left.</p><p>Example of a mask with a medium score (5-6): The mask clearly corresponds to the plate, but the boundary with the waffle is quite poor.</p><p>Example of a mask with a medium score (5-6): the object is easy to identify, and most of the edges make sense. However, there is a significant disconnected part (their arm inside the frame) that is mostly missing, as well as splotchy pixels in this region.</p><p>Example of a mask with a medium-to-high score <ref type="bibr" target="#b5">(6)</ref><ref type="bibr" target="#b6">(7)</ref><ref type="bibr" target="#b7">(8)</ref>: the mask has two small-ish regions of poor boundary, at the top of the mask and on the bottom right.</p><p>Example of a mask with a medium-to-high score (6-8): The wreath is a valid object that is the size of the box (the entire wreath + clock would also be a valid object). However, there are incorrect stray mask pixels on the clock.</p><p>Example of a mask with a high score (7-9): The boundary of the horse is almost entirely correct, except for the right side of its back leg. The mask consistently includes all of the equipment that horse is wearing, and has logical boundaries.</p><p>Example of a mask with a very high score (?9): There are only minor errors around the edge of the mask. The blocky 'pixelation' is not an error, since the image is also blocky at this scale.</p><p>Example of a mask with a very high score (9-10): the mask has only very minor errors in the edge on the bottom right.</p><p>Example of a mask with a very high score (9-10): There are only minor errors around the edge of the mask.</p><p>Figure <ref type="figure">20</ref>: Here we provide the complete guidelines given to annotations for the human review of mask quality. Some images been edited slightly and faces have been blurred to enable release. Best viewed with zoom (part 2 of 2).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>valid mask (c) Data: data engine (top) &amp; dataset (bottom) ? 1+ billion masks ? 11 million images ? privacy respecting ? licensed images annotate train data model Segment Anything 1B (SA-1B):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: We aim to build a foundation model for segmentation by introducing three interconnected components: a promptable segmentation task, a segmentation model (SAM) that powers data annotation and enables zero-shot transfer to a range of tasks via prompt engineering, and a data engine for collecting SA-1B, our dataset of over 1 billion masks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure2: Example images with overlaid masks from our newly introduced dataset, SA-1B. SA-1B contains 11M diverse, high-resolution, licensed, and privacy protecting images and 1.1B high-quality segmentation masks. These masks were annotated fully automatically by SAM, and as we verify by human ratings and numerous experiments, are of high quality and diversity. We group images by number of masks per image for visualization (there are ?100 masks per image on average).</figDesc><graphic url="image-30.png" coords="3,60.43,570.51,120.27,80.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Each column shows 3 valid masks generated by SAM from a single ambiguous point prompt (green circle).</figDesc><graphic url="image-51.png" coords="4,442.25,241.74,51.68,83.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Segment Anything Model (SAM) overview. A heavyweight image encoder outputs an image embedding that can then be efficiently queried by a variety of input prompts to produce object masks at amortized real-time speed. For ambiguous prompts corresponding to more than one object, SAM can output multiple valid masks and associated confidence scores.</figDesc><graphic url="image-89.png" coords="5,35.38,31.80,141.34,141.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Image-size normalized mask center distributions.</figDesc><graphic url="image-90.png" coords="6,310.04,72.00,233.88,53.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Estimated geographic distribution of SA-1B images. Most of the world's countries have more than 1000 images in SA-1B, and the three countries with the most images are from different parts of the world.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Point to mask evaluation on 23 datasets. (a) Mean IoU of SAM and the strongest single point segmenter, RITM<ref type="bibr" target="#b91">[92]</ref>. Due to ambiguity, a single mask may not match ground truth; circles show "oracle" results of the most relevant of SAM's 3 predictions. (b) Per-dataset comparison of mask quality ratings by annotators from 1 (worst) to 10 (best). All methods use the ground truth mask center as the prompt. (c, d) mIoU with varying number of points. SAM significantly outperforms prior interactive segmenters with 1 point and is on par with more points. Low absolute mIoU at 1 point is the result of ambiguity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Zero-shot edge prediction on BSDS500. SAM was not trained to predict edge maps nor did it have access to BSDS images or annotations during training.</figDesc><graphic url="image-117.png" coords="10,51.11,133.22,76.78,51.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>COCO [ 66 ]</head><label>66</label><figDesc>LVIS v1 [44] method AP AP S AP M AP L AP AP S AP M AP L ViTDet-H [62] 51.0 32.0 54.3 68.9 46.6 35.0 58.0 66.3 zero-shot transfer methods (segmentation module only): SAM 46.5 30.8 51.0 61.7 44.7 32.5 57.6 65.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>? 0.06, LVIS GT 8.1 ? 0.07, SAM 7.9 ? 0.08, ViTDet-H 7.6 ? 0.12, COCO GT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>Figure11: Mask quality rating distribution from our human study for ViTDet and SAM, both applied to LVIS ground truth boxes. We also report LVIS and COCO ground truth quality. The legend shows rating means and 95% confidence intervals. Despite its lower AP (Table5), SAM has higher ratings than ViTDet, suggesting that ViTDet exploits biases in the COCO and LVIS training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Zero-shot text-to-mask. SAM can work with simple and nuanced text prompts. When SAM fails to make a correct prediction, an additional point prompt can help.</figDesc><graphic url="image-128.png" coords="11,310.23,186.52,115.76,54.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 :</head><label>13</label><figDesc>Figure13: Ablation studies of our data engine stages, image encoder scaling, and training data scaling. (Left) Each data engine stage leads to improvements on our 23 dataset suite, and training with only the automatic data (our default) yields similar results to using data from all three stages. (Middle) SAM trained with ?10% of SA-1B and full SA-1B is comparable. We train with all 11M images by default, but using 1M images is a reasonable practical setting. (Right) Scaling SAM's image encoder shows meaningful, yet saturating gains. Nevertheless, smaller image encoders may be preferred in certain settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Details of the lightweight mask decoder. A two-layer decoder updates both the image embedding and prompt tokens via cross-attention. Then the image embedding is upscaled, from which the updated output tokens are used to dynamically predict masks. (Not illustrated for figure clarity: At every attention layer, positional encodings are added to the image embedding, and the entire original prompt token (including position encoding) is re-added to the token queries and keys.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Additional visualizations of zero-shot edge predictions on BSDS500. Recall that SAM was not trained to predict edge maps and did not have access to BSDS images and annotations during training.</figDesc><graphic url="image-138.png" coords="21,50.11,138.39,79.20,52.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Zero-shot instance segmentation on LVIS v1. SAM produces higher quality masks than ViTDet. As a zero-shot model, SAM does not have the opportunity to learn specific training data biases; see top-right as an example where SAM makes a modal prediction, whereas the ground truth in LVIS is amodal given that mask annotations in LVIS have no holes.</figDesc><graphic url="image-153.png" coords="22,299.67,144.23,79.32,52.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head></head><label></label><figDesc>0.17, RITM 8.2 ? 0.11, SAM -single output 8.6 ? 0.10, SAM 8.9 ? 0.06, GT (e) NDD20<ref type="bibr" target="#b98">[100]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>0.15, RITM 7.7 ? 0.12, SAM -single output 7.2 ? 0.13, SAM 8.8 ? 0.09, GT (f) OVIS<ref type="bibr" target="#b80">[81]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>3 .Dataset Analysis and Evaluation 1 . 2 . 3 .Dataset Release and Maintenance 1 .</head><label>31231</label><figDesc>How much were annotators compensated? Did you consider any particular pay standards, when determining their compensation? If so, please describe. Annotators were compensated with an hourly wage set by the vendor. The vendor is a Certified B Corporation. How do you define the quality of annotations in your context, and how did you assess the quality in the dataset you constructed? Annotators were first placed into training. They followed a 1-day training session led by the vendor and then were asked to annotate a large number of examples from a training queue. Annotators graduated from training to production after the vendor QA team, in collaboration with the research team, manually spotchecked the annotator's masks to ensure quality. On average, annotators spent one week in training before graduating. Production quality assessment followed a similar process: the vendor QA team and the research team manually reviewed the annotations weekly, sharing feedback weekly. Have you conducted any analysis on disagreement patterns? If so, what analyses did you use and what were the major findings? Did you analyze potential sources of disagreement? We pointed out common mistakes during weekly meetings with the annotators. How do the individual annotator responses relate to the final labels released in the dataset? The annotations were only used to train early versions of the SAM model and we do not currently plan to release them. Do you have reason to believe the annotations in this dataset may change over time? Do you plan to update your dataset? No, except to remove objectionable images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>80.8 87.0 63.1 63.3 58.3 zero-shot transfer methods: SAM -single out. 54.9 42.8 76.7 74.4 54.7 59.8 62.0 SAM 59.3 45.5 81.6 86.9 59.1 63.9 65.8</figDesc><table /><note><p>Object proposal generation on LVIS v1. SAM is applied zero-shot, i.e. it was not trained for object proposal generation nor did it access LVIS images or annotations.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>and layer normalization. The mask</figDesc><table><row><cell>image embedding (256x64x64)</cell><cell>image to token attn.</cell><cell>x2</cell><cell>conv. 2x</cell><cell>dot product per mask</cell><cell>masks</cell></row><row><cell></cell><cell>mlp</cell><cell></cell><cell>trans.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>output</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>token</cell><cell></cell><cell></cell></row><row><cell>(N tokens x256) prompt tokens + output tokens</cell><cell>self attn. token to image attn.</cell><cell></cell><cell>token to image attn. mask decoder token output IoU per mask</cell><cell>mlp mlp</cell><cell>IoU scores</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>?3.8 92.8 ?1.6 middle 78.2 ?0.8 91.3 ?0.3 young 77.3 ?2.7 91.5 ?0.9 SAM's performance segmenting clothing across perceived gender presentation and age group. The intervals for perceived gender are disjoint, with mIoU for masculine being higher. Confidence intervals for age group overlap.</figDesc><table><row><cell></cell><cell cols="2">mIoU at</cell><cell></cell><cell>mIoU at</cell></row><row><cell></cell><cell>1 point</cell><cell>3 points</cell><cell></cell><cell>1 point</cell><cell>3 points</cell></row><row><cell cols="3">perceived gender presentation</cell><cell cols="2">perceived age group</cell></row><row><cell>feminine</cell><cell cols="2">76.3 ?1.1 90.7 ?0.5</cell><cell>older</cell><cell>81.9</cell></row><row><cell cols="3">masculine 81.0 ?1.2 92.3 ?0.4</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Statistical tests showing significance that SAM has higher mask quality ratings than baseline and single-output SAM. P-values are calculated by paired t-test, while confidence intervals for the difference in mean scores are calculated by paired bootstrap on 10k samples. All p-values are significant, and all confidence intervals exclude zero.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>in Table9. For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description. The contributions of our dataset to the vision community are fourfold:<ref type="bibr" target="#b0">(1)</ref> We release a dataset of 11M images and 1.1B masks, by far the largest segmentation dataset to date. (2) The dataset we release is privacy protecting: we have blurred faces and license plates in all images. (3) The dataset is licensed under a broad set of terms of use which can be found at https://ai.facebook.com/datasets/segment-anything. (</figDesc><table><row><cell>F.1. Dataset Card for SA-1B</cell></row><row><cell>Motivation</cell></row><row><cell>1.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. We would like to thank <rs type="person">Aaron Adcock</rs> and <rs type="person">Jitendra Malik</rs> for helpful discussion. We thank <rs type="person">Vaibhav Aggarwal</rs> and <rs type="person">Yanghao Li</rs> for help with scaling the model. We thank <rs type="person">Cheng-Yang Fu</rs>, <rs type="person">Jiabo Hu</rs>, and <rs type="person">Robert Kuo</rs> for help with data annotation platform. We thank <rs type="person">Allen Goodman</rs> and <rs type="person">Bram Wasti</rs> for help in optimizing web-version of our model. Finally, we thank <rs type="person">Morteza Behrooz</rs>, <rs type="person">Ashley Gabriel</rs>, <rs type="person">Ahuva Goldstand</rs>, <rs type="person">Sumanth Gurram</rs>, <rs type="person">Somya Jain</rs>, <rs type="person">Devansh Kukreja</rs>, <rs type="person">Joshua Lane</rs>, <rs type="person">Lilian Luong</rs>, <rs type="person">Mallika Malhotra</rs>, <rs type="person">William Ngan</rs>, <rs type="person">Omkar Parkhi</rs>, <rs type="person">Nikhil Raina</rs>, <rs type="person">Dirk Rowe</rs>, <rs type="person">Neil Sejoor</rs>, <rs type="person">Vanessa Stark</rs>, <rs type="person">Bala Varadarajan</rs>, and <rs type="person">Zachary Winstrom</rs> for their help in making the demo, dataset viewer, and other assets and tooling.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intended Use</head><p>Primary intended uses SAM is intended to be used for any prompt-based segmentation task. We explored its use in segmenting objects from a point ( ?7.1), edge detection ( ?7.2), segmenting all objects ( ?7.3), and segmenting detected objects ( ?7.4). We explored how SAM can integrate with other vision models to segment objects from text ( ?7.5). Primary intended users SAM was primarily developed for research.</p><p>The license for SAM can be found at https://github.com/facebookresearch/segment-anything. Out-of-scope use cases See terms of use for SAM found at https://github.com/facebookresearch/segment-anything. See Use Cases under Ethical Considerations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Caveats and recommendations</head><p>SAM has impressive zero-shot performance across a wide range of tasks. We note, however, that in the zero-shot setting there may be multiple valid ground truth masks for a given input. We recommend users take this into consideration when using SAM for zero-shot segmentation. SAM can miss fine structures and can hallucinate small disconnected components. See ?8 for a discussion of limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relevant Factors</head><p>Groups SAM was designed to segment any object. This includes stuff and things.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instrumentation and environment</head><p>We benchmarked SAM on a diverse set of datasets and found that SAM can handle a variety of visual data including simulations, paintings, underwater images, microscopy images, driving data, stereo images, fish-eye images. See ?D.1 and Table <ref type="table">7</ref> for information on the benchmarks used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model performance measures</head><p>We evaluated SAM on a variety of metrics based on the downstream task in our experiments.</p><p>? mIoU: We used the mean intersection-over-union after a given number of prompts to evaluate the segmentation quality of a mask when prompted with points. ? Human evaluation: We performed a human study (detailed in ?E) to evaluate the real world performance of SAM. We compared the masks generated by SAM to a baseline state-of-the-art interactive segmentation model, RITM <ref type="bibr" target="#b91">[92]</ref>, using a perceptual quality scale from 1 to 10. ? AP: We used average precision to evaluate instance segmentation for a given box and edge detection.</p><p>? AR@1000: We used average recall to evaluate object proposal generation.</p><p>? ODS, OIS, AP, R50: We used the standard edge detection evaluation metrics from BSDS500 <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b2">3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Data</head><p>Data sources See ?D.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Data</head><p>Data source See Data Card in ?F.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Considerations Data</head><p>We trained SAM on licensed images. The images were filtered for objectionable content by the provider, but we acknowledge the possibility of false negatives. We performed a geographic analysis of the SA-1B dataset in ?6.</p><p>While SA-1B is more geographically diverse than many of its predecessors, we acknowledge that some geographic regions and economic groups are underrepresented. Cost and impact of compute SAM was trained on 256 A100 GPUS for 68 hours. We acknowledge the environmental impact and cost of training large scale models. The environmental impact of training the released SAM model is approximately 6963 kWh resulting in an estimated 2.8 metric tons of carbon dioxide given the specific data center used, using the calculation described in <ref type="bibr" target="#b76">[77]</ref> and the ML CO2 Impact calculator <ref type="bibr" target="#b60">[61]</ref>. This is equivalent to ?7k miles driven by the average gasoline-powered passenger vehicle in the US <ref type="bibr" target="#b99">[101]</ref>. We released the SAM models to both reduce the need for retraining and lower the barrier to entry for large scale vision research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Risks and harms</head><p>We evaluated SAM for fairness in ?6. Downstream use cases of SAM will create their own potential for biases and fairness concerns. As such we recommend users run their own fairness evaluation when using SAM for their specific use case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Use cases</head><p>We implore users to use their best judgement for downstream use of the model.</p><p>Table <ref type="table">9</ref>: Model Card for SAM, following the procedure detailed in <ref type="bibr" target="#b74">[75]</ref>.</p><p>We have several models that, when provided with a click or a box as input, output a mask. We would like to compare the quality of these models by rating the quality of their masks on many examples. The interface will be different than for regular mask annotation.</p><p>? Each job reviews one mask in one image.</p><p>? On the right, there will be five image thumbnails in two rows. Each thumbnail can be mousedover to show the image at a larger size. Clicking on the thumbnail will make it full screen, and clicking again will return to the original screen.</p><p>-</p><p>The images show the same mask in five different views. On the top row: (left) the image without the mask, (middle) the mask overlaid on the image, and (right) the mask alone. On the bottom row: (left) a zoomed in view of the object without a mask, and (right) a zoomed in view of the mask overlaid on the image. These views are provided to make it easy to see different types of mask errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-</head><p>The mask will be in red when overlaid on the image. -When shown by itself, the mask is yellow, and the background is purple. -Each image will include either a blue dot or a blue and white box. This is the input to the model, as if you had clicked at this location or drawn this box.</p><p>? On the left, there are buttons labeled 1-10. This is used to rate the quality of the shown mask.</p><p>Objective and Setup Example interface page. There will be five images on the right and a question box on the left.</p><p>Mouse over an image to show the full image.</p><p>Click on an image to make it full screen. The arrows will cycle between images. Click again to return to previous view.</p><p>The first image on the top row shows the image without a mask. A blue point will be on the object of interest, or a blue and white box will surround it.</p><p>The second image on the top row shows the mask for the object in red.</p><p>The third image on the top row shows the mask only. The mask is in yellow and the background is purple.</p><p>The first image on the bottom row shows a zoomed in view of the object without a mask.</p><p>The second image on the bottom row shows a zoomed in view of the object with a mask. The mask is in red.</p><p>On the left are buttons to rate the mask quality, with selections 1-10.</p><p>What we would like you to do for each job:</p><p>? Please aim to spend up to 30 seconds per job.</p><p>? Mouse-over or click each of the three images of the mask on the right to get a sense of the quality of the mask. The thumbnail is too small to judge a mask, do not judge a mask by the thumbnail alone. Each image can provide a different signal on possible mask errors:</p><p>-The unzoomed image can give context for the mask: does this mask correspond to an actual object? -</p><p>The mask-only image can show if the mask has small holes or separated, incorrect pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-</head><p>The zoomed image can show if the mask boundaries make sense.</p><p>? Judge the quality of the mask on three criterion. Examples will follow.</p><p>-Does the mask correspond to an actual object? -Does the mask have a good boundary? -Does the mask correspond to the provided point or box?</p><p>? Rate the quality of the mask on a scale of 1-10 using the drop-down box on the left. ? Example errors a mask may have. The severity of these errors may be minor or major:</p><p>-Include a piece of another object (the mask of a person including the arm of a nearby person) -Miss part of an object (the mask covers only one part of a building obscured by a tree in the foreground), -Combine two unrelated things (a single mask covers both a mug and a pen on a desk) -Include an arbitrary part of a collection for a point input (a point is on one apple, but the mask covers three apples in a pile of many apples). If a box surrounds an arbitrary collection, it is not an error to provide a mask for these objects.</p><p>? If you are unsure, a good rule-of-thumb is: can you name the object in question? However, some things that are hard to name may still be good objects (an unusual component of a machine, something at the edge of the image for which it is hard to determine what it is).</p><p>Judging Mask Quality (1 of 3)</p><p>Does the mask have a good boundary?</p><p>? Errors in the boundary can include:</p><p>-Incorrect holes in the mask -Incorrect pixels included separated from the main part of the mask -Poor edge quality, where the mask does not exactly match the edge of the object. -Failure to consistently handle obscuring foreground objects (a mask that covers obscuring objects is fine, and a mask that doesn't cover obscuring objects is fine, but one that does some of both has an error) -Pixelation of a small mask is not an error, as long as the mask still matches the edges of the object.</p><p>Judging Mask Quality (2 of 3) Does the mask correspond to the provided point or box?</p><p>? For points:</p><p>-</p><p>The point needs to be on the mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-</head><p>The size or position of the object with respect to the point does not matter (a point on someone's gloved hand can correspond to the glove or to the entire person, both are valid masks).</p><p>? For boxes:</p><p>-</p><p>The object needs to be the best object that is the size of the box (if a box is around someone's entire head but the mask is of their hair, this is an error: their hair is in the box but is not the correct object).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-</head><p>If the box clearly corresponds to a given object but is slightly smaller than it, it is okay if the mask goes slightly outside a box (if a box around a person misses their extended hand, the mask can still include their hand even if the mask goes outside the box).</p><p>Judging Mask Quality (3 of 3) Example error of 'Include a piece of another object': The elephant mask contains a piece of another nearby elephant.</p><p>Example error of 'Missing a part of an object': the mask is missing a disconnected part of the object: the back half of the zebra, and the right portion of the plate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example error of 'Include an arbitrary part of a collection':</head><p>In top top image, the point is on one orange rind, but the mask covers two orange rinds. This is a mask error: the mask covers an arbitrary number of objects in the collection, and should either cover one orange rind or all of them. In the bottom image, the box is around both vegetables. Since this is the best match to the box, this is not a mask error.</p><p>Example error for 'Incorrect holes in the mask': This mask has holes in the upper left and on the left sides (black arrows). These holes are much easier to see on the 'mask only' image.</p><p>Example error for 'Incorrect pixels included separated from the main part of the mask': The 'mask only' view reveals a few stray incorrect pixels on the clock face.</p><p>Example error for 'Poor edge quality': The mask has poor edge quality, both along the edge of the umbrella, as well as along the thin pole. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Annotation Guidelines</head><p>We provide the complete guidelines given to annotations for the human review of mask quality in Fig. <ref type="figure">19</ref> and Fig. <ref type="figure">20</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">On seeing stuff: the perception of materials by humans and machines. Human vision and electronic imaging VI</title>
		<author>
			<persName><surname>Edward H Adelson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What is an object?</title>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2010">2010. 4, 10, 21</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><surname>Beit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m">BERT pre-training of image transformers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ZeroWaste dataset: Towards deformable object segmentation in cluttered scenes</title>
		<author>
			<persName><forename type="first">Dina</forename><surname>Bashkirova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Abdelfattah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziliang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Akl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fadi</forename><surname>Alladkani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Ablavsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berk</forename><surname>Calli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">Adel</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ilastik: interactive machine learning for (bio)image analysis</title>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Kutra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorben</forename><surname>Kroeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><forename type="middle">N</forename><surname>Straehle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><forename type="middle">X</forename><surname>Kausler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Haubold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Schiegg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janez</forename><surname>Ales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Beier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Rudy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kemal</forename><surname>Eren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">I</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Buote</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fynn</forename><surname>Beuttenmueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Wolny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ullrich</forename><surname>Koethe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Kreshuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">On the opportunities and risks of foundation models</title>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simran</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><surname>Sydney Von Arx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeannette</forename><surname>Michael S Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Iterative interaction training for segmentation editing networks</title>
		<author>
			<persName><forename type="first">Gustav</forename><surname>Bredell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Tanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ender</forename><surname>Konukoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICCAI</title>
		<imprint>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: Delving into high quality object detection</title>
		<author>
			<persName><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nucleus segmentation across imaging experiments: the 2018 data science bowl</title>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allen</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><forename type="middle">W</forename><surname>Karhohs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beth</forename><forename type="middle">A</forename><surname>Cimini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeanelle</forename><surname>Ackerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marzieh</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cherkeng</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Mcquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Rohban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><forename type="middle">E</forename><surname>Carpenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName><forename type="first">John</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end object detection with Transformers</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic image colorization via multimodal predictions</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Charpiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Harsh Agrawal, Aroma Mahendru, and Dhruv Batra. Object-proposal evaluation protocol is&apos; gameable</title>
		<author>
			<persName><forename type="first">Neelima</forename><surname>Chavali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">instance segmentation of MVS buildings</title>
		<author>
			<persName><forename type="first">Jiazhou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shufang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronghua</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangliang</forename><surname>Nan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3D</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="1920">2022. 9, 19, 20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">FocalClick: towards practical interactive image segmentation</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manni</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donglian</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Masked-attention mask transformer for universal image segmentation</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Perpixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">Scaling language modeling with pathways</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Domain adaptation for traffic density estimation</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Ciampi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Santiago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Gennaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Amato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Night and day instance segmented park (NDIS-Park) dataset: a collection of images taken by day and by night for vehicle detection, segmentation and counting in parking areas</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Ciampi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Santiago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Gennaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Amato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zenodo</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantic segmentation in art paintings</title>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yael</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="1920">2022. 9, 19, 20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The Cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning parameterized skills</title>
		<author>
			<persName><forename type="first">George</forename><surname>Bruno Da Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Konidaris</surname></persName>
		</author>
		<author>
			<persName><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rescaling egocentric vision: Collection, pipeline and challenges for EPIC-KITCHENS-100</title>
		<author>
			<persName><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="1920">2022. 9, 20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">EPIC-KITCHENS VISOR benchmark: Video segmentations and object relations</title>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Darkhalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dandan</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amlan</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="1920">2022. 9, 19, 20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Terrance</forename><forename type="middle">De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<title level="m">Does object recognition work for everyone? CVPR workshops</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Crowd-WorkSheets: Accounting for individual and collective identities underlying crowdsourced dataset annotation</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>D?az</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Kivlichan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Amironesei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Vinodkumar Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><surname>Denton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Conference on Fairness, Accountability, and Transparency</title>
		<imprint>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">PhraseClick: toward achieving flexible interactive segmentation by phrase and click</title>
		<author>
			<persName><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast edge detection using structured forests</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitnick</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">21</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to recognize objects in egocentric activities</title>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient graphbased image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The validity and practicality of sun-reactive skin types i through vi</title>
		<author>
			<persName><forename type="first">B</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><surname>Fitzpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Archives of Dermatology</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ning Xu, and Franc ?ois Piti?</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Forte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07932</idno>
	</analytic>
	<monogr>
		<title level="m">Getting to 99% accuracy in interactive segmentation</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Instance segmentation for autonomous log grasping in forestry operations</title>
		<author>
			<persName><forename type="first">Jean-Michel</forename><surname>Fortin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Gamache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Grondin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Franc ?ois Pomerleau, and Philippe Gigu?re</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Datasheets for datasets</title>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Morgenstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Briana</forename><surname>Vecchione</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><forename type="middle">Daum?</forename><surname>Iii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Crawford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Simple copy-paste is a strong data augmentation method for instance segmentation</title>
		<author>
			<persName><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekin</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet in 1 hour</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Westbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Chavis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Hamburger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santhosh</forename><surname>Kumar Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fiona</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayant</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">Zhongcong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhant</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Cartillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Crane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tien</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morrie</forename><surname>Doulaty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Erapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriano</forename><surname>Fragomeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qichen</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Fuegen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abrham</forename><surname>Gebreselasie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hillis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuhua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weslie</forename><surname>Khoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jachym</forename><surname>Kolar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Landini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghava</forename><surname>Modhugu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tullie</forename><surname>Murrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takumi</forename><surname>Nishiyasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paola</forename><forename type="middle">Ruiz</forename><surname>Puentes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Merey</forename><surname>Ramazanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leda</forename><surname>Sari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiran</forename><surname>Somasundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Audrey</forename><surname>Southerland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruijie</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xindi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takuma</forename><surname>Yagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunyi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">C V</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haizhou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyun</forename><forename type="middle">Soo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoichi</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingfei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<editor>Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria Farinella, Bernard Ghanem, Vamsi Krishna Ithapu,</editor>
		<imprint/>
	</monogr>
	<note>Ego4D: Around the World in 3,000 Hours of Egocentric Video. CVPR, 2022. 20</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">LVIS: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="1920">2019. 2, 6, 7, 9, 10, 11, 19, 20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multiple choice learning: Learning to produce multiple structured outputs</title>
		<author>
			<persName><forename type="first">Abner</forename><surname>Guzman-Rivera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">SOCRATES: Introducing depth in visual wildlife monitoring using stereo vision</title>
		<author>
			<persName><surname>Timm Haucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hjalmar</surname></persName>
		</author>
		<author>
			<persName><surname>K?hl</surname></persName>
		</author>
		<author>
			<persName><surname>Volker Steinhage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2022">2022. 5, 8, 12</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mask R-CNN. ICCV</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Training compute-optimal large language models</title>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15556</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">TrashCan: A semantically-segmented dataset towards visual detection of marine debris</title>
		<author>
			<persName><forename type="first">Jungseok</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Fulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junaed</forename><surname>Sattar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08097</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep networks with stochastic depth</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Oneformer: One transformer to rule universal image segmentation</title>
		<author>
			<persName><forename type="first">Jitesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiachen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mangtik</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Orlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.06220</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling laws for neural language models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Snakes: Active contour models</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demetri</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning open-world object proposals without learning to classify</title>
		<author>
			<persName><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<editor>
			<persName><forename type="first">So</forename><surname>Kweon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Weicheng</forename><surname>Kuo</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Panoptic segmentation</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2020">2020. 2, 6, 7, 18, 19</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Quantifying the carbon emissions of machine learning</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Luccioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Dandres</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09700</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Exploring plain vision transformer backbones for object detection</title>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2022">2022. 5, 10, 11, 16, 21</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Delving into egocentric actions</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhefan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Interactive image segmentation with latent diversity</title>
		<author>
			<persName><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Coco</forename><surname>Microsoft</surname></persName>
		</author>
		<title level="m">Common objects in context. ECCV</title>
		<imprint>
			<date type="published" when="1920">2014. 2, 4, 6, 7, 11, 18, 19, 20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Sim-pleClick: Interactive image segmentation with simple vision transformers</title>
		<author>
			<persName><forename type="first">Qin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenlin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Niethammer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.11006</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Gelatinous zooplankton biomass in the global oceans: geographic variation and environmental drivers</title>
		<author>
			<persName><forename type="first">Cathy H</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">B</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><surname>Hollyhead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><forename type="middle">M</forename><surname>Robert H Condon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">M</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kylie</forename><forename type="middle">A</forename><surname>Kelly L Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Pitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jim</forename><surname>Schildhauer</surname></persName>
		</author>
		<author>
			<persName><surname>Regetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Global Ecology and Biogeography</title>
		<imprint>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Iteratively trained interactive segmentation</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Sabarinath Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Deep extreme cut: From extreme points to object segmentation</title>
		<author>
			<persName><forename type="first">Kevis-Kokitsi</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName><forename type="first">David</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doron</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">V-Net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DV</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Finely-grained annotated datasets for imagebased plant phenotyping</title>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Fischbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanno</forename><surname>Scharr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sotirios</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zaldivar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Vasserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Spitzer</surname></persName>
		</author>
		<title level="m">Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of the conference on fairness, accountability, and transparency</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Extreme clicking for efficient object annotation</title>
		<author>
			<persName><surname>Dim P Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluis-Miquel</forename><surname>Munguia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Rothchild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maud</forename><surname>Texier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10350</idno>
		<title level="m">Carbon emissions and large neural network training</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence tagging with bidirectional language models</title>
		<author>
			<persName><forename type="first">Waleed</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">EDTER: Edge detection with transformer</title>
		<author>
			<persName><forename type="first">Mengyang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingji</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">DOORS: Dataset fOr bOuldeRs Segmentation</title>
		<author>
			<persName><forename type="first">Mattia</forename><surname>Pugliatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Topputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zenodo</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Occluded video instance segmentation: A benchmark</title>
		<author>
			<persName><forename type="first">Jiyang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="1920">2022. 9, 20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="2021">2021. 1, 2, 4, 5, 8, 12</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Zero-shot textto-image generation</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2021">2021. 1, 4, 12</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R-Cnn</forename><surname>Faster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Learning a classification model for segmentation</title>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Hypersim: A photorealistic synthetic dataset for holistic indoor scene understanding</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Ramapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atulit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><forename type="middle">Angel</forename><surname>Bautista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Paczan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">M</forename><surname>Susskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">A step toward more inclusive people annotations for fairness</title>
		<author>
			<persName><forename type="first">Candice</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susanna</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utsav</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 AAAI/ACM Conference on AI</title>
		<meeting>the 2021 AAAI/ACM Conference on AI</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">LightFace: A hybrid deep face recognition framework</title>
		<author>
			<persName><forename type="first">Sefik</forename><surname>Ilkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serengil</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Alper</forename><surname>Ozpinar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ASYU</title>
		<imprint>
			<biblScope unit="page">26</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">HyperExtended LightFace: A facial attribute analysis framework</title>
		<author>
			<persName><forename type="first">Sefik</forename><surname>Ilkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serengil</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Alper</forename><surname>Ozpinar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICEET</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">TextonBoost: Joint appearance, shape and context modeling for mulit-class object recognition and segmentation</title>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">STREETS: A novel camera network dataset for traffic flow</title>
		<author>
			<persName><forename type="first">Corey</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Reviving iterative training with mask guidance for interactive segmentation</title>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Sofiiuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><forename type="middle">A</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Konushin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2022">2022. 5, 8, 9, 17, 19</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">16</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Adaptive background mixture models for real-time tracking</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Grimson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Fourier features let networks learn high frequency functions in low dimensional domains</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratul</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Fridovich-Keil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nithin</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utkarsh</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Multi-stream deep neural networks for RGB-D egocentric action recognition</title>
		<author>
			<persName><forename type="first">Yansong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianjiang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Yansong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianjiang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="1920">2017. 20. 2019. 20</date>
		</imprint>
	</monogr>
	<note>Action recognition in RGB-D egocentric videos</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">The world by income and regions</title>
		<ptr target="https://datatopics.worldbank.org/world-development-indicators/the-world-by-income-and-region.html.18" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>The World Bank</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Is learning the n-th thing any easier than learning the first? NeurIPS</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">NDD20: A large-scale few-shot dolphin dataset for coarse and fine-grained categorisation</title>
		<author>
			<persName><forename type="first">Cameron</forename><surname>Trotter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Sharpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kirsten</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Stephen</forename><surname>Mcgough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Burville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Per</forename><surname>Berggren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.13359</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<ptr target="https://www.epa.gov/energy/greenhouse-gas-equivalencies-calculator" />
		<title level="m">Greenhouse Gas Equivalencies Calculator</title>
		<imprint>
			<publisher>United States Environmental Protection Agency</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Segmentation as selective search for object recognition</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnold</forename><forename type="middle">Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Towards real-world prohibited item detection: A largescale x-ray benchmark</title>
		<author>
			<persName><forename type="first">Boying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Libo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Open-world instance segmentation: Exploiting pseudo ground truth from learned pairwise affinity</title>
		<author>
			<persName><forename type="first">Weiyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Multiview compressive coding for 3D reconstruction</title>
		<author>
			<persName><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Aude Oliva, and Antonio Torralba. SUN database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krista</forename><surname>Ehinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Deep interactive object selection</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Towards fairer datasets: Filtering and balancing the distribution of the people subtree in the imagenet hierarchy</title>
		<author>
			<persName><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klint</forename><surname>Qinami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Proceedings of the 2020 conference on fairness, accountability, and transparency</note>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<author>
			<persName><forename type="first">Lei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><forename type="middle">Zi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Yisheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.15068</idno>
		<title level="m">A first step towards irregular shape instance segmentation</title>
		<imprint>
			<date type="published" when="1920">2021. 9, 20</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">WoodScape: A multi-task, multicamera fisheye dataset for autonomous driving</title>
		<author>
			<persName><forename type="first">Senthil</forename><surname>Yogamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ciar?n</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Sistu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Padraig</forename><surname>Varley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">O</forename><surname>'dea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Uric?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Milz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Amende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Finegrained egocentric hand-object segmentation: Dataset, model, and applications</title>
		<author>
			<persName><forename type="first">Lingzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">K-Net: Towards unified image segmentation</title>
		<author>
			<persName><forename type="first">Wenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09457</idno>
		<title level="m">Men also like shopping: Reducing gender bias amplification using corpus-level constraints</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="1920">2017. 20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Semantic understanding of scenes through the ADE20K dataset</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="1920">2019. 2, 7, 9, 20</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
