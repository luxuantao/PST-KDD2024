<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">From Chaos Comes Order: Ordering Event Representations for Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-04-27">27 Apr 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nikola</forename><surname>Zubi?</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mathias Gehrig Davide Scaramuzza Robotics and Perception Group</orgName>
								<orgName type="institution">University of Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Gehrig</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mathias Gehrig Davide Scaramuzza Robotics and Perception Group</orgName>
								<orgName type="institution">University of Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">From Chaos Comes Order: Ordering Event Representations for Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-04-27">27 Apr 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2304.13455v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Voxel Grid MDES Time Surface 2D Histogram TORE ERGO-12 (ours) Swin-V2 EfficientRep ResNet-50 (a) Method overview (b) GW Discrepancy vs. Performance Figure 1: Selecting dense event representations for deep neural networks is exceedingly slow since it involves training a neural network for each representation and selecting the best one based on the validation score. In this work, we eliminate this bottleneck by selecting the representation based on the Gromov-Wasserstein Discrepancy (GWD) (a). This metric is 200 times faster to compute and preserves the task performance ranking of event representations perfectly across multiple representations, network backbones, and datasets (b). We use it to, for the first time, perform a hyperparameter search on a large family of event representations, revealing new and powerful event representations that exceed the state-of-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Event cameras are biologically inspired vision sensors that function in a fundamentally distinct way <ref type="bibr" target="#b12">[12]</ref>. Unlike traditional cameras that capture images at a fixed rate, these cameras measure brightness changes independently for each pixel, and these changes are referred to as events. The events encode the time, location, and polarity (sign) of the brightness changes. Event cameras offer several advantages over frame-based cameras, including exceptionally high temporal resolution (in the order of ?s), a high dynamic range, and low power consumption. Their numerous benefits make them attractive for a wide range of applications like robotics, autonomous vehicles, and virtual reality. However, due to their sparse and asynchronous nature, applying classical computer vision algorithms remains challenging.</p><p>Many state-of-the-art deep learning models address this challenge by converting sparse and asynchronous events into dense grid-like representations before processing them with off-the-shelf deep neural networks. By using these networks, methods like this enjoy the advantages of mature learning algorithms and network architectures and optimized hardware, but we need to make a non-trivial choice of event representation. In fact, the computer vision and robotics fields are witnessing a surge in the number of research papers utilizing event-based vision, resulting in a plethora of new event representations being proposed. Despite this, extensive comparisons of these representations remain rare, making it unclear whether these newer representations should be adopted.</p><p>No efficient methodology exists for comparing these representations. Conventionally, comparing event representations involves training a fixed deep-learning model for each event representation separately and subsequently selecting the optimal one based on a validation score. This process is very time-intensive since it requires network training in the loop which often takes hours or days (Fig. <ref type="figure">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a top).</head><p>In this work, we propose a fast method to compare event representations which circumvents the need to train a neural network, and instead computes the Gromov-Wasserstein Discrepancy (GWD) between the raw events and event representation (Fig. <ref type="figure">1</ref> a bottom). This metric effectively measures the distortion that is introduced through converting raw events to representations, and thus puts an upper bound on the amount of information that can be accessed by downstream neural networks. We show extensive experimental evidence, that this metric preserves the taskperformance ranking across a wide range of input representations for several datasets and neural network backbones (Fig. <ref type="figure">1 b</ref>). Due to its low computational cost, we apply the GWD to, for the first time, explicitly optimize over a large family of event representations, which reveals a new and powerful representation, which we term 12-channel Event Representation through Gromov-Wasserstein Optimization (ERGO-12). For the task of object detection, networks trained with these representations outperform other representations by 1.9% mAP on the 1 Mpx dataset and 8.6% mAP on Gen1, even outperforming state-of-the-art methods by 1.8 on Gen1 and state-of-the-art feed-forward methods by 6.0% mAP on the 1 Mpx dataset. We believe that the GWD is a powerful tool that opens up a new research field that searches optimized event representations. Our contributions are summarized as follows:</p><p>? We introduce a novel, efficient approach for comparing dense event representations using the Gromov-Wasserstein Discrepancy (GWD).</p><p>? We show extensive experimental evidence that it preserves the task performance ranking of neural networks trained with these representations across datasets and neural network backbones.</p><p>? We utilize it to, for the first time, conduct a hyperparameter search on a vast family of event representations, unveiling novel and powerful event representations that outperform the current state-of-the-art representations on the object detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In the field of event-based vision, two primary groups of representations exist: sparse and dense. Methods that use sparse representations <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b49">48,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b37">36]</ref> preserve the sparsity in the events, but do not yet scale to more complex tasks due to a lack of specialized hardware, and mature neural networks architectures. This frequently results in lower performance on downstream tasks. In contrast, dense representations <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b47">46,</ref><ref type="bibr" target="#b51">50,</ref><ref type="bibr" target="#b48">47]</ref> offer improved performance since they can leverage mature machine learning algorithms and neural network architectures. Sparse representations, pioneered by asynchronous SNNs <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b37">36]</ref>, are limited by the lack of specialized hardware and computationally efficient backpropagation algorithms. Point cloud encoders <ref type="bibr" target="#b46">[45,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b11">11]</ref> have been used due to the spatio-temporal nature of event data, but can be computationally expensive and noisy. Graph neural networks <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b45">44,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b10">10]</ref> are scalable and have achieved high performance on various vision tasks, but are still less accurate than dense methods for event-based vision. In this study, we focus on dense event representations and aim to achieve better task performance by utilizing existing efficient learning algorithms that are appropriate for current hardware.</p><p>Early dense representations converted events to histograms <ref type="bibr" target="#b30">[30]</ref>, generated time surfaces <ref type="bibr" target="#b47">[46]</ref> or combined both <ref type="bibr" target="#b50">[49]</ref> while relying on standard neural network backbones to process them. However, these representations only capture a low-dimensional representation of events since they typically only use a few channels. Later approaches have tried to capture more event information by either computing higher-order moments <ref type="bibr" target="#b0">[1]</ref> or stacking multiple time windows of events <ref type="bibr" target="#b51">[50]</ref>. These methods still stack events based on fixed time windows which is problematic when the event rate becomes too large or too small, and lead to the introduction of stacking based on the number of events <ref type="bibr" target="#b48">[47]</ref>. In parallel, a bio-inspired approach led to the introduction of Time Ordered Recent Event Volumes (TORE) <ref type="bibr" target="#b1">[2]</ref> which aggregate events into queues. However, they are slow to compute and perform similarly to existing Voxel Grids <ref type="bibr" target="#b51">[50]</ref>. Most recently, a powerful representation was proposed by Nam et al. <ref type="bibr" target="#b34">[33]</ref> which divides events into multiple overlapping windows which halve the number of events at each stage, which are more robust during varying scene dynamics.</p><p>Few papers study the effect of event representations on task performance. While <ref type="bibr" target="#b38">[37]</ref> and <ref type="bibr" target="#b21">[21]</ref> show small-scale ablation studies to select event representations, only Gehrig et al. <ref type="bibr" target="#b13">[13]</ref> performed a large-scale investigation of event representations by training models on various inputs for multiple tasks. Their study demonstrated the advantages of splitting polarities and incorporating timestamps into representations, and it introduced a learnable representation. However, training for a single task was still computationally expensive, which limited the number of representations that could be compared. For this reason, their study did not cover a large number of representations, and in particular did not consider different window sizes as is done in later work <ref type="bibr" target="#b48">[47,</ref><ref type="bibr" target="#b34">33]</ref>, or more advanced aggregations and measurements like in <ref type="bibr" target="#b0">[1]</ref>. Our method instead introduces an efficient metric to compare event representations that solves these limitations, allowing us to perform a search over a large family of event representations and go beyond the representations in <ref type="bibr" target="#b48">[47,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b14">14]</ref>, including even non-differentiable hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we will first introduce the preliminaries on computing event representations (Sec. 3.1) and then propose the metric we use to measure the discrepancy between events and their representation based on the GWD (Sec. 3.2) before concluding with Sec. 3.3 where we use Bayesian optimization to find an optimal event representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>Event cameras measure brightness changes as an asynchronous stream of events. Each event is triggered when the intensity L(u) at the pixel u = (x, y) changes by the contrast threshold C at time t, and thus satisfies</p><formula xml:id="formula_0">p[L(u, t) -L(u, t -?t)] = C<label>(1)</label></formula><p>where p ? {-1, 1} is the polarity of the event, and t -?t is the time of the last event. Within a time window ?T , an event camera thus generates an ordered set of events</p><formula xml:id="formula_1">E = {e k } Ne-1 k=0 , with each event e k = (u k , t k , p k ) ? R 4 .</formula><p>To bridge the gap between asynchronous events and dense neural networks, they are usually converted to a dense event representation</p><formula xml:id="formula_2">R = F (E),<label>(2)</label></formula><p>which has features f x . = R(x) ? R N f indexed by the integer-valued pixel location x. The above representation thus generates a set of features F = {f x } x?? , where ? denotes the image domain and has size |?| = N f with N f being the number of pixels in the image. In what follows, we will derive a measure to quantify the distortion between events E and features F based on the GWD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Gromov-Wasserstein Discrepancy</head><p>Converting raw events to a representation invariably distorts the events by removing important distinguishing features from the stream. We would like to measure this distortion since we expect that it correlates strongly with a neural network's ability to extract features from these events. In what follows, we will derive a measure of this distortion rate, based on the GWD.</p><p>We show an overview of the GWD in Fig. <ref type="figure" target="#fig_0">2</ref>. We start by measuring the similarity between a set of events and their representation by building a soft correspondence between events e i and features f xj , which we denote as T ij 1 . This transport plan effectively moves each event to a corresponding feature, thereby distorting the original set and destroying information. Importantly, such a plan can be interpreted as follows: We transport the events with a total weight of 1, i.e. per-event weight 1  Ne to the output features, which also need to receive a total weight of 1 or per-feature weight of 1 N f . By this construction, T ij needs to satisfy i T ij = 1/N f and j T ij = 1/N e . This means that a total weight of 1/N e moves from each event, and each feature receives a total weight of 1/N f .</p><p>In the next step, we measure the distortion introduced by this transportation plan by considering pairwise similarities of input events and features. Let e i , e k ? E be a pair of events and f xj , f x l ? F pair of features, with similarity scores C e ik . = C e (e i , e k ) and</p><formula xml:id="formula_3">C f jl . = C f (f xj , f x l )</formula><p>between events and features respectively. Next, consider how the transport plan T acts on these pairs: Generally, a weight T ij is moved from event e i to feature f xj . Similarly, the weight T kl is moved from event e k to feature f x l . Ideally, such a transport plan should preserve the similarity between pairs of source events and target features, and thus the difference in similarity scores between pairs (i, k) and (j, l) can be used as a measure of distortion. For each event pair and feature pair, we define the distortion as</p><formula xml:id="formula_4">L ijkl = T ij T kl L(C e ik , C f jl ),<label>(3)</label></formula><p>, where L denotes some disparity measurement between C e (e i , e k ) and C f jl . Summing over all possible pairs of events and features we thus arrive at the transportation cost:</p><formula xml:id="formula_5">L(T ; E, F) = i,j,k,l L ijkl = i,j,k,l T ij T kl L(C e ik , C f jl ) (4)</formula><p>Minimizing over transport plans, we arrive at the GWD:</p><formula xml:id="formula_6">L(E, F) = min T i,j,k,l T ij T kl L(C e ik , C f jl ),<label>(5)</label></formula><p>which can be optimized efficiently using <ref type="bibr" target="#b39">[38]</ref>. Since the above metric is defined for a single time window of events, we may average it over multiple samples to find:</p><formula xml:id="formula_7">GWD N = 1 N i L(E i , F i ).<label>(6)</label></formula><p>GWD N can be interpreted as an average distortion rate when going from raw events to event representations. In Sec. 4.2 we show that GWD N correlates with a NN's performance with that representation across network backbones, datasets, and event representations. It's also efficient to compute, taking 9 seconds for 50,000 events. In what follows GWD N will denote the average over N samples, while GWD denotes the average over the whole validation set. Similarity scores and distortion function As similarity scores, we choose Gaussian radial basis functions <ref type="bibr" target="#b44">[43]</ref> for both events and image features. In detail,</p><formula xml:id="formula_8">C e ik = e -e i -e k 2 2h 2 ? 2 e , C f jl = e -fx j -fx l 2 2h 2 ? 2 f (7) ? 2 e = mean i&lt;j e i -e j 2 , ? 2 f = mean i&lt;j f xi -f xj 2 . (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>with a bandwidth parameter h = 0.7. The selection of datadependent variances normalizes the distances between pairs of events and features such that the similarity score is robust to the dimensionality of the data and also the number of samples in the source and target domain. These details are discussed in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b36">35]</ref>. While more complex similarity scores could be used, this simple function already achieved good results. As the distortion function, we chose the KL-Divergence i.e.</p><formula xml:id="formula_10">L(C e ik , C f jl ) = C e ik log C e ik /C f jl .<label>(9)</label></formula><p>As a result, the optimization already rejects terms for which C e ik ? 0, i.e. pairs of events that are far apart. We found that this property was also beneficial to improve the convergence of the optimization problem in Eq. ( <ref type="formula" target="#formula_6">5</ref>) Improving Convergence of Eq. (5). We found that three features improved convergence and speed up the optimization: (i) Normalization of the event coordinates and timestamps by the sensor size and time window respectively, (ii) concatenating the normalized pixel position to the image features. and (iii) sparsification of image features. Both (i) and (ii) make the optimization more numerically stable. In fact, without concatenating position information to image features, randomly pixel-wise shuffled event representations would retain the same GWD, although intuitively, neural networks would have a harder time learning from such representations since they typically process nearby features together. Thus reintroducing the position removes this ambiguity, and also improved convergence. Finally, (iii) removes image features with f x = 0, since these correspond to pixels where no events were triggered. This step significantly sped up computation by reducing the size of the pair-wise similarity score matrix C f , with a small impact on convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Optimizing over Event Representations</head><p>With a fast method to measure the effectiveness of an event representation, we can now search for the optimal representation by minimizing Eq. ( <ref type="formula" target="#formula_7">6</ref>) over a space of possible representations with a set of hyperparameters p.</p><formula xml:id="formula_11">p * = arg min p 1 N i L(E i , F i (p)). (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>?, ? To simplify this optimization, we first describe a very general parametrization, which defines a large family of event representations, extending the family described in <ref type="bibr" target="#b13">[13]</ref> in a few important ways. These hyperparameters are a small set of categorical variables which can subsequently be optimized using Bayesian optimization.</p><formula xml:id="formula_13">Time? ? 0 ? 2 ? 3 ? 4 Hyperparams ? = ? ? , ? ? , ? ? ?=0 ? ? -1 ? Representa?on ? ? (?) Window Measure Aggregate ? = 0 ? = 1 ? = ?? -1 Measure Aggregate Measure Aggregate Raw Events ? ? 1 Window Window ? ? ? ? ? ? ? ? ? ?, ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parametrization of Event Representations</head><p>We illustrate the hyperparameters in Fig. <ref type="figure" target="#fig_1">3</ref>. In general, we assume an event representation comprises a stack of features, indexed by c (right side), each of which is derived from (i) a specific time window w c of events, (ii) a specific measurement m c of events such as polarity or timestamps, and (iii) a specific aggregation a c , such as summation or averaging. We write such a representation as N c concatenated feature maps:</p><formula xml:id="formula_14">R = [R 0 |R 1 |...|R Nc-1 ]<label>(11)</label></formula><p>with</p><formula xml:id="formula_15">R c = a c (m c (w c (E)))<label>(12)</label></formula><p>Here w c is a windowing function which selects events within an interval, m c is the measurement function which selects an event feature and a c is the aggregation function, which aggregates measurements into a single feature map. Note in our formulation each channel can have an independent set of parameters, different to <ref type="bibr" target="#b13">[13]</ref> which assumes a shared aggregation and measurement function for all channels. This makes our parametrization substantially more expressive than the one in <ref type="bibr" target="#b13">[13]</ref>. Moreover, while a c and m c were already discussed in <ref type="bibr" target="#b13">[13]</ref>, the windowing function is a more general concept, illustrated in Fig. <ref type="figure" target="#fig_1">3 (left)</ref>. While these windows can be non-overlapping (w 3 , w 4 , w 2 ), as for Voxel Grids, they can also be overlapping, (w 0 , w 1 , w 2 ) as in Mixed-Density Event Stacks <ref type="bibr" target="#b34">[33]</ref>, or describe windows of a constant event count or constant time <ref type="bibr" target="#b48">[47]</ref> In this work, we allow each feature channel to select from a basis of windows</p><formula xml:id="formula_16">W = {[t 0,k , t 1,k ]} N k -1</formula><p>k=0 , which can combine all types of windows unifying these concepts. In summary, a representation is parametrized by:</p><formula xml:id="formula_17">p = {(w c , a c , m c )} Nc-1 c=0 (13) with m c ? M, a c ? A, w c ? W.</formula><p>For the sets of aggregation functions we select A = {max, sum, mean, variance} and for the measurement functions M = {t + , t -, t, p, c + , c -, c} which are most commonly used. Here c, p, t denote event count (discarding polarity), polarity, and timestamp respectively, and the subscripts +/select only positive or negative events. For the basis of time windows, we select three equally spaced, non-overlapping windows from <ref type="bibr" target="#b51">[50]</ref>, and four overlapping windows from <ref type="bibr" target="#b34">[33]</ref>, including the global window. These are illustrated in Fig. <ref type="figure" target="#fig_6">7</ref> (right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimization</head><p>Procedure The aforementioned parametrization generates redundant combinations that can be obtained by permuting channels or selecting the same feature for different channels. To address this issue and expedite convergence, we propose a stage-wise optimization procedure. Initially, we start with a volume consisting of zeros with N c channels and optimize over a 0 , w 0 , and m 0 to fill in the first channel. Next, we optimize the feature for the second channel while keeping the first fixed. With this iterative process, we incrementally fill up the representation, avoiding the selection of redundant representations and resulting in faster optimization. At each stage, we use Gryffin <ref type="bibr" target="#b18">[18]</ref>, a specialized Bayesian optimizer for categorical variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In Sec. 4.1, we first connect the GWD and event distortion, demonstrating its behavior on several toy examples. Next, in Sec. 4.2, we showcase the correlation between the metric and task performance across multiple datasets, backbones, and representations. Lastly, in Sec. 4.3, we show the outcome of Bayesian Optimization on the GWD and offer insights on the acquired event representation before comparing it against the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Toy Example</head><p>As explained in Sec. 3.2 the GWD measures the distortion rate from raw events to event representation. Two experiments were designed to test this claim: (1) analyzing  <ref type="bibr" target="#b51">[50]</ref> and Mixed-Density Event Stacks <ref type="bibr" target="#b34">[33]</ref> with an increasing number of channels. (bottom) Effect of applying Gaussian blur with different blur kernels to the event representation.</p><p>the behavior of the metric when varying the number of bins in Voxel Grid <ref type="bibr" target="#b51">[50]</ref> and Mixed-Density Event Stack (MDES) <ref type="bibr" target="#b34">[33]</ref> representations, and (2) blurring the event representation with increasing standard deviations before measuring the GWD. We perform experiments on the validation set of Gen1 <ref type="bibr" target="#b9">[9]</ref>, and report the results in Fig. <ref type="figure" target="#fig_2">4</ref>.</p><p>Fig. <ref type="figure" target="#fig_2">4</ref> (top) confirms that the GWD decreases as the number of channels increases in both Voxel Grids and Mixed-Density Event Stacks. This aligns with our intuition that using more channels preserves more information from raw events, resulting in a lower distortion rate. Similarly, (bottom) shows that as the blur radius increases, more edges in the event representation are removed, causing an increase in the GWD. This verifies that the GWD measures the distortion from raw events to event representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Validation on Object Detection</head><p>Next, we investigate the relationship between GWD and the task performance of a NN trained for object detection. We choose two widely used object detection datasets, the Gen1 [9] and 1 Mpx <ref type="bibr" target="#b38">[37]</ref> Automotive Detection Dataset which both deal with event streams featuring labeled bounding boxes for pedestrians, cars, and other vehicles. While the former has a resolution of 304 ? 240, the latter has a resolution of 1280 ? 720. We report the GWD computed over the validation set of each dataset, and then train an off-the-shelf object detection framework based on YOLOv6 <ref type="bibr" target="#b24">[24]</ref>, pre-trained on the Microsoft Common Objects in Context (MS-COCO) <ref type="bibr" target="#b27">[27]</ref>, on different input representations as in <ref type="bibr" target="#b13">[13]</ref>. To accommodate the varying number of channels, we replace the 3-channel input convolution with a N c = 12 channel input convolution, where N c represents the number of channels in the representation. To show the generality of the result, we also vary the detection  backbone between ResNet-50 <ref type="bibr" target="#b17">[17]</ref>, EfficientRep <ref type="bibr" target="#b24">[24]</ref> and Swin Transformer V2 <ref type="bibr" target="#b29">[29]</ref>, and report results for each. Representations: We test a range of common representations listed below. We compute 12 channels for each representation, except for the 2D Histogram which has 2. Voxel Grid: Here the event time window is split into N c equal, non-overlapping time windows, and then events in the same time window are aggregated by summing their polarity on a per-pixel basis using bilinear voting <ref type="bibr" target="#b51">[50]</ref>. Mixed-Density Event Stack (MDES): For each channel c, this representation selects the most recent Ne 2 c events, where N e are the events in the time window. For each window, it aggregates the polarity at that pixel <ref type="bibr" target="#b34">[33]</ref>. Event Histograms: Events in the time window are first split by polarity, and then summed into two channels <ref type="bibr" target="#b30">[30]</ref>. Time Surface: We convert events into time surfaces from <ref type="bibr" target="#b22">[22]</ref> with a decay constant of ? = 5ms. We then sample it at Nc 2 = 6 equally spaced timestamps, once for positive and negative events resulting in N c = 12 channels. TORE: The Time-Ordered Recent Event Volumes store event timestamps in per-pixel queues. We use queues with capacity Nc 2 = 6, one for positive and one for negative events, and concatenate them to N c = 12 channels. Training Details We adopt the training procedure from YOLOv6 <ref type="bibr" target="#b24">[24]</ref>. For each backbone, representation, and dataset we train for 100 epochs, using Stochastic Gradient Descent with Nesterov momentum increasing from 0.5 to 0.83 over the first 2 epochs. We use a batch size of 32, and a Cosine learning rate schedule, starting at 0.00323 and ending at 0.000387 after 100 epochs. We adopt the classification and box regression losses in <ref type="bibr" target="#b24">[24]</ref>. Results: Figs. 5 summarizes the results of the above experiments. For both datasets, Gen1 and 1 Mpx, and all backbones, there is a clear correlation between the GWD and the task performance, i.e. task performance increases as GWD decreases. This conclusion holds for all three network backbones. In particular, MDES with a Swin-V2 detection backbone consistently achieves the highest mAP with 0.43 on Gen1 and 0.39 on 1 Mpx. It also consistently achieves the lowest GWD with 0.38 on Gen1, and 0.40 on 1 Mpx. We also see that the Swin V2 backbone outperforms other backbones on both datasets, for all tested representations. We conclude, that while the representation affects task performance, the neural network also has an influence. However, we see that the overall ranking of the representations is preserved.</p><p>Using Fewer Samples While in the previous section, we reported the GWD over the validation set of the Gen1 and the 1 Mpx datasets, averaging this metric over such a large dataset still incurs a high computational cost and would make such a metric infeasible for optimization. Therefore, here we investigate if we can use smaller sample sizes to speed up the computation of GWD. In Fig. <ref type="figure" target="#fig_5">6</ref> we show the GWD for the representations in Sec. 4.2, while varying the number of samples. We see that as the sample number decreases, the mean values of the metric change, but the ranking between representations is still preserved reliably after around 100 samples, and thus we use GWD 100 to optimize over representations. Below this number, the ranking of representations can be seen to change. However, we found that this happens due to a bad convergence of Eq. ( <ref type="formula" target="#formula_6">5</ref>). However, this is averaged out when considering larger sample sizes.</p><p>Timing Results: We time the computation of the GWD over 100 samples of the Gen1 validation set, where each sample comprises 50,000 events. We run our experiment on an AMD EPYC 7702 32-core server-grade CPU with 32 GB RAM and achieve a runtime of 15 minutes. Note that computing the GWD does not require a GPU. By contrast, training the models in Fig. <ref type="figure" target="#fig_4">5</ref> requires 2 days on a single Tesla V100 GPU with 32 GB of memory, making GWD computation 192 times faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Optimization of Event Representations</head><p>Here we report the results of optimizing the event representation according to the procedure in Sec. 3.3. We optimize N c = 12 channels, and at each optimization cycle for channel c the Bayesian Optimizer selects 100 configurations and then keeps the best-performing configuration.</p><p>We show the result of this optimization procedure in Fig. <ref type="figure" target="#fig_6">7</ref>. The left shows how the GWD decreases as new channels are added to the representation. We see that after 2 channels, our method outperforms 2D Histograms, after 6 channels we outperform Time Surfaces, after 7 channels we outperform Voxel Grids / TORE, after 9 channels we outperform Mixed-Density Event Stacks, and finally, after 12 channels, we achieve a GWD 100 of 0.47. On the right, we show the different windows (columns) and measurement functions (rows) that are selected. We do not show the order since our representation is unique up to a random channel permutation. However, in the appendix, we show which features are selected at each stage. We see that all windows and all measurement functions are selected at least once, showing how our representation tries to diversify as much as possible. Moreover, timestamp-based measurements often show multiple aggregations, which we argue are necessary to replicate their complex continuous signal.</p><p>Comparison with State-of-the-Art Here we compare our method against state-of-the-art recurrent and feedforward methods on the Gen1 and 1 Mpx test sets. We summarize the results in Tab. 1. Recurrent methods include MatrixLSTM+YOLOv3 <ref type="bibr" target="#b4">[5]</ref>, which uses a recurrent, learned input representation with a YOLOv3 <ref type="bibr" target="#b42">[41]</ref> detector, E2VID+RetinaNet <ref type="bibr" target="#b41">[40]</ref> which uses recurrent E2VID reconstructions with a RetinaNet <ref type="bibr" target="#b26">[26]</ref> detector, RVT-B which uses a recurrent transformer <ref type="bibr" target="#b16">[16]</ref>, and RED <ref type="bibr" target="#b38">[37]</ref>, a recurrent CNN. Feedforward methods are grouped into graphbased methods NVS-S <ref type="bibr" target="#b25">[25]</ref>, AEGNN <ref type="bibr" target="#b45">[44]</ref> and EAGR <ref type="bibr" target="#b15">[15]</ref>, sparse method AsyNet <ref type="bibr" target="#b31">[31]</ref>, spiking method Spiking DenseNet <ref type="bibr" target="#b8">[8]</ref>, and finally feedforward dense methods Events+RetinaNet <ref type="bibr" target="#b38">[37]</ref>, Events+SSD <ref type="bibr" target="#b19">[19]</ref> Events+RRC <ref type="bibr" target="#b5">[6]</ref>, and Events+YOLOv3 <ref type="bibr" target="#b20">[20]</ref> which use events as input, and use the RetinaNet <ref type="bibr" target="#b26">[26]</ref>, SSD <ref type="bibr" target="#b28">[28]</ref>, RRC <ref type="bibr" target="#b43">[42]</ref> or YOLOv3 <ref type="bibr" target="#b42">[41]</ref> detector repectively.</p><p>We compare these methods to our best-performing YOLOv6 detector from Sec. 4.2 with a SwinV2 transformer backbone, trained on various input representations. These include the ones analyzed in Sec. 4.2, i.e. the 2D Histogram, Time Surface, TORE, Voxel Grid, and MDES, as well as the optimized representation ERGO-12. Note that these meth- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Event Repr. Recurrent mAP? Gen1 <ref type="bibr" target="#b9">[9]</ref> 1 Mpx [37] MatrixLSTM+YOLOv3 <ref type="bibr" target="#b4">[5]</ref> MatrixLSTM 0.310 -E2VID+RetinaNet <ref type="bibr" target="#b41">[40]</ref> Reconstructions 0.270 0.250 RED <ref type="bibr" target="#b38">[37]</ref> Voxel Grid 0.400 0.430 RVT-B <ref type="bibr" target="#b16">[16]</ref> 2D Histogram 0.475 0.470 Events+RetinaNet <ref type="bibr" target="#b38">[37]</ref> Voxel Grid 0.340 0.180 Events+SSD <ref type="bibr" target="#b19">[19]</ref> 2D Histogram 0.301 0.340 Events+RRC <ref type="bibr" target="#b5">[6]</ref> 2D Histogram 0.307 0.343 Events+YOLOv3 <ref type="bibr" target="#b20">[20]</ref> 2D Histogram 0.312 0.346 AEGNN <ref type="bibr" target="#b45">[44]</ref> Graph 0.163 -EAGR <ref type="bibr" target="#b15">[15]</ref> Graph 0.321 -Spiking DenseNet <ref type="bibr" target="#b8">[8]</ref> Spike Train 0.189 -AsyNet <ref type="bibr" target="#b31">[31]</ref> 2D  <ref type="table">1</ref>: Comparison of state-of-the-art event-based object detectors on Gen1 <ref type="bibr" target="#b9">[9]</ref> and 1 Mpx <ref type="bibr" target="#b38">[37]</ref>. ods do not include data augmentation. We also trained our model with Mixup and Mosaic augmentation from <ref type="bibr" target="#b24">[24]</ref>, and is indicated by an asterisk * in Tab. 1. Results On the Gen1 dataset, we see that YOLOv6 with the SWINv2 backbone and ERGO-12 input and data augmentation outperforms all state-of-the-art methods by up to 2.9% mAP by achieving an mAP of 0.504. The runner-up is RVT-B, which uses a recurrent vision transformer. Even without data augmentation, our network with ERGO-12 achieves 0.493, which is 1.8% higher than RVT-B. Compared to the other feed-forward methods based on YOLOv6, ERGO-12 achieves an 8.7% higher mAP, the next best being YOLOv6 with MDES <ref type="bibr" target="#b34">[33]</ref> with 0.406. This large increase highlights the benefit of the optimized representation, which seems to have captured significantly more information about the events than the MDES. When comparing different feedforward detectors with 2D Histograms as input, we also see that the YOLOv6 detector with 0.339 achieves a 2.2% higher performance than the runner-up, Events+YOLOv3 with a YOLOv3 detector which achieves 0.312.</p><p>On 1 Mpx, the best-performing method is RVT-B with an mAP of 0.470, followed by RED with 0.430. From the feed-forward methods YOLOv6 with ERGO-12 and data augmentation achieves the highest score with 0.406, outper-forming runner-up method YOLOv6 with MDES by 2.5% mAP. Even without augmentation, ERGO-12 achieves a 1.9% higher score than YOLOv6 with MDES. Compared to state-of-the-art feed-forward methods, ERGO-12 with data augmentation achieves a 6% higher score, the runner-up being Events+YOLOv3 with 0.346. On this dataset, recurrent methods are known to perform better since many sequences include stops or slow-motion scenarios. This is challenging for feed-forward methods since they are not able to maintain long-term memory, explaining the difference between our method and RVT-B and RED which use LSTM blocks in their architecture. Finally, the improvement of ERGO-12 on 1 Mpx is not as high as that on Gen1. We believe that on this dataset, recurrency plays a much bigger role than on Gen1 due to frequent stops or slow-motion sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>State-of-the-art event-based deep learning methods typically need to convert raw events into dense input representations before they can be processed by standard networks. However, selecting this representation is very expensive, since it requires training a separate neural network for each representation and comparing the validation scores. In this work, we circumvent this bottleneck by measuring the quality of event representations with the Gromov Wasserstein Discrepancy (GWD), which is 200 times faster to compute. We validated extensively on multiple datasets and neural network backbones that the performance of neural networks trained with a representation perfectly correlates with its GWD. We then used this metric to, for the first time, optimize over a large family of representations, revealing a new, powerful representation, ERGO-12. With it, we outperform state-of-the-art representations by 1.9% mAP on the 1 Mpx dataset and 8.6% mAP on the Gen1 dataset. We even outperform the state-of-the-art by 1.8% mAP on Gen1 and state-of-the-art feed-forward methods by 6.0% mAP on the 1 Mpx dataset. This work thus opens a new unexplored field of explicit representation optimization that will push the limits of event-based learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgment</head><p>This work was supported by the Swiss National Science Foundation through the National Centre of Competence in Research (NCCR) Robotics (grant number 51NF40 185543), and the European Research Council (ERC) under grant agreement No. 864042 (AGILE-FLIGHT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Appendix</head><p>Here we add additional qualitative results and proofs to support the work in the main manuscript. We will refer to sections, equations, figures, and tables in the main manuscript with the prefix "M-", while referring to those in the appendix with "A-". We start by providing additional details about ERGO-12 in Sec. A-7.1 and then include two proofs regarding the robustness of Gromov-Wasserstein Discrepancy (GWD) in Sec. A-7.2. Afterward, we provide more results with fewer optimized channels in Sec. A-7.3. Finally, we show the qualitative results of our method on the Gen1 and 1 Mpx datasets in Sec. A-7.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Additional Details on ERGO-12</head><p>We provide more details of our optimized representation in Fig. <ref type="figure">A-9</ref>. As can be seen from the top sub-figure, we show the optimized channels in more detail than in figure M-7. At each new step, there is a decrease in GWD, which demonstrates that additional channels reduce the distance. We calculated GWD on the Gen1 <ref type="bibr" target="#b9">[9]</ref> validation dataset, which contained 100 samples, and plotted the results as dashed horizontal lines for chosen representations. The blue line shows the performance of the optimization process after each channel addition. We can observe that, for example, our optimized representation outperforms the Voxel Grid after seven channels and MDES after nine channels. Furthermore, we found that the optimization process initially selected the time function, which capitalizes on the high temporal resolution of event cameras to minimize GWD. Subsequently, counts and polarity were used.</p><p>In the bottom sub-figure of Fig. <ref type="figure">A-9</ref>, we visualize the channels of ERGO-12 (our optimized representation after 12 channels). For the purpose of visualization, we min-max normalized the channels within the range of 0-255. Each channel emphasizes different parts of the image. For instance, the last channel highlights the left edges of the pedestrian, while the seventh channel emphasizes the right part. Our optimization process enables us to capture as much information as possible at different scales and resolutions (spatial and temporal), which is highly advantageous when training with common object detectors. The optimized representation achieves an mAP of over 50% on the Gen1 dataset, and it represents the first non-recurrent neural network architecture that scores over 40% mAP on the 1 Mpx [37] dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Invariances of the GWD for Events</head><p>In this section, we will go over some basic properties of the GWD for events. In particular, we will show that it is invariant to affine feature transformations, concatenation with a constant, and duplication of the features. For clarity, we repeat here the definition of the GWD for events, following Eq. M-5:</p><formula xml:id="formula_18">L(E, F) = min T i,j,k,l T ij T kl L(C e ik , C f jl )<label>(14)</label></formula><p>with similarity matrices for Eqs. M-7 and M-8.</p><formula xml:id="formula_19">C e ik = e -e i -e k 2 2h 2 ? 2 e , C f jl = e -fx j -fx l 2 2h 2 ? 2 f (15) ? 2 e = mean i&lt;j e i -e j 2 , ? 2 f = mean i&lt;j f xi -f xj 2 . (<label>16</label></formula><formula xml:id="formula_20">)</formula><p>Affine transformation: We expect that if we apply an affine transformation to the event representation, the score should not change since information in the representation still remains distinctive. Moreover, we do not want the GWD to be sensitive to the scale of the feature. We see that replacing representation features with</p><formula xml:id="formula_21">f * x = af x + b<label>(17)</label></formula><p>changes only the similarity matrix C f jl to</p><formula xml:id="formula_22">C f, * jl = exp -f * xj -f * x l 2 2h 2 ? 2, * f .<label>(18)</label></formula><p>We see that the norms and data-dependent variances then transform as</p><formula xml:id="formula_23">f * xj -f * x l 2 = a 2 f xj -f x l 2<label>(19)</label></formula><formula xml:id="formula_24">? 2, * f = a 2 ? 2 f (<label>20</label></formula><formula xml:id="formula_25">)</formula><p>We thus see that</p><formula xml:id="formula_26">C f, * jl = exp -f * xj -f * x l 2 2h 2 ? 2, * f (21) = exp -a 2 f xj -f x l 2 2h 2 a 2 ? 2 f (22) = exp -f xj -f x l 2 2h 2 ? 2 f (<label>23</label></formula><formula xml:id="formula_27">)</formula><p>= C f jl <ref type="bibr" target="#b24">(24)</ref> which shows that the similarity matrix does not change. The minimizer of Eq. M-5 thus also does not change, which means the GWD is invariant to this affine transformation. This invariance is only possible through the use of a datadependent variance, and thus highlights its advantage. Invariances to Concatenation In the case of concatenation, we consider the following transformation:</p><formula xml:id="formula_28">f * x = [f x c x ]<label>(25)</label></formula><p>where [. .] denotes concatenation, and c x ? R C denotes a pixel dependent additional feature. Again, we find that only the similarity matrix C f jl is affected, and in particular, only the norm and variance which become:</p><formula xml:id="formula_29">f * xj -f * x l 2 = f xj -f x l 2 + c xj -c x l 2<label>(26)</label></formula><formula xml:id="formula_30">? 2, * f = ? 2 f + mean i&lt;j c xi -c xj 2<label>(27)</label></formula><p>We will consider two special cases: c x = c, a constant vector, and c x = f x the same feature. In the first case, the additional terms above become 0, meaning that the norm does not change, and thus the metric stays the same. In the second case, the norm transforms as in the affine case, multiplying the squared norm and variance by 2. For the same reasons as before, the metric also stays the same. Generalizing this result to more general c x remains future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Fewer optimized channels</head><p>Figure <ref type="figure">8</ref> depicts a correlation between the GWD (given on the x-axis, computed on the GEN1 validation dataset with 100 samples) and the task performance (mAP on object detection task). Since the Swin V2 backbone outperforms all other backbones, it is the only backbone shown in the plot, and the 2D Histogram, which is the poorestperforming method, is omitted. The results demonstrate that our optimized representation with nine and seven channels performs better than MDES and Voxel Grid, respectively, which is consistent with the findings in Figure <ref type="figure">9</ref>. Furthermore, we observe that the results on 1 Mpx correlate with GWD computed on the GEN1 validation dataset with 100 samples, which highlights the generalization capabilities of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Qualitative results</head><p>We present qualitative results on the Event-based Object Detection task for both the 1 Mpx and Gen1 datasets, as illustrated in Figs. <ref type="figure">10</ref> and<ref type="figure">11</ref>, respectively. Our approach exhibits the ability to detect objects that are not present in the ground truth but can be seen with event image visualizations. Figure <ref type="figure">8</ref>: Correlation of the Gromov-Wasserstein Discrepancy with the mAP (higher is better) for object detection on Gen1 <ref type="bibr" target="#b9">[9]</ref> (top) and 1 Mpx <ref type="bibr" target="#b38">[37]</ref> (bottom) datasets. ERGO-12, ERGO-9, and ERGO-7 represent our optimized representations with twelve, nine, and seven channels. The mAP is reported on the test set (without augmentation), while the Gromov-Wasserstein Discrepancy is reported on the Gen1 validation dataset with 100 chosen samples.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of the Gromov-Wasserstein Discrepancy (GWD) between raw events and representations. Events E are converted to event representations, i.e. a set of features F at pixel locations x. It is defined as the solution to an optimal transport problem which transports events pairs (e i , e k ) to feature pairs (f xj , f x l ) via transport plan T ij , T kl . If the transport plan preserves the similarities C e ik and C f jm between event and feature pairs, this results in a low GWD.</figDesc><graphic url="image-21.png" coords="3,103.49,76.06,150.03,98.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Overview of the hyperparameters we use to construct an event representation (right). For each channel c, we select one of several event time windows w c ? W (in color, left), measurement functions m c ? M (timestamp, polarity, positive timestamps, etc.), and aggregation functions a c ? A (max, mean, sum, variance), resulting in 3N c parameters.</figDesc><graphic url="image-23.png" coords="5,121.23,98.35,99.54,66.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Validation of our metric on the Gen1 validation set. (top) Our metric for Voxel Grids<ref type="bibr" target="#b51">[50]</ref> and Mixed-Density Event Stacks<ref type="bibr" target="#b34">[33]</ref> with an increasing number of channels. (bottom) Effect of applying Gaussian blur with different blur kernels to the event representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Correlation of the Gromov-Wasserstein Discrepancy with the mAP (higher is better) for object detection on the Gen1<ref type="bibr" target="#b9">[9]</ref> (top) and 1 Mpx<ref type="bibr" target="#b38">[37]</ref> (bottom) datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Gromov-Wasserstein Discrepancy for a number of samples from the validation set.</figDesc><graphic url="image-27.png" coords="7,60.68,72.00,212.63,159.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Gromov-Wasserstein Discrepancy for 100 samples (left). At each channel, a Bayesian optimizer selects the next best hyperparameter triple. The chosen hyperparameters are broken down by window and measurement function (right).</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In the optimal transport literature, this correspondence is also called a transport plan.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predictions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-Truth</head><p>Figure <ref type="figure">10</ref>: Qualitative results of our method with ERGO-12 input on the 1 Mpx <ref type="bibr" target="#b38">[37]</ref> dataset. (top row) predictions, and (bottom row) ground truth. Note that sometimes our method detects objects that do not appear in the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predictions Ground-Truth</head><p>Figure <ref type="figure">11</ref>: Qualitative results of our method with ERGO-12 input on the Gen1 <ref type="bibr" target="#b9">[9]</ref> dataset. (top row) predictions, and (bottom row) ground truth. Note that sometimes our method detects objects that do not appear in the ground truth.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">EV-SegNet: Semantic segmentation for event-based cameras</title>
		<author>
			<persName><forename type="first">I?igo</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPRW</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Time-ordered recent event (tore) volumes for event cameras</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">W</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruixu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Bakheet Almatrafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vijayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keigo</forename><surname>Asari</surname></persName>
		</author>
		<author>
			<persName><surname>Hirakawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2519" to="2532" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Alhabib Abbas, Eirina Bourtsoulatze, and Yiannis Andreopoulos. Graph-based object classification for neuromorphic vision sensing</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Chadha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Alhabib Abbas, Eirina Bourtsoulatze, and Yiannis Andreopoulos. Graph-based spatio-temporal feature learning for neuromorphic vision sensing</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Chadha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="9084" to="9098" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A differentiable recurrent surface for asynchronous event-based data</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Cannici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Ciccone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Romanoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Matteucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pseudo-labels for supervised learning on dynamic vision sensor data, applied to object detection under ego-motion</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Y</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPRW</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exponential-wrapped distributions on symmetric spaces</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Chevallier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Didong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dunson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Mathematics of Data Science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1347" to="1368" />
			<date type="published" when="2004">12 2022. 4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Visualization of the channels of ERGO-12, min-max normalized in the range 0-255. The channels are ordered in row-major order, and the hyperparameters selected are shown in the top left of each subfigure</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object detection with spiking neural networks on automotive event data. Int. Joint Conf</title>
		<author>
			<persName><forename type="first">Loic</forename><surname>Cordone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Miramond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Thierion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw. (IJCNN)</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A large scale event-based detection dataset for automotive</title>
		<author>
			<persName><forename type="first">Davide</forename><surname>Pierre De Tournemire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Nitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Perot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amos</forename><surname>Migliore</surname></persName>
		</author>
		<author>
			<persName><surname>Sironi</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>arXiv e-prints, abs/2001.08499, 2020. 6, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A voxel graph cnn for object classification with event cameras</title>
		<author>
			<persName><forename type="first">Yongjian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youfu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Pstnet: Point spatio-temporal convolution on point cloud sequences</title>
		<author>
			<persName><forename type="first">Hehe</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohan</forename><surname>Kankanhalli</surname></persName>
		</author>
		<idno>2021. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Event-based vision: A survey</title>
		<author>
			<persName><forename type="first">Guillermo</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobi</forename><surname>Delbruck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garrick</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiara</forename><surname>Bartolozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Taba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Censi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?rg</forename><surname>Conradt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da-Vide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end learning of representations for asynchronous event-based data</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Loquercio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<imprint>
			<date type="published" when="2006">2019. 3, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">photometric feature tracking using events and frames</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henri</forename><surname>Rebecq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillermo</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
		<author>
			<persName><surname>Asynchronous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference of Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="766" to="781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Pushing the limits of asynchronous graph-based object detection with event cameras. arXiv</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent vision transformers for object detection with event cameras</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gryffin: An algorithm for bayesian optimization of categorical variables informed by expert knowledge</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>H?se</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Aldeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riley</forename><forename type="middle">J</forename><surname>Hickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lo?c</forename><forename type="middle">M</forename><surname>Roch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al?n</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Physics Reviews</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">31406</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards event-driven object detection with off-the-shelf deep learning</title>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Iacono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arren</forename><surname>Glover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiara</forename><surname>Bartolozzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IROS</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mixed frame-/event-driven fast pedestrian detection</title>
		<author>
			<persName><forename type="first">Zhuangyi</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Stechele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenshan</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alois</forename><surname>Knoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">N-imagenet: Towards robust, fine-grained object recognition with event cameras</title>
		<author>
			<persName><forename type="first">Junho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehyeok</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gangin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<imprint>
			<date type="published" when="2003">October 2021. 3</date>
			<biblScope unit="page" from="2146" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">HOTS: A hierarchy of event-based time-surfaces for pattern recognition</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Lagorce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garrick</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Gallupi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bertram</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryad</forename><surname>Benosman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1346" to="1359" />
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Training deep spiking neural networks using backpropagation</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Haeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Tobi</forename><surname>Delbruck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Pfeiffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Neurosci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">508</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Yolov6: A single-stage object detection framework for industrial applications</title>
		<author>
			<persName><forename type="first">Chuyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lulu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongliang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiheng</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaidan</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiduo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph-based asynchronous event processing for rapid object recognition</title>
		<author>
			<persName><forename type="first">Yijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bangbang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaopeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guofeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference of Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference of Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Swin transformer v2: Scaling up capacity and resolution</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Event-based vision meets deep learning on steering prediction for self-driving cars</title>
		<author>
			<persName><forename type="first">Ana</forename><forename type="middle">I</forename><surname>Maqueda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Loquercio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillermo</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narciso</forename><surname>Garc?a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Event-based asynchronous sparse convolutional networks</title>
		<author>
			<persName><forename type="first">Nico</forename><forename type="middle">A</forename><surname>Messikommer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Loquercio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference of Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Moving object detection for event-based vision using graph spectral clustering</title>
		<author>
			<persName><forename type="first">Anindya</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName><surname>Shashant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jhony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Giraldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananda</forename><forename type="middle">S</forename><surname>Bouwmans</surname></persName>
		</author>
		<author>
			<persName><surname>Chowdhury</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="876" to="884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stereo depth from events cameras: Concentrate and focus on the future</title>
		<author>
			<persName><forename type="first">Yeongwoo</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Mostafavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuk-Jin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008">June 2022. 2, 3, 5, 6, 8</date>
			<biblScope unit="page" from="6114" to="6123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">HFirst: A temporal approach to object recognition</title>
		<author>
			<persName><forename type="first">Garrick</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cedric</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Etienne-Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Thakor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryad</forename><surname>Benosman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2028" to="2040" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Intrinsic Statistics on Riemannian Manifolds: Basic Tools for Geometric Measurements</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Pennec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Imaging Vis</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="154" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Mapping from frame-driven to frame-free event-driven vision systems by low-rate rate coding and coincidence processing-application to feedforward ConvNets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Perez-Carrasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bego?a</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teresa</forename><surname>Acha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shouchun</forename><surname>Serrano-Gotarredona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernab?</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Linares-Barranco</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-11">Nov. 2013</date>
			<publisher>IEEE</publisher>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2706" to="2719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to detect objects with a 1 megapixel event camera</title>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Perot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>De Tournemire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Nitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amos</forename><surname>Sironi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst. (NeurIPS)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2009">2020. 3, 6, 7, 8, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Gromov-Wasserstein Averaging of Kernel and Distance Matrices</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Peyr?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2016</title>
		<imprint>
			<date type="published" when="2004">June 2016. 4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Point-Net++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Adv. Neural Inf. Process. Syst. (NeurIPS)</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="5099" to="5108" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">High speed and high dynamic range video with an event camera</title>
		<author>
			<persName><forename type="first">Henri</forename><surname>Rebecq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Accurate single stage detector using recurrent rolling convolution</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Riemannian gaussian distributions on the space of symmetric positive definite matrices</title>
		<author>
			<persName><forename type="first">Salem</forename><surname>Said</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lionel</forename><surname>Bombrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannick</forename><surname>Berthoumieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Manton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2153" to="2170" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">AEGNN: Asynchronous event-based graph neural networks</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Schaefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2022. 2, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">EventNet: Asynchronous recursive event processing</title>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Sekikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kosuke</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideo</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">HATS: Histograms of averaged time surfaces for robust event-based object classification</title>
		<author>
			<persName><forename type="first">Amos</forename><surname>Sironi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuele</forename><surname>Brambilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Bourdis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Lagorce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryad</forename><surname>Benosman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1731" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Event-based high dynamic range image and very high frame rate video generation using conditional generative adversarial networks</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Mohammad</forename><surname>Mostafavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Yo-Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuk-Jin</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005">June 2019. 2, 3, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Feedforward categorization on AER motion events using cortex-like features in a spiking neural network</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoxi</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoushun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernabe</forename><surname>Linares-Barranco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajin</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1963" to="1978" />
			<date type="published" when="2002">Sept. 2015. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">EV-FlowNet: Self-supervised optical flow estimation for event-based cameras</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Zihao Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Chaney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems (RSS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Unsupervised event-based learning of optical flow, depth, and egomotion</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Zihao Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Chaney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006">2019. 2, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
