<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AnoPCN: Video Anomaly Detection via Deep Predictive Coding Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Muchao</forename><surname>Ye</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">Also with South</orgName>
								<orgName type="institution" key="instit1">China University of Technology</orgName>
								<orgName type="institution" key="instit2">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weihao</forename><surname>Gan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<email>yu.qiao@siat.ac.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Shenzhen Key Lab of C.V.P.R. and SIAT-SenseTime Joint Lab, SIAT, Chinese Academy of Sciences Xiaojiang Peng Shenzhen Key Lab of C.V.P.R. and SIAT-SenseTime Joint Lab</orgName>
								<orgName type="institution">SIAT</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">Shenzhen Key Lab of C.V.P.R. and SIAT-SenseTime Joint Lab</orgName>
								<orgName type="institution">SIAT</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AnoPCN: Video Anomaly Detection via Deep Predictive Coding Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B691DDF10F31040DCC80D3930AB61572</idno>
					<idno type="DOI">10.1145/3343031.3350899</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>video anomaly detection</term>
					<term>video generation</term>
					<term>predictive coding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video anomaly detection is a challenging problem due to the ambiguity and complexity of how anomalies are defined. Recent approaches for this task mainly utilize deep reconstruction methods and deep prediction ones, but their performances suffer when they cannot guarantee either higher reconstruction errors for abnormal events or lower prediction errors for normal events. Inspired by the predictive coding mechanism explaining how brains detect events violating regularities, we address the Anomaly detection problem with a novel deep Predictive Coding N etwork, termed as AnoPCN, which consists of a Predictive Coding Module (PCM) and an Error Refinement Module (ERM). Specifically, PCM is designed as a convolutional recurrent neural network with feedback connections carrying frame predictions and feedforward connections carrying prediction errors. By using motion information explicitly, PCM yields better prediction results. To further solve the problem of narrow regularity score gaps in deep reconstruction methods, we decompose reconstruction into prediction and refinement, introducing ERM to reconstruct current prediction error and refine the coarse prediction. AnoPCN unifies reconstruction and prediction methods in an end-to-end framework, and it achieves state-of-theart performance with better prediction results and larger regularity score gaps on three benchmark datasets including ShanghaiTech Campus, CUHK Avenue, and UCSD Ped2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Computing methodologies → Computer vision; Scene anomaly detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Video anomaly detection (VAD) refers to the identification of events that deviate normal scenarios. VAD empowers machines to detect abnormal events such as traffic accidents and criminal activities autonomously, which has received increasing attention in recent years for its significant value in surveillance and security. Although great progresses have been made in VAD <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref>, it remains as a challenging problem due to the ambiguity and complexity of anomalies in different scenes.</p><p>With the remarkable success of deep neural networks (DNNs) in various computer vision tasks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>, VAD approaches have evolved from using hand-crafted features <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b45">46]</ref> to using deep learning models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b44">45]</ref>. Previous deep VAD methods for modeling normal events can be categorized into two types, namely the reconstruction-based framework <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b44">45]</ref> and the prediction-based framework <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b46">47]</ref>. They both regard events which do not fit into the normality models as abnormal. Although both deep reconstruction methods and deep prediction ones have achieved impressive results on public datasets, they still suffer from the following limitations:</p><p>(1) As depicted in Figure <ref type="figure" target="#fig_0">1</ref>, some deep prediction methods cannot guarantee small prediction errors for normal events as they do not use motion information explicitly <ref type="bibr" target="#b22">[23]</ref>, which impedes better frame prediction results for VAD.</p><p>(2) As demonstrated by <ref type="bibr" target="#b22">[23]</ref>, deep reconstruction networks can well reconstruct abnormal events in practice owing to their high representation capacity, which leads to narrow regularity score gaps between normality and anomaly.</p><p>In this paper, we propose a deep Predictive Coding N etwork for Anomaly detection, termed as AnoPCN, to alleviate the limitations of reconstruction framework and prediction framework. Specifically, AnoPCN has a convolutional recurrent architecture with two basic modules, namely the Predictive Coding Module (PCM) and the Error Refinement Module (ERM). PCM imitates the predictive coding mechanism in neuroscience <ref type="bibr" target="#b30">[31]</ref> to generate frame prediction, which explicitly uses an effective motion cue called RGB difference <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b41">42]</ref>. ERM enlarges the regularity score gaps for deep reconstruction methods by solely reconstructing the prediction error yielded by PCM, which obtains a reconstructed frame by refining the coarse predicted frame of PCM with the reconstructed prediction error.</p><p>Designing PCM in the principle of predictive coding is inspired by recent neuroscience studies <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>, which discover that predictive coding mechanism strongly connects with the identification of visual mismatch in human cognition. Such a discovery motivates us to design a novel prediction network named PCM for the identification of video anomaly. Specifically, PCM acquires prediction frame from RGB difference in a predictive coding manner. It is a convolutional recurrent network using feedforward connections to process prediction errors (RGB differences) for frame prediction and exploiting feedback connections to carry predicted frames for prediction error calculation. This process is similar to how brains make perceptual inference <ref type="bibr" target="#b10">[11]</ref> by merely transmitting the prediction errors between predictions and sensory inputs.</p><p>Moreover, we release the problem of narrow regularity score gaps in previous deep reconstruction methods by decomposing reconstruction into two stages: prediction and refinement. When PCM yields a predicted frame, ERM takes as input the current prediction error from PCM and resorts to a deep encoder-decoder network to reconstruct the prediction error supervised by the current ground truth frame. By adding the reconstructed prediction error to the predicted frame, ERM addresses the problem of narrow regularity score gaps in existing reconstruction methods, which improves the performance of AnoPCN for VAD based on PCM.</p><p>Our main contributions can be summarized as follows: Firstly, we propose a novel VAD method, i.e. AnoPCN, which unifies prediction and reconstruction in an end-to-end framework. It exploits a video prediction model to generate frame predictions and a deep encoder-decoder network to reconstruct current prediction errors. Our treatment of decomposing reconstruction into prediction and refinement allows the reconstruction models to achieve better performance for VAD with wider regularity score gaps between normality and anomaly.  Secondly, inspired by the predictive coding theory <ref type="bibr" target="#b30">[31]</ref> in neuroscience, we formulate a new video prediction framework for VAD, namely the Predictive Coding Module (PCM). To the best of our knowledge, it is the first work that leverages predictive coding mechanism to design a DNN for VAD, which narrows the gap between DNNs and human brains in detecting anomalies.</p><p>Finally, our method achieves the state-of-the-art performance with better prediction results and larger regularity score gaps on three popular benchmark datasets, namely ShanghaiTech Campus <ref type="bibr" target="#b26">[27]</ref>, CUHK Avenue <ref type="bibr" target="#b24">[25]</ref>, and UCSD Ped2 <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Since it is very costly to collect and annotate different types of anomalies, a common strategy for VAD is to learn a normality representation model from normal videos and consider its outliers as anomalous. Owing to the superiority of deep convolutional features <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b44">45]</ref> over previous hand-crafted ones <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b43">44]</ref>, recent years have witnessed a research shift from traditional shallow methods to deep learning methods for VAD. In this section, we present deep VAD methods related to ours from the perspective of model characteristics, i.e. deep reconstruction methods and deep prediction methods. We also discuss the core idea of predictive coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep Reconstruction VAD Methods</head><p>Deep reconstruction VAD methods learn to reconstruct normal video data only and presume that video with high reconstruction error is abnormal. The work of <ref type="bibr" target="#b13">[14]</ref> is the first to demonstrate that convolutional autoencoder is able to model the temporal regularity of videos in an end-to-end deep neural network, and variants of autoencoder <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b42">43]</ref> thenceforth become popular among reconstruction-based techniques. In addition, Luo et al. <ref type="bibr" target="#b26">[27]</ref> reformulate traditional sparse coding method <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b45">46]</ref> for VAD in the framework of stacked RNNs.</p><p>As Figure <ref type="figure" target="#fig_2">2</ref>(a) shows, previous reconstruction-based models tend to reconstruct whole frames from scratch, but they sometimes meet  with the overfitting problem and can even reconstruct abnormal event videos well <ref type="bibr" target="#b22">[23]</ref>. As for our AnoPCN, it decomposes reconstruction into prediction and refinement stages. ERM just learns to reconstruct the prediction error made by PCM, which is a small part of original frames when PCM is well trained. Such a decomposition makes ERM less likely to be overfitting and achieve higher regularity score gaps for deep reconstruction VAD methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Prediction VAD Methods</head><p>Deep Prediction VAD methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b46">47]</ref> regard anomaly as unpredictable or unexpected event. Liu et al. <ref type="bibr" target="#b22">[23]</ref> achieve good performance in benchmark datasets with their prediction network, which manifests the potential of deep prediction methods for VAD. Zhao et al. <ref type="bibr" target="#b46">[47]</ref> design a prediction loss to improve the motion representation for feature extraction in VAD.</p><p>As Figure <ref type="figure" target="#fig_2">2</ref>(b) shows, deep prediction models need to capture the most salient appearance and motion features from previous frames to generate realistic future frames. Our PCM predicts future frames by convolutional LSTM <ref type="bibr" target="#b33">[34]</ref>, which is more natural to model long-term dynamics than the convolutional structure in <ref type="bibr" target="#b22">[23]</ref>. Moreover, PCM uses the RGB difference <ref type="bibr" target="#b41">[42]</ref> to capture the motion cue of two frames for video prediction, which does not require extra computation as optical flow does, so it will use motion information explicitly and efficiently for VAD task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Predictive Coding</head><p>We adopt the predictive coding viewpoint to design PCM for it can better bridge video prediction task and VAD task by imitating how human brains identify visual mismatches with prediction <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. Predictive coding is a neuroscience theory proposed by Rao and Ballard <ref type="bibr" target="#b30">[31]</ref>, which suggests that the brain is continuously generating predictions on sensory inputs, and the prediction errors between predictions and sensory inputs will be utilized to update the predictive estimator so as to produce a more accurate prediction in future. Several research works have tried to integrate convolutional neural network with predictive coding mechanism in the application of feature extraction <ref type="bibr" target="#b2">[3]</ref>, video prediction <ref type="bibr" target="#b23">[24]</ref>, and object detection <ref type="bibr" target="#b12">[13]</ref>.</p><p>PCM differs from the PredNet <ref type="bibr" target="#b23">[24]</ref> architecture by using a convolutional encoder-decoder structure to model the bottom-up connection of conveying prediction error for prediction generation and the top-down connection of conveying prediction for residual error calculation. The reason for this improvement is that the stacked recurrent networks in PredNet for prediction error propagation will accumulate prediction errors that should have been rectified and suppressed, which is redundant or even harmful for future frame prediction. To the best of our knowledge, we are the first to introduce the predictive coding mechanism for VAD task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED METHOD</head><p>We present AnoPCN to address the limitations of deep prediction methods and deep reconstruction methods for VAD by integrating them together. As illustrated in Figure <ref type="figure" target="#fig_4">3</ref>, AnoPCN has a deep recurrent architecture containing two important modules: Predictive Coding Module (PCM) and Error Refinement Module (ERM). We split reconstruction into two stages. First, PCM generates a prediction frame Ît from previous frames by imitating the predictive coding procedure in brains. Next, ERM reconstructs the prediction error E t and adds a refinement Êt to Ît to obtain a reconstructed frame Îr t . We provide the details of AnoPCN as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Predictive Coding Module</head><p>PCM Architecture. The neuroscience community has verified a predictive coding hypothesis <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> on how brains detect "surprising" events that violate their statistical regularities. Such an intrinsic coherence in the identification of visual anomaly between brains and deep prediction VAD methods drives us to design a novel prediction model for VAD in the principle of predictive coding. Figure <ref type="figure" target="#fig_6">4</ref>(a) shows a generic predictive coding framework <ref type="bibr" target="#b30">[31]</ref>: the predictive estimator makes a prediction before it receives actual sensory input, then the prediction is conveyed backward to generate an error signal. The error signal is fed forward to the predictive estimator to generate a more accurate prediction.</p><p>We formulate PCM in the predictive coding framework as Figure <ref type="figure" target="#fig_6">4</ref>(b) shows. The building blocks of our predictive estimator are convolutional LSTM (ConvLSTM) Unit <ref type="bibr" target="#b33">[34]</ref>, convolutional (Conv) Unit, and our Prediction Error Processing (PEP) Unit. The role of ConvLSTM Unit is to model the sequential dynamics between neighboring frames, and its hidden state is used by Conv Unit to generate a prediction frame which will be fed backward to obtain the prediction error (a RGB difference <ref type="bibr" target="#b41">[42]</ref>). The function of PEP Unit is to encode the prediction error and feed it into the ConvLSTM Unit to update its internal states, which models the feedforward connection in the predictive coding framework.</p><p>Given k consecutive previous frames {I t -k , I t -k+1 , • • • , I t -1 }, a prediction Ît for time step t is generated as follows. In each time  </p><formula xml:id="formula_0">step j ∈ {t -k, t -k + 1, • • • , t }, previous prediction error E j-1 =</formula><formula xml:id="formula_1">I j-1 -Îj-1 (E t -k-1 is set to 0) is encoded by the PEP Unit to extract its deep features, R j-1 = PEP(E j-1 ).<label>(1)</label></formula><p>Then R j-1 is fed into ConvLSTM Unit to rectify the prediction error between previous prediction Îj-1 and ground truth I j-1 , and Conv Unit generates a new prediction Îj after getting the hidden state of ConvLSTM Unit,</p><formula xml:id="formula_2">Îj = Conv(ConvLSTM(R j-1 )).<label>(2)</label></formula><p>By recursively using previous prediction error E j-1 in ConvL-STM Unit, prediction Ît will be generated finally.</p><p>ConvLSTM Unit. ConvLSTM <ref type="bibr" target="#b33">[34]</ref> is a special kind of LSTM that replaces fully-connected transformations with spatial convolutions. After the features of previous prediction error E j-1 is extracted by PEP Unit, they are put into the ConvLSTM Unit for the generation of an updated prediction Îj .</p><p>PEP Unit. PCM can be seen as an improved version of Pred-Net <ref type="bibr" target="#b23">[24]</ref> which is previously developed for video prediction task, and the main improvement is made by PEP Unit. PredNet uses stacked ConvLSTMs to extract deep prediction error features. However, as verified by our experiments, the recurrent property of ConvLSTM will preserve the unnecessary information of previous prediction errors {E t -k , • • • , E t -2 } in the prediction error representation R j-1 . Since they have been sent to ConvLSTM Unit before, they are not suitable for current rectification and should be suppressed in the current moment. In other words, the accumulation of previous prediction errors in PredNet will negatively affect the ConvLSTM Unit responsible for making prediction because of the redundant or even harmful information. Since previous prediction errors {E t -k , • • • , E t -2 } have been fed forward into the ConvLSTM Unit before, we just need to use convolutional networks to extract deep features of E t -1 for prediction in time step t, which is achieved by our PEP Unit. The design of the PEP Unit is shown in Figure <ref type="figure" target="#fig_7">5</ref>. It is a modified U-Net <ref type="bibr" target="#b32">[33]</ref> which allows PCM to preserve the property of extracting prediction error features by a multi-layer network with lateral connection.</p><p>Discussion. Combining ConvLSTM with our PEP Unit in PCM has the following advantages over previous deep prediction methods like <ref type="bibr" target="#b22">[23]</ref> for VAD: (1) ConvLSTM is more suitable for modeling sequential dynamics. ( <ref type="formula" target="#formula_2">2</ref>) By exploiting RGB difference, which is an efficient and helpful modality containing motion information <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b41">42]</ref>, PCM explicitly uses motion information for video prediction in anomaly detection. As shown in our experiments, these two enhancements help PCM generate higher PSNR for normal frames while keeping that of abnormal ones nearly unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Error Refinement Module</head><p>Reconstruction via Prediction and Refinement. As observed by Liu et al. <ref type="bibr" target="#b22">[23]</ref>, existing deep reconstruction VAD methods have suboptimal performance for they sometimes can well reconstruct abnormal frames due to the high representation capacity of deep neural networks, which is reflected by their narrow regularity score gaps ΔS between normal frames and abnormal ones,</p><formula xml:id="formula_3">ΔS = t ∈N S t T n -t ∈A S t T a ,<label>(3)</label></formula><p>where S t is the regularity score of frame t. N and A are the sequence number sets of normal frames and abnormal ones respectively, while T n and T a are the total numbers of normal frames and abnormal ones separately. Existing reconstruction models <ref type="bibr" target="#b13">[14]</ref> tend to directly reconstruct a stack of k frames I ∈ R k×h×w to incorporate motion information for frame reconstruction. To solve the problem of narrow ΔS, we propose a new strategy for reconstruction models by decomposing reconstruction into prediction and refinement. To be specific, given a stack of k + 1 frames {I t -k , I t -k+1 , • • • , I t }, a coarse prediction Ît for time step t is generated by only using the first k frames {I t -k , I t -k+1 , • • • , I t -1 } in the prediction stage, which has been accomplished by PCM. In the refinement stage, we reconstruct the current prediction error E t = I t -Ît in time step t by Error Refinement Module (ERM), which is introduced to get an estimation of E t in a reconstruction manner,</p><formula xml:id="formula_4">Êt = ERM(E t ).<label>(4)</label></formula><p>By operating an element-wise addition between Ît and Êt , a refined estimation of I t is obtained,</p><formula xml:id="formula_5">Îr t = Ît + Êt ,<label>(5)</label></formula><p>where Îr t is the reconstructed frame after the prediction and refinement treatments.</p><p>ERM Architecture. As illustrated in Figure <ref type="figure" target="#fig_7">5</ref>, ERM shares the same structure with PEP Unit except for the extra convolutional layer for outputting a reconstructed prediction error in RGB channels. By employing lateral connections to encourage feature reuse, ERM can circumvent the bottleneck information that convolutional autoencoder <ref type="bibr" target="#b13">[14]</ref> has. Besides, the consistency of using the same structure to process prediction error unifies PCM and ERM under the same computational operation, bridging the gap between prediction methods and reconstruction methods for VAD.</p><p>Discussion. Unlike the existing deep reconstruction methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b40">41]</ref> which reconstruct normal frames from scratch, ERM just learns to reconstruct a small part of regularity based on the prediction frame that PCM generates. Such a designation can prevent it from having an extreme capacity that could even well reconstruct abnormal frames. Experimental results show that AnoPCN has a larger ΔS than <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b22">[23]</ref>, which verifies the effectiveness of our decomposition strategy for reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Regularity Score</head><p>We take the normalized Peak Signal-to-Noise Ratio (PSNR) of Îr t as the regularity score S t of video frame I t as <ref type="bibr" target="#b22">[23]</ref> do. PSNR is an effective metric used for image quality assessment <ref type="bibr" target="#b28">[29]</ref>, and PSNR of video frame in time step t is PSN R t = 10 log 10 max (i,j) ( Îr t ) (i,j)</p><formula xml:id="formula_6">2 1 P I t -Îr t 2 2 ,<label>(6)</label></formula><p>where max</p><formula xml:id="formula_7">(i,j) ( Îr t ) (i,j)</formula><p>is the maximum value of image intensity elements, and P is the total number of pixels in I t .</p><p>The lower PSN R t is, the more likely I t contains anomaly. And the normalized PSN R t is then exploited to quantify the regularity of frame I t ,</p><formula xml:id="formula_8">S t = PSN R t -min t PSN R t max t PSN R t -min t PSN R t .<label>(7)</label></formula><p>Whether frame I t is abnormal or not can be judged by if S t is smaller than a given threshold. ΔS in (3) can be used to quantify the ability of discriminating anomaly and regularity for VAD techniques, which is the difference between the average regularity score of normal frames and that of abnormal ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Strategy</head><p>There are two sets of parameters to be optimized in the proposed method, i.e. θ P for PCM and θ R for ERM. The content and motion constraints as well as the adversarial training that we use to optimize θ P and θ R are similar to <ref type="bibr" target="#b22">[23]</ref>, which is discussed as follows.</p><p>Intensity Constraint L int . L int measures the pixel-wise difference between the predicted frame Î and the ground truth frame I , which guides Î to preserve most content information,</p><formula xml:id="formula_9">L int (I, Î ) = I -Î 2 2 . (<label>8</label></formula><formula xml:id="formula_10">)</formula><p>Gradient Constraint L д . To avoid a blurry prediction, it is necessary to use L д as a constraint for training,</p><formula xml:id="formula_11">L д (I, Î ) = (i,j) ( | Î(i,j) -Î(i-1,j) | -|I (i,j) -I (i-1,j) | 1 + | Î(i,j) -Î(i,j-1) | -|I (i,j) -I (i,j-1) | 1 ),<label>(9)</label></formula><p>where (i, j) denotes the spatial coordinates of the pixels.</p><p>Motion Constraint L of . L of requires Ît to have constant motion with previous ground truth frame I t -1 as I t does, which can be reflected by the optical flow loss. Denote f as the function to extract optical flow such as FlowNet <ref type="bibr" target="#b7">[8]</ref>,</p><formula xml:id="formula_12">L of (I t , Ît ) = f (I t , I t -1 ) -f ( Ît , I t -1 ) 2 2 . (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>Adversarial Training. We adopt adversarial training to generate a realistic-looking frame. The generator G is either PCM or ERM in the training phase, and the discriminator D has the same form as <ref type="bibr" target="#b15">[16]</ref> does. The generator loss L G GAN and the discriminator loss L D GAN keep the same with <ref type="bibr" target="#b22">[23]</ref> for the fairness of comparison in experiments.</p><p>Objective Functions. Note that PCM should have a good prediction ability first, otherwise it will rely excessively on ERM for refinement and make AnoPCN collapse to a general reconstruction model. Hence, PCM is trained first by minimizing the following objective function:</p><formula xml:id="formula_14">L P = λ int L int (I t , Ît ) + λ д L д (I t , Ît ) +λ of L of (I t , Ît ) + λ G GAN L G GAN (I t , Ît ).<label>(11)</label></formula><p>After that, θ P is fixed and we optimize θ R with the same loss constraints:</p><formula xml:id="formula_15">L R = λ int L int (I t , Îr t ) + λ д L д (I t , Îr t ) +λ of L of (I t , Îr t ) + λ G GAN L G GAN (I t , Îr t ),<label>(12)</label></formula><p>where λ int , λ д , λ of , and λ G GAN are the same fixed hyper-parameters for L int , L д , L of , and L G GAN in L P and L R , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>To demonstrate the effectiveness of the proposed AnoPCN, we perform experiments on three popular video anomaly detection datasets and compare our method against a number of existing shallow and deep ones. We also provide further evaluation and analysis on the proposed AnoPCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets. ShanghaiTech Campus <ref type="bibr" target="#b26">[27]</ref>, CUHK Avenue <ref type="bibr" target="#b24">[25]</ref>, and UCSD Ped2 <ref type="bibr" target="#b27">[28]</ref> are the datasets that we use for testing. We exclude UCSD Ped1 and Subway dataset for the following reasons. Firstly, some research works report VAD results only on 16 unspecified UCSD Ped1 videos <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b44">45]</ref>, while others <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> use the complete Traditional Shallow Methods MPPCA <ref type="bibr" target="#b16">[17]</ref> N/A N/A 69.3% MPPC+SFA <ref type="bibr" target="#b27">[28]</ref> N/A N/A 61.3% MDT <ref type="bibr" target="#b27">[28]</ref> N/A N/A 82.9% Del et al. <ref type="bibr" target="#b11">[12]</ref> N/A 78.3% N/A Growing Gas <ref type="bibr" target="#b37">[38]</ref> N/A N/A 94.1%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Reconstruction Methods</head><p>ConvAE+SVM <ref type="bibr" target="#b44">[45]</ref> N/A N/A 90.8% ConvAE <ref type="bibr" target="#b13">[14]</ref> 60.9% 80.0% 85.0% ConvLSTM-AE <ref type="bibr" target="#b25">[26]</ref> N/A 77.0% 88.1% AbnormalGAN <ref type="bibr" target="#b31">[32]</ref> N/A N/A 93.5% s-RNN <ref type="bibr" target="#b26">[27]</ref> 68  <ref type="bibr" target="#b24">[25]</ref> uses the first 15-minute Subway videos for training, while <ref type="bibr" target="#b3">[4]</ref> uses more than half of the videos for training. These differences in the treatment of both datasets will bring unfair comparisons. Now we briefly introduce the test datasets.</p><p>(1) ShanghaiTech Campus dataset is a challenging anomaly detection dataset recorded recently. It has 330 training videos and 107 testing ones, which is taken in 13 different scenes with various camera angles and illumination. Abnormal activities are diverse and realistic, which include appearance anomalies like bicycles, skateboards, motorbikes, cars, strollers, and motion anomalies such as fighting, chasing, pushing, jumping, etc.</p><p>(2) CUHK Avenue dataset contains anomalies such as running, throwing objects, and emergent events. It has 16 training videos and 21 testing ones with lengths varying from 10s to 60s. The difficulty of detecting anomaly in this dataset lies in the noisy background dynamic caused by pedestrians.</p><p>(3) UCSD Ped2 dataset is a part of the UCSD Pedestrian dataset. It has 16 training videos and 12 testing ones. The anomalies include anomalous pedestrian patterns and non-pedestrian objects such as bicycles, skateboards, and cars.</p><p>Some anomalous examples are illustrated in Section 4.5.</p><p>Parameters. The image frames generally are resized to 256×256, and the intensities of pixels are normalized to [-1, 1]. AnoPCN is implemented with the TensorFlow <ref type="bibr" target="#b0">[1]</ref> and trained by Adam optimizer <ref type="bibr" target="#b17">[18]</ref> with a learning rate of 1 × 10 -4 for UCSD Ped2 and 2 × 10 -4 for the others. Learning rates of GAN discriminator are 1 × 10 -5 for UCSD Ped2 and 2 × 10 -5 for the rest datasets. λ int , λ д , λ of , and λ G GAN normally are 1, 1, 1, and 0.05. The mini-batch size is 4, and k is set to 4 for the fairness of comparison with existing deep prediction VAD methods.</p><p>Evaluation Metrics. After calculating the Receiver Operating Characteristic (ROC) curve, the corresponding frame-level Area Under the Curve (AUC) is employed as the evaluation metric for the comparison of different methods, which is the most widely used and effective quantitative metric for anomaly detection performance. We also report the Equal Error Rate (EER) of the ROC curve.</p><p>To examine the effectiveness of our network architecture, PSNR is utilized to compare PCM with existing prediction methods. We also compare the regularity score gap ΔS between AnoPCN and existing deep VAD methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with Existing State-of-the-Arts</head><p>Table <ref type="table" target="#tab_0">1</ref> compares our AnoPCN with traditional shallow methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">38]</ref>, deep reconstruction methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45]</ref>, and deep prediction methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b46">47]</ref> based on the frame-level AUC. Our method achieves the state-of-the-art performance in ShanghaiTech Campus, CUHK Avenue, and UCSD Ped2.</p><p>Compared to ConvLSTM-AE <ref type="bibr" target="#b25">[26]</ref>    <ref type="bibr" target="#b22">[23]</ref> which uses U-Net <ref type="bibr" target="#b32">[33]</ref> to predict a future frame, our AnoPCN resorts to the predictive coding theory which feeds prediction error forward recurrently and refines the coarse predicted frame in a reconstruction manner. Our method outperforms <ref type="bibr" target="#b22">[23]</ref> by around 1% on all these datasets. High AUC values in different test datasets indicate that our method is robust and effective in detecting anomalies with diverse and noisy backgrounds. Moreover, Figure <ref type="figure" target="#fig_8">6</ref> compares the ROC curves of <ref type="bibr" target="#b22">[23]</ref> and AnoPCN on these datasets, which again demonstrate the effectiveness of our AnoPCN for VAD task. Besides, EERs of the ROC curves in ShanghaiTech Campus, CUHK Avenue, and UCSD Ped2 are 33.65%, 20.20%, and 10.00%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation of PCM</head><p>PCM is an important component of AnoPCN, which generates a realistic prediction for future frames and reduces the information that ERM is required to reconstruct.</p><p>Comparison to Other Prediction VAD Methods. As shown in Table <ref type="table" target="#tab_0">1</ref>, our PCM yields better performance than the prediction U-Net method <ref type="bibr" target="#b22">[23]</ref> owing to the predictive coding based architecture. To further investigate the effectiveness of PCM, we compare the PSNR values of predicted frames generated by our PCM and U-Net <ref type="bibr" target="#b22">[23]</ref>.</p><p>As shown in Table <ref type="table" target="#tab_3">2</ref>, PCM has higher average PSNR values than <ref type="bibr" target="#b22">[23]</ref> for normal frames while obtaining similar PSNRs for abnormal ones. Thus, our PCM has larger PSNR gaps between predicted normal frames and abnormal ones than those of <ref type="bibr" target="#b22">[23]</ref>. These results imply that PCM has better ability to discriminate anomalies. Ablation Study of PCM. Our PCM contains the PEP Unit and the ConvLSTM Unit. To further investigate our architecture, we conduct an ablation study as Table <ref type="table" target="#tab_4">3</ref> shows. Since existing prediction VAD methods mainly take RGB frames as input, we first verify the superiority of RGB difference (i.e. prediction error) as the input of a layer of ConvLSTM. As shown in the first two rows of Table <ref type="table" target="#tab_4">3</ref>, RGB difference consistently outperforms RGB frame on all datasets. Specially, the AUC of the second row is 8.5% better than that of the first row on the most challenging ShanghaiTech Campus dataset.</p><p>In addition, we use ground truth frames directly to predict future frames for VAD. As the third row shows, its results are inferior to PCM. These results demonstrate that RGB difference is a more robust modality than RGB frame for future frame prediction in VAD task, which partly explains the superiority of PCM for VAD task.</p><p>We also implement the stacked ConvLSTMs for anomaly detection to verify the improvement that PCM has over PredNet <ref type="bibr" target="#b23">[24]</ref>. Compared to stacked ConvLSTMs, PCM harnesses a convolutional U-Net as the PEP Unit. As demonstrated by the last two rows of Table 3, our PCM outperforms stacked ConvLSTMs by 1.9%, 1.8%, and 0.7% on ShanghaiTech Campus, CUHK Avenue, and UCSD Ped2, respectively. The results indicate that PEP Unit can alleviate the "prediction error accumulation" problem in stacked ConvLSTMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation of ERM</head><p>We also evaluate the advantages of decomposing reconstruction into prediction and refinement for VAD task. As already discussed in Table <ref type="table" target="#tab_0">1</ref>, combining ERM with PCM helps AnoPCN achieve the stateof-the-art performance in test datasets. Figure <ref type="figure" target="#fig_9">7</ref> further illustrates the average regularity score gaps between normal frames and abnormal frames of a state-of-the-art deep reconstruction method <ref type="bibr" target="#b13">[14]</ref>, a state-of-the-art deep prediction method <ref type="bibr" target="#b22">[23]</ref>, and ours. From Figure <ref type="figure" target="#fig_9">7</ref>, we observe that AnoPCN has larger regularity score gaps than theirs, which makes it easier to discriminate anomaly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis</head><p>We now conduct a further analysis of AnoPCN. First, we visualize several anomaly detection examples by the proposed method in Figure <ref type="figure" target="#fig_10">8</ref>. Meanwhile, noting that the state-of-the-art performance of AnoPCN still needs to improve, we conduct a case analysis on ShanghaiTech Campus dataset for future enhancement of our model. Finally, we discuss the computation time of AnoPCN. Visualization of VAD. Figure <ref type="figure" target="#fig_10">8</ref> depicts the frame-level regularity score curves of our AnoPCN for three abnormal events, namely a robbery in ShanghaiTech Campus where two people chase each other, a motion anomaly of running in CUHK Avenue, and an anomalous appearance of bicycle in UCSD Ped2. It is interesting to find that the regularity scores decrease dramatically when anomalies happen and they rise when anomalies disappear. Such an observation indicates that our method is able to detect the occurrence of anomalies.</p><p>Case Analysis on ShanghaiTech Campus. Although we have achieved the state-of-the-art performance on the test datasets, we understand that the VAD problem is not completely solved. Therefore, we conduct a case analysis on ShanghaiTech Campus, which is the most challenging dataset among the test datasets.</p><p>We first divide the testing videos of ShanghaiTech Campus into 12 categories. As Figure <ref type="figure" target="#fig_11">9</ref> demonstrates, we then perform VAD tests on these video subsets separately, and report the AUC of each category. On one hand, AnoPCN is good at detecting anomalies in which humans and objects have high speed, such as the anomalies of throwing, chasing, cars, motorbikes, etc. On the other hand, we find that AnoPCN cannot well detect anomalous objects which have low speed, such as strollers and oversize items carried by people. Additionally, AnoPCN still need to improve its performance on detecting the anomaly cases of fighting and jumping. Some example videos of the mentioned bad cases are displayed in Figure <ref type="figure" target="#fig_12">10</ref>.</p><p>Computation Time. The experiments are performed on NVIDIA GeForce GTX 1080 Ti GPUs with Intel Xeon CPU E5-2690 2.60GHz CPUs. It takes about 15 hours to train the AnoPCN model for VAD. The average running time for testing is about 10 fps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we present a novel framework for video anomaly detection named AnoPCN to address the problem of existing reconstruction and prediction methods, which decomposes reconstruction into prediction and refinement stages. By utilizing motion information explicitly, the proposed PCM which is partly inspired by the predictive coding mechanism in brains achieves better performance for anomaly detection. In the refinement stage, we introduce ERM to reconstruct prediction error and refine the coarse frame  predicted by PCM. We find that our model achieves the state-of-theart performance with better prediction results and larger regularity score gaps on three benchmark datasets.</p><p>In the future, we would like to explore the following directions to improve AnoPCN. For one thing, we will design auto-tuning strategies for the hyper-parameter setting. For another, we will adapt AnoPCN to run in an online mode for real-time application.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Regularity score curve of [23] for an example video. Some normal frames have low regularity scores for their high prediction errors. Blue represents ground truth scores.</figDesc><graphic coords="2,63.23,83.36,230.29,117.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( a )</head><label>a</label><figDesc>Pipeline of deep reconstruction methods. (b) Pipeline of deep prediction methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Pipelines of deep reconstruction methods and prediction methods for VAD. They differ in whether they use current frame I t for frame generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( a )</head><label>a</label><figDesc>Prediction stage in time step s = t -k , ..., t -1. (b) Refinement stage in time step t .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Overview of the proposed AnoPCN from t -k to t. It decomposes reconstruction into (a) the prediction stage and (b) the refinement stage. PCM in (a) utilizes previous prediction errors to produce a predicted frame Ît . ERM in (b) reconstructs the prediction error E t to generate a refined frame Îr t . The number of each arrow indicates the order of each step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Generic predictive coding framework. (b) Proposed Predictive Coding Module (PCM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Generic predictive coding framework and the network architecture of PCM. The predictive estimator in (a) constantly makes prediction by merely transmitting prediction error signals, and PCM in (b) generates prediction by following this framework. Conv layers are not shown in (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Network architecture of the proposed Prediction Error Processing (PEP) Unit and Error Refinement Module (ERM). They share the same U-Net architecture except for the last convolutional layer that ERM has.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparison of frame-level ROC curves between [23] and AnoPCN on different datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Comparison of regularity score gaps ΔS between normal frames and abnormal frames on different datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Regularity score curves evolved along time on three example abnormal events.</figDesc><graphic coords="8,48.59,83.36,151.93,119.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: AUCs of different anomalies on ShanghaiTech Campus. Blue represents the bad cases in ShanghaiTech Campus.</figDesc><graphic coords="8,326.99,450.44,53.47,53.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Some videos of bad cases which include the anomalies of fighting, oversize item, jumping, and stroller. The bounding box in each subfigure indicates the location of anomaly.</figDesc><graphic coords="8,383.21,450.44,53.47,53.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : AUCs of different methods on benchmark datasets.</head><label>1</label><figDesc></figDesc><table><row><cell>Types</cell><cell>Methods</cell><cell>ShanghaiTech Campus CUHK Avenue UCSD Ped2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>As for Subway dataset, we observe that different methods use different Subway videos for training. For instance,</figDesc><table><row><cell></cell><cell></cell><cell>.0%</cell><cell>81.7%</cell><cell>92.2%</cell></row><row><cell></cell><cell>Two-Stage [43]</cell><cell>N/A</cell><cell>85.3%</cell><cell>96.4%</cell></row><row><cell></cell><cell>Autoregressive-ConvAE [2]</cell><cell>72.5%</cell><cell>N/A</cell><cell>95.4%</cell></row><row><cell>Deep Prediction Methods</cell><cell>STAE [47] Liu et al. [23]</cell><cell>N/A 72.8%</cell><cell>80.9% 85.1%</cell><cell>91.2% 95.4%</cell></row><row><cell>Ours</cell><cell>PCM Only AnoPCN</cell><cell>73.2% 73.6%</cell><cell>85.3% 86.2%</cell><cell>96.0% 96.8%</cell></row><row><cell>36 ones for testing.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>which uses ConvLSTM for frame reconstruction, our AnoPCN first leverages a prediction</figDesc><table><row><cell>Session 4B: Visual Analysis &amp; Applications</cell><cell>MM '19, October 21-25, 2019, Nice, France</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 : Comparison of average PSNR values</head><label>2</label><figDesc></figDesc><table><row><cell>Dataset</cell><cell>Average PSNR</cell><cell>Liu et al. [23]</cell><cell>PCM</cell></row><row><cell></cell><cell>Normal Frames</cell><cell>31.861</cell><cell>34.445</cell></row><row><cell>ShanghaiTech</cell><cell>Abnormal Frames</cell><cell>30.936</cell><cell>32.585</cell></row><row><cell></cell><cell>PSNR Gap (Δ)</cell><cell>0.925</cell><cell>1.860</cell></row><row><cell></cell><cell>Normal Frames</cell><cell>37.128</cell><cell>38.752</cell></row><row><cell>Avenue</cell><cell>Abnormal Frames</cell><cell>28.451</cell><cell>28.848</cell></row><row><cell></cell><cell>PSNR Gap (Δ)</cell><cell>8.677</cell><cell>9.904</cell></row><row><cell></cell><cell>Normal Frames</cell><cell>38.902</cell><cell>39.009</cell></row><row><cell>Ped2</cell><cell>Abnormal Frames</cell><cell>36.522</cell><cell>35.971</cell></row><row><cell></cell><cell>PSNR Gap (Δ)</cell><cell>2.380</cell><cell>3.038</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 : Ablation study of PCM on different datasets.</head><label>3</label><figDesc></figDesc><table><row><cell>Modality</cell><cell>Method</cell><cell>ShanghaiTech</cell><cell>Avenue</cell><cell>Ped2</cell></row><row><cell>RGB Frames</cell><cell>ConvLSTM</cell><cell>61.6%</cell><cell>80.4%</cell><cell>92.1%</cell></row><row><cell>RGB Differences</cell><cell>ConvLSTM</cell><cell>70.1%</cell><cell>81.3%</cell><cell>92.4%</cell></row><row><cell>RGB Frames</cell><cell>ConvLSTM + PEP Unit</cell><cell>71.5%</cell><cell>83.1%</cell><cell>92.9%</cell></row><row><cell>RGB Differences</cell><cell>Stacked ConvLSTMs [24]</cell><cell>71.3%</cell><cell>83.5%</cell><cell>95.3%</cell></row><row><cell>RGB Differences</cell><cell>ConvLSTM + PEP Unit (our PCM)</cell><cell>73.2%</cell><cell>85.3%</cell><cell>96.0%</cell></row><row><cell cols="5">model (i.e. PCM) and then reconstructs the prediction error im-</cell></row><row><cell cols="5">age. AnoPCN outperforms ConvLSTM-AE [26] by 9.2% and 8.7%</cell></row><row><cell cols="5">on CUHK Avenue and UCSD Ped2, respectively. Compared to a</cell></row><row><cell cols="2">state-of-the-art prediction method</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is partially supported by National Natural Science Foundation of China (U1813218, U1613211), Shenzhen Basic Research Program (JCYJ20170818164704758), the Joint Lab of CAS-HK.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent Space Autoregression for Novelty Detection</title>
		<author>
			<persName><forename type="first">Davide</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelo</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Rakesh</forename><surname>Chalasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">C</forename><surname>Principe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3541</idno>
		<title level="m">Deep predictive coding networks</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Video anomaly detection and localization using hierarchical feature representation and Gaussian process regression</title>
		<author>
			<persName><forename type="first">Kai-Wen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yie-Tarng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Hsien</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2909" to="2917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sparse reconstruction cost for abnormal event detection</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3449" to="3456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international Conference on computer vision &amp; Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="428" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep representation for abnormal event detection in crowded scenes</title>
		<author>
			<persName><forename type="first">Yachuang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM international conference on Multimedia</title>
		<meeting>the 24th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="591" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with odd-one-out networks</title>
		<author>
			<persName><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3636" to="3645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Predictive coding under the free-energy principle</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Friston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Kiebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">364</biblScope>
			<biblScope unit="page" from="1211" to="1221" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A discriminative framework for anomaly detection in large videos</title>
		<author>
			<persName><forename type="first">Allison</forename><surname>Del Giorno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="334" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep predictive coding network with local recurrent processing for object recognition</title>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiguang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugenio</forename><surname>Culurciello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9221" to="9233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName><forename type="first">Mahmudul</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image-toimage translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Observe locally, infer globally: a spacetime MRF for detecting abnormal activities with incremental updates</title>
		<author>
			<persName><forename type="first">Jaechul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2921" to="2928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual mismatch negativity (vMMN): A review and meta-analysis of studies in psychiatric and neurological disorders</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kremláček</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kairi</forename><surname>Kreegipuu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Tales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piia</forename><surname>Astikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nele</forename><surname>Poldver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gábor</forename><surname>Risto Näätänen</surname></persName>
		</author>
		<author>
			<persName><surname>Stefanics</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cortex</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="76" to="112" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Anomaly detection and localization in crowded scenes</title>
		<author>
			<persName><forename type="first">Weixin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="18" to="32" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Future frame prediction for anomaly detection-a new baseline</title>
		<author>
			<persName><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongze</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6536" to="6545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep predictive coding networks for video prediction and unsupervised learning</title>
		<author>
			<persName><forename type="first">William</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Gabriel Kreiman</surname></persName>
		</author>
		<author>
			<persName><surname>Cox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08104</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Abnormal event detection at 150 fps in matlab</title>
		<author>
			<persName><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2720" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Remembering history with convolutional LSTM for anomaly detection</title>
		<author>
			<persName><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="439" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A revisit of sparse coding based anomaly detection in stacked rnn framework</title>
		<author>
			<persName><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Viral Bhalodia, and Nuno Vasconcelos</title>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1975" to="1981" />
		</imprint>
	</monogr>
	<note>Anomaly detection in crowded scenes</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05440</idno>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Anomaly detection in video using predictive convolutional long short-term memory networks</title>
		<author>
			<persName><forename type="first">Jefferson</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Medel</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Savakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00390</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Rajesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><forename type="middle">H</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">79</biblScope>
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Abnormal event detection in videos using generative adversarial nets</title>
		<author>
			<persName><forename type="first">Mahdyar</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucio</forename><surname>Marcenaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Regazzoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICIP.2017.8296547</idno>
		<ptr target="https://doi.org/10.1109/ICIP.2017.8296547" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1577" to="1581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhourong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wai-Kin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><forename type="middle">Chun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visual mismatch and predictive coding: a computational single-trial ERP study</title>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Stefanics</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Heinzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="4020" to="4030" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note>András Attila Horváth, and Klaas Enno Stephan</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visual mismatch negativity: a predictive coding view</title>
		<author>
			<persName><forename type="first">Gábor</forename><surname>Stefanics</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kremláček</surname></persName>
		</author>
		<author>
			<persName><forename type="first">István</forename><surname>Czigler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in human neuroscience</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">666</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Online growing neural gas for anomaly detection in changing surveillance scenes</title>
		<author>
			<persName><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="187" to="201" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Anomaly detection using a convolutional winner-take-all autoencoder</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><surname>Hogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<publisher>British Machine Vision Association</publisher>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Detecting Abnormality without Knowing Normality: A Two-stage Approach for Unsupervised Video Abnormal Event Detection</title>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijie</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengzhang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">En</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="636" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Chaotic invariants of lagrangian particle trajectories for anomaly detection in crowded scenes</title>
		<author>
			<persName><forename type="first">Shandong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2054" to="2060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning deep representations of appearance and motion for anomalous event detection</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.01553</idno>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Online detection of unusual events in videos via dynamic sparse coding</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3313" to="3320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Spatio-temporal autoencoder for video anomaly detection</title>
		<author>
			<persName><forename type="first">Yiru</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
