<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A system to detect houses and residential street networks in multispectral satellite images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2004-12-18">18 December 2004</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Cem</forename><forename type="middle">U</forename><surname>¨nsalan</surname></persName>
							<email>unsalan@yeditepe.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="laboratory">Signal Analysis and Machine Perception Laboratory</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<postCode>43210-1272</postCode>
									<settlement>Columbus</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kim</forename><forename type="middle">L</forename><surname>Boyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="laboratory">Signal Analysis and Machine Perception Laboratory</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<postCode>43210-1272</postCode>
									<settlement>Columbus</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A system to detect houses and residential street networks in multispectral satellite images</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2004-12-18">18 December 2004</date>
						</imprint>
					</monogr>
					<idno type="MD5">EF08152919F8A02314020521E5D6CCFE</idno>
					<idno type="DOI">10.1016/j.cviu.2004.10.006</idno>
					<note type="submission">Received 4 September 2003; accepted 18 October 2004</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Vegetation and shadow water indices</term>
					<term>Clustering</term>
					<term>Spatial coherence</term>
					<term>Binary balloon algorithm</term>
					<term>Graph theory</term>
					<term>Shortest path algorithm</term>
					<term>House detection</term>
					<term>Road detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Maps are vital tools for most government agencies and consumers. However, their manual generation and updating is tedious, time consuming, and expensive. To address these concerns, we are developing automated techniques. In this paper, we restrict our attention to residential regions. These regions provide a challenge, testing the current limits of automated image analysis. Such regions are also typically areas of rapid growth and development and, therefore, are of interest from the applications perspective. In previous studies, we introduced statistical measures to extract these kinds of regions from satellite images [in: Proceedings of the International Conference on Pattern Recognition, vol. 1, 2002, p. 127, IEEE Trans.  GeoRS (2003), IEEE Trans. PAMI]. As the next step toward automatic map generation, here we introduce a novel system to detect houses and street networks in IKONOS multispectral images. These images have one meter panchromatic resolution with 4 m resolution in the spectral bands. Our system consists of four major components: multispectral analysis to detect cultural activity, segmentation of regions of possible human activity (based on the surface material), decomposition of the segmented images, and graph theoretical algorithms over the decompositions to extract the street network and to detect houses. We tested our system on a large and diverse data set. Our results indicate the usefulness of our system in detecting houses and street networks, hence generating automated maps.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Government agencies, civil defense organizations, relief agencies, and consumers depend heavily on maps to support their missions and activities. Konecny and Schiewe <ref type="bibr" target="#b3">[4]</ref> summarize some facts regarding manual map generation. According to their analysis, 33.5% of the world was mapped at 1:25,000 scale (around one meter per pixel resolution) as of 1993. This resolution is vital for mapping most mature cities (such as European cities) because their buildings and street networks are in close proximity. For this scale, the annual manual map generation rate is around 2.8%. Similarly, the annual manual map updating rate is around 4.9%. Konecny and Schiewe underscore the urgency to automate map generation: ''On average, maps of 1:25,000-scale are 20 years out of date and 1:50,000-scale sheets may be 40 (or more) years old.'' Considering that a house can be built less than a year (and destroyed in minutes), and larger buildings can be built within two to three years, the inadequacy of current map updating rates is clearly evident.</p><p>Aerial and satellite images can provide a solution to the automatic map generation problem. Collecting data around the world with aerial sensors takes around five years (as reported by Konecny and Schiewe). More promisingly, collecting data around the world using the commercially available, high-resolution (one meter per pixel panchromatic and 4 m per pixel multispectral) IKONOS satellite images takes a matter of months and requires no access to airspace. Compared to the current 20 year updating period, IKONOS imagery provides the most promising avenue to solve the map generation problem. While the advent of commercially available, high-resolution satellite imagery addresses the data collection issue, the rate at which these sensors provide data currently far exceeds the rate at which those data can be analyzed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Overview</head><p>To address these problems, we introduce an automatic street network and house detection system using commercial IKONOS satellite images as input. We focus on to detect houses and street networks in residential regions, which have been identified in advance using the techniques in <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. Detecting houses is far more challenging than detecting larger buildings for several reasons. First, their footprints are relatively small. Second, occlusion by nearby trees is common. Third, in some neighborhoods, houses may come in fairly complex shapes. Analogous problems (small cross-section, overhanging trees, and winding curves) present challenges for street detection in residential regions.</p><p>The system described here comprises four main parts, all operating on the 4 m resolution, multispectral bands. First, we introduce measures on multispectral images to detect regions of possible human activity. <ref type="foot" target="#foot_0">1</ref> On these measures, we introduce a variation of the k-means clustering (KMC) algorithm to extract possible houses and street networks by combining both spatial and spectral features. This combination of information improves the final clustering results. From clustering, we obtain a binary image containing possible street network fragments and houses. We then decompose this binary image using a balloon algorithm based on binary mathematical morphology. We then represent this decomposition in a graph for which balloons serve as vertices, while their neighborhood relationships are encoded as edges. The street network is extracted from the graph using a shortest paths algorithm. The remaining vertices (balloons) are designated as possible houses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Prior work</head><p>In this section, we briefly review the literature on road and house detection. We group previous studies as follows: building detection (alone), road detection (alone), and combined building and road detection. The last group of studies exploits the mutual information between buildings and road networks to improve detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.1.">Building detection</head><p>Mayer <ref type="bibr" target="#b4">[5]</ref> surveyed object detection systems from aerial images, focusing on building detection. In his excellent survey, he classified building detection systems based on their complexity (in data, building model, and system strategy). Following MayerÕs format (in simplified form), we add missing and new papers published after his survey in Table <ref type="table" target="#tab_1">1</ref>. Besides the performance measures, Shufelt and McKeown <ref type="bibr" target="#b5">[6]</ref> offer another survey on aerial building detection.</p><p>In Table <ref type="table" target="#tab_1">1</ref>, data complexity summarizes the resolution of the input image, relative location of the buildings, and the complexity of the scene. As for image types, some researchers used satellite images (such as Landsat, SPOT, and IRS) with resolutions in the 5-30 m range. Others used aerial images with resolutions in the 0.3-1 m range. Still others have used digital elevation map (DEM) or digital surface model (DSM) data. Finally, some have used synthetic aperture radar (SAR) images. Each of these representations has its own benefits and shortcomings.</p><p>Model complexity characterizes the building model used. It may be a simple 2D rectangle, a 2D polynomial, or a 3D surface. The model should serve the systemÕs application. If only detection is required, a simple rectangle as a building model may suffice. However, if a detailed site model is required, 3D surface models as well as polynomial representations are necessary.</p><p>System strategy captures the complexity of the system. Perceptual organization, Bayesian networks, and graph theoretical methods are some of the approaches used in building detection systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neither Mayer nor Shufelt and</head><p>McKeown provided the classification performances of the systems they reviewed. Here, we fill this gap by providing reported classification performances in Table <ref type="table" target="#tab_0">2</ref>. In accordance with Lin and Nevatia <ref type="bibr" target="#b18">[19]</ref>, we cite probability of detection and branching factor. Probability of detection P d is the percentage of the ground truth buildings detected. Branching factor B f is the number of non-building objects (not pixels) labelled as buildings divided by the total number of objects labelled as buildings.</p><p>For the systems reported, P d varies from 41.5 to 97.6%, with B f in the 0.0-46.0% range. The performance depends on the resolution of the image, the density of buildings in the scene, and finally the size of the buildings to be detected. In most of these papers the ground truth, such as the number and size of the buildings, has not been reported in detail. Also, the definitions of successful detection vary, ranging from simple detection of any part of a building <ref type="bibr" target="#b15">[16]</ref>, to complete delineation <ref type="bibr" target="#b19">[20]</ref>. Any comparison of the numbers in Table <ref type="table" target="#tab_0">2</ref> must be done in light of these caveats.</p><p>The size of the buildings to be detected affects the performance. For example, Kim and Muller <ref type="bibr" target="#b12">[13]</ref> reported P d = 79.1% for house detection over 12 houses, which is far  <ref type="bibr" target="#b9">[10]</ref> 90.0-91.6 Lin and Nevatia <ref type="bibr" target="#b18">[19]</ref> 71.9 6.7 Collins et al. <ref type="bibr" target="#b19">[20]</ref> 89.0 46.0 Kim and Muller <ref type="bibr" target="#b12">[13]</ref> 76.3 Stassopoulou and Caelli <ref type="bibr" target="#b13">[14]</ref> 97.6 12.0 Noronha and Nevatia <ref type="bibr" target="#b15">[16]</ref> 96.4 0.0 Fradkin et al. <ref type="bibr" target="#b16">[17]</ref> 80.3 1.0 Krishnamoorthy et al. <ref type="bibr" target="#b17">[18]</ref> 41.5-86.9</p><p>Criteria for success vary; see text.  <ref type="bibr" target="#b7">[8]</ref> Low-medium Medium Medium Krishnamachari and Chellappa <ref type="bibr" target="#b8">[9]</ref> Low-medium Simple Medium Maloof et al. <ref type="bibr" target="#b9">[10]</ref> Low-medium Medium Medium Brunn and Weidner <ref type="bibr" target="#b10">[11]</ref> High Medium High Zhang <ref type="bibr" target="#b11">[12]</ref> High Simple Simple Kim and Muller <ref type="bibr" target="#b12">[13]</ref> Low Simple Medium Stassopoulou and Caelli <ref type="bibr" target="#b13">[14]</ref> Low Medium High Gamba et al. <ref type="bibr" target="#b14">[15]</ref> High Medium Simple Noronha and Nevatia <ref type="bibr" target="#b15">[16]</ref> Low Simple High Fradkin et al. <ref type="bibr" target="#b16">[17]</ref> Medium High Medium Krishnamoorthy et al. <ref type="bibr" target="#b17">[18]</ref> Medium Medium Medium less than the maximum P d obtained. Krishnamoorthy et al. <ref type="bibr" target="#b17">[18]</ref> observed that most of their errors originate from house detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.2.">Road detection</head><p>There is another excellent survey paper by Mayer et al. <ref type="bibr" target="#b20">[21]</ref> on road detection in aerial images. As in the previous section, we add unreported papers in this survey following the same format (simplified) in Table <ref type="table" target="#tab_2">3</ref>. For each paper, we provide (if available) the data complexity, representation, and the resolution in meters per pixel.</p><p>As Table <ref type="table" target="#tab_2">3</ref> shows, images with various complexities are used to detect road networks, including airborne, satellite, and SAR. Topology, parametric models, snakes, and semantic networks are the most popular representation methods. Resolution varies from 0.5 to 75 m for these studies. In the low resolution (greater than 30 m/ pixel) images only highways can be detected.</p><p>In their survey, Mayer et al. did not provide road detection performances. As in building detection, we provide the reported road detection performances in terms of probability of detection P d and probability of false alarm P f in Table <ref type="table" target="#tab_3">4</ref>.</p><p>In Table <ref type="table" target="#tab_3">4</ref>, P d varies from 72.0 to 100.0%; P f varies from 1.0 to 10.6%. In these studies, the performance depends on the type of road to be detected, whether it is a highway with six lanes, a street in a city, or a road in a rural region. A method designed for a specific road type may not be useful for other types. Based on these, Bajcsy and Takavoli <ref type="bibr" target="#b21">[22]</ref> Low Topology 79 Wang and Newkirk <ref type="bibr" target="#b22">[23]</ref> Medium Semantic network 10 Fiset and Cavayas <ref type="bibr" target="#b23">[24]</ref> Medium-high Topology, tracking 30 Netanyahu et al. <ref type="bibr" target="#b24">[25]</ref> Medium Parametric 1-3 Tupin et al. <ref type="bibr" target="#b25">[26]</ref> High Semantic network Karathanassi et al. <ref type="bibr" target="#b26">[27]</ref> Medium-high Topology, parametric 6.25, 10 Laptev et al. <ref type="bibr" target="#b27">[28]</ref> Low Snakes 0.5 Jeon and Hong <ref type="bibr" target="#b28">[29]</ref> Low Grouping, snakes Shi and Zhu <ref type="bibr" target="#b29">[30]</ref> High Topology 1 Bajcsy and Takavoli <ref type="bibr" target="#b21">[22]</ref> 85.0-100.0 Wang and Newkirk <ref type="bibr" target="#b22">[23]</ref> 87.7 10.6 Fiset and Cavayas <ref type="bibr" target="#b23">[24]</ref> 79.3-81.3 Karathanassi et al. <ref type="bibr" target="#b26">[27]</ref> 92.0 Laptev et al. <ref type="bibr" target="#b27">[28]</ref> 72.0-84.0 1.0-5.0 Jeon and Hong <ref type="bibr" target="#b28">[29]</ref> 92.2 1.6 Shi and Zhu <ref type="bibr" target="#b29">[30]</ref> 91.5-92.1 1.6</p><p>Criteria for success vary; see text.</p><p>Table <ref type="table" target="#tab_3">4</ref> gives a general idea about performance, but is not a conclusive test to pick the best method among them.</p><p>1.2.3. Combined building and road detection Some researchers have designed systems to detect both buildings and road networks simultaneously. Because buildings and road networks are not independent of each other, this strategy may improve the detection of both. Nagao et al. <ref type="bibr" target="#b30">[31]</ref> and Nagao and Matsuyama <ref type="bibr" target="#b31">[32]</ref> introduced such a system. Similarly, Hwang et al. <ref type="bibr" target="#b32">[33]</ref> introduced a hypothesis generation and testing method to detect houses and road segments. In this paper, we also detect houses and street networks simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Using multispectral information</head><p>Most prior work on house and road detection uses either grayscale images or DSM data. Besides grayscale images, we have additional multispectral information, but no surface or site models. We use this information to infer cultural activity (such as houses and street networks) and water (such as lakes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">A derived index to detect human activity</head><p>We can use vegetation indices to detect human activity in multispectral images. Vegetation indices have been used extensively to estimate the vegetation density from satellite and airborne images for many years. Rouse et al. <ref type="bibr" target="#b33">[34]</ref> introduced the normalized difference vegetation index (À1 6 NDVI 6 1):</p><formula xml:id="formula_0">NDVI ¼ q nir À q red q nir þ q red ;<label>ð1Þ</label></formula><p>where q nir and q red are the reflectance values in the near infrared and red bands. This remains one of the most popular vegetation indices, despite its non-linearity.</p><p>Although originally conceived on an ad hoc basis, in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> we provided a formal statistical explanation for the measure, and in so doing, succeeded in linearizing it. We obtain the linearized vegetation measure as follows:</p><formula xml:id="formula_1">h ¼ 4 p arctanðNDVIÞ ð<label>2Þ</label></formula><p>Through this linearization, we increase the contrast between vegetated and non-vegetated regions by avoiding the saturation problem of the NDVI.</p><p>The NDVI and its linearized version h respond with a low value (around 0) to rocks, stones, and their derivatives <ref type="bibr" target="#b36">[37]</ref>. These materials are used extensively for building and road construction, thereby indicating possible human activity (actual rock outcroppings in residential regions are rarely large enough to be seen in IKONOS data). Also, h achieves high values in areas of significant vegetation density, while returning negative values for cloud, shadow, and snow. Therefore, to measure possible building or street pixels in residential regions, we use the following derived index:</p><formula xml:id="formula_2">X ¼ 1 À jhj;<label>ð3Þ</label></formula><p>where X is normalized between 0 (suggesting low human activity) and 1 (usually meaning high human activity). Any rock outcroppings that may be detected in residential areas are easily eliminated in subsequent processing based on their irregular footprint.</p><p>To illustrate X, we use a part of the New Mexico image given in Fig. <ref type="figure" target="#fig_0">1A</ref>. This neighborhood shows characteristics typical of a mature residential region. Each house has its own garden; they are well-spaced; and there are mature trees nearby. If we consider Fig. <ref type="figure" target="#fig_0">1A</ref> we can appreciate the challenging nature of the house and street detection problem in residential regions. Even human observers find it difficult to decide whether there are houses in some parts of the image without using context, such as partial knowledge of the street layout, driveways, and so on. We give the corresponding color coded (blue corresponds to the lowest and red corresponds to the highest values) X image in Fig. <ref type="figure" target="#fig_0">1B</ref>. Houses and the street network are represented by high index values (in red) in this image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Using a shadow-water index to eliminate lakes</head><p>While high X values indicate possible house and street network pixels, they may also correspond to water regions such as streams, lakes, or ponds. Because such features can appear in residential regions, we eliminate them using a method we introduce in this section.</p><p>There seems to be little prior work focusing on water detection in satellite imagery; the most relevant works in the literature are <ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref>. Shadows (clouds) also have photometric characteristics similar to water. Shadow (cloud) detection studies include <ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref>.</p><p>Simpson and Stitt <ref type="bibr" target="#b43">[44]</ref> addressed cloud shadow detection in AVHRR (advanced very high-resolution radiometer) imagery. To detect shadows, they used geometric and optical constraints on a pixel basis in multispectral images. They discarded water regions to focus on clouds.</p><p>In the IKONOS spectrum, water shows an increasing response curve until the blue band. It reaches a maximum in this region and then decreases monotonically to the near infrared <ref type="bibr" target="#b36">[37]</ref>. So, a representative shadow water index should be composed of high blue values first. Ideally it should also consider the green and red bands, but the green band also responds strongly to vegetation and this impairs the shadow or water observation. Hence, the index should include blue and red bands at least. To obtain such an index, we applied the same framework we used for the NDVI derivation using principal components analysis with the blue, red, and near infrared bands <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>. <ref type="foot" target="#foot_3">2</ref> Based on a combinatorial search (and trying to maximize blue and red band coefficients) the best performing shadow-water index we were able to obtain is:</p><formula xml:id="formula_3">c ¼ 4 p arctan 0:6864q blue þ 0:7253q red þ 0:0537q nir ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi q 2 blue þ q 2 red þ q 2 nir p ;<label>ð4Þ</label></formula><p>where q blue , q red , and q nir are blue, red, and near-infrared bands, respectively. Because this index is unable to discriminate water from shadows, we use h and c in conjunction to distinguish the water regions. To do so, we first obtain a binary image from the c image. Our tests indicate that pixels having c values higher than 0.3 are possible shadows or water regions. Therefore, we threshold the c image at this value. We then use connected components analysis to extract eight-connected regions in this binary image. We eliminate regions smaller than 75 pixels, considering them to be insignificant (neither a significant body of water nor a shadow region that will impact subsequent processing). Because water gives a strong c response, but a low h response, while shadows generally cover at least some vegetation, we label a region (as segmented from the X image, details below) as water if its h median is less than 0.2 and its c median is greater than 0.3.</p><p>Water detection was applied to the 44 images at hand and all bodies of water covering more than 75 pixels (19 in all) were detected. There were no false positives. As an example, Fig. <ref type="figure">2</ref> shows an Indiana image containing a lake in the middle of the scene. We give its panchromatic (grayscale) image, color coded (blue corresponds to the lowest value, red corresponds to the highest value) c image, and the water region detected. The lake is correctly detected in the middle of the residential region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Segmenting the X image</head><p>Although X indicates possible human activity, we need a binary image for subsequent processing. In this binary image, possible human activity will be labelled as foreground. This section introduces an extension of the KMC algorithm that exploits spatial coherence via connected components for segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">K-means clustering with spatial coherence</head><p>K-means clustering a standard iterative technique in pattern recognition to extract natural clusters in the data <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref>. Standard k-means appears as Algorithm 1 in the Appendix, which provides all algorithms used in this paper. In standard KMC, the Euclidean distance is used in the classification step. Jain et al. <ref type="bibr" target="#b49">[50]</ref> suggested the use of the Mahalanobis distance, instead. No spatial information is used in either case.</p><p>As the zoomed (New Mexico) X image, Fig. <ref type="figure">3</ref>, illustrates, houses and street segments are connected via driveways. The street network is also connected. To use this information, we extend the KMC algorithm by introducing spatial information de- rived from connected components analysis. We call this algorithm k-means clustering with spatial coherence (KMC-SC).</p><p>Our application of KMC with spatial coherence is as follows. On the X image, we define an initial segmentation threshold t s (we show below that this value is not critical). Pixels for which X &gt; t s are the initial object (foreground) pixels (possible human activity); the rest are the initial background pixels. These serve as input to the class-conditional sample mean and covariance calculations. K-means clustering using the Mahalanobis distance is then applied to the data to update the object and background hypotheses. Here, the use of spatial coherence begins. The current object pixel hypotheses are decomposed into eight-connected sets. Those sets intersecting with the initial set of object pixels (at the beginning of this iteration) form the final object segment for this iteration. This maintains object class coherence throughout clustering; no new, isolated object segments can be created. We apply the spatial coherence constraint only to the foreground. A new iteration starts by updating the sample mean and covariance calculations. The iteration terminates when the absolute value of the difference between the current and previous object (total) pixel counts falls below a threshold. The details appear in Algorithm 2.</p><p>We present iteration steps for this algorithm on the Indiana image in Fig. <ref type="figure" target="#fig_2">4</ref>. After initial thresholding, some parts of the object segments have been obtained. As the algorithm iterates, it adds remaining object pixels to the initial object segment. Finally, most of the object segments have been labelled correctly. Importantly, pixels added early in the process present greater X values than those added later. We demonstrate the algorithm on four images from different regions. For each case, we give the panchromatic image and the binary segment (C o ) in Figs. <ref type="figure" target="#fig_3">5,</ref><ref type="figure" target="#fig_5">6</ref>. On all test images, our clustering method was able to converge in a reasonable number of iterations (approximately 10-12 iterations on the average). These experiments show that our clustering method works fairly well on different residential regions and varying environmental conditions including changes in vegetation cover and soil type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Comparison with other methods</head><p>We will now compare our modified clustering method with standard KMC, OtsuÕs method <ref type="bibr" target="#b50">[51]</ref>, and region growing <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref>. We also tested fuzzy KMC <ref type="bibr" target="#b48">[49]</ref> but the performance was similar to KMC, therefore we omit it here.</p><p>Otsu formulates the optimum threshold by maximizing the between-class variance assuming a bimodal histogram such that one peak corresponds to the object and the  other to the background. This method depends on the histogram only. Although it is extremely fast, if the bimodal histogram assumption is not satisfied, the results become less reliable.</p><p>In region growing, initial seed points are selected by some appropriate process. Then, nearby points are added to these seed points such that they do not exceed a threshold in variance or another homogeneity criterion. Here, we obtained the seed points by thresholding, and for homogeneity we used the variance threshold. One shortcoming of region growing is its heavy dependence on seed points. Also, as the region area grows, the incursion of false pixels to clusters becomes more probable <ref type="bibr" target="#b52">[53]</ref>.</p><p>We compare these four methods in Fig. <ref type="figure" target="#fig_6">7</ref>. Here, we use four different residential regions from Indiana, Maryland, New Mexico, and South Dakota. Segmentation by standard KMC performed worst of all. We see the effects of heavy dependence on the seed points in region growing on the Indiana and Maryland images. The method can not recover regions having no seed points. Finally, KMC-SC performed well on all four images. We also quantify these results in Table <ref type="table" target="#tab_4">5</ref>.</p><p>As Table <ref type="table" target="#tab_4">5</ref> shows, KMC-SC has the highest P d over three images. It falls behind OtsuÕs method only in the New Mexico image. OtsuÕs method, however, produces a significant number of disconnected segments because it invokes no spatial coherence constraint. Region growing has the lowest P f for three images except South Dakota. However, for these three images, it has P d values far below KMC with spatial coherence. Overall, KMC-SC is the best segmentation method among these four for our application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dependence on initial conditions</head><p>The only free parameter in KMC-SC is the initial segmentation threshold t s . This also serves as the seed point extraction step in region growing. Here, we test the stability of our method with respect to this initial threshold value. We consider three different initial segmentation thresholds ranging from t s very large to very small. We give the initial and the final segments in Fig. <ref type="figure" target="#fig_8">8</ref>. We also provide the total pixel count in the segment versus the iteration number, showing that convergence is nearly exponential.</p><p>The results show that the selection of t s is not highly critical. If the segmentation threshold is high (fewer seed points), the method is able to recover most of the missing parts. If the segmentation threshold is low (seed points cover more area than the actual shape), the method is able to eliminate most extra parts. Of course, if a reasonable threshold is selected we get the best performance. The threshold can also be calculated by another algorithm (such as OtsuÕs method), however we found no significant gain in doing so. Since the X image is scaled, a fixed threshold for all images suffices; we set it to 0.8.</p><p>The case of Fig. <ref type="figure" target="#fig_8">8H</ref> is interesting in that the lake was eliminated by KMC-SC, prior to the explicit water elimination step described above. This was simply the outcome of KMC-SC for this case. Because water regions present intermediate h values, which cluster they will appear in is unpredictable. That observation motivated the water elimination technique presented above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Shape decomposition of the foreground: the binary balloon algorithm</head><p>We next decompose the binary segment C o (representing possible houses and the street network) into subsets such that each either represents a structure (a house or a  street segment), or a combination represents such a structure. From this point on, C o represents a binary segment without lakes and water regions. We discussed how to discard these regions in Section 2.2.</p><p>For decomposition, we introduce a binary balloon algorithm. Balloons are deformable models that are fitted to a target object in an image using external and internal constraints <ref type="bibr" target="#b53">[54]</ref>. External constraints are provided by image (photometric) gradients, while the internal constraints are smoothness and bending forces designed to limit the balloonÕs shape complexity. Balloon algorithms have been used extensively for shape fitting in computer vision.</p><p>The extent of the objects in C o motivates us to introduce a new balloon algorithm tailored to our domain. We construct the binary balloons by set morphology operations. Mathematical morphology is a well-known tool to extract regions based on their shapes <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b52">53]</ref>. We now provide a very brief review of elementary mathematical morphology. Mathematical morphology (more properly, Minkowski algebra) is based on logical operations over, in our case, sets in Z 2 . We need three basic morphology set operations: translation, dilation, and erosion. We consider each in turn. Let points (vectors) be p = (p 1 , p 2 ), a = (a 1 , a 2 ), b = (b 1 , b 2 ). Let A be a set of points, not necessarily connected, in the plane.</p><p>The translation of set A by p is:</p><formula xml:id="formula_4">ðAÞ p ¼ fcjc ¼ a þ p; a 2 Ag:<label>ð5Þ</label></formula><p>The dilation of set A by set B is:</p><formula xml:id="formula_5">A È B ¼ [ b2B<label>ðAÞ</label></formula><formula xml:id="formula_6">b :<label>ð6Þ</label></formula><p>The erosion of set A by set B is:</p><formula xml:id="formula_7">A É B ¼ [ b2B<label>ðAÞ</label></formula><formula xml:id="formula_8">Àb :<label>ð7Þ</label></formula><p>Set B as used in Eqs. ( <ref type="formula" target="#formula_6">6</ref>) and ( <ref type="formula" target="#formula_8">7</ref>) is commonly referred to as the ''structuring element.'' It is a sort of template for the analysis of the shape of set A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Constructing initial balloons</head><p>Our primary goal in decomposing the binary image C o is to identify its elongated subsets such that they represent possible street segments (non-elongated segments will represent other structures, including houses). For this purpose, we use lines as initial balloons in C o . We extract lines by applying erosion and dilation operations, respectively. We define four linear structuring elements (one could use more, but we found no advantage in doing so): horizontal, vertical, the two diagonals. If we take x and y as coordinates in the image space, horizontal corresponds to a constant y, vertical corresponds to a constant x, diagonal-1 corresponds to a line segment y = x + b, b being the y-intercept, and diagonal-2 corresponds to a line segment y = Àx + c, c being the y-intercept.</p><p>To extract initial balloons for each structuring element, we apply Algorithm 3. The procedure is briefly as follows. We set the length of the structuring element to the maximum image size to detect the longest initial balloon possible. We then decrease the length of the structuring element until we detect an instance of it in the image. This structuring element is taken as an initial balloon. Then, we expand the initial balloon to construct the corresponding binary balloon and eliminate its pixels from the image via set subtraction. By decreasing the size of the structuring element, we continue to detect initial balloons and expand the corresponding binary balloons. Since we discard the points in each binary balloon before extracting the next, the final set of balloons will be disjoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Inflating the binary balloons</head><p>Once we obtain an initial balloon, we construct the corresponding binary balloon. We expand the initial balloon to fit into a region similar to standard balloon algorithms. However, unlike other algorithms, ours does not use a parametric form for the balloon. Instead, we expand each balloon by adding neighboring points satisfying a smoothness condition. This is a unique feature of our binary balloon algorithm; standard balloon algorithms perform poorly in our domain owing to the extreme aspect ratios of streets and roads, and small house sizes.</p><p>We start with a binary balloon obtained from Algorithm 3. The outer boundary points (eight-connected neighbors in the object class) of the initial balloon are labelled as candidate points. If there are no such foreground pixels, the balloon expands no more. Therefore, the balloon can never expand outside the object region. We group the candidate pixels into eight connected candidate sets, by connected components analysis. To ensure a smooth boundary, each added candidate set should be similar to the balloon locally. That is, we want the added set to be elongated along the same axis. This will be our internal constraint. Since all pixels in each candidate set are eight-adjacent to the current balloon, larger sets are more consistent in shape (orientated elongation) with the current balloon than are the smaller sets. Therefore, we define a size threshold t o to control the boundary smoothness; this is set to a fixed fraction of the number of pixels in the initial balloon. If we keep this threshold high, the decomposition of C o will consist of more elongated balloons with smoother boundaries; if we keep it low, fewer, less elongated balloons with rougher boundaries will result. Only if the total number of adjoining pixels exceeds t o , which we set t o equal to one-half the number of pixels in the initial balloon, do we add the candidate set to the balloon. We update the balloon, and iterate until the balloon expands no more. Initial additions will be lines, since we start with a line. As the balloon starts to take the shape of the object, curved lines can be added. At every step, the size threshold will force the shape to remain smooth. The binary balloon construction algorithm appears in Algorithm 4.</p><p>To clarify these steps, we demonstrate them on a small example in Fig. <ref type="figure">9</ref>. Here, dashed blue lines represent candidate points and red points represent the extracted balloon for each iteration. We start with the initial horizontal balloon (labelled red) in Fig. <ref type="figure">9B</ref>. Two candidate sets (labelled in green) exceed t o (separately) at the first iteration, and therefore will be added to the balloon. Two candidate sets (labelled yellow) could not exceed t o at the second iteration. At this point, there are no more candidate object pixels and the iteration stops. The final balloon extracted is labelled in red in Fig. <ref type="figure">9D</ref>.</p><p>Next, we demonstrate binary balloon extraction on the South Dakota image. To obtain the decomposition for C o in this image, we obtain all the binary balloons in C o with the four structuring elements separately as illustrated in Fig. <ref type="figure" target="#fig_0">10</ref>. For demonstration purposes, we applied a greedy graph coloring algorithm in all these fig-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Combining balloons via voting</head><p>As we apply the decomposition method, each pixel in C o belongs to at least one balloon. These multiple representations result in many overlapping and redundant balloons, none of which (usually) captures a complete ground feature on its own. To merge the balloons and eliminate redundancy as much as possible, we apply a voting method inspired by Burns et al. <ref type="bibr" target="#b56">[57]</ref>. Each pixel votes for the largest balloon of which it is a member. As votes are summed, those balloons receiving votes exceeding 80% of their areas are selected and retained. The rest are deleted as redundant since they do not convey new information. The merging does not result in a perfectly disjoint balloon set. The balloons may overlap somewhat around connecting regions; however, this overlap does not impair subsequent processing.</p><p>Applying the combination method to the initial balloons in Fig. <ref type="figure" target="#fig_0">10</ref>, we obtain the final balloons shown in Fig. <ref type="figure" target="#fig_11">11</ref>. This figure serves two purposes. First, it shows that the combination method is effective in eliminating most of the redundant balloons. Second, in obtaining elongated structures, our binary balloons work well even in such a complex environment. With the balloons in place, we are now prepared to abstract the description of the street network and houses, as discussed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Abstracting the scene: attributed balloons</head><p>To build an abstract representation of the scene, we attribute each balloon with the following properties: boundary, spine, length (of the spine), center of mass, and average width. We represent the outer boundary of each balloon using Fourier descriptors. Let a complex periodic function, u (t) = x (t) + jy (t) = u (t + rT), for any integer values of t and r, represent the outer boundary of the balloon. T is the total number of points in the contour. The complex periodic contour can be approximated by a Fourier series as <ref type="bibr" target="#b57">[58]</ref>:</p><formula xml:id="formula_9">ûðtÞ ¼ X T À1 n¼0 U n e j 2pnt T ;<label>ð8Þ</label></formula><formula xml:id="formula_10">U n ¼ 1 T X T À1 t¼0 uðtÞe Àj 2pnt T :<label>ð9Þ</label></formula><p>In this study, we use the fourth order Fourier series representation to filter the boundary shape prior to computing its features, below.</p><p>To obtain the spine of the balloon we use its curvature. The curvature is a differential geometric entity giving a measure of how rapidly the curve deviates from its tangent line <ref type="bibr" target="#b58">[59]</ref>. We find the curvature of the filtered boundary ðûðtÞxðtÞ þ jŷðtÞÞ as:</p><formula xml:id="formula_11">KðtÞ ¼ dxðtÞ dt d 2 ŷðtÞ dt 2 À dŷðtÞ dt d 2 xðtÞ dt 2 ð dxðtÞ dt Þ 2 þ ð dŷðtÞ dt Þ 2 3=2 :<label>ð10Þ</label></formula><p>Given the balloon construction and filtering above, the extremal points of this curvature correspond to the endpoints, e 1 and e 2 , of the spine (on the boundary) to be extracted. There is no algebraic solution to obtain the roots of Eq. 10 directly; we solve it numerically.</p><p>With e 1 and e 2 corresponding to the curvature extreme (spine endpoints) on the contour, we split the contour function into two parts at these points. These are arbitrarily labelled the upper and lower contours, x u + jy u and x l + jy l , respectively. Both are reindexed to run from e 1 to e 2 and interpolated to have the same number of points. Then the spine of the balloon is defined as:</p><formula xml:id="formula_12">sðkÞ ¼ 1 2 ½ðx u ðkÞ þ x l ðkÞÞ þ jðy u ðkÞ þ y l ðkÞÞ<label>ð11Þ</label></formula><formula xml:id="formula_13">for k = [0, e 2 À e 1 ].</formula><p>To demonstrate the construction of the spine of a balloon, letÕs consider a horizontal balloon from the South Dakota image. We first obtain the filtered version of the boundary by a fourth order fit as in Fig. <ref type="figure" target="#fig_12">12A</ref>. We give the corresponding curvature in Fig. <ref type="figure" target="#fig_12">12B</ref>. As we find the extremal points of this curvature (e 1 , e 2 labelled on both boundary and the curvature). As can be seen, the extremal points of the curvature are distinct and easy to extract. We finally obtain the spine curve (represented by a dashed curve) in Fig. <ref type="figure" target="#fig_12">12A</ref>.</p><p>We keep the boundary and the spine in parametric form for further processing (such as street extraction). We take the length of the spine to be its arc length. The number of pixels on the spine could also be considered to be its length. However, the arc length is robust to non-uniform pixel placements along the spine. If the spine is given as s (k) = x (k) + jy (k) then its arc length is:</p><formula xml:id="formula_14">l ¼ Z e 2 e 1 ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi dxðkÞ dk 2 þ dyðkÞ dk 2 s dk:<label>ð12Þ</label></formula><p>The center of mass of each balloon is the centroid of its point set. To compute the average width of the balloon, we compute the distance from each contour point to the spine along the direction orthogonal to the spine. The average of these distances is taken as the width of the balloon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Street network and house detection</head><p>This section describes how we extract the street network and houses from the balloons. We first eliminate balloons showing neither street nor house characteristics. Then, we construct a graph over the remaining balloons. Using graph theoretical techniques and size information as discussed below, we jointly extract street networks and houses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Eliminating balloons corresponding to large structures</head><p>In residential regions we may also encounter large buildings such as shopping malls and schools, along with their parking lots. Therefore, some balloons produced in the previous step may represent these structures. We identify and discard these balloons by their morphology.</p><p>To discard balloons representing possible large buildings and parking lots, we use their area and compactness. Compactness is defined as the ratio of a regionÕs area to the square of its perimeter, normalized by 4p <ref type="bibr" target="#b52">[53]</ref>.</p><p>Ours are metric images with the same resolution on the ground (4 m/pixel multispectral) over the entire image set; so area is a reliable feature. A typical house is far smaller than a shopping mall or a parking lot. Road networks encompass large areas, of course, but they are highly non-compact. Therefore, we calculate compactness as a second feature. Balloons possibly belonging to the street network have low compactness values; balloons representing a house are highly compact and of small area. We eliminate balloons having area &gt; 75 and compactness &gt; 4p/60 as neither part of the street network nor a house.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Forming a graph to represent the balloon neighborhoods</head><p>We extract the street network and houses using methods rooted in graph theory. Therefore, we first review some basic graph theory definitions, following StraightÕs <ref type="bibr" target="#b59">[60]</ref> notation.</p><p>A simple graph G consists of a finite non-empty set V and a set E of two-element subsets of V. The set V is called the vertex set of G; the set E is called the edge set of G. We write G = (V, E) to denote the graph G with vertex set V and edge set E. If there is an edge between two vertices, we say they are adjacent. A path in G is a finite alternating sequence of vertices and edges. The number of edges incident on a given vertex v i is called the degree of v i and is denoted by deg (v i ).</p><p>Given a graph G = (V, E), suppose there is associated with each e 2 E a positive number w (e). This number is called the weight of the edge e. In this case, the underlying graph G, together with the function w:E fi (0, 1) forms a weighted graph. In this study, the weight between two adjacent vertices (i, j) is w (i, j), the distance between them (to be defined below). This weight increases with increasing distance, such that larger weight implies a longer distance between two adjacent vertices.</p><p>We construct the balloon graph as follows. Each balloon is associated with a vertex in the graph. Therefore, each vertex v i has three properties: arc length l (v i ), width w (v i ), and the center of mass c (v i ) = (x, y) i . There is an edge between two vertices if their corresponding balloons have a common boundary section. The weight assigned to this edge is the distance between these two balloons. To calculate this distance let v i and v j be two adjacent balloons having a common boundary. Let the center of mass of the common boundary be c c = (x, y) c . The distance between these two balloons (the weight of the edge connecting them) is:</p><formula xml:id="formula_15">wði; jÞ ¼ kcðv i Þ À c c k þ kc c À cðv j Þk:<label>ð13Þ</label></formula><p>Since we use distance as a weight and will compute the shortest paths to recover the street segments, it is reasonable to use this definition. This weight definition approximates the distance between the two adjacent vertices along the spines of the corresponding balloons from one centroid to the next. Next, we consider the distance between two (not necessarily adjacent) vertices.</p><p>Berge <ref type="bibr" target="#b60">[61]</ref> defines the distance d (i, j) between two (not necessarily adjacent) vertices (i, j) in a weighted graph to be the length of the shortest path from vertex i to vertex j.</p><p>Theorem 5.1. (Berge) d (i, j) satisfies</p><formula xml:id="formula_16">1. d (i, i) = 0 2. d (i, j) + d (j, k) P d (i, k)</formula><p>In addition if the graph is symmetric, we have</p><formula xml:id="formula_17">3. d (i, j) = d (j, i)</formula><p>This function is therefore a true metric in the topological sense.</p><p>We obtain the distance between any two vertices by means of DijkstraÕs shortest path algorithm given in Algorithm 5 <ref type="bibr" target="#b61">[62]</ref>. This algorithm is one of the standard all shortest paths algorithms used in graph optimization problems. It can also be implemented in parallel, since at each step it finds the shortest path between any two given vertices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">The detection step</head><p>Now, we turn our attention to street network and house detection, introducing a novel algorithm based on graph theory. With this algorithm, we first extract the street network (via balloons) and label the remaining balloons as houses. To extract the street network, we have unary and binary constraints. Unary constraints are used to detect balloons that could represent a street segment by themselves. Binary constraints are used to lace together balloons that could represent a street segment if considered in conjunction with their neighbors.</p><p>We assume that a balloon represents either a part of a house or a street segment at this point, based on the method and constraints of their construction. There are general house and street characteristics (at least for North America) that lead to a street network and house detection algorithm from the balloon graph. These are as follows.</p><p>In residential regions, houses are connected to the street network via driveways. Since we take driveways as belonging to the street network, houses are located at its endpoints. In some cases short, wide driveways may be assigned to the street network. This is not a major problem. Compared to an elongated street segment, the aspect ratio of a house is small. Therefore, a ratio threshold can help in discriminating house and street balloons to a great extent. A street network is topologically connected, with straight and curved sections. Straight sections produce fewer, longer balloons; curved sections produce more, shorter balloons.</p><p>We apply these observations to obtain the street network and houses from the balloon graph. For each vertex (balloon) we calculate its aspect ratio. This ratio will indicate the likelihood of that balloon belonging to a street network. We take balloons having a ratio greater than 7.5 to be street segments. This unary constraint initiates street segment extraction.</p><p>We then consider binary constraints to complete the extraction of the street network. We first discard any vertex of aspect ratio smaller than t r (t r = 5, here) and having at most one neighbor. As we mentioned, such vertices are more likely to represent a house than a street segment. For vertices (balloons) having more than two adjacent vertices to belong to a street segment, their combination should exhibit the geometric characteristics of a street. Therefore, we retain paths of balloons (each member has at least two neighbors and an aspect ratio of at least five), with distances between their furthest vertices larger than a threshold t d (taken as five pixels, or 20 m here). The underlying assumption is that a path of more than 20 m length is more likely to be a street segment than a house group. In curved sections of the street network, we will have many balloons with more than one neighbor and aspect ratio less than five, but by chaining them together we can extract the street(s). Balloons not assigned to the street network are labelled as houses.</p><p>This algorithm may seem (too) simple; however it exists in the context of a larger system. We have restricted our choices to a street segment or a house by earlier processing. We have similarly restricted the region under analysis to be residential. Such regions have relatively strict rules in terms of house and street locations. Houses and street networks lie in close proximity, but a street cannot pass through a house. Therefore, this relatively simple graph theoretical street network extraction method works well. The street network and house detection algorithm is given in Algorithm 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Road tracking by prediction</head><p>Some street segments may be occluded by overhanging trees, be labelled as houses, or be eliminated as too small. To recover these missing street segments, we apply a prediction based road tracking method. Our method is similar to the correlation based road tracking methods summarized in <ref type="bibr" target="#b62">[63]</ref>.</p><p>We apply road tracking by prediction only to those street balloons having a single neighbor (deg (v i ) = 1). Therefore, we track possible streets only from endpoints of the balloon graph. We recall the parametric representation of their spines and for each spine representation we apply the following method.</p><p>Let r (t) = (x (t), y (t)) be the the spine of a balloon with deg (v i ) = 1. It can be represented by a kth order polynomial on n points:</p><formula xml:id="formula_18">xðtÞ ¼ X k i¼0 a i t i ; yðtÞ ¼ X k i¼0 b i t i<label>ð14Þ</label></formula><formula xml:id="formula_19">with t = [1, 2, . . . , n]</formula><p>. Coefficients a i and b i are obtained from the least squares fit; we take k to be one. The number of interpolated points n is set to the least common multiple of the number of points in each half-contour. Our prediction of the next point is r1 ðn þ 1Þ ¼ ðxðn þ 1Þ; ŷðn þ 1ÞÞ, extrapolating the polynomial fit. We also pick two neighbors of r1 ðn þ 1Þ, lying in the directions orthogonal to the spine curve, tagged r2 ðn þ 1Þ and r3 ðn þ 1Þ. The next street point chosen is then rðn þ 1Þ ¼ arg max r2fr 1 ;r 2 ;r 3 g Xfrg, where X is the index of human activity (Eq. 3). We then fit a polynomial to the most recent n points, and repeat the prediction. Tracking continues until max r2fr 1 ;r 2 ;r 3 g Xfrg drops below a threshold t p , discussed next.</p><p>We want road tracking to continue only for pixels having sufficiently high X. Therefore, we calculate t p in a Bayesian decision framework between object (high X values, C o ) and background (low X values, C b ) classes for each image separately (t p is therefore adaptive). We first obtain the conditional sample X distributions for object and background classes. We then set t p equal to the optimal Bayesian decision boundary value between these two classes. Rarely, this threshold may not be sufficiently strong to stop iteration. To handle these cases, we insert a control, checking the length of the extracted street segment. If the length exceeds the sum of the image width and height, we discard that prediction block. We assume that the prediction should encounter another road or the image perimeter within that distance. This constraint rarely comes into play.</p><p>We illustrate this method on the South Dakota image in Fig. <ref type="figure" target="#fig_14">13</ref>. Although most of the street segment balloons have been labelled correctly, some were discarded while combining balloons and applying size constraints. One of the actual street segment balloons is also labelled as a house. To correct these errors, we apply the road tracking algorithm. Fig. <ref type="figure" target="#fig_14">13B</ref> presents the street network extracted before tracking in green. The street network as extended by the road tracking algorithm is given in red. As can be seen, almost all missing street segments have been recovered by the tracking method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Summary of system parameters</head><p>Table <ref type="table">6</ref> summarizes the system parameters and how they are set. Eleven of 13 parameters are fixed, as explained above, while two are adaptive (ratios). This number of parameters is not excessive when one considers the complexity of the task, and  given that the system comprises several modules. As we have pointed out, all of these parameters have been carefully set in accordance with the metric and normalized nature of the images we consider, and none are particularly sensitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results and discussion</head><p>We tested our methods on 44 residential images (each panchromatic image being 800 • 800 pixels, with 200 • 200 multispectral representations) taken from different locations around North America.</p><p>We first tested KMC-SC to extract possible house and street network pixels. Our house and street detection methods depend directly on this initial step. Next, we evaluated the house and street network detection steps separately. We offer four examples to show the system performance for four different residential region types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Pixel-based classification</head><p>In classification, we take house or street pixels (C o ) as one group and the background pixels (C b ) as another group. We have 44 • 200 • 200 pixels since we are using the multispectral representation. We obtain 97.8 and 94.5% correct classification rates for the background and house or street pixels with an average classification performance of 97.3%. The errors in this section are mostly due to overhanging trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">House detection</head><p>Our image set includes a total of 6803 houses. Depending on the region, houses have different characteristics. Size, shape, and the setback from the street all vary, as does the spacing between them. To evaluate house detection performance, we report probability of detection P d and branching factor B f , as used in Table <ref type="table" target="#tab_0">2</ref>. Here, we assume that a house is detected if any part of it is detected as in <ref type="bibr" target="#b18">[19]</ref>. Because of the way we form overlapped balloons, very small (one or two pixel) house detections are virtually impossible. So we consider this approach to scoring the results to be reasonable. We obtain P d = 92.9% with B f = 9.5% under these circumstances, confirming the effectiveness of our house detection methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Street network detection</head><p>In our 44 residential test images, the total length of street networks is 55,442 pixels (55,442 • 4 m). Similar to Table <ref type="table" target="#tab_3">4</ref>, we provide P d and P f ; we define P d in terms of length. In this section, we also evaluate the quality of road tracking by the prediction technique.</p><p>The street detection algorithm achieved P d = 89.9% with P f = 3.8% before prediction. Road tracking increased both: P d = 94.8%, P f = 8.0%. It improved P d by 4.9%, at the cost of roughly doubling P f . Although somewhat application dependent, we consider the improvement in detection worth the increase in false alarm rate, at these levels. Therefore, tracking by prediction is an integral part of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Some detection examples</head><p>We present house and street network detection results for four images (Indiana, Maryland, South Dakota, and South Dakota II). To demonstrate house and street network detection performance, we followed the same color code for both (green corresponds to a correct detection, blue corresponds to a miss, and red corresponds to a false alarm in subfigures (B) and (C)). We also provided the overall detection for each image (in subfigures (D)). Driveways are ignored in scoring because they could be considered part of the street network, part of a house, or neither. We have not attempted to resolve driveways as distinct entities.</p><p>The first result is on the Indiana image (Fig. <ref type="figure" target="#fig_15">14</ref>). This residential region represents three distinctive characteristics. It has a mature region with well-spaced houses and trees nearby. There is a lake in the middle of the scene. The region in the upper left is a construction zone with new houses. Therefore, this scene is one of the hardest to process. There are 170 houses in the scene of which 165 of them are correctly localized with 12 false alarms. False alarms originate mostly from the spacing between houses and street segments. Some locations near street segments are falsely recognized as houses. Houses in the mature area are difficult to detect even for a human observer; nevertheless, the system was able to locate most of them. The 1525 pixel length street network is extracted except for a 10 pixel section, with a 90 pixel false alarm. Most false alarms occurred in the road tracking step, extending driveways for one or two extra pixels owing to the dense configuration of houses and street segments. The missing street segment could not be recovered by road tracking, since some portions of it were completely obscured by overhanging trees. The remaining street network is correctly detected with high accuracy for this scene.</p><p>The Maryland image (Fig. <ref type="figure" target="#fig_16">15</ref>) depicts another type of residential region. It presents well-spaced and similar houses. Although there are trees in the scene, they are not close to the houses. This indicates that the neighborhood is newly constructed in a previously forested region. There are 151 houses in the scene, of which 145 are correctly located, with three false alarms. The well-spaced houses in this scene give rise to a low false alarm rate. For this image, the entire 1133 pixel street network is extracted but for a 46 pixel section. The majority of the missing street section is near the top, occluded by trees.</p><p>The third result is on the South Dakota image (Fig. <ref type="figure" target="#fig_17">16</ref>). This residential region represents low density housing, with minor or no occlusions on the houses and streets. There is also little vegetation in the scene. Almost all houses and the street network can be seen clearly. This region is one of the easiest to process. In this res- The South Dakota II image (Fig. <ref type="figure" target="#fig_18">17</ref>) is similar to the Indiana image. It is composed of both mature and newly constructed regions. Although there are fewer houses in the scene, the occlusion by overhanging trees is more prevalent than in the Indiana image. The spacing of the houses is similar to that of the Maryland im-  As we observe these four results, we can comment on possible weaknesses of our system. First, as a region becomes more congested, the house and street network detection performance decreases. This is to be expected. Road tracking increases the false alarm rate considerably in street detection. Our detection system, without higher level perceptual reasoning, can not overcome significant occlusion by trees. Therefore, if a house or a street segment is totally occluded by trees, it would not  be detected. At the level of pixel classification, of course, this is not an error. This could be probably addressed by introducing a higher-level reasoning process to infer missing street segments. Even fairly simple collinearity or smooth continuation constraints may correct many of these errors <ref type="bibr" target="#b63">[64]</ref>. These are the minor problems the system exhibits. On the other hand, these four different types of regions show the robustness of our house and street detection system to different conditions. In these four images, most of the houses and street network were detected correctly. For the Indiana and South Dakota II images, it is extremely difficult for a human observer to detect the houses in the mature (wooded) sections. Even in these regions the system was able to detect and locate most houses correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>This paper introduced a system to detect street networks and houses in residential regions via satellite images. The system comprises four parts: multispectral information processing, segmentation, decomposition, and graph theoretical analysis.</p><p>To locate possible human activity we used the linearized vegetation index h, which gives a low response to rock derivatives. Since these are the basic blocks for most of the houses and streets in residential regions, their presence indicates possible human activity. And this was the case for our test images. Because surface water responds similarly, we introduced a shadow-water index, and used the combination of these indices to eliminate water regions. We believe other applications exist for these indices.</p><p>Having successfully identified the areas of human activity, we introduced a modified KMC algorithm to extract a binary segment representing possible houses, street networks, schools, malls, and parking lots. The novelty in our algorithm is its introduction of spatial coherence to clustering via connected components analysis.</p><p>To extract houses and street networks on this binary segment, we introduced a decomposition algorithm inspired by balloon algorithms. Our aim was to extract elongated structures (representing possible street segments) via this algorithm. To represent the binary image reliably (covering the curved regions, specifically) we have multiple balloon representations for the same region. We were able to overcome redundancy by applying a voting method. This overall scheme worked well.</p><p>We represented these balloons with a weighted a graph to extract street networks and houses. At this step we eliminated balloons representing neither a street nor a house structure. Therefore, at this step, a balloon in the graph is either representing a house or a street segment. By invoking simple unary and binary constraints, we were able to detect houses and street networks.</p><p>We finally tested our system with 44 residential images. Our performance on such a diverse and large test set is noteworthy. We plan to use the spatial constraints (formalizing them as of either probabilistic relaxation or in Bayesian networks) to refine our house and the street network labels. The overall system may ultimately find use as an automatic map generation system specialized for residential regions. end if end for end for H := VnR return H, R End.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. X Example. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this paper.) (A) The New Mexico image. (B) X Image (New Mexico).</figDesc><graphic coords="7,100.06,450.28,147.60,147.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. Detecting water regions. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this paper.) (A) The Indiana image. (B) c Image (Indiana). (C) The lake detected.</figDesc><graphic coords="9,115.37,107.30,87.48,86.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Sample iterations on the Indiana image by Algorithm 2, KMC-SC.</figDesc><graphic coords="10,100.56,498.53,110.16,110.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Test images and binary segments C o returned by Algorithm 2, KMC-SC, Part I.</figDesc><graphic coords="11,269.93,459.66,148.32,148.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(A) The New Mexico panchromatic image. (B) The binary segment (C o ). (D) The binary segment (C o ). (C) The South Dakota panchromatic image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Test images and binary segments C o returned by Algorithm 2, KMC-SC, Part II.</figDesc><graphic coords="12,126.14,459.56,148.32,148.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Comparison of segmentation algorithms. First row, Indiana image; second row, Maryland image; third row, New Mexico image; fourth row, South Dakota image. First column, KMC; second column, OtsuÕs method; third column, region growing; fourth column, KMC-SC.</figDesc><graphic coords="13,87.87,202.66,354.60,401.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(A) The initial segments, t s = 0.95. (B) The final segment (C o ). (C) # pixels vs. iteration. (G) The initial segment, t s = 0.40. (H) The final segment (C o ). (I) # pixels vs. iteration. (E) The final segment (C o ). (D) The initial segments, t s = 0.80. (F) # pixels vs. iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The Indiana image, segmentation results with different t s values.</figDesc><graphic coords="15,324.85,405.73,120.96,94.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(A) The object.(B) The initial balloon, labelled red.(C) The first iteration: blue, candidate points; green, accepted candidate sets.(D) The second iteration: yellow, rejected candidate sets; red, the final balloon.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Fig. 9. Extracting the binary balloon, an example. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this paper.)</figDesc><graphic coords="18,114.73,292.45,160.56,160.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. The South Dakota image, combination of balloons.</figDesc><graphic coords="20,179.94,428.13,196.78,196.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Spine extraction. (A) Filtered version of the boundary and the spine extracted. Solid curve, boundary; dashed curve, spine.</figDesc><graphic coords="22,281.62,107.54,163.87,129.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>(</head><label></label><figDesc>A) The South Dakota image. (B) The final street network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. The South Dakota image, street network extraction. Green sections are obtained by the graph theoretical method; red sections are obtained by street tracking. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this paper.)</figDesc><graphic coords="26,283.46,427.55,162.00,161.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. The Indiana image, street network and house detection.</figDesc><graphic coords="29,94.68,305.46,163.44,163.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. The Maryland image, street network and house detection. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this paper.) (C) Street network detection: green, correct; blue, miss; red, false alarm. (D) The overall detection: green, street network; red, house.</figDesc><graphic coords="30,113.88,302.80,162.72,162.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. The South Dakota image, street network and house detection. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this paper.)</figDesc><graphic coords="31,98.65,309.31,161.64,161.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 17</head><label>17</label><figDesc>Fig. 17. The South Dakota II image, street network and house detection. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this paper.)</figDesc><graphic coords="32,290.13,304.40,163.80,163.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>Fig. 17. The South Dakota II image, street network and house detection. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this paper.)</figDesc><graphic coords="32,111.27,304.40,163.80,163.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>Building detection review, reported performances</cell><cell></cell><cell></cell></row><row><cell>Approach</cell><cell>P d (%)</cell><cell>B f (%)</cell></row><row><cell>Shufelt and McKeown [8]</cell><cell>49.8-86.6</cell><cell>12.8-32.2</cell></row><row><cell>Maloof et al.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Building detection review, in MayerÕs format</figDesc><table><row><cell>Approach</cell><cell>Data</cell><cell>Model</cell><cell>Strategy</cell></row><row><cell>Huertas and Nevatia [7]</cell><cell>Low-medium</cell><cell>Simple</cell><cell>Medium</cell></row><row><cell>Shufelt and McKeown</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>Road detection review</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Approach</cell><cell>Data complexity</cell><cell>Representation</cell><cell>Resolution (m)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>Road detection review, reported performances</cell><cell></cell><cell></cell></row><row><cell>Approach</cell><cell>P d (%)</cell><cell>P f (%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="3">Comparison of performances, in percentages</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Indiana</cell><cell></cell><cell>Maryland</cell><cell></cell><cell cols="2">New Mexico</cell><cell cols="2">South Dakota</cell></row><row><cell></cell><cell>P d</cell><cell>P f</cell><cell>P d</cell><cell>P f</cell><cell>P d</cell><cell>P f</cell><cell>P d</cell><cell>P f</cell></row><row><cell>KMC</cell><cell>78.0</cell><cell>49.0</cell><cell>86.3</cell><cell>38.7</cell><cell>20.2</cell><cell>56.9</cell><cell>96.2</cell><cell>34.1</cell></row><row><cell>OtsuÕs method</cell><cell>91.9</cell><cell>6.4</cell><cell>85.3</cell><cell>1.6</cell><cell>96.7</cell><cell>36.6</cell><cell>98.2</cell><cell>7.9</cell></row><row><cell>Region growing</cell><cell>26.8</cell><cell>2.1</cell><cell>25.4</cell><cell>0.2</cell><cell>74.0</cell><cell>6.7</cell><cell>96.9</cell><cell>6.6</cell></row><row><cell>KMC-SC</cell><cell>94.4</cell><cell>5.6</cell><cell>96.4</cell><cell>1.0</cell><cell>95.3</cell><cell>13.2</cell><cell>98.7</cell><cell>3.3</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In the context of this paper, ''human activity'' means the presence of human-made materials (pavement, roofing, and so on) on the surface. C. U ¨nsalan, K.L. Boyer / Computer Vision and Image Understanding 98 (2005) 423-461</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>C. U ¨nsalan, K.L. Boyer / Computer Vision and Image Understanding 98 (2005) 423-461</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>C. U ¨nsalan, K.L. Boyer / Computer Vision and Image Understanding 98 (2005) 423-461</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3"><p>Although the methodology used to derive the land-cover indices first appeared in those papers, this paper presents the first publication of a shadow-water index.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>if ratio(v) &gt; 1.5 • t r then R := R [ {v} (Unary constraint) end if end for p (v i , v j ) = path connecting v i and v j ; set of vertices G 1 := Gnv i "v i such that ratio (v i ) &lt; t r and deg (v i ) &lt; 2 G 1 = (V 1 , E 1 ); V 1 = v 1 , v 2 , . . . , v k for i = 1 to k do for j = (i + 1) to k do if d (v i , v j ) P t d then R ;= R [ p(v i , v j ) (Binary constraint)</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Classifying land development in high resolution satellite images using straight line statistics</title>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Inter. Conf. on Pattern Recognition</title>
		<meeting>Inter. Conf. on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="127" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Classifying land development in high resolution panchromatic satellite images using straight line statistics</title>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. GeoRS</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="907" to="919" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A theoretical and experimental investigation of graph theoretical measures for land development in satellite imagery</title>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Boyer</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mapping from digital satellite image data with special reference to MOMS-02</title>
		<author>
			<persName><forename type="first">G</forename><surname>Konecny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schiewe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogrammetry Remote Sens</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="173" to="181" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic object extraction from aerial imagery-A survey focusing on buildings</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="138" to="149" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Performance evaluation and analysis of monocular building extraction from aerial imagery</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Shufelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="311" to="326" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detecting buildings in aerial images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Huertas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Graph. Image Process</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="131" to="152" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fusion of monocular cues to detect man made structures in aerial imagery</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shufelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVGIP: Image Understand</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="307" to="330" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Delineating buildings by grouping lines with MRFs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnamachari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. IP</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="164" to="168" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to detect rooftops in aerial images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Maloof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Binford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1997 Image Understanding Workshop DARPA97</title>
		<meeting>1997 Image Understanding Workshop DARPA97</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="835" to="845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hierarchical Bayesian nets for building extraction using dense digital surface models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Weidner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Photogrammetry Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="296" to="307" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimisation of building detection in satellite images by combining multispectral classification and texture filtering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Photogrammetry Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="50" to="60" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Development of a graph based approach for building detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="14" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Building detection using Bayesian networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stassopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Caelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inter. J. Pattern Recognit. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="715" to="733" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detection and extraction of buildings from iterferometric SAR data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gamba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Houshmand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saccani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. GeoRS</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="611" to="618" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detection and modeling of buildings from multiple aerial images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Noronha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="501" to="518" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Building detection from multiple aerial images in dense urban areas</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fradkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Maitre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="181" to="207" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust detection of buildings in digital surface models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Inter. Conf. on Pattern Recognition</title>
		<meeting>Inter. Conf. on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="159" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Building detection and description from a single intensity image</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="101" to="121" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">O</forename><surname>Jaynes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Stolle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Riseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Hanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Ascender system: automated site modeling from multiple aerial images</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="143" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Road extraction from aerial imagery</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>CV-Online</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Computer recognition of roads from satellite pictures</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bajcsy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Takavoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. SMC</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="623" to="637" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A knowledge based system for highway network extraction</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Newkirk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. GeoRS</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="525" to="531" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic comparison of a topographic map with remotely sensed images in a map updating perspective: the road network case</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fiset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cavayas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inter. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="991" to="1006" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Robust detection of straight and circular road segments in noisy aerial images</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Netanyahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Philomin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Stromberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1673" to="1686" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Detection of linear features in SAR images: application to road network extraction</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tupin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Maitre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Mangin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Nicolas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pechersky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. GeoRS</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="434" to="453" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A thinning based method for recognizing and extracting periurban road networks from SPOT panchromatic images</title>
		<author>
			<persName><forename type="first">V</forename><surname>Karathanassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Iossifidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rokos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inter. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="153" to="168" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automatic extraction of roads from aerial images based on scale space and snakes</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baumgartner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vis. Appl</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="23" to="31" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Road detection in spaceborne SAR images using a genetic algorithm</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. GeoRS</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The line segment match method for extracting road network from high-resolution satellite images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. GeoRS</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="511" to="514" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Region extraction and shape analysis in aerial photographs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nagao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Matsuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ikeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Graph. Image Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="195" to="223" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A Structural Analysis of Complex Aerial Photographs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nagao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Matsuyama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
			<publisher>Plenum Press</publisher>
			<pubPlace>New York, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hypothesis integration in image understanding systems</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Matsuyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Graph. Image Process</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="321" to="371" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Monitoring the vernal advancement of natural vegetation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Rouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Deering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Harlan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974">1974</date>
			<pubPlace>Greenbelt, MD</pubPlace>
		</imprint>
		<respStmt>
			<orgName>NASA/GCSFC</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Final report</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Linearized vegetation indices using a formal statistical framework</title>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IGARSS03</title>
		<meeting>of the IGARSS03</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Linearized vegetation indices based on a formal statistical framework</title>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. GeoRS</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1575" to="1585" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Avery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Berlin</surname></persName>
		</author>
		<title level="m">Fundamentals of Remote Sensing and Airphoto Interpretation</title>
		<imprint>
			<publisher>Macmillan Publishing</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note>fifth ed.</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Remote sensing of estuaries: an overview</title>
		<author>
			<persName><forename type="first">V</forename><surname>Klemas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>IEEE Ocean</publisher>
			<biblScope unit="page" from="302" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Detection of coastlines in SAR images using wavelet methods</title>
		<author>
			<persName><forename type="first">A</forename><surname>Niedermeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Romaneessen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lehner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. GeoRS</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2270" to="2281" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Arctic sea ice, cloud, water and lead classification using neural networks and 1.6-lm data</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Mcintire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Simpson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. GeoRS</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1956" to="1972" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Detecting clouds and cloud shadows on aerial photographs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="page" from="55" to="64" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Detecting clouds and cloud shadows in multispectral satellite images for tropical areas</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T S</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IPA</title>
		<meeting>of IPA</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="848" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Estimation of subpixel vegetation density of natural regions using satellite multispectral imagery</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Jasinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. GeoRS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="804" to="813" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A procedure for the detection and removal of cloud shadow from AVHRR data over land</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Stitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. GeoRS</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="880" to="897" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Development of cloud, snow, and shadow masking algorithms for vegetation imagery</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lisens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kempencers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fierens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Rensbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IGARRS2000</title>
		<meeting>of IGARRS2000</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="834" to="836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cloud shadow detection under arbitrary viewing and illumination conditions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Stitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. GeoRS</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="972" to="976" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Change detection with 1 m resolution satellite and aerial images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Spitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Franck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kollewe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rothkirch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wiemker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IGARRS</title>
		<meeting>of IGARRS</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="2256" to="2258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Clustering Algorithms</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hartigan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<title level="m">Pattern Classification</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley Interscience</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Statistical pattern recognition: a review</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P W</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="37" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A threshold selection method from gray-level histograms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. SMC</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="62" to="66" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Seeded region growing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="641" to="647" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Sonka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hlavac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boyle</surname></persName>
		</author>
		<title level="m">Image Processing, Analysis and Machine Vision</title>
		<meeting><address><addrLine>Pacific Grove, CA</addrLine></address></meeting>
		<imprint>
			<publisher>PWS Publications</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Snakes: active contour models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inter. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="page" from="321" to="331" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Gonzales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Woods</surname></persName>
		</author>
		<title level="m">Digital Image Processing</title>
		<meeting><address><addrLine>Reading, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Brualdi</surname></persName>
		</author>
		<title level="m">Introductory Combinatorics</title>
		<meeting><address><addrLine>New Jersey</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>third ed.</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Extracting straight lines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Riseman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="425" to="455" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Oppenheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Buck</surname></persName>
		</author>
		<title level="m">Discrete Time Signal Processing</title>
		<meeting><address><addrLine>New Jersey</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><surname>Carmo</surname></persName>
		</author>
		<title level="m">Differential Geometry of Curves and Surfaces</title>
		<meeting><address><addrLine>New Jersey</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Straight</surname></persName>
		</author>
		<author>
			<persName><surname>Combinatorics</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>An Invitation, Brooks/Cole Publishing</publisher>
			<pubPlace>California</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">The Theory of Graphs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Berge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Dover Publications</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Optimization Algorithms for Networks and Graphs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Minieka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Dekker</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Cooperative methods for road tracking in aerial imagery</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Denlinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR1988</title>
		<meeting>of CVPR1988</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="662" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Extracting road networks from natural terrain for visualization</title>
		<author>
			<persName><forename type="first">E</forename><surname>Riseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hanechak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth Inter. Conf. on Computer Graphics and Visualization</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
