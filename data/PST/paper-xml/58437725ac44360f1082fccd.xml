<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ACHIEVING HUMAN PARITY IN CONVERSATIONAL SPEECH RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-02">Revised February 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">J</forename><surname>Droppo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">X</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">F</forename><surname>Seide</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">M</forename><surname>Seltzer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">A</forename><surname>Stolcke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">D</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">G</forename><surname>Zweig</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ACHIEVING HUMAN PARITY IN CONVERSATIONAL SPEECH RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-02">Revised February 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">926AD92A67E3F190BCFDEEE4F3E2DC32</idno>
					<idno type="arXiv">arXiv:1610.05256v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Conversational speech recognition</term>
					<term>convolutional neural networks</term>
					<term>recurrent neural networks</term>
					<term>VGG</term>
					<term>ResNet</term>
					<term>LACE</term>
					<term>BLSTM</term>
					<term>spatial smoothing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conversational speech recognition has served as a flagship speech recognition task since the release of the Switchboard corpus in the 1990s. In this paper, we measure the human error rate on the widely used NIST 2000 test set, and find that our latest automated system has reached human parity. The error rate of professional transcribers is 5.9% for the Switchboard portion of the data, in which newly acquainted pairs of people discuss an assigned topic, and 11.3% for the CallHome portion where friends and family members have open-ended conversations. In both cases, our automated system establishes a new state of the art, and edges past the human benchmark, achieving error rates of 5.8% and 11.0%, respectively. The key to our system's performance is the use of various convolutional and LSTM acoustic model architectures, combined with a novel spatial smoothing method and lattice-free MMI acoustic training, multiple recurrent neural network language modeling approaches, and a systematic use of system combination.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Recent years have seen human performance levels reached or surpassed in tasks ranging from the games of chess and Go <ref type="bibr" target="#b4">[1,</ref><ref type="bibr" target="#b5">2]</ref> to simple speech recognition tasks like carefully read newspaper speech <ref type="bibr" target="#b6">[3]</ref> and rigidly constrained smallvocabulary tasks in noise <ref type="bibr" target="#b7">[4,</ref><ref type="bibr">5]</ref>. In the area of speech recognition, much of the pioneering early work was driven by a series of carefully designed tasks with DARPA-funded datasets publicly released by the LDC and NIST <ref type="bibr" target="#b9">[6]</ref>: first simple ones like the "resource management" task <ref type="bibr" target="#b10">[7]</ref> with a small vocabulary and carefully controlled grammar; then read speech recognition in the Wall Street Journal task <ref type="bibr">[8]</ref>; then Broadcast News <ref type="bibr">[9]</ref>; each progressively more difficult for automatic systems. One of last big initiatives in this area was in conversational telephone speech (CTS), which is especially difficult due to the spontaneous (neither read nor planned) nature of the speech, its informality, and the self-corrections, hesitations and other disfluencies that are pervasive. The Switchboard <ref type="bibr" target="#b13">[10]</ref> and later Fisher <ref type="bibr" target="#b14">[11]</ref> data collections of the 1990s and early 2000s provide what is to date the largest and best studied of the conversational corpora. The history of work in this area includes key contributions by institutions such as IBM <ref type="bibr" target="#b15">[12]</ref>, BBN <ref type="bibr" target="#b16">[13]</ref>, SRI <ref type="bibr" target="#b17">[14]</ref>, AT&amp;T <ref type="bibr" target="#b18">[15]</ref>, LIMSI <ref type="bibr" target="#b19">[16]</ref>, Cambridge University <ref type="bibr" target="#b20">[17]</ref>, Microsoft <ref type="bibr" target="#b21">[18]</ref> and numerous others. In the past, human performance on this task has been widely cited as being 4% <ref type="bibr" target="#b22">[19]</ref>. However, the error rate estimate in <ref type="bibr" target="#b22">[19]</ref> is attributed to a "personal communication," and the actual source of this number is ephemeral. To better understand human performance, we have used professional transcribers to transcribe the actual test sets that we are working with, specifically the CallHome and Switchboard portions of the NIST eval 2000 test set. We find that the human error rates on these two parts are different almost by a factor of two, so a single number is inappropriate to cite. The error rate on Switchboard is about 5.9%, and for CallHome 11.3%. We improve on our recently reported conversational speech recognition system <ref type="bibr" target="#b23">[20]</ref> by about 0.4%, and now exceed human performance by a small margin. Our progress is a result of the careful engineering and optimization of convolutional and recurrent neural networks. While the basic structures have been well known for a long period <ref type="bibr" target="#b24">[21,</ref><ref type="bibr" target="#b25">22,</ref><ref type="bibr" target="#b26">23,</ref><ref type="bibr" target="#b27">24,</ref><ref type="bibr" target="#b28">25,</ref><ref type="bibr" target="#b29">26,</ref><ref type="bibr" target="#b30">27]</ref>, it is only recently that they have dominated the field as the best models for speech recognition. Surprisingly, this is the case for both acoustic modeling <ref type="bibr" target="#b31">[28,</ref><ref type="bibr" target="#b32">29,</ref><ref type="bibr" target="#b33">30,</ref><ref type="bibr" target="#b34">31,</ref><ref type="bibr" target="#b35">32,</ref><ref type="bibr" target="#b36">33]</ref> and language modeling <ref type="bibr" target="#b37">[34,</ref><ref type="bibr" target="#b38">35,</ref><ref type="bibr" target="#b39">36,</ref><ref type="bibr" target="#b40">37]</ref>. In comparison to the standard feed-forward MLPs or DNNs that first demonstrated breakthrough performance on conversational speech recognition <ref type="bibr" target="#b21">[18]</ref>, these acoustic models have the ability to model a large amount of acoustic context with temporal invariance, and in the case of convolutional models, with frequency invariance as well. In language modeling, recurrent models appear to improve over classical N-gram models through the use of an unbounded word history, as well as the generalization ability of continuous word representations <ref type="bibr" target="#b41">[38]</ref>. In the meantime, ensemble learning has become commonly used in several neural models <ref type="bibr" target="#b42">[39,</ref><ref type="bibr" target="#b43">40,</ref><ref type="bibr" target="#b38">35]</ref>, to improve robustness by reducing bias and variance. This paper is an expanded version of <ref type="bibr" target="#b23">[20]</ref>, with the following additional material: The remainder of this paper describes our system in detail. Section 2 describes our measurement of human performance. Section 3 describes the convolutional neural net (CNN) and long-short-term memory (LSTM) models. Section 4 describes our implementation of i-vector adaptation. Section 5 presents out lattice-free MMI training process. Language model rescoring is a significant part of our system, and described in Section 6. We describe the CNTK toolkit that forms the basis of our neural network models in Section 7.</p><p>Experimental results are presented in Section 8, followed by an error analysis in section 9, a review of relevant prior work in 10 and concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">HUMAN PERFORMANCE</head><p>To measure human performance, we leveraged an existing pipeline in which Microsoft data is transcribed on a weekly basis. This pipeline uses a large commercial vendor to perform two-pass transcription. In the first pass, a transcriber works from scratch to transcribe the data. In the second pass, a second listener monitors the data to do error correction. Dozens of hours of test data are processed in each batch.</p><p>One week, we added the NIST 2000 CTS evaluation data to the work-list, without further comment. The intention was to measure the error rate of professional transcribers going about their normal everyday business. Aside from the standard twopass checking in place, we did not do a complex multi-party transcription and adjudication process. The transcribers were given the same audio segments as were provided to the speech recognition system, which results in short sentences or sentence fragments from a single channel. This makes the task easier since the speakers are more clearly separated, and more difficult since the two sides of the conversation are not interleaved. Thus, it is the same condition as reported for our automated systems. The resulting numbers are 5.9% for the Switchboard portion, and 11.3% for the CallHome portion of the NIST 2000 test set, using the NIST scoring protocol. These numbers should be taken as an indication of the "error rate" of a trained professional working in industry-standard speech transcript production. (We have submitted the human transcripts thus produced to the Linguistic Data Consortium for publication, so as to facilitate research by other groups.) Past work <ref type="bibr" target="#b44">[41]</ref> reports inter-transcriber error rates for data taken from the later RT03 test set (which contains Switchboard and Fisher, but no CallHome data). Error rates of 4.1 to 4.5% are reported for extremely careful multiple transcriptions, and 9.6% for "quick transcriptions." While this is a different test set, the numbers are in line with our findings. We note that the bulk of the Fisher training data, and the bulk of the data overall, was transcribed with the "quick transcription" guidelines. Thus, the current state of the art is actually far exceeding the noise level in its own training data. Perhaps the most important point is the extreme variability between the two test subsets. The more informal CallHome data has almost double the human error rate of the Switchboard data. Interestingly, the same informality, multiple speakers per channel, and recording conditions that make CallHome hard for computers make it difficult for people as well. Notably, the performance of our artificial system aligns almost exactly with the performance of people on both sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CONVOLUTIONAL AND LSTM NEURAL NETWORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">CNNs</head><p>We use three CNN variants. The first is the VGG architecture of <ref type="bibr" target="#b45">[42]</ref>. Compared to the networks used previously in image recognition, this network uses small (3x3) filters, is deeper, and applies up to five convolutional layers before pooling.</p><p>The second network is modeled on the ResNet architecture <ref type="bibr" target="#b46">[43]</ref>, which adds highway connections <ref type="bibr" target="#b47">[44]</ref>, i.e. a linear transform of each layer's input to the layer's output <ref type="bibr" target="#b47">[44,</ref><ref type="bibr" target="#b48">45]</ref>. The only difference is that we apply Batch Normalization before computing ReLU activations. The last CNN variant is the LACE (layer-wise context expansion with attention) model <ref type="bibr" target="#b49">[46]</ref>. LACE is a TDNN <ref type="bibr" target="#b26">[23]</ref> variant in which each higher layer is a weighted sum of nonlinear transformations of a window of lower layer frames. In other words, each higher layer exploits broader context than lower layers. Lower layers focus on extracting simple local patterns while higher layers extract complex patterns that cover broader contexts. Since not all frames in a window carry the same importance, an attention mask is applied. The LACE model differs from the earlier TDNN models e.g. <ref type="bibr" target="#b26">[23,</ref><ref type="bibr" target="#b50">47]</ref> in the use of a learned attention mask and ResNet like linear pass-through. As illustrated in detail in Figure <ref type="figure">1</ref>, the model is composed of four blocks, each with the same architecture. Each block starts Fig. <ref type="figure">1</ref>. LACE network architecture with a convolution layer with stride 2 which sub-samples the input and increases the number of channels. This layer is followed by four RELU-convolution layers with jump links similar to those used in ResNet. Table <ref type="table" target="#tab_1">1</ref> compares the layer structure and parameters of the three CNN architectures. We also trained a fused model by combining a ResNet model and a VGG model at the senone posterior level. Both base models are independently trained, and then the score fusion weight is optimized on development data. The fused system is our best single system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">LSTMs</head><p>While our best performing models are convolutional, the use of long short-term memory networks (LSTMs) is a close second. We use a bidirectional architecture <ref type="bibr" target="#b51">[48]</ref> without frameskipping <ref type="bibr" target="#b32">[29]</ref>. The core model structure is the LSTM defined in <ref type="bibr" target="#b31">[28]</ref>. We found that using networks with more than six layers did not improve the word error rate on the development set, and chose 512 hidden units, per direction, per layer, as that provided a reasonable trade-off between training time and final model accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Spatial Smoothing</head><p>Inspired by the human auditory cortex, where neighboring neurons tend to simultaneously activate, we employ a spatial smoothing technique to improve the accuracy of our LSTM models. The smoothing is implemented as a regularization term on the activations between layers of the acoustic Second, this image is high-pass filtered. The filter is implemented as a circular convolution with a 3 by 3 kernel. The center tap of the kernel has a value of 1, and the remaining eight having a value of -1/8. Third, the energy of this highpass filtered image is computed and added to the training objective function. We have found empirically that a suitable scale for this energy is 0.1 with respect to the existing cross entropy objective function. The overall effect of this process is to make the training algorithm prefer models that have correlated neurons, and to improve the word error rate of the acoustic model. Table <ref type="table" target="#tab_2">2</ref> shows the benefit in error rate for some of our early systems. We observed error reductions of between 5 and 10% relative from spatial smoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SPEAKER ADAPTIVE MODELING</head><p>Speaker adaptive modeling in our system is based on conditioning the network on an i-vector <ref type="bibr" target="#b52">[49]</ref> characterization of each speaker <ref type="bibr" target="#b53">[50,</ref><ref type="bibr" target="#b54">51]</ref>. A 100-dimensional i-vector is generated for each conversation side. For the LSTM system, the conversation-side i-vector v s is appended to each frame of input. For convolutional networks, this approach is inappropriate because we do not expect to see spatially contiguous patterns in the input. Instead, for the CNNs, we add a learnable weight matrix W l to each layer, and add W l v s to the activation of the layer before the nonlinearity. Thus, in the CNN, the i-vector essentially serves as an speaker-dependent bias to each layer. Note that the i-vectors are estimated using MFCC features; by using them subsequently in systems based on log-filterbank features, we may benefit from a form of feature combination. Performance improvements from ivectors are shown in Table <ref type="table" target="#tab_4">3</ref>. The full experimental setup is described in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">LATTICE-FREE SEQUENCE TRAINING</head><p>After standard cross-entropy training, we optimize the model parameters using the maximum mutual information (MMI) objective function. Denoting a word sequence by w and its corresponding acoustic realization by a, the training criterion is w,a∈data log P (w)P (a|w)</p><p>w P (w )P (a|w )</p><p>.</p><p>As noted in <ref type="bibr" target="#b55">[52,</ref><ref type="bibr" target="#b56">53]</ref>, the necessary gradient for use in backpropagation is a simple function of the posterior probability of a particular acoustic model state at a given time, as computed by summing over all possible word sequences in an unconstrained manner. As first done in <ref type="bibr" target="#b15">[12]</ref>, and more recently in <ref type="bibr" target="#b57">[54]</ref>, this can be accomplished with a straightforward alphabeta computation over the finite state acceptor representing the decoding search space. In <ref type="bibr" target="#b15">[12]</ref>, the search space is taken to be an acceptor representing the composition HCLG for a unigram language model L on words. In <ref type="bibr" target="#b57">[54]</ref>, a language model on phonemes is used instead. In our implementation, we use a mixed-history acoustic unit language model. In this model, the probability of transitioning into a new context-dependent phonetic state (senone) is conditioned on both the senone and phone history. We found this model to perform better than either purely word-based or phone-based models. Based on a set of initial experiments, we developed the following procedure:</p><p>1. Perform a forced alignment of the training data to select lexical variants and determine frame-aligned senone sequences.</p><p>2. Compress consecutive framewise occurrences of a single senone into a single occurrence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">LM RESCORING AND SYSTEM COMBINATION</head><p>An initial decoding is done with a WFST decoder, using the architecture described in <ref type="bibr" target="#b58">[55]</ref>. We use an N-gram language model trained and pruned with the SRILM toolkit <ref type="bibr" target="#b59">[56]</ref>. The first-pass LM has approximately 15.9 million bigrams, trigrams, and 4grams, and a vocabulary of 30,500 words. It gives a perplexity of 69 on the 1997 CTS evaluation transcripts. The initial decoding produces a lattice with the pronunciation variants marked, from which 500-best lists are generated for rescoring purposes. Subsequent N-best rescoring uses an unpruned LM comprising 145 million N-grams.</p><p>All N-gram LMs were estimated by a maximum entropy criterion as described in <ref type="bibr" target="#b60">[57]</ref>. The N-best hypotheses are then rescored using a combination of the large N-gram LM and several neural net LMs. We have experimented with both RNN LMs and LSTM LMs, and describe the details in the following two sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">RNN-LM setup</head><p>Our RNN-LMs are trained and evaluated using the CUED-RNNLM toolkit <ref type="bibr" target="#b61">[58]</ref>. Our RNN-LM configuration has several distinctive features, as described below.</p><p>1. We trained both standard, forward-predicting RNN-LMs and backward RNN-LMs that predict words in 4. We found best results with an RNN-LM configuration that had a second, non-recurrent hidden layer. This produced lower perplexity and word error than the standard, single-hidden-layer RNN-LM architecture <ref type="bibr" target="#b37">[34]</ref>. 1  The overall network architecture thus had two hidden layers with 1000 units each, using ReLU nonlinearities. Training used noise-contrastive estimation (NCE) <ref type="bibr" target="#b62">[59]</ref>.</p><p>5. The RNN-LM output vocabulary consists of all words occurring more than once in the in-domain training set. While the RNN-LM estimates a probability for unknown words, we take a different approach in rescoring: The number of out-of-set words is recorded for each hypothesis and a penalty for them is estimated for them when optimizing the relative weights for all model scores (acoustic, LM, pronunciation), using the SRILM nbest-optimize tool.</p><p>1 However, adding more hidden layers produced no further gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">LSTM-LM setup</head><p>After obtaining good results with RNN-LMs we also explored the LSTM recurrent network architecture for language modeling, inspired by recent work showing gains over RNN-LMs for conversational speech recognition <ref type="bibr" target="#b40">[37]</ref>. In addition to applying the lessons learned from our RNN-LM experiments, we explored additional alternatives, as described below.</p><p>1. There are two types of input vectors our LSTM LMs take, word based one-hot vector input and letter trigram vector <ref type="bibr" target="#b63">[60]</ref> input. Including both forward and backward models, we trained four different LSTM LMs in total.</p><p>2. For the word based input, we leveraged the approach from <ref type="bibr" target="#b64">[61]</ref> to tie the input embedding and output embedding together.</p><p>3. Here we also used a two-phase training schedule to train the LSTM LMs. First we train the model on the combination of in-domain and out-domain data for four data passes without any learning rate adjustment.</p><p>We then start from the resulting model and train on in-domain data until convergence.</p><p>4. Overall the letter trigram based models perform a little better than the word based language model. We tried applying dropout on both types of language models but didn't see an improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Convergence was improved through a variation of selfstabilization <ref type="bibr" target="#b65">[62]</ref>, in which each output vector x of nonlinearities are scaled by 1  4 ln(1 + e 4β ), where a β is a scalar that is learned for each output. This has a similar effect as the scale of the well-known batch normalization technique, but can be used in recurrent loops.</p><p>6. Table <ref type="table" target="#tab_5">4</ref> shows the impact of number of layers on the final perplexities. Based on this, we proceeded with three hidden layers, with 1000 hidden units each. The perplexities of each LSTM-LM we used in the final combination (before interpolating with the N-gram model) can be found in Table <ref type="table" target="#tab_6">5</ref>.</p><p>For the final system, we interpolated two LSTM-LMs with an N-gram LM for the forward-direction LM, and similarly for the backward-direction LM. All LSTMs use three hidden layers and are trained on in-domain and web data. Unlike for the RNN-LMs, the two models being interpolated differ not just in their random initialization, but also in the input encoding (one uses a triletter encoding, the other a one-hot word encoding). The forward and backward LM log probability scores are combined additively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Training data</head><p>The 4-gram language model for decoding was trained on the available CTS transcripts from the DARPA EARS program: Switchboard (3M words), BBN Switchboard-2 transcripts (850k), Fisher (21M), English CallHome (200k), and the Uni- For the unpruned large rescoring 4-gram, an additional LM component was added, trained on 133M word of LDC Broadcast News texts. The N-gram LM configuration is modeled after that described in <ref type="bibr" target="#b54">[51]</ref>, except that maxent smoothing was used. The RNN and LSTM LMs were trained on Switchboard and Fisher transcripts as in-domain data (20M words for gradient computation, 3M for validation). To this we added 62M words of UW Web data as out-of-domain data, for use in the two-phase training procedure described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">RNN-LM and LSTM-LM performance</head><p>Table <ref type="table" target="#tab_7">6</ref> gives perplexity and word error performance for various recurrent neural net LM setups, from simple to more complex. The acoustic model used was the ResNet CNN. Note that, unlike the results in Tables <ref type="table" target="#tab_5">4</ref> and<ref type="table" target="#tab_6">5</ref>, the neural net LMs in Table <ref type="table" target="#tab_7">6</ref> are interpolated with the N-gram LM. As can be seen, each of the measures described earlier adds incremental gains, which, however, add up to a substantial improvement overall. The total gain relative to a purely N-gram based system is a 20% relative error reduction with RNN-LMs, and 23% with LSTM-LMs. As shown later (see Table <ref type="table" target="#tab_9">8</ref>) the gains with different acoustic models are similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">System Combination</head><p>The LM rescoring is carried out separately for each acoustic model. The rescored N-best lists from each subsystem are then aligned into a single confusion network <ref type="bibr" target="#b66">[63]</ref> using the SRILM nbest-rover tool. However, the number of potential candidate systems is too large to allow an all-out combination, both for practical reasons and due to overfitting issues. Instead, we perform a greedy search, starting with the single best system, and successively adding additional systems, to find a small set of systems that are maximally complementary. The RT-02 Switchboard set was used for this search procedure. We experimented with two search algorithms to find good subsets of systems. We always start with the system giving the best individual accuracy on the development set.</p><p>In one approach, a greedy forward search then adds systems incrementally to the combination, giving each equal weight. If no improvement is found with any of the unused systems, we try adding each with successively lower relative weights of 0.5, 0.2, and 0.1, and stop if none of these give an improvement. A second variant of the search procedure that can give lower error (as measured on the devset) estimates the best system weights for each incremental combination candidate. The weight estimation is done using an expectation-maximization algorithm based on aligning the reference words to the confusion networks, and maximizing the weighted probability of the correct word at each alignment position. To avoid overfitting, the weights for an N -way combination are smoothed hi-erarchically, i.e., interpolated with the weights from the (N -1)-way system that preceded it. This tends to give robust weights that are biased toward the early (i.e., better) subsystems. The final system incorporated a variety of BLSTM models with roughly similar performance, but differing in various metaparameters (number of senones, use of spatial smoothing, and choice of pronunciation dictionaries). <ref type="foot" target="#foot_0">2</ref> To further limit the number of free parameters to be estimated in system combination, we performed system selection in two stages. First, we selected the four best BLSTM systems. We then combined these with equal weights and treated them as a single subsystem in searching for a larger combination including other acoustic models. This yielded our best overall combined system, as reported in Section 8.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">MICROSOFT COGNITIVE TOOLKIT (CNTK)</head><p>All neural networks in the final system were trained with the Microsoft Cognitive Toolkit, or CNTK <ref type="bibr" target="#b67">[64,</ref><ref type="bibr" target="#b68">65]</ref>. on a Linux-based multi-GPU server farm. CNTK allows for flexible model definition, while at the same time scaling very efficiently across multiple GPUs and multiple servers. The resulting fast experimental turnaround using the full 2000hour corpus was critical for our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Flexible, Terse Model Definition</head><p>In CNTK, a neural network (and the training criteria) are specified by its using a custom functional language (BrainScript), or Python. A graph-based execution engine, which provides automatic differentiation, then trains the model's parameters through SGD. Leveraging a stock library of common layer types, networks can be specified very tersely. Samples can be found in <ref type="bibr" target="#b67">[64]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Multi-Server Training using 1-bit SGD</head><p>Training the acoustic models in this paper on a single GPU would take many weeks or even months. CNTK made training times feasible by parallelizing the SGD training with our 1-bit SGD parallelization technique <ref type="bibr" target="#b69">[66]</ref>. This data-parallel method distributes minibatches over multiple worker nodes, and then aggregates the sub-gradients. While the necessary communication time would otherwise be prohibitive, the 1bit SGD method eliminates the bottleneck by two techniques:</p><p>1-bit quantization of gradients and automatic minibatch-size scaling. In <ref type="bibr" target="#b69">[66]</ref>, we showed that gradient values can be quantized to just a single bit, if one carries over the quantization error from one minibatch to the next. Each time a sub-gradient is quantized, the quantization error is computed and remembered, and then added to the next minibatch's sub-gradient.</p><p>This reduces the required bandwidth 32-fold with minimal loss in accuracy. Secondly, automatic minibatch-size scaling progressively decreases the frequency of model updates. At regular intervals (e.g. every 72h of training data), the trainer tries larger minibatch sizes on a small subset of data and picks the largest that maintains training loss. These two techniques allow for excellent multi-GPU/server scalability, and reduced the acoustic-model training times on 2000h from months to between 1 and 3 weeks, making this work feasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Computational performance</head><p>Table <ref type="table" target="#tab_8">7</ref> compares the runtimes, as multiples of speech duration, of various processing steps associated with the different acoustic model architectures (figures for DNNs are given only as a reference point, since they are not used in our system). Acoustic model (AM) training comprises the forward and backward dynamic programming passes, as well as parameter updates. AM evaluation refers to the forward computation only. Decoding includes AM evaluation along with hypothesis search (only the former makes use of the GPU). Runtimes were measured on a 12-core Intel Xeon E5-2620v3 CPU clocked at 2.4GHz, with an Nvidia Titan X GPU. We observe that the GPU gives a 10 to 100-fold speedup for AM evaluation over the CPU implementation. AM evaluation is thus reduced to a small faction of overall decoding time, making near-realtime operation possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">Speech corpora</head><p>We train with the commonly used English CTS (Switchboard and Fisher) corpora. Evaluation is carried out on the NIST 2000 CTS test set, which comprises both Switchboard (SWB) and CallHome (CH) subsets. The waveforms were segmented according to the NIST partitioned evaluation map (PEM) file, with 150ms of dithered silence padding added in the case of the CallHome conversations. <ref type="foot" target="#foot_1">3</ref> The Switchboard-1 portion of the NIST 2002 CTS test set was used for tuning and development. The acoustic training data is comprised by LDC corpora 97S62, 2004S13, 2005S13, 2004S11 and 2004S09; see <ref type="bibr" target="#b15">[12]</ref> for a full description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Acoustic Model Details</head><p>Forty-dimensional log-filterbank features were extracted every 10 milliseconds, using a 25-millisecond analysis window.</p><p>The CNN models used window sizes as indicated in Table <ref type="table" target="#tab_1">1</ref>, and the LSTMs processed one frame of input at a time. The bulk of our models use three state left-to-right triphone models with 9000 tied states. Additionally, we have trained several models with 27k tied states. The phonetic inventory in- cludes special models for noise, vocalized-noise, laughter and silence. We use a 30k-vocabulary derived from the most common words in the Switchboard and Fisher corpora. The decoder uses a statically compiled unigram graph, and dynamically applies the language model score. The unigram graph has about 300k states and 500k arcs. Table <ref type="table" target="#tab_4">3</ref> shows the result of i-vector adaptation and LFMMI training on several of our early systems. We achieve a 5-8% relative improvement from i-vectors, including on CNN systems. The last row of Table <ref type="table" target="#tab_4">3</ref> shows the effect of LFMMI training on the different models.</p><p>We see a consistent 7-10% further relative reduction in error rate for all models. Considering the great increase in procedural simplicity of LFMMI over the previous practice of writing lattices and post-processing them, we consider LFMMI to be a significant advance in technology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.">Overall Results and Discussion</head><p>The performance of all our component models is shown in Table <ref type="table" target="#tab_9">8</ref>, along with the BLSTM combination and full system combination results. (Recall that the four best BLSTM systems are combined with equal weights first, as described in Section 6.5.) Key benchmarks from the literature, our own best results, and the measured human error rates are compared in Table <ref type="table" target="#tab_10">9</ref>. <ref type="foot" target="#foot_2">4</ref> All models listed in Table <ref type="table" target="#tab_9">8</ref> are selected for the combined systems for one or more of the three rescoring LMs. The only exception is the VGG+ResNet system, which combines acoustic senone posteriors from the VGG and ResNet networks. While this yields our single best acoustic model, only the individual VGG and ResNet models are used in the overall system combination. We also observe that the four model variants chosen for the combined BLSTM subsystem differ incrementally by one hyperparameter (smooth-ing, number of senones, dictionary), and that the BLSTMs alone achieve an error that is within 3% relative of the full system combination. This validates the rationale that choosing different hyperparameters is an effective way to obtain complementary systems for combination purposes. We also assessed the lower bound of performance for our lattice/Nbest rescoring paradigm. The 500-best lists from the lattices generated with the ResNet CNN system had an oracle (lowest achievable) WER of 2.7% on the Switchboard portion of the NIST 2000 evaluation set, and an oracle WER of 4.9% on the CallHome portion. The oracle error of the combined system is even lower (though harder to quantify) since (1) N-best output from all systems are combined and (2) confusion network construction generates new possible hypotheses not contained in the original N-best lists. With oracle error rates less than half the currently achieved actual error rates, we conclude that search errors are not a major limiting factor to even better accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">ERROR ANALYSIS</head><p>In this section, we compare the errors made by our artificial recognizer with those made by human transcribers. We find that the machine errors are substantially the same as human ones, with one large exception: confusions between backchannel words and hesitations. The distinction is that backchannel words like "uh-huh" are an acknowledgment of the also signaling that the speaker should keep talking, while hesitations like "uh" are used to indicate that the current speaker has more to say and wants to keep his or her turn. <ref type="foot" target="#foot_3">5</ref> As turn-management devices, these two classes of words therefore have exactly opposite functions. Table <ref type="table" target="#tab_1">10</ref> shows the ten most common substitutions for both humans and the artificial system. Tables <ref type="table" target="#tab_1">11</ref> and<ref type="table" target="#tab_2">12</ref> do the same for deletions and insertions. Focusing on the substitutions, we see that by far the most common error in the ASR system is the confusion of a hesitation in the reference for a backchannel in the hypothesis. People do not seem to have this problem. We speculate that this is due to the nature of the Fisher training corpus, where the "quick transcription" guidelines were predominately used <ref type="bibr" target="#b44">[41]</ref>. We find that there is inconsistent treatment of backchannel and hesitation in the resulting data; the relatively poor performance of the automatic system here might simply be due to confusions in the training data annotations. For perspective, there are over twenty-one thousand words in each test set. Thus the errors due to hesitation/backchannel substitutions account for an error rate of only about 0.2% absolute. The most frequent substitution for people on the Switchboard corpus was mistaking a hesitation in the reference for the word "hmm." The scoring guidelines treat "hmm" as a word distinct from backchannels and hesitations, so this is not a scoring mistake. Examination of the contexts in which the error is made show that it is most often intended to acknowledge the other speaker, i.e. as a backchannel. For both people and our automated system, the insertion and deletion patterns are similar: short function words are by far the most frequent errors. In particular, the single most common error made by the transcribers was to omit the word "I." While we believe further improvement in function and content words is possible, the significance of the remaining backchannel/hesitation confusions is unclear. Table <ref type="table" target="#tab_4">13</ref> shows the overall error rates broken down by substitutions, insertions and deletions. We see that the human transcribers have a somewhat lower substitution rate, and a higher deletion rate. The relatively higher deletion rate might reflect a human bias to avoid outputting uncertain information, or the productivity demands on a professional transcriber. In all cases, the number of insertions is relatively small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">RELATION TO PRIOR WORK</head><p>Compared to earlier applications of CNNs to speech recognition <ref type="bibr" target="#b70">[67,</ref><ref type="bibr" target="#b71">68]</ref>, our networks are much deeper, and use linear bypass connections across convolutional layers. They are similar in spirit to those studied more recently by <ref type="bibr" target="#b34">[31,</ref><ref type="bibr" target="#b33">30,</ref><ref type="bibr" target="#b54">51,</ref><ref type="bibr" target="#b35">32,</ref><ref type="bibr" target="#b36">33]</ref>. We improve on these architectures with the LACE model <ref type="bibr" target="#b49">[46]</ref>, which iteratively expands the effective window size, layer-by-layer, and adds an attention mask to differentially weight distant context. Our spatial regularization technique is similar in spirit to stimulated deep neural networks <ref type="bibr" target="#b72">[69]</ref>. Whereas stimulated networks use a supervision signal to encourage locality of activations in the model, our technique is automatic. Our use of lattice-free MMI is distinctive, and extends previous work <ref type="bibr" target="#b15">[12,</ref><ref type="bibr" target="#b57">54]</ref> by proposing the use of a mixed triphone/phoneme history in the language model. On the language modeling side, we achieve a performance boost by combining multiple LSTM-LMs in both forward and backward directions, and by using a two-phase training regimen to get best results from out-of-domain data. For our best CNN system, LSTM-LM rescoring yields a relative word error reduction of 23%, and a 20% relative gain for the combined recognition system, considerably larger than previously reported for conversational speech recognition <ref type="bibr" target="#b40">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">CONCLUSIONS</head><p>We have measured the human error rate on NIST's 2000 conversational telephone speech recognition task. We find that there is a great deal of variability between the Switchboard and CallHome subsets, with 5.8% and 11.0% error rates respectively. For the first time, we report automatic recognition performance on par with human performance on this task. Our system's performance can be attributed to the systematic use of LSTMs for both acoustic and language modeling, as well as CNNs in the acoustic model, and extensive combination of complementary system for both acoustic and language modeling.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,62.79,72.00,227.05,247.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,323.58,99.18,227.05,258.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison of CNN architectures</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Accuracy</figDesc><table><row><cell></cell><cell></cell><cell cols="2">WER (%)</cell><cell></cell></row><row><cell>Model</cell><cell cols="4">9000 senones 27000 senones</cell></row><row><cell></cell><cell>CH</cell><cell>SWB</cell><cell>CH</cell><cell>SWB</cell></row><row><cell>Baseline</cell><cell>21.4</cell><cell>9.9</cell><cell>20.5</cell><cell>10.6</cell></row><row><cell cols="2">Spatial smoothing 19.2</cell><cell>9.3</cell><cell>19.5</cell><cell>9.2</cell></row></table><note><p>improvements from spatial smoothing on the NIST 2000 CTS test set. The model is a six layer BLSTM, using i-vectors and 40 dimensional filterbank features, and a dimension of 512 in each direction of each layer. model. First, each vector of activations is re-interpreted as a 2-dimensional image. For example, if there are 512 neurons, they are interpreted as the pixels of a 16 by 32 image.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>3. Estimate an unsmoothed, variable-length N-gram lan-guage model from this data, where the history state consists of the previous phone and previous senones within the current phone.To illustrate this, consider the sample senone sequence {s s2.1288, s s3.1061, s s4.1096}, {eh s2.527, eh s3.128, eh s4.66}, {t s2.729, t s3.572, t s4.748}. When predicting the state following eh s4.66 the history consists of (s, eh s2.527, eh s3.128, eh s4.66), and following t s2.729, the history is (eh, t s2.729). We construct the denominator graph from this language model, and HMM transition probabilities as determined by transition-counting in the senone sequences found in the training data. Our approach not only largely reduces the complexity of building up the language model but also provides very reliable training performance. We have found it convenient to do the full computation, with-</figDesc><table /><note><p><p><p>out pruning, in a series of matrix-vector operations on the GPU. The underlying acceptor is represented with a sparse matrix, and we maintain a dense likelihood vector for each time frame. The alpha and beta recursions are implemented with CUSPARSE level-2 routines: sparse-matrix, dense vector multiplies. Run time is about 100 times faster than real time. As in</p><ref type="bibr" target="#b57">[54]</ref></p>, we use cross-entropy regularization. In all the lattice-free MMI (LFMMI) experiments mentioned below we use a trigram language model. Most of the gain is usually obtained after processing 24 to 48 hours of data.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Performance improvements from i-vector and LFMMI training on the NIST 2000 CTS test set</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">WER (%)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Configuration</cell><cell cols="4">ReLU-DNN ResNet-CNN</cell><cell cols="2">BLSTM</cell><cell cols="2">LACE</cell></row><row><cell></cell><cell>CH</cell><cell>SWB</cell><cell>CH</cell><cell>SWB</cell><cell>CH</cell><cell>SWB</cell><cell>CH</cell><cell>SWB</cell></row><row><cell>Baseline</cell><cell>21.9</cell><cell>13.4</cell><cell>17.5</cell><cell>11.1</cell><cell>17.3</cell><cell>10.3</cell><cell>16.9</cell><cell>10.4</cell></row><row><cell>i-vector</cell><cell>20.1</cell><cell>11.5</cell><cell>16.6</cell><cell>10.0</cell><cell>17.6</cell><cell>9.9</cell><cell>16.4</cell><cell>9.3</cell></row><row><cell cols="2">i-vector+LFMMI 17.9</cell><cell>10.2</cell><cell>15.2</cell><cell>8.6</cell><cell>16.3</cell><cell>8.9</cell><cell>15.2</cell><cell>8.5</cell></row><row><cell cols="4">reverse temporal order. The log probabilities from both</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>models are added.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">2. As is customary, the RNN-LM probability estimates are</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">interpolated at the word-level with corresponding N-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">gram LM probabilities (separately for the forward and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">backward models). In addition, we trained a second</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">RNN-LM for each direction, obtained by starting with</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">different random initial weights. The two RNN-LMs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">and the N-gram LM for each direction are interpolated</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>with weights of (0.375, 0.375, 0.25).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>3. In order to make use of LM training data that is not fully matched to the target conversational speech domain, we start RNN-LM training with the union of in-domain (here, CTS) and out-of-domain (e.g., Web) data. Upon convergence, the network undergoes a second training phase using the in-domain data only. Both training phases use in-domain validation data to regulate the learning rate schedule and termination. Because the size of the out-of-domain data is a multiple of the indomain data, a standard training on a simple union of the data would not yield a well-matched model, and have poor perplexity in the target domain.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>LSTM perplexities (PPL) as a function of hidden layers, trained on in-domain data only, computed on 1997 CTS eval transcripts.</figDesc><table><row><cell>Language model</cell><cell>PPL</cell></row><row><cell cols="2">letter trigram input with one layer (baseline) 63.2</cell></row><row><cell>+ two hidden layers</cell><cell>61.8</cell></row><row><cell>+ three hidden layers</cell><cell>59.1</cell></row><row><cell>+ four hidden layers</cell><cell>59.6</cell></row><row><cell>+ five hidden layers</cell><cell>60.2</cell></row><row><cell>+ six hidden layers</cell><cell>63.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Perplexities (PPL) of the four LSTM LMs used in the final combination. PPL is computed on 1997 CTS eval transcripts. All the LSTM LMs are with three hidden layers.</figDesc><table><row><cell>Language model</cell><cell>PPL</cell></row><row><cell>RNN: 2 layers + word input (baseline)</cell><cell>59.8</cell></row><row><cell>LSTM: word input in forward direction</cell><cell>54.4</cell></row><row><cell>LSTM: word input in backward direction</cell><cell>53.4</cell></row><row><cell>LSTM: letter trigram input in forward direction</cell><cell>52.1</cell></row><row><cell cols="2">LSTM: letter trigram input in backward direction 52.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Performance of various versions of neural-net-based LM rescoring. Perplexities (PPL) are computed on 1997 CTS eval transcripts; word error rates (WER) on the NIST 2000 Switchboard test set.</figDesc><table><row><cell>Language model</cell><cell cols="2">PPL WER</cell></row><row><cell>4-gram LM (baseline)</cell><cell>69.4</cell><cell>8.6</cell></row><row><cell>+ RNNLM, CTS data only</cell><cell>62.6</cell><cell>7.6</cell></row><row><cell>+ Web data training</cell><cell>60.9</cell><cell>7.4</cell></row><row><cell>+ 2nd hidden layer</cell><cell>59.0</cell><cell>7.4</cell></row><row><cell cols="2">+ 2-RNNLM interpolation 57.2</cell><cell>7.3</cell></row><row><cell>+ backward RNNLMs</cell><cell>-</cell><cell>6.9</cell></row><row><cell>+ LSTM-LM, CTS + Web data</cell><cell>51.4</cell><cell>6.9</cell></row><row><cell>+ 2-LSTM-LM interpolation</cell><cell>50.5</cell><cell>6.8</cell></row><row><cell>+ backward LSTM-LM</cell><cell>-</cell><cell>6.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Runtimes as factor of speech duration for various aspects of acoustic modeling and decoding, for different types of</figDesc><table><row><cell>acoustic model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Processing step Hardware DNN ResNet-CNN BLSTM LACE</cell></row><row><cell>AM training</cell><cell>GPU</cell><cell>0.012</cell><cell>0.60</cell><cell>0.022</cell><cell>0.23</cell></row><row><cell>AM evaluation</cell><cell>GPU</cell><cell>0.0064</cell><cell>0.15</cell><cell>0.0081</cell><cell>0.081</cell></row><row><cell>AM evaluation</cell><cell>CPU</cell><cell>0.052</cell><cell>11.7</cell><cell>n/a</cell><cell>8.47</cell></row><row><cell>Decoding</cell><cell>GPU</cell><cell>1.04</cell><cell>1.19</cell><cell>1.40</cell><cell>1.38</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>Word error rates (%) on the NIST 2000 CTS test set with different acoustic models. Unless otherwise noted, models are trained on the full 2000 hours of data and have 9k senones.</figDesc><table><row><cell>Model</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 .</head><label>9</label><figDesc>Comparative error rates from the literature and human error as measured in this work</figDesc><table><row><cell>Model</cell><cell cols="3">N-gram LM Neural net LM CH SWB CH SWB</cell></row><row><cell cols="2">Povey et al. [54] LSTM 15.3 8.5</cell><cell>-</cell><cell>-</cell></row><row><cell>Saon et al. [51] LSTM</cell><cell>15.1 9.0</cell><cell>-</cell><cell>-</cell></row><row><cell>Saon et al. [51] system</cell><cell>13.7 7.6</cell><cell cols="2">12.2 6.6</cell></row><row><cell>2016 Microsoft system</cell><cell>13.3 7.4</cell><cell cols="2">11.0 5.8</cell></row><row><cell>Human transcription</cell><cell></cell><cell cols="2">11.3 5.9</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>We used two different dictionaries, one based on a standard phone set and another with dedicated vowel and nasal phones used only in the pronunciations of filled pauses ("uh", "um") and backchannel acknowledgments ("uh-huh", "mhm"), similar to<ref type="bibr" target="#b66">[63]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Using the sox tool options pad 0.15 0.15 dither -p 14.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>When comparing the last row in Table3with the "N-gram LM" results in Table8, note that the former results were obtained with the pruned N-gram LM used in the decoder and fixed score weights (during lattice generation), whereas the latter results are from rescoring with the unpruned N-gram LM (during N-best generation), using optimized score weighting. Accordingly, the rescoring results are generally somewhat better.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>The NIST scoring protocol treats hesitation words as optional, and we therefore delete them from our recognizer output prior to scoring. This explains why we see many substitutions of backchannels for hesitations, but not vice-versa.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Arul Menezes for access to the Microsoft transcription pipeline; Chris Basoglu, Amit Agarwal and Marko Radmilac for their invaluable assistance with CNTK; Jinyu Li and Partha Parthasarathy for many helpful conversations. We also thank X. Chen from Cambridge University for valuable assistance with the CUED-RNNLM toolkit, and the International Computer Science Institute for compute and data resources.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Most common substitutions for ASR system and humans. The number of times each error occurs is followed by the word in the reference, and what appears in the hypothesis instead</title>
	</analytic>
	<monogr>
		<title level="m">Table 10</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
		</imprint>
	</monogr>
	<note>%hesitation) / %bcack 12: a / the 29: (%hesitation) / %bcack 12: (%hesitation) / hmm 12: was / is 10: (%hesitation) / a 9: (%hesitation) / oh 10: (%hesitation) / oh</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">%hesitation</orgName>
		</author>
		<imprint/>
	</monogr>
	<note>a 10: was / is 9: was / is 9: was / is</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">%hesitation</orgName>
		</author>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note>oh 7: (%hesitation) / hmm 8: and / in 8: (%hesitation) / a 8: a / the 7: bentsy / bensi 6: (%hesitation) / i 5: in / and 7: and / in 7: is / was 6: in / and 4: (%hesitation) / %bcack 7: it / that 6: could / can 5: (%hesitation. well / oh 5: (%hesitation) / yeah 4: is / was</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Most common deletions for ASR system and humans</title>
		<idno>33: it 59: and 26: i 30: and 29</idno>
	</analytic>
	<monogr>
		<title level="j">CH SWB ASR Human ASR Human</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
		</imprint>
	</monogr>
	<note>oh 5: (%hesitation) / oh 5: jeez / jeeze 4: the / a Table 11. it 19: a 29: it 29: and 47: is 17: that 22: a 25: is 45: the 15: you 22: that 19: he 41: %bcack 13: and 22: you 18: are 37: a 12: have 17: the 17: oh 33: you 12: oh 17: to 17: that 31: oh 11: are 15: oh 17: the 30: that 11: is 15: yeah 12. REFERENCES</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Table 12. Most common insertions for ASR system and humans</title>
		<author>
			<persName><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Hoane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Blue</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="57" to="83" />
		</imprint>
	</monogr>
	<note>Artificial intelligence. and 11: i 8: a 7: of 9: you 11: the 8: that 6: do 8: is 11: you 8: the 6: is 6: they 9: it 7: have 5: but 5: do 7: oh 5: you 5: yeah 5: have 6: and 4: are 4: air 5: it 6: in 4: is 4: in 5: yeah 6: know 4: they 4: you 4: a Table 13. Overall substitution, deletion and insertion rates</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02595</idno>
		<title level="m">Deep Speech 2: End-toend speech recognition in English and Mandarin</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Super-human multi-talker speech recognition: the IBM 2006 Speech Separation Challenge system</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Kristjansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Olsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Gopinath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">155</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Singlechannel mixed speech recognition using deep neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="5632" to="5636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A look at NIST&apos;s benchmark ASR tests: past, present, and future</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Pallett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Automatic Speech Recognition and Understanding Workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="483" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The DARPA 1000-word resource management database for continuous speech recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Pallett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="651" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The design for the wall street journal-based csr corpus</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on Speech and Natural Language</title>
		<meeting>the workshop on Speech and Natural Language</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="357" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The 1996 broadcast news speech and language-model corpus</title>
		<author>
			<persName><forename type="first">D</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Macintyre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the DARPA Workshop on Spoken Language technology</title>
		<meeting>the DARPA Workshop on Spoken Language technology</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="11" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Switchboard: Telephone speech corpus for research and development</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Holliman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="517" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Fisher corpus: a resource for the next generations of speech-to-text</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LREC</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="69" to="71" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Advances in speech transcription at IBM under the DARPA EARS program</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mangu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1596" to="1608" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Advances in transcription of broadcast news and conversational telephone speech within the combined ears bbn/limsi system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Matsoukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Gauvain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Adda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Colthurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kimball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lefevre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1541" to="1556" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recent innovations in speech-to-text transcription at SRI-ICSI-UW</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Graciarena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kirchhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1729" to="1744" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Ljolje</surname></persName>
		</author>
		<title level="m">The AT&amp;T 2001 LVCSR system&quot;, NIST LVCSR Workshop</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Conversational telephone speech recognition</title>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Gauvain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Adda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lefevre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">212</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Development of the 2003 cu-htk conversational telephone speech transcription system</title>
		<author>
			<persName><forename type="first">G</forename><surname>Evermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J F</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mrva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Woodland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">249</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Conversational speech transcription using context-dependent deep neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="437" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Speech recognition by machines and humans</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Lippmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The Microsoft 2016 conversational speech recognition system</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1609.03528" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>submitted to ICASSP</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generalization of back-propagation to recurrent neural networks</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Pineda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">2229</biblScope>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="270" to="280" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Phoneme recognition using time-delay neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hanazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shikano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="328" to="339" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convolutional networks for images, speech, and time series</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The handbook of brain theory and neural networks</title>
		<imprint>
			<date type="published" when="1995">1995. 1995</date>
			<biblScope unit="volume">3361</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A recurrent error propagation network speech recognition system</title>
		<author>
			<persName><forename type="first">T</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fallside</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="259" to="274" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Long shortterm memory recurrent neural network architectures for large scale acoustic modeling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="338" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast and accurate recurrent neural network acoustic models for speech recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1468" to="1472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The IBM 2015 English conversational telephone speech recognition system</title>
		<author>
			<persName><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-K</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
		<editor>Interspeech</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3140" to="3144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Very deep multilingual convolutional neural networks for LVCSR</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<biblScope unit="page" from="4955" to="4959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Very deep convolutional neural networks for LVCSR</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3259" to="3263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Very deep convolutional neural networks for noise robust speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2263" to="2276" />
			<date type="published" when="2016-08">Aug. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cernockỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Context dependent recurrent neural network language model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="901" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Lstm neural networks for language modeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<editor>Interspeech</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="194" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improving English conversational telephone speech recognition</title>
		<author>
			<persName><forename type="first">I</forename><surname>Medennikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Prudnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zatvornitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>-T. Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deep speech: Scaling up end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5567</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Glenn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zakhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Highway networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>abs/1505.00387</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Linearly augmented deep neural network</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5085" to="5089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks with layer-wise context expansion and attention</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Consonant recognition by modular construction of large phonemic time-delay neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sawai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shikano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="112" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional LSTM and other neural network architectures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Front-end factor analysis for speaker verification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="788" to="798" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Speaker adaptation of neural network acoustic models using i-vectors</title>
		<author>
			<persName><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nahamoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Speech Recognition and Understanding Workshop</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="55" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The IBM 2016 English conversational telephone speech recognition system</title>
		<author>
			<persName><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016-09">Sep. 2016</date>
			<biblScope unit="page" from="7" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Sequential classification criteria for NNs in automatic speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="441" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Sequence-discriminative training of deep neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Veselỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2345" to="2349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Purely sequence-trained neural networks for ASR based on lattice-free MMI</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Galvez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ghahrmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2751" to="2755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Parallelizing WFST speech decoders</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mendis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maleki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Musuvathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mytkowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5325" to="5329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">SRILM-an extensible language modeling toolkit</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page">2002</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Efficient estimation of maximum entropy language models with N-gram features: An SRILM extension</title>
		<author>
			<persName><forename type="first">T</forename><surname>Alumäe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kurimo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1820" to="1823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">CUED-RNNLM: An open-source toolkit for efficient training and evaluation of recurrent neural network language models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J F</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Woodland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="6000" to="6004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management</title>
		<meeting>the 22nd ACM international conference on Conference on information &amp; knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Using the output embedding to improve language models</title>
		<author>
			<persName><forename type="first">O</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05859</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Self-stabilized deep neural network</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The SRI March 2000 Hub-5 conversational speech transcription system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings NIST Speech Transcription Workshop</title>
		<meeting>NIST Speech Transcription Workshop<address><addrLine>College Park, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-05">May 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<ptr target="https://cntk.ai" />
		<title level="m">The Microsoft Cognition Toolkit (CNTK)</title>
		<imprint>
			<publisher>Microsoft Research</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">An introduction to computational networks and the Computational Network Toolkit</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<idno>MSR-TR-2014-112</idno>
		<ptr target="https://github.com/Microsoft/CNTK" />
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Microsoft Research</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">1-bit stochastic gradient descent and its application to dataparallel distributed training of speech DNNs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1058" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for LVCSR</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<biblScope unit="page" from="8614" to="8618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition</title>
		<author>
			<persName><forename type="first">O</forename><surname>Abdel-Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Penn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="4277" to="4280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Stimulated deep neural network for speech recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Karanasou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="400" to="404" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
