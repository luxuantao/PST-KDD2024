<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Manuscript received March 31, 2000; revised August 22, 2000. This work was supported by the Hong Kong Special Administrative Region, China (Project 9040347 CityU 1044/98E)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">D</forename><forename type="middle">W C</forename><surname>Ho</surname></persName>
						</author>
						<author>
							<persName><forename type="first">J</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">City Uni-versity of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Mathematics</orgName>
								<orgName type="department" key="dep2">Institute of Systems Engineering</orgName>
								<orgName type="institution">City University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong, Xi&apos;an</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Jiaotong University</orgName>
								<address>
									<postCode>710049</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Publisher Item Identifier</orgName>
								<address>
									<postCode>1063-6706(01, 01356-X</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Manuscript received March 31, 2000; revised August 22, 2000. This work was supported by the Hong Kong Special Administrative Region, China (Project 9040347 CityU 1044/98E)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">652C30349EE10755927A8852B1D6BCF9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>I N RECENT years, wavelets have become a very active sub- ject in many scientific and engineering research areas. Especially, wavelet neural networks (WNN) inspired by both the feedforward neural networks and wavelet decompositions have received considerable attention <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref> and become a popular tool for function approximation.</p><p>The main characteristic of WNN is that some kinds of wavelet functions are used as the nonlinear transformation function in the hidden layer, instead of the usual sigmoid function. Incorporating the time-frequency localization properties of wavelets and the learning abilities of general neural network (NN), WNN has shown its advantages over the regular methods such as NN for complex nonlinear system modeling.</p><p>At present, there are two different kinds of WNN structure, one with fixed wavelet bases, where the dilation and translation parameters of wavelet basis are fixed, and only the output layer weights are adjustable. Another type is the variable wavelet bases, where the dilation parameters, translation parameters and the output layer weights are adjustable.</p><p>For the WNN with fixed wavelets, the main problem is the selection of wavelet bases/frames. The wavelet basis have to be selected appropriately since the choice of the wavelet basis can be critical to approximation performance. Recently, a node-configuration strategy to add new nodes gradually according to some simple thresholding rules are presented in <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b6">[7]</ref>. It is known that by using regularly truncated wavelet frames, the number of wavelet candidates would drastically increase with the dimension. Therefore, constructing and storing wavelet bases/frames for large dimension problems are of prohibitive cost. In <ref type="bibr" target="#b12">[13]</ref>, a genetic algorithm (GA) combined with a steepest descent technique and least squares technique for optimal selection of the basis of the wavelet based networks have been used. However, the greedy nature of GA restricted the development of the method. In <ref type="bibr" target="#b0">[1]</ref>, an algorithm for wavelet networks has been constructed by using radial wavelet frames with natural single-scaling characteristic. In his work, attentions are paid to sparse training data so that problems of large dimension could be better handled. Obviously, to improve the approximation accuracy, large number of wavelet neurons are required for WNN with fixed wavelet bases. This will result in a large complex network structure and cause overfitting problems.</p><p>For WNN with variable wavelet basis, a new approach has first been presented by initializing the WNN as truncated wavelet frames, then follows by training with a backpropagation algorithm. Also in <ref type="bibr" target="#b14">[15]</ref>, the traditional AutoRegressive eXternal input model (ARX) is incorporated with WNN introduced by <ref type="bibr" target="#b1">[2]</ref> so that the linear and nonlinear components of the system can be modeled by ARX and WNN, respectively.</p><p>In fact, since the dilation parameter has explicit physical concept, i.e., resolution, it plays a significant role in wavelet analysis and approximation of a given function. Motivated by this merit, this paper presents a new type of nonparametric model, called a fuzzy wavelet networks (FWN), inspired by both the theory of MRA and the traditional Takagi-Sugeno-Kang (TSK) fuzzy model <ref type="bibr" target="#b16">[17]</ref>. The goal of the introduction of fuzzy model into WNN is to improve function approximation accuracy in terms of the dilation and translation parameters of wavelets, meanwhile not increasing the number of wavelet bases.</p><p>In general, the TSK fuzzy models consist of a set of rules, and each rule acts like a "local model" by using fuzzy set to partition the input space into local fuzzy regions. The consequents of these rules are represented by either a constant or a linear equation which is essentially a global function. In this paper, each fuzzy rule corresponds to a sub-WNN consisting of wavelets with a specified dilation value (i.e., resolution). Thus the sub-WNNs at different resolution levels are used to capture different behaviors (global or local) of the approximated function. Here, the role of the fuzzy set is to determine the contribution of the sub-WNNs to the output of the FWN. As a result, the difficulties of selecting wavelets are reduced, also wavelets with different dilation values under these fuzzy rules are fully utilized to capture various essential components of the system. Compared with WNNs of previous works, the model accuracy and generalization capability of the FWN are improved by learning the translation parameters of the wavelets and adjusting the shape of fuzzy sets. All these advantageous can be reflected in our simulations. This paper is organized as follows. The basic concepts of WNN are introduced in Section II, and the fuzzy wavelet networks are described in Section III. An algorithm for constructing the FWN from any set of input-output data is introduced in Section IV. Four simulation examples are provided to illustrate the performance of the FWN in comparison with other methods in Section V. Finally, a brief conclusion is drawn in Section VI.</p><p>Throughout this paper, we will use the following generic notations: lower case symbols such as refer to scalar valued objects, lower case boldface symbols such as refer to vector valued objects, and finally capital symbols will be used as matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. WAVELET NEURAL NETWORKS</head><p>Let and represent the system inputs and output, respectively. Without loss of generality, in this paper, we consider only multi-input-single-output (MISO) functions .</p><p>For clarity of presentation, we briefly recall the basic concepts about wavelet transforms that are relevant to this paper.</p><p>Given a frame in the Hilbert space , for any , can be decomposed in terms of the frame elements as follows:</p><p>(1) where is the frame operator and is defined by</p><formula xml:id="formula_0">(2)</formula><p>and , is generated by dilating and translating the mother wavelet function in the following form:</p><p>(3) the index and are dilation (or scale) and translation, respectively.</p><p>Notice that, MRA suggests that the dilation parameter of a wavelet can also be interpreted as resolution. Therefore, (1) illustrates that any function in can be regarded as a linear combination of wavelets at different resolutions level. This is our primary motivation for proposing FWN.</p><p>In most practical applications, the functions of interest have finite support, therefore, it is possible for us to truncate infinite number of wavelet frames in (1) to reconstruct function . We can consider the following approximative wavelet-based representation form for the function :</p><p>(</p><formula xml:id="formula_1">)<label>4</label></formula><p>where and is the total number of wavelet function selected. Referring to the previous research works <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>, if is used as nonlinear transformation function of the hidden units and denotes the connection weights from hidden layer to output layer, then we can consider (4) as the functional expression of a three-layered WNN modeling function . In the WNN, ( is the half of the sum of the elements in ) denotes the connection weights from input layer to hidden layer and is the bias of each hidden units. Notice that, when and are taken as integer values, then ( <ref type="formula" target="#formula_1">4</ref>) is correspondent to the WNN with fixed shape of wavelets during learning process (see <ref type="bibr">[2, refs.</ref> [1], <ref type="bibr" target="#b2">[3]</ref>]). That is, once we determine the wavelets represented by index and , then the learning procedure for WNN is based on a fixed set of wavelets. Furthermore, if and are taken as real values, we have more flexible choices of wavelets during the learning process. Then, ( <ref type="formula" target="#formula_1">4</ref>) is equivalent to the WNN with variable wavelets introduced by Zhang <ref type="bibr" target="#b1">[2]</ref>.</p><p>Though functional approximators other than wavelets may have the universal approximation property, in general they do not have the multiresolution property. This property is very useful for approximation problems. The wavelets with coarse resolution can capture the global (low frequency) behavior easily, while the wavelets with fine resolution can capture the local behavior (higher frequency) of the function accurately. This distinguished characteristic leads to the wavelet-based neural networks to be of the advantages of fast convergence, easy training, and high accuracy. where is the th rule ; is the th input variable of ;</p><p>is the total number of wavelets for the th rule. Notice that is the output of the local model for rule , which is equal to the linear combination of a finite set of wavelets with the same dilation value . In this paper, both and are the symbols for translation parameter, and</p><p>, where is the dilation value for corresponding wavelet . Finally, is the fuzzy set characterized by the following Gaussian-type membership function, and is the grade of membership of in . Denote and <ref type="bibr" target="#b5">(6)</ref> where represents the center of membership function, and determine the width and the shape of membership function, respectively. The reason for selecting this Gaussian membership function is that it can approximate triangular and trapezoidal membership functions. The parameters and will be updated in Section IV.</p><p>Based on the previous description in Section II, we obtain:</p><formula xml:id="formula_2">(7)</formula><p>Using the well-known TSK fuzzy inference mechanism <ref type="bibr" target="#b16">[17]</ref> and let the output of the FWN is <ref type="bibr" target="#b7">(8)</ref> where , it is the firing strength of the th rule for current input and satisfies . Also, determines the contribution degree of the output of the wavelet based model with resolution level . From <ref type="bibr" target="#b4">(5)</ref>, it is obvious that the wavelets are single-scaling since the th rule has the same dilation index in all the dimensions. In this FWN structure, our approach is to classify the wavelets into different set of single-scaling wavelets. Hence the difficulties of selecting wavelets are reduced, also wavelets with different dilation values under these fuzzy rules are fully utilized to capture various essential components of the system.</p><p>Based on the idea of traditional fuzzy neural systems and WNN, the FWN described in (5) can be implemented by a multilayer network as shown in Fig. <ref type="figure" target="#fig_2">1</ref>. The FWN consists of four basic layers of traditional fuzzy neural networks (FNN), they are input, fuzzification, inference, and defuzzification layers, respectively. The number of neurons in each layers is , , , and , respectively. It is obvious that once we determine the number of inputs and the number of rules , the structure of the FWN is determined. Notice that in Fig. <ref type="figure" target="#fig_2">1</ref>, the activation function used in fuzzification layer is characterized by the Gaussian membership function defined in <ref type="bibr" target="#b5">(6)</ref>. Also, the weights , in Fig. <ref type="figure" target="#fig_2">1</ref> are defined as and , respectively, and means the membership function . Now we emphasize the difference between our FWN and traditional FNNs. In defuzzification layer, FWN employs sub-WNNs rather than using constants or linear equations as in the traditional FNNs. Unlike the traditional FNNs with only one localized approximation of function, the FWN uses both globalized and localized approximation of function. For this reason, the FWN has the advantages of better local accuracy, nicer generalization capability and faster convergence.</p><p>IV. FWN CONSTRUCTION Given the training data set: <ref type="bibr" target="#b8">(9)</ref> where is the total number of the training patterns, we define the input matrix and the output observation vector as follows: <ref type="bibr" target="#b9">(10)</ref> Here , are the column, row vectors of , respectively. Now our problem is to construct a FWN based on the training data set in <ref type="bibr" target="#b8">(9)</ref> so that the error between the output of FWN and is minimal. Before constructing the FWN, we are facing the following problems: 1) How to determine the number of rules? 2) How to select the wavelets for FWN? 3) How to set the initial weight values for the FWN? 4) What algorithm should we use to adjust these weights so as to minimize the error between the output of FWN and ? In this section, three systematic procedures are carried out to address these details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Selecting Wavelets and Fuzzy Rules</head><p>For our FWN, the interested range of the dilation parameter can be determined from various type of wavelet functions according to the training data in <ref type="bibr" target="#b8">(9)</ref>. In our examples, we know by experience that the dilation values in (3) for "Mexican Hat" wavelet function typically ranging from to 3 is appropriate for constructing FWN. Therefore, the range of dilation parameter is first determined for FWN according to training data.</p><p>After determining the dilation parameter, a systematic method to choose the wavelets will be proposed. The algorithm consists of two steps: selecting wavelet candidates and purifying wavelets.</p><p>Step 1-Selecting Wavelet Candidates: In this paper, in order to make the selection easier over an infinite wavelet frame, we assume that the wavelet supports are approximative hypercubes in the input space as that in Zhang <ref type="bibr" target="#b0">[1]</ref>, then we can determine the wavelet candidates easily by examining the dimension of independently.</p><p>As stated in Section II, the function to be approximated has finite supports, that is to say, for a specified dilation parameter , there are finite number of translation index for each input dimension. Obviously, the basic condition for wavelet candidates has to satisfy their supports covering the interested range of the approximated function. Therefore, the wavelet candidates are chosen according to the values for the training data . Let be the set of index of the translation parameters of the wavelets with the current dilation whose supports contain , then we have <ref type="bibr" target="#b10">(11)</ref> Notice that the number of elements in may be large, especially for large dilation value, and the size of can be adjusted by the chosen . From (11), we know that the supports of the wavelet contain at least one of the training data set . After obtaining the required translation set for each input variable, we can determine the required candidates of wavelet by searching through all the possible combinations of the terms as follows: <ref type="bibr" target="#b11">(12)</ref> where is also a chosen positive value, which controls the total number of wavelet candidate, and , are indexes of dilation parameters and input variables, respectively. This measure in ( <ref type="formula">12</ref>) reflects the characteristic of tensor product in (2), the higher values of the product indicates the wavelets contains important values of the input data in <ref type="bibr" target="#b9">(10)</ref>. Notice that, to reduce the difficulty of purifying wavelets in the next step, we usually take the size of to be 400-500 wavelet candidates.</p><p>In order to avoid heavy computation requirement in large dimension problems, should be chosen large enough such that reasonable number of elements in can be obtained. Also can be normally chosen as 1 for some unimportant components of input variables.</p><p>Step 2-Purifying the Wavelets by Orthogonal Least-Squares (OLS) Algorithm: In many cases, the training data may be distributed densely on some wavelet candidates and sparsely on the others. Hence, some wavelets in , which sets up according to input training data, are often redundant for constructing FWN. In other words, these redundant wavelets do not make a significant contribution to the output of the FWN. In general, using more wavelets than required often results in overfitting of the data and leads to poor generalization.</p><p>There are several ways for purifying wavelets, such as stepwise selection by orthogonalization <ref type="bibr" target="#b0">[1]</ref>, the backward elimination <ref type="bibr" target="#b0">[1]</ref> and the forward selection using matching pursuits <ref type="bibr" target="#b13">[14]</ref>. Here, we use the OLS algorithm to purify the wavelets from their candidates and to determine in ( <ref type="formula">5</ref>), which will be used to set the initial weights of the FWN. The main reason for this choice is that a flexible measure, called error reduction ratio <ref type="bibr" target="#b18">[19]</ref>, can be used to select the important wavelets, which give significant contribution to the output of the FWN. Since the OLS is a well-known algorithm, for the sake of simplicity, we omit the description of the algorithm. Details of this OLS algorithm can be found in <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b14">[15]</ref>.</p><p>Here, it is assumed that there are selected wavelets at the end of the OLS algorithm. Then, the final wavelet-based representation of the approximated function is</p><formula xml:id="formula_3">(13) (14)</formula><p>where is the number of selected wavelet at dilation , and . The architecture of the FWN in Section III depends on the number of input variables and the number of rules . Here, we assume that the input variables are known in advance. Then from the structure of (5), without loss of generality, it is convenient to let the number of rules be equal to the number of different dilation values.</p><p>Based on the selected wavelets, we can divide these wavelets into groups according to their dilation values. Each group corresponds to a rule of FWN. It is worth noticing that under this selection scheme, a systematic method is set up such that we can adjust the translation of the selected wavelets to improve the accuracy as well as possible.</p><p>After the first two steps, we can move to the next step to construct the FWN depicted in Fig. <ref type="figure" target="#fig_2">1</ref> directly as discussed in the Section IV-B. First, we need to initialize the unknown parameters and then follow by setting up the learning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Initialization</head><p>From ( <ref type="formula">6</ref>) and ( <ref type="formula">13</ref>)-( <ref type="formula">14</ref>), we can see that the free parameters to be trained in FWN will be , and where , , , and , respectively. Here is obtained in selecting wavelets phase, also initial can be computed easily via least squares technique from ( <ref type="formula">14</ref>) for given set of observation vector . However, all are unknown and we have to initialize them as follows.</p><p>As we know, the parameters and are free parameters of the membership functions, and these membership functions determine the contribution degree of each sub-WNN with a special resolution. We can get the approximate contribution degree through computing in ( <ref type="formula">14</ref>), and we let (15) Then, we can set the initial parameters and according to the following standard formulas:</p><p>(16) ( <ref type="formula">17</ref>)</p><formula xml:id="formula_4">or (<label>18</label></formula><formula xml:id="formula_5">)</formula><p>The initial , are chosen to be the weighted mean and standard derivation of the input data, respectively; and is chosen to shape the Gaussian-type membership function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Learning Algorithm for FWN</head><p>After setting up the network initialization, we shall discuss the learning algorithm to be used in FWN. Zhang <ref type="bibr" target="#b1">[2]</ref> used the gradient method to adjust all translation parameters, dilation parameters, and weight coefficients of the wavelet networks simultaneously. Learning procedure may be trapped in local minimum error and cause slow training. We use a two-stage efficient learning scheme, which undertakes the extended Kalman filter (EKF) algorithm for the nonlinear parametric parameters and , and then uses the least-squares (LS) estimation to update all the weights , where , and . The idea of EKF is to apply to nonlinear dynamic systems by linearizing the system around the current estimate of the above paramters. Detail description of EKF can be found in <ref type="bibr" target="#b19">[20]</ref> for training a multilayer perceptron.</p><p>Let , be the sample data set of the identified system.</p><p>The EKF algorithm is described as follows:</p><p>(</p><formula xml:id="formula_6">) (20)<label>19</label></formula><p>where is a vector defined as follows:</p><formula xml:id="formula_8">(22)</formula><p>is the error covariance matrix and set , here is a positive integer and is a identity matrix.</p><p>is the Hessian matrix containing the partial derivative of the model output with respect to each individual in , i.e., <ref type="bibr">(23)</ref> where represents the output of the model for the current . For a single output system, we define , where is a large positive integer(usually we take in our simulations).</p><p>Explicit formulas for the partial derivatives of the output of the FWN with respect to each component of the parameter vector are now listed as follows:</p><formula xml:id="formula_9">(24) (25) (26) (27)</formula><p>Obviously, the partial derivative of depends on the chosen wavelet function. Here, we take "Mexican Hat" wavelet function as an example to give the following result:</p><p>(28) where is the index of fuzzy rules, the index of input variables and the index of wavelets. After tuning the parameter and , we can use any LS algorithm to estimate the required parameters in <ref type="bibr" target="#b13">(14)</ref>.</p><p>The learning procedure on using the EKF algorithm and adjusting of weighting will be repeated according to a decay index which is defined as</p><formula xml:id="formula_10">(29)</formula><p>where is the performance index at th iteration. This decay index is evaluated with the aim to know the decay rate of the cost function. If such a rate is greater than a given threshold (the threshold is usually chosen as in our examples), a new learning epoch starts; otherwise, we can stop the learning stages. Note that various types of are chosen in our simulation for comparison purpose with previous work. We summarize the whole algorithm for constructing FWN as illustrated in Fig. <ref type="figure" target="#fig_3">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. SIMULATIONS</head><p>In this section, four examples are given to demonstrate the validity of the presented FWN. In all cases, we have selected the Mexican Hat function as our wavelet function.</p><p>For the purpose of comparing our FWN with other works, we take the measure in <ref type="bibr" target="#b1">[2]</ref> with (30)</p><p>as the performance index for the first two examples and final example. In (30), is the desired output and is the estimated output from FWN.</p><p>For the third example, we use a different measure to compare the results in <ref type="bibr" target="#b17">[18]</ref>. The measure is defined as the root-meansquared (rms) error divided by the standard deviation of the desired output, which is also known as nondimensional error index (NDEI).  Example A-Approximation of Piecewise Function: A piecewise function studied by Zhang <ref type="bibr" target="#b1">[2]</ref> and Chen <ref type="bibr" target="#b14">[15]</ref> is used to compare our FWN with other wavelet-based networks, which is defined as (31)</p><p>The piecewise function is continuous and analyzable. However, traditional analytical tools become inefficient and often fail due to two reasons, namely, the wide-band information hidden at the turning points and the coexistence of linearity and nonlinearity.</p><p>In this example, we sampled 200 points distributed uniformly over <ref type="bibr">[ , 10]</ref> as training data. In Table <ref type="table" target="#tab_0">I</ref>, we compare the performance of our FWN with other WNNs, here the number of rules used in FWN is 6. It should be noticed that the performance of FWN shown in the Table <ref type="table" target="#tab_0">I</ref> is the average of the nine simulations. Each simulation is computed on uniformly sampled test data of 200 points. Among these results, the maximal value is 0.0246 and the minimal value 0.018. Obviously, the performance of our FWN is superior to that of other WNNs in Zhang <ref type="bibr" target="#b1">[2]</ref> and Chen <ref type="bibr" target="#b14">[15]</ref>. Fig. <ref type="figure" target="#fig_4">3</ref> illustrates the validity of our wavelet basis selection algorithm and the initialization technique for FWN. Fig. <ref type="figure" target="#fig_5">4</ref> shows the excellent performance of our FWN, which corresponds to the worst performance value among the nine simulations.</p><p>Example B-Approximation of Two Variable Function:</p><p>(32)</p><p>A data set of 400 input-output pairs was collected by using random inputs being distributed uniformly over the domain . Table <ref type="table" target="#tab_1">II</ref> shows the performance comparison of our FWN with WNN, and there are five rules used in FWN. As can be seen in Table <ref type="table" target="#tab_1">II</ref>, although the number of unknown parameters used for training in FWN is remarkably reduced, the performance index is decreased significantly too. Figs. <ref type="figure" target="#fig_6">5</ref> and<ref type="figure" target="#fig_7">6</ref> show the original form of over and its approximation realized by FWN, respectively. Fig. <ref type="figure" target="#fig_7">6</ref> shows that our FWN has good generalization capability.   Example C-Predicting Chaotic Time Series: The considered chaotic series in our simulation is the chaotic Mackey-Glass differential delay equation defined as (33)</p><p>The prediction of future values of this time series is a benchmark problem. From the Mackey-Glass time series , Jang [17] extracted 1000 input-output data pairs of the following format (where and ):</p><p>(34) where to 1117. Similar to Jang <ref type="bibr" target="#b16">[17]</ref>, we use the first 500 pairs as the training data set, while the remaining 500 pairs were the checking data set for validating the learned FWN. Note  that there are 6 rules in the FWN and Table <ref type="table" target="#tab_2">III</ref> shows the performance comparison of our FWN with other models. Fig. <ref type="figure" target="#fig_8">7</ref> shows that the desired and predicted values for both training data and checking data are the same; the difference between them can only be seen on a much finer scale, as shown in Fig. <ref type="figure" target="#fig_10">8</ref>. Fig. <ref type="figure" target="#fig_11">9</ref> displays all final membership functions of the resulting FWN. Each column corresponds to the membership functions of each input for different rules. Each row displays all membership functions of a rule corresponding to different .</p><p>Example D-Nonlinear Dynamic Modeling: Finally, let us consider the FWN approach to the approximation problem for a high-dimensional system. This example is taken from <ref type="bibr" target="#b17">[18]</ref>, and the nonlinear dynamical system to be identified is defined as follows:</p><p>(35)</p><p>We collected data on-line for 200 time steps with the following data form to construct the FWN:   are applied, respectively. It can be seen that the FWN can match the real system very well. The performance indexes of (30) for the two signals are 0.0406 and 0.0449, respectively. Figs. <ref type="figure" target="#fig_12">10</ref> and<ref type="figure"></ref> 11 also indicate that the performance of the presented FWN is superior to that of the fuzzy model by observing from the figures in Nie <ref type="bibr" target="#b17">[18]</ref>, though no performance indexes were given in <ref type="bibr" target="#b17">[18]</ref> for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, a new type of network, called FWN has been introduced for function approximation from input-output observations. The presented FWN integrates fuzzy concepts with the WNNs such that the contribution degree of different sub-WNNs at different resolution levels can be controlled flexibly. Compared with other algorithms for selecting wavelets,  It is worth noticing that we only consider using wavelet frames to construct our FWN in this paper. In fact, we can also use orthonormal wavelet such as Lemarie-Meyer and the Daubechies wavelets to construct the proposed FWN. The orthonormal wavelet-based networks can provide a unique and efficient representation for the given function, but this also means that the approximation accuracy of this kind of WNN highly relies on the selected wavelets. In this case, any elimination of wavelets will result in a poor generalization capability of the obtained FWN. Moreover, some orthonormal wavelets such as Daubechies require interpolation at each sampling instant. This will cause heavy computation for real-time implementation.</p><p>It is worth mentioning that the simulation study here are noise-free data, also the generalization ability of the proposed approach under some additive random noise has not been discussed here. In particular, the accurate identification may tend to have a great influence from the noises, the adaptive adjustment of error matrix in (21) of EKF is an crucial factor for the accuracy. Further study on extending of the proposed algorithm on FWN under additive random noise is an interesting area under investigation.</p><p>In summary, the presented FWN not only reserves the multiresolution capability of WNN, but also has the advantages of high approximation accuracy and good generalization performance. It is believed that FWN can be applied to the problems of function approximation, system identification, signal processing and control.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fuzzy</head><label></label><figDesc>Wavelet Networks for Function Learning Daniel W. C. Ho, Ping-An Zhang, and Jinhua Xu Abstract-Inspired by the theory of multiresolution analysis (MRA) of wavelet transforms and fuzzy concepts, a fuzzy wavelet network (FWN) is proposed for approximating arbitrary nonlinear functions in this paper. The FWN consists of a set of fuzzy rules. Each rule corresponding to a sub-wavelet neural network (WNN) consists of single-scaling wavelets. Through efficient bases selection, the dimension of the approximated function does not cause the bottleneck for constructing FWN. Especially, by learning the translation parameters of the wavelets and adjusting the shape of membership functions, the model accuracy and the generalization capability of the FWN can be remarkably improved. Furthermore, an algorithm for constructing and training the fuzzy wavelet networks is proposed. Simulation examples are also given to illustrate the effectiveness of the method. Index Terms-Fuzzy neural networks, wavelet neural networks, wavelet transforms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>III. STRUCTURE OF FWNsMotivated by the reason stated in Section I, we present a new type of fuzzy wavelet-based model, called fuzzy wavelet networks (FWN), inspired by both the theory of MRA of wavelet transform and the fuzzy concepts.A typical FWN can be described by a set of following fuzzy rules:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The architecture of the FWN, here A means the membership function A (x ).</figDesc><graphic coords="4,136.14,62.28,321.12,389.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The algorithm for constructing FWN.</figDesc><graphic coords="6,351.60,62.28,153.12,303.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The initial results for FWN (solid line is original function and the dotted line FWN).</figDesc><graphic coords="7,135.96,200.82,318.48,248.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The comparison of outputs between original function (solid line) and the final result from FWN (dotted line).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Original function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Resulting approximation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Mackey-Glass time series from t = 124 to 1123 (solid line) and the six-step-ahead prediction by the FWN (dotted line).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Figs.10 and 11  show the behavior of the FWN (dotted line) in comparison with the actual process (solid line) when and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Prediction error curve.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Membership functions of the resulting FWN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10 .</head><label>10</label><figDesc>Fig.10. The outputs of the system (solid line) and the FWN (dotted line) for inputs x .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. The outputs of the system (solid line) and the FWN (dotted line) for inputs x .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF OUR FWN WITH WNN (FOR EXAMPLE A)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARISON</head><label>II</label><figDesc>OF OUR FWN WITH WNN (FOR EXAMPLE B)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III GENERALIZATION</head><label>III</label><figDesc>CAPABILITY COMPARISONS (THE LAST FIVE ROWS ARE FROM [17])</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Using wavelet networks in nonparametric estimation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="227" to="236" />
			<date type="published" when="1997-03">Mar. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wavelet networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Benveniste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="889" to="898" />
			<date type="published" when="1992-11">Nov. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wavelet neural networks for function learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">N W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1485" to="1497" />
			<date type="published" when="1995-06">June 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Space-frequency localized basis function networks for nonlinear system estimation and control</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J E</forename><surname>Slotine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="293" to="342" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Accuracy analysis for wavelet approximation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Delyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Benveniste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="332" to="348" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Analysis and synthesis of feedforward neural networks using discrete affine wavelet transformation</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Pati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Krishnaprasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="73" to="85" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Structurally dynamic wavelet networks for adaptive control of robotic systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J E</forename><surname>Slotine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Contr</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="405" to="421" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Self-construction algorithm for synthesis of wavelet networks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Lett</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="1953" to="1955" />
			<date type="published" when="1998-10">Oct. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Wavelet-based system identification for nonlinear control</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sureshbabu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Farrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Contr</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="412" to="417" />
			<date type="published" when="1999-02">Feb. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive neural network architectures for nonlinear function estimation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kavchak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Budman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACC&apos;1998, Philadelphia</title>
		<meeting>ACC&apos;1998, Philadelphia</meeting>
		<imprint>
			<date type="published" when="1998-06">June 1998</date>
			<biblScope unit="page" from="63" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic generation of RBF networks using wavelets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1369" to="1383" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nonlinear identification via variable wavelet networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Billings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kadirkamanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th IFAC</title>
		<meeting>14th IFAC<address><addrLine>Beijing</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-06">June 1999</date>
			<biblScope unit="page" from="109" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A new method for optimal synthesis of wavelet-based neural networks suitable for identification purposes</title>
		<author>
			<persName><forename type="first">F</forename><surname>Alonge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>D'ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mantione</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Raimondi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th IFAC</title>
		<meeting>14th IFAC<address><addrLine>Beijing, P.R. China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-06">June 1999</date>
			<biblScope unit="page" from="445" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Combined construction of wavelet neural networks for nonlinear system modeling</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th IFAC</title>
		<meeting>14th IFAC<address><addrLine>Beijing, P.R. China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-06">June 1999</date>
			<biblScope unit="page" from="451" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">WaveARX neural network development for system identification using a systematic design synthesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Bruns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ind. Eng. Chem. Res</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="4420" to="4435" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Identification and control of dynamical systems using neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Narendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Parthasarathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4" to="27" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S R</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mizutani</surname></persName>
		</author>
		<title level="m">Neuro-Fuzzy and Soft Computing</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rule-based modeling: Fast construction and optimal manipulation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, and Cybern</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="728" to="738" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fuzzy basis function, universal approximation, and orthogonal least-squares learning</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="807" to="814" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Training multilayer perceptrons with the extended Kalman algorithm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Processing Systems, D. S. Touretsky</title>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufman</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="133" to="140" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
