<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Conversing with the User Based on Eye-Gaze Patterns</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pernilla</forename><surname>Qvarfordt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer and Information Science Linköpings universitet</orgName>
								<address>
									<postCode>SE-581 83</postCode>
									<settlement>Linköping</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Shumin</forename><surname>Zhai</surname></persName>
							<email>zhai@us.ibm.com</email>
							<affiliation key="aff1">
								<orgName type="institution">IBM Almaden Research Center San Jose</orgName>
								<address>
									<postCode>95120</postCode>
									<country>California USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Conversing with the User Based on Eye-Gaze Patterns</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B1364650303A3E03D66DBE12AF7200B8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.5.2 [Information interfaces and Presentation]: User Interfaces -Input devices and strategies Multimodal interaction</term>
					<term>dialogue systems</term>
					<term>eye tracking</term>
					<term>interest detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Motivated by and grounded in observations of eye-gaze patterns in human-human dialogue, this study explores using eye-gaze patterns in managing human-computer dialogue. We developed an interactive system, iTourist, for city trip planning, which encapsulated knowledge of eyegaze patterns gained from studies of human-human collaboration systems. User study results show that it was possible to sense users' interest based on eye-gaze patterns and manage computer information output accordingly. Study participants could successfully plan their trip with iTourist and positively rated their experience of using it. We demonstrate that eye-gaze could play an important role in managing future multimodal human-computer dialogues.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Being "a window to the mind," the eye and its movements are tightly coupled with human cognitive processes. The possibility of taking advantage of information conveyed in eye-gaze has attracted many researchers in humancomputer interaction <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29]</ref>. Most of the work in this area to date are in three interrelated categories: eyegaze as a pointing mechanism in direct manipulation interfaces (e.g. Ware <ref type="bibr" target="#b26">[27]</ref>, Jacob <ref type="bibr" target="#b10">[11]</ref>, and Zhai et al <ref type="bibr" target="#b28">[29]</ref>), eyegaze as a disambiguation channel in speech or multimodal input (e.g. Tanaka <ref type="bibr" target="#b20">[21]</ref>, Kaur et al <ref type="bibr" target="#b11">[12]</ref> and Zhang <ref type="bibr" target="#b29">[30]</ref>, see also Oviatt <ref type="bibr" target="#b15">[16]</ref> for mutual disambiguation in general) and eye-gaze as a facilitator in computer supported humanhuman communication and collaboration (e.g.</p><p>Velichkovsky <ref type="bibr" target="#b23">[24]</ref> and Vertegaal et al <ref type="bibr" target="#b24">[25]</ref>).</p><p>As a new step in utilizing eye-gaze in HCI, the current work is motivated by the following combination of propositions.</p><p>1. The eye-gaze contains richer and more complex information regarding a person's interest and intentions than what is used in pointing <ref type="bibr" target="#b28">[29]</ref>. 2. Eye-gaze can be one of the critical channels in future multimodal systems. 3. Eye-gaze can be particularly useful in human-computer dialogue interfaces rather than direct manipulation interfaces. A particular challenge in developing computer dialogue systems lies in taking human-computer conversation beyond pre-defined scripts and adapting the conversation to the user's interest. Sensing where and how the user's eye-gaze moves when the dialogue subject is related to spatial content (such as maps) may provide contextual clues for the computer system to start and manage a dialogue with the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GAZE PATTERNS IN HUMAN-TO-HUMAN DIALOGUE</head><p>To investigate whether and how eye gaze can be used in managing multimodal human-computer dialogue systems, we first studied human-human conversation as a foundation of the current research. Previous work on the relationship between eye gaze and conversation mainly focused on patterns in face-to-face conversation concerning issues such as when and how long people look at each other <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26]</ref>. In conversations involving visual spatial information, such as a map, people spend most of the time looking at the spatial information rather than their partner <ref type="bibr" target="#b1">[2]</ref>.</p><p>In order to form a more direct foundation to our current work on using eye gaze to mediate human-computer dialogue, we developed and experimented with a system, RealTourist, which allowed a tourist to talk to a remote tourist consultant to plan a conference trip. The tourist and the consultant saw the same map displayed on their monitors respectively. On the consultant's side the system also superimposed the tourist's eye gaze onto the map, so the consultant could use it to determine the tourist's interests. In an experiment that involved 12 tourists and two tourist consultants, the tourist's eye gaze in relation to the trip planning conversation were collected, inspected, annotated, visualized, and analyzed. For more details of the RealTourist study and its related literature, see <ref type="bibr" target="#b16">[17]</ref>.</p><p>The RealTourist experiment can be viewed as a simulated ("Wizard of Oz") human-computer dialogue study. The results clearly showed that when conversing with another agent about spatial information the user's eye movement on the map was tightly coupled with the dynamics of the conversation. Various functions of the eye-gaze were identified in the study including implicit deictic referencing, interest detection by the consultant, bringing common focus between the tourist and the consultant, increasing redundancy in communication, and facilitating assurance and understanding.</p><p>In particular, the study revealed two types of eye-gaze patterns related to a person's interest in an object on the screen (e.g. a hotel, a restaurant etc). Two patters indicated interest in the current topic of the conversation. One of these was that the person looked at the object with high intensity and long accumulated duration. Sometimes a person exclusively looked at a place (e.g. a museum) on a map and its photo when intensely interested in it. In the second pattern that indicates the tourist's interest in the current topic of conversation, the tourist not only looked at the object of the topic (e.g. a hotel), but also at objects that were related to the current topic in some way. For example, when considering a particular attraction, e.g. the Museum of Modern Art, the person might look back at his or her hotel location to figure out the distance relationship. In this pattern, the person often returned the eye gaze to the object that was the focus of the conversation (focus object).</p><p>Similarly, disinterest could be characterized by two different eye-gaze patterns. The first was to completely leave the focus object, and when the person found a new object, he or she asked about the new object almost immediately. The second pattern was similar, however here the person did not immediately ask for the new object. Instead he or she kept an eye on the new object by returning the eye gaze to it while continuing to explore new possibilities until there was an opportunity to change the topic of the conversation to the new object.</p><p>We have also found a gaze pattern indicating interest in the relationship between two objects. Before and while people asked for the distance between the focus object and another object, they switched the eye gaze between the focus object and the other object frequently.</p><p>In addition to the RealTourist study, the current research is also based on conclusions from other gaze and speech studies in different but relevant settings. One of them is that when exposed to a visual stimulus and asked to talk about or listen to a story about it, people tend to look at the part of the stimulus that is relevant to the discourse. For example, Cooper found that people look at objects that are relevant to what they listen to <ref type="bibr" target="#b4">[5]</ref>. In his study when participants heard the word "lion," they looked at the lion in the picture. When they heard the word "Africa," they looked at the lion, the zebra and the snake. Later work has shown that the latency from the onset of the word referring to a specific object and when the eye looks at the picture of it is around 400-800 ms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. When people refer to an object, they look at an object around 900 ms before they refer to it <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ITOURIST -AN EXPERIMENTAL SYSTEM</head><p>To explore if eye-gaze information can provide useful information to multimodal dialogue systems, we developed an experimental tourist information system, iTourist, based on some of the findings summarized in the last section. iTourist provides the user with tourist information about an imaginary city, Malexander, in the form of a map and photos of different places. Information about the different places is presented by pre-recorded, synthesized speech. iTourist attempts to adapt its information output based on the user's interests and needs analyzed from the user's eyegaze pattern. Although our ultimate goal is to make eye gaze an integrated channel of multimodal systems including speech, gesture, and eye-gaze, iTourist was developed as a stress test to investigate how much information can be gained from a user's eye-gaze alone. iTourist does allow the user to use a mouse, but only as a backup channel when necessary (Figure <ref type="figure" target="#fig_0">1</ref>). iTourist contains information about 36 places shown on a map, including hotels <ref type="bibr" target="#b9">(10)</ref>, restaurants <ref type="bibr" target="#b9">(10)</ref>, attractions (8), nightclubs (5), a conference centre, a bus terminal and a tourist information office (Figure <ref type="figure" target="#fig_0">1</ref>). Each object has a number of spoken utterances (6 for each hotel or restaurant, 9 for each attraction) and images (5 for each hotel or restaurant, 8 for each attraction). When displayed, the images are connected with the place's location on the map with an arrow line. Synchronized with the changing photos displayed, the information utterances (sentences) for each place are played in a pre-defined order. After the last utterance a sound (a chime) is played to indicate the end of the information about the particular place. We found that it was difficult for the user to look at the same still image for an extended period of time, even if the user was interested in the place. We hence designed multiple images for each place that switched with new sentences. The content of the images reflects the content of the speech, e.g., when iTourist talks about the exhibition at the Royal Palace, it also shows the photos from the exhibition.</p><p>In addition to the information about each place, iTourist also contains distance and transit information between all places. It describes distance by walking time (minutes). If the walking distance is greater than 30 minutes, it gives bus transportation information instead. Simultaneously with a distance utterance, a line connecting the two places is shown on the map.</p><p>In total, 1618 utterances are included in iTourist, including 234 information utterances, 122 transit utterances, 1261 distance utterances, and one generic error message. In addition, a spoken introduction to the city is also included. This text is spoken by iTourist when it starts up to give an overview of the city. All the utterances were pre-synthesized by the Festival Speech Synthesis system and made into wavefiles.</p><p>The users' eye-gaze was tracked in iTourist by a Tobii Eye Tracker 1750 running on a server. iTourist communicates with the eye tracker server via TCP/IP. Eye-gaze data is received from the eye tracker at a rate of 50 Hz. All functions in iTourist are implemented in C++.</p><p>iTourist allows the user to interact with it solely based on eye-gaze patterns, except when the user wants to commit to a place for booking or for a visit. This is done by clicking the left mouse button. iTourist marks the place and writes its name at the bottom of the screen as a reminder.</p><p>Based on the insights and patterns identified from the RealTourist study, we designed and implemented iTourist's architecture and algorithms (Figure <ref type="figure">2</ref>). In iTourist, all objects (places) maintain two basic variables: the Interest Score (IScore) and the Focus Interest Score (FIScore).</p><p>Based on these two variables of all objects, the Event and Interaction Manager (EIM) in iTourist determines what, if anything, to talk about with the user. The object that iTourist currently talks about is called the active object. . At the core of iTourist is user interest detection. Although iTourist aims to be general enough for tourist information systems of its kind, it makes some assumptions about the nature of the information the user looks at and how a dialogue progresses in this type of situation. It assumes that the conversation is object-based and these objects are related to one another by category (e.g. hotels), location, or a task-dependent relationship, e.g. hotels and conference centre.</p><p>The two variables, IScore and FIScore, of each object represent the user's interest level in the object as an active object and inactive object, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IScore</head><p>IScore is used to determine an object's "arousal" level, or the likelihood that the user is interested in hearing about it. Its basic component is eye-gaze intensity (p), defined by accumulated gaze duration on the object (T ISon ) in a moving time window (T IS ). The moving time window ensures that measured p reflects the user's current interest.</p><formula xml:id="formula_0">IS ISon T T p = (1)</formula><p>where T ISon is the accumulated time on the object within time window T IS, and T IS is the size of the moving time window.</p><p>Critical to our design is that the intensity variable p is modified by other factors related to user interest. Some of these factors inhibit the object's excitability and others increase it. These factors are collected into one variable α, which is used to adjust the gaze intensity residual (1-p):</p><formula xml:id="formula_1">)) 1 ( 1 ( p p p is - + = α<label>(2)</label></formula><formula xml:id="formula_2">or 2 p p p p is α α - + = (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where p is is the arousal level of the object (i.e. IScore) and α is the excitability modification. The value of p is is between 1 and 0, and the value of α is between -1 and 1. A negative α inhibits the IScore, while a positive α excites the IScore.</p><p>If no other factors than the accumulated time on the object exist, then α is equal to zero and the IScore would solely be dependent on the eye-gaze intensity.</p><p>Based on our observations from the RealTourist study summarized earlier, the iTourist system uses four factors in an object's excitability modification in the following manner (Figure <ref type="figure" target="#fig_1">3</ref>):</p><formula xml:id="formula_4">a s c f a a s s c c f f c c c c c c c c c + + + + + + = α α α α α 0 (4)</formula><p>where α f is the frequency of the user's eye gaze entering and leaving the object; α c is the categorical relationship with the previous active object; α s is the relative size to a baseline object; and α a records previous activation of the object. c f , c c , c s and c a are constants empirically adjusted.</p><p>The frequency of the user's eye gaze entering and leaving (α f ), identified as one indication of a user's interest in an object in the RealTourist study, was calculated over a time window, N f . N f can be viewed as the "memory span" of an object:</p><formula xml:id="formula_5">f sw f N N = α (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where N sw is the number of times the eye gaze enters and leaves the object, and N f is the maximum possible N sw in the set time window.</p><p>The categorical relationship with the previous objects (α c ) is currently treated as binary; either the object is of the same category as the previous one, or not (although fuzzy membership is also possible in principle). The user is more likely to be interested in the same category before he/she finds a solution (a hotel, a restaurant) that matches his/her criteria in that category. When the user has committed to a place, he or she is not as likely to be interested in objects of the same category. For this reason, the value of a c can be either +1 (when the category of the current object matches the previous object), 0 (no match), or -1 (when the category of the current object matches any of the committed objects). This modification factor is particularly task-domain dependent. It models the regularity in the trip-planning task.</p><p>The interest detection algorithm was targeted at a mapbased application. On the map, most places (objects) have the same sizes (implicit bounding box, shown in Figure <ref type="figure" target="#fig_2">4</ref>). However, there are a number of larger objects, such as attractions. Since the data from the eye tracker is not noisefree, the larger objects have a higher probability of being hit by random noise. The excitability of objects is therefore adjusted according to their size:</p><p>.</p><formula xml:id="formula_7">S S S b s - = α (6)</formula><p>where S b is the area size of the common objects which are also the smallest, and S is the size of the current object. α s is 0 when S is the same as S b . Otherwise it is negative, and therefore inhibits large objects' excitability.</p><p>Finally, if a place previously has been active, it is not very likely that the user intends to activate it again. This is the last component of α, defined by α a . Previous activation of a place should inhibit its excitability. α a is -1 when the place has been activated and 0 when it has not been activated.</p><p>Our approach in modeling IScore is prescriptive, encapsulating many of the behavioral patterns observed in our RealTourist study. It is easily extendable to other factors that can influence an object's excitability. For example, user preferences of the responsiveness of the system could be added as a fifth factor to α. This approach can be made more sophisticated in the future. For example, the relative weights of the factors constituting α could be updated with machine-learning algorithms to better adapt to the user's behavior.</p><p>As soon as the IScore of an object moves above a set threshold, the object is qualified to become the active focus of dialogue, pending the Event and Interaction Manager's control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FIScore</head><p>FIScore measures how the user keeps up his or her interest in an active object. The FIScore is similar to IScore in that the base component of FIScore is the intensity of the user's eye-gaze on the object. However it only involves one other factor, which is the eye-gaze intensity on all related objects during the same time window. A weight constant c r makes sure that the intensity of related objects contributes less than the intensity of the active object to the FIScore:</p><formula xml:id="formula_8">F ron r on FIS T T c T p + = (7)</formula><p>where T on is the accumulated time on the active object within time window T F and T ron is the accumulated time of  all related objects within time window T F . Objects were considered related if their location was close to the active object, or if they were on the list of objects the user had committed to. In addition, all hotels had the Conference Centre as a related object. Objects with the same category as the active object were not considered related in calculating the FIScore, although they could be located close to one another. This allowed the user to easily switch between, for example, different hotel alternatives.</p><p>When an active object's FIScore dropped below a set threshold, the object could be deactivated by the Event and Interaction Manager.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interest in distance relation</head><p>Eye-gaze patterns can be used not only to detect interest in an object, but also to detect interest in the relationship between two objects. We observed in the RealTourist study that when judging distance, the tourist switched back and forth between two places on the map. We use this eye-gaze pattern as the basis for iTourist to give information about the distance between two objects. However, looking back and forth between two objects is a relatively common eyegaze pattern. It does not always indicate an interest in distance. One restriction imposed on iTourist in this regard is that distance information can only be triggered between the active object and another object. This assumes that the user is only interested in how other objects are located in relation to the currently active object.</p><p>The specific distance interest detection algorithm involves an object memory that sequentially stores objects being recently fixated on by the eye-gaze. The algorithm checks transitions between these objects in the memory store. If a pair of objects (one of them has to be the current active object) with two or more transitions between them is identified, iTourist will utter the distance between them to the user. If more than one pair exits with two or more transitions, the distance between the pair with the highest number of transitions is uttered.</p><p>Once the distance information is presented to the user, we found that the user tends to gaze back and forth between the two objects, which may result in a loop of distance information. For this reason, our system keeps track of the history and suppresses the immediate second occurrence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System Tuning</head><p>The coefficients in the iTourist algorithms were tuned by testing with the authors and five pilot participants. They were first set at the most plausible values according to the relative importance of each factor and then modified during pilot testing.</p><p>The IScore window (T IS ) was set to 1000 ms. The activation threshold for IScore was set to 0.45. This means that if the IScore was only influenced by the time the user looked at an object, the user needed to look at it for 450 ms to trigger it.</p><p>The FIScore window (T F ) was set to 2000 ms. This makes the FIScore slightly more conservative than the IScore. The FIScore threshold was set to 0.22. The weight (c r ) on related objects' accumulated time in T F was set to 0.3. When the user only looked at related objects and not the active object, he or she needed to look at related objects longer than 1466 ms (within the 2000 ms window) to keep it active.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dialogue Model</head><p>The algorithms for calculating IScore, FIScore, and interest in distance between two objects are based on a certain implicit dialogue model the users follow in the tourist information domain. In addition iTourist also models the order of information presented.</p><p>Each place has a number of sentences iTourist can speak.</p><p>When an object becomes deactivated and reactivated again within one minute, the Event and Interaction Manager will continue from where it stopped. If the object is deactivated for more than one minute and re-activated, the Event and Interaction Manager will start from the first sentence.</p><p>When an object becomes active, the first utterance depends on the last active object. When the last active object is related to a newly activated object, iTourist plays a transit utterance. For example, if the user listens to information about the Royal Palace and then changes interest to the Museum of Modern Art, iTourist starts out saying: "The older art collection at the Royal Palace can be complemented with more modern art at the Museum of Modern Art" before giving the regular information about the museum.</p><p>When ITourist finishes talking about an object that is still active (FIScore above deactivation threshold), it will repeat the same information from the start.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Event and Interaction Manager</head><p>The Event and Interaction Manager (EIM) manages the state of the whole system based on the states of all individual objects. Figure <ref type="figure" target="#fig_3">5</ref> illustrates the basic logic flow of the EIM.</p><p>The states of the individual objects are evaluated each time new eye-gaze data arrives from the eye tracker. EIM starts out to check if there is a current active object. If there is no active object, EIM finds the object whose IScore has passed the activation threshold. It activates that object and starts to play its information (visual images and speech).</p><p>If there is an active object, EIM first checks if the object is currently playing information (i.e. in the middle of a sentence). As long as the object plays information, it stays active. When the object is active and is not playing information, EMI determines if the object should play new information or if it should be deactivated by checking if the object's FIScore has dropped below the deactivation threshold. This procedure enables iTourist to make graceful endings with the spoken output. When the active object's FIScore drops below the deactivation threshold, EIM deactivates it. EIM then checks if there is any object whose IScore has gone above the activation threshold. If there is more than one object, EIM chooses the object to which the user most recently looked.</p><p>Mouse events can also trigger transitions from the inactive state to the active state, or change the currently active object. In this case the IScore or the FIScore does not need to be reached the thresholds for activation/deactivation. EIM processes the mouse events as soon as they are received. This means that iTourist can be interrupted in the middle of the sentence with a mouse click.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RELATED WORK</head><p>Although not all in the context of managing humancomputer dialogue, there have been several previous attempts at detecting users' interest based on patterns in eye gaze. Most of them have been focused on text reading or target selection with eye gaze.</p><p>Detecting gaze patterns in text is often used to identify words that a user has difficulty with. One example of this is the Reading Assistant by Sibert et al. <ref type="bibr" target="#b18">[19]</ref>. It pronounces words the user looks at for longer than a predefined threshold. Another popular topic is dictionaries, such as iDict by Hyrskykari et al <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. iDict decides which word's translation is displayed based on the eye-gaze dwell time. A different application using word detection from gaze patterns is SUITOR by Maglio and colleagues <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref>. SUITOR identifies content words and displays information, such as news, relevant to these words.</p><p>Gaze patterns have also been used to make selection with eye gaze more accurate. Methods for selection have been developed to fit a context; for example, Salvucci <ref type="bibr" target="#b17">[18]</ref> developed a method for eye typing based on gaze patterns and language models. Gaze patterns have also been used for identification of an object <ref type="bibr" target="#b13">[14]</ref>, and for performing actions such as zooming <ref type="bibr" target="#b6">[7]</ref>. Edwards <ref type="bibr" target="#b5">[6]</ref> presented an algorithm that attempts to distinguish if the user is merely looking around, or wanting to select a target. The goal of this algorithm was to avoid the Midas touch problem in eyegaze selection.</p><p>The methods for detecting gaze patterns used in previous work range from relatively simple accumulated time on target <ref type="bibr" target="#b18">[19]</ref> to more sophisticated statistical models <ref type="bibr" target="#b17">[18]</ref>. All of these methods include additional processing of the gaze data in addition to locating where the user is fixating. For example, all the methods detecting gaze patterns of text use algorithms to detect which line the user is currently reading. The additional processing of the eye-gaze data is specific to the purpose of the system and the type of visual information the users look at. This also means that only parts of the methods can be transferred to other domains.</p><p>The work that is most similar to the present study is by Starker and Bolt <ref type="bibr" target="#b19">[20]</ref>. They used users' eye-gaze to modify a story told by a narrator. Starker and Bolt proposed three methods for measuring users' interest in particular objects. Model 1 essentially used the accumulated time on an object.</p><p>In the two other models, the interest level raised when the user looked at an object and decayed when the user looked away from the object. The two latter models emphasized the user's most current interest while the first model emphasized the whole history of interest in an object. Starker and Bolt did not use a variable (as our FIScore) to determine if the user had lost interest in the current object. Instead they used "a low-end cut-off point of the sample standard deviation of item interest levels" (p. 8). The reason to use this procedure was that the next topic was decided based on which object had the highest interest level. Another weakness with the three different methods for calculating the interest levels was that they were never formally tested with interactive users. Starker and Bolt reported that the three algorithms were tested with prerecorded eye-gaze patterns and simulated eye gaze by an "eye cursor." They noted that "differences in the narrative behavior showed up between the three models" (p. 8), and that model 3 talked longer than model object based on the user's eye-gaze on related objects. Another difference is that while Starker and Bolt's method might be suitable for a narrative purpose, it was not designed to handle a more interactive human-computer system like ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USER STUDY OF ITOURIST</head><p>Although iTourist is still at a preliminary stage of technical development based on relatively simple algorithms, it is important to test if our goal of making eye gaze a channel of human-computer dialogue and if the iTourist approach based on observations from RealTourist study are feasible at all. We hence conducted a somewhat informal but realistic user study of iTourist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Design</head><p>Twelve people, two women and ten men from twenty-seven to forty-one (median of 30.5) years of age, participated in a user study of iTourist. They had diverse cultural backgrounds and eight of them spoke English as a second language.</p><p>Their task was to plan a conference trip to the city Malexander. The task was made rather realistic. Various constraints such as hotel and meal limits were imposed on the trip. The participants were asked to find a hotel for lodging, a restaurant for a group dinner, one night club and enough attractions for a spare day on the weekend. They were encouraged to consider their own convenience and preferences in addition to price restrictions. They were also encouraged to explore the city, get a good idea of it, and be prepared to be quizzed later.</p><p>A total number of fifteen persons volunteered for the study. Three of them were excluded for failing to comply with the instructions, having a high rate of lost eye-gaze data, or having difficulty in understanding the synthesized speech.</p><p>Given that the focus of our study was not on the quality of eye trackers or speech synthesis, the exclusion of these three participants should not impact our conclusions on the feasibility of iTourist and the soundness of the principles it embodies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>The experimental session started with an introduction to the study and eye-tracker calibration. The participant was encouraged to try to find the best position for the eye tracker (Figure <ref type="figure" target="#fig_0">1</ref>). After that, the experiment proceeded to planning the trip to Malexander. After completion, the participants were asked a few questions about the city, and asked to fill in a post-test questionnaire. The session ended with a short interview in which the participants could comment on their experience of using iTourist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance of the eye tracker</head><p>The performance of iTourist depended on the quality of eye tracking. On average, the twelve participants' eye-gaze was missed by the eye-tracker 7.6% of the time (SD=3.14). The time period of missing eye-gaze was on average 86 ms long (SD=188.9). The longer periods of loss happened when the users read instructions on paper. In a total of six cases the loss of eye tracking caused iTourist to deactivate the current focus object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of eye-gaze pattern detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IScore</head><p>As the core concept of eye-gaze interest pattern detection in iTourist, IScore worked well in the study. One measurement of its success was how often the users used the mouse to activate a place. Of the twelve users, only four ever moved the mouse to activate any object. One of these four users, User 4, used the mouse seven times. The other three used the mouse only once. On average the mouse was involved in only 3% of all object activation (SD=5.7).</p><p>The time the users spent looking at a place before it was activated depended on which state iTourist was in. When the system state was inactive, iTourist rapidly detected the user's interest in a new place. In this case the average activation time from the first moment the user looked at a new object to the moment iTourist started talking about it was 421 ms. Since IScore's threshold was 0.45 and its time window was set at 1000 ms, factors other than gaze intensity such as category relationship, frequent revisit and activation history must have contributed to the excitability of some of the objects as planned. When the iTourist system was in an active state (talking about something), it took on average 1507 ms to switch the activation to another object. This delay was due to the fact that iTourist did not stop in the middle of a sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FIScore</head><p>One user thought that iTourist stopped talking somewhat too early, suggesting that the FIScore's (deactivating) threshold was too high for him. One measurement of the success of the FIScore algorithm was how many times a user reactivated a place immediately after it was deactivated. Analysis showed that there were 1.6 (SD=1.56) or 6.3% such cases per person or 19 in total of all participants. Four of these were caused by low accuracy and missed eye gaze data by the eye-tracker. Four others were preceded by a distance utterance. This kind of utterance often made the users look at the two places iTourist was talking about and hence caused more eye movements than regular utterances. The small number of prematurely deactivated places shows that the FIScore algorithm worked well in general.</p><p>The fact that iTourist always had to finish an already started sentence was noticed by the users. On average, iTourist detected the user's disinterest (or losing interest) in an active object 1.4 sec after the start of a sentence. Since the mean length of the sentences played by iTourist was 4.7 sec (SD=1.6), it means that often iTourist had to continue for another 3.3 sec (mean) to finish its sentence, despite "knowing" the user was no longer interested in the object it was talking about. Some participants commented on this, but also stated that it would be odd if iTourist cut itself off before finishing a sentence.</p><p>In the post-test questionnaire, the participants were asked two questions regarding interest and disinterest detection: how often iTourist talked about a place they were not interested in (false alarm), and how often iTourist did not talk about places they were interested in (misses). The rating distributions on these questions are shown in Figure <ref type="figure" target="#fig_4">6</ref>. They indicate that the algorithms were not always successful, but in general were well received. Most of the false alarms were related to distance relation detection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distance relation detection</head><p>The distance interest detection in iTourist was probably most naïve. It was also the most difficult to handle. Users' reaction to it varied from person to person. Some were apparently not aware of it (even though the function was mentioned in the written introduction). One, User Other users took advantage of the distance interest detection. "Actually, when I looked at a restaurant, the distance between the hotel and the restaurant appeared. That is very nice. It was the kind of information that I wanted to know" (User 11)</p><p>One of the users listened through 18 distance information utterances. On average, each user listened to 7.2 distance information utterances (SD=4.5), of which 1.2 utterances were repeated (SD=1.4). The distance information contributed to, on average, 30.6% (SD=18.01) of all the sentences played by iTourist.</p><p>Overall Results: Acceptance and Impression of iTourist</p><p>The most remarkable evidence of the system's success was that all of the participants completed their tasks with iTourist. Only three users had minor deviations from the stated rule of checking price information of some of the restricted choices (two restaurants and one hotel), although all these three cases were in fact within the set price limit.</p><p>Considering it was the same trip planning task as in our previous RealTourist study that involved a remote human "Tourist Consultant" on the line busy looking up information and talking about places in natural language, this is quite encouraging for the role that eye-gaze may play in human-machine communication.</p><p>The participants spent between 5:15 minutes and 13:50 minutes (average 8:27 minutes, SD=2:34) using iTourist to plan their trip. They looked at, on average, 16.6 (SD=3.6) places and listened, on average, to 82.4 utterances (SD=2.3). For each place they looked at, on average they listened to 3.8 utterances (SD=1.2). The post-test interview showed that they had a correct impression of and clear motivation to visit the places they had learned about. Many of the users enjoyed using iTourist; one of the users expressed a wish to visit Malexander. In the post-test questionnaire, the iTourist average was a 3.9 (SD=.79) rating on a 5-grade scale (5 the best rating and 1 the worst).</p><p>Figure <ref type="figure" target="#fig_5">7</ref> shows the distribution of the twelve users on the question as to how well they thought iTourist worked. In the post-test interview the users gave overall quite positive comments about iTourist.</p><p>"I thought it was pretty convenient…the idea is really cute... like you look at something and it pops up this information bit" (User 7).</p><p>One of the design goals of iTourist was to utilize natural eye-gaze movement determined by the task as implicitly as possible. In the actual tests users were clearly aware of the role of their eye gaze, as indicated by the comment above.</p><p>As to whether they have to consciously control the system with their gaze (see our previous discussions on avoiding eye-control <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>), four of the twelve participants felt they had to "stare at" an object at least once. Usually this happened when iTourist was in the middle of a sentence, but the participant wanted to move on. Overall, most participants felt that it was quite natural to use iTourist without being too conscious of their eye gaze. This is evident in the following comment. "This is the best experience that I ever had with eye tracking." User 15</p><p>Some of the users wanted to have more interaction capabilities in iTourist, although they realized that the system was pushing the limit with only eye gaze as the input method.</p><p>"I think for now when it doesn't have back channel [from the user to iTourist] it works all right; you cannot do any better."(User 8)</p><p>Apparently, there were cases where iTourist talked about places the participant did not consciously want to know about. Some of the users enjoyed a certain degree of "randomness" from iTourist and did not describe the experience as being "out of control". Preference for responsiveness differed from one individual to another. Some of the users felt iTourist, with its current parameter settings, was too sensitive, while others enjoyed a high degree of responsiveness. Adapting to individual users' preferences appears to be a critical factor for the success of future multimodal information systems that use eye gaze.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DISCUSSION AND CONCLUSION</head><p>Motivated by and grounded in observations of eye-gaze patterns in human-human dialogue over spatial content, we explored the use of eye-gaze patterns in managing humancomputer interaction. In contrast to previous HCI works on eye-gaze that have primarily focused on direct manipulation user interfaces, this research targets the human-machine dialogue type of interaction. We envision future multimodal HCI systems with eye gaze as a critical contributing channel. Towards that goal, we developed a system, iTourist, that encapsulates knowledge gained in a previous study on a computer-mediated, human-human collaboration system (RealTourist) for city trip planning. iTourist provides information about a city to a user based solely or primarily on the user's eye-gaze patterns. The results show that eye-gaze can indeed play an important role in managing human-computer dialogue. In this study, by eyegaze information alone the system could manage its visual and audio (speech) output and help users to plan an entire trip to a city, including finding hotels and restaurants within price limits and to the user's liking in terms of distance, location and preferences. It is remarkable that the "tourist consulting" task, quite demanding on the part of a real human operator in attending the tourist's eye-gaze, hearing what he or she asks about, looking up information, and telling the user relevant information <ref type="bibr" target="#b16">[17]</ref>, could be accomplished by iTourist successfully. The overall reactions of the users to iTourist were quite positive. On the other hand, although it is not as flexible as interactive storytelling previously researched with eye-gaze tracking <ref type="bibr" target="#b19">[20]</ref>, trip planning is a more tolerant task than many common HCI applications. A user is not likely to be very disturbed if iTourist talked about something he or she has no intention to learn about, which happens with human agents too.</p><p>Our investigation also showed that the prescriptive approach to developing interaction algorithms based on observations in human-human communication, although relatively naïve, could work at least in a specific domain.</p><p>Note that iTourist could be made to work with any city, as long as relevant information is given to it.</p><p>Undoubtedly our approach in general and the iTourist implementation in particular are far from being mature or perfect. Eye-gaze pattern-based interaction systems, as any other recognition based systems, can produce both false alarms and misses. Some of these limitations can be overcome by developing more advanced techniques such as statistical learning, but more importantly ambiguity can be dramatically reduced when multiple modalities are combined due to the mutual disambiguation effects <ref type="bibr" target="#b15">[16]</ref>. If eye-gaze pattern alone in our stress test of iTourist could be successful most of the time, its role can be expected to be even more powerful when combined with other modalities such as speech recognition.</p><p>ACKNOWLEDGEMENT Pernilla Qvarfordt was supported by the Graduate School in Human-Machine Interaction of Stockholm-Linköping, Sweden, the Swedish Agency for Innovation Systems </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. User in front of iTourist.</figDesc><graphic coords="2,330.88,265.64,212.64,135.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. How the measured gaze intensity is filtered by other factors to finally form the IScore</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. iTourist map, shown here with superimposed object bounding box and eye-gaze fixation trace</figDesc><graphic coords="4,324.31,58.34,227.13,199.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Event and Interaction Manager logic flow</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Users' rating on false alarms and misses</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Distribution of the users' ratings on the question of how well iTourist worked (1-lowest, 5-highest)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>"</head><label></label><figDesc>Overall, I would say that it is a nice program, very cool, and you get to know quite a lot when you listen and you look everywhere and you get quite a good feeling of what you want to do quite fast ---I think this was a really cool program. I mean, it tracks down what you think and what you want to do." (User 10) "Actually, I think it is pretty good. It does get my attention. It doesn't bore me or annoy me. I think I like it. I'm quite impressed how it correctly interprets my attention to talk about places that I'm interested in." (User 11)</figDesc><table><row><cell>"I liked it. It worked pretty well." (User 12)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(VINNOVA), the Centre for Industrial Information Technology (CENIIT), and the IBM Almaden Research Center. We thank David Beymer, Arne Jönsson, and Tue Andersen for their input to this project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tacking the time course of spoken word recognition using eye movements: Evidence for continuous mapping models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Allopenna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Magnuson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Tanenhaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="419" to="439" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The central Europe experiment -Looking at persons and looking at things</title>
		<author>
			<persName><forename type="first">M</forename><surname>Argyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Environmental Psychology and Nonverbal Behaviour</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6" to="16" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Eyes at the interface</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Bolt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Human Factors in Computer Systems</title>
		<meeting>Human Factors in Computer Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1982">1982</date>
			<biblScope unit="page" from="360" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A robust algorithms for reading detection</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Maglio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Workshop on Perceptive User Interfaces</title>
		<meeting>ACM Workshop on Perceptive User Interfaces</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The control of eye fixation by the meaning of spoken language -a new methodology for the real-time investigation of speech perception, memory, and language processing</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="84" to="107" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A tool for creating eye-aware applications that adapt to changes in user behaviour</title>
		<author>
			<persName><forename type="first">G</forename><surname>Edwards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd International ACM Conference on Assistive Technologies</title>
		<meeting>3rd International ACM Conference on Assistive Technologies</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Eye-gaze determination of user intent at the computer interface</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Schryver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eye Movement Research --Mechanisms, Processes and Applications</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Findlay</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Walker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Kentridge</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier Science</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What the eye says about speaking</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">M</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="274" to="279" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Design issues of iDICT: A gaze-added translation aid</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyrskykari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Majaranta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aaltonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-J</forename><surname>Räihä</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Symposium on Eye Tracking Research &amp; Applications</title>
		<meeting>ACM Symposium on Eye Tracking Research &amp; Applications</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Proactive response to eye movements</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyrskykari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Majaranta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-J</forename><surname>Räihä</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERACT -IFIP Conference on Human-Computer Interaction</title>
		<meeting>INTERACT -IFIP Conference on Human-Computer Interaction</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">What you look at is what you get: Eye movement-based interaction techniques</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J K</forename><surname>Jacob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI</title>
		<meeting>CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Where is &quot;it&quot;? event syncronisation in gaze-speech input systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tremaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wilder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gacovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Flippo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Mantravadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fifth International Conference on Multimodal Interfaces</title>
		<meeting>Fifth International Conference on Multimodal Interfaces</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="151" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Kendon Some function of gaze direction in social interaction</title>
	</analytic>
	<monogr>
		<title level="j">Acta Psychologica</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Eye pattern analysis in intelligent virtual agents</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Selker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IVA</title>
		<title level="s">Lecture Notes in Artificial Intelligence</title>
		<meeting>IVA</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="volume">2190</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SUITOR: An attentive information system</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Maglio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Selker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Intelligent User Interfaces</title>
		<meeting>International Conference on Intelligent User Interfaces</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mutual disambiguation of recognition errors in a multimodal architecture</title>
		<author>
			<persName><forename type="first">S</forename><surname>Oviatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI</title>
		<meeting>CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="576" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Qvarfordt</surname></persName>
		</author>
		<title level="m">Eyes on multimodal interaction</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer and Information Science, Linkoping University, Linkoping Studies in Science and Technology Dissertation No</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inferring intent in eye-based interfaces: Tracing eye movements with process models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Salvucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI</title>
		<meeting>CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="254" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The reading assistant: Eye gaze triggered auditory prompting for reading remediation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Sibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gokturk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Lavine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Symposium on User Interface Software and Technology</title>
		<meeting>ACM Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="101" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A gaze-responsive selfdisclosing display</title>
		<author>
			<persName><forename type="first">I</forename><surname>Starker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Bolt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI</title>
		<meeting>CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="3" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A robust selection system using real-time multi-modal user-agent interactions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th International Conference on Intelligent User Interfaces</title>
		<meeting>4th International Conference on Intelligent User Interfaces</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="105" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Eye movements and lexical access in spoken-language comprehension: Linking hypothesis between fixations and linguistic processing</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Tanenhaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Magnuson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chambers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Psycholinguistics Research</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="557" to="580" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Integration of visual and linguistic information in spoken language comprehension</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Tanenhaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Spivey-Knowlton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Eberhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Sedivy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">268</biblScope>
			<biblScope unit="page" from="1635" to="1634" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Communicating attention-gaze position transfer in cooperative problem solving</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Velichkovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pragmatics and Cognition</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="224" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The GAZE Groupware System: Mediating Joint Attention in Multiparty Communication and Collaboration</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vertegaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI</title>
		<meeting>CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="294" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Eye gaze patterns in conversations: there is more the conversational agents than meets the eyes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vertegaal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Slagter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Van Der Veer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nijholt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI</title>
		<meeting>CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="301" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An evaluation of an eye tracker as a device for computer input</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ware</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Mikaelian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI+GI (1987)</title>
		<meeting>CHI+GI (1987)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="183" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">What&apos;s in the Eyes for Attentive Input</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="page" from="34" to="39" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Manual and gaze input cascaded (MAGIC) pointing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ihde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc</title>
		<imprint>
			<biblScope unit="page" from="246" to="253" />
			<date type="published" when="1999">1999</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Overriding errors in speech and gaze multimodal architecture</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Imamiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th International Conference on Intelligent User Interfaces</title>
		<meeting>9th International Conference on Intelligent User Interfaces</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="346" to="348" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
