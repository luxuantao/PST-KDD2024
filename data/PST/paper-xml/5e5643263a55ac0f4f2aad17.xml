<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MULTILINGUAL END-TO-END SPEECH TRANSLATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-10-31">31 Oct 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hirofumi</forename><surname>Inaguma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Kyoto University</orgName>
								<address>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Duh</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center for Language and Speech Processing</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tatsuya</forename><surname>Kawahara</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Kyoto University</orgName>
								<address>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center for Language and Speech Processing</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MULTILINGUAL END-TO-END SPEECH TRANSLATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-10-31">31 Oct 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1910.00254v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Speech translation</term>
					<term>multilingual end-to-end speech translation</term>
					<term>attention-based sequence-to-sequence</term>
					<term>transfer learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a simple yet effective framework for multilingual end-to-end speech translation (ST), in which speech utterances in source languages are directly translated to the desired target languages with a universal sequence-to-sequence architecture. While multilingual models have shown to be useful for automatic speech recognition (ASR) and machine translation (MT), this is the first time they are applied to the end-to-end ST problem. We show the effectiveness of multilingual end-to-end ST in two scenarios: one-to-many and many-to-many translations with publicly available data. We experimentally confirm that multilingual end-to-end ST models significantly outperform bilingual ones in both scenarios. The generalization of multilingual training is also evaluated in a transfer learning scenario to a very low-resource language pair. All of our codes and the database are publicly available to encourage further research in this emergent multilingual ST topic 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Breaking the language barrier for communication is one of the most attractive goals. For several decades, the speech translation (ST) task has been designed by processing speech with automatic speech recognition (ASR), text normalization (e.g. punctuation restoration, case normalization etc.), and machine translation (MT) components in a cascading manner <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Recently, end-to-end speech translation (E2E-ST) with a sequence-to-sequence model has attracted attention for its extremely simplified architecture without complicated pipeline systems <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. By directly translating speech signals in a source language to text in a target language, the model is able to avoid error propagation from the ASR module, and also leverages acoustic clues in the source language, which have shown to be useful for translation <ref type="bibr" target="#b5">[6]</ref>. Moreover, it is more memory-and computationally efficient since complicated decoding for the ASR module and the latency occurring between ASR and MT modules can be bypassed.</p><p>Although end-to-end optimization demonstrates competitive results compared to traditional pipeline systems <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref> and even outperforms them in some corpora <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>, these models are usually trained with a single language pair only (i.e. bilingual translation). There is a realistic scenario in the applications of ST models when a speech utterance is translated to multiple target languages in a lecture, news reading, and conversation domains. For example, TED talks are mostly conducted in English and translated to more than 1 Available at https://github.com/espnet/espnet.  <ref type="bibr" target="#b8">[9]</ref>. In these cases, it is a natural choice to support translation of multiple language pairs from speech.</p><p>A practical approach for multilingual ST is to construct (monoor multi-lingual) ASR and (bi-or multi-lingual) MT systems separately and combine them as in the conventional pipeline system <ref type="bibr" target="#b9">[10]</ref>. Thanks to recent advances in sequence-to-sequence modeling, we can build strong multilingual ASR <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>, and MT systems <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> even with a single model. However, when speech utterances come from multiple languages, mis-identification of the the source language by the ASR system disables the subsequent MT system from translating properly since it is trained to consume text in the correct source language <ref type="foot" target="#foot_0">2</ref> . In addition, text normalization, especially punctuation restoration, must be conducted for ASR outputs in each source language, from which additional errors could be propagated.</p><p>In this paper, we propose a simple and effective approach to perform multilingual E2E-ST by leveraging a universal sequence-tosequence model (see Figure <ref type="figure" target="#fig_0">1</ref>). Our framework is inspired by <ref type="bibr" target="#b15">[16]</ref>, where all parameters are shared among all language pairs, which also enables zero-shot translations. By building the multilingual E2E-ST system with a universal architecture, it is free from the source language identification and the complexities of training and decoding pipelines are drastically reduced. Furthermore, we do not have to care about which parameters to share among multiple language pairs, which can be learned automatically from training data. To the best of our knowledge, this is the first attempt to investigate multilingual training for the E2E-ST task.</p><p>We conduct experimental evaluations with three publicly available corpora: Fisher-CallHome Spanish (Es→En) <ref type="bibr" target="#b18">[19]</ref>, Librispeech (En→Fr) <ref type="bibr" target="#b19">[20]</ref>, and Speech-Translation TED corpus (En→De) <ref type="bibr" target="#b20">[21]</ref>. We evaluate one-to-many (O2M) and many-to-many (M2M) translations by combining these corpora and confirm significant improvements by multilingual training in both scenarios. Next, we evaluate the generalization of multilingual E2E-ST models by performing transfer learning to a very low-resource ST task: Mboshi (Bantu C25)→Fr corpus (4.4 hours) <ref type="bibr" target="#b21">[22]</ref>. We show that multilingual pretraining of the seed E2E-ST models improves the performance in the low-resource language pair unseen during training, compared to bilingual pre-training. Our codes are put to the public project so that results can be reproducible and strictly compared in the same preprocessing (e.g., data split, text normalization, and feature extraction etc.), model implementation, and evaluation pipelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND: SPEECH TRANSLATION</head><p>In this section, we describe the architecture of the pipeline and endto-end speech translation (ST) system. Our ASR, MT, and ST systems are all based on attention-based RNN encoder-decoder models<ref type="foot" target="#foot_1">3</ref>  <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. Let x src be the input speech features in a source language, y src and y tgt be the corresponding reference transcription and translation, respectively. In this work, we adopt a character-level unit both for source and target references<ref type="foot" target="#foot_2">4</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Pipeline speech translation</head><p>The pipeline ST model is composed of three modules: automatic speech recognition (ASR), text normalization, and neural machine translation (NMT) models <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Automatic speech recognition (ASR)</head><p>We build the ASR module based on hybrid CTC/attention framework <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, where the attention-based encoder-decoder is enforced to learn monotonic alignments by jointly optimizing with Connectionist Temporal Classification (CTC) objective function <ref type="bibr" target="#b28">[29]</ref>. Our ASR model consists of three modules: the speech encoder, transcription decoder, and the softmax layer for calculating the CTC loss. The speech encoder transforms input speech features x src into a high-level continuous representation, and then the transcription decoder generates a probability distribution Pasr(y src |x src ) = i Pasr(y src i |y src &lt;i , x src ) conditioned over all previously generated tokens. We adopt a location-based scoring function <ref type="bibr" target="#b29">[30]</ref>. During training, parameters are updated so as to minimize the linear interpolation of the negative log-likelihood Latt = − log Patt(y src |x src ) and the CTC loss Lctc = − log Pctc(y src |x src ) with a tunable parameter λ (0 ≤ λ ≤ 1): Lasr = (1 − λ)Latt + λLctc. During the inference, left-to-right beam search decoding is performed jointly with scores from both an external recurrent neural network language model (RNNLM) <ref type="bibr" target="#b30">[31]</ref> (referred to as shallow fusion) and the CTC outputs. We refer the readers to <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> for more details.</p><p>For multilingual ASR models, we prepend the corresponding language ID to reference labels so that the decoder can jointly identify the target language while recognizing speech explicitly, which can be regarded as multi-task learning with ASR and language identification tasks <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Text normalization</head><p>In this work, we skip punctuation restoration for the simplicity <ref type="foot" target="#foot_3">5</ref> . Instead, we train the MT model so that it translates source references without punctuation marks to target references with them, where text normalization task is jointly conducted with the MT task and it can be seen as multi-task learning. During inference, the MT model consumes hypotheses from the ASR model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3.">Neural machine translation (NMT)</head><p>Our NMT model consists of the source embedding, text encoder, and translation decoder. The text encoder maps a sequence of source tokens y src into the distributed representation following the source embedding layer. The translation decoder generates a probability distribution P (y tgt |y src ). The only differences between the transcription and translation decoders are the score function for the attention mechanism. We adopt an additive scoring function <ref type="bibr" target="#b23">[24]</ref>. Optimization is performed so as to minimize the negative log-likelihood − log P (y tgt |y src ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">End-to-end speech translation (E2E-ST)</head><p>Our end-to-end speech translation (E2E-ST) model is composed of the speech encoder and translation decoder. To compare strictly, we use the same speech encoder and translation decoder as ASR and NMT tasks, respectively. Parameters are updated so as to minimize the negative log-likelihood − log P (y tgt |x src ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MULTILINGUAL E2E SPEECH TRANSLATION</head><p>We now propose an efficient framework that extends the bilingual E2E-ST model described previously to a multilingual one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Universal sequence-to-sequence model</head><p>We adopt a universal sequence-to-sequence architecture instead of preparing separate parameters per language pair for four reasons. First, E2E-ST can be generally considered as a more challenging task than MT due to its more complex encoder, which requires more parameters (e.g., VGG+BLSTM). In addition, training sentences in standard ST corpora are much smaller than MT tasks (&lt;300k) although input speech frames are much longer than text. Therefore, by sharing all parts, the total number of parameters are also reduced considerably and the E2E-ST model can have more training samples for better translation performance. Furthermore, it is not necessary to change the existing architecture. Second, we do not have to carefully pre-define a mini-batch scheduler for the language cycle as in <ref type="bibr" target="#b32">[33]</ref> (see Section 3.3). Third, translation performance in low-resource directions can be improved by the aid of high-resource language pairs. Fourth, we can realize zero-shot translation in a direction which has never been seen during training <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Target language biasing</head><p>To perform translations for multiple target languages with a single decoder, we have to specify a target language to translate to. In  <ref type="table">1</ref>: Statistics in each corpus. Each value is calculated after normalizing references and removing short and long utterances. Speed perturbation based data augmentation <ref type="bibr" target="#b31">[32]</ref> is not performed here. †Two translation references are prepared per source speech utterance. <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>, an artificial token to represent the target language (target language ID) is prepended in the source sentence. However, this is not suitable for the ST task since the ST encoder directly consumes speech features. Instead, we replace a start-of-sentence ( sos ) token in the decoder with a target language ID 2lang (see Figure <ref type="figure" target="#fig_0">1</ref>). For example, when English speech is translated to French text, sos is replaced with French ID token 2f r .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Mixed data training</head><p>We train multilingual models with mixed training data from multiple languages. Thus, each mini-batch may contain utterances from different language pairs. We bucket all samples so that each mini-batch contains utterances of speech frames of the similar lengths regardless of language pairs. As a result, we can use the same training scheme as the conventional ASR and bilingual ST tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DATA</head><p>We build our systems on three speech translation corpora: Fisher-CallHome Spanish, Librispeech, and Speech-Translation TED (ST-TED) corpus. To the best of our knowledge, these are the only public available corpora recorded with a reasonable size of real speech data <ref type="foot" target="#foot_4">6</ref> . The data statistics are summarized in Table <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Bilingual translation (A) Fisher-CallHome Spanish: Es→En</head><p>This corpus contains about 170-hours of Spanish conversational telephone speech, the corresponding transcription, and the English translations<ref type="foot" target="#foot_5">7</ref>  <ref type="bibr" target="#b18">[19]</ref>. Following <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">35]</ref>, we report results on the five evaluation sets: dev, dev2, and test in Fisher corpus (with four references), and devtest and evltest in CallHome corpus (with a single reference). We use the Fisher/train as the training set and Fisher/dev as the validation set. All punctuation marks except for apostrophe are removed during evaluation in ST and MT tasks to compare with previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(B) Librispeech: En→Fr</head><p>This corpus is a subset of the original Librispeech corpus <ref type="bibr" target="#b35">[36]</ref> and contains 236-hours of English read speech, the corresponding transcription, and the French translations <ref type="bibr" target="#b19">[20]</ref>. We use the clean 99hours of speech data for the training set <ref type="bibr" target="#b4">[5]</ref>. Translation references in the training set are augmented with Google Translate following <ref type="bibr" target="#b4">[5]</ref>, so we have two French references per utterance. We use the dev set as the validation set and report results on the test set.</p><p>(C) Speech-Translation TED (ST-TED): En→De</p><p>This data contains 271-hours of English lecture speech, the corresponding transcription, as well as the German translation <ref type="foot" target="#foot_6">8</ref> . Since the original training set includes a lot of noisy utterances due to low alignment quality, we take a data cleaning strategy. We first forcealigned all training utterances with a Gentle forced aligner <ref type="foot" target="#foot_7">9</ref> based on Kaldi <ref type="bibr" target="#b36">[37]</ref>, then excluded all utterances where all words in the transcription were not perfectly aligned with the corresponding audio signal <ref type="bibr" target="#b37">[38]</ref>. This process reduced from 171,121 to 137,660 utterances. We sampled two sets of 2k utterances from the cleaned training data as the validation and test sets, respectively (totally 4k utterances). Note that all sets have no text overlap and are disjoint regarding speakers, and data splits are available in our codes. We report results on this test set and tst2013. tst2013 is one of the test sets provided in IWSLT2018 evaluation campaign. Since there are no human-annotated time alignment provided in these test sets, we decided to sample the disjoint test set from the training data with alignment information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Multilingual translation</head><p>We perform experiments in two scenarios: one-to-many (O2M) and many-to-many (M2M) <ref type="foot" target="#foot_8">10</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>One-to-many (O2M)</head><p>For one-to-many (O2M) translation, speech utterances in a source language are translated to multiple target languages. We concatenate Librispeech (En→Fr) and ST-TED (En→De), and build models for En→{Fr, De} translations (see Table <ref type="table">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Many-to-many (M2M)</head><p>For many-to-many (M2M) translation, speech utterances in multiple source languages are translated to all target languages given in training. We can regard this task as a more challenging optimization problem than O2M and M2O translations. We concatenate Librispeech (En→Fr) and Fisher-CallHome Spanish (Es→En), then build models for {En, Es}→{Fr, En} translations (M2Ma) <ref type="foot" target="#foot_9">11</ref> . Other combinations such as Fisher-CallHome Spanish and ST-TED ({En, Es}→{De, En}, M2Mb), and all three directions ({En, Es}→{Fr, De, En}, M2Mc) are also investigated.</p><p>Bi  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTAL EVALUATIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Settings</head><p>For data pre-processing of references in all languages, we lowercased and normalized punctuation, followed by tokenization with the tokenizer.perl script in the Moses toolkit <ref type="foot" target="#foot_10">12</ref> . For source references, we further removed all punctuation marks except for apostrophe. We report case-insensitive BLEU <ref type="bibr" target="#b39">[40]</ref> with the multi-bleu.perl script in Moses. The character vocabulary was created jointly with both source and target languages. We used 80-channel log-mel filterbank coefficients with 3dimensional pitch features, computed with a 25ms window size and shifted every 10 ms using Kaldi <ref type="bibr" target="#b36">[37]</ref>, resulting 83-dimensional features per frame. The features were normalized by the mean and the standard deviation for each training set. We augmented speech data by a factor of 3 by speed perturbation <ref type="bibr" target="#b31">[32]</ref>. We removed utterances having more than 3000 frames or more than 400 characters due to the GPU memory efficiency.</p><p>The speech encoders in ASR and ST models were composed of two VGG blocks <ref type="bibr" target="#b40">[41]</ref> followed by 5-layers of 1024-dimensional (per direction) bidirectional long short-term memory (LSTM) <ref type="bibr" target="#b41">[42]</ref>. Each VGG-like block composed of 2-layers of CNN having a 3 × 3 filter followed by a max-pooling layer with a stride of 2 × 2, which resulted in 4-fold time reduction. The text encoders in MT models were composed of 2-layers of 1024-dimensional (per direction) BLSTM. Both transcription and translation decoders were two layers of unidirectional LSTM with 1024-dimensional memory cells. The dimensions of the attention layer and embeddings for decoders were set to 1024. We used 2-layers of LSTM LM with 1024 memory cells for shallow fusion as discussed in Section 2.1.1.</p><p>Training was performed using Adadelta <ref type="bibr" target="#b42">[43]</ref> for sequence-tosequence models and Adam <ref type="bibr" target="#b43">[44]</ref> for RNNLM. For regularization, we adopted dropout <ref type="bibr" target="#b44">[45]</ref>, label smoothing <ref type="bibr" target="#b45">[46]</ref>, scheduled sampling <ref type="bibr" target="#b46">[47]</ref>, and weight decay. Beam search decoding was performed with a beam width of 20 with CTC and LM scores in the ASR task as shown in Section 2.1.1, and a beam width of 10 with a length penalty in ST and MT tasks. Detailed hyperparameter settings during training and decoding are available in our codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Baseline results: Bilingual systems</head><p>First, we evaluate baseline bilingual MT and ST systems. Bilingual E2E-ST and pipeline-ST models are labeled (E-B-1) and (P-B) in each table, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(A) Fisher-CallHome Spanish: Es→En</head><p>We present our results on Fisher-CallHome Spanish (hereafter, Fisher-CallHome) in Table <ref type="table" target="#tab_1">2</ref>. ASR and NMT results were competitive to the previous work <ref type="bibr" target="#b3">[4]</ref> while the E2E-ST and pipeline-ST models underperformed it. Note that our translation decoders in E2E-ST and NMT models were trained so as to predict lowercased references with punctuation marks to compare with multilingual models, unlike previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">35]</ref>, where all punctuation marks except for apostrophe are removed. For the comparison of our E2E-ST and pipeline-ST models, the baseline bilingual E2E-ST model (E-B-1) outperformed the pipeline-ST model (P-B) in the Fisher sets but underperformed it in the CallHome sets. To investigate this discrepancy, we evaluated them with a single reference in the Fisher tests, which results in 26.4/28.2/27.7 (Pipe-ST) vs. 23.5/25.2/24.8 (E2E-ST) and the pipeline system was shown to be better. This is intuitive since the E2E-ST model skipped the ASR decoder, RNNLM in the source language, and MT encoder parts. In our preliminary experiments, we confirmed the E2E-ST model can outperform the pipeline system by stacking more BLSTM layers on top of the speech encoder to match the number of parameters between them. Moreover, pre-training the speech encoder and translation decoder with the corresponding ASR encoder and NMT decoder also drastically improved the performances (see Table <ref type="table" target="#tab_4">5</ref> in Section 5.3). However, it is worth noting that our goal in this paper is to show the effectiveness of multilingual training for E2E-ST models and therefore we will not seek these directions here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(B) Librispeech: En→Fr</head><p>Next, results on Librispeech are shown in Table <ref type="table" target="#tab_2">3</ref>. Monolingual ASR, bilingual E2E-ST (E-B-1), and pipeline-ST (P-B) models outperformed the previous work <ref type="bibr" target="#b4">[5]</ref>. The baseline bilingual E2E-ST model (E-B-1) showed the competitive performance compared to the pipeline-ST model (P-B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(C) ST-TED: En→De</head><p>Results on ST-TED are shown in Table <ref type="table" target="#tab_3">4</ref>. Contrary to the above results, there is a large gap between the bilingual E2E-ST (E-B-1) and pipeline-ST (P-B) models in this corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Main results: Multilingual systems</head><p>We now test multilingual models trained in two scenarios: many-tomany (M2M) and one-to-many (O2M) translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Many-to-many (M2M)</head><p>Results of M2M models on Fisher-CallHome, Librispeech, ST-TED are shown at the (*-Ma/Mb/Mc-1) lines in Table <ref type="table" target="#tab_1">2</ref>, Table <ref type="table" target="#tab_2">3</ref>, and Table 4, respectively. Ma, Mb, and Mc represent M2Ma, M2Mb, and M2Mc, respectively (see Table <ref type="table">1</ref>).</p><p>In Fisher-CallHome (Table <ref type="table" target="#tab_1">2</ref>), our M2M multilingual E2E-ST models (E-Mb/Mc-1) significantly outperformed the bilingual one (E-B-1) while (E-Ma-1) slightly outperformed (E-B-1) except for Fisher/test. Among three M2M E2E-ST models, (E-Mc-1) showed the best performance, from which we can confirm that additional training data from other language pairs is effective. Multilingual ASR models slightly outperformed the monolingual ASR model. Performances of the MT models were degraded by multilingual training due to the domain mismatch especially for punctuation marks (see Table <ref type="table">1</ref>). In contrast, multilingual E2E-ST models were not affected by the domain mismatch issue since they are not conditioned on the source language text, which is one for the advantages of the end-to-end models.</p><p>In all pipeline systems in Fisher-CallHome, we used the bilingual MT model since it showed the best performance. Pipeline systems with the multilingual ASR (P-M * ) were consistently improved even though WER improvements were very small. Our multilingual E2E-ST models significantly outperformed all the pipeline models in the Fisher sets.</p><p>In Librispeech (Table <ref type="table" target="#tab_2">3</ref>), all M2M E2E-ST models (E-Ma/Mc-1) outperformed the bilingual one (E-B-1). Multilingual ASR models also outperformed the monolingual one. Pipeline systems (P-Ma/Mc) are improved in proportion to the WER improvements. However, E2E-ST models got more gains from multilingual training.</p><p>In ST-TED (Table <ref type="table" target="#tab_3">4</ref>), we also confirmed the consistent BLEU improvements by the proposed multilingual framework. The similar trends can be seen as in Fisher-CallHome and Librispeech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>One-to-many (O2M)</head><p>Results of O2M models on Librispeech and ST-TED are shown in Table <ref type="table" target="#tab_3">3 and Table 4</ref>, respectively. We also obtained significant improvements of the E2E-ST models from multilingual training as well as in the M2M scenario on both corpora. Since the amount of additional training data for O2M and M2Mb from ST-TED is 99-hours (+Librispeech) and 170-hours (+Fisher-CallHome), respectively, and the O2M E2E-ST model is better than the M2Mb E2E-ST model in ST-TED (see Table <ref type="table" target="#tab_3">4</ref>), we can conclude that O2M training is more effective than M2M training in terms of data efficiency. However, the combination of all training data (M2Mc) got a further small gain. We can confirm the effectiveness of O2M training from WER improvements in the ASR task (6.6 vs. 8.6 at the second and third lines from bottom in Table <ref type="table" target="#tab_2">3</ref>). Thus, further additional multilingual training data could lead to the improvement. Gains from multilingual training were larger in the E2E-ST model (E-O-1) than in the best pipeline model (P-O) 13 . Considering the fact that the O2M NMT model underperformed the bilingual one, O2M multilingual training benefits from not only additional English speech data but also the direct optimization, which is one of our motivations in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training with the ASR encoder</head><p>Finally, we show results of pre-training with the ASR encoder in Table <ref type="table" target="#tab_4">5</ref>. We observed improvements by pre-training both in bilingual and multilingual cases, similar to <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b47">48]</ref>. Pre-training with the NMT decoder was not necessarily effective. The best multilingual E2E-ST with pre-training (E-Mc-2) outperformed the corresponding best pipeline system in most test sets.</p><p>In summary, the proposed multilingual framework has shown to be effective regardless of the language combination, corpus domain, and data size. Although it is possible to improve the pipeline systems by carefully designing the source representations between ASR and MT modules (e.g., adding punctuation restoration module), it can be overcome by simply optimizing the direct mapping from source speech to target text with punctuation marks as we have shown. In this section, we evaluate generalization of multilingual ST models by performing transfer learning to a very low-resource ST task. We used Mboshi-French corpus <ref type="foot" target="#foot_11">14</ref>  <ref type="bibr" target="#b21">[22]</ref>, which contains 4.4-hours of spoken utterances and the corresponding Mboshi transcriptions and French translations. Mboshi <ref type="bibr" target="#b48">[49]</ref> is a Bantu C25 language spoken in Congo-Brazzaville and does not have standard orthography. We sampled 100 utterances from the training set as the validation set, and report results on the dev set (514 utterances) as in <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b47">48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>We tried four different ways to transfer a non-Mboshi E2E-ST model to this task. In the bilingual case, we used the bilingual ST model in Librispeech ((E-B-1) in Table <ref type="table" target="#tab_2">3</ref>) as seed, then fine-tuned on the Mboshi-French data. In the multilingual case, we tried seeding with multilingual ST models in M2Ma (E-Ma-1), M2Mc (E-Mc-1), and O2M (E-O-1) settings. All parameters including the output layer are transferred from pre-trained ST models and we do not include any characters in Mboshi transcriptions in the vocabularies. Note that French references appear in the target side of all seed models during the pre-training stage.</p><p>Results are shown in Table <ref type="table" target="#tab_5">6</ref>. Multilingual E2E-ST models are more effective than the bilingual one, and O2M showed the best performance among three models. Although our transferred models underperformed <ref type="bibr" target="#b47">[48]</ref>, it is worth mentioning that they used other English ASR data (Switchboard corpus) and initialized the decoder with the French ASR decoder. Further improvements could be possible by leveraging Mboshi transcriptions, but we did not use any prior knowledge about Mboshi characters. This is a desired scenario for endangered language documentation and quite useful for automatic word discovery <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seed</head><p>Multi </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">RELATED WORK</head><p>End-to-end speech translation</p><p>In <ref type="bibr" target="#b3">[4]</ref>, the E2E-ST model is simultaneously optimized with an auxiliary ASR task by sharing the whole encoder parameters. Pre-training approaches from the ASR encoder <ref type="bibr" target="#b47">[48]</ref> and MT decoder are also investigated in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8]</ref>. <ref type="bibr" target="#b7">[8]</ref> proposed a data augmentation strategy, where weakly-supervised paired data is generated from monolingual source text data with text-to-speech (TTS) and MT systems (similar to back translation <ref type="bibr" target="#b52">[53]</ref>) and speech data with a pipeline ST system (similar to knowledge distillation <ref type="bibr" target="#b52">[53]</ref>). <ref type="bibr" target="#b49">[50]</ref> proposed an efficient framework to better leverage higher-level intermediate representations by jointly attending to speech encoder and transcription decoder states. The most relevant work to ours is <ref type="bibr" target="#b47">[48]</ref>, where well-trained ASR parameters from the other language are used to initialize ST models and improve the ST performance in low-resource scenarios. Our work is distinct in that we focus on exploiting corpora in the multilingual setting and show that it outperforms the bilingual setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multilingual ASR</head><p>In the multilingual ASR study, the language-independent acoustic representations can be obtained by sharing parameters, and then adapted to low-resource languages <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref>. Recently, this approach is extended to end-to-end ASR paradigms: Connectionist Temporal Classification (CTC) <ref type="bibr" target="#b12">[13]</ref>, and attention-based encoderdecoder <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. Our work adopts this multilingual ASR in the pipeline system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multilingual NMT</head><p>Crosslingual parameter sharing approaches are investigated by tying a part of parameters <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58]</ref>, and even all parameters with a shared vocabulary <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> among multiple languages. Since the main drawback of the shared vocabulary is that the size of the vocabulary grows rapidly in proportion to the number of language pairs or the capacity per language shrinks when using BPE units <ref type="bibr" target="#b25">[26]</ref>, fully character-level multilingual framework is proposed to overcome the issue to some extent <ref type="bibr" target="#b58">[59]</ref>. Our work is along with this trend of utilizing a universal translation model in one-to-many and many-to-many ST scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSION AND FUTURE WORK</head><p>We performed multilingual training and end-to-end speech translation jointly, which has not yet been investigated before. We proposed a universal sequence-to-sequence framework and it outperformed the bilingual end-to-end, and the gap between strong pipeline systems became smaller. Its effectiveness was also confirmed by performing transfer learning to a very low-resource speech translation task. To encourage further research in this topic, we will place our codes to the public project. In future work, we will support more languages <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b33">34]</ref> on our codebase and investigate multilingual training with non-related languages such as Chinese and Japanese.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: System overview for the multilingual end-to-end speech translation model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table</head><label></label><figDesc></figDesc><table><row><cell>Translation</cell><cell>Corpus</cell><cell>#hours</cell><cell>#utterances</cell><cell>#words</cell><cell>#vocab</cell><cell>domain</cell></row><row><cell></cell><cell>(A) Fisher-CallHome Spanish (Es→En)</cell><cell>170</cell><cell>138 k</cell><cell>1.7 M</cell><cell>66</cell><cell>conversation</cell></row><row><cell>Bilingual</cell><cell>(B) Librispeech (En→Fr)</cell><cell>99</cell><cell>45 k †</cell><cell>0.8 M</cell><cell>112</cell><cell>reading</cell></row><row><cell></cell><cell>(C) ST-TED (En→De)</cell><cell>203</cell><cell>133 k</cell><cell>2.2 M</cell><cell>109</cell><cell>lecture</cell></row><row><cell>One-to-many (O2M)</cell><cell>(B) + (C) (En→{Fr, De})</cell><cell>302</cell><cell>178 k</cell><cell>3.3 M</cell><cell>153</cell><cell>mixed</cell></row><row><cell>Many-to-many (M2Ma)</cell><cell>(A) + (B) ({En, Es}→{Fr, En})</cell><cell>269</cell><cell>183 k</cell><cell>2.8 M</cell><cell>121</cell><cell>mixed</cell></row><row><cell>Many-to-many (M2Mb)</cell><cell>(A) + (C) ({En, Es}→{De, En})</cell><cell>373</cell><cell>272 k</cell><cell>4.0 M</cell><cell>119</cell><cell>mixed</cell></row><row><cell>Many-to-many (M2Mc)</cell><cell>(A) + (B) + (C) ({En, Es}→{Fr, De, En})</cell><cell>472</cell><cell>317 k</cell><cell>5.1 M</cell><cell>157</cell><cell>mixed</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>- * : Bilingual, Mono- * : Monolingual, Multi- * : Multilingual Results of MT, ST, and ASR systems on Fisher-CallHome Span-</figDesc><table><row><cell></cell><cell>Model</cell><cell>Multi-lingual</cell><cell>dev</cell><cell>Fisher dev2</cell><cell cols="3">CallHome devtest evltest BLEU (↑) test</cell><cell></cell><cell>Model Bi-NMT [5]</cell><cell cols="2">Multi-lingual BLEU (↑) -19.2</cell></row><row><cell></cell><cell>Bi-SMT [35]</cell><cell>-</cell><cell>-</cell><cell>65.4</cell><cell>62.9</cell><cell>-</cell><cell>-</cell><cell></cell><cell>Google Translate [5]</cell><cell>-</cell><cell>22.2</cell></row><row><cell>MT E2E ST</cell><cell>Bi-NMT [4] Bi-NMT [39] Bi-NMT Multi-NMT Multi-NMT Multi-NMT Bi-ST [4] + ASR task [4] (E-B-1) Bi-ST (E-Ma-1) Multi-ST (E-Mb-1) Multi-ST (E-Mc-1) Multi-ST</cell><cell>---M2Ma M2Mb M2Mc ---M2Ma M2Mb M2Mc</cell><cell>58.7 61.9 60.6 50.2 57.4 56.7 46.5 48.3 40.4 41.1 43.5 44.1</cell><cell>59.9 62.8 62.0 50.6 58.3 57.5 47.3 49.1 41.4 41.7 44.5 45.4</cell><cell>57.9 60.4 59.6 49.5 56.7 56.2 47.3 48.7 41.5 41.3 44.2 45.2</cell><cell>28.2 -29.4 22.8 27.9 27.8 16.4 16.8 14.1 15.1 15.3 16.4</cell><cell>27.9 -28.9 22.8 27.7 27.7 16.6 17.4 14.2 15.2 15.8 16.2</cell><cell>MT E2E ST</cell><cell>Bi-NMT Multi-NMT Multi-NMT Multi-NMT Bi-ST [5] + Pre-training + MTL [5] Bi-ST + KD [7] (E-B-1) Bi-ST (E-O-1) Multi-ST (E-Ma-1) Multi-ST (E-Mc-1) Multi-ST</cell><cell>-O2M M2Ma M2Mc ----O2M M2Ma M2Mc</cell><cell>18.3 16.2 12.2 14.8 12.9 13.4 17.0 15.7 17.2 16.4 17.3</cell></row><row><cell></cell><cell>Mono-ASR/Bi-SMT [35]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>40.4</cell><cell>-</cell><cell>-</cell><cell></cell><cell>Mono-ASR/Bi-NMT [5]</cell><cell>-</cell><cell>14.6</cell></row><row><cell>Pipe ST</cell><cell>Mono-ASR/Bi-NMT [4] (P-B) Mono-ASR/Bi-NMT (P-Ma) Multi-ASR/Bi-NMT</cell><cell>--M2Ma</cell><cell>45.1 37.3 37.9</cell><cell>46.1 39.6 40.3</cell><cell>45.5 38.6 39.2</cell><cell>16.2 16.8 17.6</cell><cell>16.6 16.5 17.2</cell><cell>Pipe ST</cell><cell>(P-B) Mono-ASR/Bi-NMT (P-O) Mono-ASR †/Bi-NMT (P-Ma) Muti-ASR/Bi-NMT</cell><cell>-O2M M2Ma</cell><cell>15.8 16.7 16.4</cell></row><row><cell></cell><cell>(P-Mb) Multi-ASR/Bi-NMT</cell><cell>M2Mb</cell><cell>37.6</cell><cell>39.6</cell><cell>38.9</cell><cell>17.0</cell><cell>17.0</cell><cell></cell><cell>(P-Mc) Muti-ASR/Bi-NMT</cell><cell>M2Mc</cell><cell>16.7</cell></row><row><cell></cell><cell>(P-Mc) Multi-ASR/Bi-NMT</cell><cell>M2Mc</cell><cell>37.6</cell><cell>39.7</cell><cell>38.5</cell><cell>17.0</cell><cell>16.9</cell><cell></cell><cell>Model</cell><cell></cell><cell>WER (↓)</cell></row><row><cell></cell><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell>WER (↓)</cell><cell></cell><cell></cell><cell></cell><cell>Mono-ASR [5]</cell><cell>-</cell><cell>17.9</cell></row><row><cell>ASR</cell><cell>Mono-ASR [4] Mono-ASR (Es) Multi-ASR (Es, En) Multi-ASR (Es, En) Multi-ASR (Es, En)</cell><cell>--M2Ma M2Mb M2Mc</cell><cell>25.7 26.0 25.6 25.9 26.0</cell><cell>25.1 25.6 25.0 25.2 25.4</cell><cell>23.2 23.6 22.9 23.3 23.6</cell><cell>44.5 45.4 43.5 44.2 44.5</cell><cell>45.3 45.9 44.5 44.7 44.2</cell><cell>ASR</cell><cell>Mono-ASR (En) Mono-ASR † (En) Multi-ASR (En, Es) Multi-ASR (En, Es)</cell><cell>-O2M M2Ma M2Mc</cell><cell>9.0 6.6 8.6 6.8</cell></row></table><note>ish (Es→En). (E-B-1): Bilingual E2E-ST. (E-Ma/Mb/Mc-1): manyto-many (M2M) E2E-ST. (P-B): Bilingual pipeline-ST. (P-Ma/Mb/Mc): M2M pipeline-ST.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results of MT, ST, and ASR systems on Lib-</figDesc><table /><note>rispeech (En→Fr). †Training data is augmented with ST-TED. (E-O-1): Proposed one-to-many (O2M) E2E-ST. (P-O): O2M pipeline-ST.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results of MT, ST, and ASR systems on ST-TED (En→De). †Training data is augmented with Librispeech.</figDesc><table><row><cell></cell><cell>Model</cell><cell>Multi-lingual</cell><cell cols="2">test BLEU (↑) tst2013</cell></row><row><cell></cell><cell>Bi-NMT</cell><cell>-</cell><cell>23.0</cell><cell>24.9</cell></row><row><cell>MT</cell><cell>Multi-NMT Multi-NMT</cell><cell cols="2">O2M M2Mb 17.5 18.9</cell><cell>20.3 18.7</cell></row><row><cell></cell><cell>Multi-NMT</cell><cell cols="2">M2Mc 17.2</cell><cell>18.0</cell></row><row><cell></cell><cell>(E-B-1) Bi-ST</cell><cell>-</cell><cell>16.0</cell><cell>12.5</cell></row><row><cell>E2E</cell><cell>(E-O-1) Multi-ST</cell><cell>O2M</cell><cell>17.6</cell><cell>14.4</cell></row><row><cell>ST</cell><cell>(E-Mb-1) Multi-ST</cell><cell cols="2">M2Mb 16.7</cell><cell>12.9</cell></row><row><cell></cell><cell>(E-Mc-1) Multi-ST</cell><cell cols="2">M2Mc 17.7</cell><cell>14.8</cell></row><row><cell></cell><cell>(P-B) Mono-ASR/Bi-NMT</cell><cell>-</cell><cell>18.1</cell><cell>13.1</cell></row><row><cell>Pipe</cell><cell>(P-O) Mono-ASR †/Bi-NMT</cell><cell>O2M</cell><cell>18.5</cell><cell>14.0</cell></row><row><cell>ST</cell><cell cols="3">(P-Mb) Multi-ASR †/Bi-NMT M2Mb 17.7</cell><cell>12.6</cell></row><row><cell></cell><cell cols="3">(P-Mc) Multi-ASR †/Bi-NMT M2Mc 18.1</cell><cell>13.3</cell></row><row><cell></cell><cell>Model</cell><cell></cell><cell cols="2">WER (↓)</cell></row><row><cell></cell><cell>Mono-ASR (En)</cell><cell>-</cell><cell>20.3</cell><cell>36.6</cell></row><row><cell>ASR</cell><cell>Mono-ASR † (En) Multi-ASR (En, Es)</cell><cell cols="2">O2M M2Mb 20.5 19.0</cell><cell>33.9 38.7</cell></row><row><cell></cell><cell>Multi-ASR (En, Es)</cell><cell cols="2">M2Mc 20.1</cell><cell>36.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results of the end-to-end ST systems with pre-training</figDesc><table><row><cell></cell><cell></cell><cell>Multi-lingual</cell><cell>dev</cell><cell cols="3">BLEU (↑) devtest evltest Fisher-CallHome dev2 test</cell><cell>Librispeech test</cell><cell cols="2">ST-TED test tst2013</cell></row><row><cell></cell><cell>(E-B-1) Bi-ST</cell><cell>-</cell><cell cols="2">40.4 41.4 41.5</cell><cell>14.1</cell><cell>14.2</cell><cell>15.7</cell><cell>16.0</cell><cell>12.5</cell></row><row><cell></cell><cell>(E-B-2) + ASR-PT</cell><cell>-</cell><cell cols="2">43.5 45.1 44.7</cell><cell>15.6</cell><cell>16.4</cell><cell>16.3</cell><cell>17.1</cell><cell>13.1</cell></row><row><cell>E2E-ST</cell><cell>(E-B-3) + MT-PT</cell><cell>-</cell><cell cols="2">44.4 45.1 45.2</cell><cell>15.6</cell><cell>15.4</cell><cell>16.8</cell><cell>17.4</cell><cell>13.5</cell></row><row><cell></cell><cell>(E-Mc-1) Multi-ST</cell><cell cols="3">M2Mc 44.1 45.4 45.2</cell><cell>16.4</cell><cell>16.2</cell><cell>17.3</cell><cell>17.7</cell><cell>14.8</cell></row><row><cell></cell><cell cols="4">(E-Mc-2) + ASR-PT M2Mc 46.3 47.1 46.3</cell><cell>17.3</cell><cell>17.2</cell><cell>17.6</cell><cell>18.6</cell><cell>14.6</cell></row><row><cell cols="2">Pipe-ST Best system</cell><cell>-</cell><cell cols="2">37.9 40.3 39.2</cell><cell>17.6</cell><cell>17.2</cell><cell>16.7</cell><cell>18.5</cell><cell>14.0</cell></row><row><cell cols="3">6. TRANSFER LEARNING FOR A VERY</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">LOW-RESOURCE LANGUAGE SPEECH TRANSLATION</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Results</figDesc><table><row><cell>Encoder</cell><cell>Decoder</cell><cell cols="2">-lingual BLEU</cell></row><row><cell cols="2">En300h-ASR French20h-ASR [48]</cell><cell>-</cell><cell>7.1</cell></row><row><cell>Libri-ST</cell><cell>Libri-ST</cell><cell>-</cell><cell>4.55</cell></row><row><cell>O2M-ST</cell><cell>O2M-ST</cell><cell></cell><cell>6.92</cell></row><row><cell>M2Ma-ST</cell><cell>M2Ma-ST</cell><cell></cell><cell>5.50</cell></row><row><cell>M2Mc-ST</cell><cell>M2Mc-ST</cell><cell></cell><cell>6.52</cell></row><row><cell></cell><cell cols="3">of E2E-ST systems transferred from pre-trained</cell></row><row><cell cols="4">E2E-ST models on a very low-resource corpus (Mboshi→Fr, 4.4</cell></row><row><cell cols="4">hours). The former and latter part of hyphen represents data and</cell></row><row><cell cols="4">task for pre-training, respectively (data-task). Note that all mod-</cell></row><row><cell cols="4">els do not use any transcriptions in Mboshi during pre-training nor</cell></row><row><cell>adaptation stage.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">In case of one-to-many situation, this does not occur since only the monolingual ASR is required. However, error propagation from the ASR module and latency between the ASR and MT modules is still problematic.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">We leave to investigate Transformer architectures<ref type="bibr" target="#b22">[23]</ref> for future work. However, our framework is model agnostic and can be applied to any sequence-to-sequence models.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">Although we also conducted experiments with byte-pair-encoding (BPE)<ref type="bibr" target="#b25">[26]</ref>, the character unit is better than BPE in all settings due to the data sparseness issue. Therefore, we only report results on the character-level unit.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">In this paper, we use lowercased references. Therefore, we do not consider truecasing as text normalization.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4">We noticed publicly available one-to-many multilingual ST corpus<ref type="bibr" target="#b33">[34]</ref> right before submission. However, this dataset has English speech only.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5">https://github.com/joshua-decoder/ Fisher-CallHome-corpus</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6">https://sites.google.com/site/ iwsltevaluation2018/Lectures-task</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7">https://github.com/lowerquality/gentle</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8">For many-to-one (M2O) scenario, none of the corpora combinations exists in publicly available corpora, therefore we leave the exploration of this task for future work. However, O2M and M2M are the realistic scenarios for multilingual speech translation as mentioned in Section 1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9"><ref type="bibr" target="#b10">11</ref> Readers might think that this scenario is not suitable for the M2M evaluation since French does not appear in source side as in the multilingual MT task<ref type="bibr" target="#b15">[16]</ref>. However, such public corpora are not currently available.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_10">https://github.com/moses-smt/mosesdecoder</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_11">https://github.com/besacier/ mboshi-french-parallel-corpus</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Speech translation: Coupling of recognition and translation</title>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="517" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">NMT-based segmentation and punctuation insertion for real-time spoken language translation</title>
		<author>
			<persName><forename type="first">Eunah</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2645" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Listen and translate: A proof of concept for end-to-end speech-to-text translation</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Bérard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Servan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Besacier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01744</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence models can directly translate foreign speech</title>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2625" to="2629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end automatic speech translation of audiobooks</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Bérard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><forename type="middle">Can</forename><surname>Kocabiyikoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6224" to="6228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploring the use of acoustic embeddings in neural machine translation</title>
		<author>
			<persName><forename type="first">Salil</forename><surname>Deena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranava</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Madhyastha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName><surname>Hain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="450" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">End-to-end speech translation with knowledge distillation</title>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08075</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Leveraging weakly supervised data to improve end-to-end speech-totext translation</title>
		<author>
			<persName><forename type="first">Ye</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Laurenzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Wit3: Web inventory of transcribed and translated talks</title>
		<author>
			<persName><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Girardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of European Association for Machine Translation</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="261" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Kit lecture translator: Multilingual speech translation with one-shot learning</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Dessloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thanh-Le</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Son</forename><surname>Thai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ngoc-Quan</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Salesky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Sperber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName><surname>Zenkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations</title>
				<meeting>the 27th International Conference on Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="89" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Language independent end-to-end architecture for joint language identification and speech recognition</title>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="265" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multilingual speech recognition with a single end-to-end model</title>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4904" to="4908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sequence-based multi-lingual low resource speech recognition</title>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Dalmia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramon</forename><surname>Sanabria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4909" to="4913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multilingual sequence-to-sequence speech recognition: architecture, transfer learning, and language modeling</title>
		<author>
			<persName><forename type="first">Jaejin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murali</forename><surname>Karthick Baskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruizhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harish</forename><surname>Sri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Mallidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Yalta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takaaki</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><surname>Hori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SLT</title>
				<meeting>SLT</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="512" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Transfer learning of language-independent end-to-end asr with language model fusion</title>
		<author>
			<persName><forename type="first">Hirofumi</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaejin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murali</forename><surname>Karthick Baskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsuya</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6096" to="6100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Google&apos;s multilingual neural machine translation system: enabling zero-shot translation</title>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernanda</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><surname>Corrado</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Transactions of the Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Toward multilingual neural machine translation with universal encoder and decoder</title>
		<author>
			<persName><forename type="first">Thanh-Le</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IWSLT</title>
				<meeting>IWSLT</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Effective strategies in zero-shot neural machine translation</title>
		<author>
			<persName><forename type="first">Thanh-Le</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IWSLT</title>
				<meeting>IWSLT</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved speech-to-text translation with the Fisher and Callhome Spanish-English speech translation corpus</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damianos</forename><surname>Karakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IWSLT</title>
				<meeting>IWSLT</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Augmenting Librispeech with French translations: A multimodal corpus for direct speech translation evaluation</title>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Ali Can Kocabiyikoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName><surname>Kraif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
				<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The iwslt 2018 evaluation campaign</title>
		<author>
			<persName><forename type="first">Niehues</forename><surname>Jan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roldano</forename><surname>Cattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stüker</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IWSLT</title>
				<meeting>IWSLT</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A very low resource language speech corpus for computational language documentation experiments</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Adda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martine</forename><surname>Adda-Decker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Benjumea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamison</forename><surname>Cooper-Leavitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy-Noël</forename><surname>Kouarata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lori</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hélène</forename><surname>Maynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Müller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03501</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Processings of NIPS</title>
				<meeting>essings of NIPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Joint CTC/attention decoding for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="518" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hybrid CTC/attention architecture for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyoun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoki</forename><surname>Hayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1240" to="1253" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">K</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Černockỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Audio augmentation for speech recognition</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijayaditya</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3586" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-way, multilingual neural machine translation with a shared attention mechanism</title>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
				<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="866" to="875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">MuST-C: a multilingual speech translation corpus</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Mattia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roldano</forename><surname>Gangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Cattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName><surname>Turchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL-HLT</title>
				<meeting>the NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2012" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Some insights from translating conversational telephone speech</title>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3231" to="3235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Librispeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP. IEEE</title>
				<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnab</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nagendra</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirko</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanmin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fine-tuning on clean data for end-to-end speech translation: FBK@IWSLT 2018</title>
		<author>
			<persName><forename type="first">Mattia</forename><surname>Antonino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Gangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Dessì</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roldano</forename><surname>Cattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IWSLT</title>
				<meeting>IWSLT</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="147" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards fluent translations from disfluent speech</title>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Salesky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susanne</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SLT. IEEE</title>
				<meeting>SLT. IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="921" to="926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Adadelta: an adaptive learning rate method</title>
		<author>
			<persName><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Recurrent neural network regularization</title>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pre-training on high-resource speech recognition improves low-resource speech-to-text translation</title>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herman</forename><surname>Kamper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
				<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="58" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Annie</forename><surname>Rialland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martine</forename><surname>Adda-Decker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy-Noël</forename><surname>Kouarata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Adda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lori</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elodie</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamison</forename><surname>Cooper-Leavitt</surname></persName>
		</author>
		<title level="m">Parallel corpora in Mboshi (Bantu C25</title>
				<meeting><address><addrLine>Congo-Brazzaville</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4272" to="4276" />
		</imprint>
	</monogr>
	<note>Proceedings of LREC</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Tied multitask learning for neural speech translation</title>
		<author>
			<persName><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
				<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="82" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unwritten languages demand attention too! word discovery with encoder-decoder models</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Marcely Zanon Boito</surname></persName>
		</author>
		<author>
			<persName><surname>Bérard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ASRU. IEEE</title>
				<meeting>ASRU. IEEE</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="458" to="465" />
		</imprint>
	</monogr>
	<note>Aline Villavicencio, and Laurent Besacier</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Unsupervised word segmentation from speech with attention</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcely</forename><surname>Zanon-Boito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Ondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Berard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aline</forename><surname>Franc ¸ois Yvon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Villavicencio</surname></persName>
		</author>
		<author>
			<persName><surname>Besacier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2678" to="2782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The language-independent bottleneck features</title>
		<author>
			<persName><forename type="first">Karel</forename><surname>Veselỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName><forename type="first">František</forename><surname>Grézl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miloš</forename><surname>Janda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekaterina</forename><surname>Egorova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SLT</title>
				<meeting>SLT</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="336" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multilingual deep neural network based acoustic modeling for rapid language adaptation</title>
		<author>
			<persName><forename type="first">Ngoc Thang</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Imseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanja</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Bourlard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of ICASSP</title>
				<meeting>eeding of ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="7639" to="7643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Analysis of multilingual blstm acoustic model on low and high resource languages</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murali</forename><surname>Karthick Baskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karel</forename><surname>Veselỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">František</forename><surname>Grézl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of ICASSP</title>
				<meeting>eeding of ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5789" to="5793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multi-task learning for multiple language translation</title>
		<author>
			<persName><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1723" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A neural interlingua for multilingual machine translation</title>
		<author>
			<persName><forename type="first">Yichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Keung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Ladhak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaonan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation (WMT)</title>
				<meeting>the Third Conference on Machine Translation (WMT)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="84" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Fully characterlevel neural machine translation without explicit segmentation</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Transactions of the Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
