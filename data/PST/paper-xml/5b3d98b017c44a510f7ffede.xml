<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Intelligent video surveillance for real-time detection of suicide attempts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Wassim</forename><surname>Bouachir</surname></persName>
							<email>wassim.bouachir@teluq.ca</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LICEF research center</orgName>
								<orgName type="institution" key="instit2">T ÉLUQ University Montréal (QC)</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rafik</forename><surname>Gouiaa</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">École de technologie supérieure</orgName>
								<address>
									<settlement>Montréal</settlement>
									<region>QC)</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">École de technologie supérieure</orgName>
								<address>
									<settlement>Montréal</settlement>
									<region>QC)</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rita</forename><surname>Noumeir</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">École de technologie supérieure</orgName>
								<address>
									<settlement>Montréal</settlement>
									<region>QC)</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">LICEF research center</orgName>
								<orgName type="institution">T ÉLUQ University Montréal (QC)</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Intelligent video surveillance for real-time detection of suicide attempts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1016/j.patrec.2018.03.018</idno>
					<note type="submission">Received date: 3 October 2017 Revised date: 25 January 2018 Accepted date: 18 March 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Pattern Recognition Letters Pattern Recognition Letters</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Suicide by hanging is a sentinel event and a major cause of death in prisons, with an increasing frequency over recent years. The rapid detection of suicidal behavior can reduce the mortality rate and increase the odds of survival for the suicide victim. Significant efforts have been made to develop technologies for preventing hanging attempts, but most of them use cumbersome devices, or they are mainly depending on human attention and intervention. In this paper, we propose a vision-based method to automatically detect suicide by hanging. Our intelligent video surveillance system operates using depth streams provided by an RGB-D camera, regardless of illumination conditions. The proposed algorithm is based on the exploitation of the body joints'positions to model suicidal behavior. Both dynamic and static pose characteristics are calculated in order to efficiently capture the body joints'movement and model usicidal behavior. Results from the experiments on realistic video sequences, show that our system achieves a high accuracy in detecting suicide attempts, while meeting real-time requirements.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T 1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Highlights</head><p>• A real-time intelligent system for detecting suicide by hanging in prisons is introduced.</p><p>• A challenging dataset including realistic videos simulating suicidal behavior is created.</p><p>• An efficient algorithm is used to rebalance the dataset by oversampling the minority class.</p><p>• Numeros experiements have been done using several classifiers.</p><p>• A high detection accuracy has been obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Suicide is a major cause of premature and preventable death within the correctional settings. For instance, inmate suicide was reported as the leading cause of death in Canadian prisons between 1994 and 2014 (Office of the Correctional Investigator of Canada, 2014). In addition, about one-third of prison deaths were caused by suicide in the USA between 2000 and 2010 <ref type="bibr" target="#b10">(Noonan and Ginder, 2013)</ref>. Generally, hanging is the most common method of inmate suicide, where inmates use clothes, cord or bed linen to hang themselves. Suicide attempts tend to occur when the victims are being held in isolation or segregation cells, and during the night or weekend when the security staff is the lowest (World Health Organization).</p><p>To deal with the phenomenon of suicide by hanging in prisons, correctional officers tend to establish, in the first place, a suicide-safe environment. This can be a cell where hanging points and unsupervised access to lethal materials are eliminated or minimized. As the technology is developing rapidly, video surveillance systems (e.g CCTV) have been used, for a long period, as an alternative to the direct observation by correctional staff. However, camera blind spot together with busy camera operators can seriously limit the performance of such systems. Tragically, numerous examples of suicides continue to occur in full view of camera equipment, which brings up the question of the effectiveness of such vision-based systems for preventing suicide by hanging attempts in jails and prisons (Office of the Correctional Investigator of Canada, 2014). Numerous other solutions have been established for the purpose of detecting and preventing suicide by hanging attempts. For instance, a special protective clothing (Safety smocks and blankets) has been designed to be worn by actively suicidal inmates <ref type="bibr" target="#b7">(Hayes, 2013)</ref>. A top door alarm <ref type="bibr" target="#b4">(Cook, 2011)</ref>, which triggers an alarm if the door is used as a ligature point, allowing for a life-saving proactive emergency response. Moreover, the 'bracelet for life <ref type="bibr">' (World Health Organization, 2007)</ref> has been used for monitoring several physiological parameters. If prisoners vital signs are detected as being outside the normal range, an alarm is triggered and an emergency response is activated. Despite their effectiveness in detecting a number of cases of suicide by hanging, establishing such systems in practice is very challenging. In fact, most of these systems require either wearing cumbersome equipments or they are a source of false alarm even if the inmate simply removes the equipment</p><formula xml:id="formula_0">A C C E P T E D M A N U S C R I P T (bracelet, clothes etc).</formula><p>With the increasing development of new technologies for human action recognition, a few solutions have been proposed in the aim of improving the existing video-surveillance systems to automatically detect suicidal behaviors and trigger an alarm. In this sense, <ref type="bibr" target="#b9">(Lee et al., 2014)</ref> presented a method for automatically analyzing depth images captured by an Asus Xtion Pro camera, and detect suicidal behavior. However, this is a preliminary work, where only the case of a partial suspension hanging was considered, without dealing with real-world difficulties such as occlusion and scale invariance. Besides, only a few video sequences with a short duration of 3 seconds each one, have been used to evaluate the proposed algorithm. Lately, we presented an intelligent video-based system for automated detection of suicide by hanging attempts <ref type="bibr" target="#b1">(Bouachir and Noumeir, 2016)</ref>. Unlike in <ref type="bibr" target="#b9">(Lee et al., 2014)</ref>, we performed our experiments on a large dataset captured by an RGB-D camera, where 21 persons are asked to simulate different scenarios for unsuspected behavior and suicide attempts. Furthermore, video sequences have been captured, where challenging real-world conditions such as occlusions, illumination changes, and scale changes have been considered. Our system is also able to operate day and night without bothering the prisoners, owing to the invisible illumination used by the Kinect camera. Despite the promising results given by our algorithm, it faced difficulties for recognizing some daily living activities such as wearing or removing clothes, which involve movements similar to the movement of the body during suicide by hanging attempts. In this paper, we aim to extend our previous work in the following directions:</p><p>• A real-time scaling algorithm is proposed to deal with the effect of morphological difference within candidates and keep the features invariant with respect to people.</p><p>• A feature selection approach is applied in order to speed up our algorithm for real-time application and improve its generalization capacity.</p><p>• We analyze the performance of numerous classifiers for detecting suicide attempts presented in realistic videos.</p><p>• We propose to oversample the minority class to overcome the problem of the unbalanced dataset.</p><p>• We perform a more complete performance evaluation using 2 strategies: a frame-based evaluation and a sequencebased evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>In the past few decades, human action recognition has drawn much importance in the field of video analysis technology owing to the increasing demand from a wide range of applications (e.g human computer interaction, ambient assisted living and video surveillance). In the context of video surveillance applications, the automatic detection of abnormal behaviors can be used to alert the security staff of a potential danger such as reporting an inmate committing a suicide by hanging. Generally, a camera-based action recognition system operates on two main steps: 1) feature extraction which consists in building visual cues intended to be informative and non-redundant with respect to the corresponding activities, and 2) action learning and classification which based on training classifiers using the extracted features, and using them for classifying observation. According to the type of the extracted features: human action recognition methods can be categorized into 3 groups <ref type="bibr">(Weinland et al., 2011)</ref> as follows: The first group used global approaches that consist on detecting the whole human body without caring about identifying and labeling the individual body parts. Numerous type of features have been used such as silhouette <ref type="bibr" target="#b6">(Gouiaa and Meunier, 2015)</ref>, contour <ref type="bibr" target="#b3">(Cheema et al., 2011)</ref> or optical flow <ref type="bibr" target="#b5">(Fathi and Mori, 2008)</ref>. Most proposed methods in this category operate on the whole human body and obey to a global representation, which is not flexible enough to capture intra-class variations of activities.</p><p>Unlike the first category, methods in the second category mainly focus on designing a local representation of human actions instead, where the image/video are divided into small patches, regardless the body parts labeling, illumination changes or the body localization. These small patches catch the regions of high variations in time and spatial domain and involve appearance and/or motion information. For instance, the space-time interest descriptors (Yang and Tian, 2014) were proposed to generalize the concept of interest points and local descriptors <ref type="bibr" target="#b0">(Bay et al., 2006)</ref>. Despite their effectiveness to overcome some global representation limitations, including noise and partial occlusion, such methods still only contain limited spatiotemporal information, which is insufficient for representing complex activities.</p><p>The methods in the third group are based on pose estimation approaches to represent a human action as a sequence of poses over time. This modeling is often done by exploiting the spatial configuration of human body structure. This representation is derived from the principle, published in <ref type="bibr" target="#b8">(Johansson, 1973)</ref>, explaining how humans observe actions. This work demonstrates that humans are able to identify an action from a few poses only. Several methods have been proposed in this context to tackle the problem of human action recognition using human body joints localization. For instance, <ref type="bibr" target="#b12">(Parisi and Wermter, 2013;</ref><ref type="bibr" target="#b11">Parisi et al., 2015)</ref> proposed methods for human action recognition based on the clustering of pose and motion features with self-organized maps. The reader can find further details in the survey <ref type="bibr">(Sarafianos et al., 2016)</ref>. However, pose-based human action recognition can be difficult due to the complexity of getting high quality poses from RGB videos in a real-world scenario, except in controlled environments (e.g static and calibrated cameras and simple backgrounds). Pose-based activity recognition is still a challenging research area, demanding significant improvements to deal with numerous limitations.</p><p>In this paper, we are interested in methods of the third category, since they are more suitable to represent complex human behaviors. To deal with the limitations of such systems, we propose to use images captured by a depth camera, providing the 3D spatial information in addition to the RGB images. The proposed method is detailed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>The relevant cues provided by the depth information to localize the body joints can help dealing with some of the previously described limitations and open a new line of work to tackle the human activity recognition problem. The proposed method relies on using the spatial configuration of body joints in the 3D space to compute pose and motion features. To detect suicidal behavior, the extracted features corresponding to the current observation are fed to a binary classifier. A suicide by hanging attempt is detected if the percentage of positive observations exceeds a certain threshold during a sliding temporal window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pose representation and analysis</head><p>The human body is modeled as an articulated structure of rigid segments connected by joints. Thus, the action of interest can be seen as the temporal evolution of the joints'spatial configuration. With the emergence of depth cameras, the provided 3D depth data largely facilitates the task of human body parsing. Moreover, it has helped to develop a rather powerful human motion capture technique <ref type="bibr">(Shotton et al., 2013</ref>) that outputs, in real time, the 3D joints'positions of the human skeleton. In our algorithm, this method is applied on the depth images to obtain the 3D location of joints'human body.</p><p>We experimentally verified that the lower body joints are not relevant for our joint pairwise distance features. Indeed, the pairwise distances between the lower body joints remain almost constant, as the spatial configuration of the lower body parts does not undergo any significant changes during the activity of interest. We therefore propose to track only the upper body parts. Thus, we consider a subset of N = 16 upper body joints (See Fig. <ref type="figure" target="#fig_0">1</ref>). In each frame t, the 3D joint coordinates are noted as:</p><formula xml:id="formula_1">X t = {J i t = (x i , y i , z i )|i = 1 • • • N} (1)</formula><p>Where (x i , y i , z i ) are the 3D coordinates of the ith joint J at time t which is noted as J i t . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pose and motion features</head><p>Usually, inmates commit a suicide by hanging by placing a strangling object around the neck <ref type="bibr">(World Health Organization, 2007)</ref>. This typically requires moving the hands to the neck from top to bottom, implying the movement of several upper body joints. Based on this, we model the upper body configuration change using the relative distances between joints. The 3D joints are used to calculate two different features:</p><formula xml:id="formula_2">• P t = {dist(J i t , J j t )|i, j = 1 • • • N; i j},</formula><p>which is pairwise disjoint distances between the list of joints, used to describe the pose at frame t.</p><formula xml:id="formula_3">• M t = {dist(J i t−1 , J j t )|i, j = 1 • • • N},</formula><p>which is pairwise distances between the list of joints in frame t and frame (t-1), used to describe the elementary motion performed between 2 subsequent frames.</p><p>For each frame t, we combine the pose features vector P t and the motion feature vector M t having respectively C 2 N = 120 and N 2 = 256 components to form a single 376-dimensional vector F t = (P t , M t ). Note that the feature vector F t is invariant to rotation, since we perform pairwise comparisons instead of directly using joint positions (in the 3D camera coordinate system). To ensure scale invariance, we normalize each feature vector F t by a scaling parameter, estimated as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Scaling parameter estimation</head><p>Scale invariance is considered as an important criterion for designing robust feature descriptors. In our case, the pairwise distances between the 3D joints within one frame or across a sequence of neighboring frames are largely variant with respect to the monitored person. For example, the distance between shoulder and elbow of people with large body size is greater than that of people with smaller body size. To deal with this difficulty, we propose to normalize the computed feature vector in order to remove the influence of body size. The normalization is achieved by dividing each feature vector by the corresponding distance between the neck and the spine middle. Our choice is explained by 3 reasons:</p><p>• The distance between these two joints is proportionate to the person's height,</p><p>• For one specific person, the distance between the neck and the spine middle is stable and does not dramatically changed with respect to the current pose,</p><p>• The neck and middle spine joints are always visible for a Kinect camera. As shown in Fig. <ref type="figure">2</ref>, we see that three joints, which are head, neck and spine middle are always observable. This gives us a heuristic basis for choosing the distance between the neck and the spine middle as a scaling parameter noted s.</p><p>Note that the scaling parameter can be simply calculated from one frame. However, using a sequence of frames, we propose to estimate s by using the median value of the list of distances generated across the sequence. The main reason for choosing The scaling parameter is the value of the root of min heap 8: end if 9: if Either min or max heap is with full size then 10:</p><formula xml:id="formula_4">A C C E P T E D M A N U S C R I P T</formula><p>Abandon the last half elements in both min and max heap 11: end if the median value is that it is robust to outliers. For this, we implement an algorithm based on Min-Max heaps for estimating the median value in a constant time (see algorithm 1).</p><p>In this algorithm, one min-heap and one max-heap are used to dynamically and adaptively estimate the median value. Both heaps are maintained to make their size difference no larger than one. In this case, the median value is trivially given as follows:</p><p>• If both heaps have the same size, the median value is the averaged of the two roots.</p><p>• Otherwise, the median value is the root value of the heap with a larger size</p><p>In addition, if both heaps are full, we abandon the second half of elements in both heaps to allow the median to be updated from the new measurements. Once the scaling parameter is available, current feature sample is scaled as:</p><formula xml:id="formula_5">S F t = F t s .</formula><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Feature selection and learning</head><p>Generally, a large number of features leads to the over-fitting problem and consequently decreases the generalization capacity of the machine learning model. To overcome this problem, various dimensionality reduction approaches have been proposed. Feature selection is one of the most popular techniques used for dimensionality reduction, by removing noise and redundant feature. The goal is to select a small subset of features from the original ones according to certain relevance criterion, which leads to higher generalization capacity, lower running time, and better model interpretability. According to the learning approach (i.e supervised, unsupervised or semi-supervised), different feature selection algorithms have been proposed. In the context of supervised learning, feature selection techniques can be categorized into filters, wrappers, and embedded techniques. In this work, we are particularly interested in methods of the first category due to their simplicity and computational efficiency. This method aims to identify a subset of features among a possibly large set of features that are relevant for predicting a response, according to the score attributed to the individual features. However, using only a scoring function to decide the features relevance can lead to a rich redundancy when the features are very dependent. A popular filter technique that is able to handle the redundancy among the selected feature is the Minimum Redundancy Maximum Relevance (mRMR) <ref type="bibr">(Peng et al., 2005)</ref>. Using this algorithm, the subset features {x j | j = 1 • • • m} are sequentially selected as follows: max</p><formula xml:id="formula_6">x j ∈Λ\Γ =         I(x j ; c) − 1 |Γ| x i ∈Γ I(x i ; x j ) | i = 1 • • • |Λ|        <label>(3)</label></formula><p>where I(x j ; c) is the mutual information value between individual feature x i and class c, I(x i ; x j ) is the mutual information between two features x i and x j , Γ is the subset of the best features, Λ is the original set of features with |Λ| cardinality. The idea behind the mRMR is to sequentially select a feature that is relevant with respect to the target class c (max-relevance) and simultaneously has a low dependence to the already selected features in Γ (min-redundancy). Both criterions are evaluated based on the mutual information. In the literature, different methods have been proposed to ensure the max-relevance (correlation between a feature vector and the target class c) criterion such as Fisher score, t-statistics, and entropy score. Thus, in this work, we consider a generalized version of the mRMR algorithm presented by the following equation: max</p><formula xml:id="formula_7">x j ∈Λ\Γ =         S (x j ) − 1 |Γ| x i ∈Γ I(x i ; x j ) | i = 1 • • • |Λ|         (4)</formula><p>where S (x j ) is the score of the jth feature.</p><p>Feature selection is a crucial step that can help us for speeding up our algorithm to meet the real-time requirements and improve the learning performance of our system, by removing the redundancy. Thus, once the feature selection is performed, we obtain a new observation S F t at time t, with a smaller dimension (| S F t | &lt; 376).</p><p>In our system, activity recognition is achieved by applying a binary classification on a single observation S F t in order to categorize it as 'suicide' or 'unsuspected'. For this purpose, we tested several binary classifiers which are constructed and tuned using the extracted features S F t of the training set. For this purpose, different binary classifiers including Linear Discriminant detected = true 13: else 14:</p><formula xml:id="formula_8">A C C E P T E D M A N U S C</formula><p>-Shift temporal window by S 15:</p><p>-Retrieve frame t + 1 16: end if Analysis (LDA), Linear Support Vector Machines (L-SVM), Support Vector Machine with radial basis kernel (RBF-SVM) and Naive Bayes (NB) have been constructed and tuned using the extracted features S F t of the training set. Due to its better performance, we chose RBF-SVM which outperforms the classifiers that we tested. Moreover, we note that due to the real-time requirements, we only tested bit complex classifiers. The flowchart of the offline training procedure is depicted in Fig. <ref type="figure">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Action recognition</head><p>Real-time activity recognition is achieved using the procedure summarized by algorithm 2. Once the RBF-SVM classifier is trained, the algorithm processes a stream of depth images in order to detect whether a suicide by hanging attempt is underway. First, the positions of the body joints are estimated using the method published in <ref type="bibr">(Shotton et al., 2013)</ref>. These joints are employed to derive the two sets of features: pose and motion features combined in a single vector F t . F t is then scaled and feature selection is applied to generate the observations S F t . These observations are sequentially classified as positive or negative using the RBF-SVM classifier. Suicide detection is based on observing the person's behavior during a sliding temporal window of width ∆ 0 , which is regularly shifted by a temporal step S at each iteration. Finally, a suicide by hanging attempt is detected if the percentage of positive observations θ t exceeds a threshold θ min .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>Since there is no public video dataset that can be used to evaluate the proposed system. We thus created our own sequence collection, where 21 persons participated to perform numerous simulations in a room whose dimensions are close to those of prisons. Our system is composed of an RGB-D sensor (Kinect v2) installed in an upper corner, at a distance of approximately 0.30 m from the ceiling, with a tilt angle of 35 • . To guarantee an efficient body pose estimation, we suppose that the distance between the candidate and the camera is between 0.5 m and 4.5 m.</p><p>To create the dataset, the participants were asked to perform two scenarios:</p><p>• In the first scenario, the candidate was invited to simulate a suicide attempt through three main steps: 1) he/she tries to create a hangmans knot using a bed sheet (each person can create the knot in a different way), 2) he/she places the knot in a fixed point in the room, 3) he/she places the knot around the neck.</p><p>• During the second scenario, the participant performed some unsuspected actions from different viewpoints with respect to the camera, such as walking in the room, sitting, removing a piece of clothing, wearing clothes, etc.</p><p>To learn suicidal behavior, we focus on the action of putting the knot from overhead down to the neck. For each video sequence corresponding to suicide by hanging, we annotated the frames containing this behavior as positive. The frames in the nonsuicidal video sequences were annotated as negative. All participants performed suicidal actions, while only the first 15 ones performed non-suicidal actions. The descriptions of the experimental dataset are given in Table1 and Table <ref type="table" target="#tab_2">2</ref>. Our dataset includes a total number of 42 videos. 27 video sequences containing suicide by hanging attempts and 15 video sequences contain only normal activities. In term of frames, our dataset contains 12991 positives frames and 31474 (21393+10081) negative frames. The dataset was collected and research was conducted with approval from the Research Ethics Board (REB) of École de Technologie Supérieure, Montréal (QC), Canada.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In order to test the accuracy and stability of the presented method, we carried out two different tests using the dataset described above. • In the first test, we use the leave-one-sequence out cross validation procedure to evaluate our system on all sequences in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D</head><formula xml:id="formula_9">M A N U S C R I P T 7</formula><p>• Since the dataset was unbalanced because the action of interest takes a short time (a few seconds) compared to non suspecious activities, we perform the second test by splitting the dataset into testing data and training data, and rebalance the latter using the SMOTE: Synthetic Minority Over-sampling Technique <ref type="bibr" target="#b2">(Chawla et al., 2002)</ref> For each experiment, we applied the algorithm, depicted in Fig. <ref type="figure">3</ref>, on the training set in order to extract feature vectors and generate the trained binary classifier. Different simple classifiers have been tried including Linear Discriminant Analysis (LDA), Linear Support Vector Machines (L-SVM), Naive Bayes (NB) and Support Vector Machines with radial basis kernel (RBF-SVM). Note that the optimal results in all experiments have been obtained using only 100 selected features by using the entropy score as a max-relevance criterion in the mRMR algorithm described in the section 3.4. Once the trained binary classifier was obtained, we evaluated it using two strategies. The first one is called Sequence-based suicide detection which is based on evaluating the classifier on detecting whether a suicide attempt exists in a video sequence. For this purpose, we used the algorithm 2 where the hyper-parameters were empirically fixed. The sliding temporal window ∆ 0 was fixed empirically to 4 seconds and shifted at each iteration by S = 0.07 seconds. A suicide action was detected if the threshold θ t exceeds θ min = .75. The second strategy is frames-based suicide detection and it was introduced for the following reason: According to the first strategy, the system can recognize a suicide action in a sequence by correctly classifying only the positive frames in the last temporal window. In other words, the system only correctly classify a few positive frames from the whole positive frames in the suicide sequence. For this, we introduce this test to evaluate the effectiveness of our system in classifying individual frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Leave-one-sequence out</head><p>In this test, a leave-one-sequence-out cross validation procedure has been applied. In this way, the system is trained with all sequences except one, which is the one that evaluates the accuracy score. By iterating over all the 42 sequences (27 suicidal sequences and 15 normal sequences), the average success rate is used as a final result. Table <ref type="table" target="#tab_3">3</ref> shows the results of sequencebased suicide detection using the leave-one-sequence out technique. We can see that the best performance was achieved using the L-SVM classifier with 90% of accuracy and 0% of false alarm. Although our system did not make any false alarm, it failed to detect some suicidal attempts. This is can refer to two main reasons: 1) The training dataset is unbalanced such that the number of negative frames is much greater than the number of the positive frames. This allows the classifier to more learn and generalize on the majority class (the negative class) and consequently do not make much false alarm, 2) taking a closer look to the misclassified sequences, it can be seen that some suicidal actions are very similar to the normal activities such as wearing clothes action which usually requires moving the hands from bottom to top around the neck. Table <ref type="table" target="#tab_4">4</ref> summarizes the results of classifying individual frames using the leave-one-sequence out. As in the first test, we remark that the best result was obtained using the L-SVM classifier. The small false alarm rate confirms the result obtained in the first test. However, even if the accuracy of classifying individual frames is acceptable with respect to the first test, it can be improved by rebalancing the dataset to allow the classifier an equitable learning on both classes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Cross-validation on rebalanced dataset</head><p>In this test, We randomly split the available video sequences into a training and a testing sets: 32 video sequences are used to train the proposed system, while the rest (10 sequences) are employed exclusively for testing. As described in Table <ref type="table" target="#tab_2">1 and  Table 2</ref>, the number of negative frames are much greater than the number of positive frames. Thus, the classifiers tend to draw a good accuracy on the majority class, having simultaneously a very poor accuracy on the minority class. This is due to the fact that most classifiers pursue minimizing the error rate without taking into account the large difference between the number of cases belonging to each class. To overcome this problem, we propose to use the SMOTE: Synthetic Minority Over-sampling Technique <ref type="bibr" target="#b2">(Chawla et al., 2002)</ref> in order to rebalance the training data set by oversampling the positive class (suicidal behavior). Table <ref type="table" target="#tab_5">5</ref> presents the sequence-based suicide detection results obtained using the balanced training data. The best accuracy was obtained using NB (Naive Bayes) and RBF-SVM (SVM with radial basis kernel). We obtained a 100% accuracy with 0% false alarm using 5 normal video sequences and 5 sequences containing suicidal behavior. In Table <ref type="table" target="#tab_6">6</ref>, the frame-based suicide detection results obtained using the balanced data set confirm the effectiveness of our data balancing strategy to improve the system performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Limitations discussion</head><p>The Kinect camera provides an informative 3D-skeleton in real-time, which is extremely valuable for activity recognition applications. However, some limitations of such sensor can be figured out and discussed.</p><p>From one hand, although that Kinect camera does not require a perfect illumination condition, the direct sunlight can still saturate the sensor, in which case the depth values could not be accurately estimated. Fortunately, prison cells are often not directly naturally enlightened owing to the absence of windows. In the case of a naturally illuminated scene, another RGB-based method could be used in parallel(e.g. detecting the presence of a hangmans knot using object detection). From the other hand, even that prison cells are not generally cluttered scenes, the occlusion problem still exists and can affect the reliability of the 3D skeleton detection. In this case, two approaches could be explored: 1) a traditional multi-cameras system based on two Kinect sensors operating in parallel can be used to overcome this problem. We also believe that the use of two cameras, with the appropriate positioning, can resolve the problem of the operation range, which is limited to 1 to 4 meters. 2) A method based on tracking the head and check the presence of a hangmans knot object around the neck can be developed. In fact, the head region is often visible to the camera and its tracking can be achieved by mapping a 2D skeletons on RGB images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We introduced an intelligent surveillance system for detecting suicide by hanging attempts in prisons. Our algorithm exploits the 3D joint's positions acquired in real time by a Kinect camera. We used pose and motion features to represent human movements and model the action of interest. A feature selection technique is applied to speed up our algorithm and improve its generalization capacity. Once the system is trained, we apply an online detection procedure, specifically designed to meet the real-time requirements. Several experiments have been carried out using numerous classifiers in order to evaluate our system. These tests allowed to achieve a high accuracy, especially after rebalancing the training set.</p><p>In the future work, we plan to collect more data to really rebalance (without using artificial data) the dataset and increase the testing data. We will also focus on improving the capacity of our system on classifying individual frames which consequently can help the system to detect the suicide attempts in a shorter time. In this work, an important line of work is to model and recognize the action of creating the knot using the strangling object. This can be achieved by combining depth information with the corresponding near infrared and RGB images offered by the Kinect camera, which serves for detecting the knot.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Location of human joints given by the method of Shotton et al. (Shotton et al., 2013).</figDesc><graphic url="image-3.png" coords="5,93.12,531.55,125.58,147.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :Algorithm 1</head><label>21</label><figDesc>Fig. 2: Percentage of tracked instances for each joint.The total number of frames is 51003. Algorithm 1 Online scaling parameter estimation Input: neck position and spine middle position from current frame; min heap and max heaps Output: estimated scaling parameter Assumption: the size difference of min and max heaps is less than or equal to one. Both min and max heap are not full 1: Calculate the distance d between the neck and spine middle 2: if the sizes of min heap and max heap are not equal then 3: Add d to the heap with the smaller size</figDesc><graphic url="image-4.png" coords="6,68.00,60.56,175.80,115.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Number of positive frames (suicidal) in each suicidal video sequences. The rest of frames are annotated as negatives.</figDesc><table><row><cell>Sequence index</cell><cell># of positive frames / sequence size</cell><cell>Sequence index</cell><cell># of positive frames / sequence size</cell></row><row><cell>1</cell><cell>219 / 918</cell><cell>15</cell><cell>302 / 905</cell></row><row><cell>2</cell><cell>320 / 895</cell><cell>16</cell><cell>1072 / 1637</cell></row><row><cell>3</cell><cell>462 / 1186</cell><cell>17</cell><cell>662 / 1160</cell></row><row><cell>4</cell><cell>644 / 909</cell><cell>18</cell><cell>658 / 778</cell></row><row><cell>5</cell><cell>464 / 882</cell><cell>19</cell><cell>286 / 758</cell></row><row><cell>6</cell><cell>392 / 711</cell><cell>20</cell><cell>520 / 811</cell></row><row><cell>7</cell><cell>890 / 1126</cell><cell>21</cell><cell>366 / 753</cell></row><row><cell>8</cell><cell>552 / 1338</cell><cell>22</cell><cell>686 / 1483</cell></row><row><cell>9</cell><cell>594 / 1054</cell><cell>23</cell><cell>480 / 570</cell></row><row><cell>10</cell><cell>250 / 834</cell><cell>24</cell><cell>268 / 395</cell></row><row><cell>11</cell><cell>574 / 1989</cell><cell>25</cell><cell>566 / 1079</cell></row><row><cell>12</cell><cell>376 / 1086</cell><cell>26</cell><cell>292 / 916</cell></row><row><cell>13</cell><cell>298 / 1017</cell><cell>27</cell><cell>630 / 1451</cell></row><row><cell>14</cell><cell>168 / 2960</cell><cell>Total</cell><cell>12991 / 29610</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Number of negative frames in the video sequences of unsuspected actions.</figDesc><table><row><cell>Sequence index</cell><cell># of negative frames</cell><cell>Sequence index</cell><cell># of negative frames</cell></row><row><cell>1</cell><cell>1106</cell><cell>9</cell><cell>2084</cell></row><row><cell>2</cell><cell>1249</cell><cell>10</cell><cell>1132</cell></row><row><cell>3</cell><cell>1370</cell><cell>11</cell><cell>1032</cell></row><row><cell>4</cell><cell>2621</cell><cell>12</cell><cell>1339</cell></row><row><cell>5</cell><cell>1563</cell><cell>13</cell><cell>971</cell></row><row><cell>6</cell><cell>1591</cell><cell>14</cell><cell>1567</cell></row><row><cell>7</cell><cell>1666</cell><cell>15</cell><cell>1090</cell></row><row><cell>8</cell><cell>1012</cell><cell>Total</cell><cell>21393</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Sequence-based suicide detection results using leave-one-sequence out test. We present the results obtained by 4 classifier: Linear Discriminant Analysis (LDA), Linear Support Vector Machines (L-SVM), Support Vector Machines with radial basis kernel (RBF-SVM: σ = 4, C = .5) ,</figDesc><table><row><cell>Naive Bayes</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Frame-based suicide detection results using leave-one-sequence out test.We present the results obtained by 4 classifier: Linear Discriminant Analysis, Linear Support Vector Machines, Support Vector Machines with radial basis kernel (σ = .5, C = .5) , Naive Bayes</figDesc><table><row><cell></cell><cell cols="4">LDA L-SVM NB RBF-SVM</cell></row><row><cell>Accuracy (%)</cell><cell>84</cell><cell>85</cell><cell>84</cell><cell>83</cell></row><row><cell>False alarm (%)</cell><cell>10</cell><cell>6</cell><cell>13</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Sequence-based suicide detection results using a balanced dataset. We present the results obtained by 4 classifier: Linear Discriminant Analysis, Linear Support Vector Machines, Support Vector Machines with radial basis kernel ( σ = 4, C = .5) , Naive Bayes</figDesc><table><row><cell></cell><cell cols="4">LDA L-SVM NB RBF-SVM</cell></row><row><cell>Accuracy (%)</cell><cell>90</cell><cell>90</cell><cell>100</cell><cell>100</cell></row><row><cell>False alarm (%)</cell><cell>80</cell><cell>40</cell><cell>0</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Frame-based suicide detection results using a balanced dataset. We present the results obtained by 4 classifier: Linear Discriminant Analysis, Linear Support Vector Machines, Support Vector Machines with radial basis kernel (σ = 4, C = .5) , Naive Bayes</figDesc><table><row><cell></cell><cell cols="4">LDA L-SVM NB RBF-SVM</cell></row><row><cell>Accuracy (%)</cell><cell>89</cell><cell>89</cell><cell>89</cell><cell>90</cell></row><row><cell>False alarm (%)</cell><cell>9</cell><cell>10</cell><cell>10</cell><cell>8</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T 9 Peng, H., <ref type="bibr">Long, F., Ding, C., 2005.</ref> Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy. IEEE Transactions on pattern analysis and machine intelligence 27, 1226-1238. Sarafianos, N., <ref type="bibr">Boteanu, B., Ionescu, B., Kakadiaris, I.A., 2016. 3d</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Acknowledgment</head><p>This work was supported by research grants from the Natural Sciences and Engineering Research Council of Canada, MITACS, and an industrial funding from Aerosystems International Inc. The authors would also like to thank their collaborators from Aerosystems International Inc. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Surf: Speeded up robust features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="404" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automated video surveillance for preventing suicide attempts</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bouachir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Noumeir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th IET International Conference on Imaging for Crime Detection and Prevention</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>ICDP 2016</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority over-sampling technique</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Action recognition by learning discriminative key poses</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cheema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Eweiwi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Thurau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1302" to="1309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Cook</surname></persName>
		</author>
		<title level="m">Door suicide alarm. US Patent RE42</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page">991</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Action recognition by learning mid-level motion features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2008">2008. 2008. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Human posture recognition by combining silhouette and infrared cast shadows</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gouiaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 International Conference on Image Processing Theory, Tools and Applications (IPTA)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Suicide prevention in correctional facilities: Reflections and next steps</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of law and psychiatry</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="188" to="194" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visual perception of biological motion and a model for its analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Johansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception, &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="201" to="211" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
	<note>Attention</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Detection of a suicide by hanging based on a 3-d image analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Myung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sensors Journal</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="2934" to="2935" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Mortality in local jails and state prisons, 2000-2011, statistical tables</title>
		<author>
			<persName><forename type="first">M</forename><surname>Noonan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ginder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2013. 2014. 2011-2014</date>
			<publisher>Office of the Correctional Investigator of Canada</publisher>
			<pubPlace>Bureau of Justice Statistics Washington, DC</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>A Three Year Review of Federal Inmate Suicides</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-organizing neural integration of pose-motion features for human action recognition</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wermter</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnbot.2015.00003</idno>
		<idno>doi:10.3389/fnbot.2015.00003</idno>
		<ptr target="https://www.frontiersin.org/article/10.3389/fnbot.2015.00003" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neurorobotics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierarchical som-based detection of novel behavior for 3d human tracking</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wermter</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2013.6706727</idno>
	</analytic>
	<monogr>
		<title level="m">The 2013 International Joint Conference on Neural Networks (IJCNN)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
