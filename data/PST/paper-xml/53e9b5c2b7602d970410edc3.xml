<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ridge Regression Learning Algorithm in Dual Variables</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">C</forename><surname>Saunders</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Royal Holloway</orgName>
								<orgName type="institution" key="instit2">University of London Egham</orgName>
								<address>
									<postCode>TW20 0EX</postCode>
									<settlement>Surrey</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">A</forename><surname>Gammerman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Royal Holloway</orgName>
								<orgName type="institution" key="instit2">University of London Egham</orgName>
								<address>
									<postCode>TW20 0EX</postCode>
									<settlement>Surrey</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">V</forename><surname>Vovk</surname></persName>
							<email>vovk@dcs.rhbnc.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Royal Holloway</orgName>
								<orgName type="institution" key="instit2">University of London Egham</orgName>
								<address>
									<postCode>TW20 0EX</postCode>
									<settlement>Surrey</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Ridge Regression Learning Algorithm in Dual Variables</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0C7330911483AF05547C6158DF1FDB7D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we study a dual version of the Ridge Regression procedure. It allows us to perform non-linear regression by constructing a linear regression function in a high dimensional feature space. The feature space representation can result in a large increase in the number of parameters used by the algorithm. In order to combat this "curse of dimensionality", the algorithm allows the use of kernel functions, as used in Support Vector methods. We also discuss a powerful family of kernel functions which is constructed using the ANOVA decomposition method from the kernel corresponding to splines with an infinite number of nodes. This paper introduces a regression estimation algorithm which is a combination of these two elements: the dual version of Ridge Regression is applied to the ANOVA enhancement of the infinitenode splines. Experimental results are then presented (based on the Boston Housing data set) which indicate the performance of this algorithm relative to other algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>First of all, let us formulate regression estimation problem. Suppose we have a set of vectors<ref type="foot" target="#foot_0">1</ref> x 1 , . . . , x T , and we also have a supervisor which gives us a real value y t , for each of the given vectors. Our problem is to construct a learning machine which when given a new set of examples, minimises some measure of discrepancy between its prediction ŷt and the value of y t . The measure of loss which we are using, average square loss (L), is defined by</p><formula xml:id="formula_0">L = 1 l l t=1</formula><p>(y t -ŷt ) 2 , where y t are the supervisor's answers, ŷt are the predicted values, and l is the number of vectors in the test set.</p><p>Least Squares and Ridge Regression are classical statistical algorithms which have been known for a long time. They have been widely used, and recently some papers such as Drucker et al. <ref type="bibr" target="#b1">[2]</ref> have used regression in conjunction with a high dimensional feature space. That is the original input vectors are mapped into some feature space, and the algorithms are then used to construct a linear regression function in the feature space, which represents a non-linear regression in the original input space. There is, however, a problem encountered when using these algorithms within a feature space. Very often we have to deal with a very large number of parameters, and this leads to serious computational difficulties that can be impossible to overcome. In order to combat this "curse of dimensionality" problem, we describe a dual version of the Least Squares and Ridge Regression algorithms, which allows the use of kernel functions. This approach is closely related to Vapnik's kernel method as used in the Support Vector Machine. Kernel functions represent dot products in a feature space, which allows the algorithms to be used in a feature space without having to carry out computations within that space. Kernel functions themselves can take many forms and particular attention is paid to a family of kernel functions which are constructed using ANOVA decomposition (Vapnik <ref type="bibr" target="#b9">[10]</ref>; see also Wahba <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>). There are two major objectives of this paper:</p><p>1. To show how to use kernel functions to overcome the curse of dimensionality in the above mentioned algorithms.</p><p>2. To demonstrate how ANOVA decomposition kernels can be constructed, and evaluate their performance compared to polynomial and spline kernels, on a real world data set.</p><p>Results from experiments performed on the well known Boston housing data set are then used to show that the Least Squares and Ridge Regression algorithms perform well in comparison with some other algorithms.</p><p>The results also show that the ANOVA kernels, which only consider a subset of the input parameters, can improve on results obtained on the same kernel function without the ANOVA technique applied. In the next section we present the dual form of Least Squares and Ridge Regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RIDGE REGRESSION IN DUAL VARIABLES</head><p>Before presenting the algorithms in dual variables, the original formulation of Least Squares and Ridge Regression is stated here for clarity.</p><p>Suppose we have a training set (x 1 , y 1 ), . . . , (x T , y T ), where T is the number of examples, x t are vectors in IR n (n is the number of attributes) and y t ∈ IR, t = 1, . . . , T . Our comparison class consists of the linear functions y = w • x, where w ∈ IR n .</p><p>The Least Squares method recommends computing w = w 0 which minimizes</p><formula xml:id="formula_1">L T (w) = T t=1 (y t -w • x t ) 2</formula><p>and using w 0 for labeling future examples: if a new example has attributes x, the predicted label is w 0 • x.</p><p>The Ridge Regression procedure is a slight modification on the least squares method and replaces the objective function L T (w) by</p><formula xml:id="formula_2">a w 2 + T t=1 (y t -w • x t ) 2 ,</formula><p>where a is a fixed positive constant.</p><p>We now derive a "dual version" for Ridge Regression (RR); since we allow a = 0, this includes Least Squares (LS) as a special case. In this derivation we partially follow Vapnik <ref type="bibr" target="#b7">[8]</ref>. We start with re-expressing our problem as: minimize the expression</p><formula xml:id="formula_3">a w 2 + T t=1 ξ 2 t (1)</formula><p>under the constraints</p><formula xml:id="formula_4">y t -w • x t = ξ t , t = 1, . . . , T.<label>(2)</label></formula><p>Introducing Lagrange multipliers α t , t = 1, . . . , T , we can replace our constrained optimization problem by the problem of finding the saddle point of the function</p><formula xml:id="formula_5">a w 2 + T t=1 ξ 2 t + T t=1 α t (y t -w • x t -ξ t ) .<label>(3)</label></formula><p>In accordance with the Kuhn-Tucker theorem, there exist values of Lagrange multipliers α = α KT for which the minimum of (3) equals the minimum of (1), under constraints (2). To find the optimal w and ξ, we will do the following; first, minimize (3) in w and ξ and then maximize it in α. Notice that for any fixed values of α the minimum of (3) (in w and ξ) is less than or equal to the value of the optimization problem (1)-( <ref type="formula" target="#formula_4">2</ref>), and equality is attained when α = α KT . By doing this, we will therefore find the solution to our original constrained minimization problem (1)- <ref type="bibr" target="#b1">(2)</ref>.</p><p>Differentiating (3) in w, we obtain the condition 2aw -</p><formula xml:id="formula_6">T t=1 α t x t = 0, i.e., w = 1 2a T t=1 α t x t .<label>(4)</label></formula><p>(Lagrange multipliers are usually interpreted as reflecting the importance of the corresponding constraints, and equation ( <ref type="formula" target="#formula_6">4</ref>) shows that w is proportional to the linear combination of x t , each of which is taken with a weight proportional to its importance.) Substituting this into (3), we obtain</p><formula xml:id="formula_7">1 4a T s,t=1 α s α t (x s • x t ) + T t=1 ξ 2 t + 1 2a T t=1 α t x t • - T t=1 α t x t + T t=1 y t α t - T t=1 α t ξ t = - 1 4a T s,t=1 α s α t (x s • x t ) + T t=1 ξ 2 t + T t=1 y t α t - T t=1 α t ξ t .</formula><p>(5) Differentiating ( <ref type="formula">5</ref>) in ξ t , we obtain</p><formula xml:id="formula_8">ξ t = α t 2 , t = 1, . . . , T<label>(6)</label></formula><p>(i.e., the importance of the tth constraint is proportional to the corresponding residual); substitution into <ref type="bibr" target="#b4">(5)</ref> gives</p><formula xml:id="formula_9">- 1 4a T s,t=1 α s α t (x s • x t ) - 1 4 T t=1 α 2 t + T t=1 y t α t . (<label>7</label></formula><formula xml:id="formula_10">)</formula><p>Denoting K as the T × T matrix of dot products</p><formula xml:id="formula_11">K s,t = x s • x t ,</formula><p>and differentiating in α t , we obtain the condition</p><formula xml:id="formula_12">- 1 2a Kα - 1 2 α + y = 0, which is equivalent to α = 2a(K + aI) -1 y.</formula><p>Recalling (4), we obtain that the prediction y given by the Ridge Regression procedure on the new unlabeled example x is</p><formula xml:id="formula_13">w • x = 1 2a T t=1 α t x t • x = 1 2a α • k = y (K + aI) -1 k,</formula><p>where k = (k 1 , . . . , k T ) is the vector of the dot products:</p><formula xml:id="formula_14">k t := x t • x, t = 1, . . . , T.</formula><p>Lemma 1 RR's prediction of the label y of a new unlabeled example x is</p><formula xml:id="formula_15">y (K + aI) -1 k, (<label>8</label></formula><formula xml:id="formula_16">)</formula><p>where K is the matrix of dot products of the vectors x 1 , . . . , x T in the training set,</p><formula xml:id="formula_17">K s,t = K(x s , x t ), s = 1, . . . , T, t = 1, . . . , T,</formula><p>k is the vector of dot products of x and the vectors in the training set,</p><formula xml:id="formula_18">k t := K(x t , x), t = 1, . . . , T,</formula><p>and K(x, x ) = x•x is simply a function which returns the dot product of the two vectors, x and x .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LINEAR REGRESSION IN FEATURE SPACE</head><p>When K(x i , x j ) is simply a function which returns the dot product of the given vectors, formula (8) corresponds to performing linear regression within the input space IR n defined by the examples. If we want to construct a linear regression in some feature space, we first have to choose a mapping from the original space X to a higher dimensional feature space F (φ : X → F ).</p><p>In order to use Lemma 1 to construct the regression in the feature space, the function K must now correspond to the dot product φ(x i ) • φ(x j ). It is not necessary to know φ(x) as long as we know</p><formula xml:id="formula_19">K(x i , x j ) = φ(x i )•φ(x j ).</formula><p>The question of which functions K correspond to a dot product in some feature space F is answered by Mercer's theorem and addressed by Vapnik <ref type="bibr" target="#b8">[9]</ref> in his discussion of support vector methods. As an illustration of the idea, an example of a simple kernel function is presented here. (See Girosi <ref type="bibr" target="#b3">[4]</ref>.) Suppose there is a mapping function φ which maps a two-dimensional vector into 6 dimensions:</p><formula xml:id="formula_20">φ : (x 1 , x 2 ) → ((x 1 ) 2 , (x 2 ) 2 , √ 2x 1 , √ 2x 2 , √ 2x 1 x 2 , 1),</formula><p>then dot products in F take the form</p><formula xml:id="formula_21">(φ(x) • φ(y)) = (x 1 ) 2 (y 1 ) 2 + (x 2 ) 2 (y 2 ) 2 + 2x 1 y 1 +2x 2 y 2 + 2x 1 y 1 x 2 y 2 + 1 = ((x • y) + 1) 2 .</formula><p>One possible kernel function is therefore ((x • y) + 1) 2 . This can be generalised into a kernel function of the form K(x, y) = ((x • y) + 1) d , and more than 2 dimensions.</p><p>The use of kernel functions allows us to construct a linear regression function in a high dimensional feature space (which corresponds to a non-linear regression in the input space) avoiding the curse of having to carry out computations in the high dimensional space. In particular, kernel functions are a way to combat the curse of dimensionality problems such as those faced in Drucker et al. <ref type="bibr" target="#b1">[2]</ref>, where a regression function was also constructed in a feature space, but computations were carried out in the high dimensional space, leading to huge number of parameters for non-trivial problems.</p><p>For more information on the kernel technique, see Vapnik <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b8">9]</ref> and Wahba <ref type="bibr" target="#b10">[11]</ref>.</p><p>Before indicating how ANOVA decomposition can be used to form kernels, a brief description is needed of the family of kernels to which the ANOVA decomposition can be applied, this being the family of multiplicative kernels. This refers to the set of kernels where the multi-dimensional case is calculated as the product of the one-dimensional case. That is, if the onedimensional case is k(x i , y i ), then the n-dimensional case is</p><formula xml:id="formula_22">K n (x, y) = n i=1 k(x i , y i ).</formula><p>One such kernel (to which the ANOVA decomposition is applied here) is the spline kernel with an infinite number of nodes (see Vapnik <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref> and Kimeldorf and Wahba <ref type="bibr" target="#b4">[5]</ref>). A spline approximation which has an infinite number of nodes can be defined on the interval (0, a), 0 &lt; a &lt; ∞, as the expansion</p><formula xml:id="formula_23">f (x) = a 0 a(t)(x -t) d + dt + d i=0 a i x i ,</formula><p>where a i , i = 0, . . . , d, are unknown values, and a(t) is an unknown function which defines the expansion. This can be considered as an inner product, and the kernel which generates splines of dimension d with an infinite number of nodes can be expressed as</p><formula xml:id="formula_24">k d (x, y) = a 0 (x -t) + (y -t) d + dt + d r=0</formula><p>x r y r .</p><p>Note that when t &gt; min(x, y) the function under the integral sign will have value zero. It is therefore sufficient only to consider the interval (0, min(x, y)), which makes the formula above equivalent to</p><formula xml:id="formula_25">k d (x, y) = d r=0 d r 2d -r + 1 min(x, y) 2d-r+1 |x -y| r + d r=0</formula><p>x r y r .</p><p>In particular, for the case of linear splines (d = 1) we have :</p><formula xml:id="formula_26">k 1 (x, y) = 1 + xy + 1 2</formula><p>|y -x| min(x, y) 2 + min(x, y) 3  3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ANOVA DECOMPOSITION KERNELS</head><p>The ANOVA decomposition kernels are inspired by their namesake in statistics, which analyses different subsets of variables. The actual decomposition can be adapted to form kernels (as in, e.g., Vapnik <ref type="bibr" target="#b9">[10]</ref>) which involve different subsets of the attributes of the examples up to a certain size. There are two main reasons for choosing to use ANOVA decomposition. Firstly, the different subsets which are considered may group together like variables, which can lead to greater predictive power. Also, by only considering some subsets of the input parameters, ANOVA decomposition reduces the VC dimension of the set of functions that you are considering, which can avoid overfitting your training data.</p><p>Given a one-dimensional kernel k, the ANOVA kernels are defined as follows:</p><formula xml:id="formula_27">K 1 (x, y) = 1≤k≤n k(x k , y k ), K 2 (x, y) = 1≤k1&lt;k2≤n k(x k1 , y k1 )k(x k2 , y k2 ),</formula><p>. . . , K n (x, y) = k(x k1 , y k1 ) . . . k(x kn , y kn ).</p><p>From Vapnik <ref type="bibr" target="#b9">[10]</ref> the following recurrent procedure can be used when calculating the value of K n (x, y). Let</p><formula xml:id="formula_28">K s (x, y) = n i=1 (k(x i , y i )) s</formula><p>and K 0 (x, y) = 1; then</p><formula xml:id="formula_29">K p (x, y) = 1≤k1&lt;k2&lt;•••&lt;kp≤n k(x k1 , y k1 ) . . . k(x kp , y kp ), K p (x, y) = 1 p p s=1 (-1) s+1 K p-s (x, y)K s (x, y).</formula><p>For the purposes of this paper, when using kernels produced by ANOVA decomposition, only the order p is considered:</p><formula xml:id="formula_30">K(x, y) = K p (x, y).</formula><p>An alternative method of using ANOVA decomposition would be to consider order p and all lower orders (as in Stitson <ref type="bibr" target="#b6">[7]</ref>), i.e.,</p><formula xml:id="formula_31">K(x, y) = p i=1 K i (x, y).</formula><p>Experiments were conducted on the Boston Housing data set<ref type="foot" target="#foot_1">2</ref> . This is a well known data set for testing non-linear regression methods; see, e.g., Breiman <ref type="bibr" target="#b0">[1]</ref> and Saunders <ref type="bibr" target="#b5">[6]</ref>. The data set consists of 506 cases in which 12 continuous variables and 1 binary variable determine the median house price in a certain area of Boston in thousands of dollars. The continuous variables represent various values pertaining to different locational, economic and structural features of the house. The prices lie between $5000 and $50,000 in units of $1000. Following the method used by Drucker et al. <ref type="bibr" target="#b1">[2]</ref>, the data set was partitioned into a training set of 401 cases, a validation set of 80 cases and a test set of 25 cases. This partitioning was carried out randomly 100 times, in order to carry out 100 trials on the data. For each trial the Ridge Regression algorithm was applied using:</p><p>• a kernel which corresponds to a spline approximation with an infinite number of nodes,</p><p>• the same kernel but with the ANOVA decomposition technique applied,</p><p>• and polynomial kernels.</p><p>For each kernel the set of parameters (the order of spline/degree of polynomial and the value of coefficient a) was selected which gave the smallest error on the validation set, and then the error on the test set was measured. This experiment was then repeated using a support vector machine (SVM), with the same kernels and exactly the same 100 training files (see Stitson <ref type="bibr" target="#b6">[7]</ref> for full details). As an illustration of the number of parameters which were considered by the Ridge Regression Algorithm (and the SVM), consider the polynomial kernel which was outlined earlier, using a degree of 5. This maps the input vectors into a high dimensional feature space which is equivalent to evaluating 13 5 = 371, 293 different parameters.</p><p>The results obtained from the experiments are shown in Table <ref type="table" target="#tab_0">1</ref>. The measure of error used for the tests was the average squared error. For each of the 100 test files, the algorithm was run and the square of the difference between the predicted and actual value was taken. This was then averaged over the 25 test cases. This produces an average error for each of the 100 test files, and an average of these were taken, which produces the final error which is quoted in the 3rd column of the table. The variance measure in the table is the average squared difference, between the squared error measured on each sample and the average squared error.</p><p>There are two additional results which should be noted here. One is from Breiman <ref type="bibr" target="#b0">[1]</ref> using bagging with average squared error of 11.7, and one from Drucker et al. <ref type="bibr" target="#b1">[2]</ref> using Support Vector regression with polynomial kernels with average squared error of 7.2. The result obtained by Drucker et al. is slightly better than the one obtained here using a similar machine; this may be, however, due to the random selection of the training, validation and testing sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">COMPARISONS</head><p>In this section we will give a comparison of the results of this paper with the known results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">SV MACHINES</head><p>In this subsection we describe in more detail the connection of the approach of this paper with the Support Vector Machine.</p><p>Our optimization problem (minimizing (1) under constraints (2)) is essentially a special case of the following general optimization problem: minimize the expression 1 2</p><formula xml:id="formula_32">w 2 + C k T t=1 (ξ * t ) k + T t=1 (ξ t ) k<label>(9)</label></formula><p>under the constraints</p><formula xml:id="formula_33">y t -w • x t ≤ + ξ * t , t = 1, . . . , T,<label>(10)</label></formula><formula xml:id="formula_34">w • x t -y t ≤ + ξ t , t = 1, . . . , T ;<label>(11)</label></formula><p>&gt; 0 and k ∈ {1, 2} are some constants. This optimization problem (along with a similar problem corresponding to Huber's loss function) is considered in Vapnik <ref type="bibr" target="#b9">[10]</ref>, Chapter 11 (Vapnik, however, considers more general regression functions of the form w • x + b rather than w • x; the difference is minor because we can always add an extra attribute which is always 1 to all examples).</p><p>Our problem (1)-( <ref type="formula" target="#formula_4">2</ref>) corresponds to the problem ( <ref type="formula" target="#formula_32">9</ref>)- <ref type="bibr" target="#b10">(11)</ref> with k = 2, = 0 and C = 1/a. Vapnik <ref type="bibr" target="#b9">[10]</ref> gives a dual statement of his, and a fortiori our, problem; he does not reach, however, the closed-form expression (8) As we mentioned before, our derivation of formula <ref type="bibr" target="#b7">(8)</ref> follows <ref type="bibr" target="#b7">[8]</ref>. The dual Ridge Regression is also known in traditional statistics, but statisticians usually use some clever matrix manipulations rather than the Lagrange method. Our derivation (modelled on Vapnik's) gives some extra insight: see, e.g., equations ( <ref type="formula" target="#formula_6">4</ref>) and <ref type="bibr" target="#b5">(6)</ref>. For an excellent survey of connections between Support Vector Machine and the work done in statistics we refer the reader to Wahba <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> and Girosi <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">KRIEGING</head><p>Formula ( <ref type="formula" target="#formula_15">8</ref>) is well known in the theory of Krieging; in this subsection we will explain the connection for readers who are familiar with Krieging. Consider the Bayesian setting where:</p><p>• the vector w of weights is distributed according to the normal distribution with mean 0 and covariance matrix 1 2a I; • y t = w • x t + t , t = 1, . . . , T , where t are random variables distributed normally with mean 0 and variance 1 2 .</p><p>Then the optimization problem (1) under the constraints (2) becomes the problem of finding the posterior mode (which, because of our normality assumption, coincides with the posterior mean) of w; therefore, formula <ref type="bibr" target="#b7">(8)</ref> gives the mean value of the random variable w • x (which is the "clean version" of the label y = w • x + of the next example). Notice that the random variables y 1 , . . . , y T , w • x are jointly normal and the covariances between them are cov(y s , y</p><formula xml:id="formula_35">t ) = cov(w•x s + s , w•x t + t ) = 1 2a (x s •x t )+ 1 2 and cov(y t , w • x) = cov(w • x t + t , w • x) = 1 2a (x t • x).</formula><p>In accordance with the Krieging formula the best prediction for w • x will be</p><formula xml:id="formula_36">y 1 2a K + 1 2 I -1 1 2a k = y (K + aI) -1 k,</formula><p>which coincides with (8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSIONS</head><p>A formula for Ridge Regression (which included Least Squares as a special case) in dual variables was derived using the method of Lagrange multipliers. This was then used to perform linear regression in a feature space. Therefore, we once more showed how the problem of learning in a very high dimensional space can be solved by using kernel functions. This allowed the algorithm to overcome the "curse of dimensionality" and run efficiently, even though a very large number of parameters were being considered. Experimental results show that Ridge Regression performs well. The results also indicate that applying ANOVA decomposition to a kernel can achieve better results than using the same kernel without the technique applied. Both Ridge Regression and the Support Vector method gave a smaller error when using ANOVA splines compared to the other spline kernel.</p><p>A weak part of our experimental section is that, though the Boston housing data is a useful benchmark, we have not applied our algorithm to a wider range of practical problems. This is what we plan to do next.</p><p>In order to confirm that ANOVA kernels can outperform kernels in their orginal form, the ANOVA decomposition technique should be applied to other multiplicative kernels. The technique of applying kernel functions to overcome problems of high dimensionality should also be investigated futher, to see if it can be applied to any other algorithms which prove computationally difficult or impossible when faced with a large number of parameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Experimental Results on the Boston Housing Data</figDesc><table><row><cell>METHOD</cell><cell>KERNEL</cell><cell cols="2">SQUARED ERROR VARIANCE</cell></row><row><cell cols="2">Ridge Regression Polynomial</cell><cell>10.44</cell><cell>18.34</cell></row><row><cell cols="2">Ridge Regression Splines</cell><cell>8.51</cell><cell>11.19</cell></row><row><cell cols="2">Ridge Regression ANOVA Splines</cell><cell>7.69</cell><cell>8.27</cell></row><row><cell>SVM [7]</cell><cell>Polynomial</cell><cell>8.14</cell><cell>15.13</cell></row><row><cell>SVM</cell><cell>Splines</cell><cell>7.87</cell><cell>12.67</cell></row><row><cell>SVM</cell><cell>Anova Splines</cell><cell>7.72</cell><cell>9.44</cell></row><row><cell cols="2">(because he was mainly interested in positive values of</cell><cell></cell><cell></cell></row><row><cell>).</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We will use subscripts to indicate a particular vector (e.g. xt is the tth vector), and superscripts to indicate a particular vector element (e.g x i is the ith element of the vector x).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Available by anonymous FTP from: ftp://ftp.ics.uci.com/pub/ machine-learning-databases/housing.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We feel that a very interesting direction of developing the results of this paper would be to combine the dual version of Ridge Regression with the ideas of Gammerman et al. <ref type="bibr" target="#b2">[3]</ref> to obtain a measure of confidence for predictions output by our algorithms. We expect that in this case simple closed-form formulas can be obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank EPSRC for providing financial support through grant GR/L35812 ("Support Vector and Bayesian Learning Algorithms").</p><p>The referees' thoughtful comments are gratefully appreciated.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Bagging predictors. Technical Report 421</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<ptr target="ftp://ftp.stat.berkely.edu/pub/tech-reports/421.ps.Z" />
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>Department of Statistics, University of California, Berkley</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Support Vector regression machines</title>
		<author>
			<persName><forename type="first">H</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 9</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">155</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning by transduction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gammerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vovk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An equivalence between sparce approximations and Support Vector Machines</title>
		<author>
			<persName><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Department of Brain and Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<date type="published" when="1997-05">May 1997</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology Artificial Intelligence Laboratory and Center for Biological and Computational Learning</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>L. Paper No.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Some results on Tchebycheffian spline functions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kimeldorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="82" to="95" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Ridge regression in dual variables</title>
		<author>
			<persName><forename type="first">C</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gammerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vovk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>Royal Holloway, University of London</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Support Vector regression with ANOVA decomposition kernels</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Stitson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gammerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vovk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>University of London</publisher>
			<pubPlace>Royal Holloway</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">The Nature of Statistical Learning Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Statistical learning theory</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Learning and Probabilistic Reasoning</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Gammerman</surname></persName>
		</editor>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
		<imprint>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spline models for observational data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CBMS-NSF Regional Conference Series in Applied Mathematics</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">59</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Support Vector machines, reproducing kernel Hilbert spaces and the randomized GACV</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wahba</surname></persName>
		</author>
		<idno>984</idno>
		<imprint>
			<date type="published" when="1997">1997</date>
			<pubPlace>USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Statistics, University of Wisconsin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
