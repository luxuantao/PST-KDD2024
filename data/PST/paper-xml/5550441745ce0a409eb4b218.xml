<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Click-through-based Cross-view Learning for Image Search *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ting</forename><surname>Yao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">City University of Hong Kong</orgName>
								<address>
									<settlement>Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Research Institute</orgName>
								<orgName type="institution">City University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
							<email>tmei@microsoft.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Houqiang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">City University of Hong Kong</orgName>
								<address>
									<settlement>Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Research Institute</orgName>
								<orgName type="institution">City University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Rui</surname></persName>
							<email>yongrui@microsoft.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Click-through-based Cross-view Learning for Image Search *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BEA56AEE242167BDD41541EE7E6368E2</idno>
					<idno type="DOI">10.1145/2600428.2609568</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Retrieval models Algorithm</term>
					<term>Experimentation Image search</term>
					<term>cross-view learning</term>
					<term>subspace learning</term>
					<term>clickthrough data</term>
					<term>DNN image representation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One of the fundamental problems in image search is to rank image documents according to a given textual query. Existing search engines highly depend on surrounding texts for ranking images, or leverage the query-image pairs annotated by human labelers to train a series of ranking functions. However, there are two major limitations: 1) the surrounding texts are often noisy or too few to accurately describe the image content, and 2) the human annotations are resourcefully expensive and thus cannot be scaled up.</p><p>We demonstrate in this paper that the above two fundamental challenges can be mitigated by jointly exploring the cross-view learning and the use of click-through data. The former aims to create a latent subspace with the ability in comparing information from the original incomparable views (i.e., textual and visual views), while the latter explores the largely available and freely accessible click-through data (i.e., "crowdsourced" human intelligence) for understanding query. Specifically, we propose a novel cross-view learning method for image search, named Click-through-based Crossview Learning (CCL), by jointly minimizing the distance between the mappings of query and image in the latent subspace and preserving the inherent structure in each original space. On a large-scale click-based image dataset, C-CL achieves the improvement over Support Vector Machinebased method by 4.0% in terms of relevance, while reducing the feature dimension by several orders of magnitude (e.g., from thousands to tens). Moreover, the experiments also demonstrate the superior performance of CCL to several state-of-the-art subspace learning techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Keyword-based image search has received intensive research attention since the early of 1990s <ref type="bibr" target="#b20">[20]</ref>. The significance of the topic can be partly reflected from the huge volume of published papers, particularly for addressing the problems of learning the rank or similarity functions. Despite these efforts, the fact that the queries (texts) and search targets (images) are of two different modalities (or views) has resulted in the open problem of "semantic gap." Specifically, a query in the form of textual keywords is not directly comparable with the visual content of images. The commercial search engines to date primarily reply on textual features extracted from the surrounding texts of images. However, the text description might not fully depict the salient aspect of visual content, not to mention that some images actually do not come along with any text description. One feasible solution is learning image rankers from the query-image pairs labeled by human subjects. However, the labeling process is generally time consuming, and in practice difficult to ensure the quality of labels. Furthermore, as the user search intents are not likely to always align with these pre-defined labels, image rankers used to suffer from the poor generalization performance.</p><p>Inspired by the success of multi-view embedding <ref type="bibr" target="#b31">[31]</ref>, this paper studies the cross-view (i.e., text to image views) search problem by learning a common latent subspace that allows direct comparison of text queries and images. Specifically, by mapping to the latent subspace, the relevance or similarity between a textual query and an image can be directly measured between their projections, making the information from the original incomparable cross-view space comparable in the shared subspace. In addition, the dimensionality of the latent subspace is significantly reduced compared with that of any input view, making the memory costs much saved for existing search systems.</p><p>Moreover, we consider exploring user click-through data, aiming to bridge the user intention gap for image search. In general, image rankers obtain training data by manually labelling the relevance of query-image pairs. However, it is difficult to fathom user intent based on the queries, especially for those ambiguous queries. For example, given the query "mustang cobra," experts tend to label the images of animal "mustang" and "cobra" as highly relevant. However, empirical evidence suggests that most users wish to retrieve images of a car of brand "mustang cobra." The experts' labels therefore could be erroneous. This will bias the training set and the ranker will be learned sub-optimal. On the other hand, the click-through data provide an alternative to address this problem. In an image search engine, users browse image search results before clicking a specific image. The decision to click is likely dependent on the relevance of an image. Therefore, the click data can serve as a reliable and implicit feedback for image search. We hypothesize that, most of the clicked images are relevant to the given query judged by the real users.</p><p>By jointly integrating cross-view learning and click-through data, this paper presents a novel Click-through-based Crossview Learning (CCL) approach to image search, as shown in Figure <ref type="figure" target="#fig_1">1</ref>. Specifically, a bipartite graph between the user queries and images is constructed based on the search logs from a commercial image search engine. An edge between a query and an image is established when the users who issued the query clicked the image. Moreover, the textual and visual space is formed by constructing a graph on each view, respectively. The link between every two nodes in each space represents the query or image similarity. The spirit of CCL is to learn a latent subspace in the way of minimizing the distance between the mappings of query and image, while preserving the inherent structure in each original space. After the optimization of subspace learning, the relevance score between a query and an image in the original spaces can be directly computed based on their mappings. For any query, the image search list will be returned by sorting their relevance scores with the query.</p><p>In summary, this paper makes the following contributions:</p><p>• We study the problem of keyword-based image search by jointly exploring cross-view learning and the use of click-through data. To the best of knowledge, this paper represents one of the first efforts towards this target in the information retrieval research community.</p><p>• We propose a novel click-through-based cross-view learning (CCL), which aims to learn a latent subspace by simultaneously minimizing the distance between the mappings of query and image in the latent subspace, and preserving the structure in each original space. By mapping to the subspace, text queries and visual images can be directly compared.</p><p>• We evaluate the proposed click-through based image search approach on a large-scale click-based image dataset with over 23 millions of log records, which were sampled from one-year click data of a commercial image search engine.</p><p>The remaining sections are organized as follows. Section 2 describes related work on multi-view embedding and the use of click data, while Section 3 presents our click-throughbased cross-view learning method. Section 4 provides empirical evaluations, followed by the discussions and conclusions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>We briefly group the related work into two categories: multi-view embedding, and search by using click data. The former draws upon research in integrating multiple views to improve learning performance by exploiting either the consensus or the complementary principle, while the latter investigates Web search by mining click-through data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multi-view Embedding</head><p>The research in this direction has proceeded along three dimensions: co-training <ref type="bibr" target="#b16">[16]</ref>[22] <ref type="bibr" target="#b33">[33]</ref>, subspace learning <ref type="bibr" target="#b2">[2]</ref>[9] <ref type="bibr" target="#b25">[25]</ref>, and multi-kernel learning <ref type="bibr" target="#b5">[5]</ref>[14] <ref type="bibr" target="#b17">[17]</ref>.</p><p>Co-training seeks consensus on two distinct views of the data.  <ref type="bibr" target="#b16">[16]</ref>. The idea of subspace learning is similar to co-training except the consensus is solved by learning a latent subspace shared by multiple views by assuming that the input views are generated from this latent subspace. Canonical correlation analysis (CCA) <ref type="bibr" target="#b9">[9]</ref>, a classical technique, explored the mapping matrices by maximizing the correlation between the projections in the subspace. Similarly, Partial Least Squares (PLS) also aims to model the relations between two or more sets of data by projecting them into the latent subspace <ref type="bibr" target="#b25">[25]</ref>. The difference between CCA and PLS is that CCA utilizes cosine as the similarity function while PLS learns dot product. Later in <ref type="bibr" target="#b2">[2]</ref>, polynomial semantic indexing (PSI) is performed by learning two low-rank mapping matrices in a learning to rank framework, and then a polynomial model is considered to measure the relevance between query and document.</p><p>Different from co-training and subspace learning, multikernel learning exploits different kernels to different views and fuses them either linearly or non-linearly for exploring complementary properties of different views. In <ref type="bibr" target="#b17">[17]</ref>, a linear (or convex) combination of a set of predefined kernels were learned to identify a good target kernel for the applications. Later in <ref type="bibr" target="#b5">[5]</ref>, Kernel target alignment was proposed to learn the entries of a kernel matrix by using the outer product of the label vector as the ground-truth. Kloft et al. extended the multi-kernel learning framework to arbitrary l p -norm by adding a regularizer over the mixing coefficients <ref type="bibr" target="#b14">[14]</ref>.</p><p>In summary, our work belongs to subspace learning. Different from these aforementioned subspace learning methods, our approach contributes by studying not only forming the shared latent subspace with the standard objective of subspace learning (i.e., the consensus between views is maximized) but also preserving the inherent structure in each original space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Search by Using Click Data</head><p>Click-through data has been studied and analyzed widely with different Web mining techniques for improving the efficacy and usability of search engines. The use of the click-  through data for query clustering was suggested by Befferman and Berger <ref type="bibr" target="#b3">[3]</ref>, who proposed an agglomerative clustering technique to identify related queries and Web pages. Wen et al. combined query content information and clickthrough information and applied a density-based method to cluster queries <ref type="bibr" target="#b28">[28]</ref>. Mei et al. proposed an approach to query suggestion by computing the hitting time on a click graph <ref type="bibr" target="#b19">[19]</ref>. Li et al. presented the use of click graphs in improving query intent classifiers <ref type="bibr" target="#b18">[18]</ref>.</p><p>There are also several approaches that have tried to model the representation of queries or documents on the clickthrough bipartite. In <ref type="bibr" target="#b1">[1]</ref>, the authors introduced another vectorial representation for the queries without considering the content information. Queries were represented as points in a high dimensional space, where each dimension corresponds to a unique URL. The weight assigned to each dimension was equal to the click frequency. Poblete et al. proposed the query-set document model by mining frequent query patterns to represent documents rather than the content information of the documents <ref type="bibr" target="#b24">[24]</ref>.</p><p>In addition, click-through data have also been used to learn the rank function <ref type="bibr" target="#b12">[12]</ref>. Joachims et al. observed the relationship between clicked links and the relevance of the target pages by an eye tracking experiment <ref type="bibr" target="#b13">[13]</ref>. Wu et al. formalized the learning of similarity as learning of mappings that maximize the similarities of query-documents pairs from the click-through bipartite graph <ref type="bibr" target="#b30">[30]</ref>. For image search, click-through data has been found to be very reliable <ref type="bibr">[6][11]</ref>. In <ref type="bibr" target="#b6">[6]</ref>, Craswell et al. built a query-image click graph and performed backward random walks to determine a probability distribution over images conditioned on the given query. In <ref type="bibr" target="#b11">[11]</ref>, Jain et al. reranked the image search results so as to promote images that are likely to be clicked to the top of the ranked list. Later in <ref type="bibr" target="#b27">[27]</ref>, an in-depth analysis of several ranking algorithms was performed on Flickr user log data to investigate the importance of many factors, including internal and external image popularity, the overall attentions, diversity, semantic categories and visual appearance. In <ref type="bibr" target="#b23">[23]</ref>, Pan et al. employed neighborhood graph search to find the nearest neighbors on an image similarity graph and further aggregated their clicked queries/click counts to get the labels of the new image. In another work by Yao et al. <ref type="bibr" target="#b32">[32]</ref>, by combining click-through and video document features for deriving a latent subspace, the dot product of the mappings in the latent subspace is taken as the similarity between videos and the similarity is further applied for video tagging tasks.</p><p>Most of the above approaches focus on leveraging both the click data and the features only from the textual view. Our work is different that we aim to compute the distance between the textual query and visual features from two different views on the observed query-image pairs and apply the learned distance for image search purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CLICK-THROUGH-BASED CROSS-VIEW LEARNING</head><p>The main goal of click-through-based cross-view learning is to construct a latent common subspace with the ability of directly comparing textual query and image content. The training of CCL is performed simultaneously by minimizing the distance between query and image mappings in the latent subspace weighted by their clicks, and preserving the structure relationships between the training examples in the original feature space. In particular, the objective function of CCL is composed of two components, i.e., distance between views in the latent subspace, and the structure preservation in the original space. After we obtain the latent subspace, the relevance between query and image is directly measured by their mappings. The approach overview is shown in Figure <ref type="figure" target="#fig_1">1</ref>.</p><p>In the following, we will first define the bipartite graph that naturally encodes user actions in the query log, followed by constructing the two learning components of CCL. Then the joint overall objective and its optimization strategy are provided. Finally, the whole algorithm for image search is presented. It is worth noticing that although the two views here are visual (image) and textual (query), our approach is applicable to any other domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notation</head><p>Let G = (V, E) denote a click-through bipartite. V = Q ∪ V is the set of vertices, which consists of a query set Q and an image set V . E is the set of edges between the query and image vertices. The number associated with the edge represents the clicked times in the image search results of the query. Suppose there are n triads {q i , v i , c i } n i=1 generated from the click-through bipartite in total, where c i is the click counts of image v i in response to query</p><formula xml:id="formula_0">q i . Let Q = {q 1 , q 2 , . . . , q n } ⊤ ∈ R n×d q and V = {v 1 , v 2 , . . . , v n } ⊤ ∈ R n×d v denote</formula><p>the query and image feature matrix, where q i and v i are the textual and visual feature of query q i and image v i , and d q and d v are the feature dimensionality, respectively. The click matrix C is a diagonal n × n matrix with its diagonal elements as c i . Please note that the query q i and image v i may not be unique in each view as one single query can correspond to multiple clicked images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cross-view Distance</head><p>We assume that a low-dimensional common subspace exists for the representation of query and image. The linear mapping function can be derived from this subspace by</p><formula xml:id="formula_1">f (q i ) = q i W q , and f (v i ) = v i W v ,<label>(1)</label></formula><p>where d is the dimensionality of the common subspace, and W q ∈ R d q ×d and W v ∈ R d v ×d are the transformation matrices that project the query textual semantics and image content into the common subspace, respectively.</p><p>To measure the relations between the textual query and image visual content, one natural way is to measure the distance between their mappings in the latent subspace as min</p><formula xml:id="formula_2">W q ,W v tr (QW q -VW v ) ⊤ C(QW q -VW v ) s.t. W ⊤ q W q = I, W ⊤ v W v = I<label>(2)</label></formula><p>where tr(•) denotes the trace function. The matrices W q and W v have orthogonal columns, i.e., W ⊤ q W q = W ⊤ v W v = I, where I is an identity matrix. The constrains restrict W q and W v to converge to reasonable solutions rather than go to 0 which is meaningless in practice.</p><p>Specifically, we view the click number of a query and an image as an indicator of their relevance. As most image search engines display results as thumbnails. The users can see the entire image before clicking on it. As such, barring distracting images and intent changes, users predominantly tend to click on images that are relevant to their query. Therefore, click data can serve as a reliable connection between the queries and images. The underlying assumption is that the higher the click number, the smaller the distance between the query and the image in the latent subspace. To learn this shared latent subspace, we intuitively incorporate the distance as a regularization on the mapping matrices W q and W v weighted by the click numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Structure Preservation</head><p>Structure preservation or manifold regularization has been shown effective for semi-supervised learning <ref type="bibr" target="#b21">[21]</ref> and multi-view learning <ref type="bibr">[7]</ref>. This regularizer indicates that similar points in the original space should be mapped to the positions closely in the shared latent subspace. The estimation of the underlying structure can be measured by the appropriate pairwise similarity between the training samples. Specifically, it can be given by n i,j=1</p><formula xml:id="formula_3">S q ij q i W q -q j W q 2 + n i,j=1 S v ij v i W v -v j W v 2 ,</formula><p>(3) where S q ∈ R n×n and S v ∈ R n×n denote the affinity matrices defined on the queries and images, respectively. Under the structure preservation criterion, it is reasonable to minimize Eq.( <ref type="formula">3</ref>), since it will incur a heavy penalty if two similar examples are mapped far away.</p><p>There are many ways of defining the affinity matrices S q and S v . Inspired by <ref type="bibr">[7]</ref>, the elements are computed by Gaussian functions in this work, i.e.,</p><formula xml:id="formula_4">S t ij =    e - t i -t j 2 σ 2 t if t i ∈ N k (t j ) or t j ∈ N k (t i ) 0 otherwise ,<label>(4)</label></formula><p>where t ∈ {q, v} for simplicity, i.e., t can be replaced by any one of q and v. σ t is the bandwidth parameters. N k (t i ) represents the set of k nearest neighbors of t i .</p><p>By defining the graph Laplacian L t = D t -S t for t ∈ {q, v}, where D t is a diagonal matrix with its elements defined as D t ij = j S t ij , Eq.( <ref type="formula">3</ref>) can be rewritten as</p><formula xml:id="formula_5">tr (QW q ) ⊤ L q (QW q ) + tr (VW v ) ⊤ L v (VW v ) .<label>(5)</label></formula><p>By minimizing this term, the similarity between examples in the original space can be preserved in the learned latent subspace. Therefore, we add this regularizer in our framework for optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Overall Objective</head><p>The overall objective function integrates the distance between views in Eq.( <ref type="formula" target="#formula_2">2</ref>) and structure preservation in Eq.( <ref type="formula" target="#formula_5">5</ref>). Hence we get the following optimization problem min</p><formula xml:id="formula_6">W q ,W v tr (QW q -VW v ) ⊤ C(QW q -VW v ) +λ tr (QW q ) ⊤ L q (QW q ) + tr (VW v ) ⊤ L v (VW v ) s.t. W ⊤ q W q = I, W ⊤ v W v = I , (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>where λ is the tradeoff parameter. The first term is the crossview distance, while the second term represents structure preservation.</p><p>For simplicity, we denote L(W q , W v ) as the objective function in Eq.( <ref type="formula" target="#formula_6">6</ref>). Thus, the optimization problem can be rewritten as min</p><formula xml:id="formula_8">{W q ,W v } L(W q , W v ), s.t. W ⊤ q W q = I, W ⊤ v W v = I.<label>(7)</label></formula><p>The optimization above is a non-convex problem. Nevertheless, the gradient of the objective function with respect to W q and W v can be easily obtained as follows:</p><formula xml:id="formula_9">∇ W q L(W q , W v ) = 2Q ⊤ C(QW q -VW v ) + 2λQ ⊤ L q QW q ∇ W v L(W q , W v ) = 2V ⊤ C(VW v -QW q ) + 2λV ⊤ L v VW v .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Optimization</head><p>To address the difficult non-convex problem in Eq.( <ref type="formula" target="#formula_8">7</ref>) due to the orthogonal constrains, we use a gradient descent optimization procedure with curvilinear search <ref type="bibr" target="#b29">[29]</ref> for a local optimal solution in this work.</p><p>In each iteration of the gradient descent procedure, given the current feasible mapping matrices {W q ,W v } and their corresponding gradients</p><formula xml:id="formula_10">{G q = ∇ W q L(W q ,W v ), G v = ∇ W v L(W q ,W v )},</formula><p>we define the skew-symmetric matrices P q and P v as</p><formula xml:id="formula_11">P q = G q W ⊤ q -W q G ⊤ q , P v = G v W ⊤ v -W v G ⊤ v . (9)</formula><p>The new point can be searched as a curvilinear function of a step size τ , such that</p><formula xml:id="formula_12">F q (τ ) = (I + τ 2 P q ) -1 (I - τ 2 P q )W q , F v (τ ) = (I + τ 2 P v ) -1 (I - τ 2 P v )W v .<label>(10)</label></formula><p>Then, it is easy to verify that F q (τ ) and F v (τ ) lead to several characteristics. The matrices F q (τ ) and F v (τ ) satisfy (F q (τ )) ⊤ F q (τ ) = (F v (τ )) ⊤ F v (τ ) = I for all τ ∈ R. The derivatives with respect to τ are given as</p><formula xml:id="formula_13">F ′ q (τ ) = -(I + τ 2 P q ) -1 P q ( W q +F q (τ ) 2 ) F ′ v (τ ) = -(I + τ 2 P v ) -1 P v ( W v +F v (τ ) 2 ) .<label>(11)</label></formula><p>In particular, we can obtain F q ′ (0) = -P q W q and F v ′ (0) = -P v W v . Then, {F q (τ ), F v (τ )} τ ≥0 is a descent curve. We use the classical Armijo-Wolfe based monotone curvilinear search algorithm <ref type="bibr" target="#b26">[26]</ref> to determine a suitable step τ as one satisfying the following conditions:</p><formula xml:id="formula_14">L(F q (τ ), F v (τ )) ≤ L(F q (0), F v (0)) +ρ 1 τ L τ ′ (F q (0), F v (0)), L τ ′ (F q (τ ), F v (τ )) ≥ ρ 2 L τ ′ (F q (0), F v (0)),<label>(12)</label></formula><p>where ρ 1 and ρ 2 are two parameters satisfying 0 &lt; ρ 1 &lt; ρ 2 &lt; 1. L τ ′ (F q (τ ), F v (τ )) is the derivative of L with respect to τ and is calculated by</p><formula xml:id="formula_15">L τ ′ (F q (τ ), F v (τ )) = - t∈{q,v} tr R t (τ ) ⊤ (I + τ 2 P t ) -1 P t W t + F t (τ ) 2 ,<label>(13)</label></formula><p>where R t (τ ) = ∇ W t L(F q (τ ), F v (τ )) for t ∈ {q, v}. In particular, we have</p><formula xml:id="formula_16">L τ ′ (F q (0), F v (0)) = - t∈{q,v} tr G ⊤ t (G t W ⊤ t -W t G ⊤ t )W t = - 1 2 P q 2 F - 1 2 P v 2 F . (<label>14</label></formula><formula xml:id="formula_17">)</formula><p>Please refer to <ref type="bibr" target="#b29">[29]</ref> for the theoretical proof details of curvilinear search algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">CCL Algorithm</head><p>After the optimization of W q and W v , we can obtain the linear mapping functions defined in Eq. <ref type="bibr" target="#b1">(1)</ref>. With this, original incomparable textual query and visual image become comparable. Specifically, given a test query-image pair (q ∈ R d q , v ∈ R d v ), we compute the distance value between Algorithm 1 Click-through-based Cross-view Learning (C-CL)</p><p>1: Input: 0 &lt; µ &lt; 1, 0 &lt; ρ 1 &lt; ρ 2 &lt; 1, ε ≥ 0, and initial W q and W q . 2: for iter = 1 to T max do 3: compute gradients G q and G v via Eq.( <ref type="formula" target="#formula_9">8</ref>). 4:</p><formula xml:id="formula_18">if G q 2 F + G v 2 F ≤ ε then 5:</formula><p>exit. 6: end if 7:</p><p>compute P q and P v by using Eq.( <ref type="formula">9</ref>). 8:</p><p>compute L τ ′ (F q (0), F v (0)) according to Eq.( <ref type="formula" target="#formula_16">14</ref>). 9:</p><p>set τ = 1. 10: repeat 11: τ = µτ 12:</p><p>compute F q (τ ) and F v (τ ) via Eq.( <ref type="formula" target="#formula_12">10</ref>). 13: compute L τ ′ (F q (τ ), F v (τ )) via Eq.( <ref type="formula" target="#formula_15">13</ref>). 14:</p><p>until Armijo-Wolfe conditions in Eq.( <ref type="formula" target="#formula_14">12</ref>) are satisfied 15:</p><p>update the transformation matrices:</p><formula xml:id="formula_19">W q = F q (τ ) W v = F v (τ ) 16: end for 17: Output: distance function: ∀q, v, r(q, v) = qW q -vW v 2 .</formula><p>the pair as</p><formula xml:id="formula_20">r(q, v) = qW q -vW v 2 .<label>(15)</label></formula><p>This value reflects how relevant the query could be used to describe the given image, with lower numbers indicating higher relevance. For any query, sorting by its corresponding values for all its associated images gives the retrieval ranking for these images. The algorithm is given in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Complexity Analysis</head><p>The time complexity of CCL mainly depends on the computation of G q , G v , P q , P v , F q (τ ), F v (τ ), and L τ ′ (F q (τ ), F v (τ )). Obviously, the computation complexity of G q and G v is O(n 2 × d q ) and O(n 2 × d v ), respectively. P q and P v take O(d 2 q × d) and O(d 2 v × d). The matrix inverse (I + τ 2 P q ) -1 and (I + τ 2 P v ) -1 dominate the computation of F q (τ ) and F v (τ ) in Eq. <ref type="bibr" target="#b10">(10)</ref>. By forming P q and P v as the outer product of two low-rank matrices, the inverse computation cost decreases a lot. As defined in Eq.( <ref type="formula">9</ref>),</p><formula xml:id="formula_21">P q = G q W ⊤ q -W q G ⊤ q and P v = G v W ⊤ v - W v G ⊤</formula><p>v , P q and P v can be equivalently rewritten as</p><formula xml:id="formula_22">P q = X q Y ⊤ q and P v = X v Y ⊤ v , where X q = [G q , W q ], Y q = [W q , -G q ] and X v = [G v , W v ], Y v = [W v , -G v ]. Accord- ing to Sherman-Morrison-Woodbury formula, i.e., (A + αXY ⊤ ) -1 = A -1 -αA -1 X(I + αY ⊤ A -1 X) -1 Y ⊤ A -1 ,</formula><p>the matrix inverse (I + τ 2 P q ) -1 can be re-expressed as</p><formula xml:id="formula_23">(I + τ 2 P q ) -1 = I - τ 2 X q (I + τ 2 Y ⊤ q X q ) -1 Y ⊤ q .</formula><p>Furthermore, F q (τ ) can be rewritten as</p><formula xml:id="formula_24">F q (τ ) = W q -τ X q (I + τ 2 Y ⊤ q X q ) -1 Y ⊤ q W q .</formula><p>For F v (τ ), we can get the corresponding conclusion. Since we typically have d ≪ d q , the cost of inverting (I + τ 2 Y ⊤ q X q ) ∈ R 2d×2d is much lower than inverting (I+ τ 2 P q ) ∈ R d q ×d q . The inverse of (I + τ 2 Y ⊤ q X q ) -<ref type="foot" target="#foot_0">1</ref> takes O(d 3 ), thus the computation complexity of</p><formula xml:id="formula_25">F q (τ ) is O(d q d 2 ) + O(d 3 ). Similarly, F v (τ ) is O(d v d 2 ) + O(d 3 ). The computation of L τ ′ (F q (τ ), F v (τ )) has a cost of O(n 2 × d q ) + O(n 2 × d v ) + O(d q d 2 ) + O(d v d 2 ) + O(d 3 ). As d ≪ d q , d v ≪ n, the overall complexity of the Al- gorithm 1 is T max × T × O(n 2 × max(d q , d v ))</formula><p>, where T is the number of searching for appropriate τ which satisfies the Armijo-Wolfe conditions and it is usually less than ten in our experiments. Take the training of W q and W v on one million {query, image, click} triads with d v = 1, 024 and d q = 10, 000 for example, our algorithm takes about 32 hours on a server with 2.40GHz CPU and 128GB RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Extensions</head><p>Although we only present the distance function between query and image on the learned mapping matrices in the Algorithm 1, the optimization actually can also help learning of query-query and image-image distance. Similar to the distance function between query and image, the distance between query and query, image and image, is computed as (∀q, q, r(q, q) = qW q -qW q 2 ) and (∀v, v, r(v, v) = vW v -vW v 2 ), respectively. Furthermore, the obtained distance can be applied for several IR applications, e.g., query suggestion, query expansion, image clustering, image classification, and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>We conducted our experiments on the Clickture dataset <ref type="bibr" target="#b10">[10]</ref> and evaluated our approaches for image search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>The dataset, Clickture, is a large-scale click based image dataset <ref type="bibr" target="#b10">[10]</ref>. It was collected from one year click-through data of one commercial image search engine. The dataset comprises two parts, i.e., the training and development (dev) sets. The training set consists of 23.1 million {query, image, click} triads, where query is a textual word or phrase, image is a base64 encoded JPEG image thumbnail, and click is an integer which is no less than one. There are 11.7 millions distinct queries and 1.0 million unique images of the training set. Figure <ref type="figure" target="#fig_2">2</ref> shows a few exemplary images with their clicked queries and click counts in the Clickture. For example, users clicked the first image 146 times in the search results when submitting query "obama" in total. It is worth noting that there is no surrounding text or description of images provided in the Clickture.</p><p>In the dev dataset, there are 79,926 query, image pairs generated from 1,000 queries, where each image to the corresponding query was manually annotated on a three point ordinal scale: Excellent, Good, and Bad. In the experiments, the training set is used for learning the latent subspace, while the dev set is used for performance evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Settings</head><p>Task. We investigate whether our proposed approach can be used to improve image search in this work. Specifically, we use Clickture as "labeled" data for semantic queries and train the ranking model. The task is to estimate the relevance of the image and the query for each test query-image pair, and then for each query, we order the images based on the prediction scores returned by our trained ranking model.</p><p>Textual and Visual Features. We take the word in queries as "word features." Words are stemmed and stop words are removed. With word features, each query is represented by a tf vector in the query space. In our experiments, we use the top 10,000 most frequent words as the word vocabulary. Inspired by the success of deep neural networks (DNN) <ref type="bibr" target="#b4">[4]</ref>, we use it to generate image representation in this work, which is a 1024-dimensional feature vector. Specifically, similar to <ref type="bibr" target="#b15">[15]</ref>, the used DNN architecture is denoted as Image -C64 -P -N -C128 -P -N -C192 -C192 -C128 -P -F 4096 -F 1024 -F 1000, which contains five convolutional layers (denoted by C following the number of filters) while the last three are fully-connected layers (denoted by F following the number of neurons); the max-pooling layers (denoted by P ) follow the first, second and fifth convolutional layers; local contrast normalization layers (denoted by N ) follow the first and second max-pooling layers. The weights of DNN are learned on ILSVRC-2010 1 , which is a subset of ImageNet<ref type="foot" target="#foot_1">2</ref> dataset with 1.26 million training images from 1,000 categories. For an image, its representation is the neuronal responses of the layer F 1024 by input the image into the learned DNN.</p><formula xml:id="formula_26">¡ ¢£¢¥ §¤¦ ¡ ¢£¢ ¨©¥ ¢¢ ¡ ¢£¢¥ ¤ ¨© ¡ ¢¢ ¡ ¢£¢¥ §! ¨"¡¡ ¡ ¡ ¢£¢ # ¢©$%¥ § ¨"¡¡ ¡ ¢£¢¥ § ¨© ¡ ¡ ¢£¢¥ ! ¢&amp; ¢¡$ ¢¨¨©' ©¨¥ § (%¢$ ¢¨¨©'¥ § ¢$&amp; ¢¨¨©'¥ ¡¡# ¢¨¨©'¥ § "¡(©#¢&amp; ¨¢&amp; ¡¡# ¢$# ¢¨¨©'¥ § ¢¡$ ¢¨¨©' ©¨¥ § ¢% ¢¥ § ¢%¥ ¦ ¢% ¢¥ §) ¢% ¢ #©$&amp; ¨©¥ § ¢% ¢ £¡0©¥ § ¢% ¡£ ¢¥ ¤ ( ¡$ 1¢£¥ §! ( ¡$ 1¢£ #¢2©$%¥ § ( ¡$ 1¢£ "¡¥ § ( ¡$ 1¢£ ©£¢%¥ ( ¡$ 1¢£ ¨©¥ 5 ( ¡$ 1¢£ ¨(¢&amp;©$% ¢((¥ § " #(( 2©¡$©$¥ § ¡¨ 2©¡$©$ #((¥ § 2© #((¥ § 2©¡$©$ #((¥ §) 2©¡$©$ #(( ¨¡¥ § 2©¡$©$#((¥ § "©£¢ ¡¡©¥ 3) "©£¢ ¡¡© ¡ ©#¥ "©£¢ ¡¡© ©£¢%¥ ! "©£¢ ¡¡© ¨©¥ § "©£¢ ¨©¥ ¦¦</formula><p>Compared Approaches. We compare the following approaches for performance evaluation:</p><p>• N-Gram SVM Modeling (N-Gram SVM). We use all the clicked images of a given query as positive samples and randomly select negative samples from the rest of the training dataset to build a support vector machine (SVM) model for each query, and then use this model to predict the relevance of the query to a new image.</p><p>In addition, in order to extend the capability of the training data to model queries that are not covered in the dataset, n-gram modeling, which attempts to model each n-gram as a "query," is used. In other words, Figure <ref type="figure">3</ref>: The average overall objective value of Eq. ( <ref type="formula" target="#formula_6">6</ref>) for each query-image pair with the increase of the iteration. The changes of the value are given at different dimensionality of the latent subspace.</p><p>if a query is not in the training set, but its n-grams appear in some queries of the training set, we can generate the model by linearly fusing the SVM models of these queries. No latent subspace is learned in this baseline. We name this run as N-Gram SVM.</p><p>• Canonical Correlation Analysis <ref type="bibr" target="#b8">[8]</ref>[9] (CCA). A classical and successful approach for mapping visual and textual features into a latent subspace where the correlation between the two views is maximized. This run is named as CCA.</p><p>• Partial Least Squares <ref type="bibr" target="#b25">[25]</ref>[30] (PLS). Similar to CCA, PLS aims to learn linear mapping functions to project two views into a common latent subspace as well. But different from CCA, PLS learns dot product as the similarity function while cosine similarity is used in C-CA. Deriving from the ideas in <ref type="bibr" target="#b30">[30]</ref>, the learning of the mappings is performed by maximizing the similarities of the observed query-image pairs on the click-through data here. We name this run as PLS.</p><p>• Polynomial Semantic Indexing <ref type="bibr" target="#b2">[2]</ref>[32] (PSI). Similar in spirit, PSI first chooses a low dimensional feature representation space for query and image, and then a polynomial model is discriminatively learned for mapping the query-image pair to a relevance score. This run is named as PSI.</p><p>• Click-through-based Cross-view Learning (CCL). We designed the run, CCL, for our proposed approach described in Algorithm 1.</p><p>Parameter Settings. N-Gram SVM is a baseline without low-dimensional latent subspace learning, thus the relevance score is predicted on the original visual features. For the other four subspace learning methods, the dimensionality of the latent subspace is in the range of {40, 80, 120, 160}. The k nearest neighbors preserved in Eq.( <ref type="formula" target="#formula_4">4</ref>) is chosen within {100, 500, 1000, 1500, 2000}. The tradeoff parameter λ in the overall objective function is set within {0.1, 0.2, ..., 1.0}. We set µ=0.3, ρ 1 =0.2, and ρ 2 =0.9 in the curvilinear search by using a validation set.</p><p>Evaluation Metrics. For the evaluation of image search, we adopted Normalized Discounted Cumulative Gain (N DCG) which takes into account the measure of multi-level relevancy as the performance metric. Given an image ranked list, the N DCG score at the depth of d in the ranked list is defined by:</p><formula xml:id="formula_27">N DCG@d = Z d d j=1 2 r j -1 log(1 + j)<label>(16)</label></formula><p>where r j = {Excellent = 3, Good = 2, Bad = 0} is the manually judged relevance for each image with respect to the query. Z d is a normalizer factor to make the score for d Excellent results 1. The final metric is the average of N DCG@d for all queries in the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Optimization Analysis</head><p>As we choose the step τ satisfying the Armijo-Wolfe conditions to achieve an approximate minimizer of L(F q (τ ), F v (τ )) in Algorithm 1 instead of finding the global minimization due to its computationally expense, we depict the average overall objective value of Eq.( <ref type="formula" target="#formula_6">6</ref>) for one query-image pair versus iterations to illustrate the convergence of the algorithm. As shown in Figure <ref type="figure">3</ref>, the value does decrease as the iterations increase at all the dimensionality of the latent subspace. Specifically, after 100 iterations, the average objective value between query mapping and image projection is around 10 when the latent subspace dimension is 40. Thus, the experiment verifies that our algorithm can always reach a reasonable local optimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance Comparison</head><p>Figure <ref type="figure" target="#fig_4">4</ref> shows the NDCG performances on image search of five runs averaged over 1,000 queries in Clickture dev dataset. It is worth noting that the prediction of N-Gram SVM is performed on the original image visual features of 1,024 dimensions and for other four methods, the performances are given by choosing 80 as the dimensionality of the latent subspace.</p><p>Overall, our proposed CCL consistently outperforms the other runs across different depths of NDCG. In particular, the NDCG@10 of CCL can achieve 0.5738, which makes the improvement over N-Gram SVM model by 4.0%. More importantly, by learning a low-dimensional latent subspace, the dimension of the mappings of textual query and visual image is reduced by several orders of magnitude. Furthermore, CCL by additionally incorporating structure preservation leads to a performance boost against PLS and CCA. The result basically indicates the advantage of minimizing distance between views in the latent subspace and preserving similarity in the original space simultaneously.  "golden anchor cabins," "women bicycle," and "pumpkin faces" (better viewed in color). The relevance scale is provided at the top left corner for each image.</p><p>There is a performance gap between CCA and PLS. Though both runs attempt to learn linear mapping functions for forming a subspace, they are different in the way that C-CA learns cosine as a similarity function, and PLS learns dot product instead. As indicated by our results, maximizing the correlation between the mappings in the latent subspace can lead to a better performance. Moreover, PSI utilizing click-through data as relative relevance judgements rather than absolute click numbers is superior to PLS, but is still lower than CCL. Another observation is that the performance gain is almost consistent when going deeper into the list. This further confirms the effectiveness of CCL.</p><p>Figure <ref type="figure" target="#fig_5">5</ref> shows the top 10 image search results by different approaches for the query "mustang cobra," "golden anchor cabins," "women bicycle," and "pumpkin faces." We can easily see the proposed CCL method gets the most satisfying ranking results. Specifically, compared to other baselines, the top images by CCL are more visually similar to each other, especially of the query "women bicycle" and "pumpkin faces." That is mainly caused by the effect of structure preservation regularization term in the overall objective, which restricts the similar images in the original space to remain close in the low-dimensional latent subspace. Therefore, the ranks of these group of images are likely to be moved up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effect of the Dimensionality of the Latent Subspace</head><p>In order to show the relationship between the performance and the dimensionality of the latent subspace, we compared the results of the dimension in the range of 40, 80, 120, and 160. As the method N-Gram SVM performs training and prediction by only using the original features rather than learning a latent subspace, it is excluded in this comparison.</p><p>The results are shown in Figure <ref type="figure" target="#fig_6">6</ref>. Compared to the other three runs, performance improvement is consistently observed at each dimensionality of the latent subspace by our proposed CCL method. Furthermore, CCL achieves the best result at the latent subspace dimensionality of 80, and the results at other dimensionality are pretty close to the best one. This observation basically verifies that CCL has a good property of being affected very slightly with the change of the dimensionality of the latent subspace. Another important observation is that when the dimensionality of the latent subspace increases, the performances of all the methods are not always improved accordingly. For example, the best performance of CCL happens at the dimensionality of 80 and for the method CCA, the highest NDCG@25 is observed at the dimensionality of 40. This somewhat indicates a general conclusion that the selection of the latent subspace dimensionality is related to the optimized objective considered in learning the subspace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Effect of the Number of Nearest Neighbors</head><p>The number of nearest neighbors considered in the structure preservation is another parameter in CCL. In the previous experiments, the number was fixed to 2,000. Next, we conducted experiments to evaluate the performance of our CCL method with the number of nearest neighbors in range of {100, 500, 1000, 1500, 2000} at different dimensionality of the latent subspace.</p><p>The NDCG@25 with the different number of nearest neighbors are shown in Figure <ref type="figure" target="#fig_7">7</ref>. As illustrated in the figure, the optimal k differs at different dimensionality of the latent subspace. However, at each dimensionality of the latent subspace, the performance difference by using different number of nearest neighbors is within 0.0002, which softens the difficulty on choosing the optimal number of nearest neighbors in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Effect of the Parameter λ</head><p>A common problem with multiple regularization terms in a joint optimization objective is the need to set the parameters to tradeoff each component. In the previous experiments, the tradeoff λ is optimally set in order to examine the performance of CCL on image search irrespective of the parameter influence. We further conducted experiments to test the sensitivity of λ towards search performance.</p><p>Figure <ref type="figure" target="#fig_8">8</ref> shows the NDCG@25 performance with respect to different values of λ at different dimensionality of the latent subspace. Similar to the effect of the number of nearest neighbors, we can see that the performance curve is very smooth when λ varies in a range from {0.1, 0.2, ..., 1.0} at each dimension of the latent subspace. Specifically, when the dimension of the latent subspace is 80, the performance fluctuates within the range of 0.001. Thus, the performance is not sensitive to the change of the tradeoff parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DISCUSSION AND CONCLUSION</head><p>In this paper, we have investigated the issue of directly learning the multi-view distance between a textual query and an image by leveraging both click data and subspace learning techniques. The click data represent the click relations between queries and images, while the subspace learning aims to learn a latent common subspace between multiple views. We have proposed a novel click-through-based cross-view learning to solve the problem in a principle way. Specifically, we use two different linear mappings to project textual queries and visual images into a latent subspace. The mappings are learned by jointly minimizing the distance of the observed query-image pairs on the click-through bipartite graph and preserving the inherent structure in original single view. Moreover, we make orthogonal assumptions on the mapping matrices. Then the mappings can be obtained efficiently through curvilinear search. We take l 2 norm between the projections of query and image in the latent subspace as the distance function to measure the relevance of a pair of (query, image).</p><p>Our future works are as follows. First, the two learned mapping matrices can be extended to the learning of queryquery and image-image distances. Next, the learned distances will be further explored for applications such as query expansion, query suggestion, and image clustering, in the learned low-dimensional space. Furthermore, we will investigate the kernel version of our method, making it applicable when kernel matrices instead of features are available.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Click-through-based image search framework (better viewed in color). (a) Latent subspace learning between textual query and visual image: click-through-based cross-view learning by simultaneously minimizing the distance between the query and image mappings in the latent subspace (weighted by their clicks) and preserving the inherent structure in each original feature space. (b) With the learned mapping matrices W q and W v , queries and images are projected into this latent subspace and then the distance in the latent subspace is directly taken as the relevance of query-image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples in Clickture dataset (upper row: clicked images; lower row: search query with click times on the upper image).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The NDCG of different approaches for image search. The numbers in the brackets represent the feature dimension used in each approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Examples showing the top 10 image search results by different methods of queries "mustang cobra," "golden anchor cabins," "women bicycle," and "pumpkin faces" (better viewed in color). The relevance scale is provided at the top left corner for each image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: The NDCG@25 performance with different dimensionalities of the latent subspace. We can see that CCL achieves the best performance among the four methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The NDCG@25 performance curve at different dimensionalities of the latent subspace with different numbers of nearest neighbors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The NDCG@25 performance curve at different dimensionalities of the latent subspace with different λ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Muslea et al. combined active learning with cotraining and proposed robust semi-supervised learning algorithms [22]. Yu et al. developed a Bayesian undirected graphical model for co-training and a novel co-training kernel for Gaussian process classifiers [33]. Kumar et al. advanced co-training for data clustering and designed effective algorithms for multi-view data</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www.image-net.org/challenges/LSVRC/2010/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://www.image-net.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGMENTS</head><p>This work was partially supported by the National Natural Science Foundation of China (No. 61390514, No. 61272290), the Fundamental Research Funds for the Central Universities (No. WK2100060011), and the Shenzhen Research Institute, City University of Hong Kong.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Extracting semantic relations from query logs</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tiberi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Conference on Knowledge Discovery and Data Mining</title>
		<meeting>ACM Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Polynomial semantic indexing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sadamasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Agglomerative clustering of a search engine query log</title>
		<author>
			<persName><forename type="first">D</forename><surname>Beeferman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Conference on Knowledge Discovery and Data Mining</title>
		<meeting>ACM Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The neural representation benchmark and its evaluation on brain and machine</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Cadieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Majaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Two-stage learning kernel algorithms</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Random walks on the click graph</title>
		<author>
			<persName><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Conference on Research and Development in Information Retrieval</title>
		<meeting>ACM Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discriminative feature selection for multi-view cross-domain learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Conference of Information and Knowledge Management</title>
		<meeting>ACM Conference of Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A multi-view embedding space for modeling internet images, tags, and their semantics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">106</biblScope>
			<biblScope unit="page" from="210" to="233" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis: An overview with application to learning methods</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hardoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2639" to="2664" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Clickage: Towards bridging semantic and intent gaps via mining click logs of search engines</title>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Multimedia</title>
		<meeting>ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to re-rank: Query-dependent image re-ranking using click data</title>
		<author>
			<persName><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International World Wide Web Conference</title>
		<meeting>International World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Optimizing search engines using clickthrough data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Conference on Knowledge Discovery and Data Mining</title>
		<meeting>ACM Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Evaluating the accuracy of implicit feedback from clicks and query reformulations in web search</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Granka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hembrooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Information Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Evaluating search engines by modeling the relationship between relevance and clicks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Brefeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sonnenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Efficient and accurate l p -norm multiple kernel learning</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Co-regularized multi-view spectral clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning the kernel matrix with semidefinite programming</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="27" to="72" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning query intent from regularized click graphs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Conference on Research and Development in Information Retrieval</title>
		<meeting>ACM Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Query suggestion using hitting time</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Conference of Information and Knowledge Management</title>
		<meeting>ACM Conference of Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Search Reranking: A Literature Survey. ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014-09">Sept. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Laplacian support vector machines trained in the primal</title>
		<author>
			<persName><forename type="first">S</forename><surname>Melacci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1149" to="1184" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Active learning with multiple views</title>
		<author>
			<persName><forename type="first">I</forename><surname>Muslea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Minton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Knoblock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="203" to="233" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image search by graph-based label propagation with image representation from dnn</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Multimedia</title>
		<meeting>ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Query-sets: using implicit feedback and query patterns to organize web documents</title>
		<author>
			<persName><forename type="first">B</forename><surname>Poblete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Baeza-Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International World Wide Web Conference</title>
		<meeting>International World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Overview and recent advances in partial least squares</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rosipal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Krämer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Subspace, Latent Structure and Feature Selection</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="34" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Optimization theory and methods: nonlinear programming</title>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-X</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>springer</publisher>
			<biblScope unit="volume">98</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image ranking based on user browsing behavior</title>
		<author>
			<persName><forename type="first">M</forename><surname>Trevisiol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chiarandini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Aiello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaimes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Conference on Research and Development in Information Retrieval</title>
		<meeting>ACM Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Clustering user queries of a search engine</title>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International World Wide Web Conference</title>
		<meeting>International World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A feasible method for optimization with orthogonality constrains</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">142</biblScope>
			<biblScope unit="page" from="397" to="434" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning query and document similarities from click-through bipartite graph with metadata</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Conference on Web Search and Data Mining</title>
		<meeting>ACM Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A survey on multi-view learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<idno>CoRR abs/1304.5634</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Annotation for free: Video tagging by mining user search behavior</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Multimedia</title>
		<meeting>ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bayesian co-training</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krishnapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rosales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="2649" to="2680" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
