<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Planning with Diffusion for Flexible Behavior Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Michael</forename><surname>Janner</surname></persName>
							<email>janner@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yilun</forename><surname>Du</surname></persName>
							<email>yilundu@mit.edu</email>
							<affiliation key="aff1">
								<orgName type="department">MIT</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">MIT</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Planning with Diffusion for Flexible Behavior Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Model-based reinforcement learning methods often use learning only for the purpose of estimating an approximate dynamics model, offloading the rest of the decision-making work to classical trajectory optimizers. While conceptually simple, this combination has a number of empirical shortcomings, suggesting that learned models may not be well-suited to standard trajectory optimization. In this paper, we consider what it would look like to fold as much of the trajectory optimization pipeline as possible into the modeling problem, such that sampling from the model and planning with it become nearly identical. The core of our technical approach lies in a diffusion probabilistic model that plans by iteratively denoising trajectories. We show how classifier-guided sampling and image inpainting can be reinterpreted as coherent planning strategies, explore the unusual and useful properties of diffusion-based planning methods, and demonstrate the effectiveness of our framework in control settings that emphasize longhorizon decision-making and test-time flexibility.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Planning with a learned model is a conceptually simple framework for reinforcement learning and data-driven decision-making. Its appeal comes from employing learning techniques only where they are the most mature and effective: for the approximation of unknown environment dynamics in what amounts to a supervised learning problem. Afterwards, the learned model may be plugged into classical trajectory optimization routines <ref type="bibr" target="#b55">(Tassa et al., 2012;</ref><ref type="bibr" target="#b45">Posa et al., 2014;</ref><ref type="bibr" target="#b29">Kelly, 2017)</ref>, which are similarly wellunderstood in their original context. However, this combination rarely works as described. Because powerful trajectory optimizers exploit learned models, plans generated by this procedure often look more like adversarial examples than optimal trajectories <ref type="bibr" target="#b53">(Talvitie, 2014;</ref><ref type="bibr" target="#b28">Ke et al., 2018)</ref>. As a result, contemporary modelbased reinforcement learning algorithms often inherit more from model-free methods, such as value functions and policy gradients <ref type="bibr" target="#b57">(Wang et al., 2019)</ref>, than from the trajectory optimization toolbox. Those methods that do rely on online planning tend to use simple gradient-free trajectory optimization routines like random shooting <ref type="bibr" target="#b38">(Nagabandi et al., 2018)</ref> or the cross-entropy method <ref type="bibr" target="#b4">(Botev et al., 2013;</ref><ref type="bibr" target="#b8">Chua et al., 2018)</ref> to avoid the aforementioned issues.</p><p>In this work, we propose an alternative approach to datadriven trajectory optimization. The core idea is to train a model that is directly amenable to trajectory optimization, in the sense that sampling from the model and planning with it become nearly identical. This goal requires a shift in how the model is designed. Because learned dynamics models are normally meant to be proxies for environment dynamics, improvements are often achieved by structuring the model according to the underlying causal process <ref type="bibr" target="#b3">(Bapst et al., 2019)</ref>. Instead, we consider how to design a model in line with the planning problem in which it will be used. For example, because the model will ultimately be used for planning, action distributions are just as important as state dynamics and long-horizon accuracy is more important than single-step error. On the other hand, the model should remain agnostic to reward function so that it may be used in multiple tasks, including those unseen during training. Finally, the model should be designed so that its plans, and not just its predictions, improve with experience and are resistant to the myopic failure modes of standard shootingbased planning algorithms.</p><p>We instantiate this idea as a trajectory-level diffusion probabilistic model <ref type="bibr" target="#b49">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b23">Ho et al., 2020)</ref> called Diffuser, visualized in Figure <ref type="figure" target="#fig_0">2</ref>. Whereas standard model-based planning techniques predict forward in time autoregressively, Diffuser predicts all timesteps of a plan simultaneously. The iterative sampling process of diffusion models leads to flexible conditioning, allowing for auxiliary guides to modify the sampling procedure to recover trajectories with high return or satisfying a set of constraints. This formulation of data-driven trajectory optimization has several appealing properties:</p><p>Long-horizon scalability Diffuser is trained for the accuracy of its generated trajectories rather than its singlestep error, so it does not suffer from the compounding rollout errors of single-step dynamics models and scales more gracefully with respect to long planning horizon.</p><p>Task compositionality Reward functions provide auxiliary gradients to be used while sampling a plan, allowing for a straightforward way of planning by composing multiple rewards simultaneously by adding together their gradients.</p><p>Temporal compositionality Diffuser generates globally coherent trajectories by iteratively improving local consistency, allowing it to generalize to novel trajectories by stitching together in-distribution subsequences.</p><p>Effective non-greedy planning By blurring the line between model and planner, the training procedure that improves the model's predictions also has the effect of improving its planning capabilities. This design yields a learned planner that can solve the types of long-horizon, sparse-reward problems that prove difficult for many conventional planning methods.</p><p>The core contribution of this work is a denoising diffusion model designed for trajectory data and an associated probabilistic framework for behavior synthesis. While unconventional compared to the types of models routinely used in deep model-based reinforcement learning, we demonstrate that Diffuser has a number of useful properties and is particularly effective in control settings that require long-horizon reasoning and test-time flexibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Our approach to planning is a learning-based analogue of past work in behavioral synthesis using trajectory optimization <ref type="bibr" target="#b59">(Witkin &amp; Kass, 1988;</ref><ref type="bibr" target="#b55">Tassa et al., 2012)</ref>. In this section, we provide a brief background on the problem setting considered by trajectory optimization and the class of generative models we employ for that problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Setting</head><p>Consider a system governed by the discrete-time dynamics s t+1 = f (s t , a t ) at state s t given an action a t . Trajectory optimization refers to finding a sequence of actions a * 0:T that maximizes (or minimizes) an objective J factorized over per-timestep rewards (or costs) r(s t , a t ):</p><formula xml:id="formula_0">a * 0:T = arg max a 0:T J (s 0 , a 0:T ) = arg max a 0:T T t=0 r(s t , a t )</formula><p>where T is the planning horizon. We use the abbreviation τ = (s 0 , a 0 , s 1 , a 1 , . . . , s T , a T ) to refer to a trajectory of interleaved states and actions and J (τ ) to denote the objective value of that trajectory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Diffusion Probabilistic Models</head><p>Diffusion probabilistic models <ref type="bibr" target="#b49">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b23">Ho et al., 2020)</ref> pose the data-generating process as an iterative denoising procedure p θ (τ i−1 | τ i ). This denoising is the reverse of a forward diffusion process q(τ i | τ i−1 ) that slowly corrupts the structure in data by adding noise. The data distribution induced by the model is given by:</p><formula xml:id="formula_1">p θ (τ 0 ) = p(τ N ) N i=1 p θ (τ i−1 | τ i )dτ 1:N</formula><p>where p(τ N ) is a standard Gaussian prior and τ 0 denotes (noiseless) data. Parameters θ are optimized by minimizing a variational bound on the negative log likelihood of the reverse process: θ * = arg min θ −E τ 0 log p θ (τ 0 ) . The reverse process is often parameterized as Gaussian with fixed timestep-dependent covariances:</p><formula xml:id="formula_2">p θ (τ i−1 | τ i ) = N (τ i−1 | µ θ (τ i , i), Σ i ).</formula><p>The forward process q(τ i | τ i−<ref type="foot" target="#foot_0">1</ref> ) is typically prespecified.</p><p>Notation. There are two "times" at play in this work: that of the diffusion process and that of the planning problem. We use superscripts (i when unspecified) to denote diffusion timestep and subscripts (t when unspecified) to denote planning timestep. For example, s 0 t refers to the t th state in a noiseless trajectory. When it is unambiguous from context, superscripts of noiseless quantities are omitted: τ = τ 0 . We overload notation slightly by referring to the t th state (or action) in a trajectory τ as τ st (or τ at ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Planning with Diffusion</head><p>A major obstacle to using trajectory optimization techniques is that they require knowledge of the environment dynamics f . Most learning-based methods attempt to overcome this obstacle by training an approximate dynamics model and plugging it in to a conventional planning routine. However, learned models are often poorly suited to the types of planning algorithms designed with ground-truth models in mind, leading to planners that exploit learned models by finding adversarial examples.</p><p>We propose a tighter coupling between modeling and planning. Instead of using a learned model in the context of a classical planner, we subsume as much of the planning process as possible into the generative modeling framework, such that planning becomes nearly identical to sampling. We do this using a diffusion model of trajectories, p θ (τ ). The iterative denoising process of a diffusion model lends itself to flexible conditioning by way of sampling from perturbed distributions of the form:</p><formula xml:id="formula_3">pθ (τ ) ∝ p θ (τ )h(τ ).<label>(1)</label></formula><p>The function h(τ ) can contain information about prior evidence (such as an observation history), desired outcomes (such as a goal to reach), or general functions to optimize (such as rewards or costs). Performing inference in this perturbed distribution can be seen as a probabilistic analogue to the trajectory optimization problem posed in Section 2.1, as it requires finding trajectories that are both physically realistic under p θ (τ ) and high-reward (or constraint-satisfying) under h(τ ). Because the dynamics information is separated from the perturbation distribution h(τ ), a single diffusion model p θ (τ ) may be reused for multiple tasks in the same environment.</p><p>In this section, we describe Diffuser, a diffusion model designed for learned trajectory optimization. We then discuss two specific instantiations of planning with Diffuser, realized as reinforcement learning counterparts to classifierguided sampling and image inpainting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A Generative Model for Trajectory Planning</head><p>Temporal ordering. Blurring the line between sampling from a trajectory model and planning with it yields an unusual constraint: we can no longer predict states autoregressively in temporal order. Consider the goalconditioned inference p(s 1 | s 0 , s T ); the next state s 1 depends on a future state as well as a prior one. This example is an instance of a more general principle: while dynamics prediction is causal, in the sense that the present is determined by the past, decision-making and control can be anti-causal, in the sense that decisions in the present are conditional on the future. 1 Because we cannot use a temporal autoregressive ordering, we design Diffuser to predict all timesteps of a plan concurrently.</p><p>Temporal locality. Despite not being autoregressive or Markovian, Diffuser features a relaxed form of temporal locality. In Figure <ref type="figure" target="#fig_0">2</ref>, we depict a dependency graph for a diffusion model consisting of a single temporal convolution.</p><p>The receptive field of a given prediction only consists of nearby timesteps, both in the past and the future. As a result, each step of the denoising process can only make predictions based on local consistency of the trajectory. By composing many of these denoising steps together, however, local consistency can drive global coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trajectory representation.</head><p>Diffuser is a model of trajectories designed for planning, meaning that the effectiveness of the controller derived from the model is just as important as the quality of the state predictions. As a result, states and actions in a trajectory are predicted jointly; for the purposes of prediction the actions are simply additional dimensions of the state. Specifically, we represent inputs (and outputs) of Diffuser as a two-dimensional array:</p><formula xml:id="formula_4">τ = s 0 s 1 . . . s T a 0 a 1 a T .<label>(2)</label></formula><p>with one column per timestep of the planning horizon.</p><p>Architecture. We now have the ingredients needed to specify a Diffuser architecture: (1) an entire trajectory should be predicted non-autoregressively, (2) each step of the denoising process should be temporally local, Algorithm 1 Guided Diffusion Planning 1: Require Diffuser µ θ , guide J , scale α, covariances Σ i 2: while not done do 3:</p><p>Observe state s; initialize plan τ N ∼ N (0, I)</p><formula xml:id="formula_5">4:</formula><p>for i = N, . . . , 1 do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>// parameters of reverse transition 6:</p><p>µ ← µ θ (τ i ) 7:</p><p>// guide using gradients of return 8:</p><formula xml:id="formula_6">τ i−1 ∼ N (µ + αΣ∇J (µ), Σ i ) 9:</formula><p>// constrain first state of plan 10:</p><formula xml:id="formula_7">τ i−1 s 0 ← s 11:</formula><p>Execute first action of plan τ 0 a0 and (3) the trajectory representation should allow for equivariance along one dimension (the planning horizon) but not the other (the state and action features). We satisfy these criteria with a model consisting of repeated (temporal) convolutional residual blocks. The overall architecture resembles the types of U-Nets that have found success in image-based diffusion models, but with two-dimensional spatial convolutions replaced by onedimensional temporal convolutions (Figure <ref type="figure">A1</ref>). Because the model is fully convolutional, the horizon of the predictions is determined not by the model architecture, but by the input dimensionality; it can change dynamically during planning if desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training.</head><p>We use Diffuser to parameterize a learned gradient θ (τ i , i) of the trajectory denoising process, from which the mean µ θ can be solved in closed form <ref type="bibr" target="#b23">(Ho et al., 2020)</ref>. We use the simplified objective for training themodel, given by:</p><formula xml:id="formula_8">L(θ) = E i, ,τ 0 − θ (τ i , i) 2 ,</formula><p>in which i ∼ U{1, 2, . . . , N } is the diffusion timestep, ∼ N (0, I) is the noise target, and τ i is the trajectory τ 0 corrupted with noise . Reverse process covariances Σ i follow the cosine schedule of <ref type="bibr">Nichol &amp; Dhariwal (2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Reinforcement Learning as Guided Sampling</head><p>In order to solve reinforcement learning problems with Diffuser, we must introduce a notion of reward. We appeal to the control-as-inference graphical model <ref type="bibr" target="#b35">(Levine, 2018)</ref> to do so. Let O t be a binary random variable denoting the optimality of timestep t of a trajectory, with p(O t = 1) = exp(r(s t , a t )). We can sample from the set of optimal trajectories by setting h(τ ) = p(O 1:T | τ ) in Equation <ref type="formula" target="#formula_3">1</ref>:</p><formula xml:id="formula_9">pθ (τ ) = p(τ | O 1:T = 1) ∝ p(τ )p(O 1:T = 1 | τ ).</formula><p>We have exchanged the reinforcement learning problem for one of conditional sampling. Thankfully, there has been much prior work on conditional sampling with diffusion models. While it is intractable to sample from this distribution exactly, when p(O 1:T | τ i ) is sufficiently smooth, the reverse diffusion process transitions can be approximated as Gaussian <ref type="bibr" target="#b49">(Sohl-Dickstein et al., 2015)</ref>:</p><formula xml:id="formula_10">p θ (τ i−1 | τ i , O 1:T ) ≈ N (τ i−1 ; µ + Σg, Σ)<label>(3)</label></formula><p>where µ, Σ are the parameters of the original reverse process transition p θ (τ i−1 | τ i ) and</p><formula xml:id="formula_11">g = ∇ τ log p(O 1:T | τ )| τ =µ = T t=0 ∇ st,at r(s t , a t )| (st,at)=µt = ∇J (µ).</formula><p>This relation provides a straightforward translation between classifier-guided sampling, used to generate classconditional images <ref type="bibr">(Dhariwal &amp; Nichol, 2021)</ref>, and the reinforcement learning problem setting. We first train a diffusion model p θ (τ ) on the states and actions of all available trajectory data. We then train a separate model J φ to predict the cumulative rewards of trajectory samples τ i . The gradients of J φ are used to guide the trajectory sampling procedure by modifying the means µ of the reverse process according to Equation <ref type="formula" target="#formula_10">3</ref>. The first action of a sampled trajectory τ ∼ p(τ | O 1:T = 1) may be executed in the environment, after which the planning procedure begins again in a standard receding-horizon control loop. Pseudocode for the guided planning method is given in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Goal-Conditioned RL as Inpainting</head><p>Some planning problems are more naturally posed as constraint satisfaction than reward maximization. In these settings, the objective is to produce any feasible trajectory that satisfies a set of constraints, such as terminating at a goal location. Appealing to the two-dimensional array representation of trajectories described by Equation <ref type="formula" target="#formula_4">2</ref>, this setting can be translated into an inpainting problem, in which state and action constraints act analogously to observed pixels in an image <ref type="bibr" target="#b49">(Sohl-Dickstein et al., 2015)</ref>. All unobserved locations in the array must be filled in by the diffusion model in a manner consistent with the observed constraints.</p><p>The perturbation function required for this task is a Dirac delta for observed values and constant elsewhere. Concretely, if c t is state constraint at timestep t, then</p><formula xml:id="formula_12">h(τ ) = δ ct (s 0 , a 0 , . . . , s T , a T ) = +∞ if c t = s t 0 otherwise</formula><p>The definition for action constraints is identical. In practice, this may be implemented by sampling from Even reward maximization problems require conditioningby-inpainting because all sampled trajectories should begin at the current state. This conditioning is described by line 10 in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Properties of Diffusion Planners</head><p>We discuss a number of Diffuser's important properties, focusing on those that are are either distinct from standard dynamics models or unusual for non-autoregressive trajectory prediction.</p><p>Learned long-horizon planning. Single-step models are typically used as proxies for ground-truth environment dynamics f , and as such are not tied to any planning algorithm in particular. In contrast, the planning routine in Algorithm 1 is closely tied to the specific affordances of diffusion models. Because our planning method is nearly identical to sampling (with the only difference being guidance by a perturbation function h(τ )), Diffuser's effectiveness as a long-horizon predictor directly translates to effective long-horizon planning. We demonstrate the benefits of learned planning in a goal-reaching setting in Figure <ref type="figure" target="#fig_1">3a</ref>, showing that Diffuser is able to generate feasible trajectories in the types of sparse reward settings where shooting-based approaches are known to struggle. We explore a more quantitative version of this problem setting in Section 5.1.</p><p>Temporal compositionality. Single-step models are often motivated using the Markov property, allowing them to compose in-distribution transitions to generalize to outof-distribution trajectories. Because Diffuser generates globally coherent trajectories by iteratively improving local consistency (Section 3.1), it can also stitch together familiar subsequences in novel ways. In Figure <ref type="figure" target="#fig_1">3b</ref>, we train Diffuser on trajectories that only travel in a straight line, and show that it can generalize to v-shaped trajectories by composing trajectories at their point of intersection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variable-length plans.</head><p>Because our model is fully convolutional in the horizon dimension of its prediction, its planning horizon is not specified by architectural choices. Instead, it is determined by the size of the input noise τ N ∼ N (0, I) that initializes the denoising process, allowing for variable-length plans (Figure <ref type="figure" target="#fig_1">3c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task compositionality.</head><p>While Diffuser contains information about both environment dynamics and behaviors, it is independent of reward function. Because the model acts as a prior over possible futures, planning can be guided by comparatively lightweight perturbation functions h(τ ) (or even combinations of multiple perturbations) corresponding to different rewards. We demonstrate this by planning for a new reward function unseen during training of the diffusion model (Figure <ref type="figure" target="#fig_1">3d</ref>). Single-task Average 9.1 7.7 47.0 119.5</p><p>Multi2D U-Maze --24.8 128.9 ±1.8 Multi2D Medium --12.1 127.2 ±3.4 Multi2D Large --13.9 132.1 ±5.8   Multi-task Average --16.9 129.4</p><p>Table <ref type="table">1</ref>. (Long-horizon planning) The performance of Diffuser and prior model-free algorithms in the Maze2D environment, which tests long-horizon planning due to its sparse reward structure. The Multi2D setting refers to a multi-task variant with goal locations resampled at the beginning of every episode. Diffuser substantially outperforms prior approaches in both settings. Appendix A details the sources for the scores of the baseline algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Evaluation</head><p>The focus of our experiments is to evaluate Diffuser on the capabilities we would like from a data-driven planner.</p><p>In particular, we evaluate (1) the ability to plan over long horizons without manual reward shaping, (2) the ability to generalize to new configurations of goals unseen during training, and (3) the ability to recover an effective controller from heterogeneous data of varying quality. We conclude by studying practical runtime considerations of diffusion-based planning, including the most effective ways of speeding up the planning procedure while suffering minimally in terms of performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Long Horizon Multi-Task Planning</head><p>We evaluate long-horizon planning in the Maze2D environments <ref type="bibr" target="#b15">(Fu et al., 2020)</ref>, which require traversing to a goal location where a reward of 1 is given. No reward shaping is provided at any other location. Because it can take hundreds of steps to reach the goal location, even the best model-free algorithms struggle to adequately perform credit assignment and reliably reach the goal (Table <ref type="table">1</ref>).</p><p>We plan with Diffuser using the inpainting strategy to condition on a start and goal location. (The goal location is also available to the model-free methods; it is identifiable by being the only state in the dataset with non-zero reward.) We then use the sampled trajectory as an open-loop plan. Diffuser achieves scores over 100 in all maze sizes, indicating that it outperforms a reference expert policy. We visualize the reverse diffusion process generating Diffuser's plans in Figure <ref type="figure" target="#fig_3">4</ref>.</p><p>While the training data in Maze2D is undirected -consisting of a controller navigating to and from randomly selected locations -the evaluation is single-task in that the goal is always the same. In order to test multi-task flexibility, we modify the environment to randomize the goal location at the beginning of each episode. This setting is denoted as Multi2D in Table <ref type="table">1</ref>. Diffuser is naturally a multi-task planner; we do not need to retrain the model from the single-task experiments and simply change the conditioning goal. As a result, Diffuser performs as well in the multitask setting as in the single-task setting. In contrast, there is a substantial performance drop of the best model-free algorithm in the single-task setting (IQL; <ref type="bibr" target="#b32">Kostrikov et al. 2022)</ref> when adapted to the multi-task setting. Details of our multi-task IQL with hindsight experience relabeling <ref type="bibr" target="#b0">(Andrychowicz et al., 2017)</ref> are provided in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Test-time Flexibility</head><p>In order to evaluate the ability to generalize to new test-time goals, we construct a suite of block stacking tasks with three settings: (1) Unconditional Stacking, for which the task is to build a block tower as tall as possible; (2) Conditional Stacking, for which the task is to construct a block tower with a specified order of blocks, and (3) Rearrangement, for which the task is to match a set of reference blocks' locations in a novel arrangement. We train all methods on 10000 trajectories from demonstrations generated by PDDLStream <ref type="bibr" target="#b17">(Garrett et al., 2020)</ref>; rewards are equal to one upon successful stack placements and zero otherwise. These block stacking are challenging diagnostics of testtime flexibility; in the course of executing a partial stack for a randomized goal, a controller will venture into novel states not included in the training configuration.</p><p>We use one trained Diffuser for all block-stacking tasks, only Table <ref type="table">3</ref>. (Test-time flexibility) Performance of BCQ, CQL, and Diffuser on block stacking tasks. A score of 100 corresponds to a perfectly executed stack; 0 is that of a random policy.</p><p>modifying the perturbation function h(τ ) between settings.</p><p>In the Unconditional Stacking task, we directly sample from the unperturbed denoising process p θ (τ ) to emulate the PDDLStream controller. In the Conditional Stacking and Rearrangement tasks, we compose two perturbation functions h(τ ) to bias the sampled trajectories: the first maximizes the likelihood of the trajectory's final state matching the goal configuration, and the second enforces a contact constraint between the end effector and a cube during stacking motions. (See Appendix B for details.)</p><p>We compare with two prior model-free offline reinforcement learning algorithms: BCQ <ref type="bibr" target="#b16">(Fujimoto et al., 2019)</ref> and CQL <ref type="bibr" target="#b33">(Kumar et al., 2020)</ref>, training standard variants for Unconditional Stacking and goal-conditioned variants for Conditional Stacking and Rearrangement. (Baseline details are provided in Appendix A.) Quantitative results are given in Table <ref type="table">3</ref>, in which a score of 100 corresponds to a perfect execution of the task. Diffuser substantially outperforms both prior methods, with the conditional settings requiring flexible behavior generation proving especially difficult for the model-free algorithms. A visual depiction of an execution by Diffuser is provided in Figure <ref type="figure" target="#fig_5">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Offline Reinforcement Learning</head><p>Finally, we evaluate the capacity to recover an effective single-task controller from heterogeneous data of varying quality using the D4RL offline locomotion suite <ref type="bibr" target="#b15">(Fu et al., 2020)</ref>. We guide the trajectories generated by Diffuser toward high-reward regions using the sampling procedure described in Section 3.2 and condition the trajectories on the current state using the inpainting procedure described in Section 3.3. The reward predictor J φ is trained on the same trajectories as the diffusion model.</p><p>We compare to a variety of prior algorithms spanning other approaches to data-driven control, including the modelfree reinforcement learning algorithms CQL <ref type="bibr" target="#b33">(Kumar et al., 2020)</ref>, IQL <ref type="bibr" target="#b32">(Kostrikov et al., 2022)</ref>, and AWAC <ref type="bibr" target="#b39">(Nair et al., 2020)</ref>  <ref type="bibr" target="#b30">(Kidambi et al., 2020)</ref>, and MBOP <ref type="bibr" target="#b1">(Argenson &amp; Dulac-Arnold, 2021)</ref>.</p><p>In the single-task setting, Diffuser performance comparably to prior algorithms: better than the model-based MOReL and MBOP and return-conditioning DT, but worse than the best offline techniques designed specifically for singletask performance. We also investigated a variant using Diffuser as a dynamics model in conventional trajectory optimizers such as MPPI <ref type="bibr" target="#b58">(Williams et al., 2015)</ref>, but found The performance of Diffuser and a variety of prior algorithms on the D4RL locomotion benchmark <ref type="bibr" target="#b15">(Fu et al., 2020)</ref>. Results for Diffuser correspond to the mean and standard error over 15 random seeds (three independently trained diffusion models and five trajectories per model). We detail the sources for the performance of prior methods in Appendix A.3. Following <ref type="bibr" target="#b32">Kostrikov et al. (2022)</ref>, we emphasize in bold scores within 5 percent of the maximum per task (≥ 0.95 • max). that this combination performed no better than random, suggesting that the effectiveness of Diffuser stems from coupled modeling and planning, and not from improved open-loop predictive accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Warm-Starting Diffusion for Faster Planning</head><p>A limitation of Diffuser is that individual plans are slow to generate (due to iterative generation). Naïvely, as we execute plans open loop, a new plan must be regenerated at each step of execution. To improve execution speed of Diffuser, we may further reuse previously generated plans to warm-start generations of subsequent plans.</p><p>To warm-start planning, we may run a limited number of forward diffusion steps from a previously generated plan and then run a corresponding number of denoising steps from this partially noised trajectory to regenerate an updated plan. In Figure <ref type="figure" target="#fig_7">7</ref>, we illustrate the trade-off between performance and runtime budget as we vary the underlying number of denoising steps used to regenerate each a new plan from 2 to 100. We find that we may reduce the planning budget of our approach markedly with only modest drop in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Advances in deep generative modeling have recently made inroads into model-based reinforcement learning, with multiple lines of work exploring dynamics models parameterized as convolutional U-networks <ref type="bibr" target="#b27">(Kaiser et al., 2020)</ref>, stochastic recurrent networks <ref type="bibr" target="#b28">(Ke et al., 2018;</ref><ref type="bibr" target="#b20">Hafner et al., 2021a;</ref><ref type="bibr" target="#b19">Ha &amp; Schmidhuber, 2018)</ref>, vectorquantized autoencoders <ref type="bibr" target="#b21">(Hafner et al., 2021b;</ref><ref type="bibr" target="#b43">Ozair et al., 2021)</ref>, neural ODEs <ref type="bibr" target="#b10">(Du et al., 2020a)</ref>, normalizing flows <ref type="bibr" target="#b46">(Rhinehart et al., 2020;</ref><ref type="bibr" target="#b25">Janner et al., 2020)</ref>, generative ). These investigations generally assume an abstraction barrier between the model and planner. Specifically, the role of learning is relegated to approximating environment dynamics; once learning is complete the model may be inserted into any of a variety of planning <ref type="bibr" target="#b4">(Botev et al., 2013;</ref><ref type="bibr" target="#b58">Williams et al., 2015)</ref> or policy optimization <ref type="bibr" target="#b52">(Sutton, 1990;</ref><ref type="bibr" target="#b57">Wang et al., 2019)</ref> algorithms because the form of the planner does not depend strongly on the form of the model. Our goal is to break this abstraction barrier by designing a model and planning algorithm that are trained alongside one another, resulting in a non-autoregressive trajectory-level model for which sampling and planning are nearly identical.</p><p>A number of parallel lines of work have studied how to break the abstraction barrier between model learning and planning in different ways. Approaches include training an autoregressive latent-space model for reward prediction <ref type="bibr" target="#b54">(Tamar et al., 2016;</ref><ref type="bibr" target="#b42">Oh et al., 2017;</ref><ref type="bibr" target="#b48">Schrittwieser et al., 2019)</ref>; weighing model training objectives by state values <ref type="bibr" target="#b14">(Farahmand et al., 2017)</ref>; and applying collocation techniques to learned single-step energies. In contrast, our method plans by generating all timesteps of a trajectory concurrently, instead of autoregressively, and conditioning the sampled trajectories with auxiliary guidance functions.</p><p>Diffusion models have emerged as a promising class of generative model that formulates the data-generating process as an iterative denoising procedure <ref type="bibr" target="#b49">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b23">Ho et al., 2020)</ref>. The denoising procedure can be seen as parameterizing the gradients of the data distribution <ref type="bibr" target="#b51">(Song &amp; Ermon, 2019)</ref>, connecting diffusion models to score matching <ref type="bibr" target="#b24">(Hyvärinen, 2005)</ref> and EBMs <ref type="bibr" target="#b34">(LeCun et al., 2006;</ref><ref type="bibr" target="#b11">Du &amp; Mordatch, 2019;</ref><ref type="bibr" target="#b41">Nijkamp et al., 2019;</ref><ref type="bibr" target="#b18">Grathwohl et al., 2020)</ref>. Iterative, gradientbased sampling lends itself towards flexible conditioning <ref type="bibr">(Dhariwal &amp; Nichol, 2021)</ref> and compositionality <ref type="bibr" target="#b12">(Du et al., 2020b)</ref>, which we use to recover effective behaviors from heterogeneous datasets and plan for reward functions unseen during training. While diffusion models have been developed for the generation of images <ref type="bibr" target="#b50">(Song et al., 2021)</ref>, waveforms <ref type="bibr" target="#b7">(Chen et al., 2021c)</ref>, 3D shapes <ref type="bibr" target="#b61">(Zhou et al., 2021)</ref>, and text <ref type="bibr" target="#b2">(Austin et al., 2021)</ref>, to the best of our knowledge they have not previously been used in the context of reinforcement learning or decision-making.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have presented Diffuser, a denoising diffusion model for trajectory data. Planning with Diffuser is almost identical to sampling from it, differing only in the addition of auxiliary perturbation functions that serve to guide samples. The learned diffusion-based planning procedure has a number of useful properties, including graceful handling of sparse rewards, the ability to plan for new rewards without retraining, and a temporal compositionality that allows it to produce out-of-distribution trajectories by stitching together in-distribution subsequences. Our results point to a new class of diffusion-based planning procedures for deep modelbased reinforcement learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Diffuser samples plans by iteratively denoising twodimensional arrays consisting of a variable number of state-action pairs. A small receptive field constrains the model to only enforce local consistency during a single denoising step. By composing many denoising steps together, local consistency can drive global coherence of a sampled plan. An optional guide function J can be used to bias plans toward those optimizing a test-time objective or satisfying a set of constraints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. (Properties of diffusion planners) (a) Learned long-horizon planning: Diffuser's learned planning procedure does not suffer from the myopic failure modes common to shooting algorithms and is able to plan over long horizons with sparse reward. (b) Temporal compositionality: Even though the model is not Markovian, it generates trajectories via iterated refinements to local consistency. As a result, it exhibits the types of generalization usually associated with Markovian models, with the ability to stitch together snippets of trajectories from the training data to generate novel plan. (c) Variable-length plans: Despite being a trajectory-level model, Diffuser's planning horizon is not determined by its architecture. The horizon can be updated after training by changing the dimensionality of the input noise. (d) Task compositionality: Diffuser can be composed with new reward functions to plan for tasks unseen during training. In all subfigures, denotes a starting state and denotes a goal state.</figDesc><graphic url="image-36.png" coords="5,134.78,166.97,70.86,70.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. (Planning as inpainting) Plans are generated in the Maze2D environment by sampling trajectories consistent with a specified start and goal condition. The remaining states are "inpainted" by the denoising process.</figDesc><graphic url="image-46.png" coords="6,318.96,179.15,222.65,53.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>→Figure 5 .</head><label>5</label><figDesc>Figure 5. (Block stacking) A block stacking sequence by Diffuser. This task is best illustrated by videos viewable at diffusion-planning.github.io.</figDesc><graphic url="image-49.png" coords="7,308.83,59.97,105.29,105.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. (Guided sampling) Diffuser generates all timesteps of a plan concurrently, instead of autoregressively, through the denoising process.</figDesc><graphic url="image-53.png" coords="8,76.73,178.59,210.58,52.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. (Fast Planning) Performance of Diffuser on Walker2d Medium-Expert when varying the number of diffusion steps to warm-start planning. Performance suffers only minimally even when using one-tenth the number of diffusion steps, as long as plans are initialized from the previous timestep's plan.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>(Offline reinforcement learning)  </figDesc><table><row><cell cols="2">Dataset Environment</cell><cell>BC</cell><cell>CQL</cell><cell cols="2">IQL AWAC</cell><cell>DT</cell><cell cols="3">TT MOReL MBOP</cell><cell>Diffuser</cell></row><row><cell cols="2">Medium-Expert HalfCheetah Medium-Expert Hopper Medium-Expert Walker2d</cell><cell>55.2 52.5 107.5</cell><cell>91.6 105.4 108.8</cell><cell>86.7 91.5 109.6</cell><cell>42.8 55.8 74.5</cell><cell>86.8 107.6 108.1</cell><cell>95.0 110.0 101.9</cell><cell>53.3 108.7 95.6</cell><cell>105.9 55.1 70.2</cell><cell>79.8 ±1.7 107.2 ±1.3 108.4 ±1.9</cell></row><row><cell>Medium Medium Medium</cell><cell>HalfCheetah Hopper Walker2d</cell><cell>42.6 52.9 75.3</cell><cell>44.0 58.5 72.5</cell><cell>47.4 66.3 78.3</cell><cell>43.5 57.0 72.4</cell><cell>42.6 67.6 74.0</cell><cell>46.9 61.1 79.0</cell><cell>42.1 95.4 77.8</cell><cell>44.6 48.8 41.0</cell><cell>44.2 ±0.8 58.5 ±5.1 79.7 ±1.5</cell></row><row><cell cols="2">Medium-Replay HalfCheetah Medium-Replay Hopper Medium-Replay Walker2d</cell><cell>36.6 18.1 26.0</cell><cell>45.5 95.0 77.2</cell><cell>44.2 94.7 73.9</cell><cell>40.5 37.2 27.0</cell><cell>36.6 82.7 66.6</cell><cell>41.9 91.5 82.6</cell><cell>40.2 93.6 49.8</cell><cell>42.3 12.4 9.7</cell><cell>42.2 ±2.8 96.8 ±0.8 61.2 ±6.7</cell></row><row><cell></cell><cell>Average</cell><cell>51.9</cell><cell>77.6</cell><cell>77.0</cell><cell>50.1</cell><cell>74.7</cell><cell>78.9</cell><cell>72.9</cell><cell>47.8</cell><cell>75.3</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">In general reinforcement learning contexts, conditioning on the future emerges from the assumption of future optimality for the purpose of writing a dynamic programming recursion. Concretely, this appears as the future optimality variables Ot:T in the action distribution log p(at | st, Ot:T )(Levine,  </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2018" xml:id="foot_1">).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Ajay Jain for feedback on an early draft and Leslie Kaelbling, Tomás Lozano-Pérez, Jascha Sohl-Dickstein, Ben Eysenbach, Amy Zhang, Colin Li, and Toru Lin for helpful discussions. This work was partially supported by computational resource donations from Microsoft. M.J. is supported by fellowships from the National Science Foundation and the Open Philanthropy Project. Y.D. is supported by a fellowship from the National Science Foundation.</p></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code References</head><p>We used the following open-source libraries for this work: NumPy <ref type="bibr" target="#b22">(Harris et al., 2020)</ref>, PyTorch <ref type="bibr" target="#b44">(Paszke et al., 2019)</ref>, and Diffusion Models in PyTorch <ref type="bibr" target="#b56">(Wang, 2020)</ref>.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Baseline details and sources</head><p>In this section, we provide details about baselines we ran ourselves. For scores of baselines previously evaluated on standardized tasks, we provide the source of the listed score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Maze2D experiments</head><p>Single-task. The performance of CQL and IQL on the standard Maze2D environments is reported in the D4RL whitepaper <ref type="bibr" target="#b15">(Fu et al., 2020)</ref> in Table <ref type="table">2</ref>.</p><p>We ran IQL using the offical implementation from the authors:</p><p>github.com/ikostrikov/implicit q learning.</p><p>We tuned over two hyperparameters:</p><p>1. temperature ∈ <ref type="bibr" target="#b64">[3,</ref><ref type="bibr">10]</ref> 2. expectile ∈ [0.65, 0.95] Multi-task. We only evaluated IQL on the Multi2D environments because it is the strongest baseline in the single-task Maze2D environments by a sizeable margin. To adapt IQL to the multi-task setting, we modified the Qfunctions, value function, and policy to be goal-conditioned. To select goals during training, we employed a strategy based on hindsight experience replay, in which we sampled a goal from among those states encountered in the future of a trajectory. For a training backup (s t , a t , s t+1 ), we sampled goals according to a geometric distribution over the future</p><p>recalculated rewards based on the sampled goal, and conditioned all relevant models on the goal during updating.</p><p>During testing, we conditioned the policy on the groundtruth goal.</p><p>We tuned over the same IQL parameters as in the single-task setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Block stacking experiments</head><p>Single-task. We ran CQL using the following implementation https://github.com/young-geng/cql.</p><p>and used default hyperparameters in the code. We ran BCQ using the author's original implementation https://github.com/sfujim/BCQ.</p><p>For BCQ, we tuned over two hyperparameters:</p><p>1. discount factor ∈ [0.9, 0.999]  Multi-task. To evaluate BCQ and CQL in the multitask setting, we modified the Q-functions, value function and policy to be goal-conditioned. We trained using goal relabeling as in the Multi2D environments. We tuned over the same hyperparameters described in the single-task block stacking experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Offline Locomotion</head><p>The scores for BC, CQL, IQL, and AWAC are from Table <ref type="table">1</ref> in <ref type="bibr" target="#b32">Kostrikov et al. (2022)</ref>. The scores for DT are from </p><p>, where τ ci corresponds to the underlying dimension in state τ si that specifies the presence or absence of contact between the Kuka arm and block A. We apply the contact constraint between the Kuka arm and block A for the first 64 timesteps in a trajectory, corresponding to initial contact with block A in a plan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C Implementation Details</head><p>In this section we describe the architecture and record hyperparameters.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hindsight experience replay</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Model-based offline planning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Argenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dulac-Arnold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Structured denoising diffusion models in discrete state-spaces</title>
		<author>
			<persName><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Structured agents for physical construction</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Stachenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cross-entropy method for optimization</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">I</forename><surname>Botev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kroese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ecuyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Statistics</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">TransDreamer: Reinforcement learning with transformer world models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Decision transformer: Reinforcement learning via sequence modeling</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rajeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Laskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Estimating gradients for waveform generation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><surname>Wavegrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021c</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning in a handful of trials using probabilistic dynamics models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Calandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Diffusion models beat GANs on image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Modelbased reinforcement learning for semi-markov decision processes with neural odes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Futoma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2020a. 2019</date>
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Model based planning with energy based models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Compositional visual generation with energy based models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Eysenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khazatsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.02758</idno>
		<title level="m">Mismatched no more: Joint modelpolicy optimization for model-based rl</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Valueaware loss function for model-based reinforcement learning</title>
		<author>
			<persName><forename type="first">A.-M</forename><surname>Farahmand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nikovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><surname>D4rl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07219</idno>
		<title level="m">Datasets for deep data-driven reinforcement learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Off-policy deep reinforcement learning without exploration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Integrating symbolic planners and blackbox samplers via optimistic adaptive planning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lozano-Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><surname>Pddlstream</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automated Planning and Scheduling</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning the stein discrepancy for training and evaluating energy-based models without sampling</title>
		<author>
			<persName><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recurrent world models facilitate policy evolution</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning latent dynamics for planning from pixels</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mastering atari with discrete world models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Array programming with NumPy</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Millman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gommers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Picus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Van Kerkwijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Haldane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Del Río</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gérard-Marchant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sheppard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Weckesser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gohlke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">585</biblScope>
			<biblScope unit="issue">7825</biblScope>
			<biblScope unit="page" from="357" to="362" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Estimation of non-normalized statistical models by score matching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">γ-models: Generative temporal difference learning for infinitehorizon prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Janner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Offline reinforcement learning as one big sequence modeling problem</title>
		<author>
			<persName><forename type="first">M</forename><surname>Janner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Model based reinforcement learning for atari</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Miłos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Osiński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Czechowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kozakowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Michalewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Modeling the long term future in model-based reinforcement learning</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Touati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An introduction to trajectory optimization: How to do your own direct collocation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="849" to="904" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">MOReL: Model-based offline reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kidambi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rajeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Netrapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Offline reinforcement learning with implicit Q-learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Conservative Q-learning for offline reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A tutorial on energy-based learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Predicting Structured Data</title>
				<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00909</idno>
		<title level="m">Reinforcement learning and control as probabilistic inference: Tutorial and review</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A. 3d neural scene representations for visuomotor control</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torralba</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mish: A self regularized non-monotonic neural activation function</title>
		<author>
			<persName><forename type="first">D</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nagabandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fearing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Accelerating online reinforcement learning with offline datasets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09359</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning non-convergent non-persistent short-run MCMC toward energy-based model</title>
		<author>
			<persName><forename type="first">E</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Value prediction network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Vector quantized models for planning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><surname>Pytorch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A direct method for trajectory optimization of rigid bodies through contact</title>
		<author>
			<persName><forename type="first">M</forename><surname>Posa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cantu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tedrake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep imitative models for flexible inference, planning, and control</title>
		<author>
			<persName><forename type="first">N</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Graph networks as learnable physics engines for inference and control</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Mastering atari, go, chess and shogi by planning with a learned model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08265</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Integrated architectures for learning, planning, and reacting based on approximating dynamic programming</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Model regularization for stable sample rollouts</title>
		<author>
			<persName><forename type="first">E</forename><surname>Talvitie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Value iteration networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Synthesis and stabilization of complex behaviors through online trajectory optimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Todorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Implementation of denoising diffusion probabilistic models in pytorch</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://github.com/lucidrains/denoising-diffusion-pytorch" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Clavera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Langlois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02057</idno>
		<title level="m">Benchmarking model-based reinforcement learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Model predictive path integral control using covariance variable importance sampling</title>
		<author>
			<persName><forename type="first">G</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aldrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Theodorou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.01149</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Spacetime constraints</title>
		<author>
			<persName><forename type="first">A</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Siggraph Computer Graphics</title>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">3D shape generation and completion through point-voxel diffusion</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Timestep embeddings are produced by a single fully-connected layer and added to the activations of the first temporal convolution within each block</title>
	</analytic>
	<monogr>
		<title level="m">and a final Mish nonlinearity</title>
				<imprint>
			<publisher>Misra</publisher>
			<date type="published" when="2018">2018. 2019</date>
		</imprint>
	</monogr>
	<note>The architecture of Diffuser (Figure A1) consists of a U-Net structure with 6 repeated residual blocks</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">We train the model using the Adam optimizer (Kingma &amp; Ba, 2015) with a learning rate of 4e−05 and batch size of 32. We train the models for 500k steps</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">The return predictor J has the structure of the first half of the U-Net used for the diffusion model</title>
		<imprint/>
	</monogr>
	<note>with a final linear layer to produce a scalar output</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">We use a planning horizon T of 100 in all locomotion tasks, 128 for blocking stacking</title>
	</analytic>
	<monogr>
		<title level="m">Maze2D / Multi2D U-Maze, 265 in Maze2D / Multi2D Medium, and 384 in Maze2D / Multi2D Large</title>
				<imprint>
			<biblScope unit="volume">128</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">We use N = 100 diffusion steps</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">We use a guide scale of α = 0</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">We tuned over discount factors for the return prediction J φ , but found that above γ = 0.99 planning was insensitive to changes in discount factor. We found that control performance was not substantially affected by the choice of predicting noise versus uncorrupted data τ 0 with the diffusion model</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
