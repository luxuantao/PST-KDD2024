<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning Identity-Preserving Face Space</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhenyao</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<email>xgwang@ee.cuhk.edu.hk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
							<email>xtang@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>Kong</surname></persName>
						</author>
						<title level="a" type="main">Deep Learning Identity-Preserving Face Space</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ICCV.2013.21</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Face recognition with large pose and illumination variations is a challenging problem in computer vision. This paper addresses this challenge by proposing a new learningbased face representation: the face identity-preserving (FIP) features. Unlike conventional face descriptors, the FIP features can significantly reduce intra-identity variances, while maintaining discriminativeness between identities. Moreover, the FIP features extracted from an image under any pose and illumination can be used to reconstruct its face image in the canonical view. This property makes it possible to improve the performance of traditional descriptors, such as LBP [2] and Gabor [31],</head><p>which can be extracted from our reconstructed images in the canonical view to eliminate variations. In order to learn the FIP features, we carefully design a deep network that combines the feature extraction layers and the reconstruction layer. The former encodes a face image into the FIP features, while the latter transforms them to an image in the canonical view. Extensive experiments on the large MultiPIE face database <ref type="bibr" target="#b6">[7]</ref> demonstrate that it significantly outperforms the state-of-the-art face recognition methods. * indicates equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In many practical applications, the pose and illumination changes become the bottleneck for face recognition <ref type="bibr" target="#b35">[36]</ref>. Many existing works have been proposed to account for such variations. The pose-invariant methods can be generally separated into two categories: 2D-based <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b22">23]</ref> and 3D-based <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b2">3]</ref>. In the first category, poses are either handled by 2D image matching or by encoding a test image using some bases or exemplars. For example, Carlos et al. <ref type="bibr" target="#b4">[5]</ref> used stereo matching to compute the similarity between two faces. Li et al. <ref type="bibr" target="#b16">[17]</ref> represented a test face as a linear combination of training images, and utilized the linear regression coefficients as features for face recognition. 3D-based methods usually capture 3D face data or estimate 3D models from 2D input, and try to match them to a 2D probe face image. Such methods make it possible to synthesize any view of the probe face, which makes them generally more robust to pose variation. For instance, Li et al. <ref type="bibr" target="#b17">[18]</ref> first generated a virtual view for the probe face by using a set of 3D displacement fields sampled from a 3D face database, and then matched the synthesized face with the gallery faces. Similarly, Asthana et al. <ref type="bibr" target="#b2">[3]</ref> matched the 3D model to a 2D image using the view-based active appearance model.</p><p>The illumination-invariant methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b16">17]</ref> typically make assumptions about how illumination affects the face images, and use these assumptions to model and remove the illumination effect. For example, Wagner et al. <ref type="bibr" target="#b25">[26]</ref> designed a projector-based system to capture images of each subject in the gallery under a few illuminations, which can be linearly combined to generate images under arbitrary illuminations. With this augmented gallery, they adopted sparse coding to perform face recognition. The above methods have certain limitations. For example, capturing 3D data requires additional cost and resources <ref type="bibr" target="#b17">[18]</ref>. Inferring 3D models from 2D data is an illposed problem <ref type="bibr" target="#b22">[23]</ref>. As the statistical illumination models <ref type="bibr" target="#b25">[26]</ref> are often summarized from controlled environment, they cannot be well generalized in practical applications.</p><p>In this paper, unlike previous works that either build physical models or make statistical assumptions, we propose a novel face representation, the face identitypreserving (FIP) features, which are directly extracted from face images with arbitrary poses and illuminations. This new representation can significantly remove pose and illumination variations, while maintaining the discriminativeness across identities, as shown in Fig. <ref type="figure" target="#fig_0">1 (a)</ref>. Furthermore, unlike traditional face descriptors, e.g. LBP <ref type="bibr" target="#b1">[2]</ref>, Gabor <ref type="bibr" target="#b30">[31]</ref>, and LE <ref type="bibr" target="#b3">[4]</ref>, which cannot recover the original images, the FIP features can reconstruct face images in the frontal pose and with neutral illumination (we call it the canonical view) of the same identity, as shown in Fig. <ref type="figure" target="#fig_0">1 (b)</ref>. With this attractive property, the conventional descriptors and learning algorithms can utilize our reconstructed face images in the canonical view as input so as to eliminate the negative effects from poses and illuminations.</p><p>Specifically, we present a new deep network to learn the FIP features. It utilizes face images with arbitrary pose and illumination variations of an identity as input, and reconstructs a face in the canonical view of the same identity as the target (see Fig. <ref type="figure" target="#fig_2">3</ref>). First, input images are encoded through feature extraction layers, which have three locally connected layers and two pooling layers stacked alternately. Each layer captures face features at a different scale. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, the first locally connected layer outputs 32 feature maps. Each map has a large number of high responses outside the face region, which mainly capture pose information, and some high responses inside the face region, which capture face structures (red indicates large response and blue indicates no response). On the output feature maps of the second locally connected layer, high responses outside the face region have been significantly reduced, which indicates that it discards most pose variations while retain the face structures. The third locally connected layer outputs the FIP features, which is sparse and identity-preserving.</p><p>Second, the FIP features recover the face image in the canonical view using a fully-connected reconstruction layer. As there are large amount of parameters, our network is hard to train using tranditional training methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b11">12]</ref>. We propose a new training strategy, which contains two steps: parameter initialization and parameter update. First, we initialize the parameters based on the least square dictionary learning. We then update all the parameters by back-propagating the summed squared reconstruction error between the reconstructed image and the ground truth.</p><p>Existing deep learning methods for face recognition are generally in two categories: (1) unsupervised learning features with deep models and then using discriminative methods (e.g. SVM) for classification <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15]</ref>; (2) directly using class labels as supervision of deep models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24]</ref>. In the first category, features related to identity, poses, and lightings are coupled when learned by deep models. It is too late to rely on SVM to separate them later. Our supervised model makes it possible to discard pose and lighting features from the very bottom layer. In the second category, a '0/1' class label is a much weaker supervision, compared with ours using a face image (with thousands of pixels) of the canonical view as supervision. We require the deep model to fully reconstruct the face in the canonical view rather than simply predicting class labels, and this strong regularization is more effective to avoid overfitting. This design is suitable for face recognition, where a canonical view exists. Different from convolutional neural networks whose filters share weights, our filers are localized and do not share weights since we assume different face regions should employ different features.</p><p>This work makes three key contributions. (1) We propose a new deep network that combines the feature extraction layers and the reconstruction layer. Its architecture is carefully designed to learn the FIP features. These features can eliminate the poses and illumination variations, and maintain discriminativeness between different identities.</p><p>(2) Unlike conventional face descriptors, the FIP features can be used to reconstruct a face image in the canonical view. We also demonstrate significant improvement of the existing methods, when they are applied on our reconstructed face images. (3) Unlike existing works that need to know the pose of a probe face, so as to build models for different poses specifically, our method can extract the FIP features without knowing information on pose and illumination. The FIP features outperform the state-of-the-art methods, including both 2D-based and 3D-based methods, on the MultiPIE database <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This section reviews related works on learning-based face descriptors and deep models for feature learning.</p><p>Learning-based descriptors. Cao et al. <ref type="bibr" target="#b3">[4]</ref> devised an unsupervised feature learning method (LE) with randomprojection trees and PCA trees, and adopted PCA to gain a compact face descriptor. Zhang et al. <ref type="bibr" target="#b34">[35]</ref> extended <ref type="bibr" target="#b3">[4]</ref> by introducing an inter-modality encoding method, which can match face images in two modalities, e.g. photos and sketches, significantly outperforming traditional methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">30]</ref>. There are studies that learn the filters and patterns for the existing handcrafted descriptors. For example, Guo et al. <ref type="bibr" target="#b7">[8]</ref> proposed a supervised learning approach with the Fisher separation criterion to learn the patterns of LBP <ref type="bibr" target="#b1">[2]</ref>. Zhen et al. <ref type="bibr" target="#b15">[16]</ref> adopted a strategy similar to LDA to learn the filters of LBP. Our FIP features are learned with a multi-layer deep model in a supervised manner, and have more discriminative and representative power than the above works. We illustrate the feature space of FIP compared with LE <ref type="bibr" target="#b3">[4]</ref> and LBP <ref type="bibr" target="#b1">[2]</ref> in Fig. <ref type="figure" target="#fig_1">2 (a), (b) and (d)</ref>, respectively, which show that the FIP space better maintains both the intra-identity consistency and the inter-identity discriminativeness.</p><p>Deep models. The deep models learn representations by stacking many hidden layers, which are layer-wisely trained in an unsupervised manner. For example, the deep belief networks <ref type="bibr" target="#b8">[9]</ref> (DBN) and deep Boltzmann machine <ref type="bibr" target="#b21">[22]</ref> (DBM) stack many layers of restricted Boltzmann machines (RBM) and can extract different levels of features. Recently, Huang et al. <ref type="bibr" target="#b9">[10]</ref> introduced the convolutional restricted Boltzmann machine (CRBM), which incorporates local filters into RBM. Their learned filters can preserve the local structures of data. Sun et al. <ref type="bibr" target="#b23">[24]</ref> proposed a hybrid Convolutional Neural Network-Restricted Boltzmann Machine (CNN-RBM) model to learn relational features for comparing face similarity. Unlike DBN and DBM employ fully connected layers, our deep network combines both locally and fully connected layers, which enables it to extract both the local and global information. The locally connected architecture of our deep network is similar to CRBM <ref type="bibr" target="#b9">[10]</ref>, but we learn the network with a supervised scheme and the FIP features are required to recover the frontal face image. Therefore, this method is more robust to pose and illumination variations, as shown in Fig. <ref type="figure" target="#fig_1">2 (d</ref>). In the first layer, x 0 is transformed to 32 feature maps through a weight matrix W<ref type="foot" target="#foot_0">1</ref> that contains 32 sub-matrices</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Network Architecture</head><formula xml:id="formula_0">W 1 = [W 1 1 ; W 1 2 ; . . . ; W 1 32 ], ∀W 1 i ∈ R n 0 ,n 0 1</formula><p>, each of which is sparse to retain the locally connected structure <ref type="bibr" target="#b12">[13]</ref>. Intuitively, each row of W 1 i represents a small filter centered at a pixel of x 0 , so that all of the elements in this row equal zeros except for the elements belonging to the filter. As our weights are not shared, the non-zero values of these rows are not the same <ref type="foot" target="#foot_1">2</ref> . Therefore, the weight matrix W 1 results in 32 feature maps {x 1  i } 32 i=1 , each of which has n 0 dimensions. Then, a matrix V 1 , where V ij ∈ {0, 1} encodes the 2D topography of the pooling layer <ref type="bibr" target="#b12">[13]</ref>, downsamples each of these feature map to 48 × 48 in order to reduce the number of parameters need to be learned and obtain more robust features. Each x 1 i can be computed as<ref type="foot" target="#foot_2">3</ref> </p><formula xml:id="formula_1">x 1 i = V 1 σ(W 1 i x 0 ),<label>(1)</label></formula><p>where σ(x) = max(0, x) is the rectified linear function <ref type="bibr" target="#b18">[19]</ref> that is feature-intensity-invariant. So it is robust to shape and illumination variations. x 1 can be obtained by concatenating all the x 1 i ∈ R 48×48 together, obtaining a large feature map in n 1 = 48 × 48 × 32 dimensions.</p><p>In the second layer, each</p><formula xml:id="formula_2">x 1 i is transformed to x 2 i 32 sub- matrices {W 2 i } 32 i=1 , ∀W 2 i ∈ R 48×48,48×48 , x 2 i = 32 j=1 V 2 σ(W 2 j x 1 i ),<label>(2)</label></formula><p>where x 2 i is down-sampled using V 2 to 24× 24 dimensions. Eq.2 means that each small feature map in the first layer is multiplied by 32 sub-matrices and then summed together. Here, each sub-matrix has sparse structure as discussed above. We can reformulate Eq.2 into a matrix form</p><formula xml:id="formula_3">x 2 = V 2 σ(W 2 x 1 ),<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">W 2 = [W 2 1 ; . . . ; W 2 32 ], ∀W 2 i ∈ R 48×48,n 1 and x 1 = [x 1 1 ; . . . ; x 1 32 ] ∈ R n 1 , respectively. W 2 i is simply obtained by repeating W 2</formula><p>i for 32 times. Thus, x 2 has n 2 = 24 × 24 × 32 dimensions.</p><p>In the third layer, x 2 is transformed to x 3 , i.e. the FIP features, similar to the second layer, but without pooling. Thus, x 3 is the same size as x 2 .</p><formula xml:id="formula_5">x 3 = σ(W 3 x 2 ),<label>(4)</label></formula><p>where</p><formula xml:id="formula_6">W 3 = [W 3 1 ; . . . ; W 3 32 ], ∀W 3 i ∈ R 24×24,n 2 and x 2 = [x<label>2</label></formula><p>1 ; . . . ; x 2  32 ] ∈ R n 2 , respectively. Finally, the reconstruction layer transforms the FIP features x 3 to the frontal face image y, through a weight matrix W 4 ∈ R n 0 ,n 2 , y = σ(W 4 x 3 ).</p><p>(5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Training</head><p>Training our deep network requires estimating all the weight matrices {W i } as introduced above, which is challenging because of the millions of parameters. Therefore, we first initialize the weights and then update them all. V 1 and V 2 are manually defined <ref type="bibr" target="#b12">[13]</ref> and fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Parameter Initialization</head><p>We cannot employ RBMs <ref type="bibr" target="#b8">[9]</ref> to unsupervised pre-train the weight matrices, because our input/output data are in different spaces. Therefore, we devise a supervised method based on the least square dictionary learning. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, </p><formula xml:id="formula_7">X 3 = {x 3 i } m i=1 are</formula><formula xml:id="formula_8">W 1 ,W 2 ,W 3 ,W 4 Y − σ(W 4 X 3 ) 2 F , (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>where • F is the Frobenius norm. Optimizing Eq.6 is not trivial because of its nonlinearity. However, we can initialize the weight matrices layer-wisely as arg min</p><formula xml:id="formula_10">W 1 Y − OW 1 X 0 2 F ,<label>(7)</label></formula><p>arg min</p><formula xml:id="formula_11">W 2 Y − P W 2 X 1 2 F ,<label>(8)</label></formula><p>arg min</p><formula xml:id="formula_12">W 3 Y − QW 3 X 2 2 F , (<label>9</label></formula><formula xml:id="formula_13">)</formula><p>arg min</p><formula xml:id="formula_14">W 4 Y − W 4 X 3 2 F . (<label>10</label></formula><formula xml:id="formula_15">)</formula><p>In Eq.7, X 0 = {x 0 i } m i=1 is a set of input images. W 1 has been introduced in Sec.3, so that W 1 X 0 results in 32 feature maps for each input. O is a fixed binary matrix that sums together the pixels in the same position of these feature maps, which makes OW 1 X 0 at the same size as Y . In Eq.8, X 1 = {x 1 i } m i=1 is a set of outputs of the first locally connected layer before pooling and P is also a fixed binary matrix, which sums together the corresponding pixels and rescales the results to the same size as Y . Q, X 2 in Eq.9 are defined in the same way.</p><p>Intuitively, we first directly use X 0 to approximate Y with a linear transform W 1 without pooling. Once W 1 has been initialized, X 1 = V 1 σ(W 1 X 0 ) is used to approximate Y again with another linear transform, W 2 . We repeat this process until all the matrices have been initialized. A similar strategy has been adopted by <ref type="bibr" target="#b32">[33]</ref>, which learns different levels of representations with a convolutional architecture. All of the above equations have closed-form solutions. For example,</p><formula xml:id="formula_16">W 0 = (O T O) −1 (O T Y X 0 T )(X 0 X 0 T ) −1 .</formula><p>The other matrices can be computed in the same way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Parameter Update</head><p>We update all the weight matrices after the initialization by minimizing the loss function of reconstruction error</p><formula xml:id="formula_17">E(X 0 ; W) = Y − Y 2 F , (<label>11</label></formula><formula xml:id="formula_18">)</formula><p>where W = {W 1 , . . . , W 4 }. X 0 = {x 0 i }, Y = {y i }, and Y = {y i } are a set of input images, a set of target images, and a set of reconstructed images, respectively. We update W using the stochastic gradient descent, in which the update rule of W i , i = 1 . . . 4, in the k-th iteration is</p><formula xml:id="formula_19">Δ k+1 = 0.9 • Δ k − 0.004 • •W i k − • ∂E ∂W i k ,<label>(12)</label></formula><formula xml:id="formula_20">W i k+1 = Δ k+1 + W i k , (<label>13</label></formula><formula xml:id="formula_21">)</formula><p>where Δ is the momentum variable <ref type="bibr" target="#b19">[20]</ref>, is the learning rate, and ∂E ∂W i = x i−1 (e i ) T is the derivative, which is computed as the outer product of the back-propagation error e i and the feature of the previous layer x i−1 . In our deep network, there are three different expressions of e i . First, for the transformation layer, e 4 is computed based on the derivative of the linear rectified function <ref type="bibr" target="#b18">[19]</ref> </p><formula xml:id="formula_22">e 4 j = [y − y] j , δ 4 j &gt; 0 0, δ 4 j ≤ 0 , (<label>14</label></formula><formula xml:id="formula_23">)</formula><p>where δ 4 j = [W 4 x 3 ] j . [•] j denotes the j-th element of a vector.</p><p>Similarly, back-propagation error for e 3 is computed as</p><formula xml:id="formula_24">e 3 j = [W 4 T e 4 ] j , δ 3 j &gt; 0 0, δ 3 j ≤ 0 , (<label>15</label></formula><formula xml:id="formula_25">)</formula><p>where δ 3 j = [W 3 x 2 ] j . We compute e 1 and e 2 in the same way as e 3 since they both adopt the same activation function. There is a slight difference due to down-sampling. For these two layers, we must up-sample the corresponding back-propagation error e so that it has the same dimensions as the input feature. This strategy has been introduced in <ref type="bibr" target="#b13">[14]</ref>. We need to enforce the weight matrices to have locally connected structures after each gradient step as introduced in <ref type="bibr" target="#b11">[12]</ref>. We implement this by setting the corresponding matrix elements to zeros, if there supposed to be no connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We conduct two sets of experiments. Sec.5.1 compares with state-of-the-art methods and learning-based descriptors. Sec.5.2 demonstrates that classical face recognition methods can be significantly improved when applied on our reconstructed face images in the canonical view.</p><p>Dataset. To extensively evaluate our method under different poses and illuminations, we select the MultiPIE face database <ref type="bibr" target="#b6">[7]</ref>, which contains 754,204 images of 337 identities. Each identity has images captured under 15 poses and 20 illuminations. These images were captured in four sessions during different periods. Like the previous methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b16">17]</ref>, we evaluate our algorithm on a subset of the MultiPIE database, where each identity has images from all the four sections under seven poses from yaw angles −45 • ∼ +45 • , and 20 illuminations marked as ID 00-19 in MultiPIE. This subset has 128,940 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Face Recognition</head><p>The existing works conduct experiments on MultiPIE with three different settings: Setting-I was introduced in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34]</ref>; Setting-II and Setting-III were introduced in <ref type="bibr" target="#b16">[17]</ref>. We describe these settings below.</p><p>Setting-I and Setting-II only adopt images with different poses, but with neutral illumination marked as ID 07. They evaluate robustness to pose variations. For Setting-I, the images of the first 200 identities in all the four sessions are chosen for training, and the images of the remaining 137 identities for test. During test, one frontal image (i.e. 0 • ) of each identity in the test set is selected to the gallery, so there are 137 gallery images in total. The remaining images from −45 • ∼ +45 • except 0 • are selected as probes. For Setting-II, only the images in session one are used, which only has 249 identities. The images of the first 100 identities are for training, and the images of the remaining 149 identities for test. During test, one frontal image of each identity in the test set is selected in the gallery. The remaining images from −45 • ∼ +45 • except 0 • are selected as probes.</p><p>Setting-III also adopts images in session one for training and test, but it utilizes the images under all the 7 poses and 20 illuminations. This is to evaluate the robustness when both pose and illumination variations are present. The selection of probes and gallery are the same as Setting-II.</p><p>We evaluate both the FIP features and the reconstructed images using the above three settings. Face images are roughly aligned according to the positions of eyes, and rescaled to 96×96. They are converted to grayscale images. The mean value over the training set is subtracted from each pixel. For each identity, we use the images with 6 poses ranging from −45 • ∼ +45 • except 0 • , and 19 illuminations marked as ID 00-19 except 07, as input to train our deep network. The reconstruction target is the</p><formula xml:id="formula_26">−45 • −30 • −15 • +15 • +30 • +45 • Avg Pose</formula><p>LGBP <ref type="bibr" target="#b33">[34]</ref> 37.7 62.5 77 83 59.2 36.1 59.3 VAAM <ref type="bibr" target="#b2">[3]</ref> 74.1 91 95.7 95.7 89.5 74.8 86.9 FA-EGFC <ref type="bibr" target="#b17">[18]</ref> 84.7 95 99.3 99 92.9 85.2 92.7 SA-EGFC <ref type="bibr" target="#b17">[18]</ref> 93 98.7 99.7 99. Table <ref type="table">1</ref>. Recognition rates under Setting-I. The first and the second highest rates are highlighted. " " indicates the method needs to know the pose; "×", otherwise. image captured in 0 • under neutral illumination (ID 07).</p><formula xml:id="formula_27">−45 • −30 • −15 • +15 • +30 • +45 • Avg</formula><p>In the test stage, in order to better demonstrate the proposed methods, we directly adopt the FIP and the reconstructed images (denoted as RL) as features for face recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Results of Setting-I</head><p>In this setting, we show superior results in Table <ref type="table">1</ref>, where the FIP and RL features are compared with four methods, including LGBP <ref type="bibr" target="#b33">[34]</ref>, VAAM <ref type="bibr" target="#b2">[3]</ref>, FA-EGFC <ref type="bibr" target="#b17">[18]</ref>, and SA-EGFC <ref type="bibr" target="#b17">[18]</ref>, and two learning-based descriptors, including LE <ref type="bibr" target="#b3">[4]</ref> and CRBM <ref type="bibr" target="#b9">[10]</ref>. As discussed in Sec.1, LGBP is a 2D-based method, while VAAM, FA-EGFC, and SA-EGFC used 3D face models. We apply LDA on LE, CRBM, FIP, and RL to obtain compact features. Note that LGBP, VAAM, and SA-EGFC need to know the pose of a probe, which means that they build different models to account for different poses specifically. We do not need to know the pose of the probe, since our deep network can extract FIP features and reconstruct the face image in the canonical view given a probe under any pose and any illumination. This is one of our advantages over existing methods. Several observations can be made from Table <ref type="table">1</ref>. First, RL performs best on the averaged recognition rates and five poses. The improvement is larger for larger pose variations. It is interesting to note that RL even outperforms all the 3D-based models, which verifies that our reconstructed face images in the canonical view are of high quality and robust to pose changes. Fig. <ref type="figure" target="#fig_8">4</ref> shows several reconstructed images, indicating that RL can effectively remove the variations of poses and illuminations, while still retains the intrinsic shapes and structures of the identities.  Second, FIP features are better than the two learningbased descriptors and the other three methods except SA-EGFC, which used the 3D model and required the pose of the probe. We further report the results of FIP compared with LE and CRBM using only 2 distance in Table <ref type="table" target="#tab_0">2</ref> . The RL and FIP outperform the above two learning based features, especially when large pose variations are present.</p><p>Third, although FIP does not exceed RL, its still a valuable representation, because it has the sparse property and can reconstruct RL efficiently and losslessly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Results of Setting-II and Setting-III</head><p>Li et al. <ref type="bibr" target="#b16">[17]</ref> evaluated on these two settings and reported the state-of-the-art results. Setting-II covers only pose variations and Setting-III covers both pose and illumination variations.</p><p>For Setting-II, the results of RL+LDA compared with <ref type="bibr" target="#b16">[17]</ref> are reported in Table <ref type="table" target="#tab_1">3</ref>, which shows that RL obtains the best results on all the poses. Note that the poses of probes in <ref type="bibr" target="#b16">[17]</ref> are assumed to be given, which means they trained a different model for each pose separately. <ref type="bibr" target="#b16">[17]</ref> did not report detailed recognition rates when the poses of the probes are unknown, except for describing a 20-30% decline of the overall recognition rate.</p><p>For Setting-III, RL+LDA is compared with <ref type="bibr" target="#b16">[17]</ref> on images with both pose and illumination variations. Table <ref type="table" target="#tab_2">4</ref> reports that our approach achieves better results on all the poses and illuminations. The recognition rate under a pose is the averaged result over all the possible illuminations. Similarly, the recognition rate under one illumination condition is the averaged result of all the possible poses. We observe that the performance of RL+LDA under different illuminations is close because RL can well remove the effect of different types of illuminations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Improve Classical Face Recognition Methods</head><p>In this section, we will show that the conventional feature extraction and dimension reduction methods in the face recognition literature, such as LBP <ref type="bibr" target="#b1">[2]</ref>, Gabor <ref type="bibr" target="#b30">[31]</ref>, PCA <ref type="bibr" target="#b10">[11]</ref>, LDA <ref type="bibr" target="#b0">[1]</ref>, and Sparse Coding (SC) <ref type="bibr" target="#b31">[32]</ref>, can achieve significant improvement when they adopt our reconstructed images as input.</p><p>We conduct three experiments using the training/testing data of Setting-I. First, we show the advantage of our reconstructed images in the canonical view over the original images. Second, we show the improvements of Gabor when it is extracted on our reconstructed images. Third, we show that LBP can be improved as well.</p><p>In the first experiment, 2 distance, SC, PCA, LDA, and PCA+LDA are directly applied on the raw pixels of the original images and our reconstructed images, respectively. The recognition rates are reported in Fig. <ref type="figure" target="#fig_7">5(a)</ref>, where the results on the original images and the reconstructed images are illustrated as solid bars (front) and hollow bars (back). We observe that each of the above methods can be improved at least 30% on average. They can achieve relatively high performance on different poses, because our reconstruction layer can successfully recover the frontal face image. For example, the recognition rates of SC on different poses using the original images are 20.9%, 43.6%, 65.0%, 66.1%, 38.3%, and 26.9%, respectively, while 92.7%, 97.1%, 97.8%, 98.5%, 97.8%, and 81.8%, respectively, using the reconstructed images.</p><p>In the second experiment, we extract Gabor features on both the original images and reconstructed images. We observe large improvements by using the reconstructed images. Specifically, for each image in 96 × 96, we evenly select 11 × 10 keypoints and apply 40 Gabor kernels (5 scales × 8 orientations) on each of these keypoints. We again use the 2 distance, PCA, LDA, and PCA+LDA for face recognition. The results are shown in Fig. <ref type="figure" target="#fig_7">5(b)</ref>.</p><p>In the third experiment, we extract LBP features on both original images and reconstructed images. Specifically, we divide each 96 × 96 image into 12 × 12 cells, and the 59 uniform binary patterns are computed in each cell. We then adopt the χ 2 distance, PCA, LDA, and PCA+LDA for face recognition. Fig. <ref type="figure" target="#fig_7">5</ref>(c) shows that LBP combined with all these methods can also be significantly improved. For instance, the averaged recognition rate of LBP+χ 2 using the original images is 75.9%, and the corresponding accuracy on our reconstructed images, i.e. RL+LBP+χ 2 , is 96.5%, which is better than 94.9% of RL+ 2 in Table <ref type="table" target="#tab_0">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have proposed identity-preserving features for face recognition. The FIP features are not only robust to pose and illumination variations, but can also be used to reconstruct face images in the canonical view. FIP is learned using a deep model that contains feature extraction layers and a reconstruction layer. We show that FIP features outperform the state-of-the-art face recognition methods. We have aslo improved classical face recognition methods by applying them on our reconstructed face images. In the future work, we will extend the framework to deal with robust face recognition in other difficult conditions such as expression change and face sketch recognition <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">30]</ref>, and will combine FIP features with more classic face recognition approaches to further improve the performance <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b26">27]</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Three face images under different poses and illuminations of two identities are shown in (a). The FIP features extracted from these images are also visualized. The FIP features of the same identity are similar, although the original images are captured in different poses and illuminations. These examples indicate that FIP features are sparse and identity-preserving (blue indicates zero value). (b) shows some images of two identities, including the original image (left) and the reconstructed image in the canonical view (right) from the FIP features. The reconstructed images remove the pose and illumination variations and retain the intrinsic face structures of the identities. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The LBP (a), LE (b), CRBM (c), and FIP (d) features of 50 identities, each of which has 6 images in different poses and illuminations are projected into two dimensions using Multidimensional scaling (MDS). Images of the same identity are visualized in the same color. It shows that FIP has the best representative power. Best viewed in color.</figDesc><graphic url="image-208.png" coords="2,60.21,69.72,232.85,173.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Architecture of the deep network. It combines the feature extraction layers and reconstruction layer. The feature extraction layers include three locally connected layers and two pooling layers. They encode an input face x 0 into FIP features x 3 . x 1 , x 2 are the output feature maps of the first and second locally connected layers. FIP features can be used to recover the face image y in the canonical view. y is the ground truth. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3</head><label>3</label><figDesc>Fig.3 shows the architecture of our deep model. The input is a face image x 0 under an arbitrary pose and illumination, and the output is a frontal face image under neutral illumination y. They both have n 0 = 96 × 96 = 9216 dimensions. The feature extraction layers have three</figDesc><graphic url="image-311.png" coords="3,0.00,90.00,612.00,612.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>a set of FIP features and Y = {y i } m i=1 are a set of target images, where m denotes the number of training examples. Our objective is to minimize the reconstruction error arg min</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>7 98.3 93.6 97.2 LE[4]+LDA 86.9 95.5 99.9 99.7 95.5 81.8 93.2 CRBM[10]+LDA 80.3 90.5 94.9 96.4 88.3 75.2 87.6 FIP+LDA 93.4 95.6 100.0 98.5 96.4 89.8 95.6 RL+LDA 95.6 98.5 100.0 99.3 98.5 97.8 98.3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>− 45 •</head><label>45</label><figDesc>−30 • −15 • +15 • +30 • +45 • Avg PoseLi<ref type="bibr" target="#b16">[17]</ref> 97.0 97.0 100.0 100.0 97.0 92.0 96.8 RL+LDA 97.8 98.6 100.0 100.0 98.6 98.4 98.4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The conventional face recognition methods can be improved when they are applied on our reconstructed images. The results of three descriptors (pixel intensity, Gabor, and LBP) and four face recognition methods ( 2 or χ 2 distance, sparse coding (SC), PCA, and LDA) are reported in (a), (b) and (c), respectively. The hollow bars are the performance of these methods applied on our reconstructed images, while the solid bars are on the original images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Examples of face reconstruction.For each identity, we select its images with 6 poses and arbitrary illuminations. The reconstructed frontal face images under neutral illumination are visualized below. We clearly see that our method can remove the effects of both poses and illuminations, and retains the intrinsic face shapes and structures of the identity.</figDesc><graphic url="image-323.png" coords="8,96.59,70.56,422.53,216.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Recognition rates under Setting-I. The proposed features are compared with LE and CRBM using only the 2 distance for face recognition. The first and the second highest rates are highlighted. " " indicates the method needs to know the pose; "×", otherwise.</figDesc><table><row><cell>Pose</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Recognition rates of RL+LDA compared with Li<ref type="bibr" target="#b16">[17]</ref> under Setting-II. " " indicates the method needs to know the pose; "×", otherwise.Recognition Rates on Different Poses−45• −30 • −15 • +15 • +30 • +45 • Avg Pose</figDesc><table><row><cell>Li [17]</cell><cell cols="6">63.5 69.3 79.7 75.6 71.6 54.6 69.3</cell><cell></cell></row><row><cell>RL+LDA</cell><cell cols="6">67.1 74.6 86.1 83.3 75.3 61.8 74.7</cell><cell></cell></row><row><cell></cell><cell cols="6">Recognition Rates on Different Illuminations</cell><cell></cell></row><row><cell></cell><cell>00</cell><cell>01</cell><cell>02</cell><cell>03</cell><cell>04</cell><cell>05</cell><cell>06</cell></row><row><cell>Li [17]</cell><cell>51.5</cell><cell>49.2</cell><cell>55.7</cell><cell>62.7</cell><cell>79.5</cell><cell>88.3</cell><cell>97.5</cell></row><row><cell>RL+LDA</cell><cell>72.8</cell><cell>75.8</cell><cell>75.8</cell><cell>75.7</cell><cell>75.7</cell><cell>75.7</cell><cell>75.7</cell></row><row><cell></cell><cell>08</cell><cell>09</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell></row><row><cell>Li [17]</cell><cell>97.7</cell><cell>91.0</cell><cell>79.0</cell><cell>64.8</cell><cell>54.3</cell><cell>47.7</cell><cell>67.3</cell></row><row><cell>RL+LDA</cell><cell>75.7</cell><cell>75.7</cell><cell>75.7</cell><cell>75.7</cell><cell>75.7</cell><cell>75.7</cell><cell>73.4</cell></row><row><cell></cell><cell>15</cell><cell>16</cell><cell>17</cell><cell>18</cell><cell>19</cell><cell cols="2">Avg.</cell></row><row><cell>Li [17]</cell><cell>67.7</cell><cell>75.5</cell><cell>69.5</cell><cell>67.3</cell><cell>50.8</cell><cell cols="2">69.3</cell></row><row><cell>RL+LDA</cell><cell>73.4</cell><cell>73.4</cell><cell>73.4</cell><cell>72.9</cell><cell>72.9</cell><cell cols="2">74.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Recognition rates of RL+LDA compared with Li<ref type="bibr" target="#b16">[17]</ref> under Setting-III. " " indicates the method needs to know the pose; "×", otherwise.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">In our notation, X ∈ R a,b means X is a two dimensional matrix with a rows and b columns. x ∈ R a×b means x is a vector with a × b dimensions. Also, [x; y] means that we concatenate vectors or matrices x and y column-wisely, while [xy] means that we concatenate x and y row-wisely.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">For the convolutional neural network such as<ref type="bibr" target="#b13">[14]</ref>, the non-zero values are the same for each row.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">Note that in the conventional deep model<ref type="bibr" target="#b8">[9]</ref>, there is a bias term b, so that the output is σ(W x + b). Since W x + b can be written as W x, we drop the bias term b for simplification.</note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>† This work is supported by the General Research Fund sponsored by the Research Grants Council of the Kong Kong SAR (Project No. CUHK 416312 and CUHK 416510) and Guangdong Innovative Research Team Program (No.201001D0104648280).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Discriminant correspondence analysis. Encyclopedia of Measurement and Statistics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Abdi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Face description with local binary patterns: Application to face recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2037" to="2041" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fully automatic pose-invariant face recognition via 3d pose normalization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Tieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face recognition with learning-based descriptor</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Wide-baseline stereo for face recognition with large pose variation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-pie</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automatic Face and Gesture Recognition</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Descriptor learning based on fisher separation criterion for texture classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning hierarchical representations for face verification with convolutional deep belief networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Principal component analysis</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">487</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tiled convolutional neural networks</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
				<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th International Conference on Machine Learning</title>
				<meeting>26th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discriminant image filter learning for face recognition with local binary pattern like representation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Coupled bias-variance tradeoff for cross-pose face recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="305" to="315" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Morphable displacement field based image matching for face recognition across pose</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 27th International Conference on Machine Learning</title>
				<meeting>27th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the momentum term in gradient descent learning algorithms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On deep generative models with applications to recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep boltzmann machines</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
				<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="448" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pose, illumination and expression invariant pairwise face-similarity measure via doppelgänger list comparison</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Treibitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hybrid deep learning for face verification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Face sketch recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Toward a practical face recognition system: Robust alignment and illumination by sparse representation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="372" to="386" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dual-space linear discriminant analysis for face recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A unified framework for subspace face recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1222" to="1228" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Random sampling for subspace face recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="104" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Face photo-sketch synthesis and recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1955" to="1967" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Face recognition by elastic bunch graph matching</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wiskott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Fellous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kuiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Der Malsburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="775" to="779" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adaptive deconvolutional networks for mid and high level feature learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Local gabor binary pattern histogram sequence (lgbphs): A novel non-statistical model for face representation and recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Coupled information-theoretic encoding for face photo-sketch recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Face recognition across pose: A review</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2876" to="2896" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
