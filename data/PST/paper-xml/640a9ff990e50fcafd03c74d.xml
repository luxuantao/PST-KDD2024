<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structure-Aware Group Discrimination with Adaptive-View Graph Encoder: A Fast Graph Contrastive Learning Framework</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-03-09">9 Mar 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhenshuo</forename><surname>Zhang</surname></persName>
							<email>zs.zhang@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yun</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haizhou</forename><surname>Shi</surname></persName>
							<email>haizhou.shi@rutgers.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siliang</forename><surname>Tang</surname></persName>
							<email>siliang@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Structure-Aware Group Discrimination with Adaptive-View Graph Encoder: A Fast Graph Contrastive Learning Framework</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-03-09">9 Mar 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2303.05231v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Albeit having gained significant progress lately, large-scale graph representation learning remains expensive to train and deploy for two main reasons: (i) the repetitive computation of multi-hop message passing and non-linearity in graph neural networks (GNNs); (ii) the computational cost of complex pairwise contrastive learning loss. Two main contributions are made in this paper targeting this twofold challenge: we first propose an adaptiveview graph neural encoder (AVGE) with a limited number of message passing to accelerate the forward pass computation, and then we propose a structure-aware group discrimination (SAGD) loss in our framework which avoids inefficient pairwise loss computing in most common GCL and improves the performance of the simple group discrimination. By the framework proposed, we manage to bring down the training and inference cost on various large-scale datasets by a significant margin (250x faster inference time) without loss of the downstream-task performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">introduction</head><p>Graph Neural Networks (GNNs) have shown superiority in dealing with graph-structured data, such as social networks-fan2019graph, traffic networks <ref type="bibr" target="#b1">[Derrow-Pinion et al., 2021]</ref>, and molecular graphs <ref type="bibr" target="#b9">[Xie et al., 2021]</ref>. In real-world scenarios, however, large-scale graph data often lack humanannotated labels, which creates a huge barrier for the traditional supervised learning paradigm. To conquer this limitation, self-supervised graph representation learning methods have been widely studied, among which Graph Contrastive Learning (GCL) is dominant due to its ability to learn robust and generalizable representations for the downstream tasks <ref type="bibr" target="#b9">[Velickovic et al., 2019;</ref><ref type="bibr" target="#b2">Hassani and Khasahmadi, 2020;</ref><ref type="bibr">Thakoor et al., 2021a]</ref>. In GCL, the graph data encoder is trained to produce the representation space that minimizes the distance between the semantically invariant perturbed instances, e.g., sub-graphs created with mild augmentation, and maximizes the distance between irrelevant instances, e.g., randomly sampled sub-graphs.</p><p>Figure <ref type="figure" target="#fig_1">1</ref>: The architecture of separating 2-hop message passing and feature transform (above), compare to 2-layer GCN architecture (below). GCN will degrade to MLP if feature message passing is predcomputed and removed in each GCN layer.</p><p>Although proven to be effective, the existing GCL methods have limitations in real-world large-scale graph data applications: since they typically require large amounts of time and computational resources to deploy. For one thing, the most common GNN encoders utilize multi-hop information in graphs by multi-layer message passing in every calculation step, which leads to large computational costs for both training and inference. And for another, the predominant pairwise constrictive loss is not efficient enough and takes lots of time until convergence. In the supervised setting, there are several works addressing the first problem by reducing the number of parameterized message passing <ref type="bibr" target="#b9">[Wu et al., 2019]</ref> or distilling the trained GNN to Multi-Layer Perceptron (MLP) to improve inference speed <ref type="bibr" target="#b8">[Tian et al., 2022]</ref>. As for the second problem, various techniques have been studied such as simplifying positive and negative sample construction process <ref type="bibr">[Mo et al., 2022]</ref>, and removing the negative sample generation process <ref type="bibr">[Thakoor et al., 2021a;</ref><ref type="bibr">Grill et al., 2020;</ref><ref type="bibr" target="#b8">Shi et al., 2020]</ref>. However, those works didn't explore the application of a more efficient encoder in GCL and discuss the relationship between encoder and pretext tasks.</p><p>In this paper, we propose a novel GCL framework (AVGE-SAGD) to tackle the aforementioned two challenges. It contains an adaptive-view graph encoder (AVGE) that achieves higher training and inference speed than the GNN counterparts, and a structure-aware group discrimination (SAGD) module that increases the speed of the pretext contrastive task training. In the AVGE, instead of using GNN, we first perform a limited number of message-passing to generate a multi-view feature vector that consists of multi-hop features.</p><p>During training, the multi-view features are adaptively input to the encoder. And then we use an MLP encoder to further learn high-level representations for the pretext task. This encoder is significantly more efficient since it separates message passing and feature encoding and strictly controls the number of both operations. In the SAGD module, we first introduce the group discrimination loss to avoid inefficient pairwise contrastive loss computation <ref type="bibr" target="#b12">[Zheng et al., 2022]</ref>. Considering that the AVGE views the input multi-hop vector as a collection of independent features, it will lose the structural information of the original graph. Therefore to empower AVGE in the scenario of self-supervised representation learning, a novel structure prediction loss is added. It requires the encoded feature to further divide the graph into meaningful groups. By this extra topological constraint, we manage to prevent performance degradation and even achieve performance improvement.</p><p>To summarize, our contributions are as follows:</p><p>? We propose a graph encoder that adaptively utilizes multi-hop neighbor information, and separates the message passing from the encoder calculation procedure to save the repeated message passing calculation steps in the traditional GNN encoders, thereby improving the training speed and inference speed of our framework.</p><p>? We propose a novel structure-aware group discrimination (SAGD) module for GCL. It is built on graph group discrimination and further requires the encoder to subdivide the group into topology-based mini-groups so that the pre-trained model preserves more structural information and achieves better generalization ability for downstream tasks.</p><p>? Experiments on various node classification datasets showcase the effectiveness of our framework in terms of training and inference efficiency and downstreamtask performance. Especially on large-scale graph data, our method achieves comparable performance with less training time and 250x faster inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our framework involves two aspects: GNN encoder architecture and GCL methods. In this chapter, we introduce several previous works, discuss their limitations in GCL and propose our ideas for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">GNN Architecture</head><p>Architecture design is an important part of GNN research. The most mainstream GNN structures are designed based on message passing, the most widely known of which is GCN <ref type="bibr" target="#b4">[Kipf and Welling, 2016]</ref>. There are several works that tried to accelerate the computing speed of GNN by separating the message passing phase and feature calculation as shown in Figure <ref type="figure" target="#fig_1">1</ref>. The most known architecture is SGC <ref type="bibr" target="#b9">[Wu et al., 2019]</ref>, which removes the non-linearity calculation between GCN layers and simplified it to linear transform, showing that parameters-free linear message passing can achieve similar performance to GCNs. NAFS <ref type="bibr">[Zhang et al., 2022]</ref> present learning-free node-adaptive feature smoothing, assign fixed weights for features in different hops by computing the distance from the aggregated features to the extreme oversmoothed features, and combine the features in different hops by summation. Some studies have shown that under a certain design, the joint use of different hop features can enhance expressive ability. ASGC <ref type="bibr">[Chanpuriya and Musco, 2022]</ref> uses the linear regression method to fit the raw features by constructing a linear combination of different hop features, thereby solving the problem of heterophily graph node classification. <ref type="bibr">GCN-PND [He et al., ]</ref> updates graph topology based on the similarity between the local neighborhood distribution of nodes and designing extensible aggregation from multi-hop neighbors.</p><p>These methods of separating message passing and feature computation are all applied in supervised scenarios, and a combination method such as summation is used for multi-hop information to keep data scale. We explore the separation of message passing and feature computation GNN architectures in self-supervised scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Self-Supervised Graph Representation Learning</head><p>Self-supervised learning was first proposed in the computer vision area and has quickly received widespread attention in the community of graph learning due to its excellent performance in scenarios with few labeled training data. There are three levels of contrastive learning in the GCL field: graphgraph level <ref type="bibr">[You et al., 2020]</ref>, node-graph level <ref type="bibr" target="#b9">[Velickovic et al., 2019;</ref><ref type="bibr" target="#b2">Hassani and Khasahmadi, 2020]</ref>, and node-node level <ref type="bibr">[Peng et al., 2020a;</ref><ref type="bibr">Zhu et al., 2020b;</ref><ref type="bibr" target="#b16">Zhu et al., 2022]</ref>. Among those methods, we focus on the node-graph level since our framework is also a kind of nodegraph level contrastive learning. DGI <ref type="bibr" target="#b9">[Velickovic et al., 2019]</ref> obtains the graph-level representation by applying a readout function on the graph and maximizes the mutual information between the patch and the graph representation to perform node-graph level graph contrastive learning. MVGRL <ref type="bibr" target="#b2">[Hassani and Khasahmadi, 2020]</ref> uses multi-view constructiveness to extend the idea of DGI and borrow the idea from graph diffusion networks <ref type="bibr" target="#b4">[Klicpera et al., 2019]</ref> to improve the performance. The training loss of DGI can be simplified into a binary classification loss which is empirically and theoretically proven in <ref type="bibr" target="#b12">[Zheng et al., 2022]</ref>. The training scheme in <ref type="bibr" target="#b12">[Zheng et al., 2022]</ref> is coined as Group Discrimination which can implement efficient training but neglect the inner group relations which can be used to divide the original group into multiple mini-groups. In order to overcome these obstacles, we design SAGD by dividing it into mini-groups according to the structure, which is more helpful to our encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this chapter, we will introduce our framework AVGE-SAGD in detail. The overall processes are shown in Figure <ref type="figure" target="#fig_0">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Given a graph G = (X, A) with node attribute matrix X ? R N ?d , where N is the number of nodes, d is node attribute The training data will be fed to the MLP encoder. After projection and aggregation, the generated embeddings can be discriminated into the positive group and negative group.</p><p>dimension, and graph adjacency matrix A ? R N ?N , where A i,j = 1 if node i and j are connected, else A i,j = 0. For message passing, we follow the setting in GCN, using normalized adjacency matrix ? = D -1 2 AD -1 2 where D is the diagonal degree matrix and D ii = d i represent the number of degrees of node i. In our framework, we define the encoder as f ? : R N ?d ? R N ?d where d is the dimension of node representations.</p><p>The goal of GCL is to train a generalized graph encoder f ? by a pretext loss L without labels. For evaluating the pre-trained model on a specific downstream task (e.g., node classification), we will obtain the node representations by the frozen encoder (H = f ? (A, X)). Then, we will train a linear classifier built on these node representations from the training set by a supervised loss (e.g., cross-entropy loss). Lastly, we will use the test set to evaluate the performance of our pre-trained model with the linear classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generating Positive and Negative Samples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data augmentation</head><p>We adopt data augmentation in generating positive samples. Node attribute masking is a popular technique and is widely used in GCL methods (e.g., <ref type="bibr">GraphCL [You et al., 2020]</ref>, GGD <ref type="bibr" target="#b12">[Zheng et al., 2022]</ref>). We adopt this data augmentation technique to enrich the features of positive samples. In practice, partial dimensions of node attributes will be masked with 0.</p><p>The augmented node attributes X is obtained by:</p><formula xml:id="formula_0">X = X ? M,<label>(1)</label></formula><p>where M ? R N ?D is masking matrix and each row vector in M are equal (i.e., m i = m j , ?i, j), each element m ij in</p><formula xml:id="formula_1">m i ? {0, 1} D is is drawn from a Bernoulli distribution with probability p m (i.e., m ij ? B (1 -p m )).</formula><p>In order to keep the notation uncluttered, we use X to represent the augmented feature matrix in later sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corruption</head><p>We adopt corruption to generate negative samples. We randomly permutate the node attributes matrix and keep the topology structure unchanged:</p><formula xml:id="formula_2">? = { X, A}, X = PX, (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where P is a permutation matrix. This corruption technique is widely used in node-graph level GCL frameworks (e.g., DGI <ref type="bibr" target="#b9">[Velickovic et al., 2019]</ref>, MVGRL <ref type="bibr" target="#b2">[Hassani and Khasahmadi, 2020]</ref>) to encourage the representations including structural similarities of different nodes in the graph properly. In our framework, this corruption operation will mislead message passing (e.g., A X) to generate erroneous node attributes as negative samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adaptive-View Graph Encoder Post-message-passing features as training data</head><p>With data augmentation and corruption, we can obtain multiview features by parameters-free linear message passing: [ ?X; ?2 X; ...; ?K X] and [ ? X; ?2 X; ...; ?K X] which will be used to train the encoder f ? . These features can be reused during training and inference which can save a lot of computational time.</p><p>Considering that we store K views of attributes for each node by parameter-free linear message passing, the scale of training data of a graph with N nodes increases from N to KN compared to standard GNN encoders, which is certainly contradictory to the goal of reducing computing time and memory. We use a simple sample method to make a trade-off between performance and computation cost. In each epoch, we randomly sample N K nodes to keep the input training data size as N , which is consistent with the standard GNN encoder training process.</p><p>Another alternative approach is to use the average or summation of features in different hops. Unfortunately, the information of different hops will be mixed up which leads to performance degeneration. However, our sample method can explicitly use features in more views that provide more distinct and useful information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adaptive weighted training</head><p>Different hop features will be fed to train the encoder, but some hops' information is redundant and high-order hop features may incur oversmoothing <ref type="bibr">[Chen et al., 2020]</ref>. So, the contributions of each hop's feature should be disparate.</p><p>We assign individual adversarially learnable weight ? i to each hop feature ?i X. The training data can be reformulated as</p><formula xml:id="formula_4">[? 1 ?X, ? 2 ?2 X, ..., ? K ?K X].</formula><p>In order to avoid the training loss easily converging to 0 (i.e., through minimizing training loss, discrepant high-order features will have large weights ? i and weights of indiscernible low-order will easily collapse to zero), we use a twostep min-max optimization method to train the MLP encoder with adaptive weighted multiple receptive field features. This training method can be formulated as:</p><formula xml:id="formula_5">min ? max ? L(? i ?i X, ? i ?i X, ?) (3) s.t. K i ? i = 1, ?? i ? [0, 1],<label>(4)</label></formula><p>where ? is Xavier initialised <ref type="bibr">[Glorot and Bengio, 2010]</ref> and L is our training loss which will be described in Section 3.4. At each training step, firstly, we optimize adaptive weights with frozen model parameters by maximizing the training loss. Then, we optimize the parameters of the encoder with fixed adaptive weights by minimizing training loss. The optimization algorithm is described in Algorithm 1.</p><p>From a more theoretical aspect, our motivation is given from the analysis of research on homophilous graphs and heterophilous graphs <ref type="bibr">[Zhu et al., 2020a]</ref>. Homophily describes the similarity between adjacent nodes. The relevant studies <ref type="bibr" target="#b10">[Yan et al., 2021]</ref> show that graph representation learning will benefit from message passing in a homophilous graph and the opposite in a heterophilous graph. The corruption operation disrupts the graph connection relationship, which will make the corrupted graph turn into a heterophilous graph. As the order of message passing hop increases, the node attributes in the positive group and negative group will be separated spontaneously. So the model without an adaptive weighted training method will take shortcuts by overly using high-order-hop features during pre-training, which will con-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Structure-Aware Group Discrimination</head><p>Group discrimination It has been empirically proven that the contrastive learning task in DGI can be transformed into a binary classification task named group discrimination <ref type="bibr" target="#b12">[Zheng et al., 2022]</ref>. Following this method, we use a projector g ? (?) which consists of an MLP to map node representations into another latent space and then aggregate the projected representations. At last, we use binary cross entropy (BCE) loss to discriminate them into positive and negative groups, which are labeled as y i = 1 and y i = 0 respectively. The group discrimination loss can be formulated as:</p><formula xml:id="formula_6">L GD = - 1 2N 2N i=1 [y i log(? i ) + (1 -y i ) log(1 -?i )] ,<label>(5)</label></formula><p>where ?i = agg(g ? (h i )), agg(?) is summation aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preserving structure</head><p>Simple MLP encoder cannot preserve structure information <ref type="bibr" target="#b8">[Tian et al., 2022]</ref> because the training data are all independent node attributes in disparate hops. To solve this problem, we design auxiliary classification tasks to preserve structural information and capture the inner group relations so that the discriminated groups will be implicitly divided into mini-groups. Figure <ref type="figure" target="#fig_2">3</ref> shows the procedure of SAGD.</p><p>Here we introduce the concept of relative degree which evaluates the node degree compared to its neighbors' degrees. The definition of the relative degree of node v i is:</p><formula xml:id="formula_7">ri = 1 d i j?Ni d i d j .<label>(6)</label></formula><p>According to <ref type="bibr" target="#b10">[Yan et al., 2021]</ref>, the nodes with a high relative degree are more sensitive to homophily and heterophily. In our case, the positive (high homophily) and negative (high homophily) samples generated by nodes with a high relative degree are highly discrepant. That is, the relative degree is a qualified graph structure indicator.</p><p>So encoders that can distinguish r preserve structural information and will therefore have stronger expressive power.</p><p>We hope that the formulation of the structure-preserving task can hold a consistent format of the discrimination task, which will be beneficial for model optimization.</p><p>Considering that relative degree is a continuous variable, we set 1 as the threshold to discriminate whether a node has a high relative degree, which means y ri = 1 for a node with ri &gt; 1 and y ri = 0 otherwise.</p><p>The relative degree loss can be written as:</p><formula xml:id="formula_8">L degree = - 1 2N 2N i=1 [y ri log(? ri ) + (1 -y ri ) log(1 -?ri )] ,<label>(7)</label></formula><p>where f r : R D ? R is a summation aggregation function and ?r is the prediction result. Furthermore, we also use the hop order as the structural information that needs to be preserved. Similar to relative degree, we conduct a classification task to predict the order number of hop it belongs to through input features. The hop loss can be written as:</p><formula xml:id="formula_9">L hop = - 1 2N 2N i=1 y hop i log(? hop i )+ (1 -y hop i ) log(1 -?hop i )) ,<label>(8)</label></formula><p>where f hop : R D ? R and ? is the prediction result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Final SAGD loss</head><p>Our structure-aware group discrimination loss can be written as:</p><formula xml:id="formula_10">L = ?L GD + ?L hop + ?L degree ,<label>(9)</label></formula><p>where ?, ?, ? are hyper-parameters used for controlling the contributions of each loss. Empirically, we set ?, ?, ? as 1, 0.01, 0.05 respectively in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we demonstrate that our framework AVGE-SAGD can achieve comparable performance in unsupervised representation learning for node classification with exceptional training and inference time. We evaluate the performance and computation time cost on various node classification datasets with the standard experiment settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>The datasets we use to evaluate our approach contain two types desperated by the data scale: small-scale datasets include Cora, CiteSeer, PubMed <ref type="bibr" target="#b8">[Sen et al., 2008]</ref>, Amazon Computers and Amazon Photo <ref type="bibr" target="#b8">[Shchur et al., 2018]</ref> and large-scale datasets include ogbn-arxiv and ogbn-products provided by Open Graph Benchmark <ref type="bibr" target="#b3">[Hu et al., 2020]</ref>. Dataset statistics can be found in Appendix B.</p><p>In our implementation, we follow the standard data splits in <ref type="bibr" target="#b10">[Yang et al., 2016]</ref>. And for Amazon Computers and Photos, we randomly allocate 10/10/80% of data to training/validation/test set respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setup Model</head><p>For the encoder f ? , we use a 1-layer MLP for all datasets to save computing time. The projector g ? is also a 1-layer MLP. f r and f hop are summation functions used for structure prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inference</head><p>During the inference phase, we freeze the trained MLP encoder f ? and obtain final node representations H which can be used for downstream tasks with the processed input data. Since the input data comes from pre-processing, and the encoder is an MLP structure, the graph G is not needed in the inference stage, which saves a lot of computing resources in the message passing phase compared to the GNN encoder.</p><p>Different from the training step we use features in all hops as the training data, the final node representation is given from the features of the last hop only. In the training phase, we use the features of each hop. In order to maintain the consistency of the encoder input data, we only use one hop for inference in the inference stage. We choose the feature of the last hop because it contains the most neighbor information. Simple averaging of individual hop features will destroy high-order neighbor information. The final node representation is given by:</p><formula xml:id="formula_11">H = f ? ( ?K X),<label>(10)</label></formula><p>where f ? is the MLP encoder, ? is the normalized adjacency matrix of graph G and X is the original node attribute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation</head><p>In our experiment, we evaluate the performance of our method by node classification tasks, following the most common GCL methods <ref type="bibr">[Velickovic et al.,</ref>  our framework, the message passing step does not need back propagation so that it can be separated from the encoder training procedure. This calculation can be done on other servers in a distributed system. Therefore we do not measure the time required for message passing in ogbn-arxiv. However, in ogbn-products, even if other servers are used to calculate message passing, the calculation time is still very long. So we count the time required to calculate message passing locally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>First, we compare our framework with supervised GNNs (i.e., GCN <ref type="bibr" target="#b4">[Kipf and Welling, 2016]</ref>, GAT <ref type="bibr">[Velickovic et al., 2017]</ref>, SGC <ref type="bibr" target="#b9">[Wu et al., 2019]</ref>). Then we compare with some classical GCL methods (i.e., DGI <ref type="bibr" target="#b9">[Velickovic et al., 2019]</ref>, MVGRL <ref type="bibr" target="#b2">[Hassani and Khasahmadi, 2020]</ref>, GRACE <ref type="bibr">[Zhu et al., 2020b]</ref>, GMI <ref type="bibr">[Peng et al., 2020a]</ref>, <ref type="bibr">BGRL [Thakoor et al., 2021a]</ref>, GBT <ref type="bibr" target="#b0">[Bielak et al., 2022]</ref>). Finally, we compare a newly proposed efficient GCL method GGD <ref type="bibr" target="#b12">[Zheng et al., 2022]</ref>. The reported results of some baselines are from previous papers if available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results and Analysis</head><p>Results for small-scale datasets Table <ref type="table" target="#tab_0">1</ref> shows the classification results on five small-scale datasets, and we can draw some conclusions: (i) Experiment results show that our framework outperforms supervised GNNs and other state-of-the-art GCL baselines in all datasets, which shows the superiority of our AVGE-SAGD framework. (ii) Compared with GGD, our method surpasses it by a considerable margin (e.g., 1% absolute improvement on Photo dataset) indicating the significance of structureaware group discrimination. Our structure-aware group discrimination performs topology-based mini-group classification on the basis of graph group discrimination, which helps the model to learn more rich knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results for large-scale datasets</head><p>We evaluate the classification accuracy and computational efficiency of our model on two large-scale datasets provided by OGB <ref type="bibr" target="#b3">[Hu et al., 2020]</ref>: ogbn-arxiv and ogbn-products. Experiment results in Table <ref type="table" target="#tab_1">2</ref> and Table <ref type="table">3</ref> show that our framework has faster training speed and faster inference speed than most GCL frameworks, as well as GGD, which also uses group discrimination instead of pairwise contrastive learning paradigm. Although the result of our method is slightly lower than GRACE and BRGL in ogbn-arxiv, it saves a lot of computing resources and is memory-friendly. For ogbn-arxiv, we are 266 ? faster than GGD in inference time and for ogbn-products we are 301 ? faster. Since our message passing process does not contain parameters, our framework is still faster than the other GCL frameworks using GCN encoder. Due to the addition of auxiliary modules and tasks in our framework, which increases the number of additional calculations, the training speed improvement is relatively limited. But in the inference stage, the size of our model is equivalent to a simple MLP. So the inference efficiency has been greatly improved.</p><p>On the other hand, it is observed that on the large-scale dataset provided by OGB, the performance of GCL is inferior Table <ref type="table">3</ref>: Accuracy on node classification task and speed test on the large-scale dataset ogbn-products. In the method column, the number in the brackets means the dimension of embeddings. In the accuracy column, the number in the brackets means the training time with message passing. GGD* is the re-implementation on our devices with their official code.</p><p>to the basic supervised GCN. The reason is there are plenty of training data on these datasets while the main contribution of GCL is the scenario lacking training data, so it cannot performs better than supervised models on these datasets. In the small-scale datasets with very limited training data mentioned in the last paragraph, however, the overall performance of GCL is significantly improved compared with the supervised models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>To prove the effectiveness of the design module of our framework, we conduct ablation experiments with different modules under the same hyperparameters on Cora dataset. More results are in Appendix C. In max adaptive weight training method is the most significant part in the framework since the performance degrades without it. And with structure-preserving module, SAGD outperforms GGD in our framework. Furthermore, we observe that using fixed multi-hop feature training performs worse than using the last-hop feature only, which underscores the importance of our adaptive weighted training approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Visualization</head><p>To visually assess the quality of our learned embeddings, we adopt t-SNE [ <ref type="bibr">Van der Maaten and Hinton, 2008]</ref> to visualize the 2D projection of node embeddings on Cora dataset using raw features, random-init of AVGE-SAGD, GGD, and trained AVGE-SAGD in Figure <ref type="figure" target="#fig_3">4</ref>, where nodes in different labels have different colors. We can observe that the distribution of node embeddings in raw features and random-init are messy and intertwined. After training, node embeddings learned by AVGE-SAGD have a clear separation of clusters, which indicates the model can help learn representative features for downstream tasks. Compared to GGD, the margins of each cluster of node embeddings learned from AVGE-SAGD are much wider, which means higher quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we approach to the challenge of increasing the training and inference efficiency of the graph contrastive representation learning frameworks. In terms of improving the encoder's efficiency, we separate the message passing from the embedding prediction and design a novel adversarially adaptive weights multi-hop features. As for the pretraining loss, we built a new structure-aware group discrimination loss that helps our fast encoder to preserve more structural information, which consequently improves its general-ization ability on the downstream tasks. Extensive experiments conducted on both small-scale and large-scale datasets have shown the effectiveness of our framework regarding both downstream task performance and the training and inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Algorithm</head><p>The steps of the procedure of our method are summarised as:</p><p>Algorithm 2 Algorithm for structure-aware group discrimination with adaptive-view graph encoder Input: Graph G = (X, A), encoder f ? , projector g ? , structure preserver f r and f hop , training step E. Compute the final loss L = ?L GD + ?L hop + ?L degree 13:</p><p>Update trainable parameters using adaptive weighted training algorithm in Algorithm 1 14: end for 15: Obtain final embeddings H = f ? (A K X)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experiment Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset statistics</head><p>We present the details of node classification datasets we used in Table <ref type="table" target="#tab_4">5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Experiments</head><p>We conduct more ablation experiments on five small and medium datasets following the setting in Section 4.4. The results in Table <ref type="table">7</ref> show that our AVGE module and SAGD module can improve the performance separately, and the model achieves the best performance when we use both AVGE and SAGD techniques together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D More on Homo/Hetero-phily and Multi-Hop Features</head><p>In this section, we will discuss the relationship between homophily and adaptive-view graph encoder and group discrimination.</p><p>Homophily of a graph is typically defined based on the probability of edges connection between nodes within the same class in supervised node classification datasets <ref type="bibr">[Zhu et al., 2020a]</ref>. The homophily ratio of edges is defined as</p><formula xml:id="formula_12">h = 1 E (i,j)?E 1(y i = y j ),<label>(11)</label></formula><p>where |E| is the number of edges, 1(?) denotes the indicator function. Homophily characterizes the properties of the connection of nodes within the same class in graphs. Nodes in graphs with high homophily tend to be connected to nodes of the same class, while nodes in graphs with low homophily tend to be connected to nodes of different classes which is also called heterophily.</p><p>Homophily has an important impact on the performance of message-passing-based GNNs. On graphs with high homophily, message passing can improve the expressive ability of node representations while on graphs with low homophily, message passing will reduce the expressive ability of node representations instead. On some heterophilous graph datasets, the performance of GCN is not as good as MLP which directly uses node features <ref type="bibr">[Zhu et al., 2020a]</ref>.</p><p>The method we use to construct negative samples through permutation can be seen as disrupting the connection relationship of the graph, which means that the homophily of the graph becomes very low. Figure <ref type="figure" target="#fig_5">5</ref> shows the disparate results of message passing in homophilous and heterophilous graphs with the number of hops from 0 to 8. On the homophilous dataset (Cora, h = 0.81), with the increase of message passing order, the node features are divided into obvious positive and negative sample clusters from the initially mixed state, while on the hetreophilous dataset (Chameleon h = 0.32) <ref type="bibr" target="#b5">[Pei et al., 2020]</ref>, the node features are mixed together from start to finish. In our framework, the random permutation corruption method changes the node connection relationship while preserving the graph's global topology. So a graph with high homophily will degrade to a graph with low homophily after corruption. Therefore, on the homophilous graph, the difference between positive and negative samples will increase with the increase of the number of message-passing layers and finally lead to the spontaneous formation of two clusters. Features in higher-order hops will perform better in group discrimination pretext task but are not conducive to model learning because the loss is too low. This observation indicates the necessity of our two-step min-max optimization training method in Section 3.3, aiming to automatically control the significance of features in the different hops.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: The architecture of AVGE-SAGD. Given a graph G and node attribute matrix X, we first adopt an optional data augmentation and then generate negative samples by randomly permutating the node attributes matrix. Message passing is processed for both the positive sample and negative sample which will give K-views of features in K-hop. We sample N/K features in each view to keep the scale of training data.The training data will be fed to the MLP encoder. After projection and aggregation, the generated embeddings can be discriminated into the positive group and negative group.</figDesc><graphic url="image-2.png" coords="3,54.00,54.00,503.99,211.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>Adaptive weighted training algorithm Input: initial model parameter ? (0) , adaptive weight ? (0) , total training epoch E Parameter: ?, ? Output: Optimized model parameter ? (N ) 1: for e = 1 to E do 2:Maximization: fix ? = ? (e-1) and calculate the gradient of ?(e)   3:Minimization: fix ? = ? (e) and calculate the gradient of ?(e)   4: update ? and ? 5: end for 6: return Optimized parameter ?(E)   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The schematic diagram of SAGD.LGD means group discrimination loss and LSA means structure aware loss. On the basis of LGD distinguishing positive and negative samples, LSA further distinguishes the mini-group according to the structure.</figDesc><graphic url="image-3.png" coords="4,315.00,233.33,243.00,75.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The t-SNE visualization result of node embeddings on Cora dataset. (a) is the raw features, (b) is the node embeddings from random initialized AVGE-SAGD, (c)is the learned representation of GGD, (d) is the learned representation of AVGE-SAGD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1:</head><label></label><figDesc>Data augmentation and corruption. Augmented positive graph samples: {G i = (A, X)} and corrupted negative graph samples: { ?i = (A, X)}, where i ? N E 2: Linear message passing for positive graph samples {D i = [ ?X; ?2 X; ...; ?K X] i } and negative graph samples: { ?i = [ ? X; ?2 X; ...; ?K X] i }, where i ? N E 3: for e = 1 to E do 4: Sample N K node to obtain D and ? from D and ? 5: Concatenate D and ? to obtain D 6: Assign adaptive weight ? to D 7: Input D to obtain node embeddings H = f ? ( D) 8: obtain the group discrimination prediction vector ? = agg(g ? (H)) ? R 2N 9: obtain the structure-aware prediction vector ?r = f r (H)) ? R 2N and ?hop = f hop (H)) ? R 2N 10: Compute group discrimination loss L GD 11:Compute structure-aware loss L degree and L hop 12:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>MultiTableFigure 5 :</head><label>5</label><figDesc>Figure 5: Results of message passing in Cora (above) and Chameleon (below). The purple nodes are positive samples and the yellow nodes are negative samples.</figDesc><graphic url="image-12.png" coords="11,117.03,275.12,70.56,70.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Training efficiency is measured by the time spent per training epoch and inference efficiency is measured by the time spent for node embedding generation. We do not measure the time spent for classifying embeddings because we keep the complexity of the classifier the same. Note that in Experiments results for node classification task on small-scale datasets. We report accuracy(%) for all datasets. The best performance is in bold. OOM represents out-of-memory on NVIDIA GeForce RTX 3090 (24GB).</figDesc><table><row><cell>2019; Zhu et al., 2020c;</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Accuracy on node classification task and speed test on the large-scale dataset ogbn-arxiv. 'Training Time' represents the average training time in each epoch. 'Inference Time' represents the time required from inputting data to computing the embedding. GGD* is the re-implementation on our devices with their official code. '/' means the method is OOM under a full-graph training setting.</figDesc><table><row><cell cols="2">Methods Accuracy (%)</cell><cell>Training Time (s)</cell><cell>Inference Time (s)</cell></row><row><cell>GCN</cell><cell>71.7?0.3</cell><cell>-</cell><cell>-</cell></row><row><cell>MLP</cell><cell>55.5?0.2</cell><cell>-</cell><cell>-</cell></row><row><cell>Node2vec</cell><cell>70.1?0.1</cell><cell>-</cell><cell>-</cell></row><row><cell>DGI</cell><cell>70.3?0.2</cell><cell>/</cell><cell>/</cell></row><row><cell>GRACE</cell><cell>71.5?0.1</cell><cell>/</cell><cell>/</cell></row><row><cell>BGRL</cell><cell>71.6?0.1</cell><cell>/</cell><cell>/</cell></row><row><cell>GBT</cell><cell>70.1?0.2</cell><cell>6.19</cell><cell>0.13</cell></row><row><cell>GGD*</cell><cell>71.2?0.2</cell><cell>1.00</cell><cell>0.08</cell></row><row><cell>Ours</cell><cell>71.3?0.3</cell><cell>0.54</cell><cell>0.0003</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>, 'Multi-View Weights'</cell></row><row><cell>includes different strategies for adopting weights on multi-</cell></row><row><cell>view attributes by masking different components. The first</cell></row><row><cell>three rows assign fixed weights to different hop attributes.</cell></row><row><cell>[0, 0, ..., 1] means we only use the attributes of the last hop to</cell></row><row><cell>train the encoder. [1, 1, ..., 1] means we keep the contributions</cell></row><row><cell>of different hop attributes the same. The last three columns</cell></row><row><cell>represent that we use learnable weights to adjust weights</cell></row><row><cell>adaptively. 'min' represents that we optimize weights and</cell></row><row><cell>model parameters by minimizing training loss. 'min-max'</cell></row><row><cell>represents that we use a two-step adaptive weighted training</cell></row><row><cell>method, 'Structure Preserving' means structure-aware mod-</cell></row><row><cell>ule.</cell></row><row><cell>The results show that all of the modules we design are help-</cell></row><row><cell>ful for the performance of our framework. The two-step min-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies for AVGE-SAGD on Cora dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Datasets statisticsHyperparameters Among all hyperparameters, we consider hidden size, hop order, learning rate, ?, ?, and ? which are listed in Table6, where ?, ? and ? are the weight of loss discussed in Section 3.4</figDesc><table><row><cell>Datasets</cell><cell>Nodes</cell><cell>Edges</cell><cell cols="2">Features Classes</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,429</cell><cell>1,433</cell><cell>7</cell></row><row><cell>Citeseer</cell><cell>3,327</cell><cell>4,732</cell><cell>3,703</cell><cell>6</cell></row><row><cell>PubMed</cell><cell>19,717</cell><cell>44,338</cell><cell>500</cell><cell>3</cell></row><row><cell>Amazon Computers</cell><cell>13,752</cell><cell>245,861</cell><cell>767</cell><cell>10</cell></row><row><cell>Amazon Photo</cell><cell>7,650</cell><cell>119,081</cell><cell>745</cell><cell>8</cell></row><row><cell>ogbn-arxiv</cell><cell>169,343</cell><cell>1,166,243</cell><cell>128</cell><cell>40</cell></row><row><cell>ogbn-products</cell><cell cols="2">2,449,029 61,859,140</cell><cell>100</cell><cell>47</cell></row><row><cell cols="3">Computer infrastructures specifications</cell><cell></cell><cell></cell></row><row><cell cols="5">For hardware, we conduct all experiments on a computer</cell></row><row><cell cols="5">server with eight GeForce RTX 3090 GPUs with 24GB mem-</cell></row><row><cell cols="5">ory and 64 AMD EPYC 7302 CPUs. Besides, our code is</cell></row><row><cell cols="4">implemented based on PyTorch 1.12.1 and DGL0.9.1.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Parameter settings on seven datasets</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Graph barlow twins: A self-supervised representation learning framework for graphs. Knowledge-Based Systems</title>
		<author>
			<persName><surname>Bielak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.04139</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2022. 2022. 2022. 2020. 2020</date>
			<biblScope unit="volume">256</biblScope>
			<biblScope unit="page" from="3438" to="3445" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Measuring and relieving the oversmoothing problem for graph neural networks from the topological view</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Glorot and Bengio, 2010] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><surname>Derrow-Pinion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<editor>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mohammad</forename><surname>Gheshlaghi Azar</surname></persName>
		</editor>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010">2021. 2021. 2010. 2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21271" to="21284" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">He et al., ] Liancheng He, Liang Bai, and Jiye Liang. The impact of neighborhood distribution in graph convolutional networks</title>
		<author>
			<persName><forename type="first">Khasahmadi</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaveh</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hosein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khasahmadi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="4116" to="4126" />
		</imprint>
	</monogr>
	<note>Contrastive multi-view representation learning on graphs</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems</title>
		<author>
			<persName><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22118" to="22133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Welling ;</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><surname>Klicpera</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<idno>arXiv:1911.05485</idno>
	</analytic>
	<monogr>
		<title level="m">Diffusion improves graph learning</title>
		<editor>
			<persName><surname>Mo</surname></persName>
		</editor>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2016">2016. 2016. 2019. 2019. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Simple unsupervised graph representation learning</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName><forename type="first">Pei</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05287</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph representation learning via graphical mutual information maximization</title>
		<author>
			<persName><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020</title>
		<meeting>The Web Conference 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="259" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graph representation learning via graphical mutual information maximization</title>
		<author>
			<persName><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020</title>
		<meeting>The Web Conference 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="259" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Run away from your teacher: Understanding byol by a novel self-supervised approach</title>
		<author>
			<persName><surname>Sen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<idno>arXiv:2208.10010</idno>
	</analytic>
	<monogr>
		<title level="m">ICLR 2021 Workshop on Geometrical and Topological Representation Learning</title>
		<editor>
			<persName><surname>Thakoor</surname></persName>
		</editor>
		<imprint>
			<publisher>Laurens Van der Maaten and Geoffrey Hinton</publisher>
			<date type="published" when="2008">2008. 2008. 2018. 2018. 2020. 2020. 2021. 2021. 2022. 2008. 2008. 2017</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="93" to="93" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>AI magazine. Velickovic et al., 2017] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. stat, 1050:20</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mars: Markov molecular sampling for multi-objective drug discovery</title>
		<author>
			<persName><surname>Velickovic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10432</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019. 2019. 2019. 2019. 2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Simplifying graph convolutional networks</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Two sides of the same coin: Heterophily and oversmoothing in graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yan</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06462</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2021. 2021. 2016. 2016. 2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5812" to="5823" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nafs: A simple yet tough-to-beat baseline for graph representation learning</title>
		<author>
			<persName><surname>Zbontar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021. 2021. 2022. 2022</date>
			<biblScope unit="page" from="26467" to="26483" />
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Rethinking and scaling up graph contrastive learning: An extremely efficient approach with group discrimination</title>
		<author>
			<persName><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.01535</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7793" to="7804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04131</idno>
		<title level="m">Deep graph contrastive representation learning</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04131</idno>
		<title level="m">Deep graph contrastive representation learning</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.13846</idno>
		<title level="m">A robust self-aligned framework for node-node graph contrastive learning</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
