<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PREFAIL: A Programmable Tool for Multiple-Failure Injection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pallavi</forename><surname>Joshi</surname></persName>
							<email>pallavi@cs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">EECS</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<region>UC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haryadi</forename><forename type="middle">S</forename><surname>Gunawi</surname></persName>
							<email>haryadi@cs.berkeley.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">EECS</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<region>UC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Koushik</forename><surname>Sen</surname></persName>
							<email>ksen@cs.berkeley.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">EECS</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<region>UC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PREFAIL: A Programmable Tool for Multiple-Failure Injection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B53746A8272F4F1147F5BA0D881242A9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>General Terms Reliability</term>
					<term>Verification fault injection</term>
					<term>distributed systems</term>
					<term>testing Categories and Subject Descriptors D.4.5 [Operating Systems]: Reliability; D.2.5 [Software Engineering]: Testing and Debugging</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As hardware failures are no longer rare in the era of cloud computing, cloud software systems must "prevail" against multiple, diverse failures that are likely to occur. Testing software against multiple failures poses the problem of combinatorial explosion of multiple failures. To address this problem, we present PreFail, a programmable failure-injection tool that enables testers to write a wide range of policies to prune down the large space of multiple failures. We integrate PreFail to three cloud software systems (HDFS, Cassandra,  and ZooKeeper), show a wide variety of useful pruning policies that we can write for them, and evaluate the speed-ups in testing time that we obtain by using the policies. In our experiments, our testing approach with appropriate policies found all the bugs that one can find using exhaustive testing while spending 10X-200X less time than exhaustive testing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the arrival of the cloud computing era, large-scale distributed systems are increasingly in use. These systems are built out of hundreds or thousands of commodity machines that are not fully reliable and can exhibit frequent failures <ref type="bibr">[13,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b42">41]</ref>. Due to this reason, today's "cloud software" (i.e., software that runs on large-scale deployments) does not assume perfect hardware reliability. Cloud software has a great responsibility to correctly recover from diverse hardware failures such as machine crashes, disk errors, and network failures.</p><p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. OOPSLA <ref type="bibr">'11, October 22-27, 2011</ref>, Portland, Oregon, USA. Copyright c 2011 ACM 978-1-4503-0940-0/11/10. . . $10.00</p><p>Even if existing cloud software systems are built with reliability and failure tolerance as primary goals <ref type="bibr">[10,</ref><ref type="bibr">13,</ref><ref type="bibr" target="#b17">16]</ref>, their recovery protocols are often buggy. For example, the developers of Hadoop File System <ref type="bibr" target="#b39">[38]</ref> have dealt with 91 recovery issues over its four years of development <ref type="bibr" target="#b19">[18]</ref>. There are two main reasons for this. Sometimes developers fail to anticipate the kind of failures that a system can face in a real setting (e.g., only anticipate fail-stop failures like crashes, but forget to deal with data corruption), or they incorrectly design/implement the failure recovery code. There have been many serious consequences (e.g., data loss, unavailability) of the presence of recovery bugs in real cloud systems <ref type="bibr" target="#b6">[5,</ref><ref type="bibr" target="#b8">7,</ref><ref type="bibr">8,</ref><ref type="bibr" target="#b19">18]</ref>.</p><p>To test recovery, there has been some work that has proposed novel failure-injection tools and frameworks, but they primarily address single failures during program execution. However, cloud software systems face frequent, multiple, and diverse failures. In this regard, there is a need to advance the state-of-the-art of failure testing -multiple failures need to be systematically explored in program execution. Unfortunately, exercising multiple failures is not straight-forward. The challenge to deal with is the combinatorial explosion of multiple failures that can be exercised.</p><p>From our personal experience and our conversation with some developers of cloud software systems, we found that a tester can employ many different heuristics to prune the large combinations of multiple failures. For example, a tester might only want to fail a representative subset of the components of a system, or inject only a subset of all possible failure types, or reduce the number of failure-injection points with some optimizations, or explore failure-injection points that satisfy some code-coverage objectives, or fail probabilistically. Furthermore, the tester might want to use multiple heuristics together.</p><p>To enable testers to express many different pruning heuristics or policies, we design, implement, and evaluate PREFAIL, a programmable failure-injection tool for multiple-failure injection. More specifically, we make the following contributions in this paper.</p><p>1. We build a programmable failure-injection tool that allows testers to write policies to express the set of multiple-failure combinations (or sequences) that they want to explore. This way, we alleviate the need to ex-plore all possible multiple-failure combinations, which can be too huge to test with reasonable resources and time. To enable programmability, we decouple PREFAIL into two pieces: the failure-injection engine which is capable of interposing different execution points of the system under test and is responsible for performing failure injection at those points, and the failure-injection driver where testers can write pruning policies that "drive" the engine (i.e., make decisions about which failures to inject). PREFAIL provides suitable abstractions of failures and the execution points where the failures can be injected, and also profiles of executions where failures are injected. These abstractions can be used by testers to easily write a wide variety of pruning policies.</p><p>2. We present a number of pruning policies that we have written for distributed systems. If a tester has a good knowledge about the system being tested, then she can easily write appropriate policies to explore the failures that meet her testing objectives. But, even if the tester does not know much about the system, we show that she can still use generic coverage based policies (e.g., code-coverage and recovery-coverage based policies) to systematically test the system. In our experiments, we found all bugs in a system with appropriate policies in time that is much lesser than the time to exhaustively test all possible failure sequences (e.g., 20 hours vs. 1/2 hour).</p><p>3. We have integrated PREFAIL to three popular cloud systems: Hadoop File System (HDFS) <ref type="bibr" target="#b39">[38]</ref>, ZooKeeper <ref type="bibr" target="#b24">[23]</ref>,</p><p>and Cassandra <ref type="bibr" target="#b30">[29]</ref>. We provide a thorough evaluation of the speed-ups in the testing process that we obtain by using the pruning policies that we wrote. In terms of bug finding, so far we have focused more on HDFS. We found all of the 16 new bugs in HDFS that we had found in previous work <ref type="bibr" target="#b19">[18]</ref>, and also found 6 newer bugs.</p><p>We have made PREFAIL publicly available for download from http://sourceforge.net/projects/prefail/. We have, in fact, already worked with engineers from Cloudera Inc. to test their version of Hadoop software. More realworld adoption of PREFAIL is in progress.</p><p>In our previous work <ref type="bibr" target="#b19">[18]</ref>, we had begun the quest of finding techniques to prune down multiple-failure sequences. In this prior work, we only presented two rigid pruning policies which are hard-coded in the failure-injection tool that we built. Based on more experience and conversation with some developers of cloud software systems, we found that there were many more pruning policies that a tester would like to use. This led us to re-think and re-structure our failureinjection tool so that it can let testers easily and rapidly write various kinds of policies.</p><p>In the rest of the paper, we present an extended motivation for having a programmable failure-injection tool ( §2), the design and implementation of PREFAIL ( §3), examples of a wide range of pruning heuristics that we can write in  PREFAIL ( §4), evaluation of PREFAIL ( §5), limitations ( §6), related work ( §7), and finally conclusion ( §8) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Extended Motivation</head><p>In this section, we present an extended motivation for having a programmable tool for multiple-failure injection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Combinatorial Explosion of Multiple Failures</head><p>Testing systems against multiple failures is unfortunately not straight-forward -the challenge to deal with is the combinatorial explosion of multiple failures. This explosion is attributed to the complex characteristics of failures that can arise: different types of failures (e.g., crashes, disk failures, rack failures, network partitioning), different parts of the hardware (e.g., two among four nodes fail), and different timings (e.g., failures happen at different stages of the protocol). Exhaustively exploring all possible failure sequences can take a lot of computing resources and time.</p><p>Let's consider the code segment in Figure <ref type="figure" target="#fig_0">1</ref> that runs on a distributed system with two nodes, A and B. This program executes reads and writes (to the network and the disk) in each node. Given this program, hardware failures such as transient I/O failures, crashes, data corruptions, and network failures, can happen around the I/O operations. Thus, we would like to test the tolerance of this code against different kinds of failures by injecting those failures during its execution. For example, we can inject a transient I/O failure at the write call on line A1 by executing code that throws an IOException instead of executing the write call.</p><p>Let us suppose that a tester wants to test against crashes before read and write calls, and that she wants to inject two crashes in an execution. One possible combination is to crash before the write at A4 and then to crash before the write at B5. Overall, since there are 5 possible points to inject a crash on every node, there are 5 2 * N (N -1) possible ways to inject two crashes, where N is the number of participating nodes (N = 2 in the above example). Again, considering many other factors such as different failure types and more failures that can be injected during recovery, the number of all possible failure sequences can be too many to explore with reasonable computing resources and time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Need For Programmable Failure-Injection</head><p>To address the aforementioned challenge, we believe that there are many different ways in which a tester could reduce the number of failures to inject. Below, we present some ex-amples based on our personal experience and our conversation with some developers of cloud software systems. Our goal is to allow testers to express failure space pruning policies of different complexities so that they can choose a suitable policy based on testing budget and requirement.</p><p>Failing a component subset: Let's suppose a tester wants to test a distributed write protocol that writes four replicas to four machines, and let's suppose that the tester wants to inject two crashes in all possible ways in this execution to show that the protocol could survive and continue writing to the two surviving machines. A brute-force technique will inject failures on all possible combinations of two nodes (i.e., 4  2 ). However, to do this quickly, the tester might wish to specify a policy that just injects failures in any two nodes.</p><p>Failing a subset of failure types: Another way to prune down a large failure space is to focus on a subset of the possible failure types. For example, let's imagine a testing process that, at every disk I/O, can inject a machine crash or a disk I/O failure. Furthemore, let's say the tester knows that the system is designed as a crash-only software <ref type="bibr" target="#b7">[6]</ref>, that is, all I/O failures are supposed to translate to system crash (followed by a reboot) in order to simplify the recovery mechanism. In this environment, the tester might want to just inject I/O failures but not crashes because it is useless to inject additional crashes as I/O failures will lead to crashes anyway. Another good example is the rack-aware data placement protocol common in many cloud systems to ensure high availability <ref type="bibr" target="#b15">[14,</ref><ref type="bibr" target="#b39">38]</ref>. The protocol should ensure that file replicas should be placed on multiple racks such that if one rack goes down, the file can be accessed from other racks. In this scenario, if the tester wants to test the rack-awareness property of the protocol, only rack failures need to be injected (e.g., vs. individual node or disk failures).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coverage-based policies:</head><p>A tester might want to speed up the testing process with some coverage-based policies. For example, let's imagine two different I/Os (A and B) that if failed could initiate the same recovery path that performs another two I/Os (M and N). To ensure correct recovery, a tester should inject more failures in the recovery path. A brute-force method will perform 4 experiments by injecting two failures at AM, AN, BM, and BN (M and N cannot be exercised by themselves unless A or B has been failed). But a tester might wish to finish the testing process when she has satisfied some code coverage policy, for example, by stopping after all I/O failures in the recovery path (at M and N) have been exercised. With this policy, she only needs to run 2 experiments with failures at AM and AN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain-specific optimization:</head><p>In some cases, systemspecific knowledge can be used to reduce the number of failures. For example, consider 10 consecutive Java read I/Os that read from the same input file (e.g., f.readInt(), f.readLong(), ...). In this scenario, disk failure can start  The pruning policies written in the driver make failure decisions that drive the engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FI Task + Information Abstractions</head><p>to happen at any of these 10 calls. In a brute-force manner, a tester would run ten experiments where disk failure begins at 10 different calls. However, with some operating system knowledge, the tester might inject disk failure only on the first read. The reasoning behind this is that a file is typically already buffered by the operating system after the first call. Thus, it is unlikely (although possible) to have earlier reads succeed and the subsequent reads fail. In our experience, by reducing these individual failures, we greatly reduce the combinations of multiple failures.</p><p>Failing probabilistically: Multiple failures can also be reduced by only injecting them if the likelihood of their occurrence is greater than a predefined threshold <ref type="bibr" target="#b37">[36,</ref><ref type="bibr" target="#b41">40]</ref>. This technique is useful especially if the tester is interested in correlated failures. For example, two machines put within the same rack are more likely to fail together compared to those put across in different racks <ref type="bibr" target="#b15">[14]</ref>. A tester can use real-world statistical data to implement policies that employ some failure probability distributions.</p><p>In summary, there are many different ways in which a tester can reduce the number of failures and their combinations to be injected. Thus, we believe that there is a need for a programmable failure-injection tool that enables testers to express different pruning policies. In the following section, we describe our approach in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Programmable Failure Injection</head><p>In this section we present the design of PREFAIL. To enable programmability, we borrow the classic principle of separation of mechanism and policy <ref type="bibr">[2,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b40">39]</ref>. With this principle, we decouple our failure-injection framework into two pieces: the FI engine and the FI driver as depicted in Figure <ref type="figure" target="#fig_2">2</ref> (FI stands for failure-injection). The FI engine is the component that injects failures in the system under test, and the FI driver is the component that takes tester-specified policies to decide where to inject failures. The FI engine exposes failure related abstractions to the FI driver that can be used by the testers in their policies. In the following sections, we first illustrate the test workflow in PREFAIL ( §3.1), and then we explain the FI engine ( §3.2), the abstraction interface ( §3.3), the FI driver and policies ( §3.4), and finally the detailed al- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Test Workflow</head><p>Figure <ref type="figure">3</ref> shows an example scenario of the testing process in PREFAIL. The tester specifies three failures as the maximum number of failures to inject in an execution of the system under test. The FI engine first runs the system with zero failure during execution (i.e. without injecting any failure during execution). During this execution, it obtains the set of all execution points where failures can be injected (i.e., failure-injection points as described in Section 3.3): A, B, and C. Let us assume that we are interested only in crashes, and let A c , B c , and C c denote the injection of crashes (i.e., failure-injection tasks as described in Section 3.3) at the failure-injection points A, B, and C, respectively (for ease of reading, a failure-injection task X c is represented as X in a box in Figure <ref type="figure">3</ref>). Using the tester-specified policies, suppose PREFAIL prunes down the set of failure-injection tasks to A c and B c , and then exercises each failure-injection task in the pruned down set.</p><p>After exercising a failure-injection task, the FI engine records all failure-injection points seen where further crashes can be injected. For example, after exercising A c (that is, injecting a crash at A), the FI engine observes the failure-injection points D and E. From this information, the FI engine creates the set of sequences of two failureinjection tasks A c D c and A c E c that can be exercised while injecting two crashes in an execution. Similarly, it creates B c E c after observing the failure-injection point E in the execution that exercises B c .</p><p>As mentioned before, the number of all sequences of failure-injection tasks that can be exercised tends to be large. Thus, PREFAIL again uses the tester-specified policies to reduce this number. For example, a tester might want to test just one sequence of two crashes that exercises E c as the second crash. Thus, PREFAIL would automatically exercise just one of A c E c and B c E c to satisfy this policy instead of exercising both of them. The step from injecting two failures to three failures per execution is similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">FI Engine</head><p>The failure-injection tasks described above are created by the FI engine. The FI engine interposes different execution points in the system under test and injects failures at those points. The target failure-injection points and the range of failures that can be injected all depend on the objective of the tester. For example, interposition can be done at Java/C library calls <ref type="bibr" target="#b19">[18,</ref><ref type="bibr" target="#b32">31]</ref>, TCP-level I/Os <ref type="bibr" target="#b13">[12]</ref>, disklevel I/Os <ref type="bibr" target="#b36">[35]</ref>, POSIX system calls <ref type="bibr" target="#b29">[28]</ref>, OS-driver interfaces <ref type="bibr" target="#b25">[24]</ref>, and at many other points. Depending on the target failure-injection points, the range of failures that can be injected varies.</p><p>In our work, we use a failure-injection tool that we had built in prior work <ref type="bibr" target="#b19">[18]</ref> as the FI engine. This particular FI engine interposes all I/O related to calls to Java libraries and emulates hardware failures by supporting diverse failure types such as crashes, disk failures, and network partitioning at node and rack levels.</p><p>The FI driver tells the FI engine to run a set of experiments that satisfy the written policies. An experiment is an execution of the system under test with a particular failure scenario (could be one or multiple failures). For example, using the example in Figure <ref type="figure" target="#fig_0">1</ref>, the FI driver could tell the FI engine to run one experiment with one specific failure (e.g., a crash before the write at A4) or two concurrent failures (e.g., the same crash plus a crash before the write at B4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Abstractions</head><p>In this section, we provide the abstractions that bridge the FI engine and the FI driver. The FI engine provides the following abstractions of failures and execution points where failures can be injected, and of failure-injection experiments. These abstractions can be used by testers in their pruning policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Failure-Injection Point (fip). A failure-injection point</head><p>(fip) is a map from a set of keys K to a set of values V. It is a static abstraction of an execution point where a failure can be injected. A key k in K represents a part of the static or dynamic context associated with the execution point. For example, k could be 'func' to represent the function call being executed. It would be mapped to the name of the function in the fip. Other examples for k are: 'loc' for the location of the function call in the source code, 'node' for the node ID on which the execution occurs, 'target' for the target of the I/O executed by the function call (e.g., the name of the file being written to in case of a disk write I/O), and 'stack' for the stack trace. Table <ref type="table" target="#tab_2">1</ref> shows the fip corresponding to the execution point at the read call at line L5 in node B in Figure <ref type="figure" target="#fig_0">1</ref>. We denote the set of all failure-injection points by P.</p><p>2. Failure-Injection Task (fit). A failure-injection task (fit) is a pair of a failure type (e.g., crash, disk failure) and a failure-injection point. Thus, a fit f ∈ F × P, where F denotes the set of all failure types. Given a failure-injection point, there are different types of failures that can be injected at that point. For example, Table <ref type="table" target="#tab_2">1</ref> shows different fits that can be formed for the fip illustrated in the same table for three different types of failures (crash, data corruption, and disk failure). Exercising a fit f = (ft, fp) means injecting the failure type ft at the fip fp.</p><p>Since we are interested in injecting multiple failures during execution in addition to single failures, we also consider sequences of failure-injection tasks. We denote the set of all sequences of failure-injection tasks by Q. We call a sequence of failure-injection tasks as a failure sequence in short.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Per-experiment profile.</head><p>To allow powerful policies (a variety of policies) to be written, the FI driver profiles the execution of the system in every experiment, and makes the profiling information available to testers. Testers can use the profiles of already executed experiments to decide in their policies which failure sequences to exercise in future experiments. Our strategy in profiling an execution is by recording the set of failure-injection points observed during the execution. The reasoning behind this is that failure-injection points are typically built out of I/O calls, library calls, or system calls, and these calls can be used to approximately represent an execution of the system under test. Thus, an execution profile exp ∈ 2 P . Let allFips: Q → 2 P and postInjectionFips: Q → 2 P be the functions that return execution profiles of failure-injection experiments. Given a failure sequence fs, allFips(fs) returns the execution profile consisting of all fips observed during the experiment in which fs is injected, and postInjectionFips(fs) returns the set of all fips observed after fs has been injected. For the FSP = FSP ∪ {fs} 8: end for 9: return FSP empty sequence (), allFips and postInjectionFips both return the set of all fips seen in the execution in which no failure is injected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">FI Driver</head><p>Based on the abstractions above, the FI driver provides support for writing predicates that it uses to generate policies that express how to prune the failure space. For convenience and brevity, whenever we say that a tester writes a policy, we mean that the tester writes the predicate that is later used by the FI driver to generate the policy. A policy is a function p : 2 Q → 2 Q . It takes a set of failure sequences, and returns a subset of the sequences to be explored by the FI engine. Testers can use the failure and execution point abstractions, and execution profiles provided by the FI engine in their predicates. There are two different kinds of predicates that can be written to generate two different kinds of policies: filter and cluster policies. PREFAIL can also compose the policies generated from different predicates to obtain more complex policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Filter Policy</head><p>A filter policy uses a tester-written predicate flt: Q → Boolean. The predicate takes a failure sequence fs as an argument and implements a condition that decides whether to exercise fs or not. Algorithm 1 explains how a filter policy works. Given a predicate flt, the function fpGen fs, and retains fs in its result set FS P if the predicate holds for it.</p><formula xml:id="formula_0">: (Q → Boolean) → (2 Q → 2 Q ) (implemented</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Cluster Policy</head><p>A cluster policy uses a tester-implemented predicate cls: </p><formula xml:id="formula_1">Q × Q → Boolean.</formula><formula xml:id="formula_2">: (Q × Q → Boolean) → (2 Q → 2 Q ) (implemented in PREFAIL)</formula><p>generates a cluster policy out of it. The policy uses the predicate to partition its argument set of failure sequences FS into disjoint subsets FS/R cls . It then randomly selects one failure sequence fs from each equivalence class. Thus, the tester implements her notion of equivalence of failure sequences, and the policy uses the equivalence relation to select failure sequences such that all equivalence classes in its argument set of failure sequences are covered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Example Policies</head><p>We give brief examples of how one can use the filter and cluster policies. Suppose that a tester is interested in testing the tolerance of the setup stage of a distributed systems protocol against crashes. The tester can write the flt predicate in Figure <ref type="figure">4</ref>. The filter policy fpGen(flt) would retain a failure sequence fs only if every fit in fs corresponds to a crash in the setup stage (execution of the setup function).</p><p>In failure testing, since we are concerned with testing the correctness of recovery paths of a system, one way to reduce the number of failure sequences to test would be to cluster them according to the recovery paths that they would lead to. Out of all failure sequences that would lead to a particular recovery path, we can just choose and test one. To achieve this, we can write the cluster predicate in Figure <ref type="figure" target="#fig_4">5</ref>. If two failure sequences fs 1 and fs 2 have the same last fit, and their prefixes that leave the last fit out (fs 1P and fs 2P respectively) result in the same recovery path, then we can consider fs 1 and fs 2 to be equivalent in terms of the recovery paths that they would lead to since they involve injecting the same failure at the same execution point in the same recovery λ fs 1 , fs 2 . ( let rec(fs) = allFips(fs) \ allFips(()) in let eq(fs 1 , fs 2 ) = (rec(fs 1 ) == rec(fs 2 )) in let (f 11 , ..., path. PREFAIL's test workflow is such that when deciding whether to test a failure sequence (e.g., fs 1 ), all of its prefixal sequences (e.g., fs 1P ) would have already been tested, and thus we would have already seen the recovery paths that they lead to. Figure <ref type="figure" target="#fig_4">5</ref> uses the function rec to characterize a recovery path. It uses the set of all fips seen in the recovery path to characterize it. From all fips observed during an execution in which a failure sequence is injected, we subtract out the fips that are observed during normal program execution (that is, when no failure is injected) to obtain the fips seen in the recovery path. More details about recovery path clustering can be found in Section 4.4.</p><formula xml:id="formula_3">f 1m ) = fs 1 in let fs 1P = (f 11 , ..., f 1(m-1) ) in let (f 21 , ..., f 2n ) = fs 2 in let fs 2P = (f 21 , ..., f 2(n-1) ) in (eq(fs 1P , fs 2P ) ∧ (f 1m == f 2n ) ∧ (m ≥ 2) ∧ (n ≥ 2)) )</formula><p>PREFAIL also enables composition of policies. For example, the policies that use the predicates in Figures <ref type="figure">4</ref> and<ref type="figure" target="#fig_4">5</ref> (fpGen(flt) and cpGen(cls)) can be composed to first filter out only those failure sequences that have crashes in the setup stage, and then to cluster the filtered sequences according to the recovery paths that they would lead to. Section 4 shows how to write the policies in Python in PREFAIL, and also gives many other examples of policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Test Workflow Algorithm</head><p>Having outlined the major components of PREFAIL, this section presents the detailed algorithm of PREFAIL's test workflow (Algorithm 3). PREFAIL takes a system Sys to test, a list of tester-written predicates Preds, and the maximum number of failures N to inject in an execution of the system. The testing process runs in N + 1 steps. At step i (0 ≤ i ≤ N ), the FI engine of PREFAIL executes the system Sys once for each failure sequence of length i that it wants to test, and injects the failure sequence during the execution of the system. FS c is the set of all failure sequences that should be tested in the current step, and FS n is the set of failure sequences that should be tested in the next step. Initially FS c is set to a singleton set with the empty failure sequence as the only element. Therefore, in step 0 the FI engine executes Sys and injects an empty sequence of failures, i.e. it does not inject any failure. The FI engine observes the fips that are seen during execution, computes fits from them, and adds singleton failure sequences with these fits if pr is a cluster predicate then FSP = p(FSP ) 10: end for 11: return FSP to FS n . Therefore, FS n has failure sequences that the FI engine can exercise in the next step, i.e. in the i = 1 step. Before PREFAIL proceeds to the next step, it prunes down the set FS n using the predicates written by testers. The predicates in Preds are used to generate policies that are then applied to FS n (Algorithm 4). The policy generated from the first predicate is applied first to FS n , the second policy is then applied to the result of the first policy and so on. Note that the order of predicates is important since the policies generated from them may not commute. In step i = 1, FS c is set to the pruned down FS n from the previous step, and FS n is reset to the empty set. For each failure sequence fs in FS c , the failure-injection tool executes Sys and injects fs during execution. For each fit f that it computes from a fip observed after fs has been injected (that is, a fip in postInjectionFips(fs)), it generates a new failure sequence fs by appending f to fs, and adds fs to FS n . After Sys has been executed once for each failure sequence in FS c , PREFAIL prunes down the set FS n with predicates and moves to the next step. This process is repeated till the last step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Crafting Pruning Policies</head><p>In this section, we present the pruning policies that we have written and their advantages. More specifically, we present our integration of PREFAIL to Hadoop File System (HDFS) <ref type="bibr" target="#b39">[38]</ref>, an underlying storage system for Hadoop MapReduce [1], and show the policies that we wrote for it. We begin with an introduction to HDFS and then present the policies.</p><p>Overall, we make three major points in this section. First, by clearly separating the failure-injection mechanism and policy and by providing useful abstractions, we can write many different pruning policies clearly and concisely. Second, we show that policies can be easily composed together to achieve different testing objectives. Finally, we show that some policies can be reused for different target systems. We believe these advantages show the power of PREFAIL. We chose Python as the language in which testers can write policies in PREFAIL, though any other language could have also been chosen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">HDFS Primer</head><p>HDFS is a distributed file system that can scale to thousands of nodes. Here we describe the HDFS write protocol in detail. Figure <ref type="figure">6</ref> shows a simplified illustration of the write I/Os (both file system and network writes) occurring within the protocol. The protocol by default stores three replicas in three nodes, and is divided mainly into two stages: the setup stage and the data transfer stage; later, we will see how the recovery for each stage is different.</p><p>Our FI engine is able to emulate hardware failures on every I/O (every box in Figure <ref type="figure">6</ref>). As illustrated, there are 13 failure points that the FI engine interposes in this write protocol. (Note that, in reality, the write protocol performs more than 40 I/Os). At every I/O, the FI engine can inject a crash, a disk failure (if it's a disk I/O), or a network failure (if it's a network I/O). The figure also depicts many possible ways in which multiple failures can occur. For example, two crashes can happen simultaneously at failure-injection points B1 and B2, or a disk failure at D1 and a network failure at E3, and many more. Interested readers can learn more about HDFS from here <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b43">42]</ref> and our extended technical report (which depicts the write protocol in more detail) <ref type="bibr" target="#b26">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pruning by Failing a Component Subset</head><p>In distributed systems like HDFS, it is common to have multiple nodes participating in a distributed protocol. As mentioned earlier, let's say we have N participating nodes, and the developer wants to inject two failures on two nodes. Then there are N  2 failure sequences that one could inject. Worse, on every node (as depicted in Figure <ref type="figure">6</ref>), there could be many possible points to exercise the failure on that node.  To reduce the number of failure sequences to test, a developer might just wish to inject failures at all possible failureinjection points in any two nodes. She can write a cluster policy that uses the function in Figure <ref type="figure">7</ref> to cluster failure sequences that have the same context when the node is not considered as part of the context. With this policy, the developer can direct the FI engine to exercise failure sequences with two failures such that if the FI engine has already explored failures on a pair of nodes then it should not explore the same failures on a different pair of nodes. Using Figure <ref type="figure">6</ref> as an example, a failure sequence with simultaneous crashes at D1 and D2 is equivalent to another with crashes at D2 and D3.</p><p>We also want to emphasize that this type of pruning policy could be used for other systems. Consider a RAID system <ref type="bibr" target="#b34">[33]</ref> with N disks that a tester wishes to test by injecting failures at any two of its N disks. To do this, we definitely need a FI engine that works for RAID systems, but we can re-use much of the policy that we wrote for distributed systems for RAID systems. The only difference would be in the keys in the fips whose mappings we want to remove (i.e., for distributed systems we removed the mappings for the 'node' key in Figure <ref type="figure">7</ref>, for RAID systems we remove the mappings for 'disk' key). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pruning via Code-Coverage Objectives</head><p>Developers can achieve high-level testing objectives using policies. One common objective in the world of testing is to have some notion of "high coverage". In the case of failure testing, we can write policies that achieve different types of coverage. For example, a developer might want to achieve a high coverage of source locations of I/O calls where failures can happen.</p><p>To achieve high code-coverage with as few experiments as possible, the tester can simply compose the policies that use the flt function shown in Figure <ref type="figure">8</ref> and the cls function shown in Figure <ref type="figure">9</ref>. The filter policy explores failures at previously unexplored source locations by filtering out a failure sequence if the fip in its latest fit (last) has an unexplored source location. The function FIP in Figure <ref type="figure">8</ref> returns the fip in the argument fit. The function explored returns true if a failure has already been injected at the source location in the last fip in a previous failure-injection experiment. For brevity, we do not show the source code of fit Recovery Path (Fig. <ref type="figure" target="#fig_10">10</ref>)</p><formula xml:id="formula_4">SL SL+N A1c {ABCDE}×{234} B2c {ABCDE}×{134} C1c {FGI}×{23}, {CDE}×{23} ■ ■ C2c {FGJ}×{13}, {CDE}×{13} • • D1c {FG}×{23}, {CDE}×{23} E2c {FG}×{13}, {CDE}×{13}</formula><p>Table <ref type="table">2</ref>. HDFS Write Recovery. The table shows the detailed recovery I/Os of some fit s within the HDFS write protocol. The first column shows the fit s. A1c is the fit for crash at the I/O A1. (For simplicity, we do not distinguish here between an I/O and the failure-injection point that corresponds to the execution of the I/O). The second column shows the recovery paths returned by the getRecoveryPath function (Figure <ref type="figure" target="#fig_10">10</ref>) for every fit shown in the first column<ref type="foot" target="#foot_0">1</ref> . To save space, we use ×; {AB}×{12} represents the I/Os A1, A2, B1, and B2. The third and fourth columns represent two ways of characterizing the recovery path; the same shape represents the same class of recovery path. For example, the third column represents the characterization shown in Figure <ref type="figure" target="#fig_11">11</ref> which uses source location (SL) to characterize recovery. The fourth column uses source location and node ID (SL+N) to characterize recovery. these functions. The cls function in Figure <ref type="figure">9</ref> clusters failure sequences that have the same source location in their last fits. Thus, after the filter policy has filtered out failure sequences that have unexplored source locations, the cluster policy would cluster the failure sequences with the same unexplored source location into one group. With these policies, PREFAIL would exercise a failure sequence for each unexplored source location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Pruning via Recovery-Coverage Objectives</head><p>In failure testing, since we are concerned with testing the correctness of recovery paths of a system, another useful testing goal is to rapidly explore failures that lead to different recovery paths. To do this, a tester can write a cluster policy that clusters failure sequences leading to the same recovery path into a single class. PREFAIL can then use this policy to exercise a failure sequence from each cluster, and thus exercise a different recovery path with each failure sequence. Below, we first describe the HDFS write recovery protocol, and then explain the whole process of recoverycoverage based pruning in two steps: characterizing recovery path, and clustering failure sequences based on the recovery characterization. 3) to get the set of all fip s, a , observed during the execution in which fs is injected. Line 3 obtains the set of fip s observed when no failure is injected (represented by "[]"). Line 4 performs the "diff" of the two sets to obtain the fip s in the recovery path taken when fs is injected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">HDFS Write Recovery</head><p>As mentioned before, the HDFS write protocol is divided mainly into two stages: the setup stage and the data transfer stage. The recovery for each stage is different. Table <ref type="table">2</ref> shows in detail the recovery I/Os, that is, the I/Os that occur during execution while recovering from an injected failure (or failure sequence). We will gradually discuss the contents of the table in the following sections. In the setup stage, if a node crashes, the recovery protocol will repeat the whole write process again with a new pipeline. For example, in the first row of Table <ref type="table">2</ref>, after N1 crashes at I/O A1 (A1 c ), the protocol executes the entire set of I/Os again (ABCDE) in the new pipeline (N2-N3-N4). However, if a node crashes in the second stage, the recovery protocol will only repeat the second stage with some extra recovery I/Os on the surviving datanodes. For example, in the fifth row of Table <ref type="table">2</ref>, after N1 crashes at D1 (D1 c ), the protocol first performs some synchronization I/Os (FG), and then repeats the second stage I/Os (CDE) on the surviving nodes (N2 and N3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Characterizing Recovery Path</head><p>To write a recovery clustering policy, a tester has to first decide how to characterize the recovery path taken by a system. One way to characterize would be to use the set of fips observed in the recovery path. Figure <ref type="figure" target="#fig_10">10</ref> returns the "difference" of the fips observed in the execution in which a failure sequence is injected and the fips in the execution in which no failure is injected. The difference can be thought of as the fips that are observed in the "extra" execution that results or the recovery path that is taken when the failure sequence is injected.</p><p>A tester can use the set of fips observed in the recovery path to characterize the recovery path. Thus, two failure sequences that result in the same set of fips in the recovery path are considered to be equivalent. Instead of using all of the context in the fips, the tester might abstract out the fips and use only part of the context in them to characterize a recovery path. For example, the tester might want to use only the source locations of fips. Thus, she might consider two recovery paths to be the same if the I/Os in them occur at the same set of source locations. The function in Figure <ref type="figure" target="#fig_11">11</ref> considers this relaxed characterization of recovery paths. Thus, in PREFAIL, a tester has the power and flexibility to decide how to characterize and cluster recovery paths.</p><p>If we use the equivalence function in Figure <ref type="figure" target="#fig_11">11</ref> to cluster failure sequences that result in the same recovery path into the same class, then we would obtain four different equivalence classes for the HDFS write protocol. The third column in Figure <ref type="figure" target="#fig_2">2</ref> shows the four classes: , ■, •, and which represent the recovery paths {ABCDE}, {CDEF}, {CDEG}, and {CDE} respectively. Note that the recovery paths of A1 c and B2 c are considered to be equivalent ( ) as they have I/Os at the same set of source locations {ABCDE} even if the I/Os are executed in different nodes. However, if the tester decides to characterize recovery paths using both source location and node ID, then the recovery paths of A1 c and B2 c would be considered to be different ( and ), as shown in the last column in Table <ref type="table">2</ref>.</p><p>Figure <ref type="figure" target="#fig_13">12</ref> provides more details of how different I/Os shown in Figure <ref type="figure">6</ref> are grouped into different recovery classes. The left figure shows 4 recovery classes that result from the use of only source location to distinguish between different recovery paths. Even by just using source location, PREFAIL is able to distinguish between the two main recovery classes in the protocol ( and ). Furthermore, PRE-FAIL also finds two unique cases of failures that result in two more recovery classes (■ and •). In the first one (■), a crash at C1 leaves the surviving nodes (N2 and N3) with zerolength blocks, and thus the recovery protocol executes I/Os at a different source location (labeled with I in Table <ref type="table">2</ref>). In the second one (•), a crash at C2 leaves the surviving nodes (N1 and N3) with different block sizes (the first node has received the bytes, but not the last node), and thus I/Os at yet another different source location (labeled as J) are executed.</p><p>Figure <ref type="figure" target="#fig_13">12b</ref> shows the 8 recovery classes that result when node ID is used in addition to the source location to char-  The symbols (e.g., A1, A2) represent I/Os described in Figure <ref type="figure">6</ref>. A shape (e.g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>, ) surrounding an I/O #X represents the equivalence class of the I/O with regard to the recovery path that is taken by HDFS when a crash occurs at that I/O. Different shapes represent different equivalence classes. The two figures show how the I/Os are grouped differently into equivalence classes when recovery paths are characterized in different ways (e.g., (a) using source location only and (b) using source location and node ID).</head><p>1 d e f c l s ( fs1 , fs2 ): acterize recovery paths. If the tester uses all of the context present in a fip, the I/Os in the write protocol will be grouped into 10 recovery classes. Interested readers can find the explanations behind the different numbers of recovery classes in <ref type="bibr" target="#b26">[25]</ref>. In general, the more context information in fips considered, the more we can distinguish between different recovery paths, and hence the more the number of recovery classes of I/Os. Lesser context leads to fewer recovery classes and thus fewer failure-injection experiments, but might miss some corner-case bugs.</p><formula xml:id="formula_5">2 last1 = fs1 [ len ( fs1 ) -1 ] 3 last2 = fs2 [ len ( fs2 ) -1 ] 4 prefix1 = fs1 [ 0 : len ( fs1 ) -1 ] 5 prefix2 = fs2 [ 0 : len ( fs2 ) -1 ] 6 isEqv = eqvBySrcLoc ( prefix1 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Clustering Failure Sequences</head><p>After specifying the characterization of a recovery path, the tester can simply write a cluster policy that uses the cls function in Figure <ref type="figure" target="#fig_14">13</ref>. Given this policy, if there are two failure sequences, (prefix1, last) and (prefix2, last), such that prefix1 and prefix2 result in the same recovery path, then PREFAIL will exercise only one of the two sequences. To illustrate the result of this policy, let's consider the example in Table <ref type="table">2</ref>. The fit F c (crash at I/O F) can be exercised after any of the crashes at {DE}×{123} (i.e., 6 fits). Without the specified equivalent-recovery clustering, PRE-FAIL will run 6 experiments (D1 c F c .. E3 c F c ). But with this policy, PREFAIL will group all of the 6 failure sequences into a single class (D1 c /../E1 c + F c ) as all the prefixes have the same recovery class ( , as shown in Figure <ref type="figure" target="#fig_13">12</ref>), and thus will run only 1 experiment to exercise any of the 6 failure sequences. If the tester changes the clustering function such that it uses both source location and node ID to characterize a recovery path (Figure <ref type="figure" target="#fig_13">12b</ref>), then PREFAIL will run three experiments as the prefixes now fall into three different recovery classes ( , , and ▼).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Pruning via Optimizations</head><p>In general, failures can be injected before and/or after every read and write I/O, system call or library call. For some types of failures like crashes or disk failures, there are optimizations that can be performed to eliminate unnecessary failureinjection experiments. In the following sections, we present policies that implement optimizations for crashes and disk failures in distributed systems. Appendix A describes the optimizations for network failures and disk corruption. By reducing the number of individual failure-injection tasks, these optimizations also help in reducing the number of multiplefailure sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Crashes</head><p>In a distributed system, read I/Os performed by a node affect only the local state of the node, while write I/Os potentially affect the states and execution of other nodes. Therefore, we do not need to explore crashing of nodes around read I/Os. We can just explore crashing of nodes before write I/Os. Figure <ref type="figure" target="#fig_0">14</ref> shows a flt function that can be used to implement this optimization.</p><p>The second optimization that we can do for crashes is that we do not crash a node before the node performs a network write I/O that sends a message to an already crashed node. This is because crashing a node before a network write I/O can only affect the node to which the message is being sent, but the receiver node is itself dead in this case. The flt function that implements this optimization is shown in Figure <ref type="figure" target="#fig_0">16</ref> in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Disk Failures</head><p>For disk failures (permanent and transient), we inject failures before every write I/O call, but not before every read I/O call. Consider two adjacent Java read I/Os from the same input file (e.g., f.readInt() and f.readLong()). It is unlikely that the second call throws an I/O exception, but not the first one. This is because the file is typically already buffered by the OS. Thus, if there is a disk failure, it is more likely the case that an exception is already thrown by the first call. Thus, we can optimize and only inject read disk failures on the first read of every file (i.e., we assume that files are always buffered after the first read). The subsequent reads to the file will naturally fail. The policy for this optimization is similar to the one for network failure optimization that is explained in Appendix A (Figure <ref type="figure" target="#fig_18">17</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Failing Probabilistically</head><p>Finally, a tester can inject multiple failures if they satisfy some probabilistic criteria. We have not explored this strategy in great extent because we need some real-world failure statistic to perform real evaluation. However, we believe that specifying this type of policy in PREFAIL will be straightforward. For example, the tester can write a policy as simple as: return true if prob(fs) &gt; 0.1. That is, inject a failure sequence fs only if the probability of the failures happening together is larger than 0.1. The tester needs to implement the prob function that ideally uses some real-world failure statistic (e.g., a statistic that shows the probability distribution of two machine crashes happening at the same time).</p><p>In summary, the programmable policy framework allows testers to write various failure exploration policies in order to achieve different testing and optimization objectives. In addition, as different systems and workloads employ different recovery strategies, we believe this programmability is valuable in terms of systematically exploring failures that are appropriate for each strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation</head><p>In this section, we evaluate the different aspects of PREFAIL. We first list our target systems and workloads, along with the bugs that we found ( §5.1 and §5.2). Then, we quantify the effectiveness of pruning policies that we have written ( §5.3). Finally, we show the implementation complexity of PREFAIL ( §5.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Target Systems, Workloads, and Bugs</head><p>We have integrated PREFAIL on different releases of three popular "cloud" systems: HDFS <ref type="bibr" target="#b39">[38]</ref> v0.20.0, v0.20.2+320, and v0.20.2+737 (the last one is a release used by Cloudera customers <ref type="bibr" target="#b10">[9]</ref>), ZooKeeper <ref type="bibr" target="#b24">[23]</ref> v3.2.2 and v3.3.1, and Cassandra <ref type="bibr" target="#b30">[29]</ref> v0.6.1 and v0.6.5. These integrations show that it is easy to port our tool to real-world systems and releases. We evaluate PREFAIL on four HDFS workloads (log recovery, read, write, and append), two Cassandra workloads (key-value insert and log recovery), and one ZooKeeper workload (leader election). In this work, we focused more on Cloudera's HDFS, and thus present extensive evaluation numbers for it. We present partial results for other releases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Bugs Found</head><p>With PREFAIL, we were able to find all of the 16 bugs in HDFS v0.20.0 that we had reported in our previous work <ref type="bibr" target="#b19">[18]</ref>. We were told that many internal designs of HDFS have changed since that version. After we integrated PRE-FAIL to a much newer HDFS version (v0.20.2+737), we found 6 more previously unknown bugs (three have been confirmed, and three are still under consideration). Importantly, the developers believe that the bugs are crucial ones and are hard to find without a multiple-failure testing tool. These bugs are basically availability (e.g., the HDFS master node is unable to reboot permanently) and reliability bugs (e.g., user data is permanently lost). For brevity of space, we explain below only one of the new recovery bugs. This bug is present in the HDFS append protocol, and it happens because of multiple failures.</p><p>The task of the append protocol is to atomically append new bytes to three replicas of a file that are stored in three nodes. With two node failures and three replicas, append should be successful as there is still one working replica. However, we found a recovery bug when two failures were injected; the append protocol returns error to the caller and the surviving replica (that has the old bytes) is inaccessible. Here are the events that lead to the bug: The first nodecrash causes the append protocol to initiate a quite complex distributed recovery protocol. Somewhere in the middle of this recovery, a second node-crash happens, which leaves the system in an unclean state. The protocol then initiates another recovery again. However, since the previous recovery did not finish and the system state was not properly cleaned, this last initiation of recovery (which should be successful) cannot proceed. Thus, an error is returned to the append caller, and worse since the surviving replica is in an unclean state, the file cannot be accessed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effectiveness of Policies</head><p>We now report the effectiveness of some of the pruning policies that we have written. We first present the codecoverage (Section 4.3) and recovery-coverage (Section 4.4) based policies, and then the optimization-based policies (Section 4.5). CC and BF represent the code-coverage policy and brute-force exploration, respectively. R-L, R-LN, and R-All represent recoverycoverage policies that use three different ways to characterize recovery ( §4.3): using source location only (L), source location and node (LN ), and all information in fip (All). We stopped our experiments when they reached 10,000 (Hence, the maximum number of experiments is 10,000).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Coverage-Based Policies</head><p>We show the benefits of using different coverage-based failure exploration policies to prune down the failure space in different ways. Figure <ref type="figure" target="#fig_16">15</ref> shows the different number of experiments that PREFAIL runs for different policies. An experiment takes between 5 to 9 seconds to run. Here, we inject crash-only failures so that the numbers are easy to compare. The figure only shows numbers for multiple-failure experiments because injecting multiple failures is where the major bottleneck is.</p><p>With PREFAIL, a tester can choose different policies, and hence different numbers of experiments and speed-ups, depending on her time and resource constraints. For example, the code-coverage policy (CC) gives two orders of magnitude improvement over the brute-force approach because it simply explores possible crashes at source locations that it has not exercised before (e.g., after exploring two crashes, there is no new source location to cover in 3-crash cases). Recovery clustering policies (R-L, R-LN, etc.) on the other hand run more experiments, but still give an order of magnitude improvement over the brute-force approach. The more  <ref type="figure" target="#fig_16">15</ref>) and the number of crashes per run (#F), along with the actual number of bugs that trigger the failed experiments (#Bugs). The last column (#BugsR) is the number of bugs that can be found using randomized failure injection. (*) implies that these are the same bugs (i.e., bugs in 2failure cases often appear again in 3-failure cases).</p><p>relaxed the recovery characterization, the lesser the number of experiments (e.g., R-L vs. R-All).</p><p>Pruning is not beneficial if it is not effective in finding bugs. In our experience, the recovery clustering policies are effective enough in rapidly finding important bugs in the system. To capture recovery bugs in the system, we wrote simple recovery specifications for every target workload. For example, for HDFS write, we can write a specification that says "if a crash happens during the data transfer stage, there should be two surviving replicas at the end". If a specification is not met, the corresponding experiment is marked as failed.</p><p>Table <ref type="table">3</ref> shows the number of bugs that we found even with the use of the most relaxed recovery clustering policy (R-L, which only uses source location to characterize recovery). But again, a more exhaustive policy could find bugs that were not caught by a more relaxed one. For example, we know an old bug that might not surface with R-L policy, but does surface with R-LN policy which uses source location and node ID to characterize recovery. The last column in the table shows the number of bugs that we can find by using randomized failure injection, that is, by randomly choosing the execution points at which to inject crashes. For each workload, we execute the system as many times as we do for the recovery clustering policy, and randomly inject crashes in each execution. Randomized failure injection can find the bugs for the write and append workloads, but not for the log recovery workload. This is because the bugs for the log recovery workload are corner-case bugs; the proportion of failure sequences that lead to a log recovery bug is much smaller than that for a write or an append bug. This shows that randomized failure injection, though simple to implement, is not effective in finding corner-case bugs that manifest only in specific failure scenarios.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Optimization-Based Policies</head><p>Table <ref type="table" target="#tab_7">4</ref> shows the effectiveness of the optimizations of different failure types that we described in Section 4.5. The optimizations for network failures and data corruption are shown in Appendix A. Each cell presents two numbers X/Y where Y and X are the numbers of failure-injection experiments for single failures without using and with using the optimization respectively. Overall, depending on the workload, the optimizations bring 21 to 1 times (5 on average) of reduction in the number of failure-injection experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Complexity</head><p>The FI engine is based on our previous work <ref type="bibr" target="#b19">[18]</ref>, which is written in 6000 lines of Java code. We added around 160 lines of code to this old tool so it passes on appropriate failure and execution abstractions to the FI driver. The FI driver is implemented in 1266 lines of Python code. It implements a library of functions that testers can use to access fits, fips and execution profiles passed on by the FI engine. It also uses the policies written by testers to prune down the set of failure sequences that can be exercised by FI engine. We have written a number of different pruning policies in Python using the library provided by the FI driver. On an average, we wrote a policy in 17 lines of Python code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Limitations</head><p>PREFAIL does not control all kinds of non-determinism present in a system execution (e.g., network message ordering). Therefore, two executions of the same system against the same workload might be different, and PREFAIL might not be able to inject a failure sequence that seemed possible to inject from a previous system execution. In future work, we plan to control the non-determinism that arises out of network message ordering and also expose it to testers and provide support for writing policies that can express the message orderings to test for.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Related Work</head><p>In this section, we compare our work with other work that relates to failure-injection. More specifically, we discuss other related work that provide some language support for specifying failure-injection tasks, and present techniques to prune down large failure spaces.</p><p>There has been some work in designing a clear language support for expressing which failures to inject. FAIL (Fault Injection Language) is a domain-specific language that describes failure scenarios for Grid middleware <ref type="bibr" target="#b22">[21]</ref>. FIG also uses a domain-specific language to inject failures at library level <ref type="bibr" target="#b5">[4]</ref>. Orchestra uses TCL scripts to inject failures at TCP level <ref type="bibr" target="#b13">[12]</ref>. Genesis2 uses a scripting language to specify service-level failures <ref type="bibr" target="#b27">[26]</ref>. LFI uses an XML-based language to trigger failures at library level <ref type="bibr" target="#b32">[31]</ref>. These works however do not describe how a wide range of policies can be written in their languages. Furthemore, the tester might need to write code from scratch to build the failure-injection tasks in these languages. In contrast, in our work, we abstract out a failure-injection task, and let testers easily use the information in the abstraction to write policies.</p><p>Our work is motivated by the need to exercise multiple failures especially to test cloud software systems. As mentioned before, one major challenge is the large number of combinations of failures to explore. One direct way to explore the space is via randomness. For example, random injection of failures is employed by the developers at Google <ref type="bibr" target="#b8">[7]</ref>, Yahoo! <ref type="bibr" target="#b41">[40]</ref>, Microsoft <ref type="bibr" target="#b44">[43]</ref>, Amazon <ref type="bibr" target="#b21">[20]</ref>, and other places <ref type="bibr" target="#b23">[22]</ref>. Random failure-injection is relatively simple to implement, but the downside is that it can easily miss corner-case bugs that manifest only when specific failure sequences are injected.</p><p>Another approach is to exhaustively explore all possible failure scenarios by injecting sequences of failures in all possible ways during execution. However, we found that within the execution of a protocol (e.g., distributed write protocol, log recovery), there are potentially thousands of possible combinations of failures that can be exercised, which can take hundreds of hours of testing time <ref type="bibr" target="#b20">[19]</ref>. Thus, exhaustive testing is plausible only if the tester has enough time budget and computing resources.</p><p>Other than random and exhaustive approaches, there has been some work in devising smart techniques that systematically prune down large failure spaces. Extensible LFI <ref type="bibr" target="#b33">[32]</ref> for example automatically analyzes the system to find code that is potentially buggy in its handling of failures (e.g., system calls that do not check some error-codes that could be returned). AFEX <ref type="bibr" target="#b28">[27]</ref> automatically figures out the set of failure scenarios that when explored can meet a certain given coverage criterion like a given level of code coverage. It uses a variation of stochastic beam search to find the failure scenarios that would have the maximal effect on the coverage criterion. Fu et al. <ref type="bibr" target="#b16">[15]</ref> use compile-time analysis to find which failure-injection points would lead to the execution of which error recovery code. They use this information to guide failure injection to obtain a high coverage of recovery code. To the best of our knowledge, the authors of these works do not address pruning of combinations of multiple failures in distributed systems.</p><p>The multiple-failure combinatorial explosion problem is similar to the state explosion problem in model checking. Existing system model-checkers <ref type="bibr" target="#b44">[43,</ref><ref type="bibr" target="#b45">44]</ref> use domainspecific optimization techniques to address the state explosion problem. However, when it comes to multiple failures, we did not find any system model-checker that is able to effectively prune down combinations of multiple failures. We believe that some of the pruning strategies that we have introduced in our work can be integrated within a system model checker.</p><p>There has been some work in program testing <ref type="bibr" target="#b4">[3,</ref><ref type="bibr" target="#b12">11,</ref><ref type="bibr" target="#b18">17</ref>] that uses tester-written specifications or input generators to produce all non-isomorphic test inputs bounded by a given size. The specifications or generators can be thought of as being analogous to the tester-written pruning policies in PREFAIL, and the process of generating inputs from them by pruning down the input space can be thought of as being analogous to the process of pruning down the failure space using policies. The specifications are used only for the purpose of generating test inputs, and there is no support to address failures in the specifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We have presented PREFAIL, a programmable failureinjection tool that provides appropriate failure abstractions and execution profiles to let testers write a wide variety of policies to prune down large spaces of multiple-failure combinations. Currently, we are adding two other important features to PREFAIL: support for triaging of failed experiments, and parallelizing the whole architecture of PREFAIL. Since debugging each failed experiment can take a significant amount of time (many hours or even days), being able to automatically triage failed experiments according to the bugs that caused them can be very useful. Policies in PRE-FAIL already prune down a failure space and result in a speed-up of the entire failure testing process, but parallelizing PREFAIL would lead to an even greater speed-up. The test workflow of PREFAIL can in fact be very easily parallelized.</p><p>Overall, our goal in building PREFAIL is to help today's large-scale distributed systems "prevail" against possible hardware failures that can arise. Although so far we use PREFAIL primarily to find reliability bugs, we envision PREFAIL will empower many more program analyses "un-der failures". That is, we note that many program analyses (related to data races, deadlocks, security, etc.) are often done when the target system faces no failure. However, we did find data races and deadlocks under some failure scenarios. Therefore, for today's pervasive cloud systems, we believe that existing analysis tools should also run when the target system faces failures. The challenge is that some program analyses might already be time-consuming. Running them with failures will prolong the testing time. We believe the pruning policies that PREFAIL supports will be valuable in reducing the testing time for these analyses. And again, we hope that our work attracts other researchers to present other pruning alternatives.</p><p>Figure <ref type="figure" target="#fig_0">16</ref>. Crash optimization for network writes. The function accepts a failure sequence if for each crash at a network write to a receiver node rNode in the sequence, there is no preceding crash in the sequence that occurs in the node rNode . The function nodeAlreadyCrashed (also implemented by the tester but not shown) takes a failure sequence and a node as arguments, and returns true if there is a crash failure in the sequence that occurs in the given node.</p><p>network read that a thread of a node performs. If a particular thread performs a read call that has the same sender as the previous call, then we assume that it is a subsequent read on the same network message from the same sender to this thread (potentially buffered by the OS), and thus we do not explicitly inject a network failure on this subsequent read. In addition, we clear the read history if the node performs a network write, so that we can inject network failures when the node performs future reads on different network messages. Also, we do not inject a network failure if one of the nodes participating in the message is already dead. Figure <ref type="figure" target="#fig_18">17</ref> shows the flt function that can be used to implement the optimization for network failures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Data Corruption</head><p>In the case of disk corruption, after data gets corrupted, all reads of the data give unexpected values for the data. It is possible but very unlikely that the first read of the data gives a non-corrupt value and the second read in the near future gives a corrupt one. Thus, we can perform an optimization similar to the disk-failure case (Section 4.5.2).  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Example code.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. PREFAIL Architecture. The figure shows the separation of failure-injection engine (mechanism) and driver (policy).The pruning policies written in the driver make failure decisions that drive the engine.</figDesc><graphic coords="3,316.22,76.06,213.33,64.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Inputs: A filter predicate flt and a set of failure sequences FS 2: Output: A set of failure sequences FSP 3: FSP = {} 4: for fs in FS do Inputs: A cluster predicate cls and a set of failure sequences FS 2: Output: A set of failure sequences FSP 3: FSP = {} 4: E = FS/Rcls 5: for e in E do 6: fs = select an element from e randomly 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Recovery path cluster. Cluster two failure sequences if their last fit s are the same and their prefixes (that exclude the last fit s) result in the same recovery path.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 3 6 : 7 :Algorithm 4 3 :</head><label>36743</label><figDesc>PREFAILTest Workflow 1: INPUT: System under test (Sys), List of flt and cls predicates (Preds), Maximum number of failures per execution (N ) 2: FSc = {()} 3: FSn = {} 4: for 0 ≤ i ≤ N do 5: for each failure sequence fs in FSc do Execute Sys and inject fs during execution Profile execution using fips observed during execution 8: for each fit f computed from a fip in postInjectionFips(fs) do 9: fs = Append f to fs 10: FSn = FSn ∪ fs 11: Prune(Preds, FS) 1: FSP = FS 2: for predicate pr in Preds do if pr is a filter predicate then</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>to downstream nodes. (The last node does not need to forward the setup message) B After receiving setup message, create temporary block and meta files. C Stream data bytes to downstream nodes. (The last node does not need to stream the data bytes) D Write bytes to the block and meta files. E Send commit acks to upstream nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Figure 6. HDFS Write Protocol. The figure presents a simplified illustration of the HDFS write protocol. Each box represents an I/O(file system or network), and thus a failure-injection point. For ease of reading, we label each failure-injection point with an alphabetical symbol plus the node ID. The protocol begins with the client forming a pipeline (Client-N1-N2-N3) to the three nodes where replicas of a file will be stored. The client obtains these target nodes from the master (communication between client and master is not shown). For simplicity, we don't show many other I/Os such as other acknolwedgment and disk I/Os. We also do not show the rack-aware placement of replicas.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>1Figure 8 .Figure 9 .</head><label>89</label><figDesc>Figure 8. New source location filter. Return true if the source location of the last fip has not been explored. The function FIP returns the fip in the argument fit .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Obtaining Recovery Path FIPs. Line 2 uses the function allFips ( §3.3) to get the set of all fip s, a , observed during the execution in which fs is injected. Line 3 obtains the set of fip s observed when no failure is injected (represented by "[]"). Line 4 performs the "diff" of the two sets to obtain the fip s in the recovery path taken when fs is injected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Equivalence of recovery paths. Return true if two failure sequences result in the system executing I/Os at the same set of source locations during recovery. The function abstractIn retains only the mappings for the source locations ('loc') in the fip s in its argument set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Src. Loc. + Node ID. 8 recovery classes: , , , ■, •, , , ▼.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Recovery Classes of HDFS Write Protocol.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Equivalent-recovery clustering. Cluster two failure sequences if their prefixes (that exclude the last fit s) result in the same recovery path and their last fit s are the same. Line 6 uses the eqvBySrcLoc function in Figure 11 to compute the equivalence of the recovery paths of the prefixes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>1r e t u r n False 9 r e t u r n True Figure 14 .</head><label>True14</label><figDesc>Figure 14. Generic crash optimization. The function accepts a failure sequence if all crash failures in the sequence are injected before write I/Os. If a failure sequence has a crash that is not injected before a write I/O, then that sequence is rejected, and thus not exercised by the FI engine.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. #Experiments run with different coveragebased policies.The y-axis shows the number of failure-injection (just crash-only failure) experiments for a given policy and a workload. The x-axis shows the workloads: the write (Wrt), append (App), and log recovery (LogR) protocols from Cloudera's version of HDFS. We also run workloads from the old HDFS release v0.<ref type="bibr" target="#b21">20</ref>.0 (marked with *), which has a different design (and hence different results). Two and three crashes were injected per experiment for the bars on the left-and right-hand sides respectively. CC and BF represent the code-coverage policy and brute-force exploration, respectively. R-L, R-LN, and R-All represent recoverycoverage policies that use three different ways to characterize recovery ( §4.3): using source location only (L), source location and node (LN ), and all information in fip (All). We stopped our experiments when they reached 10,000 (Hence, the maximum number of experiments is 10,000).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>1</head><label></label><figDesc>d e f f l t ( fs ): 2 f o r i i n range ( len ( fs )): 3 fp = FIP ( fs [i ]) 4 isNetFail = ( fp [' failure '] == ' netfail ') 5 isRead = ( fp [' ioType '] == ' read ') 6 sender = fp [' sender '] 7 node = fp [' node '] 8 thread = fp [' thread '] 9 time = fp [' time '] 10 pfx = fs [0: i] 11 allFS = allFitSeqs () 12 i f isNetFail and isRead and 13 ( n o t first ( pfx , node , thread , time , sender , allFS )): 14 r e t u r n False 15 r e t u r n True</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 17 .</head><label>17</label><figDesc>Figure 17. Network failure optimization. The function checks for each network failure at a read I/O in a failure sequence to see if it is the first read of data in its thread that is sent by its sender to its node. The function first (also implemented by the tester, but not shown) determines this condition for each network failure in the failure sequence. The key time in a fip records the time when the fip was observed during execution in the FI engine. This key helps in determining the temporal position of a read in the list of all failure sequences allFS passed on by the FI engine.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 . Failure-Injection Point (fip) and Failure- Injection Task (fit). The left table illustrates a fip with label</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>fip B5</cell></row><row><cell cols="2">Key func loc node target file f Value read() Read.java (line L5) B stack stack trace</cell><cell>Possible Failures at fip B5 Crash Corruption Disk failure (disk failure, B5) / B5 d fit (crash, B5) / B5c (corruption, B5) / B5cr</cell></row><row><cell>...</cell><cell>...</cell></row><row><cell cols="3">B5 . More context can be added by adding more key-value map-</cell></row><row><cell cols="3">pings. The right-hand side table shows different fit s that can be</cell></row><row><cell cols="2">formed for the fip .</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>in PREFAIL) generates a filter policy out of it. The policy takes a set of failure sequences FS, applies the flt predicate on each sequence λ fs. ( let ((ft 1 , fp 1 ), ..., (ft n , fp</figDesc><table /><note><p><p>n )) = fs in let isCrash(ft) = (ft == crash) in let inSetup(fp) = fp['stack'] has 'setup' in i∈{1,...,n} isCrash(ft i ) ∧ inSetup(fp i ) )</p>Figure 4. Setup-stage filter. Return true if all fit s (ft1, fp1), . . . , (ftn, fpn) in fs correspond to a crash within the setup function.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 . Benefits of Optimization-based Policies. The</head><label>4</label><figDesc></figDesc><table><row><cell>table shows the benefits of the optimization-based policies on four</cell></row><row><cell>HDFS workloads (H), two Cassandra workloads (C), and one</cell></row><row><cell>ZooKeeper workload (Z). Each cell shows two numbers X/Y where</cell></row><row><cell>Y and X are the numbers of failure-injection experiments for single</cell></row><row><cell>failures without using and with using the optimization respectively.</cell></row><row><cell>N.A. represents a not applicable case; the failure type never occurs</cell></row><row><cell>for the workload. For write workloads, the replication factor is</cell></row><row><cell>3 (i.e., 3 nodes participating). (*) These write workloads do not</cell></row><row><cell>perform any disk read, and thus the optimization does not work</cell></row><row><cell>here.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The reader might wonder why the I/Os A, B, C, D, and E appear again in the recovery paths even though the getRecoveryPath function returns the "diff" between the I/Os in the execution with failures and in the normal execution path, and thus should exclude those I/Os. The answer is that these I/Os are executed in the recovery path too, but with different contexts (e.g. different message content, different generation number) that we incorporate in the fip. For simplicity, we do not discuss these detailed contexts here.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Acknowledgments</head><p>This material is based upon work supported by the NSF/ CRA Computing Innovation Fellowship and the National Science Foundation under grant numbers CCF-1016924, CNS-0720906, CCF-0747390, CCF-1018729, and CCF-0747390. The third author is supported in part by an Alfred P. Sloan Foundation Fellowship. We also thank Eli Collins and Todd Lipcon from Cloudera Inc. for helping us confirm the HDFS bugs that we found. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of NSF or other institutions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>We explain the optimizations that we perform to eliminate redundant failure-injection experiments for network failures and disk corruption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Network Failures</head><p>For network failures, we can perform an optimization similar to disk failures (Section 4.5.2). Since there is no notion of file in network I/Os, we keep information about the latest</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m">eqvBySrcLoc ( fs1 , fs2 ): 9 r1 = getRecoveryPath</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">getRecoveryPath ( fs2 ) 11 c1 = abstractIn (r1 , &apos; loc &apos;) 12 c2 = abstractIn (r2</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="http://hadoop.apache.org/mapreduce" />
		<title level="m">Hadoop MapReduce</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ownership Domains: Separating Aliasing Policy from Mechanism</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Aldrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Chambers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th European Conference on Object-Oriented Programming (ECOOP &apos;04)</title>
		<meeting>the 18th European Conference on Object-Oriented Programming (ECOOP &apos;04)<address><addrLine>Oslo, Norway</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-06">June 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automated Testing Based on Java Predicates</title>
		<author>
			<persName><forename type="first">Chandrasekhar</forename><surname>Boyapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarfraz</forename><surname>Khurshid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darko</forename><surname>Marinov</surname></persName>
		</author>
		<author>
			<persName><surname>Korat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Software Testing and Analysis (ISSTA &apos;02)</title>
		<meeting>the International Symposium on Software Testing and Analysis (ISSTA &apos;02)<address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07">July 2002</date>
			<biblScope unit="page" from="123" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">FIG: A Prototype Tool for Online Verification of Recovery Mechanisms</title>
		<author>
			<persName><forename type="first">Pete</forename><surname>Broadwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Traupman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Self-Healing</title>
		<imprint/>
		<respStmt>
			<orgName>Adaptive and Self-Managed Systems</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Chubby lock service for loosely-coupled distributed systems</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Burrows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Symposium on Operating Systems Design and Implementation (OSDI &apos;06)</title>
		<meeting>the 7th Symposium on Operating Systems Design and Implementation (OSDI &apos;06)<address><addrLine>Seattle, Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-11">November 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Crash-Only Software</title>
		<author>
			<persName><forename type="first">George</forename><surname>Candea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armando</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Ninth Workshop on Hot Topics in Operating Systems (HotOS IX)</title>
		<meeting><address><addrLine>Lihue, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-05">May 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Paxos Made Live -An Engineering Perspective</title>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Griesemer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Redstone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM Symposium on Principles of Distributed Computing (PODC &apos;07)</title>
		<meeting>the 26th ACM Symposium on Principles of Distributed Computing (PODC &apos;07)<address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-08">August 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bigtable: A Distributed Storage System for Structured Data</title>
		<author>
			<persName><forename type="first">Fay</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename><forename type="middle">C</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deborah</forename><forename type="middle">A</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Burrows</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Fikes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Gruber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Symposium on Operating Systems Design and Implementation (OSDI &apos;06)</title>
		<meeting>the 7th Symposium on Operating Systems Design and Implementation (OSDI &apos;06)<address><addrLine>Seattle, Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-11">November 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Contact Persons at Cloudera Inc</title>
		<author>
			<persName><forename type="first">Eli</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Lipcon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Benchmarking Cloud Serving Systems with YCSB</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erwin</forename><surname>Tam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 ACM Symposium on Cloud Computing (SoCC &apos;10)</title>
		<meeting>the 2010 ACM Symposium on Cloud Computing (SoCC &apos;10)<address><addrLine>Indianapolis, Indiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
		</imprint>
	</monogr>
	<note>Raghu Ramakrishnan, and Russell Sears</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automated Testing of Refactoring Engines</title>
		<author>
			<persName><forename type="first">Brett</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Dig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kely</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darko</forename><surname>Marinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering (ESEC/FSE &apos;07)</title>
		<meeting>the ACM SIGSOFT Symposium on the Foundations of Software Engineering (ESEC/FSE &apos;07)<address><addrLine>Dubrovnik, Croatia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">September 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Experiments on Six Commercial TCP Implementations Using a Software Fault Injection Tool. Software-Practice and Experience</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Dawson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farnam</forename><surname>Jahanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Mitton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1385" to="1410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Underneath the covers at google: Current systems and future directions</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Availability in Globally Distributed Storage Systems</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franis</forename><surname>Labelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florentina</forename><forename type="middle">I</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murray</forename><surname>Stokely</surname></persName>
		</author>
		<author>
			<persName><surname>Van-Anh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luiz</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carrie</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName><surname>Quinlna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Symposium on Operating Systems Design and Implementation (OSDI &apos;10)</title>
		<meeting>the 9th Symposium on Operating Systems Design and Implementation (OSDI &apos;10)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-10">October 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Testing of Java Web Services for Robustness</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><forename type="middle">G</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Milanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wonnacott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Software Testing and Analysis (ISSTA &apos;04)</title>
		<meeting>the International Symposium on Software Testing and Analysis (ISSTA &apos;04)<address><addrLine>Boston, Massachusetts</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07">July 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reliability/Resilience Panel</title>
		<author>
			<persName><forename type="first">Garth</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High-End Computing File Systems and I/O Workshop (HEC FSIO &apos;10)</title>
		<meeting><address><addrLine>Arlington, VA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-08">August 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Test generation through programming in UDITA</title>
		<author>
			<persName><forename type="first">Milos</forename><surname>Gligoric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tihomir</forename><surname>Gvero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vilas</forename><surname>Jagannath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarfraz</forename><surname>Khurshid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Kuncak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darko</forename><surname>Marinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering (ICSE &apos;10)</title>
		<meeting>the 32nd ACM/IEEE International Conference on Software Engineering (ICSE &apos;10)<address><addrLine>Cape Town, South Africa</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-05">May 2010</date>
			<biblScope unit="page" from="225" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">FATE and DESTINI: A Framework for Cloud Recovery Testing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Haryadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thanh</forename><surname>Gunawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pallavi</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">M</forename><surname>Alvaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><forename type="middle">C</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Remzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koushik</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Symposium on Networked Systems Design and Implementation (NSDI &apos;11)</title>
		<meeting>the 8th Symposium on Networked Systems Design and Implementation (NSDI &apos;11)<address><addrLine>Boston, Massachusetts</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-03">March 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards Automatically Checking Thousands of Failures with Micro-specifications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Haryadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thanh</forename><surname>Gunawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pallavi</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><forename type="middle">C</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Remzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koushik</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 6th Workshop on Hot Topics in System Dependability (HotDep &apos;10)</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-10">October 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cloud Storage FUD: Failure and Uncertainty and Durability</title>
		<author>
			<persName><forename type="first">Alyssa</forename><surname>Henry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th USENIX Symposium on File and Storage Technologies (FAST &apos;09)</title>
		<meeting>the 7th USENIX Symposium on File and Storage Technologies (FAST &apos;09)<address><addrLine>San Francisco, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-02">February 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Journal of Future Generation Computer Systems archive</title>
		<author>
			<persName><forename type="first">William</forename><surname>Hoarau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastien</forename><surname>Tixeuil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabien</forename><surname>Vauchelles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-08">August, 2007</date>
			<biblScope unit="volume">23</biblScope>
		</imprint>
	</monogr>
	<note>FAIL-FCI: Versatile fault injection</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Todd</forename><surname>Hoff</surname></persName>
		</author>
		<author>
			<persName><surname>Netflix</surname></persName>
		</author>
		<ptr target="http://highscalability.com" />
		<title level="m">Continually Test by Failing Servers with Chaos Monkey</title>
		<imprint>
			<date type="published" when="2010-12">December 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Wait-free coordination for Internetscale systems</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahadev</forename><surname>Konar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flavio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Junqueira</surname></persName>
		</author>
		<author>
			<persName><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><surname>Zookeeper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 USENIX Annual Technical Conference (ATC &apos;10)</title>
		<meeting>the 2010 USENIX Annual Technical Conference (ATC &apos;10)<address><addrLine>Boston, Massachusetts</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Error Propagation Profiling of Operating Systems</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neeraj</forename><surname>Suri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Dependable Systems and Networks (DSN &apos;05)</title>
		<meeting>the International Conference on Dependable Systems and Networks (DSN &apos;05)<address><addrLine>Yokohama, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06">June 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">PREFAIL: A Programmable Failure-Injection Framework</title>
		<author>
			<persName><forename type="first">Pallavi</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haryadi</forename><forename type="middle">S</forename><surname>Gunawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koushik</forename><surname>Sen</surname></persName>
		</author>
		<idno>UCB/EECS-2011-30</idno>
		<imprint>
			<date type="published" when="2011-04">April 2011</date>
		</imprint>
		<respStmt>
			<orgName>UC Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Programmable Fault Injection Testbeds for Complex SOA</title>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Juszczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Schahram</forename><surname>Dustdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Service Oriented Computing (ICSOC &apos;10)</title>
		<meeting>the 8th International Conference on Service Oriented Computing (ICSOC &apos;10)<address><addrLine>San Francisco, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-12">December 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">AFEX: An Automated Fault Explorer for Faster System Testing</title>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Marinescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Candea</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Comparing the Robustness of POSIX Operating Systems</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Koopman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Devale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Symposium on Fault-Tolerant Computing (FTCS-29)</title>
		<meeting>the 29th International Symposium on Fault-Tolerant Computing (FTCS-29)<address><addrLine>Madison, Wisconsin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-06">June 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cassandra -a decentralized structured storage system</title>
		<author>
			<persName><forename type="first">Avinash</forename><surname>Lakshman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prashant</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 3rd ACM SIGOPS International Workshop on Large Scale Distributed Systems and Middleware (LADIS &apos;09)</title>
		<meeting><address><addrLine>Florianopolis, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-10">October 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Policy/mechanism separation in Hydra</title>
		<author>
			<persName><forename type="first">R</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Corwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Pollack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wulf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM Symposium on Operating Systems Principles (SOSP &apos;75)</title>
		<meeting>the 5th ACM Symposium on Operating Systems Principles (SOSP &apos;75)<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1975-11">November 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">LFI: A Practical and General Library-Level Fault Injector</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Marinescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Candea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Dependable Systems and Networks (DSN &apos;09)</title>
		<meeting>the International Conference on Dependable Systems and Networks (DSN &apos;09)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06">June 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An Extensible Technique for High-Precision Testing of Recovery Code</title>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">D</forename><surname>Marinescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Banabic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Candea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 USENIX Annual Technical Conference (ATC &apos;10)</title>
		<meeting>the 2010 USENIX Annual Technical Conference (ATC &apos;10)<address><addrLine>Boston, Massachusetts</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Case for Redundant Arrays of Inexpensive Disks (RAID)</title>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garth</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randy</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1988 ACM SIGMOD Conference on the Management of Data (SIGMOD &apos;88)</title>
		<meeting>the 1988 ACM SIGMOD Conference on the Management of Data (SIGMOD &apos;88)<address><addrLine>Chicago, Illinois</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988-06">June 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Failure Trends in a Large Disk Drive Population</title>
		<author>
			<persName><forename type="first">Eduardo</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolf-Dietrich</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luiz</forename><forename type="middle">Andre</forename><surname>Barroso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th USENIX Symposium on File and Storage Technologies (FAST &apos;07)</title>
		<meeting>the 5th USENIX Symposium on File and Storage Technologies (FAST &apos;07)<address><addrLine>San Jose, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-02">February 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">IRON File Systems</title>
		<author>
			<persName><surname>Vijayan Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lakshmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Bairavasundaram</surname></persName>
		</author>
		<author>
			<persName><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Haryadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><forename type="middle">C</forename><surname>Gunawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remzi</forename><forename type="middle">H</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName><surname>Arpaci-Dusseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM Symposium on Operating Systems Principles (SOSP &apos;05)</title>
		<meeting>the 20th ACM Symposium on Operating Systems Principles (SOSP &apos;05)<address><addrLine>Brighton, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-10">October 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Automated multiple failure FMEA</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reliability Engineering and System Safety</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2002-04">April 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Disk failures in the real world: What does an MTTF of 1,000,000 hours mean to you?</title>
		<author>
			<persName><forename type="first">Bianca</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garth</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th USENIX Symposium on File and Storage Technologies (FAST &apos;07)</title>
		<meeting>the 5th USENIX Symposium on File and Storage Technologies (FAST &apos;07)<address><addrLine>San Jose, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-02">February 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The Hadoop Distributed File System</title>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Shvachko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hairong</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Radia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Chansler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th IEEE Symposium on Massive Storage Systems and Technologies (MSST &apos;10)</title>
		<meeting>the 26th IEEE Symposium on Massive Storage Systems and Technologies (MSST &apos;10)<address><addrLine>Incline Village, Nevada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-05">May 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Decoupling Policy from Mechanism in Internet Routing</title>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Snoeren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barath</forename><surname>Raghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2004-01">January 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Hadoop</forename><surname>Team</surname></persName>
		</author>
		<ptr target="http://hadoop.apache.org/hdfs/docs/r0.21.0/faultinject_framework.html" />
		<title level="m">Hadoop Fault Injection Framework and Development Guide</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Characterizing Cloud Computing Hardware Reliability</title>
		<author>
			<persName><forename type="first">Kashi</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nachi</forename><surname>Nagappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 ACM Symposium on Cloud Computing (SoCC &apos;10)</title>
		<meeting>the 2010 ACM Symposium on Cloud Computing (SoCC &apos;10)<address><addrLine>Indianapolis, Indiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Hadoop The Definitive Guide</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>White</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<pubPlace>O&apos;Reilly</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">MODIST: Transparent Model Checking of Unmodified Distributed Systems</title>
		<author>
			<persName><forename type="first">Junfeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tisheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Symposium on Networked Systems Design and Implementation (NSDI &apos;09)</title>
		<meeting>the 6th Symposium on Networked Systems Design and Implementation (NSDI &apos;09)<address><addrLine>Boston, Massachusetts</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-04">April 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Using Model Checking to Find Serious File System Errors</title>
		<author>
			<persName><forename type="first">Junfeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Twohey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawson</forename><surname>Engler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madanlal</forename><surname>Musuvathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Symposium on Operating Systems Design and Implementation (OSDI &apos;04)</title>
		<meeting>the 6th Symposium on Operating Systems Design and Implementation (OSDI &apos;04)<address><addrLine>San Francisco, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-12">December 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m">f o r i i n range ( len ( fs )): 3 fp = FIP</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">4 isNet = ( fp [&apos; ioTarget &apos;] == &apos; net &apos;)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">5 isWrite = ( fp [&apos; ioType &apos;] == &apos; write &apos;)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">6 isCrash = ( fp [&apos; failure &apos;] == &apos; crash &apos;)</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
