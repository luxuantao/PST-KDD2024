<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sparsity preserving projections with applications to face recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lishan</forename><surname>Qiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Aeronautics and Astronautics</orgName>
								<address>
									<postCode>210016</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematics Science</orgName>
								<orgName type="institution">Liaocheng University</orgName>
								<address>
									<postCode>252000</postCode>
									<settlement>Liaocheng</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Songcan</forename><surname>Chen</surname></persName>
							<email>s.chen@nuaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Aeronautics and Astronautics</orgName>
								<address>
									<postCode>210016</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoyang</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Aeronautics and Astronautics</orgName>
								<address>
									<postCode>210016</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sparsity preserving projections with applications to face recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">76525CA0FAA4D8435B0AF1D0D81F5BE9</idno>
					<idno type="DOI">10.1016/j.patcog.2009.05.005</idno>
					<note type="submission">Received 21 July 2008 Received in revised form 25 April 2009 Accepted 7 May 2009</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Dimensionality reduction Sparse representation Compressive sensing Face recognition</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dimensionality reduction methods (DRs) have commonly been used as a principled way to understand the high-dimensional data such as face images. In this paper, we propose a new unsupervised DR method called sparsity preserving projections (SPP). Unlike many existing techniques such as local preserving projection (LPP) and neighborhood preserving embedding (NPE), where local neighborhood information is preserved during the DR procedure, SPP aims to preserve the sparse reconstructive relationship of the data, which is achieved by minimizing a L1 regularization-related objective function. The obtained projections are invariant to rotations, rescalings and translations of the data, and more importantly, they contain natural discriminating information even if no class labels are provided. Moreover, SPP chooses its neighborhood automatically and hence can be more conveniently used in practice compared to LPP and NPE. The feasibility and effectiveness of the proposed method is verified on three popular face databases (Yale, AR and Extended Yale B) with promising results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In many application domains, such as appearance-based object recognition, information retrieval and text categorization, the data are usually provided in high-dimensional form. Dimensionality reduction (DR) is an effective approach to deal with such data, due to its potential to mitigate the so-called "curse of dimensionality" <ref type="bibr" target="#b0">[1]</ref> and to improve the computational efficiency. Up to now, researchers have developed a variety of dimensionality reduction methods under supervised, unsupervised and semi-supervised scenarios. The supervised DRs include typically linear discriminant analysis (LDA) <ref type="bibr" target="#b1">[2]</ref>, marginal Fisher analysis (MFA) <ref type="bibr" target="#b2">[3]</ref>, maximum margin criterion (MMC) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, etc.; the unsupervised DRs include principal component analysis (PCA) <ref type="bibr" target="#b5">[6]</ref>, locality preserving projections (LPP) <ref type="bibr" target="#b6">[7]</ref>, etc.; and the semi-supervised DRs include semi-supervised dimensionality reduction (SSDR) <ref type="bibr" target="#b7">[8]</ref>, semi-supervised discriminant analysis (SDA) <ref type="bibr" target="#b8">[9]</ref>, just to name a few. In this paper, we only focus on unsupervised scenario, mainly for justifying its effectiveness and feasibility as a new DR method for classification, though our algorithm presented here can be easily and straightforwardly extended to include supervised information (e.g. class-label information and must/cannot-link pairwise constraints) following the existing semi-supervised dimensionality reduction framework <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>In the unsupervised DRs, PCA seems to be the most popular one. It is very simple and effective for some practical applications throughout science and engineering. However, PCA may fail to discover essential data structures that are nonlinear. Although the kernel-based techniques such as KPCA <ref type="bibr" target="#b10">[11]</ref> can implicitly deal with nonlinear DR problems, most of them do not explicitly treat the manifold structure of the data. Furthermore, how to select kernel and assign optimal kernel parameter is generally difficult and unsolved fully in many practical applications.</p><p>Another technique for nonlinear DR is manifold learning. In the past decade years, a variety of manifold-based techniques such as Isomap <ref type="bibr" target="#b11">[12]</ref>, LLE <ref type="bibr" target="#b12">[13]</ref>, Laplacian Eigenmaps <ref type="bibr" target="#b13">[14]</ref> and their variations <ref type="bibr" target="#b14">[15]</ref> have been developed to explicitly discover the nonlinear manifold structure concealed in the data. However, some desirable virtues the traditional PCA possesses are not inherited. For example, (1) how to evaluate the map for unseen test samples is not as natural as PCA, and thus special tricks <ref type="bibr" target="#b15">[16]</ref> are required to handle the "out-of-sample" problem; (2) a recent research <ref type="bibr" target="#b14">[15]</ref> has shown that nonlinear techniques perform well on some artificial data sets, but do not necessarily outperform the traditional PCA for real-world tasks yet; (3) it is generally difficult to select suitable values for the hyper-parameters (e.g., the neighborhood size) in such models. One effective approach to overcome the above limitations is approximating the nonlinear DRs using linear ones. For example, LPP <ref type="bibr" target="#b6">[7]</ref> is a linearized version of Laplacian Eigenmaps; neighborhood preserving embedding (NPE) <ref type="bibr" target="#b17">[18]</ref> and locally linear embedded Eigenspace analysis (LEA) <ref type="bibr" target="#b18">[19]</ref> are two linearized counterparts of LLE; isometric projection (IsoProjection) <ref type="bibr" target="#b19">[20]</ref> can be seen as a linearized Isomap. Most of these linearized versions can generally outperform PCA on real-world data due to the simplicity (linearity) of these models and their capability to preserve spatial consistency between the input space and the output space. In addition, the "out-of-sample" problem is usually addressed as well in these methods. However, it is still unclear how to select the neighborhood size and how to assign optimal values for other hyper-parameters for them.</p><p>In this paper, motivated by the recent development of sparse representation (SR) <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref>, we propose a simple dimensionality reduction method called sparsity preserving projections (SPP). Specifically, in the proposed algorithm, an "adjacent" weight matrix of the data set is firstly constructed based on a modified sparse representation (MSR) framework, and then the low-dimensional embedding of the data is evaluated to best preserve such weight matrix. Although supervised information is not needed, SPP tends to find the discriminative mapping since the sparsest representation has natural discriminating power: taking face images into account, the most compact expression of a certain face image is generally given by the face images from the same class <ref type="bibr" target="#b20">[21]</ref>. We now enumerate several characteristics of our presented algorithm as follows:</p><p>(1) SPP shares some advantages of both LPP and many other linear DRs. For example, it is linear and defined everywhere, thus the "out-of-sample" problem is naturally solved. In addition, the weight matrix is kept sparse like in most locality preserving algorithms, which is beneficial to computational tractability.</p><p>(2) SPP does not have to encounter model parameters such as the neighborhood size and heat kernel width incurred in LPP, NPE, etc., which are generally difficult to set in practice.</p><p>Although cross-validation technique <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b39">40]</ref> can be used in these cases, it is very time-consuming and tends to waste the limited training data. In contrast, SPP does not need to deal with such parameters, which makes it very simple to use in practice.</p><p>(3) Although SPP belongs to global methods in nature, it owns some local properties due to the sparse representation procedure. In Section 4, we will show that SPP has some factual connection with several popular locality preserving algorithms under certain conditions. (4) The technique proposed here can be easily extended to supervised and semi-supervised scenarios based on the existing dimensionality reduction framework <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>The rest of the paper is organized as follows: Section 2 reviews PCA, LPP and NPE, three popular linear DRs. The sparsity preserving projections algorithm is introduced in Section 3. In Section 4, we compare SPP with some related works. The experimental results are presented in Section 5. Finally, we provide some concluding remarks and future work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Linear unsupervised dimensionality reduction</head><p>In this paper, we mainly focus on linear approaches though our SPP can be easily kernelized as a nonlinear one <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25]</ref>. In fact, up to now, linear techniques are still an important research subject in pattern recognition and machine learning mainly due to their simplicity, mathematical tractability, efficiency and effectiveness for many real-world problems such as face recognition. In the numerous linear DRs, PCA, LPP and NPE are three popular ones. In face recognition, they are known as Eigenface <ref type="bibr" target="#b5">[6]</ref>, Laplacianface <ref type="bibr" target="#b25">[26]</ref> and NPEface <ref type="bibr" target="#b17">[18]</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Principal component analysis</head><p>PCA seeks a low-dimensional representation of the data to retain as much of the variance in the data as possible. Given a set of data points {x i } n i=1 , where x i ∈ R m is an m-dimensional column vector, we expect to get their low-dimensional images {y i } n i=1 by projecting each x i onto the direction vector w ∈ R m . The objective function of PCA is defined as follows:</p><formula xml:id="formula_0">max ||w||=1 n i=1 (y i -ȳ) 2<label>(1)</label></formula><p>where y i = w T x i , and ȳ is the mean of {y i } n i=1 . Eq. ( <ref type="formula" target="#formula_0">1</ref>) can be rewritten as</p><formula xml:id="formula_1">max ||w||=1 w T Rw (2)</formula><p>where is the sample covariance matrix. The eigenvectors of corresponding to the largest d eigenvalues span the optimal subspace of PCA. In face recognition, x i represents a face image, and the eigenvectors are so-called Eigenfaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Locality preserving projections</head><p>While PCA aims to preserve the global structure of the data, LPP aims to preserve the local (i.e., neighborhood) structure of the data. Intuitively, LPP may keep more discriminating information than PCA, assuming that the samples from the same class are likely close to each other in the input space. With the same mathematical notations as in PCA, the objective function of LPP is defined as follows:</p><formula xml:id="formula_2">min w i,j (y i -y j ) 2 p ij<label>(3)</label></formula><p>where y i = w T x i , i = 1, 2, . . . , n, and P = (p ij ) n×n is a similarity matrix defined as follows:</p><formula xml:id="formula_3">p ij = ⎧ ⎨ ⎩ exp(-||x i -x j || 2 /t) if x i is among kNN of x j or if x i is among kNN of x i 0 otherwise</formula><p>Minimizing (3) aims to encourage that if two points x i and x j are close to each other in the input space, then so should be in the corresponding output space. With simple formulation, the objective function is equivalent to minimizing</p><formula xml:id="formula_4">1 2 i,j (y i -y j ) 2 p ij = 1 2 i,j (w T x i -w T x j ) 2 p ij = w T X(D -P)X T w = w T XLX T w (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where D is a diagonal matrix with its entries being the row (or column since P is symmetric) sums of P, i.e., d ii = j p ij , and L = D -P is the Laplacian matrix. By imposing a constraint w T XDX T w = 1, LPP reduces to</p><formula xml:id="formula_6">min w w T XLX T w w T XDX T w<label>(5)</label></formula><p>The optimal w is given by the minimum eigenvalue solution to the following generalized eigenvalue problem:</p><formula xml:id="formula_7">XLX T w = XDX T w (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Neighborhood preserving embedding</head><p>Similar to LPP, NPE also aims at preserving the local neighborhood structure of the data. However, NPE evaluates the affinity weight matrix using local least squares approximation instead of defining it directly as in LPP. The local approximation error in NPE is measured by minimizing the cost function <ref type="bibr" target="#b17">[18]</ref>:</p><formula xml:id="formula_8">(N) = i x i - j N ij x j 2 (7)</formula><p>where x j ' s are k neighbors of x i . A reasonable criterion for choosing a "good" projection is minimizing the cost function <ref type="bibr" target="#b17">[18]</ref>:</p><formula xml:id="formula_9">(w) = i ⎛ ⎝ w T x i - j Ñij w T x j ⎞ ⎠ 2 (8)</formula><p>where Ñij is the optimal solution of Eq. <ref type="bibr" target="#b6">(7)</ref>. By removing an arbitrary scaling factor, minimizing Eq. ( <ref type="formula">8</ref>) leads to min w w T XMX T w w T XX T w <ref type="bibr" target="#b8">(9)</ref> where M = (I -N) T (I -N). The optimization problem boils down to a generalized eigenvalue problem as in LPP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Sparsity preserving projections</head><p>Recently some researchers have shown that most of the existing DRs can be explained from the kernel view <ref type="bibr" target="#b26">[27]</ref> and unified under a graph framework <ref type="bibr" target="#b24">[25]</ref>, where constructing a specific graph and its affinity weight matrix plays a key role. Although, according to the celebrated "No Free Lunch" theorem <ref type="bibr" target="#b28">[29]</ref>, there is no clear evidence that any of affinity weight matrix is always superior to the others, the weight matrices in most locality-based DRs such as LPP and NPE have a common characteristic: sparsity <ref type="bibr" target="#b16">[17]</ref>. The sparsity is an important way to encode the domain knowledge thus helpful to improve the generalization capability of the model. Motivated by this, here we present a new method to design the weight matrix straightforwardly based on sparse representation theory <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41]</ref>, through which the sparsity can be optimally and naturally derived. For completeness, we briefly review the concept of sparse representation before going into the details of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Sparse representation</head><p>Sparse representation is initially proposed as an extension to traditional signal representations such as Fourier and wavelet representations. In the past few years, SR has been successfully applied to solve many practical problems in signal processing, statistics and pattern recognition. For example, in signal and image processing fields, SR is used for signal compression and coding <ref type="bibr" target="#b29">[30]</ref>, image denoising <ref type="bibr" target="#b30">[31]</ref>, image super-resolution <ref type="bibr" target="#b31">[32]</ref>, etc.; in statistics, SR is an effective tool for variable selection, and keeps close relation with the popular LASSO <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>; in machine learning and pattern recognition communities, SR is used for objection detection and classification tasks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. In the emerging field of compressive sensing <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>, as a very attractive theory challenging Shannon-Nyquist sampling theorem, SR seeks to recover the signal from the compressed measures in a most economical way. Especially, recent researches <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b36">37]</ref> showed that classifier based on SR is exceptionally effective and achieves by far the best recognition rate on some face databases.</p><p>SR has compact mathematical expression. Given a signal (or an image with vector pattern) x ∈ R m , and a matrix X = [x 1 , x 2 , ... , x n ] ∈ R m×n containing the elements of an overcomplete dictionary <ref type="bibr" target="#b27">[28]</ref> in its columns, the goal of SR is to represent x using as few entries of X as possible. This can be formally expressed as follows: x 1</p><formula xml:id="formula_10">min s ||s|| 0 s.t. x = Xs (10)</formula><formula xml:id="formula_11">x 2 x 3 t 1 t 2 t 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2. (a)</head><p>A sub-block of the weight matrix S constructed by Eq. ( <ref type="formula" target="#formula_23">16</ref>). (b) The optimal ti's for three different samples.</p><p>where s ∈ R n is the coefficient vector, and ||s|| 0 is the pseudo-0 norm which is equal to the number of non-zero components in s. Unfortunately, this criterion is not convex, and finding the sparsest solution of Eq. ( <ref type="formula">10</ref>) is NP-hard. This difficulty can be bypassed by convexizing the problem and solving</p><formula xml:id="formula_12">min s ||s|| 1 s.t. x = Xs (<label>11</label></formula><formula xml:id="formula_13">)</formula><p>where 1 is used instead of 0 . It can be shown that if the solution s 0 sought is sparse enough, the solution of 0 minimization problem is equal to the solution of 1 minimization problem <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>. Fig. <ref type="figure" target="#fig_0">1</ref> shows that the sparse solution can be found by solving a 1 minimization problem but may not by other traditional strategies like 2 minimization.</p><p>In fact, suboptimal solutions can be found by a variety of approaches such as greedy <ref type="bibr" target="#b37">[38]</ref> algorithms and Bayesian <ref type="bibr" target="#b38">[39]</ref> strategies. However, the equivalence of the 0 and 1 problem has been studied deeply from a mathematical perspective, which makes the 1 approximate strategy more reliable than others for practical applications. In general, the 1 minimization problem can be solved by standard linear programming (LP) <ref type="bibr" target="#b40">[41]</ref>.</p><p>In many practical problems, the signal x is generally noisy, thus the constraint x = Xs in Eq. ( <ref type="formula" target="#formula_12">11</ref>) does not always hold. According to <ref type="bibr" target="#b20">[21]</ref>, at least two robust extensions can be used to handle this problem: (1) relax the constraint to ||x -Xs|| &lt; , where can be seen as an error tolerance; (2) simply replace X with [X, I], where I is an m-order identity matrix. Both the strategies are considered in this paper. Although the second strategy is often used to deal with occlusion and corruption <ref type="bibr" target="#b20">[21]</ref>, our experiments show that it also works well in our algorithm even when there are no occlusion and corruption in face images. This is partially due to that the strategy can provide illumination compensation for representing a given face image (see the next subsection and Fig. <ref type="figure">2</ref> for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Sparse reconstructive weights</head><p>Since DR is mainly characterized by specific affinity weight matrix of the data, we try to construct the matrix based on a modified sparse representation framework, and then explain why it is helpful to both the compact representation of data and the subsequent classification task.</p><p>Given a set of training samples {x i } n i=1 , where x i ∈ R m , let X = [x 1 , x 2 , ... , x n ] ∈ R m×n be the data matrix including all the training samples in its columns. We expect to reconstruct each sample x i , e.g., a face image, using as few samples as possible. Hence we first seek a sparse reconstructive weight vector s i for each x i through the following modified 1 minimization problem:</p><formula xml:id="formula_14">min s i ||s i || 1 s.t. x i = Xs i 1 = 1 T s i (<label>12</label></formula><formula xml:id="formula_15">)</formula><p>where</p><formula xml:id="formula_16">s i = [s i1 , ... , s i,i-1 , 0, s i,i+1 , ... , s in ]</formula><p>T is an n-dimensional vector in which the i-th element is equal to zero (implying that the x i is removed from X), and the elements s ij , j i denote the contribution of each x j to reconstructing x i ; 1 ∈ R n is a vector of all ones.</p><p>The MSR problem can be solved by standard linear programming as original 1 minimization problem Eq. ( <ref type="formula" target="#formula_12">11</ref>), since the sum-to-one constraint 1 = 1 T s i is also linear. We will explain the reason for this constraint shortly. After computing the weight vector s i for each x i , i = 1, 2, . . . , n, we can define the sparse reconstructive weight matrix S = (s ij ) n×n as follows:</p><formula xml:id="formula_17">S = [s 1 , s2 , ... , sn ] T (<label>13</label></formula><formula xml:id="formula_18">)</formula><p>where si is the optimal solution of Eq. ( <ref type="formula" target="#formula_14">12</ref>). The element sij in S is not simple similarity measure between samples x i and x j , and in this sense S is essentially different from the adjacency weigh matrix in LPP. Now we give some insights into the effectiveness of S as a weight matrix for dimensionality reduction and the subsequent recognition task.</p><p>(1) Each weight vector s i obeys an important symmetry: it is invariant to rotations and rescalings due to the first constraint in Eq. ( <ref type="formula" target="#formula_14">12</ref>), and invariant to translations due to the sum-to-one constraint 1 = 1 T s i . As a result, the weight matrix S reflects some intrinsic geometric properties 1 of the data. (2) Discriminant information can be naturally preserved in the weight matrix S, even if no class-labels are provided. Let us take face recognition as an example. One particularly simple but effective assumption in face recognition is that the samples from the same class lie on a linear subspace (so-called face subspace). Given a face image x j i from the j-th class, x j i can be theoretically represented using the samples from the j-th class according to the subspace assumption. That is,</p><formula xml:id="formula_19">x j i = 0 • x 1 1 + • • • + i,i-1 x j i-1 + i,i+1 x j i+1 + • • • + 0 • x c n (<label>14</label></formula><formula xml:id="formula_20">)</formula><p>where j = 1, ... , c denotes the class label. The weight vector s 0 i = [0, ... , i,i-1 , 0, i,i+1 , ... , 0] T is sparse, since class number is generally large 2 in most face recognition problems. Although Eq. ( <ref type="formula" target="#formula_19">14</ref>) 1 These properties can also be obtained from the popular LLE algorithm. We will discuss the similarities and differences between LLE and our proposed algorithm in Section 4. 2 For example, if c = 10, then at least 90% of the entries in si should be zeros.</p><p>does not always hold due to insufficient sampling, our experiments show the sparse s 0 i can actually be approximated by the optimal solutions si (see Fig. <ref type="figure">2</ref>). In other words, the non-zero entries in si mostly correspond to the samples from the j-th class, which implies that si may help to distinguish that class from the others. Therefore, the weight vector si , constructed using all the samples with sparsity constraint, tends to include potential discriminant information.</p><p>As described previously, in some real-world applications, the constraint x i = Xs i in Eq. ( <ref type="formula" target="#formula_14">12</ref>) does not always hold. By considering the two strategies mentioned in Section 3.1, the MSR problem can be extended to the following two stable versions, <ref type="bibr" target="#b14">(15)</ref> and <ref type="bibr" target="#b15">(16)</ref>. The first extension is defined as</p><formula xml:id="formula_21">min s i ,t ||s i || 1 s.t. ||x i -Xs i || &lt; 1 = 1 T s i (<label>15</label></formula><formula xml:id="formula_22">)</formula><p>where is the error tolerance and generally fixed across various instances of the problem <ref type="bibr" target="#b20">[21]</ref>. It is easy to validate that its optimal solution still reflects some intrinsic geometric properties (e.g. invariant to translations and rotations) of the original data. Another extension <ref type="foot" target="#foot_0">3</ref> of MSR can be expressed as</p><formula xml:id="formula_23">min [s T i t T ] T ||[s T i t T i ] T || 1 s.t. x i 1 = X I 1 T 0 T s i t i (<label>16</label></formula><formula xml:id="formula_24">)</formula><p>where t i is an m-dimensional vector and 0 is an m-dimensional vector of all zeros. The optimal solution of ( <ref type="formula" target="#formula_23">16</ref>) is also invariant to translations, but the invariance to rotations and rescalings does not rigorously hold any longer. But our experiments indicate that such a loss of the invariance does not much invoke influence on the final classification performance and conversely increases robustness to lighting change. Here, we take extended Yale B face database (see Section 5.2 for special description about this database) as an example to intuitively show why Eq. ( <ref type="formula" target="#formula_23">16</ref>) may work.</p><p>Let us assume the training data matrix X=[X 1 , X 2 , ... , X 38 ], where X i denotes the data samples that belong to the i-th class. Then, we calculate the weight matrix S based on Eq. ( <ref type="formula" target="#formula_23">16</ref>). For space limitation, only a sub-block of S corresponding to the first five classes is shown in Fig. <ref type="figure">2(a</ref>) with the gray level <ref type="foot" target="#foot_1">4</ref> denoting the value of the element sij . In Fig. <ref type="figure">2</ref>(b) we show three t i ' s (i.e., t 1 , t 2 and t 3 ), which are respectively associated with three samples from the first class. From the example, we find most of the non-zero adjacency weights link the samples from the same class, and intuitively the t i plays a role to compensate the illumination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Preserving sparse reconstructive weights</head><p>By the above design, the sparse weight matrix S can reflect intrinsic geometric properties of the data to some extent and contains natural discriminating information. We thereby expect that the desirable characteristics in the original high-dimensional space can be preserved in the low-dimensional embedding subspace. Therefore, similar to LLE and NPE, we define the following objective</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm: Sparsity Preserving Projections</head><p>Step 1: Construct weight matrix S using MSR <ref type="bibr" target="#b11">(12)</ref> or stable MSR(15), (16);</p><p>Step 2: Calculate the projection vectors using <ref type="bibr" target="#b21">(22)</ref>, and the eigenvectors corresponding to the largest d eigenvalues span the optimal subspace. function to seek the projections which best preserve the optimal weight vector si . min</p><formula xml:id="formula_25">w n i=1 ||w T x i -w T Xs i || 2<label>(17)</label></formula><p>With simple algebric formulation, we can get</p><formula xml:id="formula_26">n i=1 ||w T x i -w T Xs i || 2 = w T ⎛ ⎝ n i=1 (x i -Xs i )(x i -Xs i ) T ⎞ ⎠ w<label>(18)</label></formula><p>Let e i be an n-dimensional unit vector with the i-th element 1, 0 otherwise, then Eq. ( <ref type="formula" target="#formula_26">18</ref>) is equal to</p><formula xml:id="formula_27">w T ⎛ ⎝ n i=1 (Xe i -Xs i )(Xe i -Xs i ) T ⎞ ⎠ w = w T X ⎛ ⎝ n i=1 (e i -si )(e i -si ) T ⎞ ⎠ X T w = w T X ⎛ ⎝ n i=1 e i e T i -si e T i -e i sT i + si sT i ⎞ ⎠ X T w = w T X(I -S -S T + S T S)X T w<label>(19)</label></formula><p>To avoid degenerate solutions, we constrain w T XX T w = 1. Thus, the objective function can be recast as the following optimization problem:</p><formula xml:id="formula_28">min w w T X(I -S -S T + S T S)X T w w T XX T w<label>(20)</label></formula><p>For compact expression, the minimization problem can further be transformed to an equivalent maximization problem as follows:</p><formula xml:id="formula_29">max w w T XS X T w w T XX T w<label>(21)</label></formula><p>where S = S + S T -S T S. Another benefit of this transform is that the maximum formulation in some case can get a more numerically stable solution <ref type="bibr" target="#b16">[17]</ref>. Then, the optimal w's are the eigenvectors corresponding to the largest d eigenvalues of the following generalized eigenvalue problem:</p><formula xml:id="formula_30">XS X T w = XX T w (22)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">SPP algorithm</head><p>Based on the above discussion, we summarize the proposed algorithm as shown in Fig. <ref type="figure" target="#fig_1">3</ref>.</p><p>The algorithm is simple, since it does not involve any hyperparameters except the subspace dimension d. For example, in step 1, a E denotes the matrix of all ones, and its corresponding adjacency graph is given in <ref type="bibr" target="#b24">[25]</ref>.</p><p>MSR can be efficiently solved by standard linear programming using publicly available packages such as l1-magic. <ref type="foot" target="#foot_2">5</ref> In addition, if the sparsity is well considered, the sparse representation problem can be more efficiently solved <ref type="bibr" target="#b41">[42]</ref>. In step 2, one can directly calculate eigenvectors for most practical applications or resort to the recently proposed techniques such as spectral regression <ref type="bibr" target="#b16">[17]</ref> and densityweighted Nystrom method <ref type="bibr" target="#b50">[51]</ref> for large scale problems.</p><p>For some high-dimensional data, the matrix XX T is generally singular since the training sample size is much smaller than the feature dimensions. To address this problem, the training set can be first projected onto a PCA subspace spanned by its leading eigenvectors:</p><formula xml:id="formula_31">W pca = [w 1 , w 2 , ... , w d ].</formula><p>The matrix XX T is then approximated by X XT ( X = W T pca X), which is obviously nonsingular.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Comparison with related works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">PCA</head><p>PCA can be seen as a globality preserving DR method in that a single hyper-plane is used to represent the data, hence not facing problem of selecting appropriate neighborhood size. SPP also does not need to worry about this since it actually uses all the training samples to construct the weight matrix without explicitly setting the neighborhood size. But compared to PCA, SPP has an extra advantage that it is capable of implicitly and naturally employing the "local" structure of the data by imposing the sparsity prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">NPE and other locality preserving DRs</head><p>SPP has a similar objective function to NPE (c.f., Eqs. ( <ref type="formula">9</ref>) and ( <ref type="formula" target="#formula_29">21</ref>)). Both of them can be closely related to LLE. In fact, NPE is a directly linearized version of LLE, while our SPP constructs the "affinity" weight matrix in a completely different manner from LLE. In particular, SPP constructs the weight matrix using all the training samples (with sparsity constraint) instead of k nearest neighbors, preventing it from suffering from the difficulty of parameter selection as in the case of NPE and other locality preserving DRs. Despite such difference, SPP can actually be thought as a regularized extension of NPE through the modified 1 -regularization problem. From the Bayesian  of sparsity, allowing it to extract more discriminating information from the data than NPE does. For clarity, three key components (i.e., model parameters, "affinity" weight matrices and the way to construct them) of several common DR methods are summarized in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Sparse subspace learning</head><p>Sparse subspace learning (SSL) <ref type="bibr" target="#b43">[44]</ref> is a special family of DR methods which also consider "sparsity". The representative SSL methods include sparse principal component analysis (SPCA) <ref type="bibr" target="#b44">[45]</ref>, nonnegative sparse PCA <ref type="bibr" target="#b45">[46]</ref>, sparse nonnegative matrix factorization <ref type="bibr" target="#b46">[47]</ref>, etc. Although having different objective functions, they share a common goal, i.e., to find a subspace spanned by sparse base vectors. Different from those methods whose sparsity is encoded in the projection w and associated with the feature dimension, SPP aims at the sparse reconstructive weight s i associated with the sample size. Naturally, we can also enforce the projection w sparse in SPP, but that is beyond the focus of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Sparse representation classifier (SRC)</head><p>SRC <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b36">37]</ref> is a recently proposed supervised classification framework based on sparse representation. Surprisingly, <ref type="bibr" target="#b20">[21]</ref> shows that the classification performance of most meaningful features converges when the feature dimension increases if a SCR classifier is used. Although this does provide some new insight into the role of feature extraction played in a pattern classification task, we argue that designing effective and efficient feature extractor is still of great importance since the classification algorithm could become simple and tractable.</p><p>Here we note several remarkable differences between SPP and SRC: (1) SPP is a feature extractor, while SRC is a classifier. As a result, SPP can be used as a preprocessor for any typical classifiers, such as 1NN, SVM and even SRC; (2) SPP is an unsupervised algorithm, meaning that SPP may enjoy wider applications such as data visualization (e.g., Fig. <ref type="figure" target="#fig_4">5</ref>) and other unsupervised learning tasks; and (3) SRC is essentially a lazy classifier and uses time-consuming sparse reconstruction for each test sample. By contrast, in SPP, the sparse reconstruction is involved only in the training process. Once the lowdimensional projection vectors are obtained, they can be used for both the training data and the test data, thus being able to effectively improve the efficiency of recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Illustrative examples</head><p>In this section, we first use two simple data sets including a toy data set and a real-word data set to intuitively show how and why our algorithm works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Toy problem</head><p>Let us consider the following toy binary classification problem on a 3D space where the samples from each class lie on an intrinsic 1D subspace (see Fig. <ref type="figure" target="#fig_3">4</ref>(b0), where each class is denoted by a bar): We randomly sample 10 points from each class, and half of them are added by Gaussian white noise with standard deviation 0.1, which makes the samples from each class actually lie on the 3D space instead of the 1D one (Fig. <ref type="figure" target="#fig_3">4(a0)</ref>). Then we construct four 1D subspaces for these data using PCA, LPP, NPE and SPP, For LPP  and NPE, we empirically<ref type="foot" target="#foot_3">6</ref> set the neighborhood size k = min{n i } -1, where n i denotes the number of the i-th class samples. The heat kernel parameter t in LPP is empirically chosen as the mean norm of the samples. The results are plotted in Fig. <ref type="figure" target="#fig_3">4(a1-a4</ref>). We repeat the experiments using another data set with 100 points generated in the same way as above and the results are shown in Fig. <ref type="figure" target="#fig_3">4</ref>(c1-c4).</p><p>From Fig. <ref type="figure" target="#fig_3">4</ref>, we have the following observations:</p><p>(1) For PCA, LPP and NPE, the samples from two classes are overlapped together, but the degree of overlapping is different for these methods, respectively. The PCA suffers most since it tries to use a single hyper-plane to model the data according to the directions of large sample variance. Both LPP and NPE improve over this by explicitly taking the local structure of data into account. But the Euclidean distance measure and the predefined neighborhood size in these methods fail to identify the real local structure they supposed. (2) On the contrary, one can see from Fig. <ref type="figure" target="#fig_3">4</ref>(a4, c4) that, with SPP, the two classes can be perfectly separated in the low-dimensional subspace. This can be explained from the angle of sparse prior, which assumes that a point should be best explained by a set of samples as small as possible. Further, this illustrates that SPP can effectively and implicitly use the subspace assumption, even when the two classes are close to each other and the data are noisy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Wine data set from UCI</head><p>Now we use Wine data set, a real-life data set from the UCI machine learning repository, <ref type="foot" target="#foot_4">7</ref> to give SPP further explanation. Wine has 13 features, 3 classes and 178 instances. The basic statistics including means, variances and ranges of 13 features are presented in Table <ref type="table" target="#tab_1">2</ref>.</p><p>It is easy to see from Table <ref type="table" target="#tab_1">2</ref> that the last feature should play a decisive role in the data distribution due to its large range and variance. Here, we apply PCA, LPP, NPE and the proposed SPP, respectively, to project the data onto a 2D subspace. The hyper-parameters in LPP and NPE are chosen by following the same scheme as in the toy problem. According to the results shown in Fig. <ref type="figure" target="#fig_4">5</ref>, we have the following observations:</p><p>(1) The leading projection directions of PCA are decided by the 13th feature, since it has a much larger variance and range than the others. Thus the 2D data distribute in a large range along the direction dominated by the last feature, which makes the projected data mixed up. (2) The locality preserving methods such as LPP and NPE suffer from the same problem as in PCA, even though the local information is considered. This is because the neighborhood of a certain sample point is still dominated by the 13th feature due to its large variance and range. (3) The data projected by SPP form a point-cloud distribution instead of a "linear" one. This reflects that the SPP gives other features besides the 13th one a chance to play a role in capturing a reasonable structure of the data.</p><p>From the two illustrative examples, we can see that sparsity actually works, especially when the data from each class lie on a subspace. Furthermore, SPP is not quite sensitive to the imbalance of the feature distribution which incurs the failure of LPP and NPE, due to the fact that neighborhood is mainly decided by the features with large range and variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Face representation and recognition</head><p>PCA, LPP and NPE are three popular unsupervised DR methods. They have been successfully applied to face recognition where they are known as Eigenface <ref type="bibr" target="#b5">[6]</ref>, Laplacianface <ref type="bibr" target="#b25">[26]</ref> and NPEface <ref type="bibr" target="#b17">[18]</ref>, respectively. In what follows, we test the performance of the three popular algorithms and our proposed algorithm on three face databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Data sets and experimental settings</head><p>We firstly give simple descriptions of the data sets used later. Yale: this database <ref type="bibr" target="#b1">[2]</ref> contains 165 face images of 15 individuals. There are 11 images per subject, and these 11 images are , respectively, under the following different facial expression or configuration: center-light, wearing glasses, happy, left-light, wearing no glasses, normal, right-light, sad, sleepy, surprised and wink. In our experiment, the images are cropped to a size of 32×32, and the gray level values of all images are rescaled to [0 1]. AR: In our experiments, we use the cropped images with the resolution of 32×32, which is directly downloaded from http://www.cs.uiuc.edu/homes/dengcai2. Fig. <ref type="figure" target="#fig_5">6</ref> shows some face images from the three face databases used here.</p><p>For these databases, we randomly select half of the images per class for training (i.e., 6, 7 and about 32 images per subject for Yale, AR and Extended Yale B databases, respectively), and the remaining for test. Since AR database has naturally been partitioned into two sessions, we also consider this case in our experiments. We simply use "AR_fixed" to denote the AR database partitioned based on two fixed sessions, and "AR_random" to the one partitioned randomly. In particular, with the given training set, the projection matrix W is learned by PCA, LPP, NPE and SPP, respectively, and the test samples are subsequently transformed by the learned projection matrix. Then, specific classifiers are employed to evaluate the recognition rates on the test data. In the experiments, 20 training/test splits are randomly generated and the average classification accuracies over these splits are reported. The codes of PCA, LPP <ref type="foot" target="#foot_5">8</ref> and NPE are all from http://www.cs.uiuc.edu/homes/dengcai2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Parameter selection</head><p>For PCA, the only model parameter is the subspace dimension. For LPP, the model parameters include neighborhood size k and kernel width t. In our experiments, we set their appropriate values by searching in a large range of candidates and report the best results. A similar strategy is used to decide the neighborhood size k in NPE. In particular, for Yale and AR databases, the neighborhood size k is searched from {1, 2, . . . , l -1}; for Extended Yale B database, from {1, 2, 5, 10, . . . , l -1}, where l is the number of the training samples in each class. The kernel width t is empirically set as the mean norm t 0 of the training data, or the adjacency weight is directly calculated based on "cosine" distance. For the proposed SPP, when directly us-ing the MSR <ref type="bibr" target="#b11">(12)</ref> or the stable MSR ( <ref type="formula" target="#formula_23">16</ref>) to construct the adjacency weight matrix, it is parameter-free; when using another stable version <ref type="bibr" target="#b14">(15)</ref>, it has an error tolerance parameter which is generally fixed across various instances of the problem <ref type="bibr" target="#b36">[37]</ref>, and thus in our experiments we simply set it to 0.05 as in <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">Experimental results</head><p>(A) Based on 1-NN classifier: To verify the effectiveness of the proposed method, in this series of experiments we evaluate the performance of the proposed method and compare it to that of several methods using the simplest nearest neighbor (1-NN) classifier. As a baseline, we also give the classification results of 1-NN classifier directly using the raw data without dimensionality reduction. The recognition rates are shown in Fig. <ref type="figure" target="#fig_6">7</ref>, where SPP1 denotes the SPP algorithm based on the stable MSR <ref type="bibr" target="#b14">(15)</ref> and SPP2 denotes the SPP based on <ref type="bibr" target="#b15">(16)</ref>. We also summarize the best results of these methods in Table <ref type="table" target="#tab_2">3</ref>. Furthermore, based on the training set of AR database, the first 10 Eigenfaces, Laplacianfaces, NPEfaces and SPPfaces are shown in Fig. <ref type="figure" target="#fig_7">8</ref>.</p><p>(B) Based on other classifiers: We further evaluate the above mentioned DRs based on several other popular classifiers including k-NN, SVM and SRC. Here, we just report the experimental results on Yale database since the simple 1-NN classifier does not work well on it. One can refer to <ref type="bibr" target="#b20">[21]</ref> for some related discussions on AR and Extended Yale B databases. The experimental results are presented in Table <ref type="table" target="#tab_4">4</ref>. For k-NN classifier, we empirically set neighborhood size k = 6 (i.e., the training sample size per subject). For SVM, we simply use the linear kernel following the scheme of <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4.">Overall observations and discussion of the above experimental results</head><p>(1) PCA is simple to perform, but it generally performs much worse than LPP, NPE and SPP. Based on 1-NN classifier, its recognition rates are just close to the baseline on all the used databases, which is consistent with the results in many publications such as recent <ref type="bibr" target="#b39">[40]</ref>. (2) LPP and NPE always outperform PCA when the subspace dimension exceeds a certain threshold. This shows that by preserving the local structure of the data, the recognition rate can be improved. That is, when nearest neighbor search is considered, local structure seems to be more important than global structure. However, LPP and NPE are less tractable than PCA due to the difficulty of parameter selection involved. Results presented in   (3) On the tested databases, the two versions of SPP consistently outperform PCA, LPP and NPE with 1-NN classifier, even though no parameter needs to be adjusted. This suggests that the projections found by SPP can preserve more discriminating infor-mation than those of the compared methods. Furthermore, the performance of the two versions is data-driven. Concretely, SPP1 achieves better performance on AR database, while SPP2 generally outperforms SPP1 on Yale and Extended Yale B databases.  (4) As presented in Table <ref type="table" target="#tab_4">4</ref>, the classifiers also affect the recognition performance significantly. However, the proposed SPP can generally achieve better performance than PCA, LPP and NPE based on most of the classifiers (i.e., 1NN, k-NN and SVM). The only exception is that for SRC classifier, all the mentioned DRs achieve comparable performance. This further illustrates that SRC is insensitive to different feature extractors as pointed out in <ref type="bibr" target="#b20">[21]</ref>. Furthermore, by combining the SPP algorithm and the SVM classifier, we can achieve the best performance among all the combinations of the compared feature extractors and classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and future work</head><p>In this paper, based on sparse representation, we propose a new algorithm called sparsity preserving projections for unsupervised dimensionality reduction. In the proposed algorithm, the projections of SPP are sought such that the sparse reconstructive weights can be best preserved. SPP is shown to outperform PCA, LPP and NPE on all the data sets used here, and is very simple to perform like PCA by avoiding the difficulty of parameter selection as in LPP and NPE. Since it remains unclear how to define the "locality" theoretically for many locality-based algorithms like LPP and NPE, SPP can be considered as an alternative to them.</p><p>However, each approach has its own advantages and disadvantages. SPP is sensitive to large variations in pose as many wholepattern based feature extractors such as PCA, LPP and NPE. Therefore, in this paper, we only focus on front-view face images with variations in illumination and expression. In the future work, we will try to overcome this limitation using sub-pattern based strategy <ref type="bibr" target="#b47">[48]</ref> and absorb supervised information into the algorithm to further improve its performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A 2D example of optimization under (left) 2 minimization and (right) 1 minimization. The skew line denotes the feasible solution space, i.e., {s ∈ R 2 |x = Xs} under 2D case. The two bold arrow lines denote the optimal solutions of 2 and 1minimization problems, respectively.</figDesc><graphic coords="3,331.16,240.65,213.56,101.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. SPP algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>[ 43 ]</head><label>43</label><figDesc>view, such regularization essentially encodes prior knowledge</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The toy data and their 1D images based on four DRs above mentioned algorithms.</figDesc><graphic coords="6,132.86,61.19,320.09,298.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The 2D results of Wine data set based on four different DR methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. (a) All the 11 images of the first person in Yale database. (b) All the 14 images of the first person in the subset of AR. The first seven images are from the first session, and the last seven images are from the second session. (c) Partial images of the first person in extended Yale B database.</figDesc><graphic coords="8,130.60,61.31,324.00,117.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The recognition rates of 1-NN classifier based on several mentioned DRs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The first 10 basis vectors of PCA, LPP, NPE and SPP calculated from the training set of AR database.</figDesc><graphic coords="10,39.10,61.91,237.60,299.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>The construction manners of weight matrices for different DR methods.</figDesc><table><row><cell></cell><cell>Model parameters</cell><cell>Weight matrices</cell><cell>Construction manners</cell></row><row><cell>PCA</cell><cell>No</cell><cell>E</cell><cell>Globality a</cell></row><row><cell>LPP</cell><cell>k, t</cell><cell>P</cell><cell>Local neighborhood distance</cell></row><row><cell>NPE</cell><cell>k</cell><cell>N</cell><cell>Local neighborhood reconstruction</cell></row><row><cell>SPP</cell><cell>No</cell><cell>S</cell><cell>Global sparse reconstruction</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>The means, variances and ranges of the 13 features on Wine dataset.</figDesc><table><row><cell>F e a t u r e s</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>1 0</cell><cell>1 1</cell><cell>1 2</cell><cell>1 3</cell></row><row><cell>Mean</cell><cell>13.0</cell><cell>2.3</cell><cell>2.4</cell><cell>19.5</cell><cell>99.7</cell><cell>2.3</cell><cell>2.0</cell><cell>0.4</cell><cell>1.6</cell><cell>5.1</cell><cell>1.0</cell><cell>2.6</cell><cell>746.9</cell></row><row><cell>Variance</cell><cell>0.7</cell><cell>1.2</cell><cell>0.08</cell><cell>11.2</cell><cell>204.0</cell><cell>0.4</cell><cell>1.0</cell><cell>0.02</cell><cell>0.3</cell><cell>5.4</cell><cell>0.05</cell><cell>0.5</cell><cell>99166.7</cell></row><row><cell>Range</cell><cell>3.8</cell><cell>5.1</cell><cell>1.9</cell><cell>19.4</cell><cell>92</cell><cell>2.9</cell><cell>4.7</cell><cell>0.5</cell><cell>3.2</cell><cell>11.7</cell><cell>1.2</cell><cell>2.7</cell><cell>1402</cell></row><row><cell></cell><cell></cell><cell cols="2">500 1000 1500</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell>0.8</cell><cell>Yale</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">AR_fixed</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell></row><row><cell>Recognition Rate</cell><cell>0.4 0.5 0.6</cell><cell></cell><cell></cell><cell>Baseline PCA NPE SPP1 LPP</cell><cell>Recognition Rate</cell><cell>0.4 0.5 0.6 0.7</cell><cell></cell><cell>SPP1 NPE Baseline PCA LPP</cell></row><row><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell>SPP2</cell><cell></cell><cell>0.3</cell><cell></cell><cell>SPP2</cell></row><row><cell></cell><cell>20</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell></cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell></row><row><cell></cell><cell></cell><cell cols="2">Dimensions</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Dimensions</cell></row><row><cell></cell><cell></cell><cell cols="2">AR_random</cell><cell></cell><cell></cell><cell>1</cell><cell cols="2">Extended Yale B</cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recognition Rate</cell><cell>0.4 0.5 0.6 0.7</cell><cell></cell><cell cols="2">Baseline PCA LPP NPE</cell><cell>Recognition Rate</cell><cell>0.6 0.7 0.8 0.5</cell><cell></cell><cell>Baseline PCA LPP NPE</cell></row><row><cell></cell><cell>0.3</cell><cell></cell><cell cols="2">SPP1 SPP2</cell><cell></cell><cell>0.4</cell><cell></cell><cell>SPP1 SPP2</cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell></row><row><cell></cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell></cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell></row><row><cell></cell><cell></cell><cell cols="2">Dimensions</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Dimensions</cell></row></table><note><p>indicate that one has to adjust the values of parameters in LPP and NPE for different training set in order to achieve good performance. However, we have also observed that small neighborhood size k in LPP and NPE empirically tends to perform better on the data sets used.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>The best recognition rates of 1NN classifier based on different DRs.</figDesc><table><row><cell>DRs</cell><cell>Baseline</cell><cell>PCA</cell><cell>LPP</cell><cell>NPE</cell><cell>SPP1</cell><cell>SPP2</cell></row><row><cell>Yale (PCA ratio = 1)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>0.6993</cell><cell>0.6993</cell><cell>0.7407</cell><cell>0.7513</cell><cell>0.766</cell><cell>0.7680</cell></row><row><cell>Dimensions</cell><cell>1024</cell><cell>86</cell><cell>89</cell><cell>86</cell><cell>89</cell><cell>81</cell></row><row><cell>Parameters</cell><cell>No</cell><cell>No</cell><cell>k = 1, cosine</cell><cell>k = 5</cell><cell>N o</cell><cell>N o</cell></row><row><cell>AR_fixed (PCA ratio = 0.98)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>0.7814</cell><cell>0.7729</cell><cell>0.7843</cell><cell>0.7943</cell><cell>0.9057</cell><cell>0.8557</cell></row><row><cell>Dimensions</cell><cell>3168</cell><cell>232</cell><cell>235</cell><cell>236</cell><cell>216</cell><cell>176</cell></row><row><cell>Parameters</cell><cell>No</cell><cell>No</cell><cell>k = 1, t = t0</cell><cell>k = 3</cell><cell>N o</cell><cell>N o</cell></row><row><cell>AR_random (PCA ratio = 0.98)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>0.6627</cell><cell>0.6564</cell><cell>0.7961</cell><cell>0.7904</cell><cell>0.9308</cell><cell>0.9041</cell></row><row><cell>Dimensions</cell><cell>3168</cell><cell>240</cell><cell>240</cell><cell>240</cell><cell>227</cell><cell>159</cell></row><row><cell>Parameters</cell><cell>No</cell><cell>No</cell><cell>k = 1, t = t0</cell><cell>k = 1</cell><cell>N o</cell><cell>N o</cell></row><row><cell cols="2">Extended Yale B (PCA ratio = 0.98)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>0.7887</cell><cell>0.7589</cell><cell>0.9273</cell><cell>0.9319</cell><cell>0.9414</cell><cell>0.9518</cell></row><row><cell>Dimensions</cell><cell>1024</cell><cell>195</cell><cell>190</cell><cell>195</cell><cell>195</cell><cell>193</cell></row><row><cell>Parameters</cell><cell>No</cell><cell>No</cell><cell>k = 2, t = t0</cell><cell>k = 2</cell><cell>N o</cell><cell>N o</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>* PCA ratio denotes the energy ratio kept in the PCA preprocessing step.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>The recognition rates on Yale database based on different feature extractor and classifier pairs.</figDesc><table><row><cell>DRs</cell><cell>PCA</cell><cell>LPP</cell><cell>NPE</cell><cell>SPP1</cell><cell>SPP2</cell></row><row><cell>k-NN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>0.7087</cell><cell>0.7400</cell><cell>0.7887</cell><cell>0.8447</cell><cell>0.8220</cell></row><row><cell>Dimensions</cell><cell>86</cell><cell>46</cell><cell>87</cell><cell>88</cell><cell>89</cell></row><row><cell>SVM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>0.8293</cell><cell>0.9073</cell><cell>0.9073</cell><cell>0.9593</cell><cell>0.9613</cell></row><row><cell>Dimensions</cell><cell>89</cell><cell>89</cell><cell>89</cell><cell>86</cell><cell>88</cell></row><row><cell>SRC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>0.9493</cell><cell>0.9560</cell><cell>0.9520</cell><cell>0.9493</cell><cell>0.9493</cell></row><row><cell>Dimensions</cell><cell>89</cell><cell>89</cell><cell>89</cell><cell>89</cell><cell>89</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>In fact, we can further consider the trade-off between si and ti in<ref type="bibr" target="#b15">(16)</ref> to design a more general version. However, we empirically find such extension is generally not helpful on our used databases.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>For convenience of display, the black pixel denotes 1, while the white pixel denotes 0.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>The web site (http://www.dsp.ece.rice.edu/cs/) provides many practical toolboxes and recent research works to solve the sparse representation problem. In our experiments, l1-Magic toolbox is used due to its simplicity.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>In the experiments on the toy problem and the following real-world UCI data set, we attempt to assign the parameter values by searching from a large range of candidates, but most of them cannot achieve satisfying results.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>http://archive.ics.uci.edu/ml</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>We use the unsupervised LPP instead of the supervised extension, since we only focus on unsupervised dimensionality reduction in this paper.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank the anonymous reviewers for their valuable comments and suggestions to improve the quality of this paper. This work was partly supported by National Natural Science Foundation of China (60773061, 60773060), the Innovation Foundation of NUAA (Y0603-042) and Project sponsored by SRF for ROCS, SEM. The authors also thank Cai et al. for providing the codes of LPP and NPE on their homepage.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Statistical pattern recognition: a review</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="37" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Eigenfaces vs. Fisherfaces: recognition using class specific linear projection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hepanha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Marginal Fisher analysis and its variants for human gait recognition and content-based image retrieval</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2811" to="2821" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient and robust feature extraction by maximum margin criterion</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="157" to="165" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Comments on efficient and robust feature extraction by maximum margin criterion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1862" to="1864" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Eigenfaces for recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cognitive Neurosci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="86" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Locality preserving projections</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>Conference on Advances in Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised dimensionality reduction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM Conference on Data Mining (ICDM)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-supervised discriminant analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A unified framework for semi-supervised dimensionality reduction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2789" to="2799" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Kernel principal component analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods-Support Vector Learning</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="327" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mapping a manifold of perceptual observations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Laplacian Eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Postma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Herik</surname></persName>
		</author>
		<ptr target="http://ticc.uvt.nl/∼lvdrmaaten/Laurens_van_der_Maaten/Publications.html" />
		<title level="m">Dimensionality reduction: a comparative review</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Out-ofsample extensions for LLE, ISOMAP, MDS, Eigenmaps, and spectral clustering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paiement</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Delalleau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ouimet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Spectral regression for dimensionality reduction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno>UIUCDCS-R-2007-2856</idno>
		<imprint>
			<date type="published" when="2007-05">May 2007</date>
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, UIUC</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neighborhood preserving embedding</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings in International Conference on Computer Vision (ICCV)</title>
		<meeting>in International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Locally linear embedded eigenspace analysis, IFP-TR</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005-01">January 2005</date>
		</imprint>
		<respStmt>
			<orgName>University of Illinois at Urbana-Champaign</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Isometric projection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence</title>
		<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sparse representation for signal classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aviyente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The smashed filter for compressive classification and target recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Davenport</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Takhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IS&amp;T/SPIE Symposium on Electronic Imaging: Computational Imaging</title>
		<meeting>IS&amp;T/SPIE Symposium on Electronic Imaging: Computational Imaging</meeting>
		<imprint>
			<date type="published" when="2007-01">January 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Detection and estimation with compressive measurements</title>
		<author>
			<persName><forename type="first">M</forename><surname>Davenport</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baraniuk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-01-24">January 24, 2007</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graph embedding and extensions: a general framework for dimensionality reduction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="51" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Face recognition using Laplacianfaces</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE. Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A kernel view of the dimensionality reduction of manifolds</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visual recognition and inference using dynamic overcomplete sparse learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kreutz-Delgado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2301" to="2352" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stork</surname></persName>
		</author>
		<title level="m">Pattern Classification</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An overview of JPEG-2000</title>
		<author>
			<persName><forename type="first">M</forename><surname>Marcellin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gormish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bilgin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boliek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Data Compression Conference</title>
		<meeting>the Data Compression Conference</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image denoising via sparse and redundant representations over learned dictionaries</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3736" to="3745" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image super-resolution as sparse representation of raw image patches</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the LASSO</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Regularization and variable selection via the elastic net</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="301" to="320" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A lecture on compressive sensing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="118" to="121" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Compressed sensing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1289" to="1306" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Feature selection in face recognition: a sparse representation perspective</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
		<idno>UCB/EECS-2007-99</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>UC Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Matching pursuits with time-frequency dictionaries</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3397" to="3415" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bayesian compressive sensing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2346" to="2356" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Local learning projections</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Atomic decomposition by basis pursuit</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="129" to="159" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Fast solution of l1-norm minimization problems when the solution may be sparse</title>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsaig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<pubPlace>USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Institute for Computational and Mathematics and Engineering, Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sparse Bayesian learning and the relevance vector machine</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Tipping</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="211" to="244" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Spectral regression: a unified approach for sparse subspace learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Data Mining (ICDM)</title>
		<meeting>International Conference on Data Mining (ICDM)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<title level="m">Sparse principle component analysis</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>Statistics Department, Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Non-negative sparse PCA</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Non-negative matrix factorization with sparseness constraints</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1457" to="1469" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Subpattern-based principal component analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1081" to="1083" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">PCA versus LDA</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="233" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Acquiring linear subspaces for face recognition under variable lighting</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="684" to="698" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Density-weighted Nystrom method for computing large kernel eigen-systems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="146" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">About the Author-Songcan Chen received the B.Sc. degree in mathematics from Hangzhou University (now merged into Zhejiang University) in1983. In December 1985, he completed the M.Sc. degree in computer applications at Shanghai Jiaotong University and then worked at Nanjing University of Aeronautics and Astronautics (NUAA) in January 1986 as an assistant lecturer. There he received a Ph.D. degree in communication and information systems in 1997. Since 1998, as a full professor, he has been with the Department of Computer Science and Engineering at NUAA. His research interests include pattern recognition, machine learning and neural computing. In these fields, he has authored or coauthored over 130 scientific journal papers. About the Author-Xiaoyang Tan received his B.S. and M.S. degrees in computer applications from Nanjing University of Aeronautics and Astronautics (NUAA) in 1993 and 1996, respectively. Then he worked at NUAA in June 1996 as an assistant lecturer. He received a Ph.D. degree for his thesis</title>
		<imprint>
			<date type="published" when="2004">2004. 2005</date>
			<pubPlace>China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Chengdu University of Technology ; Department of Computer Science and Engineering, Nanjing University of Aeronautics and Astronautics (NUAA) ; Department of Computer Science and Technology of Nanjing University</orgName>
		</respStmt>
	</monogr>
	<note>His research interests focus on image processing, pattern recognition and machine learning. robust face recognition from a single training sample per person using Self-Organizing Map&quot; from</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
