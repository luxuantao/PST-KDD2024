<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-05-19">19 May 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Shibo</forename><surname>Hao</surname></persName>
							<email>s5hao@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Mohamed bin Zayed</orgName>
								<orgName type="institution">University of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianyang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mohamed bin Zayed</orgName>
								<orgName type="institution">University of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mohamed bin Zayed</orgName>
								<orgName type="institution">University of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mohamed bin Zayed</orgName>
								<orgName type="institution">University of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Uc</forename><forename type="middle">San</forename><surname>Diego</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mohamed bin Zayed</orgName>
								<orgName type="institution">University of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-05-19">19 May 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2305.11554v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Augmenting large language models (LLMs) with external tools has emerged as a promising approach to solving complex problems. However, traditional methods, which finetune LLMs with tool demonstration data, can be both costly and restricted to a predefined set of tools. Recent in-context learning paradigm alleviates these issues, but the limited context length only allows for a few shots of demonstrations, leading to suboptimal understandings of the tools. Moreover, when there are numerous tools to choose from, in-context learning could completely fail to work. In this paper, we propose an alternative approach, ToolkenGPT, which combines the benefits of both sides. Our approach represents each tool as a token ("toolken") and learns an embedding for it, enabling tool calls in the same way as generating a regular word token. Once a toolken is triggered, the LLM is prompted to complete arguments for the tool to execute. ToolkenGPT offers the flexibility to plug in an arbitrary number of tools by expanding the set of toolkens on the fly. In addition, it improves tool use by allowing extensive demonstration data for learning the toolken embeddings. In diverse domains, including numerical reasoning, knowledge-based question answering, and embodied plan generation, our approach effectively augments LLMs with tools and substantially outperforms various latest baselines. ToolkenGPT demonstrates the promising ability to use relevant tools from a large tool set in complex scenarios.</p><p>Recent advancements in LLMs have witnessed two primary lines of research approaches for tool integration with LLMs <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b49">50]</ref> (Table <ref type="table">1</ref>). The first paradigm involves fine-tuning LLMs to learn specific tools <ref type="bibr" target="#b46">[47]</ref>. For example, there are enormous efforts to integrate the retrieval tool into LLMs <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b2">3]</ref> and the recent Toolformer [53] fine-tuned GPT-J to learn five tools. While this method could yield promising results, it is computationally expensive and lacks the adaptability Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large Language Models (LLMs) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b43">44]</ref> have established themselves as powerful tools for diverse real-world applications, ranging from writing assistance to automated customer support <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14]</ref>. As these models continue to evolve, there is a growing interest in their potential to interact with the real world and enhance their functionality through integration with other tools, such as the calculator, databases, etc <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b49">50]</ref>. The capability of these models to master and control a wide array of tools not only serves as an indicator of their intelligence, but also signals a promising path to overcome some of their fundamental weaknesses. These include updating the latest world knowledge <ref type="bibr" target="#b42">[43]</ref>, reducing their hallucinations <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b54">55]</ref>, and executing symbolic operations <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b44">45]</ref>, etc. However, the rapid emergence of new tools, such as advanced software libraries, novel APIs, or domain-specific utilities <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b25">26]</ref>, introduces additional richness and complexity to the task of tool learning for LLMs. This continuous evolution accentuates the importance of empowering LLMs with the ability to adapt and master massive new tools swiftly. Fine-tuning [e.g., <ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b46">47]</ref> In-context learning [e.g., <ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b6">7]</ref> ToolkenGPT <ref type="bibr">(Ours)</ref> to new tools. The second approach relies on in-context learning <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b49">50]</ref>, where LLMs learn how to use the tool through in-context demonstrations provided in the prompt. This method allows LLMs to handle newly introduced tools and drives successful applications like LangChain <ref type="bibr" target="#b6">[7]</ref> and ChatGPT Plugin<ref type="foot" target="#foot_0">1</ref> . However, in-context learning comes with its own unique limitations. Specifically, it struggles with the inherent limitation of context length, making it impossible to demonstrate massive tools in the context. Also, mastering new tools simply via few-shot examples could be challenging. For example, even the latest models like GPT-4 face difficulties when handling unusual tools <ref type="bibr" target="#b5">[6]</ref>.</p><p>In this paper, we introduce ToolkenGPT, an alternative solution that enables LLMs to master massive tools without the need for any LLM fine-tuning, while still allowing for quick adaptation to new tools.</p><p>The key idea of ToolkenGPT is to represent each tool as a new token ("toolken") to augment the vocabulary. Specifically, each tool is associated with an embedding inserted into the LLM head like a regular word token embedding. During generation, once a toolken is predicted, the LLM temporarily switches into a special mode (through prompting) to produce input arguments for the tool to execute, and inject the outputs back into the generation (see Figure <ref type="figure" target="#fig_1">1</ref>). This approach offers an efficient way for LLMs to master tools by only learning the lightweight toolken embeddings. Consequently, ToolkenGPT combines the strengths of both fine-tuning and in-context learning paradigms while avoiding their limitations (Table <ref type="table" target="#tab_0">1</ref>): Compared to in-context learning that can only accommodate a small number of tools and few-shot demonstrations, ToolkenGPT allows massive tools (by simply inserting respective toolkens in the vocabulary) and can use extensive demonstration data for learning toolken embeddings; In contrast to LLM fine-tuning, the tool embeddings not only requires minimal training cost, but also provide a convenient means for plugging in arbitrary new tools on the fly by expanding the toolken vocabulary.</p><p>We demonstrate the flexibility and effectiveness of our ToolkenGPT in leveraging numerous external tools for solving a diverse set of problems, spanning from numerical reasoning to knowledge-based question answering and embodied plan generation. In complex numerical reasoning problems that involve a number of mathematical tools (numerical operations such as finding greatest common divisor), we show that ToolkenGPT can effectively utilize these tools during the reasoning process, which outperforms some of latest popular approaches, such as Chain-of-Thought <ref type="bibr" target="#b61">[62]</ref> and ReAct <ref type="bibr" target="#b64">[65]</ref>. For knowledge-based question answering, ToolkenGPT accommodates a substantial number of relation APIs (over 200) from the knowledge base, thereby facilitating factual predictions. Furthermore, we apply our framework to task planning for embodied agents, where an agent interacts with an environment using tools, namely the actions and objects. The findings illustrate that our method offers better grounding by learning toolken embeddings for 58 grounded actions and objects than previous in-context learning and specialized decoding methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Fine-tuning LLMs to use tools. Early research relied heavily on fine-tuning to augment LMs with tools. In these works, LMs were mostly fine-tuned to use one or a few tools in a specific domain.</p><p>For example, the retriever has been a crucial tool for augmenting LLMs with external knowledge sources <ref type="bibr" target="#b65">[66]</ref>. The prominent works in this line include REALM <ref type="bibr" target="#b17">[18]</ref>, RAG <ref type="bibr" target="#b31">[32]</ref>, and RETRO <ref type="bibr" target="#b2">[3]</ref>. More recently, WebGPT <ref type="bibr" target="#b42">[43]</ref> fine-tuned GPT-3 on human web search behaviors to learn how to use the web browser. With the advancements in LLMs, there has also been growing interest in tuning these models on a collection of general tools, including the QA model, calculator, translator, etc. Example works include TALM <ref type="bibr" target="#b46">[47]</ref> and Toolformer <ref type="bibr" target="#b52">[53]</ref>. However, LLM fine-tuning is costly and these tuned LLMs struggle to generalize to emergent or updated tools. ToolkenGPT learns lightweight toolken embeddings for new tools, without any gradient calculation for the parameters of LLMs. This enables efficient adaption to new tools and maintains a minimal GPU memory overhead for training toolken embeddings, at a cost similar to LLM inference.</p><p>The maximal side length of each section is 16 meters, so the area is &lt;square&gt;( <ref type="formula">16</ref>) Question: John has a rectangular garden, of which the length is 96 meters and the width is 64 meters. He wants to divide the garden into identical square sections, each with the largest possible area. What's the area of each section?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Execution</head><p>Answer: The maximal side length of each section is 16 meters. Therefore, the area is _____ Toolken embeddings are appended to the language model head like regular word tokens. In the "reasoning mode" for solving the problem, the LLM generates text as usual, except that any plugged-in toolkens are also considered for the next token generation. Once a toolken is predicted, (1) the LLM switch to the "tool mode", which provides a few demonstrations of the same tool to complete the arguments. Then, (2) the tool call is executed and, (3) the result is sent back to the text to continue the reasoning mode until the final answer is generated.</p><p>In-context learning for tools. LLMs exhibit a strong in-context learning ability <ref type="bibr" target="#b4">[5]</ref>, which becomes a prevalent method to use tools by showing tool descriptions and demonstrations in context <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b49">50]</ref>. Building on this idea, reasoning chains can be incorporated to tackle more complex problems <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b45">46]</ref>. This paradigm has given rise to popular industry products such as ChatGPT plugins and Langchain <ref type="bibr" target="#b6">[7]</ref>, along with many successful applications in important research topics. For instance, a code interpreter can effectively address the LLM's shortcomings in symbolic operations <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b36">37]</ref>. Furthermore, by calling "tools" that have an effect on the virtual or physical world, the LLM is capable of guiding embodied agents to accomplish various household tasks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b23">24]</ref>. Recent attempts to utilize LLMs as a controller to coordinate multiple neural models also achieve promising progress in multimodal reasoning tasks <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b39">40]</ref>. Nevertheless, all methods based on in-context learning suffer from inferior performance in complex scenarios, where the tools are unfamiliar or numerous. One concurrent work, Li et al. <ref type="bibr" target="#b32">[33]</ref> propose to retrieve the tools based on the text embedding of their documents, which may mitigate that issue. However, ToolkenGPT is fundamentally different from their method, in that the toolken embeddings can encode the implicit semantics of tools from extensive demonstrations, which can never be inferred from the surface text (A concrete example is shown in Figure <ref type="figure" target="#fig_4">3</ref>). Also, note that ToolkenGPT is compatible with the recent advanced prompting techniques, e.g., Chain-of-Thought (CoT) <ref type="bibr" target="#b61">[62]</ref>, to improve the LLMs performance further.</p><p>Efficient tuning large language models. Adapting pre-trained frozen LLMs efficiently to new tasks is an active research area, leading to a surge of interest in parameter-efficient fine-tuning (PEFT) methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b37">38]</ref>. The idea is only to fine-tune a small subset of parameters of the LLM while freezing most of its parameters, which bears similarity to our toolken embedding method. Which part of parameters to tune is the key to PEFT methods; for instance, Adapters <ref type="bibr" target="#b19">[20]</ref> insert trainable layers, BitFit <ref type="bibr" target="#b66">[67]</ref> tunes the bias parameters, prompt tuning <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b60">61]</ref> appends parameters to the input embedding layer, and LoRA <ref type="bibr" target="#b20">[21]</ref> learns low-rank matrices within specific dense layers, etc. However, existing PEFT methods have not proven suitable for efficient tool learning, and utilizing these methods on tool demonstrations may not efficiently capture the desired tool knowledge as ToolkenGPT does. To the best of our knowledge, we are the first to explore efficient tuning methods for predicting tools as tokens for tool learning of massive tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ToolkenGPT for Mastering Massive Tools</head><p>In this section, we present ToolkenGPT, which enables LLMs to learn and use massive tools for complex problem-solving without the need for heavily fine-tuning the LLM. We begin by introducing the background and notations of language modeling for tool use. Typically, LLMs model the probability of a sequence of word tokens s = (t 1 , t 2 , ..., t n ) as P (s) =</p><formula xml:id="formula_0">n i P (t i | t &lt;i )</formula><p>, where each word token comes from the vocabulary of the LLM, i.e. t i ? V and t &lt;i denotes the partial word token sequence before i-th step. In practice, the user often sets the prefix of a sequence (referred to as the prompt) to steer LLMs to generate desired contents, e.g., answering a question. Taking a step deeper, the distribution of the next token is predicted as</p><formula xml:id="formula_1">P (t i |t &lt;i ) = softmax(W ? ? h i-1 )</formula><p>, where h i-1 ? R d is the last hidden state of the current context and W ? ? R |V|?d is the embedding matrix for word tokens (also known as language model head).</p><p>Given a set of useful tools T = {? 1 , ? 2 , ...}, our goal is to enable LLMs to call a subset of these tools for solving the complex problem. Our flexible formulation allows tools to play a role by either returning some results that can help LLMs with text generation (e.g. calculation) or affecting the real-world environment (e.g. robot action). To call a tool during generation, the LLM first needs to select a tool and then input the arguments. In the running examples shown in Figure1, during the answer generation process ("reasoning mode"), a math operator square is selected as the tool, and an operand 16 is generated as the argument in the "tool mode". Once the external tool receives the call, it executes the tool and returns the result 256, back to the "reasoning mode".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Framework Overview</head><p>The core idea of ToolkenGPT is explicitly formulating tools as tokens (called "toolkens"). Each toolken is parameterized as a toolken embedding vector, and we denote a set of toolken embeddings as a matrix, i.e. W ? ? R |T |?d . Assuming we have trained toolken embeddings (to be described in Section 3.2), we first give an overview of our framework by introducing how it works in inference.</p><p>As shown in Figure <ref type="figure" target="#fig_1">1</ref>, the LLM is in the reasoning mode by default, generating the next token. Our framework allows the LLM to consider word tokens and toolkens uniformly. Specifically, the tool embedding matrix is concatenated with W ? . Therefore, the LLM predicts the next token with the probability as follows:</p><formula xml:id="formula_2">P (t i |t &lt;i ) = softmax([W ? ; W ? ] ? h i-1 )<label>(1)</label></formula><p>where the next token can be either a word token or a toolken, i.e. t i ? V ? T , and [; ] is the concatenation operation. As we can see, our formulation of tools as toolken embeddings naturally allows for the fast adaption of new tools by expanding the toolken embedding matrix easily.</p><p>To execute a tool, the LLM switches into the "tool mode" once its toolken is predicted as the next token (as shown in the "mode switch" in Figure <ref type="figure" target="#fig_1">1</ref>), which aims to generate the arguments for the tool. Specifically, the LLM pauses the generation and appends the current generated context to another prompt. The prompt in tool mode consists of in-context demonstrations for the predicted tool, showing how to generate the tool arguments by quoting the tool calls in a special syntax of [tool](arguments). Then the LLM can follow the pattern in demonstrations to complete the arguments of the current tool call. Contrasting previous methods <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b49">50]</ref> that fully rely on in-context learning for tool learning, our framework only leaves the easy work of completing arguments to in-context learning. Besides, there would be abundant context space for extensive demonstrations of a single specified tool. This design shares similarities with the classic divide-andconquer methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b12">13]</ref>. Finally, the arguments are sent to the specified tool for execution, and the returned value is sent back to the text in the reasoning mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning Toolken Embeddings</head><p>Our framework keeps the original LLM parameters frozen and introduces a minimal additional training overhead with the toolken embeddings, W ? . This embedding matrix contains the only parameters to optimize, but unlike other efficient LLM tuning methods, e.g., prompt tuning <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b60">61]</ref> or prefix tuning <ref type="bibr" target="#b34">[35]</ref>, it does not require the gradients flowing through the major body of LLM parameters, leading to much stable and efficient training. Therefore, the tuning of toolken embeddings maintains nearly the same GPU memory as LLM inference. Whenever a new tool is added, the toolken embedding can be conveniently expanded and then, subsequent training on tool demonstration data involving the new tool gradually refines its embedding. Moreover, unlike in-context learning methods that only digest a few examples as training signals, ToolkenGPT is capable of tuning toolken embeddings from massive demonstrations.</p><p>Drawing parallels to how infants learn a new tool through demonstrations from adults <ref type="bibr" target="#b14">[15]</ref>, in this paper, we primarily focus on learning toolken embeddings with tool demonstrations, which can be either in-domain training data or synthetic data generated by LLMs (see Section 4.1 and Section 4.2). We first describe the format of training data and the training objective and we use the same example from Figure <ref type="figure" target="#fig_1">1</ref> to showcase how it can be used for training. Specifically, "the area is 256 square feet ..." can be tokenized into a word token sequence s = ("the", "area", "is", "2", "5", "6", "square", "feet", ...).</p><p>To indicate when to predict the toolkens, we need a parallel sequence mixed with word tokens and toolkens, i.e. s = ("the", "area", "is", "[square]", "[N/A]", "[N/A]", "square", "feet", ...). The subsequence of ("2", "5", "6") in s is where the returned tool results should fill in, and we choose the corresponding first token in s as the toolken for the tool call with the following tokens are filled with [N/A], indicating neglect in loss calculation. Thus, given a dataset composed of paired sequences D = {(s, s )}, the training objective of ToolkenGPT is:</p><formula xml:id="formula_3">L = (s,s )?D N i=1 -log P (t i |t &lt;i )1 t i =[N/A]<label>(2)</label></formula><p>where</p><formula xml:id="formula_4">1 t i =[N/A]</formula><p>is the indicator function signaling we ignore the [N/A] tokens during the training.</p><p>Thus, our training process is largely consistent with the inference in the reasoning mode. That is, to call a tool, the only job for the LLM is to predict a toolken at the beginning, and then the returned value will be filled back to the text. Here, [N/A] is introduced to skip the generation of the returned value of a tool call.</p><p>There are two primary ways to get the paired data. First, some datasets provide ground truth tool calls along with natural language sequences, e.g. the facts in KB supporting the answer to a question (Secion 4.2), or the calculation trace for solving a math problem (Section 4.1). To use these data for supervised learning, we preprocess them to get the paired data required for training as described in the previous paragraph. Second, we explore synthesizing tool demonstrations with LLMs, sharing a similar idea to self-instruct <ref type="bibr" target="#b59">[60]</ref>. An intuitive interpretation of this process is to distill the knowledge inside LLM to the new toolken embeddings. Specifically, we can use in-context learning to teach LLMs to generate text that utilizes a given tool and quote the tool call with some syntax, resulting in examples like "The capital of U.S. is &lt;capital&gt; ("U.S.")="Washington D.C.". We can then easily locate the tool calls in this format and process the data into the paired data for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we apply ToolkenGPT to three distinct applications characterized by meaningful tool-use scenarios: arithmetic tools for numerical reasoning, database APIs for knowledge-based question answering, and robot actions for embodied plan generation. We focus on how methods can accurately call the tools and how successfully they can solve the tasks. Our experiments show that ToolkenGPT can efficiently master massive tools while leveraging them to solve complex problems with improved performance, consistently better than advanced prompting techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Numerical Reasoning</head><p>LLMs often struggle with mathematical tasks since the models are inherently designed for probabilistic estimation rather than symbolic operations. In this section, we aim to assess the tool-learning capabilities of ToolkenGPT, compared with in-context tool learning (e.g., ReAct <ref type="bibr" target="#b64">[65]</ref>). We first demonstrate that ToolkenGPT consistently matches or outperforms the performance of in-context learning with the availability of four basic arithmetic functions (+, -, ?, ?). Moreover, to benchmark the tool-handling capability in more complex math problems, we include more available tools, i.e., an expanded (13) set of functions, and create a set of synthetic data. The results show that ToolkenGPT significantly outperforms baselines by training only on the synthetic data. Note that our focus is not to reach a state-of-the-art accuracy; Rather, the experiment is designed to evaluate the tool learning ability in the setting where certain tools are available.</p><p>Datasets. To evaluate the tool-learning proficiency in numerical reasoning comprehensively, we curate two new test datasets: (1) GSM8K-XL, an enhanced version of the existing GSM8K <ref type="bibr" target="#b9">[10]</ref> dataset. GSM8K is a dataset of linguistically diverse grade school math word problems, involving performing a sequence of calculations using 4 basic arithmetic operations (+, -, ?, ?) to reach the final answer. In the original GSM8K dataset, the numbers for calculations are typically small, which might be less challenging for the recent powerful LLMs <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b5">6]</ref>. So in the test set, we magnify the numbers to increase the computational difficulty for LLMs, which results in the GSM8K-XL dataset, featuring 568 test cases with much larger numbers. (2) FuncQA is a synthetic dataset we created to increase the complexity of math problems involving more arithmetic tools, which serves as Table <ref type="table">2</ref>: Results on the GSM8K-XL and FuncQA datasets. The quoted number indicates the number of available tools. For GSM8K-XL and FuncQA one dataset, accuracy is evaluated based on an exact match (float numbers rounded to two decimals). In FuncQA multi , we allow a margin of error of 0.1% to account for potential errors at each step of multi-hop reasoning.</p><p>Method GSM8K-XL (4) FuncQA <ref type="bibr" target="#b12">(13)</ref> One-Hop Multi-Hops 0-shot ChatGPT 0.17 0.55 0.09 CoT <ref type="bibr" target="#b61">[62]</ref> 0.18 0.20 0.03 ReAct <ref type="bibr" target="#b64">[65]</ref> 0.32 0.57 0.06 ToolkenGPT (Ours) 0.33 0.73 0.15 a much more challenging benchmark to test the model's tool-learning capabilities. Specifically, This dataset requires at least 13 operators (e.g., power, sqrt, lcm) to solve, and it is challenging for both humans and LLMs to solve without an external calculator. Furthermore, FuncQA is categorized into two subsets: 68 one-hop questions (FuncQA one ) solvable with just one operation, and 60 multi-hop questions (FuncQA multi ) requiring a few reasoning steps.</p><p>To train the toolken embeddings used in GSM8K-XL, we preprocess the original training set of GSM8K which has the calculation annotation as described in Section 3.2. We get 6,054 examples, of which 1,000 were allocated for validation, and 5,054 for the training data. For the FuncQA dataset, we prompt ChatGPT to generate some one-hop QA patterns for each operator, and then randomly assign values to the patterns. This process yields 47 training data points and 3 validation data points for each operator, resulting in a total of 611 samples for training and 39 samples for validation.</p><p>Comparison methods. We train toolken embeddings for each available math operator as described in Section 3.2. During inference, we prompt the LLM with 4 Chain-of-Thought <ref type="bibr" target="#b61">[62]</ref> examples to enhance the reasoning ability of LLMs. The following baselines are evaluated for comparison: (1) 0-shot CharGPT is the straightforward method asking LLMs to answer a question. No examples will be provided in the context and tools are not available. We use ChatGPT as the base LLM in our experiment. This baseline measures the ability of the LLM to answer complex numerical reasoning problems with its own reasoning and calculation ability. ( <ref type="formula" target="#formula_3">2</ref>) Chain-of-thougts (CoT) <ref type="bibr" target="#b61">[62]</ref> is a more advanced prompting techniques. In this approach, a series of interconnected prompts are carefully crafted to guide the LLMs through a step-by-step reasoning process. The example reasoning chains are the same as the ones we used for ToolkenGPT, but no functions are available. (3) ReAct <ref type="bibr" target="#b64">[65]</ref> combines reasoning and tools by prompting the LLMs to generate verbal reasoning traces and tool calls in an interleaved manner. Concretely, instead of just providing reasoning chains such as "... The cost is 50*3.2=160", ReAct incorporates special syntax to call operators, e.g."... The cost is 50*3.2=&lt;multiply&gt;(50,3.2)=160". Once the syntax is detected during inference, the tool would be called to calculate the result. We use the same reasoning chain examples as in both CoT and ToolkenGPT, with minor differences in the tool calling syntax. LLaMA-30B <ref type="bibr" target="#b58">[59]</ref> is used as the LLM for all settings other than zero-shot prompting.</p><p>Result analysis. Table <ref type="table">2</ref> shows the performance of all the methods on the GSM8K-XL and FuncQA datasets. On the GSM8K-XL dataset, 0-shot ChatGPT and few-shot learning with CoT struggle to calculate large numbers without the help of tools, while ReAct and ToolkenGPT manage to increase accuracy consistently by a large margin. Generally, both methods can call the correct tools when necessary, as the toolset is comprised of only the four basic operators. However, for both FuncQA one and FuncQA multi datasets, learning to call applicable tools becomes challenging to ReAct as the number of tools increases. In ReAct, though all the tools are listed at the beginning of the prompt, it is infeasible to include demonstrations of every tool in the limited context (In our experiment, we provide 4 examples including 5 tool demonstrations). As a result, ReAct is susceptible to missing, making wrong tool calls, and predicting wrong arguments, especially for the tools not demonstrated in context. ToolkenGPT outperforms all the baselines across both one-hop and multi-hop scenarios, showing superior tool learning ability when there are numerous tools. It is important to note that even though toolken embeddings are trained solely using one-hop synthetic data, and without any CoT examples, they still manage to enhance performance in multi-hop problem contexts and can be integrated effectively with CoT prompting. This implies a degree of generalization of toolken embeddings, which is a very desired property that lowers the requirements of in-domain training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Knowledge-based Question Answering</head><p>LLMs are known to often make factual errors and hallucinate <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b0">1]</ref>. Equipping them with access to knowledge bases (KBs) has been a promising research direction to reduce their hallucinations <ref type="bibr" target="#b54">[55]</ref>.</p><p>We formulate the access to the KB as APIs querying the database <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b15">16]</ref>. Thus, each relational query can be treated as a tool to which the input argument is a subject entity, and the output is the corresponding tail entity. An example tool call is "P1346(2005-06 FA CUP) ? LIVERPOOL F.C.".</p><p>Here "P1346" is a relation identifier in Wikidata, representing winner of a competition or similar event (referred to winner_of below for ease of reading). In this section, we show that ToolkenGPT can accurately query a large knowledge base of up to 234 tools (relations). We further show that even only with synthetic data (as described in Section 3.2 and explained below), we can train strong toolken embeddings that outperform popular tool-learning methods.</p><p>Dataset. KAMEL <ref type="bibr" target="#b26">[27]</ref> is a question-answering dataset built with the facts in Wikidata. In line with ToolFormer <ref type="bibr" target="#b52">[53]</ref>, which uses its earlier version <ref type="bibr" target="#b47">[48]</ref> as a benchmark to evaluate the tool use, we adopt KAMEL to evaluate the use of KB query tools. KAMEL contains knowledge about 243 relations from Wikidata, each of which is associated with a question template (e.g. winner_of: "Who is the winner of [S]?") to turn a fact in Wikidata into a question. We have 234 tools in total for this dataset.</p><p>In order to analyze the performance provided with different numbers of tools, we create four subsets by sampling from the original test set. Each subset consists of questions related to different numbers of relations, corresponding to 30, 60, 100, and 234, respectively. The size of each subset is 500.</p><p>Comparison methods. We set up two different variants of our framework.  We introduce the following baselines for comparisons: (1) Prompting <ref type="bibr" target="#b26">[27]</ref> is a straightforward method that answers the questions with the LLM's internal knowledge. We frame each question within the prompt "Question:</p><p>[QUESTION]\nThe answer is" and ask the LLM to continue the sentence.</p><p>(2) In-context Learning (ICL) <ref type="bibr" target="#b49">[50]</ref> is a standard method to augment LLMs with tools as introduced in Section 2. Before asking the question, we list the tool demonstrations and descriptions of all available tools. The demonstrations are shown in a specific syntax so that the LLM can generate in a similar style to be parsed. An example demonstration for winner_of is "Question: Who is the winner of 2005-06 FA Cup?\nAnswer: The answer is &lt;winner_of&gt;(2005-06 FA Cup)=Liverpool F.C." In a recent survey <ref type="bibr" target="#b49">[50]</ref>, this setting is referred to as "few-shot". (3) In-context Learning (desc) <ref type="bibr" target="#b49">[50]</ref> is another common practice to augment LLMs with tools. The descriptions of all available tools will be provided in context, but their demonstrations are not directly shown. Instead, we show 8 demonstrations of the tools not included in the test subset to inform LLMs about the tool call format. This setting is referred to as "zero-shot" in Qin et al. <ref type="bibr" target="#b49">[50]</ref>.</p><p>The base models are LLaMA-13B <ref type="bibr" target="#b58">[59]</ref>.</p><p>Result analysis. We show the experiment results on 4 testsets involving different numbers of relations in Figure <ref type="figure" target="#fig_3">2</ref>. Note that the number of involved relations is the number of tools we can use. For all testsets, the accuracy of Prompting is about 20%, which indicates LLMs still struggle to store accurate facts in their parameters and it's necessary to augment them with a knowledge base. ToolkenGPT (sup) achieves the highest results with a large margin, showing that learning toolken embeddings is an effective method when there is massive in-domain training data. On the contrary, even though In-context learning also sees in-domain training data in the context, it still gets confused about which tools to call. Furthermore, the context length limit leads to drastic performance drops when there are more than 30 tools to use. The failure in the many-tools scene reveals the fundamental limitation of the in-context learning paradigm. ToolkenGPT (syn) also outperforms all other baselines in all subsets, without seeing any in-domain training data. The synthetic training data, often in very different expression styles from the dataset, still helps the LLM understand these relations. This success reflects the flexibility of our framework which can be applied even if there is no in-domain training data available. In-context learning (desc) generally fails in this task, because the LLM has difficulties memorizing text descriptions shown in contexts and mapping them to relation identifiers.</p><p>The results provide more evidence to the previous discovery that LLMs have trouble using unfamiliar tools <ref type="bibr" target="#b5">[6]</ref>. Based on this observation, it is reasonable to speculate that LLMs mostly recall the tools from their identifier instead of really learning to use tools from their descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Embodied Plan Generation</head><p>Recently, there have been many research attempts to utilize LLMs as the controller of embodied agents <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b23">24]</ref>. Despite the preliminary success of prompting LLMs, teaching LLMs about an environment and enabling them to make grounded predictions remain challenging. As discussed in Mialon et al. <ref type="bibr" target="#b41">[42]</ref>, tools that gather additional information (e.g. math or KB tools) and tools that have an effect on the physical world (e.g. actions taken by embodied agents) can be called in similar styles by the LLM. In this section, we demonstrate how our framework can also be applied to plan generation for embodied agents. Compared to previous methods that prompt LLMs, our ToolkenGPT can understand the environment better by learning toolken embeddings for agent action and object.</p><p>Dataset. VirtualHome <ref type="bibr" target="#b48">[49]</ref> is a simulation platform for typical household activities, and ActivityPrograms knowledge base <ref type="bibr" target="#b48">[49]</ref> consists of many tasks with plans executable in VirtualHome. We derive a subset of 297 tasks from ActivityPrograms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Work</head><p>Go to office, sit at desk, turn on computer, enter password, open application and begin work  The model is expected to output an executable plan, which is an ordered list of verb-object instructions (e.g. "[FIND] &lt;novel&gt;"). Each task comes with an initial and final state graph, enabling the verification of the generated plans with the simulator and the comparison of the resulting final state with ground truth. We split the dataset into a training set of 247 tasks and a test set of 50 tasks, with a total of 25 verbs and 32 objects used in the dataset.</p><p>Comparison methods. We consider all the actions and objects in VirtualHome as tools. With an additional [END] function indicating the end of a plan, we have 58 toolkens in total. For this dataset, we do not need the argument generation process described in Figure <ref type="figure" target="#fig_1">1</ref> because the tools do not take arguments. During inference, ToolkenGPT alternatively generates action toolkens and object toolkens, and ends with the [END] toolken. The toolken embeddings are trained with the training set. We compare our method to the following baselines: (1) In-context Learning prompts the LLM and parses its outputs as the plan. The LLM is shown with the action list, 3 demonstration plans, and a new task with its goal, detailed description, and environment description. This method is the base of most recent methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b23">24]</ref> that apply LLMs to embodied AI. (2) Translation <ref type="bibr" target="#b21">[22]</ref>: To avoid plans that include unavailable actions or objects, Huang et al. <ref type="bibr" target="#b21">[22]</ref> proposes to use a translation model to translate the LLM's generation to admissible instructions. Following Huang et al. <ref type="bibr" target="#b21">[22]</ref>, we use SentenceRoBERTa-large <ref type="bibr" target="#b50">[51]</ref> and translate the actions or objects to available ones with the highest cosine similarities. (3) Grounded Decoding <ref type="bibr" target="#b23">[24]</ref> is a recent decoding-stage grouding method. The next token is predicted considering both LLM logits and "grounded functions". Specifically, we apply the affordance grounding function <ref type="bibr" target="#b23">[24]</ref>, encouraging LLMs to generate valid actions and objects. We do not consider other previous methods that heavily fine-tune the whole language model <ref type="bibr" target="#b33">[34]</ref>. The base model of all methods is LLaMA-13B <ref type="bibr" target="#b58">[59]</ref>.</p><p>Result analysis. We list results in Table <ref type="table" target="#tab_2">3</ref>. Though all valid actions and objects are explicitly listed in the context for the LLM using In-context Learning, it sometimes fails to ground its prediction to admissible instructions. Even though the instructions are valid, they often violate the physical rule in VirtualHome, resulting in a low success rate. We notice that while most of the plans generated with In-context Learning appear reasonable to humans, they are not tailored to the specific environment of VirtualHome. Translation <ref type="bibr" target="#b21">[22]</ref> helps solve some shallow grounding problems, e.g. [tv] ? [television], while Grounded Decoding <ref type="bibr" target="#b23">[24]</ref> further improves executable and success rate by considering grounding earlier in the decoding stage. Although these methods ensure all plans are grounded, neither significantly improves the LLM's understanding of actions and objects, leading to unsatisfactory executable and success rates. ToolkenGPT not only predict valid actions and objects naturally by its design, but also achieves the highest success rate by learning toolken embeddings from more training tasks. A concrete example is shown in Figure <ref type="figure" target="#fig_4">3</ref> to illustrate the difference: All the baselines predict [SIT] &lt;desk&gt;, presumably guided by the description "sit at desk", but in VirtualHome [SIT] refers to "sit on", and a desk is regarded as not sittable. ToolkenGPT is the only one to successfully learn this rule from demonstrations and instead predict &lt;chair&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present ToolkenGPT, an innovative approach for augmenting frozen LLMs with massive external tools without expensive fine-tuning. Our method distinguishes itself by leveraging the concept of toolken embeddings that correspond to specific tools, enabling LLMs to call and use these tools as easily as generating a word token. Our approach overcomes the limitations of current fine-tuning and in-context learning paradigms, enabling LLMs to accommodate a much larger set of tools and use extensive demonstration data for learning toolken embeddings. Our experiments demonstrated the compelling advantages of ToolkenGPT. On a series of complex tasks, ranging from numerical reasoning, knowledge-based question answering, to embodied plan generation, we observed a significant enhancement in LLM performance with the help of toolken embeddings. More importantly, ToolkenGPT is able to rapidly adapt and leverage new tools, demonstrating its capacity to keep pace with the constantly evolving landscape of massive tools.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Overview of ToolkenGPT framework. Toolken embeddings are appended to the language model head like regular word tokens. In the "reasoning mode" for solving the problem, the LLM generates text as usual, except that any plugged-in toolkens are also considered for the next token generation. Once a toolken is predicted, (1) the LLM switch to the "tool mode", which provides a few demonstrations of the same tool to complete the arguments. Then, (2) the tool call is executed and, (3) the result is sent back to the text to continue the reasoning mode until the final answer is generated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 1 )</head><label>1</label><figDesc>ToolkenGPT (sup): We sample 200 examples per relation from the training set of KAMEL and train the toolken embeddings via supervised learning. This setting represents real-world scenarios where sufficient in-domain training data is available. (2) ToolkenGPT (syn): In a more challenging setting where we assume in-domain training data is not available, we use the text description of each relation to synthesize training data with ChatGPT, e.g. "The Nobel Peace Prize in 2020 was awarded to the United Nations World Food Programme for its efforts...", where the underlying tool call is winner_of(NOBEL PEACE PRIZE IN 2020)?UNITED NATIONS WORLD FOOD PROGRAMME. On average about 40 examples are used to train each toolken embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance of ToolkenGPT and baselines on 4 testsets involving different numbers of tools (relations) from KAMEL. ICL is short for In-context Learning [50]. Due to the context length limit of 2048 tokens, we list the descriptions and demonstrations of up to 30 relations for ICL, and the descriptions of up to 60 relations for ICL (desc).</figDesc><graphic url="image-13.png" coords="7,271.81,419.11,226.77,173.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Case study on VirtualHome. ToolkenGPT predicts a successful script while other baselines fail to produce an executable one due to their misunderstanding of the SIT action. Specifically, for each task, the model is given a high-level goal (e.g. "Read book"), a detailed instruction (e.g. "I would go lie down in my bed and open the book and start reading.", and a description of the environment, which includes the initial state of the agent, and the object list of the environment (e.g. "I am in ['home_office']. The objects I can manipulate are ['mail', 'freezer', 'television', ..., 'novel']".The model is expected to output an executable plan, which is an ordered list of verb-object instructions (e.g. "[FIND] &lt;novel&gt;"). Each task comes with an initial and final state graph, enabling the verification of the generated plans with the simulator and the comparison of the resulting final state with ground truth. We split the dataset into a training set of 247 tasks and a test set of 50 tasks, with a total of 25 verbs and 32 objects used in the dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of different tool learning paradigms.</figDesc><table><row><cell>Tool Learning Paradigms</cell><cell>Frozen LMs</cell><cell>Massive Tools</cell><cell>Plug-&amp;-Play</cell><cell>Ability to Use Extensive Data</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results on VirtualHome. Grounding means the proportion of scripts in which all the actions and objects can be grounded to the environment. Executable means the proportion of scripts that can be executed in VirtualHome without violating any rules. Success means the proportion of scripts that leads to the correct final state. Success (R) is a relaxed variant meaning the proportion of scripts that have reached the correct final state, but not necessarily ending with it.</figDesc><table><row><cell cols="5">Method Grounding Executable Success Success (R)</cell></row><row><cell>In-context Learning</cell><cell>0.74</cell><cell>0.42</cell><cell>0.20</cell><cell>0.30</cell></row><row><cell>+ Translation [22]</cell><cell>1.00</cell><cell>0.52</cell><cell>0.24</cell><cell>0.32</cell></row><row><cell>+ Grounded Decoding [24]</cell><cell>1.00</cell><cell>0.66</cell><cell>0.38</cell><cell>0.42</cell></row><row><cell>ToolkenGPT (Ours)</cell><cell>1.00</cell><cell>0.82</cell><cell>0.68</cell><cell>0.70</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://openai.com/blog/chatgpt-plugins</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large language models and the perils of their hallucinations</title>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Azamfirei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Sapna R Kudchadkar</surname></persName>
		</author>
		<author>
			<persName><surname>Fackler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Critical Care</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bommarito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Martin</forename><surname>Katz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.14402</idno>
		<title level="m">Gpt takes the bar exam</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving language models by retrieving from trillions of tokens</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">Bm</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2206" to="2240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Do as i can, not as i say: Grounding language in robotic affordances</title>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Brohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yevgen</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Irpan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Julian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="287" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">S?bastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ece</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Tat Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Lundberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.12712</idno>
		<title level="m">Sparks of artificial general intelligence: Early experiments with gpt-4</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Harrison</forename><surname>Chase</surname></persName>
		</author>
		<author>
			<persName><surname>Langchain</surname></persName>
		</author>
		<ptr target="https://github.com/hwchase17/langchain" />
		<imprint>
			<date type="published" when="2022">10 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks</title>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueguang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.12588</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">Scaling language modeling with pathways</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Training verifiers to solve math word problems</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14168</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Min</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weize</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.06904</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level</title>
		<author>
			<persName><forename type="first">Iddo</forename><surname>Drori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reece</forename><surname>Shuttleworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonard</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunny</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Newman</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">32</biblScope>
			<biblScope unit="page">2123433119</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Successive prompting for decomposing complex questions</title>
		<author>
			<persName><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivanshu</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.04092</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Gpts are gpts: An early look at the labor market impact potential of large language models</title>
		<author>
			<persName><forename type="first">Tyna</forename><surname>Eloundou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Rock</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.10130</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">What does it take for an infant to learn how to use a tool by observation?</title>
		<author>
			<persName><forename type="first">Jacqueline</forename><surname>Fagard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lauriane</forename><surname>Rat-Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rana</forename><surname>Esseily</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eszter</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>O'regan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in psychology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">267</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A survey on complex question answering over knowledge base: Recent advances and challenges</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunqi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengguang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13069</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.10435</idno>
		<title level="m">Pal: Program-aided language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Retrieval augmented language model pre-training</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingwei</forename><surname>Chang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3929" to="3938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Solving math word problems by combining language models with symbolic solvers</title>
		<author>
			<persName><forename type="first">Joy</forename><surname>He-Yueya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Poesia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rose</forename><forename type="middle">E</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.09102</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for nlp</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2790" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lora: Low-rank adaptation of large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Language models as zero-shot planners: Extracting actionable knowledge for embodied agents</title>
		<author>
			<persName><forename type="first">Wenlong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="9118" to="9147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Inner monologue: Embodied reasoning through planning with language models</title>
		<author>
			<persName><forename type="first">Wenlong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ted</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harris</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacky</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yevgen</forename><surname>Chebotar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.05608</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Grounded decoding: Guiding text generation with grounded models for robot control</title>
		<author>
			<persName><forename type="first">Wenlong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Driess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Hausman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.00855</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Survey of hallucination in natural language generation</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nayeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rita</forename><surname>Frieske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiezheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etsuko</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Genegpt: Augmenting large language models with domain tools for improved access to biomedical information</title>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Kamel: Knowledge analysis with multitoken entities in language models</title>
		<author>
			<persName><forename type="first">Jan-Christoph</forename><surname>Kalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leandra</forename><surname>Fichtel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Automated Knowledge Base Construction</title>
		<meeting>the Conference on Automated Knowledge Base Construction</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Compacter: Efficient low-rank hypercomplex adapter layers</title>
		<author>
			<persName><forename type="first">Rabeeh</forename><surname>Karimi Mahabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1022" to="1035" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Decomposed prompting: A modular approach for solving complex tasks</title>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02406</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A path towards autonomous machine intelligence version 0.9</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2022" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3045" to="3059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive nlp tasks</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9459" to="9474" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feifan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongbin</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.08244</idno>
		<title level="m">Api-bank: A benchmark for tool-augmented llms</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pre-trained language models for interactive decision-making</title>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Paxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clinton</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linxi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekin</forename><surname>Aky?rek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="31199" to="31212" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4582" to="4597" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Yaobo</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenshan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoguang</forename><surname>Mao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.16434</idno>
		<title level="m">Completing tasks by connecting foundation models with millions of apis</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Llm+ p: Empowering large language models with optimal planning proficiency</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqian</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joydeep</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Stone</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.11477</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixuan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weng</forename><surname>Lam Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07602</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Chameleon: Plug-and-play compositional reasoning with large language models</title>
		<author>
			<persName><forename type="first">Pan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.09842</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Qing</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shreya</forename><surname>Havaldar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Delip</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marianna</forename><surname>Apidianaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.13379</idno>
		<title level="m">Faithful chain-of-thought reasoning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Augmented language models: a survey</title>
		<author>
			<persName><forename type="first">Gr?goire</forename><surname>Mialon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Dess?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Lomeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoforos</forename><surname>Nalmpantis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ram</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberta</forename><surname>Raileanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Baptiste Rozi?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asli</forename><surname>Dwivedi-Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Celikyilmaz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.07842</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchir</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Saunders</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09332</idno>
		<title level="m">Browser-assisted question-answering with human feedback</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">OpenAI. Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Batu</forename><surname>Ozturkler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Malkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nebojsa</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName><surname>Thinksum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.01293</idno>
		<title level="m">Probabilistic reasoning over sets using large language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Automatic multi-step reasoning and tool-use for large language models</title>
		<author>
			<persName><forename type="first">Bhargavi</forename><surname>Paranjape</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tulio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ribeiro</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2303.09014</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<pubPlace>Art</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Fiedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.12255</idno>
		<title level="m">Talm: Tool augmented language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01066</idno>
		<title level="m">Language models as knowledge bases? arXiv preprint</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Virtualhome: Simulating household activities via programs</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Ra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marko</forename><surname>Boben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaman</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingwu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8494" to="8502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Tool learning with foundation models</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weize</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheni</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaojun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><forename type="middle">Ren</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huadong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runchu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunlun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bokai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yining</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenning</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya-Ting</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Han</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongshuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<idno>ArXiv, abs/2304.08354</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Zhiyuan Liu, and Maosong Sun</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Sentence-bert: Sentence embeddings using siamese bertnetworks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Recipes for building an open-domain chatbot</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">Michael</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter</title>
		<meeting>the 16th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics: Main Volume</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="300" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Dwivedi-Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Dess?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberta</forename><surname>Raileanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Lomeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Cancedda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Scialom</surname></persName>
		</author>
		<author>
			<persName><surname>Toolformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.04761</idno>
		<title level="m">Language models can teach themselves to use tools</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Yongliang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><surname>Hugginggpt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.17580</idno>
		<title level="m">Solving ai tasks with chatgpt and its friends in huggingface</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Retrieval augmentation reduces hallucination in conversation</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spencer</forename><surname>Poff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3784" to="3803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Progprompt: Generating situated robot task plans using large language models</title>
		<author>
			<persName><forename type="first">Ishika</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valts</forename><surname>Blukis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Garg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.11302</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The web as a knowledge-base for answering complex questions</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="641" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08239</idno>
		<title level="m">Language models for dialog applications</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timoth?e</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozi?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Self-instruct: Aligning language model with self generated instructions</title>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeganeh</forename><surname>Kordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alisa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.10560</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Multitask prompt tuning enables parameter-efficient transfer learning</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Chain of thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11903</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">Yaqi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongyao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbin</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ze</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harold</forename><surname>Soh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.05128</idno>
		<title level="m">Translating natural language to planning goals with large-language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.04129</idno>
		<title level="m">Pieter Abbeel, and Dale Schuurmans. Foundation models for decision making: Problems, methods, and opportunities</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.03629</idno>
		<title level="m">React: Synergizing reasoning and acting in language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A survey of knowledge-enhanced text generation</title>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaitang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11s</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Ben Zaken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
