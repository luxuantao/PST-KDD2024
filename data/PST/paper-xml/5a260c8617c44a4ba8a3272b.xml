<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lin</forename><surname>Wu</surname></persName>
							<email>lin.wu@uq.edu.au</email>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Xuemin</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Junbin</forename><surname>Gao</surname></persName>
							<email>junbin.gao@sydney.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Lin</forename><forename type="middle">Wu</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<postCode>116024</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of New South Wales</orgName>
								<address>
									<postCode>2052</postCode>
									<settlement>Sydney</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Information Technology and Electrical Engineering</orgName>
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<postCode>4072</postCode>
									<settlement>Brisbane</settlement>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The University of New South Wales</orgName>
								<address>
									<postCode>2052</postCode>
									<settlement>Sydney</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Sydney Business School</orgName>
								<orgName type="institution" key="instit1">The University</orgName>
								<orgName type="institution" key="instit2">The University of Sydney</orgName>
								<address>
									<postCode>2006</postCode>
									<settlement>Sydney</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7159D1EB2F25ADB5BF3D2F54599268EB</idno>
					<idno type="DOI">10.1109/TNNLS.2017.2777489</idno>
					<note type="submission">received August 4, 2016; revised April 10, 2017 and August 27, 2017; accepted November 6, 2017.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Iterative multiview clustering agreement</term>
					<term>lowrank matrix factorization</term>
					<term>low-rank representation (LRR)</term>
					<term>multiview spectral clustering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multiview data clustering attracts more attention than their single-view counterparts due to the fact that leveraging multiple independent and complementary information from multiview feature spaces outperforms the single one. Multiview spectral clustering aims at yielding the data partition agreement over their local manifold structures by seeking eigenvalueeigenvector decompositions. Among all the methods, low-rank representation (LRR) is effective, by exploring the multiview consensus structures beyond the low rankness to boost the clustering performance. However, as we observed, such classical paradigm still suffers from the following stand-out limitations for multiview spectral clustering of overlooking the flexible local manifold structure, caused by aggressively enforcing the lowrank data correlation agreement among all views, and such a strategy, therefore, cannot achieve the satisfied between-views agreement; worse still, LRR is not intuitively flexible to capture the latent data clustering structures. In this paper, first, we present the structured LRR by factorizing into the latent lowdimensional data-cluster representations, which characterize the data clustering structure for each view. Upon such representation, second, the Laplacian regularizer is imposed to be capable of preserving the flexible local manifold structure for each view. Third, we present an iterative multiview agreement strategy by minimizing the divergence objective among all factorized latent data-cluster representations during each iteration of optimization process, where such latent representation from each view serves to regulate those from other views, and such an intuitive process iteratively coordinates all views to be agreeable. Fourth, we remark that such data-cluster representation can flexibly encode the data clustering structure from any view with an adaptive input cluster number. To this end, finally, a novel nonconvex objective function is proposed via the efficient alternating minimization strategy. The complexity analysis is also presented. The extensive experiments conducted against the real-world multiview data sets demonstrate the superiority over the state of the arts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S PECTRAL clustering [1]- [4] aims at exploring the local nonlinear manifold (spectral graph) <ref type="foot" target="#foot_0">1</ref> structure <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, attracting great attention within recent years. With the development of information technology, multiview spectral clustering, due to the fact of outperforming the single-view counterparts by leveraging the complementary information from multiview spaces. As implied by multiview research <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b8">[9]</ref>, an individual view is not capable of being faithful for effective multiview learning. Therefore, exploring multiview information is necessary, and has been demonstrated by a wide spectrum of applications, e.g., similarity search <ref type="bibr" target="#b9">[10]</ref> and human action recognition <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>.</p><p>Essentially, given the complementary information from multiviews, the critical issue of multiview clustering is to achieve the multiview clustering agreement/consensus <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> to yield a substantial superior clustering performance over the single-view paradigm. Numerous multiview-based methods are proposed for spectral clustering. References <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b15">[16]</ref> perform multiview information incorporation into the clustering process by optimizing certain objective loss function. Early fusion strategy can also be developed by concatenating the multiview features into a uniform one <ref type="bibr" target="#b16">[17]</ref>, upon which the similarity matrix is calculated for further multiview spectral clustering. As mentioned in <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b18">[19]</ref>, such a strategy will be more likely to destroy the inherent property of original feature representations within each view, hence resulting into a worse performance; worse still, sometimes, as indicated by the experimental reports from our previous research <ref type="bibr" target="#b17">[18]</ref>, it may even be inferior to the clustering performance with a single view. In contrast, late fusion strategy <ref type="bibr" target="#b19">[20]</ref> conducts a spectral clustering performance for each view, and then combining multiple them afterward, which, however, cannot achieve the multiview agreement, without collaborating with each other. Canonical correlation analysis (CCA)-based methods <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> for multiview spectral clustering project the data from multiview feature spaces onto one common lower dimensional subspace, where the spectral clustering is subsequently conducted. One limitation of such a method lies in the fact that such a common lower dimensional subspace cannot flexibly characterize the local manifold structures from heterogeneous views, resulting into an inferior performance. Kumar et al. <ref type="bibr" target="#b22">[23]</ref> proposed a stateof-the-art coregularized spectral clustering for multiview data.</p><p>Similarly, a cotraining <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> model is proposed for this problem <ref type="bibr" target="#b25">[26]</ref>.</p><p>One assumption for the above work <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref> is the scenario with noise corruption free for each view. However, it is not easily met. To this end, low-rank representation (LRR) <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b29">[30]</ref> is proposed. As summarized in <ref type="bibr" target="#b8">[9]</ref>, where the basic idea is to decompose data representation into a view-dependent noise corruption term and a common LRR shared by all views, leading to common data affinity matrix for clustering, the effectiveness of low-rank model also leads to numerous research on multiview subspace learning <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> applied to the pattern recognition field.</p><p>LRR tries a common multiview LRR, but overlooks the distinct manifold structures. To remedy the limitations, inspired by the latest development of graph regularized LRR <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, we recently proposed another iterative views agreement strategy <ref type="bibr" target="#b8">[9]</ref> with a graph regularized LRR for multiview spectral clustering, named LRRGL; to characterize the nonlinear manifold structure from each view, LRRGL couples LRR with multigraph regularization, where each one can characterize the view-dependent nonlinear local data manifold structure <ref type="bibr" target="#b34">[35]</ref>. A novel iterative view agreement process is proposed for optimizing the proposed, where, during each iteration, the LRR yielded from each view serves as the constraint to regulate the representation learning from other views, to achieve the consensus, implemented by applying a linearized alternating direction method with adaptive penalty <ref type="bibr" target="#b35">[36]</ref>.</p><p>Despite the effectiveness of LRRGL, we still identify the following nontrivial observations that are not addressed by LRRGL to obtain the further improvement.</p><p>1) It is less flexible for Z i yielded by low-rank constraint to capture the flexible latent data similarity that can encode the more rich similarity information than Z i over X i , which can be better solved by matrix factorization. 2) LRRGL mainly focused on yielding the low-rank primal data similarity matrix Z i derived from X i . However, such primal Z i is less intuitive to understand and less effective to reveal the ideal data clustering structure for the i th view, as well as multiviews. Hence, it will prevent achieving the better multiview spectral clustering performance. The structured consensus loss term imposed over Z i (i ∈ V ) may not effectively achieve the consensus regarding the multiview spectral clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Our Contributions</head><p>This paper is the extension of our recent work <ref type="bibr" target="#b8">[9]</ref>; upon that, we deliver the following novel contributions to achieve the further improvement over multiview spectral clustering.</p><p>1) Instead of focusing on primal low-rank data similarity matrix Z i such that i = 1, . . . , V , we perform a symmetric matrix factorization over Z i into the datacluster indicator matrix, so that such latent factorization provides the better chance to preserve the ideal cluster structure besides flexible manifold structure for each view. 2) We impose the Laplacian regularizer over factorized data-clustered representation to further characterize Fig. <ref type="figure">1</ref>. Visualization results of the multiview (please refer to Section IV for specific multiview features) affinity matrix between ours and LRRGL over NUS data. The more whiter the diagonal blocks, the more ideally the cluster is to characterize the data objects within the larger similarity; meanwhile, the more blacker the nondiagonal blocks, the more reasonable the nonsimilarity data objects are unlikely to cluster together. For such a result, we can see that the diagonal blocks from the third to the eight of our method are more whiter than those of LRRGL, leading to the result that the surrounding black nondiagonal blocks of our method are more salient than those of LRRGL, which demonstrate the advantages of our method via a latent factorized data-cluster representation over LRRGL.</p><p>the nonlinear local manifold structure for each view.</p><p>We remark that the factorized data-cluster matrix can effectively encode the clustering structure, and we provide an example to illustrate this in Fig. <ref type="figure">1</ref>. To reach the multiview clustering agreement, we set the same clustering number for all views to the data-clustering representation for all views. 3) We impose the consensus loss term to minimize the divergence among all the latent data-cluster matrices instead of Z i to achieve the multiview spectral clustering agreement. 4) To implement all the above insights, we propose a novel objective function and an efficient alternating optimization strategy together with the complexity analysis to solve the objective function. Moreover, we deliver the intuitions of iterative multiview agreement over the factorized latent data-cluster representation during each iteration of our optimization strategy that will eventually lead to the multiview clustering agreement. 5) Extensive experiments over real-world multiview data sets demonstrate the advantages of our technique over the state of the arts, including our recently proposed LRRGL <ref type="bibr" target="#b8">[9]</ref>. Recently, another elegant graph-based principal component analysis method <ref type="bibr" target="#b36">[37]</ref> is proposed for spectral clustering with out-of-sample case. Unlike this effective technique, we study the multiview case to address the effective consensus for spectral clustering. We summarize the main notations in Table <ref type="table">I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. STRUCTURED LOW-RANK MATRIX FACTORIZATION TO SPECTRAL CLUSTERING</head><p>We get started from each single view, e.g., the i th view as min</p><formula xml:id="formula_0">Z i ,E i θ 2 ||X i -X i Z i -E i || 2 F + ||Z i || * + β||E i || 1<label>(1)</label></formula><p>where θ and β are the tradeoff parameters, and as aforementioned, we always adopt D i to be X i , so that X i can be decomposed as clean component X i Z i and another corrupted </p><formula xml:id="formula_1">||Z i || * = min U i ,V i ,Z i =U i V T i 1 2 ||U i || 2 F + ||V i || 2 F (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where U i ∈ R n×d and V i ∈ R n×d , and d is always less than d i , since high-dimensional data objects always characterize the low-rank structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Notes Regarding U i and V i for Multiview Spectral Clustering</head><p>Before further discussing the low-rank matrix factorization, one may consider the following notes that the factorized U i and V i may need to satisfy in the context of both the within-view data structure preserving and multiview spectral clustering agreement.</p><p>1) The low-rank data structure should be characterized by the factorized U i or V i for the i th view, especially to characterize the underlying data clustering structure. 2) Thefactorized latent factors should well encode the manifold structure for the i th view, which, as previously mentioned, is critical to the spectral clustering performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Either the row-based matrix U i</head><p>or column-based matrix V i is considered to meet the above two notes? if so, which one? One may claim both to be considered, which, however, may inevitably raise more parameters to be tuned. 4) Not only the factorized latent low-dimensional factors, e.g., U i or V i , should meet the above notes within each view, e.g., the i th view, but also need the same scale to unify all views to reach possible agreement. To address all the above notes, in what follows, we will present our technique of data-cluster-based structured low-rank matrix factorization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data-Cluster (Landmark)-Based Structured Low-Rank Matrix Factorization</head><p>We aim at factorizing Z i as an approximate symmetric lowrank data-cluster matrix to minimize the reconstruction error</p><formula xml:id="formula_3">min Z i ||X i -X i Z i || 2 F (3)</formula><p>where we assume the rank of Z i is k i , such that k i is related to the data cluster number for the i th view. As indicated in <ref type="bibr" target="#b29">[30]</ref>, minimizing (3) is equivalent to finding the optimal rank k i approximation relying on the skinny singular value decomposition of X i = V U T to yield the following optimal solution:</p><formula xml:id="formula_4">Z * i = U i U T i (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where U i ∈ R n×k i , such that k i denotes the top k i principle basis of X i . Here, we follow the assumption in <ref type="bibr" target="#b38">[39]</ref> to see k i as the cluster number of data objects within the i th view, and the data-cluster symmetric matrix factorization has been widely adopted by the numerous existing research, including semisupervised learning <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, metric fusion <ref type="bibr" target="#b18">[19]</ref>, and clustering <ref type="bibr" target="#b2">[3]</ref>. We aim at solving the following equivalent lowrank minimization over Z i via the clustered symmetric matrix factorization as:</p><formula xml:id="formula_6">||Z i || * = min U i ,Z i =U i U T i ||U i || 2 F (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>where we often minimize the following for derivative convenience with respect to U i :</p><formula xml:id="formula_8">||Z i || * = min U i ,Z i =U i U T i 1 2 ||U i || 2 F . (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>Remark: Following (3) and (4), we initialize</p><formula xml:id="formula_10">U i ∈ R n×k i via a k-means clustering over X i and normalize U i ( j, k) = (1/|C k |) provided X i (•, j ), i.e.</formula><p>, the j th data object is assigned to C k . By such normalization, all the columns of U i are orthonormal; moreover, they are within the same magnitude so as to perform the agreement minimization. Furthermore, such factorization can well address the aforementioned challenges, and it is worthwhile to summarize them as follows.</p><p>1) The data cluster structure can be well encoded by such low-rank data-cluster representation within each view. The setting U i = V i can avoid the more parameters and importance weight discussion provided U i = V i . 2) More importantly, inspired by the reasonable assumption hold by all the multiview clustering research <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref>. As indicated in <ref type="bibr" target="#b17">[18]</ref>, the ideal multiview clustering performance is that the common underlying data clustering structure is shared by all the views; we naturally set all the U i values with the same size by adopting the same value for</p><formula xml:id="formula_11">k i = d(i = 1, . . . , V ), i.e.</formula><p>, the clustering number, upon the same data object number n for all views, so that the feasible loss functions can be developed to seek the multiview clustering agreement with the same clustering number for all views. For spectral clustering from each view, we preserve the nonlinear local manifold structure of X i via such low-rank data-cluster representation U i for the i th view, which can be formulated as</p><formula xml:id="formula_12">1 2 n j,k ||u i j -u i k || 2 2 W i ( j, k) = N j =1 u i j T u i j H i ( j, j ) - N j,k u i k T u i j W i ( j, k) = Tr U T i H i U i -Tr U T i W i U i = Tr U T i L i U i (<label>7</label></formula><formula xml:id="formula_13">)</formula><p>where u i k ∈ R d is the kth row vector of U i ∈ R n×d representing the linear correlation between x k and x j ( j = k) in the i th view, W i ( j, k) encodes the similarity between x j and x k for the i th view, H i is a diagonal matrix with its kth diagonal entry to be the summation of the kth row of W i , and L i = H i -W i is the graph Laplacian matrix for the i th view.</p><p>Following <ref type="bibr" target="#b8">[9]</ref>, we choose Gaussian kernel to define W i j k :</p><formula xml:id="formula_14">W i ( j, k) = e - ||x i j -x i k || 2 2 2σ 2 . (<label>8</label></formula><formula xml:id="formula_15">)</formula><p>We aim to minimize the difference of low-rank based data-cluster representations for all views via a mutual consensus loss function term to coordinate all views to reach clustering agreement while structuring such representation with Laplacian regularizer to encode the local manifold structure for each view.</p><p>Unlike the traditional LRR to achieve the common data similarity by all views, we propose to learn a variety of factorized low-rank data-cluster representations for different views to preserve the flexible local manifold structure while achieving the data cluster structure for each view; upon that, the consensus loss term is imposed to achieve the multiview consensus, leading to our iterative views agreement in Section II-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Objective Function With Structured Low-Rank Matrix Factorized Representation</head><p>We propose the objective function with structured LRR U i for each view, e.g., the i th view with factorized low-rank via (6) data-clustered representation via <ref type="bibr" target="#b2">(3)</ref>. Then, we have the following:</p><formula xml:id="formula_16">min U i ,E i (i∈V ) i∈V ⎛ ⎜ ⎜ ⎝ 1 2 ||U i || 2 F minimize ||Z i || * via Eq.(6) + λ 1 ||E i || 1</formula><p>noise and corruption robustness</p><formula xml:id="formula_17">+ λ 2 Tr U T i L i U i Graph Structured Regularization + β 2 j ∈V , j =i ||U i -U j || 2 F ⎞ ⎟ ⎟ ⎠ Views-agreement s.t. i = 1, . . . , V, X i = X i U i U T i + E i , U i ≥ 0 ( 9 )</formula><p>where it has the following.</p><p>1) U i ∈ R n×d denotes the factorized low-rank data-cluster representation of X i for the i th view. Tr(U T i L i U i ) makes U i to be structured with local manifold structure for the i th view. ||E i || 1 is responsible for possible noise with X i . λ 1 , λ 2 , and β are all tradeoff parameters. 2) One reasonable assumption hold by a lot of multiview clustering research <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b41">[42]</ref> is that all the views should share the similar underlying clustering structure. i, j ∈V ||U i -U j || 2 F aims to achieve the view agreement regarding the factorized LRRs U i from all |V | views; unlike the traditional LRR method to enforce an identical representation, we construct different values of U i for each view, and then further minimize their divergence to generate a view agreement.</p><p>3 <ref type="formula">9</ref>) is nonconvex; we, hence, alternately optimize each variable while fixing the others, that is, updating all the U i and E i (i ∈ {1, . . . , V }) values in an alternative way until the convergence is reached. As solving all the {U i , E i }(i ∈ V ) pairs shares the similar optimization strategy, only the i th view is presented. To this end, we introduce two auxiliary variables D i and G i , then solving <ref type="bibr" target="#b8">(9)</ref> with respect to U i , E i , D i , and G i that can be written as follows:</p><formula xml:id="formula_18">) U i ≥ 0 is a nonnegative constraint, through X i = X i Z i + E i = X i U i U T i + E i for the i th view. Equation (</formula><formula xml:id="formula_19">min U i ,E i ,D i ,G i 1 2 ||U i || 2 F + λ 1 ||E i || 1 + λ 2 Tr(U i L i U T i ) + β 2 j ∈V , j =i ||U i -U j || 2 F s.t. X i = D i U T i + E i , D i = X i U i , G i = U i , G i ≥ 0 (<label>10</label></formula><formula xml:id="formula_20">)</formula><p>where D i ∈ R d i ×d . We will show the intuition for the auxiliary variable relationship D i = X i U i by introducing the augmented Lagrangian function based on (10) as follows:</p><formula xml:id="formula_21">L U i , E i , D i , G i , K i 1 , K i 2 , K i 3 = 1 2 ||U i || 2 F + λ 1 ||E i || 1 + λ 2 Tr U T i L i U i + β 2 j ∈V , j =i ||U i -U j || 2 F + K i 1 , X i -D i U T i -E i + K i 2 , U i -G i + K i 3 , D i -X i U i + μ 2 X i -D i U T i -E i 2 F + ||U i -G i || 2 F + ||D i -X i U i || 2 F (<label>11</label></formula><formula xml:id="formula_22">)</formula><p>where</p><formula xml:id="formula_23">K i 1 ∈ R d i ×n , K i 2 ∈ R n×d , and K i 3 ∈ R d i ×d</formula><p>are Lagrange multipliers, and μ &gt; 0 is a penalty parameter.</p><p>From <ref type="bibr" target="#b10">(11)</ref>, we can easily show the intuition on</p><formula xml:id="formula_24">D i = X i U i , that is, minimizing ||X i -D i U T i -E i || 2</formula><p>F with respect to D i is similar as dictionary learning, while pop out U T i as corresponding representations learning, and both of them reconstruct X i for the i th view. Besides the above intuition, it is quite simple to optimize only single U T i by merging the other into D i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OPTIMIZATION STRATEGY</head><p>We minimize <ref type="bibr" target="#b10">(11)</ref> by updating each variable while fixing the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Solve U i</head><p>Minimizing U i is to resolve</p><formula xml:id="formula_25">L 1 = 1 2 ||U i || 2 F + λ 2 Tr U T i L i U i + β 2 j ∈V , j =i ||U i -U j || 2 F + K i 1 , X i -D i U T i -E i + K i 2 , U i -G i + K i 3 , D i -X i U i + μ 2 ||X i -D i U T i -E i || 2 F + ||U i -G i || 2 F +||D i -X i U i || 2 F . (<label>12</label></formula><formula xml:id="formula_26">)</formula><p>We set the derivative of <ref type="bibr" target="#b11">(12)</ref> with respect to U i to be the zero matrix, which yields the following equation:</p><formula xml:id="formula_27">∂L 1 ∂U i = U i + 2λ 2 L i U i + β j ∈V , j =i (U i -U j ) -K i 1 T D i + K i 2 -X T i K i 3 + μU i D T i D i + μE T i D i + μ (U i -G i ) -μX T i X i U i = 0<label>(13)</label></formula><p>where 0 ∈ R n×d shares the same size as U i . Rearranging the other terms further yields the following:</p><formula xml:id="formula_28">U i = 2λ 2 L i + (1 + β(|V | -1) + μ)I n -μX T i X i -1 with computational complexity O(n 3 ) S (<label>14</label></formula><formula xml:id="formula_29">)</formula><formula xml:id="formula_30">S = j ∈V , j =i U j + K i 1 T -μU i D T i -μE T i D i + X T i K i 3 + μX T i X i U i . (<label>15</label></formula><formula xml:id="formula_31">)</formula><p>The bottleneck of computing <ref type="bibr" target="#b13">(14)</ref> lies in the inverse matrix computation over the matrix of the size R n×n causing the computational complexity O(n 3 ), which is computationally prohibitive provided that n is large. Therefore, we turn to update each row of U i ; without loss of generality, we present the derivative with respect to U i (l, •) as</p><formula xml:id="formula_32">U i (l, •) + U i (l, •) n k=1 (2λ 2 L i (k, l) -μ (X T i X i )(k, l)) + K i 1 T (l, •)D i + μU i (l, •)D T i D i + K i 2 (l, •) -X T i (l, •)K i 3 + β j ∈V , j =i (U i (l, •) -U j (l, •)) + μ U i (l, •) + E T i (l, •)D i -G i (l, •) = 0 (<label>16</label></formula><formula xml:id="formula_33">)</formula><p>where 0 ∈ R d denotes the vector of the size d with all entries to be 0 and U i (l, •) ∈ R d represents the lth row of U i . We rearrange the terms to yield the following: where</p><formula xml:id="formula_34">T l i = X T i (l, •)K i 3 + μ G i (l, •) -E T i (l, •)D i -K i 2 (l, •) -K i 1 T (l, •)D i I d ∈ R d×d is the identity matrix.</formula><p>1) Complexity Discussion for the Row Updating Strategy for U i : Unlike the closed form regarding U i , it is apparent that the major computational complexity lies in the inverse matrix computation over the size of R d×d , which leads to O(d 3 ) according to <ref type="bibr" target="#b16">(17)</ref>, as shown at the top of the next page, which is much smaller than O(n 3 ). Besides, as d is set as the cluster number across all views, moreover, aforementioned, it should be less than the inherent rank of X i and, hence, a small value. Upon the above facts, it is tremendously efficient via O(d 3 ) to sequentially update each row of U i .</p><p>2) Intuitions for Views Agreement: The iterative views clustering agreement can be immediately captured via the terms underlined in <ref type="bibr" target="#b16">(17)</ref>. Specifically, during each iteration, U i (l, •) is updated via the influence from others view, while served as the constraint to generate U j (l, •)( j = i ), the divergence among all U i (l, •) is decreased gradually toward an agreement for all views, and such a process repeats until the convergence is reached.</p><p>Unlike the existing LRR method by directly imposing the common representation, our iterative multiview agreement can better preserve the flexible manifold structure for each view and, meanwhile, achieve the multiview agreement, which will be critical to finalize multiview spectral clustering.</p><p>Remark: After the whole U i is updated for the i th view, we simply perform a K -means clustering over it to assign each data object to one cluster exclusively, and then normalized each column of U i to form an orthonormal matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Solve D i</head><p>The optimization process regarding D i is equivalent to the following:</p><formula xml:id="formula_35">min D i K i i , X i -D i U T i -E i + K i 3 , D i -X i U i + μ 2 ||X i -D i U i -E i || 2 F + ||D i -X i U i || 2 F . (<label>18</label></formula><formula xml:id="formula_36">)</formula><p>We get the derivative with respect to D i , and then, it yields the following closed-form updating rule:</p><formula xml:id="formula_37">D i = K i 1 U i -K i 3 + μ(2X i -E i )U i I d + U T i U i -1 μ (<label>19</label></formula><formula xml:id="formula_38">)</formula><p>where the major computational complexity lies in the inverse computation over matrix 3 ), as aforementioned, that is the same as updating each row of U i , and hence quite efficient.</p><formula xml:id="formula_39">(I d + U T i U i ) ∈ R d×d , resulting into O(d</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Solve E i</head><p>It is equivalent to solving the following:</p><formula xml:id="formula_40">min E i λ 1 ||E i || 1 + μ 2 ||E i -X i -D i U T i + 1 μ K i 1 || 2 F (<label>20</label></formula><formula xml:id="formula_41">)</formula><p>where the following closed-form solution can be yielded for E i according to <ref type="bibr" target="#b42">[43]</ref>:</p><formula xml:id="formula_42">E i = S λ 1 μ X i -D i U T i + 1 μ K i 1 . (<label>21</label></formula><formula xml:id="formula_43">) U i (l, •) = ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ T l i + β j =i, j ∈V U j (l, •) Influences from other views ⎞ ⎟ ⎟ ⎟ ⎟ ⎠ × 1 + μ + n k=1 (2λ 2 L i (k, l) -μ X T i X i (k, l)) I d + D T i D i -1 with computational complexity O(d 3 ) (17)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Solve G i</head><p>It is equivalent for the following:</p><formula xml:id="formula_44">K i 2 , U i -G i + μ 2 ||G i -U i || 2 F (<label>22</label></formula><formula xml:id="formula_45">)</formula><p>where the following closed-form solution of G i can be derived as:</p><formula xml:id="formula_46">G i = U i + K i 2 μ . (<label>23</label></formula><formula xml:id="formula_47">)</formula><formula xml:id="formula_48">E. Updating K i 1 , K i 2 , K i 3 , and μ We update Lagrange multipliers K i 1 , K i 2 , and K i 3 via K i 1 = K i 1 + μ(X i -D i U i -E i ) (<label>24</label></formula><formula xml:id="formula_49">)</formula><formula xml:id="formula_50">K i 2 = K i 2 + μ(U i -G i ) (<label>25</label></formula><formula xml:id="formula_51">)</formula><formula xml:id="formula_52">K i 3 = K i 3 + μ(D i -X i U i ). (<label>26</label></formula><formula xml:id="formula_53">)</formula><p>Following <ref type="bibr" target="#b8">[9]</ref>, μ is tuned using the adaptive updating strategy <ref type="bibr" target="#b35">[36]</ref> to yield a faster convergence. The optimization strategy alternatively updates each variable while fixing others until the convergence, which is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Notes Regarding Algorithm 1</head><p>It is worthwhile to highlight some critical notes regarding Algorithm 1 in the following. 1) We initialize U i [0] ∈ R n×d for all views, such that each entry of U i [0] represents similarity between each data object and one of the d anchors (cluster representatives), which can be seen as the centers from the clusters generated from the k-means or spectral clustering. 2) For our initialization, we adopt the spectral clustering outcome with the clustering number to be d, where the similarity matrix is calculated via the original X i feature representation within each view, and then, the U i [0](i, j ) entry, i.e., the similarity between the i th data object and the j th anchor is yielded via <ref type="bibr" target="#b7">(8)</ref>. The Laplacian matrix L i (i = 1, . . . , V ) is computed once offline also within the original X i feature representation. 3) More importantly, we set the identical value of d(the cluster number) to the column size of</p><formula xml:id="formula_54">U i [0](i = 1, ••, V ) ∈ R n×d</formula><p>for all the views. We remark that the above initial setting for U i [0] with the same d is reasonable, and as stated before, all the views should share the similar underlying data clustering structure. This fact also implies that the initialized U i [0] is reasonably not divergent a lot among all views.</p><p>Algorithm 1 Alternating Optimization Strategy for ( <ref type="formula">9</ref>)</p><formula xml:id="formula_55">Input: X i (i = 1, . . . , V ), d, λ 1 , λ 2 , β Output: U i , D i , E i , G i (i ∈ V ) Initialize: U i [0], L i (i = 1, . . . , V ) computation, set all entries of K i 1 [0], G i [0], K i 2 [0] to be 0, initialize E i [0]</formula><p>with sparse noise as 20% entries corrupted with uniformly distributed noise over <ref type="bibr">[-5,5]</ref>,</p><formula xml:id="formula_56">μ[0] = 10 -3 , 1 = 10 -3 , 2 = 10 -1 k = 0 for i ∈ V do Solve U i :</formula><p>Sequentially update each row of U i according to Eq.( <ref type="formula">17</ref>). Orthonormalized each column of U i . Update E i :</p><formula xml:id="formula_57">E i [k + 1] = S λ 1 μ[k] (X i -D i U T i [k] + 1 μ[k] K i 1 [k]) Update G i : G i [k + 1] = U i [k] + K i 2 [k] μ[k] Update K i 1 , K i 2 , K i 3 and μ: K i 1 [k + 1] = K i 1 [k] + μ(X i -D i U T i [k] -E i [k]) K i 2 [k + 1] = K i 2 [k] + μ(U i [k] -G i [k]) K i 3 [k + 1] = K i 3 [k] + μ(D i [k] -X i U i [k]) Update μ according to [36] whether converged if ||X i -D i U T i [k + 1] -E i [k + 1]||/||X i || &lt; 1 and max{ξ ||U i [k + 1] -U i [k]||, μ[k]||G i [k + 1] - G i [k]||, μ[k]||E i [k + 1] -E i [k]||} &lt; 2 then</formula><p>Remove the i th view from the view set as</p><formula xml:id="formula_58">V = V -i U i [N] = U i [k + 1], s.t. N is any positive integer. else k = k + 1 Return U i [k + 1], D i [k + 1], E i [k + 1], G i [k + 1] (i = 1, . . . , V )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Convergence Discussion</head><p>Often, the above alternating minimization strategy can be seen as the coordinate descent method. According to <ref type="bibr" target="#b43">[44]</ref>, the above-mentioned sequences (U i , D i , E i , G i ) will eventually converge to a stationary point. However, we are not sure whether the converged stationary point is a global optimum, as it is not jointly convex to all the above-mentioned variables. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Clustering</head><p>Following <ref type="bibr" target="#b8">[9]</ref>, once the converged U i (i = 1, . . . , V ) values are ready, all column vectors of U i (i = 1, . . . , V ), while set small entries under given threshold τ to be 0. Afterward, the similarity matrix for the i th view between the j th and kth data objects as</p><formula xml:id="formula_59">W i ( j, k) = U i U T i ( j, k). (<label>27</label></formula><formula xml:id="formula_60">)</formula><p>Following <ref type="bibr" target="#b8">[9]</ref>, the final data similarity matrix can be defined as:</p><formula xml:id="formula_61">W = V i W i |V | . (<label>28</label></formula><formula xml:id="formula_62">)</formula><p>The clustering is carried out against W via (28) to yield final outcome of d data groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We adopt the data sets mentioned in <ref type="bibr" target="#b8">[9]</ref> as follows. 1) UCI Handwritten Digit set<ref type="foot" target="#foot_1">2</ref> : It consists of features for hand-written digits (0-9) with 6 features, and contains 2000 samples with 200 in each category. Analogous to <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b35">[36]</ref>, we choose two views as 76 Fourier coefficients of the character shapes and the 216 profile correlations. . Following <ref type="bibr" target="#b8">[9]</ref>, 80 images for each category and get 4000 images in total. 3) NUS-WIDE-Object (NUS) <ref type="bibr" target="#b44">[45]</ref>: 30000 images from 31 categories. Five views are adopted using five features as provided by the website<ref type="foot" target="#foot_3">4</ref> : 65-D color histogram (CH), 226-D color moments, 145-D color correlation (CORR), 74-D edge estimation (EDH), and 129-D wavelet texture. 4) PASCAL VOC 2012<ref type="foot" target="#foot_4">5</ref> : We select 20 categories with 11 530 images, and two views are constructed with color features (1500-D) and HOG features (250-D). Among them, 5600 images are selected by removing the images with multiple categories. We summarize the above throughout in Table <ref type="table">II</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Animal With Attribute</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Baselines</head><p>The following state-of-the-art baselines used in <ref type="bibr" target="#b8">[9]</ref> are compared.</p><p>1) MFMSC: Concatenating multifeatures to be the multiview representation for similarity matrix, the spectral clustering is then conducted <ref type="bibr" target="#b16">[17]</ref>. 2) Multifeature representation similarity aggregation for spectral clustering (MAASC) <ref type="bibr" target="#b14">[15]</ref>. 3) Canonical Correlation Analysis Model <ref type="bibr" target="#b21">[22]</ref>: Projecting multiview data into a common subspace, then perform spectral clustering. 4) Coregularized Multiview Spectral Clustering <ref type="bibr" target="#b22">[23]</ref>: It regularizes the eigenvectors of view-dependent graph laplacians and achieve consensus clusters across views. 5) Cotraining <ref type="bibr" target="#b25">[26]</ref>: Alternately, modify one view's Laplacian eigenspace by learning from the other views 's eigenspace, and the spectral clustering is then conducted. 6) Robust LRR method (RLRR) <ref type="bibr" target="#b26">[27]</ref>, after obtaining the data similarity matrix, upon which, the spectral clustering is performed to be the final multi-view spectral clustering result. 7) LRRGL <ref type="bibr" target="#b8">[9]</ref> regularizer over the nonfactorized LRRs, with each of which corresponds to one view to preserve the individual manifold structure, while iteratively boost all these LRRs to reach agreement. The final multiview spectral clustering is performed upon the similarity representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Settings and Parameters Study</head><p>We implement these competitors under the experimental setting as mentioned in <ref type="bibr" target="#b8">[9]</ref>. Following <ref type="bibr" target="#b8">[9]</ref>, σ in ( <ref type="formula" target="#formula_14">8</ref>) is learned via <ref type="bibr" target="#b1">[2]</ref>, and s = 20 to construct s-nearest neighbors for <ref type="bibr" target="#b7">(8)</ref>. We adopt two standard metrics: clustering accuracy (ACC) and normalized mutual information (NMI) as the metric defined as</p><formula xml:id="formula_63">ACC = n i=1 δ(map(r i ), l i ) n (<label>29</label></formula><formula xml:id="formula_64">)</formula><p>where r i denotes the cluster label of x i l i denotes the true class label, n is the total number of images, δ(x, y) is the function that equals one if x = y and equals zero otherwise, and map(r i ) is the permutation mapping function that maps each cluster label r i to the equivalent label from the database. Meanwhile, the NMI is formulated as follows:</p><formula xml:id="formula_65">NMI = c i=1 c j =1 n i, j log n i, j n i n j c i=1 n i log n i n c j =1 n j log n j n (<label>30</label></formula><formula xml:id="formula_66">)</formula><p>where n i is the sample number in cluster C i (1 ≤ i ≤ c), n j is the sample number from class L j (1 ≤ j ≤ c), and n i, j denotes the sample number in the intersection between C i and L j . Remark: Following <ref type="bibr" target="#b8">[9]</ref>, we repeated the running ten times, and their averaged mean value for multiview spectral clustering for all methods is reported. For each method including ours, we input the clustering number as the number of groundtruth classes from all data sets.</p><p>Feature Noise Modeling for Robustness: Following [9] and <ref type="bibr" target="#b45">[46]</ref>, 20% feature elements are corrupted with uniform distribution over the range <ref type="bibr">[5, -5]</ref>, which is consistent to the practical setting while matching with LRRGL, RLRR, and our method.</p><p>Following <ref type="bibr" target="#b8">[9]</ref>, we set λ 1 = 2 in (9) for sparse noise term. We test ACC and NMI over a different value of λ 2 and β in ( <ref type="formula">9</ref>) in Section IV-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Validation Over Factorized Low-Rank Latent Data-Cluster Representation</head><p>First, we would like to validate our method regarding the multigraph regularization and iterative views agreement over factorized latent data-cluster representation.</p><p>Following <ref type="bibr" target="#b8">[9]</ref>, we test λ 2 and β within the interval [0.001, 10], with one parameter while fixing the value of the other parameter, and the ACC results are shown in Fig. <ref type="figure" target="#fig_1">2</ref>, where we have the following.</p><p>1) Increasing β will improve the performance, and vice versa, that is, increasing λ 2 will improve the performance.</p><p>2) The clustering metric ACC increases when both λ 2 and β increase. Based on the above, we choose a balance pair values: λ 2 = 0.7 and β = 0.2 for our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results</head><p>According to Tables III and IV, the following identification can be drawn, and note that we mainly deliver the analysis between our method and LRRGL, as the analysis over other competitors has been detailed in <ref type="bibr" target="#b8">[9]</ref>.</p><p>1) First,our method outperforms LRRGL, implying the effectiveness of the factorized latent data-cluster representation, as it can better encode the data-cluster representation for each view as well as all views.</p><p>We provide more insights about that in Fig. <ref type="figure" target="#fig_0">3</ref>. 2) Second, both our method and LRRGL outperform the model of learning a common low-dimensional subspace among multiview data, as indicated in <ref type="bibr" target="#b8">[9]</ref>, and it is incapable of encoding local graph structures within a single subspace. 3) Our method and LRRGL are more effective under noise corruptions than other methods. More analysis can be referred to our conference version <ref type="bibr" target="#b8">[9]</ref>. Our method achieves the best performance over PASCAL VOC 2012 under the selected two views via the tuned the parameters. We present Fig. <ref type="figure" target="#fig_0">3</ref> to show more intuitions on why our method with the multiview affinity matrix yielded from factorized data-cluster representation outperforms the primal similarity matrix for LRRGL. For example, it has the following.</p><p>1) For UCI data set, i.e., the multiview affinity matrix illustrated in Fig. <ref type="figure" target="#fig_0">3</ref>(a) and (d), we can see that both the fourth and fifth diagonal blocks of our method in Fig. <ref type="figure" target="#fig_0">3</ref>(d) are more whiter than those of LRRGL illustrated in Fig. <ref type="figure" target="#fig_0">3</ref>(a); meanwhile, the surrounding nondiagonal black blocks, e.g., (4, 5)th and (5, 4)th, are more black than those of LRRGL. 2) For Animal with Attribute (AwA) data set, the diagonal blocks of our method from the second to the sixth are whiter than those of LRRGL, leading to a slight deeper black color over the surrounding nondiagonal blocks than LRRGL.</p><p>3) The similar conclusions also hold for the NUS data set.</p><p>We can see that the diagonal blocks from the third to the eight of our method are more whiter than those of Fig. <ref type="figure" target="#fig_0">3</ref>. Recovered multiview-based consensus affinity matrix over both our proposed method and LRRGL on three multiview data sets with noise corruption. For UCI digit data set, we plot the affinity matrix over all data samples. For AwA and NUS data sets, we randomly select 10 classes, where 80 samples are randomly selected for each of them. The ten diagonal block represents the data samples within the ten clusters with respect to ground-truth classes, where more white the color is, the ideally larger affinity value will be to better reveal the data samples clusters within the same classes. Meanwhile, for nondiagonal blocks, the more black the color is, the ideally smaller affinity to reveal the data samples within different clusters.</p><p>LRRGL, leading to the result that the surrounding black nondiagonal blocks of our method are more salient than those of LRRGL. From the above observations, we can safely infer the advantages of the affinity matrix representation yielded by our factorized latent data-cluster representation over the primal affinity matrix of LRRGL for multiview spectral clustering.</p><p>V. CONCLUSION In this paper, we propose to learn a clustered LRR via structured matrix factorization for multiview spectral clustering. Unlike the existing methods, we propose an iterative strategy of intuitively achieving the multiview spectral clustering agreement by minimizing the between-view divergences in terms of the factorized latent data-clustered representation for each view. Upon that, we impose the graph Laplacian regularizer over such low-dimensional data-cluster representation, so as to adapt to the multiview spectral clustering, as demonstrated by the extensive experiments.</p><p>The future work includes the following directions. The graph regularized low-rank embedding out-of-sample case has been researched <ref type="bibr" target="#b3">[4]</ref>, and will be applied for multiview outof-sample scenario. Unlike the predefined graph similarity value, inspired by <ref type="bibr" target="#b46">[47]</ref>, we will simultaneously learn and achieve the consensus graph clustering result and graph structure, i.e., graph similarity. Besides, the latest nonparametric graph construction model <ref type="bibr" target="#b47">[48]</ref> will also be incorporated for multiview spectral clustering. The practice of our method can be improved by reducing the tuned parameters further. Upon that, we will also investigate the problem of learning the weight <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref> for each view. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 :</head><label>3</label><figDesc>It consists of 50 kinds of animals described by 6 features (views): color histogram (CQ, 2688-D), local self-similarity (2000-D), pyramid histogram of oriented gradients (HOG) (252-D), scale-invariant feature transform (SIFT) (2000-D), color SIFT (RGSIFT, 2000-D), and speed up robust features (2000-D)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Study over λ 2 and β over latent factorized data-cluster representation on three data sets.</figDesc><graphic coords="8,229.33,58.16,151.52,137.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Yang</head><label></label><figDesc>Wang received the Ph.D. degree from The University of New South Wales, Sydney, NSW, Australia, in 2015. He is currently a Research Fellow with The University of New South Wales. He has authored 40 research papers together with a book chapter, most of which have appeared at the competitive venues, including the IEEE TRANSACTIONS ON IMAGE PROCESSING (TIP), the IEEE TRANSAC-TIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS (TNNLS), the IEEE TRANSACTIONS ON CYBERNETICS, the Pattern Recognition, the Very Large Data Bases Journal, the ACM Multimedia, the ACM Special Interest Group on Information Retrieval, the International Joint Conference on Artificial Intelligence, the ACM Conference on Information and Knowledge Management, the IEEE International Conference on Data Mining, and the Knowledge and Information Systems. His current research interests include data mining and learning over visual data objects from multiview spaces. Dr. Wang served as the Program Committee Member for the Scientific Research Track of European Conference on Machine Learning &amp; Principles and Practice of Knowledge Discovery in Databases (ECMLPKDD) 2014, ECMLPKDD 2015, Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD) 2018, PAKDD 2017, International Symposium on Methodologies for Intelligent Systems 2017, Asian Conference on Machine Learning 2016, and Australian Database Conference 2016. He was a recipient of the Best Research Paper Runner-up Award for PAKDD 2014. He is the Program Co-Chair for Big Data Analytics for Social Computing in conjunction with PAKDD 2018, Melbourne. He regularly served as an Invited Journal Reviewer for more than ten leading journals, such as the IEEE TIP, the IEEE TNNLS, the IEEE TKDE, and the Machine Learning (Springer). He is serving as a Guest Editor for the Pattern Recognition Letters (Elsevier), the Multimedia Tools and Application (Springer), and the Advances in multimedia.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II DATA SETS</head><label>IISETS</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III ACC RESULTS TABLE IV NMI</head><label>IIIRESULTSIV</label><figDesc>RESULTS</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In the rest of this paper, we will alternatively use nonlinear manifold structure or spectral graph structure.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://archive.ics.uci.edu/ml/datasets/Multiple+Features.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://attributes.kyb.tuebingen.mpg.de.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>lms.comp.nus.edu.sg/research/NUS-WIDE.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>http://host.robots.ox.ac.uk/pascal/VOC/voc2012/.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self-tuning spectral clustering</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1601" to="1608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large scale spectral clustering via landmarkbased sparse representation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1669" to="1680" />
			<date type="published" when="2015-08">Aug. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral embedded clustering: A framework for in-sample and out-of-sample spectral clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1796" to="1808" />
			<date type="published" when="2011-11">Nov. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative embedded clustering: A framework for grouping high-dimensional data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1287" to="1299" />
			<date type="published" when="2015-06">Jun. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Effective discriminative feature selection with nontrivial solution</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="796" to="808" />
			<date type="published" when="2016-04">Apr. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-view intact space learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2531" to="2544" />
			<date type="published" when="2015-12">Dec. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A survey on multi-view learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1304.5634" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Iterative views agreement: An iterative low-rank based structured optimization method to multi-view spectral clustering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th Int. Joint Conf</title>
		<meeting>25th Int. Joint Conf<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2153" to="2159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multiview alignment hashing for efficient image search</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="956" to="966" />
			<date type="published" when="2015-03">Mar. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A multigraph representation for improved unsupervised/semi-supervised learning of human actions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
			<biblScope unit="page" from="820" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Kernelized multiview projection for robust action recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="129" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Group sparse multiview patch alignment framework with view consistency for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3126" to="3137" />
			<date type="published" when="2014-07">Jul. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-view subspace clustering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
			<biblScope unit="page" from="4238" to="4246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Affinity aggregation for spectral clustering</title>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="773" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-view clustering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scheffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Data Mining</title>
		<meeting>IEEE Int. Conf. Data Mining</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image retrieval via probabilistic hypergraph ranking</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="3376" to="3383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust subspace clustering for multi-view data by exploiting correlation consensus</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3939" to="3949" />
			<date type="published" when="2015-11">Nov. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised metric fusion over multiview data by graph random walk-based crossview diffusion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="70" />
			<date type="published" when="2017-01">Jan. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A matrix factorization approach for integrating multiple data views</title>
		<author>
			<persName><forename type="first">D</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Joint Eur. Conf</title>
		<meeting>Joint Eur. Conf</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="423" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Correlational spectral clustering</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-view clustering via canonical correlation analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th Int. Conf. Mach. Learn</title>
		<meeting>26th Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Co-regularized multi-view spectral clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th Annu. Conf. Neural Inf</title>
		<meeting>25th Annu. Conf. Neural Inf</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1413" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Annu. Conf. Comput. Learn. Theory</title>
		<meeting>11th Annu. Conf. Comput. Learn. Theory</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A new analysis of co-training</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 27th Int. Conf. Mach. Learn</title>
		<meeting>27th Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1135" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A co-training approach for multiview spectral clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName><surname>Iii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th Int. Conf. Mach. Learn</title>
		<meeting>28th Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="393" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust multi-view spectral clustering via low-rank and sparse decomposition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th AAAI Conf</title>
		<meeting>28th AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2149" to="2155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust subspace segmentation by lowrank representation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 27th Int. Conf. Mach. Learn</title>
		<meeting>27th Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="663" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Latent low-rank representation for subspace segmentation and feature extraction</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1615" to="1622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust recovery of subspace structures by low-rank representation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="171" to="184" />
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robust multi-view subspace learning through dual low-rank decompositions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th AAAI Conf</title>
		<meeting>13th AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1181" to="1187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Low-rank common subspace for multi-view learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th IEEE Int. Conf. Data Mining</title>
		<meeting>14th IEEE Int. Conf. Data Mining</meeting>
		<imprint>
			<date type="published" when="2014-12">Dec. 2014</date>
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Laplacian regularized low-rank representation and its applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="504" to="517" />
			<date type="published" when="2016-03">Mar. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dual graph regularized latent low-rank representation for subspace clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4918" to="4933" />
			<date type="published" when="2015-12">Dec. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Localitypreserving low-rank representation for graph construction from nonlinear manifolds</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">175</biblScope>
			<biblScope unit="page" from="715" to="722" />
			<date type="published" when="2016-01">Jan. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Linearized alternating direction method with adaptive penalty for low-rank representation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th Annu. Conf. Neural Inf</title>
		<meeting>25th Annu. Conf. Neural Inf</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="612" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A graph regularized dimension reduction method for out-of-sample data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">225</biblScope>
			<biblScope unit="page" from="58" to="63" />
			<date type="published" when="2017-02">Feb. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fazel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Parrilo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="471" to="501" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Symmetric nonnegative matrix factorization for graph clustering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SDM</title>
		<meeting>SDM</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="106" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scalable semi-supervised learning by efficient anchor graph regularization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1864" to="1877" />
			<date type="published" when="2016-07">Jul. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficient manifold ranking for image retrieval</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 34th Int</title>
		<meeting>34th Int</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="525" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-view clustering via joint nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SDM</title>
		<meeting>SDM</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="252" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A singular value thresholding algorithm for matrix completion</title>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1956" to="1982" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Nonlinear Programming</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Athena Scientific</publisher>
			<pubPlace>Belmont, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">NUS-WIDE: A real-world Web image database from National University of Singapore</title>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th ACM Int. Conf. Image Video Retr</title>
		<meeting>8th ACM Int. Conf. Image Video Retr</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A supervised lowrank method for learning invariant subspace</title>
		<author>
			<persName><forename type="first">F</forename><surname>Siyahjani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Almohsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sabri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
			<biblScope unit="page" from="4220" to="4228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-view clustering and semisupervised classification with adaptive neighbours</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2408" to="2414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The constrained Laplacian rank algorithm for graph-based clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1969" to="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Parameter-free auto-weighted multiple graph learning: A framework for multiview clustering and semi-supervised classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1881" to="1887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multi-view K -means clustering on big data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2598" to="2604" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
