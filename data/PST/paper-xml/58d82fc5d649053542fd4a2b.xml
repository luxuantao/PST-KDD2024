<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Utility-Driven Adaptive Preprocessing for Screen Content Video Compression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shiqi</forename><surname>Wang</surname></persName>
							<email>wangshiqi@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="laboratory">(ROSE) Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinfeng</forename><surname>Zhang</surname></persName>
							<email>xfzhang@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="laboratory">(ROSE) Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xianming</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">(ROSE) Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin In-stitute of Technology</orgName>
								<address>
									<postCode>150001</postCode>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">(ROSE) Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Institute of Digital Media</orgName>
								<orgName type="department" key="dep2">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siwei</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">(ROSE) Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Institute of Digital Media</orgName>
								<orgName type="department" key="dep2">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Wen</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">(ROSE) Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Institute of Digital Media</orgName>
								<orgName type="department" key="dep2">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Utility-Driven Adaptive Preprocessing for Screen Content Video Compression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D64CC4CF55D001C72F841EBE5F51F02F</idno>
					<idno type="DOI">10.1109/TMM.2016.2625276</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2016.2625276, IEEE Transactions on Multimedia 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Screen content video</term>
					<term>block type identification</term>
					<term>temporal masking</term>
					<term>utility information</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we propose a utility-driven preprocessing technique for high efficiency screen content video (SCV) compression based on the temporal masking effect, which was found to be a fundamental attribute that plays an important role in human visual perception of video quality, but has not been fully exploited in the context of SCV coding. Specifically, we investigate the temporal masking effect from the perspective of perceived utility, which allows us to preserve the quality of the high utility content and substitute the low utility regions with the corresponding smooth version. To distinguish the regional utilities, a specifically designed block type identification algorithm for screen content is employed to measure the local properties. Subsequently, the Gaussian filter is applied to smooth out the high frequency components in the detected low utility regions to save consumption bits. Validations based on subjective testings show that the proposed approach is capable of achieving significant bitrate savings with little sacrifice on the final utility compared with the conventional SCV coding scheme.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>R ECENT years have witnessed dramatically increased in- terest and demand for the mobile computer devices, such as laptops, tablets, PDAs, smartphones. Due to the constraints imposed by the local computing capacity and data resources on such devices, many remote computing and virtualization scenarios have emerged. The purpose of these applications is to access the remote data and control the computational resources via the networks <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>. In these scenarios, remote computing can be achieved by the users' interactions with the local interface. The content of such interface is usually generated by the computer, and can be regarded to be a kind of time-varying screen content video (SCV) composed of both computer generated textual/graphical and natural images. In such an environment, there is considerable concern regarding how the captured SCVs can be efficiently delivered.</p><p>Compared with natural videos, SCVs exhibit distinguished properties in both spatial and temporal domains. In spatial domain, the noise free screen content frames are usually featured with repeated patterns, thin lines, limited colors and large smooth areas <ref type="bibr" target="#b4">[5]</ref>. In temporal domain, various types of motion, such as long range, irregular and global motions, are usually involved in a typical SCV. In addition to the conventional hybrid video coding scheme <ref type="bibr" target="#b5">[6]</ref> and advanced super-resolution based image/video compression approaches with superior coding performance <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b8">[9]</ref>, these distinguished properties have motivated numerous specifically developed SCV coding techniques, many of which have been adopted into the screen content coding (SCC) extension to the High Efficiency Video Coding (HEVC) standard <ref type="bibr" target="#b9">[10]</ref>. For example, the residuals of the textual content are usually sparse and sometimes contain sharp directional edges, which may not follow the vertical or horizontal Discrete Cosine Transform (DCT) directions. In view of this, strategies of skipping the transform process were proposed <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>. Moreover, it is observed that the screen content typically contains a limited number of distinct colors, inspired by which color table/palette method has been widely investigated <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b13">[14]</ref>. The intra motion compensation approach was also proposed to remove the redundancy from the repeated patterns occurred in one frame <ref type="bibr" target="#b14">[15]</ref>. Recently, to further reduce the redundancy of the prediction residuals in different color components, the adaptive color-space transform technique was subsequently adopted <ref type="bibr" target="#b15">[16]</ref>. In temporal domain, inspired by the observation that motion in screen content is usually based on full-pel resolution, adaptive motion vector resolution (AMVR) scheme was proposed in <ref type="bibr" target="#b16">[17]</ref> to adapt the resolutions of motion vectors between full-and sub-pel. Moreover, hash based block matching techniques for text/graphics have been developed for intra and inter block search <ref type="bibr" target="#b17">[18]</ref>.</p><p>In addition to these advanced coding techniques, perceptually relevant properties of screen content image (SCI) have also been extensively studied in the literature. In <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref>, various screen quality assessment algorithms were developed based on the SCI quality assessment database in <ref type="bibr" target="#b20">[21]</ref>. Yet, these methods only focus on perceptual quality for an individual image, and the substantial difference between SCI and SCV lies in the visual sensitivity exploration in the temporal domain. For natural video coding, the temporal masking effects have been widely exploited to improve the coding efficiency <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref>. However, most of them are designed and validated on natural videos, which may not always share the same properties of SCVs.</p><p>As widely hypothesized in computational vision science, the major task of the human visual system (HVS) when viewing an image is to act as an optimal information extractor <ref type="bibr" target="#b24">[25]</ref>. This motivates us to study the temporal masking properties of SCVs from the perspective of perceived utility, as high utility corresponds to high information content that needs to be extracted through the interactive screen-remoting mechanism. However, with numerous work proposed to evaluate the utility of natural image <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b27">[28]</ref>, much less work has been dedicated to SCVs. Based on the philosophy of the image utility assessment, appropriately injected distortions can be tolerated in lower utility regions as long as the underlying task is reliably performed, as the HVS will regard the usefulness of the image as a surrogate for a reference <ref type="bibr" target="#b26">[27]</ref>. Regarding SCV, the utility can be characterized by the temporal stability in screen content refreshing. For instance, the fading in/out between two slides or the zooming in/out operations may produce low utility regions, as the major task of these frames is to guarantee a smooth transition during content refreshing.</p><p>As such, a smooth version of such content can provide a utility equivalent SCV.</p><p>In this paper, we propose an utility-driven adaptive preprocessing approach for SCV compression. The approach adaptively identifies and processes the low utility content using Gaussian low-pass filtering. As such, higher coding efficiency is ensured by effectively reducing the coding bitrate of the low utility regions. The proposed scheme applies on the captured SCV such that the SCV generation process is not affected. Experimental results show that the proposed scheme can significantly save the bitrate in transmitting the SCVs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. CHARACTERISTICS OF SCREEN CONTENT VIDEO</head><p>In this section, we analyze the characteristics of the SCV, especially focusing on the video content fluctuations along the temporal direction. Specifically, four video sequences (Parkrun, Johnny, Map and Slideshow) are employed for investigation. The resolution of them is 1280×720 and 300 frames are used for testing. Among them, Map and Slideshow are typical SCVs, Johnny is the natural video with static background, and Parkrun is the natural video with global motion.</p><p>The mean square error (MSE) between two adjacent frames is firstly computed to demonstrate the content variations. This is a simple but effective measure that can well reflect the frame difference caused by any motion or scene change. The variations of frame difference are demonstrated in Fig. <ref type="figure">1</ref>, from which we can observe that there are two poles for SCVs. At one extreme, adjacent screen frames remain quite stable, leading to approximate zero difference. At the other extreme, the irregular and global motions caused by zooming in/out, flipping over and dragging etc. may produce extraordinarily large difference. However, as the content of natural video evolves over time, the frame differences of natural videos are usually larger than zero and exhibit smooth variations over a period of time.</p><p>Furthermore, we compress these sequences using HEVC codecs to investigate the content variations in terms of the frame level coding bits. In particular, the natural videos are compressed with HEVC main profile (HM16.2) and the SCVs are compressed with the SCC extension of HEVC (HM16.2-SCM3.1), respectively. The configuration is Low Delay and the quantization parameter (QP) is set to be 24 for all the sequences. The coding bit as a function of frame index serves as another indicator to reflect the residual energy and motion activities, and their variations are illustrated in Fig. <ref type="figure">2</ref>. Since the hierarchical bit allocation structure (rate-GoP) is employed in HEVC <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, each frame will be coded with different QPs and Lagrangian multipliers according to the corresponding layer index, leading to periodic coding bits variations within every four frames. We can observe that there exist significant coding bits variations for SCVs, which originate from the SCV content fluctuations with irregular motions. However, such large variations of coding bits may bring challenges in the deployment of SCV compression, as many SCV applications are constrained by limited buffer space and bandwidth. These observations demonstrate the potentials and necessities of reducing the coding bits in those frames with irregular motions, and suggest us to exploit the perceptual redundancy to improve the SCV coding efficiency.</p><p>1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SCREEN CONTENT VIDEO PREPROCESSING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Motivation</head><p>The motivation behind the SCV preprocessing is to generate the utility equivalent frames that consume less bits than the directly recorded ones. In human-computer interaction, the time variant interface is usually subject to various operations, such as zooming in/out, fading in/out, windows moving, and page scrolling to accomplish the goal of screen sharing. These operations usually create large content variations, and some of them cannot be efficiently coded as there does not exist an appropriate reference for prediction. However, the central role of such unstable content is ensuring smooth transitions rather than providing useful information, as the high utility content usually remains steady in several consecutive frames for the purpose of information extraction. As a result, the capability of blurring such low utility content for compression allows one to save bitrate at the current moment of low utility content, and reserve bandwidth and buffer capacities for future frames that desire more bitrates to maintain the SCV quality.</p><p>Generally speaking, the low utility content can be a whole frame, or only regions within the SCV. To identify such content, the spatial and temporal local properties of each frame should firstly be accessed to distinguish the block type. Subsequently, we perform an object level region detection to locate the low utility content. Finally, a Gaussian smooth filter is applied to generate the utility equivalent SCV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Block Type Identification</head><p>The local statistical properties of the SCV are captured by means of block type identification. Specifically, each frame is divided into non-overlapping 16×16 blocks, which are further categorized into one of three types: skip, text, and natural image <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b30">[31]</ref>. We firstly introduce the detection algorithms for skip and text, and then the remaining blocks are classified as natural image blocks.</p><p>1) Skip Block: SCVs often exhibit high temporal redundancy in terms of motion compensation. Therefore, detecting the skip block is one essential procedure in block type identification <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. One extreme case is that there is a high probability of fixed blocks with no scene change. In addition, scrolling up/down and moving a window typically produce content updates, which exhibits large areas of global motion <ref type="bibr" target="#b32">[33]</ref>. As such, skip blocks are further classified into fixed blocks and global motion blocks. Fixed block is easy to examine by comparing the current block with the co-located block in the adjacent frames, and if there is no difference, the block is identified as fixed. To detect the blocks that are subjected to global motion, we employ the global motion detection technique that makes use of feature comparison to locate the global motion areas <ref type="bibr" target="#b33">[34]</ref>. In this manner, the long range motions that frequently occur in SCV can be efficiently estimated. It is also worth mentioning that the future frames will also be used for detecting the skip block to infer its temporal properties, which may introduce a slight delay in this process.</p><p>2) Text Block: Motivated by the observation that text blocks are usually characterized by sharp edges, we firstly employ a binary feature derived from the number of high gradient pixels in an M × N block for high gradient block type classification <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b34">[35]</ref>,</p><formula xml:id="formula_0">ζ = u M -1 k=1 N -1 l=1 γ k,l -T HF ,<label>(1)</label></formula><p>where T HF represents a certain threshold of high gradient pixels and function u is defined as a step function as follows,</p><formula xml:id="formula_1">u(x) = 1 x &gt; 0 0 otherwise . (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>The parameter γ k,l is a binary value indicating whether the (i, j)-th pixel is a high gradient pixel</p><formula xml:id="formula_3">γ k,l = u(|I k,l -I k-1,l | &gt; I th ∨ |I k,l -I k,l-1 | &gt; I th ),<label>(3)</label></formula><p>where I th is the pre-defined threshold and I k,l represents the luma samples at the location (k, l). However, in a typical SCV frame, the high gradient block can be either a text block, or an edge block from natural image regions. To further differentiate the pictorial and textual content, the limited color is employed as another distinguished feature of text block <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b34">[35]</ref>. Specifically, the text block can be represented by a few number of colors and their corresponding positions, which are also known as base color and index map. Such unique feature has been widely employed in screen content compression and processing <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. One typical example is illustrated in Fig. <ref type="figure" target="#fig_1">3</ref>, in which only limited number of sample values exists in the text block. As such, the text block can be represented using their index map, as shown in Fig. <ref type="figure" target="#fig_1">3 (c</ref>). To effectively identify these base colors, we build the pixel value histogram where a majority of pixels are converged to a small number of colors, as illustrated in Fig. <ref type="figure" target="#fig_1">3 (d)</ref>. These colors are treated as base colors and the number of base colors is limited to four. Equal size windows around the base colors are used to range the sample values and those that cannot be represented by base colors are identified as escape colors. When the number of escape colors is smaller than a certain threshold, suggesting that limit colors suffice to represent the block, the current block is categorized into text block. Otherwise, it is treated as a natural image block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Low Utility Region Detection</head><p>Given the block types, the low utility regions are adaptively detected for each frame. First of all, skip blocks should not be included in the low utility region for further processing, since such blocks may easily get referenced by the subsequent frames or inherited from the previous frames. In view of this, including skip blocks in the low utility region may break the temporal consistency and decrease the coding efficiency. Regarding the remaining text and natural image blocks, we locate the low utility regions using a probabilistic strategy.</p><p>Let the binary value m (m ∈ {1, 0}) denote whether the current block is included in the low utility region or not. Given the (i, j)-th block in the t-th frame, the posterior distribution of m i,j,t can be predicted with the Bayes' theorem,</p><formula xml:id="formula_4">p(m i,j,t |f i,j,t ) = p(f i,j,t |m i,j,t )p(m i,j,t ) p(f i,j,t ) ,<label>(4)</label></formula><p>where f i,j,t ∈ {1, 0} indicates the block type, and 1 denotes text and 0 denotes natural image, respectively. Therefore, the maximum a posteriori (MAP) estimation of m i,j,t is given by mi,j,t = argmax p(m i,j,t |f i,j,t ) = argmax p(f i,j,t |m i,j,t )p(m i,j,t ).</p><p>The likelihood function can be rewritten as p(f i,j,t |m i,j,t ) = p(f i,j,t |1) mi,j,t p(f i,j,t |0) 1-mi,j,t .</p><p>Inspired by the pairwise interaction markov random field, we model the prior distribution p(m i,j,t ) based on the previous t 0 frames in the following form p(m i,j,t ) ∝ exp t0 s=0 β t,t-s mi,j,t=mi,j,t-s ,</p><p>where β t,t-s follows 1-D Gaussian distribution with standard deviation (std) that is empirically chosen as σ = 1.5,</p><formula xml:id="formula_8">β t,t-s = α σ • √ 2π exp(- s 2 2σ 2 ).<label>(8)</label></formula><p>Here the parameter α is used to balance the prior and likelihood. The term mi,j,t=mi,j,t-s is set to one if m i,j,t = m i,j,t-s and zero otherwise. With ( <ref type="formula" target="#formula_6">6</ref>) and ( <ref type="formula" target="#formula_7">7</ref>), the log-posterior distribution of ln p(m i,j,t |f i,j,t ) is computed as follows, ln p(m i,j,t |f i,j,t ) = ln p(f i,j,t |0) + φ i,j,t • m i,j,t + t0 s=0 β t,t-s mi,j,t=mi,j,t-s , <ref type="bibr" target="#b8">(9)</ref> where φ i,j,t denotes the log-likelihood ratio,</p><formula xml:id="formula_9">φ i,j,t = ln[p(f i,j,t |1)/p(f i,j,t |0)].<label>(10)</label></formula><p>In practice, considering that we are specifically interested in the case when m i,j,t = 1, the likelihood is empirically defined as follows,</p><formula xml:id="formula_10">p(1|1) = e 1 + e and p(0|1) = 1 1 + e ,<label>(11)</label></formula><p>where e denotes the base of the natural logarithm. This implies that the text blocks most likely locate in the low utility region, which is in line with the design philosophy of the proposed scheme. The low utility regions in the smooth transitions are mainly composed of unstable text blocks and uniformly flat areas, from which the useful information is difficult to extract. As the uniformly flat areas do not need such preprocessing, only the areas with abundant text blocks are taken into account. Moreover, it is reasonable to assign natural image blocks with much lower probability, which further improves the robustness of the algorithm in the scenario of natural video playing. As a result, the probability is transformed into a weighting factor assigned for each block type, which is given by</p><formula xml:id="formula_11">ρ i,j = 0 skip block p(1|f i,j,t ) otherwise . (<label>12</label></formula><formula xml:id="formula_12">)</formula><p>To finally locate the low utility region, two features representing the horizontal and vertical image block activities, which are obtained by accumulating ρ i,j along the vertical and horizontal directions, are employed as follows,</p><formula xml:id="formula_13">A Hor (i) = H j=1 ρ i,j and A V er (j) = W i=1 ρ i,j ,<label>(13)</label></formula><p>where H and W indicate the height and width of the frame and are measured in terms of a 16×16 block. As illustrated in Fig. <ref type="figure" target="#fig_2">4</ref>, typical SCV frames are locally analyzed and the distributions of the horizontal and vertical activities are depicted.</p><p>In each direction, the blocks that are concentrated in the high value ranges indicate rich text blocks, corresponding to a high possibility of being inside a low utility region. Therefore, a threshold that is dependent on the average activity value along each direction is applied to filter the low utility region. With such a procedure, the bound of the natural video region is adaptively detected by capturing the properties of SCVs at the object level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Low Utility Region Processing</head><p>Blur is essentially a natural effect which was discovered to be highly relevant to the motion in the perception of HVS <ref type="bibr" target="#b36">[37]</ref>. In the literature, various types of blur have been purposely added to the video sequences to enhance the visual experience <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>. However, little has been done in the context of SCV compression. In this work, the circular-symmetric Gaussian filter is applied to process the identified low utility region and smooth out the high frequency information. The reasons of adopting the Gaussian kernel function are manifold. First, as the main task of low utility content is to ensure the smooth transitions in the irregular motions, it is natural to apply the circular-symmetric low-pass filter to further process it. Second, the Gaussian filter is well designed to achieve the redundancy reduction, and has been widely adopted explicitly or implicitly as a HVS channel in preprocessing the signal for similarity comparison <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>. Third, the Gaussian filter is friendly for implementation, enabling its applications in real scenarios <ref type="bibr" target="#b41">[42]</ref>. The decoded frames in Fig. <ref type="figure" target="#fig_2">4</ref> with and without preprocessing are illustrated in Fig. <ref type="figure">5</ref>, from which we can observe that the unstable regions with abundant text will get blurry, such that transitions in SCV playing may become smoother and consume less coding bits simultaneously. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Description Score</head><p>The first one is much worse than the second one -3 The first one is worse than the second one -2 The first one is slightly worse than the second one -1 The first one has the same utility as the second one 0 The first one is slightly better than the second one 1 The first one is better than the second one 2 The first one is much better than the second one 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>In this section, the proposed adaptive preprocessing scheme is implemented and validated in terms of the utility based subjective testing. Specifically, four sequences (SC Map, SC Slideshow, SC Wordediting, and Kimono) are used, which cover common screen-sharing scenarios, such as web browsing, office working (slide and word), and natural video playing. It is worth mentioning that the natural video sequence Kimono is treated as a special SCV to examine the robustness of the scheme in the scenario of full screen natural video playing. As such, it is converted into YUV4:4:4 format as well. The To assess the utility of the preprocessed SCVs, subjective studies were further conducted, in which 14 non-expert subjects (10 males, 4 females) were invited. Specifically, in each trial, a subject is shown a pair of video sequences compressed at the same QP value, and is asked to provide the score based on the guidelines in Table <ref type="table" target="#tab_0">I</ref>. Each pair is played in a random order and the obtained score is further processed, such that the value larger than zero indicates that the preprocessed video with the proposed scheme is inferior to the compared one in terms of utility. In each pair, the videos are played on after the other. The subjects were asked to offer their opinions based on the utility resemblance. In other words, the conveyed information is used as the criterion to evaluate how much information loss has been incurred by the proposed preprocessing scheme. </p><formula xml:id="formula_14">∆R = R anc -R pro R anc .<label>(14)</label></formula><p>To evaluate the utility variation, the average subjective score is calculated, which is denoted to be S avg . The performances with two different Gaussian windows of stds 3.5 and 5.5 are demonstrated. The first row in each Gaussian window corresponds to the subjective tests in the uncompressed case, and the rest rows show the results obtained by using different QPs to compress the SCVs. It is observed that for sequences SC M ap and SC Slideshow, the utility degradations are ignorable while up to 24% and 40% ∆R can be achieved.</p><p>For sequence SC W ordEditing, the results show that the preprocessing may slightly decrease the utility of the SCV, given the fact that the high frequency screen operations may produce some flickering artifacts after preprocessing. For the natural video sequence Kimono, as the proposed scheme makes the sequence untouched, the bitrates are identical and the utility variations can be regarded as the random noise in subjective testing. Furthermore, we compute the score variance at each QP point, and the average value of all QP points for each sequence is demonstrated in Table <ref type="table" target="#tab_1">III</ref>, which suggests that subjects have a relatively good agreement on judging the utility of SCVs. This further confirms the robustness of the proposed scheme. Moreover, the frame level comparisons of the coding bits are shown in Fig. <ref type="figure">6</ref>, which demonstrates that the proposed scheme can achieve significant bitrate savings for the frames with high content variations, such that the bandwidth and buffer capacities for future frames that desire more bitrates can be reserved.</p><p>To further show the advantage of our approach, the proposed scheme is compared with the conventional perceptual video coding algorithm based on divisive normalization <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, which was incorporated into the HEVC SCC extension platform HM16.2-SCM3.1. Subjective testings with the same protocols as in Table II are conducted, and the results are shown in Table <ref type="table" target="#tab_0">IV</ref>. We can observe that compared with the perceptual coding algorithm, our approach can still obtain significant rate reduction with little sacrifice on S avg . This is mainly due to the fact that the proposed preprocessing scheme considers the distinct temporal properties of SCVs, such that the coding performance can be further improved from the perspective of temporal masking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We propose a utility-driven preprocessing technique to save the coding bits of low utility content in screen content video compression. The novelty of this paper lies in identifying and processing the low utility regions to generate a utility equivalent SCV. Unlike the previous perceptual natural video compression schemes that maintain a constant video quality frame by frame, the proposed approach allows to serve users with the time variant interface of large variations in terms of frame-level quality, such that the perceptual redundancies can be further removed by taking the temporal masking into account. Subjective results demonstrate superior performance as compared to HEVC screen content extension by offering significant rate reduction, while keeping the similar level of utility information. Moreover, the proposed scheme also provides useful evidence that the screen content coding performance can be further improved by taking advantages of the meaningful perceptual cues, and opens up new space in regulating the bitrate for practical SCV rate control in real application scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. Variations of the temporal frame difference for SCVs and natural video sequences.</figDesc><graphic coords="2,61.15,298.52,111.59,83.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Demonstration of the limited color representation. (a) Amplified luminance block; (b) Luminance pixel values of (a); (c) The map of major color index; (d) The histogram of pixel value.</figDesc><graphic coords="3,72.37,140.03,73.70,73.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Illustration of the low utility region detection process in SC Wordediting, SC Slideshow and SC Map. Gray: skip block; Red: text block; Blue: natural image block. The heat map is employed to visualize the local vertical and horizontal activities.</figDesc><graphic coords="5,64.75,223.63,108.00,61.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. Comparisons between the anchor and proposed SCV coding schemes of SC Wordediting (288-th), SC Slideshow (150-th) and SC Map (143-th) frames with QP=37. (a)(c)(e): Anchor; (b)(d)(f): Proposed.</figDesc><graphic coords="5,326.32,349.78,109.43,82.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I RATING</head><label>I</label><figDesc>CRITERIONS FOR SUBJECTIVE EVALUATION.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II PERFORMANCE</head><label>II</label><figDesc>OF THE PROPOSED ADAPTIVE PREPROCESSING SCHEME. Specific instructions and examples were given before the test.The performance of the proposed adaptive preprocessing scheme is shown in TableII, where R anc and R pro indicate the bitrate generated by the original and prepropcessed sequences. The bitrate variation ∆R is given by</figDesc><table><row><cell>Gaussian std</cell><cell>QP</cell><cell>Ranc</cell><cell cols="2">SC Map Rpro</cell><cell>∆R</cell><cell>Savg</cell><cell>Ranc</cell><cell cols="2">SC Slideshow Rpro ∆R</cell><cell>Savg</cell><cell>Ranc</cell><cell cols="2">SC Wordediting Rpro ∆R</cell><cell>Savg</cell><cell>Ranc</cell><cell>Kimono Rpro</cell><cell>∆R</cell><cell>Savg</cell></row><row><cell></cell><cell>-</cell><cell>---</cell><cell>---</cell><cell cols="2">---</cell><cell>0.214</cell><cell>---</cell><cell>---</cell><cell>---</cell><cell>0.429</cell><cell>---</cell><cell>---</cell><cell>---</cell><cell>1.000</cell><cell>---</cell><cell>---</cell><cell>---</cell><cell>0.000</cell></row><row><cell></cell><cell>22</cell><cell>1832.68</cell><cell>1427.61</cell><cell cols="2">22.10%</cell><cell>0.143</cell><cell>599.01</cell><cell>377.58</cell><cell>36.97%</cell><cell>0.286</cell><cell>1176.47</cell><cell>985.33</cell><cell>16.25%</cell><cell>1.071</cell><cell>7236.17</cell><cell>7236.17</cell><cell>0.00%</cell><cell>-0.071</cell></row><row><cell>3.5</cell><cell>27 32</cell><cell>1182.27 721.62</cell><cell>981.89 621.85</cell><cell cols="2">16.95% 13.83%</cell><cell>0.286 0.214</cell><cell>345.98 203.93</cell><cell>234.11 149.23</cell><cell>32.33% 26.83%</cell><cell>0.500 0.357</cell><cell>845.60 577.48</cell><cell>717.19 514.31</cell><cell>15.19% 10.94%</cell><cell>0.786 0.857</cell><cell>2873.64 1282.30</cell><cell>2873.64 1282.30</cell><cell>0.00% 0.00%</cell><cell>0.500 0.000</cell></row><row><cell></cell><cell>37</cell><cell>427.71</cell><cell>370.88</cell><cell cols="2">13.29%</cell><cell>0.429</cell><cell>126.92</cell><cell>98.44</cell><cell>22.44%</cell><cell>0.429</cell><cell>400.61</cell><cell>371.63</cell><cell>7.24 %</cell><cell>0.786</cell><cell>595.42</cell><cell>595.42</cell><cell>0.00%</cell><cell>-0.286</cell></row><row><cell></cell><cell>-</cell><cell>---</cell><cell>---</cell><cell cols="2">---</cell><cell>0.286</cell><cell>---</cell><cell>---</cell><cell>---</cell><cell>0.214</cell><cell>---</cell><cell>---</cell><cell>---</cell><cell>1.071</cell><cell>---</cell><cell>---</cell><cell>---</cell><cell>-0.143</cell></row><row><cell></cell><cell>22</cell><cell>1832.68</cell><cell>1381.56</cell><cell cols="2">24.62%</cell><cell>0.357</cell><cell>599.01</cell><cell>357.86</cell><cell>40.26%</cell><cell>0.286</cell><cell>1176.47</cell><cell>932.31</cell><cell>20.75%</cell><cell>1.143</cell><cell>7236.17</cell><cell>7236.17</cell><cell>0.00%</cell><cell>0.357</cell></row><row><cell>5.5</cell><cell>27 32</cell><cell>1182.27 721.62</cell><cell>955.01 607.67</cell><cell cols="2">19.22% 15.79%</cell><cell>0.286 0.214</cell><cell>345.98 203.93</cell><cell>222.51 142.07</cell><cell>35.69% 30.33%</cell><cell>0.286 0.357</cell><cell>845.60 577.48</cell><cell>680.88 489.13</cell><cell>19.48% 15.30%</cell><cell>1.214 1.000</cell><cell>2873.64 1282.30</cell><cell>2873.64 1282.30</cell><cell>0.00% 0.00%</cell><cell>0.143 0.214</cell></row><row><cell></cell><cell>37</cell><cell>427.71</cell><cell>364.61</cell><cell cols="2">14.75%</cell><cell>0.286</cell><cell>126.92</cell><cell>94.10</cell><cell>25.86%</cell><cell>0.500</cell><cell>400.61</cell><cell>356.88</cell><cell>10.92%</cell><cell>1.071</cell><cell>595.42</cell><cell>595.42</cell><cell>0.00%</cell><cell>0.000</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>The authors would like to thank the Associate Editor and anonymous reviewers for their valuable comments that significantly helped us in improving the quality of the paper. 1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2016.2625276, IEEE Transactions on Multimedia</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the National Natural Science Foundation of China under Grant 61322106, Grant 61571017, Grant 61300110 and Grant 61672193, the National Basic Research Program of China (973 Program) under Grant 2015CB351800, which are gratefully acknowledged. S. Wang and X. Zhang are with the Rapid-Rich Object Search</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Virtualized screen: A third element for cloud-mobile convergence</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE MultiMedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4" to="11" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Thinc: a virtual display architecture for thin-client computing</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Baratto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="277" to="290" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Virtual Network Computing (VNC)</title>
		<ptr target="https://www.realvnc.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Remote Desktop Protocol (RDP)</title>
		<author>
			<persName><surname>Microsoft</surname></persName>
		</author>
		<ptr target="https://msdn.microsoft.com/en-us/library/aa383015(v=vs.85).aspx" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Compress compound images in H.264/MPGE-4 AVC by exploiting spatial correlation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="946" to="957" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Overview of the high efficiency video coding (HEVC) standard</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Ohm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1649" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bayesian resolution enhancement of compressed video</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Segall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mateos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="898" to="911" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A MAP approach for joint motion estimation, segmentation, and super resolution</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="479" to="490" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive multiple-frame image super-resolution based on u-curve</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3157" to="3170" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Overview of the emerging HEVC screen content coding extension</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50" to="62" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatial transform skip in the emerging high efficiency video coding standard</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gabriellini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naccari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mrak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="185" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive transform skipping for improved coding of motion compensated residuals</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gabriellini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naccari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mrak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Wallendael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="197" to="208" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving screen content coding in HEVC by transform skipping</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mrak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th European Signal Processing Conference</title>
		<meeting>the 20th European Signal Processing Conference</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1209" to="1213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Screen content coding based on HEVC framework</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1316" to="1326" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">RCE3: Results of test 3.3 on intra motion compensation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Budagavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JCTVC-N0205, 14th Meeting</title>
		<meeting><address><addrLine>Vienna, AT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptive color-space transform for HEVC screen content coding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karczewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Compression Conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptive motion vector resolution for screen content</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JCTVC-S0085, 19th JCT-VC meeting: Strasbourg, FR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hash-based block matching for screen content coding</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="935" to="944" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Perceptual screen content image quality assessment and compression</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1434" to="1438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Saliency-guided quality assessment of screen content images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1098" to="1110" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Perceptual quality assessment of screen content images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4408" to="4421" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SSIM-motivated rate-distortion optimization for video coding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="516" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Perceptually-friendly H.264/AVC video coding based on foveated just-noticeable-distortion model</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="806" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Consistent visual quality control in video coding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Ngan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="975" to="989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Information content weighting for perceptual image quality assessment</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1185" to="1198" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video compression using blur compensation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Budagavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">882</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Natural image utility assessment using image contours</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Rouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Hemami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2217" to="2220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A novel technique to acquire perceived utility scores from textual descriptions of distorted natural images</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Rouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Hemami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2505" to="2508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rate-distortion optimized reference picture management for high efficiency video coding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1844" to="1857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rate-GOP based rate control for high efficiency video coding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of selected topics in signal processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1101" to="1111" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Content-aware layered compound video compression</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Circuits and Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="145" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A high-performanance remote computing platform</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pervasive Computing and Communications</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">High frame rate screen video coding for screen sharing applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2157" to="2160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast motion detection for thin client compression</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">O</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Schauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Compression Conference</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="332" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint chroma downsampling and upsampling for screen content image</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Circuits and Systems for Video Technology</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A low-complexity screen compression scheme for interactive screen sharing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="949" to="960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Perception of motion using blur pattern information in the moderate and high-velocity domains of vision</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Harrington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Harrington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Psychologica</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="227" to="237" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Modeling motion blur in computergenerated images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Potmesil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Chakravarty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="389" to="399" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Image-based motion blur for stop motion animation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 28th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="561" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Retina model inspired image quality assessment</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kaup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Communications and Image Processing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Recursive implementation of gaussian filters with switching and reset hardware</title>
		<author>
			<persName><forename type="first">S</forename><surname>Khorbotly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hassan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Midwest Symposium on Circuits and Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1399" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Common test conditions for screen content coding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rapaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno>JCTVC-S1015</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<pubPlace>Strasbourg, France</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Perceptual video coding based on SSIM-inspired divisive normalization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1418" to="1429" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">SSIM-inspired perceptual video coding for hevc</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="497" to="502" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
