<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Branch Prediction with Perceptrons</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Jiménez</surname></persName>
							<email>djimenez@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Sciences</orgName>
								<orgName type="institution">The University of Texas at Austin Austin</orgName>
								<address>
									<postCode>78712</postCode>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Calvin</forename><surname>Lin</surname></persName>
							<email>lin@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Sciences</orgName>
								<orgName type="institution">The University of Texas at Austin Austin</orgName>
								<address>
									<postCode>78712</postCode>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Branch Prediction with Perceptrons</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a new method for branch prediction. The key idea is to use one of the simplest possible neural networks, the perceptron as an alternative to the commonly used two-bit counters. Our predictor achieves increased accuracy by making use of long branch histories, which are possible because the hardware resources for our method scale linearly with the history length. By contrast, other purely dynamic schemes require exponential resources.</p><p>We describe our design and evaluate it with respect to two well known predictors. We show that for a 4K byte hardware budget our method improves misprediction rates for the SPEC 2000 benchmarks by 10.1% over the gshare predictor. Our experiments also provide a better understanding of the situations in which traditional predictors do and do not perform well. Finally, we describe techniques that allow our complex predictor to operate in one cycle. This structure limits the length of the history register to the logarithm of the number of counters. Our scheme not only uses a more sophisticated prediction mechanism, but it can consider much longer histories than saturating counters. This paper explains why and when our predictor performs well. We show that the neural network we have chosen works well for the class of linearly separable branches, a term we introduce. We also show that programs tend to have many linearly separable branches.</p><p>This paper makes the following contributions:</p><p>¯We introduce the perceptron predictor, the first dynamic predictor to successfully use neural networks, and we show that it is more accurate than existing dynamic global branch predictors. For a 4K byte hardware budget, our predictor improves misprediction rates on the SPEC 2000 integer benchmarks by 10.1%.</p><p>¯We explore the design space for two-level branch predictors based on perceptrons, empirically identifying good values for key parameters.</p><p>¯We provide new insights into the behavior of branches, classifying them as either linearly separable or inseparable. We show that our predictor performs better on linearly separable branches, but worse on linearly inseparable branches. Thus, our predictor is complementary to existing predictors and works well as part of a hybrid predictor.</p><p>¯We explain why perceptron-based predictors introduce interesting new ideas for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural networks</head><p>Artificial neural networks learn to compute a function using example inputs and outputs. Neural networks have been used for a variety of applications, including pattern recognition, classification <ref type="bibr" target="#b7">[8]</ref>, and image understanding <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Static branch prediction with neural networks. Neural networks have been used to perform static branch prediction <ref type="bibr" target="#b2">[3]</ref>, where the likely direction of a branch is predicted at compile-time by supplying program features, such as controlflow and opcode information, as input to a trained neural network. This approach achieves an 80% correct prediction rate, compared to 75% for static heuristics <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>. Static branch prediction performs worse than existing dynamic techniques, but is useful for performing static compiler optimizations.</p><p>Branch prediction and genetic algorithms. Neural networks are part of the field of machine learning, which also includes genetic algorithms. Emer and Gloy use genetic algorithms to "evolve" branch predictors [5], but it is important to note the difference between their work and ours. Their work uses evolution to design more accurate predictors, but the end result is something similar to a highly tuned traditional predictor. We propose putting intelligence in the microarchitecture, so the branch predictor can learn and adapt on-line. In fact, their approach cannot describe our new predictor.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modern computer architectures increasingly rely on speculation to boost instruction-level parallelism. For example, data that is likely to be read in the near future is speculatively prefetched, and predicted values are speculatively used before actual values are available <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24]</ref>. Accurate prediction mechanisms have been the driving force behind these techniques, so increasing the accuracy of predictors increases the performance benefit of speculation. Machine learning techniques offer the possibility of further improving performance by increasing prediction accuracy. In this paper, we show that one machine learning technique can be implemented in hardware to improve branch prediction.</p><p>Branch prediction is an essential part of modern microarchitectures. Rather than stall when a branch is encountered, a pipelined processor uses branch prediction to speculatively fetch and execute instructions along the predicted path. As pipelines deepen and the number of instructions issued per cycle increases, the penalty for a misprediction increases. Recent efforts to improve branch prediction focus primarily on eliminating aliasing in two-level adaptive predictors <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4]</ref>, which occurs when two unrelated branches destructively interfere by using the same prediction resources. We take a different approach-one that is largely orthogonal to previous work-by improving the accuracy of the prediction mechanism itself.</p><p>Our work builds on the observation that all existing twolevel techniques use tables of saturating counters. It's natural to ask whether we can improve accuracy by replacing these counters with neural networks, which provide good predictive capabilities. Since most neural networks would be prohibitively expensive to implement as branch predictors, we explore the use of perceptrons, one of the simplest possible neural networks. Perceptrons are easy to understand, simple to implement, and have several attractive properties that differentiate them from more complex neural networks.</p><p>We propose a two-level scheme that uses fast perceptrons instead of two-bit counters. Ideally, each static branch is allocated its own perceptron to predict its outcome. Traditional two-level adaptive schemes use a pattern history table (PHT) of two-bit saturating counters, indexed by a global history shift register that stores the outcomes of previous branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dynamic Branch Prediction</head><p>Dynamic branch prediction has a rich history in the literature. Recent research focuses on refining the two-level scheme of Yeh and Patt <ref type="bibr" target="#b25">[26]</ref>. In this scheme, a pattern history table (PHT) of two-bit saturating counters is indexed by a combination of branch address and global or per-branch history. The high bit of the counter is taken as the prediction. Once the branch outcome is known, the counter is incremented if the branch is taken, and decremented otherwise. An important problem in two-level predictors is aliasing <ref type="bibr" target="#b19">[20]</ref>, and many of the recently proposed branch predictors seek to reduce the aliasing problem <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4]</ref> but do not change the basic prediction mechanism. Given a generous hardware budget, many of these two-level schemes perform about the same as one another <ref type="bibr" target="#b3">[4]</ref>.</p><p>Most two-level predictors cannot consider long history lengths, which becomes a problem when the distance between correlated branches is longer than the length of a global history shift register <ref type="bibr" target="#b6">[7]</ref>. Even if a PHT scheme could somehow implement longer history lengths, it would not help because longer history lengths require longer training times for these methods <ref type="bibr" target="#b17">[18]</ref>.</p><p>Variable length path branch prediction <ref type="bibr" target="#b22">[23]</ref> is one scheme for considering longer paths. It avoids the PHT capacity problem by computing a hash function of the addresses along the path to the branch. It uses a complex multi-pass profiling and compiler-feedback mechanism that is impractical for a real architecture, but it achieves good performance because of its ability to consider longer histories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Branch Prediction with Perceptrons</head><p>This section provides the background needed to understand our predictor. We describe perceptrons, explain how they can be used in branch prediction, and discuss their strengths and weaknesses. Our method is essentially a two-level predictor, replacing the pattern history table with a table of perceptrons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Why perceptrons?</head><p>Perceptrons are a natural choice for branch prediction because they can be efficiently implemented in hardware. Other forms of neural networks, such as those trained by backpropagation, and other forms of machine learning, such as decision trees, are less attractive because of excessive implementation costs. For this work, we also considered other simple neural architectures, such as ADALINE <ref type="bibr" target="#b24">[25]</ref> and Hebb learning <ref type="bibr" target="#b7">[8]</ref>, but we found that these were less effective than perceptrons (lower hardware efficiency for ADALINE, less accuracy for Hebb).</p><p>One benefit of perceptrons is that by examining their weights, i.e., the correlations that they learn, it is easy to understand the decisions that they make. By contrast, a criticism of many neural networks is that it is difficult or impossible to determine exactly how the neural network is making its decision. Techniques have been proposed to extract rules from neural networks <ref type="bibr" target="#b20">[21]</ref>, but these rules are not always accurate. Perceptrons do not suffer from this opaqueness; the perceptron's decision-making process is easy to understand as the result of a simple mathematical formula. We discuss this property in more detail in Section 5.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">How Perceptrons Work</head><p>The perceptron was introduced in 1962 <ref type="bibr" target="#b18">[19]</ref> as a way to study brain function. We consider the simplest of many types of perceptrons <ref type="bibr" target="#b1">[2]</ref>, a single-layer perceptron consisting of one artificial neuron connecting several input units by weighted edges to one output unit. A perceptron learns a target Boolean function Ø´Ü½ ÜÒµ of Ò inputs. In our case, the Ü are the bits of a global branch history shift register, and the target function predicts whether a particular branch will be taken. Intuitively, a perceptron keeps track of positive and negative correlations between branch outcomes in the global history and the branch being predicted.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows a graphical model of a perceptron. A perceptron is represented by a vector whose elements are the weights. For our purposes, the weights are signed integers. The output is the dot product of the weights vector, Û¼ Ò , and the input vector, Ü½ Ò (Ü¼ is always set to 1, providing a "bias" input). The output Ý of a perceptron is computed as</p><formula xml:id="formula_0">Ý Û¼ • Ò ½ Ü Û</formula><p>The inputs to our perceptrons are bipolar, i.e., each Ü is either -1, meaning not taken or 1, meaning taken. A negative output is interpreted as predict not taken. A non-negative output is interpreted as predict taken. ÛÒ. These products are summed, along with the bias weight Û ¼ , to produce the output value Ý.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Perceptrons</head><p>Once the perceptron output Ý has been computed, the following algorithm is used to train the perceptron. Let Ø be -1 if the branch was not taken, or 1 if it was taken, and let be the threshold, a parameter to the training algorithm used to decide when enough training has been done.</p><formula xml:id="formula_1">if sign´ÝÓÙØµ Ø or ÝÓÙØ then for := 0 to Ò do Û Û • ØÜ end for end if</formula><p>Since Ø and Ü are always either -1 or 1, this algorithm increments the th weight when the branch outcome agrees with Ü , and decrements the weight when it disagrees. Intuitively, when there is mostly agreement, i.e., positive correlation, the weight becomes large. When there is mostly disagreement, i.e., negative correlation, the weight becomes negative with large magnitude. In both cases, the weight has a large influence on the prediction. When there is weak correlation, the weight remains close to 0 and contributes little to the output of the perceptron.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Linear Separability</head><p>A limitation of perceptrons is that they are only capable of learning linearly separable functions <ref type="bibr" target="#b7">[8]</ref>. Imagine the set of all possible inputs to a perceptron as an Ò-dimensional space. The solution to the equation</p><formula xml:id="formula_2">Û¼ • Ò ½ Ü Û ¼</formula><p>is a hyperplane (e.g. a line, if Ò ¾) dividing the space into the set of inputs for which the perceptron will respond false and the set for which the perceptron will respond true <ref type="bibr" target="#b7">[8]</ref>. A Boolean function over variables Ü½ Ò is linearly separable if and only if there exist values for Û¼ Ò such that all of the true instances can be separated from all of the false instances by that hyperplane. Since the output of a perceptron is decided by the above equation, only linearly separable functions can be learned perfectly by perceptrons. For instance, a perceptron can learn the logical AND of two inputs, but not the exclusive-OR, since there is no line separating true instances of the exclusive-OR function from false ones on the Boolean plane.</p><p>As we will show later, many of the functions describing the behavior of branches in programs are linearly separable. Also, since we allow the perceptron to learn over time, it can adapt to the non-linearity introduced by phase transitions in program behavior. A perceptron can still give good predictions when learning a linearly inseparable function, but it will not achieve 100% accuracy. By contrast, two-level PHT schemes like gshare can learn any Boolean function if given enough training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Putting it All Together</head><p>We can use a perceptron to learn correlations between particular branch outcomes in the global history and the behavior of the current branch. These correlations are represented by the weights. The larger the weight, the stronger the correlation, and the more that particular branch in the global history contributes to the prediction of the current branch. The input to the bias weight is always 1, so instead of learning a correlation with a previous branch outcome, the bias weight, Û¼, learns the bias of the branch, independent of the history.</p><p>Figure <ref type="figure">2</ref> shows a block diagram for the perceptron predictor. The processor keeps a table of AE perceptrons in fast SRAM, similar to the table of two-bit counters in other branch prediction schemes. The number of perceptrons, AE , is dictated by the hardware budget and number of weights, which itself is determined by the amount of branch history we keep. Special circuitry computes the value of Ý and performs the training. We discuss this circuitry in Section 6. When the processor encounters a branch in the fetch stage, the following steps are conceptually taken:</p><p>1. The branch address is hashed to produce an index ¾ ¼ AE ½ into the table of perceptrons.</p><p>2. The th perceptron is fetched from the table into a vector register, È¼ Ò , of weights.</p><p>3. The value of Ý is computed as the dot product of È and the global history register.</p><p>4. The branch is predicted not taken when Ý is negative, or taken otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Once the actual outcome of the branch becomes known, the training algorithm uses this outcome and the value of Ý to update the weights in È .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.</head><p>È is written back to the th entry in the table.</p><p>It may appear that prediction is slow because many computations and SRAM transactions take place in steps 1 through 5. However, Section 6 shows that a number of arithmetic and microarchitectural tricks enable a prediction in a single cycle, even for long history lengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Branch Address</head><p>History Register Branch Outcome</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Select</head><p>Entry ¹ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Design Space</head><p>This section explores the design space for perceptron predictors. Given a fixed hardware budget, three parameters need to be tuned to achieve the best performance: the history length, the number of bits used to represent the weights, and the threshold.</p><p>History length. Long history lengths can yield more accurate predictions <ref type="bibr" target="#b6">[7]</ref> but also reduce the number of table entries, thereby increasing aliasing. In our experiments, the best history lengths ranged from 12 to 62, depending on the hardware budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Representation of weights.</head><p>The weights for the perceptron predictor are signed integers. Although many neural networks have floating-point weights, we found that integers are sufficient for our perceptrons, and they simplify the design.</p><p>Threshold. The threshold is a parameter to the perceptron training algorithm that is used to decide whether the predictor needs more training. Because the training algorithm will only change a weight when the magnitude of ÝÓÙØ is less than the threshold , no weight can exceed the value of . Thus, the number of bits needed to represent a weight is one (for the sign bit) plus ÐÓ ¾ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>We use simulations of the SPEC 2000 integer benchmarks to compare the perceptron predictor against two highly regarded techniques from the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Methodology</head><p>Predictors simulated. We compare our new predictor against gshare <ref type="bibr" target="#b16">[17]</ref> and bi-mode <ref type="bibr" target="#b15">[16]</ref>, two of the best purely dynamic global predictors from the branch prediction literature. We also evaluate a hybrid gshare/perceptron predictor that uses a 2K byte choice table and the same choice mechanism as that of the Alpha 21264 <ref type="bibr" target="#b13">[14]</ref>. The goal of our hybrid predictor is to show that because the perceptron has complementary strengths to gshare, a hybrid of the two performs well.</p><p>All of the simulated predictors use only global pattern information, i.e., neither per-branch nor path information is used. Thus, we have not yet compared our hybrid against existing global/per-branch hybrid schemes. Per-branch and path information can yield greater accuracy <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14]</ref>, but our restriction to global information is typical of recent work in branch prediction <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Gathering traces. Our simulations use the instrumented assembly output of the gcc 2.95.1 compiler with optimization flags -O3 -fomit-frame-pointer running on an AMD K6-III under Linux. Each conditional branch instruction is instrumented to make a call to a trace-generating procedure. Branches in libraries or system calls are not profiled. The traces, consisting of branch addresses and outcomes, are fed to a program that simulates the different branch prediction techniques.</p><p>Benchmarks simulated. We use the 12 SPEC 2000 integer benchmarks. All benchmarks are simulated using the SPEC test inputs. For 253.perlbmk, the test run executes perl on many small inputs, so the concatenation of the resulting traces is used. We feed up to 100 million branch traces from each benchmark to our simulation program; this is roughly equivalent to simulating half a billion instructions.</p><p>Tuning the predictors. We use a composite trace of the first 10 million branches of each SPEC 2000 benchmark to tune the parameters of each predictor for a variety of hardware budgets. For gshare and bi-mode, we tune the history lengths by exhaustively trying every possible history length for each hardware budget, keeping the value that gives the best prediction accuracy. For the perceptron predictor, we find, for each history length, the best value of the threshold by using an intelligent search of the space of values, pruning areas of the space that give poor performance. For each hardware budget, we tune the history length by exhaustive search.</p><p>Table <ref type="table" target="#tab_1">1</ref> shows the results of the history length tuning. We find an interesting relationship between history length and threshold: the best threshold for a given history length is always exactly ½ ¿ • ½ . This is because adding another weight to a perceptron increases its average output by some constant, so the threshold must be increased by a constant, yielding a linear relationship between history length and threshold. Since the number of bits needed to represent a perceptron weight is one (for the sign bit) plus ÐÓ ¾ , the number of bits per weight range from 7 (for a history length of 12) to 9 (for a history length of 62).</p><p>Our hybrid gshare/perceptron predictor consists of gshare and perceptron predictor components, along with a mechanism, similar to the one in the Alpha 21264 <ref type="bibr" target="#b13">[14]</ref>, that dynamically chooses between the two using a 2K byte table of twobit saturating counters. Our graphs reflect this added hardware expense. For each hardware budget, we tune the hybrid predictor by examining every combination of table sizes for the gshare and perceptron components and choosing the combination yielding the best performance. In almost every case, the best configuration has resources distributed equally among the two prediction components.</p><p>Estimating area costs. Our hardware budgets do not include the cost of the logic required to do the computation. By examining die photos, we estimate that at the longest history lengths, this cost is approximately the same as that of 1K of SRAM. Using the parameters tuned for the 4K hardware budget, we estimate that the extra hardware will consume about the same logic as 256 bytes of SRAM. Thus, the cost for the computation hardware is small compared to the size of the table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Impact of History Length on Accuracy</head><p>One of the strengths of the perceptron predictor is its ability to consider much longer history lengths than traditional two-level schemes, which helps because highly correlated branches can occur at a large distance from each other <ref type="bibr" target="#b6">[7]</ref>. Any global branch prediction technique that uses a fixed amount of history information will have an optimal history length For a given set of benchmarks. As we can see from Table 1, the perceptron predictor works best with much longer histories than the other two predictors. For example, with a 64K byte hardware budget, gshare works best with a history length of 15, even though the maximum possible length for gshare at 64K is 18. At the same hardware budget, the perceptron predictor works best with a history length of 62.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance</head><p>Figure <ref type="figure" target="#fig_1">3</ref> shows the harmonic mean of prediction rates achieved with increasing hardware budgets on the SPEC 2000 benchmarks. The perceptron predictor's advantage over the PHT methods is largest at a 4K byte hardware budget, where the perceptron predictor has a misprediction rate of 6.89%, an improvement of 10  The perceptron predictor is more accurate than the two PHT methods at all hardware budgets over one kilobyte.</p><p>bimode. For comparison, the bi-mode predictor improves only 2.1% over gshare at the 4K budget. Interestingly, the SPEC 2000 integer benchmarks are, as a whole, easier for branch predictors than the SPEC95 benchmarks, explaining the smaller separation between gshare and bi-mode than observed previously <ref type="bibr" target="#b15">[16]</ref>. Figures <ref type="figure">4 and 5</ref> show the misprediction rates on the SPEC 2000 benchmarks for hardware budgets of 4K and 16K bytes, respectively. The hybrid predictor has no advantage at the 4K budget, since three tables must be squeezed into a small space. At the 16K budget, the hybrid predictor has a slight advantage over the perceptron predictor by itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Training Times</head><p>To compare the training speeds of the three methods, we examine the first 40 times each branch in the 176.gcc benchmark is executed (for those branches executing at least 40 times). Figure <ref type="figure">6</ref> shows the average accuracy of each of the 40 predictions for each of the static branches. The average is weighted by the relative frequencies of each branch. We choose 176.gcc because it has the most static branches of all the SPEC benchmarks.</p><p>The perceptron method learns more quickly the other two. For the perceptron predictor, training time is independent of history length. For techniques such as gshare that index a table of counters, training time depends on the amount of history considered; a longer history may lead to a larger working set of two-bit counters that must be initialized when the predictor is first learning the branch. This effect has a negative impact on prediction rates, and at a certain point, longer histories begin to hurt performance for these schemes <ref type="bibr" target="#b17">[18]</ref>. As we will see in the next section, the perceptron prediction does not have this weakness, as it always does better with a longer history length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Why Does it Do Well?</head><p>We hypothesize that the main advantage of the perceptron predictor is its ability to make use of longer history lengths. Schemes like gshare that use the history register as an index into a table require space exponential in the history length, while the perceptron predictor requires space linear in the history length.</p><p>To provide experimental support for our hypothesis, we simulate gshare and the perceptron predictor at a 512K hardware budget, where the perceptron predictor normally outperforms gshare. However, by only allowing the perceptron predictor to use as many history bits as gshare (18 bits), we find that gshare performs better, with a misprediction rate of 4.83% compared with 5.35% for the perceptron predictor. The inferior performance of this crippled predictor has two likely causes: there is more destructive aliasing with perceptrons because they are larger, and thus fewer, than gshare's two-bit counters, and perceptrons are capable of learning only linearly separable functions of their input, while gshare can potentially learn any Boolean function.</p><p>Figure <ref type="figure">7</ref> shows the result of simulating gshare and the perceptron predictor with varying history lengths on the SPEC 2000 benchmarks. Here, an 8M byte hardware budget is used to allow gshare to consider longer history lengths than usual. As we allow each predictor to consider longer histories, each becomes more accurate until gshare becomes worse and then runs out of bits (since gshare requires resources exponential in the number of history bits), while the perceptron predictor continues to improve. With this unrealistically huge hardware budget, gshare performs best with a history length of 18, where it achieves a misprediction rate of 5.20%. The perceptron predictor is best at a history length of 62, the longest history considered, where it achieves a misprediction rate of 4.64%. History Length vs. Performance. The accuracy of the perceptron predictor improves with history length, while gshare's accuracy bottoms out at 18.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">When Does It Do Well?</head><p>The perceptron predictor does well when the branch being predicted exhibits linearly separable behavior. To define this term, let Ò be the most recent Ò bits of global branch history. For a static branch , there exists a Boolean function ´ Ò µ that best predicts 's behavior. It is this function, , that all branch predictors strive to learn. If is not linearly separable, then gshare may predict better than the perceptron predictor, and we say that such branches are linearly inseparable. We compute</p><p>´ ½¼ µ for each static branch in the first 100 million branches of each benchmark and test for linear separability of the function. (Our algorithm for this test takes time superexponential in Ò, so we are unable to go beyond 10 bits of history or 100 million dynamic branches. We believe these numbers are good estimates for the purpose of this discussion.)</p><p>Figure <ref type="figure">8</ref> shows the misprediction rates for each benchmark for a 512K budget, as well as the percentage of dynamically executed branches that is linearly inseparable. We choose a large hardware budget to minimize the effects of aliasing and to isolate the effects of linear separability. We see that the perceptron predictor performs better than gshare for the benchmarks to the left, which have more linearly separable branches than inseparable branches. Conversely, for all but one of the benchmarks for which there are more linearly inseparable branches, gshare performs better. Note that although the perceptron predictor performs best on linearly separable branches, it still has good performance overall. <ref type="bibr" target="#b24">25</ref>  Some branches require longer histories than others for accurate prediction, and the perceptron predictor often has an advantage for these branches. Figure <ref type="figure" target="#fig_3">9</ref> shows the relationship between this advantage and the required history length, with one curve for linearly separable branches and one for inseparable branches. The Ý axis represents the advantage of our predictor, computed by subtracting the misprediction rate of the perceptron predictor from that of gshare. We sorted all static branches according to their "best" history length, which is represented on the Ü axis. Each data point represents the average misprediction rate of static branches (without regard to execution frequency) that have a given best history length. We use the perceptron predictor in our methodology for finding these best lengths: Using a perceptron trained for each branch, we find the most distant of the three weights with the greatest magnitude. This methodology is motivated by the work of Evers et al, who show that most branches can be predicted by looking at three previous branches <ref type="bibr" target="#b6">[7]</ref>. As the best history length increases, the advantage of the perceptron predictor generally increases as well. We also see that our predictor is more accurate for linearly separable branches. For linearly inseparable branches, our predictor performs generally better when the branches require long histories, while gshare sometimes performs better when branches require short histories. Ü axis, the perceptron predictor is better on average. Below the Ü axis, gshare is better on average. For linearly separable branches, our predictor is on average more accurate than gshare. For inseparable branches, our predictor is sometimes less accurate for branches that require short histories, and it is more accurate on average for branches that require long histories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Additional Advantages of Our Predictor</head><p>Assigning confidence to decisions. Our predictor can provide a confidence-level in its predictions that can be useful in guiding hardware speculation. The output, Ý, of the perceptron predictor is not a Boolean value, but a number that we interpret as taken if Ý ¼. The value of Ý provides important information about the branch since the distance of Ý from 0 is proportional to the certainty that the branch will be taken <ref type="bibr" target="#b11">[12]</ref>. This confidence can be used, for example, to allow a microarchitecture to speculatively execute both branch paths when confidence is low, and to execute only the predicted path when confidence is high. Some branch prediction schemes explicitly compute a confidence in their predictions <ref type="bibr" target="#b10">[11]</ref>, but in our predictor this information comes for free. We have observed experimentally that the probability that a branch will be taken can be accurately estimated as a linear function of the output of the perceptron predictor.</p><p>Analyzing branch behavior with perceptrons. Perceptrons can be used to analyze correlations among branches. The perceptron predictor assigns each bit in the branch history a weight. When a particular bit is strongly correlated with a particular branch outcome, the magnitude of the weight is higher than when there is less or no correlation. Thus, the perceptron predictor learns to recognize the bits in the history of a particular branch that are important for prediction, and it learns to ignore the unimportant bits. This property of the perceptron predictor can be used with profiling to provide feedback for other branch prediction schemes. For example, our methodology in Section 5.6 could be used with a profiler to provide path length information to the variable length path predictor <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Effects of Context Switching</head><p>Branch predictors can suffer a loss in performance after a context switch, having to warm up while relearning patterns <ref type="bibr" target="#b5">[6]</ref>. We simulate the effects of context switching by interleaving branch traces from each of the SPEC 2000 integer benchmarks, switching to the next program after 60,000 branches. This workload represents an unrealistically heavy amount of context switching, but it serves as a good indicator of performance in extreme conditions, and it uses the same methodology as other recent work <ref type="bibr" target="#b3">[4]</ref>. Note that previous studies have used the 8 SPEC 95 integer benchmarks, so our use of the 12 SPEC 2000 benchmarks will likely lead to higher misprediction rates. For each predictor, we consider the effect of re-initializing the table of counters after each context switch (which would be done with a privileged instruction in a real operating system) and use this technique when it gives better performance.</p><p>Figure <ref type="figure" target="#fig_0">10</ref> shows that context switching affects the perceptron predictor more significantly than the other two predictors. Nevertheless, the perceptron predictor still maintains an advantage over the other two predictors at hardware budgets of 4K bytes or more. The hybrid gshare/perceptron predictor performs better in the presence of context switching; this benefit of hybrid predictors has been noticed by others <ref type="bibr" target="#b5">[6]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Implementation</head><p>We now suggest ways to implement our predictor efficiently.</p><p>Computing the Perceptron Output. Since -1 and 1 are the only possible input values to the perceptron, multiplication is not needed to compute the dot product. Instead, we simply add when the input bit is 1 and subtract (add the two'scomplement) when the input bit is -1. This computation is similar to that performed by multiplication circuits, which must find the sum of partial products that are each a function of an integer and a single bit. Furthermore, only the sign bit of the result is needed to make a prediction, so the other bits of the output can be computed more slowly without having to wait for a prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training.</head><p>The training algorithm of Section 3.3 can be implemented efficiently in hardware. Since there are no dependences between loop iterations, all iterations can execute in parallel. Since in our case both Ü and Ø can only be -1 or 1, the loop body can be restated as "increment Û by 1 if Ø Ü , and decrement otherwise," a quick arithmetic operation since the Û are at most 9-bit numbers:</p><formula xml:id="formula_3">for each bit in parallel if Ø Ü then Û := Û • ½ else Û := Û ½ end if</formula><p>Delay. A ¢ multiplier in a 0.25 m process can operate in 2.7 nanoseconds <ref type="bibr" target="#b8">[9]</ref>, which is approximately two clock cycles with a 700 MHz clock. At the longer history lengths, an implementation of our predictor resembles a 54 ¢ 54 multiply, but the data corresponding to the partial products (i.e., the weights) are narrower, at most 9 bits. Thus, any carrypropagate adders, of which there must be at least one in a multiplier circuit, will not need to be as deep. We believe that a good implementation of our predictor at a large hardware budget will take no more than two clock cycles to make a prediction. For smaller hardware budgets, one cycle operation is feasible. Two cycles is also the amount of time claimed for the variable length path branch predictor <ref type="bibr" target="#b22">[23]</ref>. That work proposes pipelining the predictor to reduce delay.</p><p>Jiménez et al study a number of techniques for reducing the impact of delay on branch predictors <ref type="bibr" target="#b12">[13]</ref>. For example, a cascading perceptron predictor would use a simple predictor to anticipate the address of the next branch to be fetched, and it would use a perceptron to begin predicting the anticipated address. If the branch were to arrive before the perceptron predictor were finished, or if the anticipated branch address were found to be incorrect, a small gshare table would be consulted for a quick prediction. The study shows that a similar predictor, using two gshare tables, is able to use the larger table 47% of the time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this paper we have introduced a new branch predictor that uses neural networks-the perceptron in particular-as the basic prediction mechanism. Perceptrons are attractive because they can use long history lengths without requiring exponential resources. A potential weakness of perceptrons is their increased computational complexity when compared with two-bit counters, but we have shown how a perceptron predictor can be implemented efficiently with respect to both area and delay. Another weakness of perceptrons is their inability to learn linearly inseparable functions, but despite this weakness the perceptron predictor performs well, achieving a lower misprediction rate, at all hardware budgets, than two well-known global predictors on the SPEC 2000 integer benchmarks.</p><p>We have shown that there is benefit to considering history lengths longer than those previously considered. Variable length path prediction considers history lengths of up to 23 <ref type="bibr" target="#b22">[23]</ref>, and a study of the effects of long branch histories on branch prediction only considers lengths up to 32 <ref type="bibr" target="#b6">[7]</ref>. We have found that additional performance gains can be found for branch history lengths of up to 62.</p><p>We have also shown why the perceptron predictor is accurate. PHT techniques provide a general mechanism that does not scale well with history length. Our predictor instead performs particularly well on two classes of branches-those that are linearly separable and those that require long history lengths-that represent a large number of dynamic branches.</p><p>Because our approach is largely orthogonal to many of the recent ideas in branch prediction, there is considerable room for future work. We can decrease aliasing by tuning our predictor to use the bias bits that were introduced by the Agree predictor <ref type="bibr" target="#b21">[22]</ref>. We can also employ perceptrons in a hybrid predictor that uses both global and local histories, since hybrid predictors have proven to work better than purely global schemes <ref type="bibr" target="#b5">[6]</ref>. We have preliminary experimental evidence that such hybrid schemes can be improved by using perceptrons, and we intend to continue this study in more detail.</p><p>More significantly, perceptrons have interesting characteristics that open up new avenues for future work. Because the perceptron predictor has different strengths and weaknesses from counter-based predictors, new hybrid schemes can be developed. We also plan to develop compiler-based branch classification techniques to make such hybrid predictors even more effective. We already have a starting point for this work, which is to focus on the distinction between linearly separable and inseparable branches, and between branches that require short history lengths and long history lengths. As noted in Section 5.7, perceptrons can also be used to guide speculation based on branch prediction confidence levels, and perceptron predictors can be used in recognizing important bits in the history of a particular branch.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Perceptron Model. The input values Ü ½ ÜÒ, are propagated through the weighted connections by taking their respective products with the weights Û ½ÛÒ. These products are summed, along with the bias weight Û ¼ , to produce the output value Ý.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Hardware Budget vs. Prediction Rate on SPEC 2000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Classifying the Advantage of our Predictor. Above the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>.1% over gshare and 8.2% over Best History Lengths. This table shows the best amount of global history to keep for each of the branch prediction schemes.</figDesc><table><row><cell>Hardware budget</cell><cell></cell><cell>History Length</cell><cell></cell></row><row><cell>in kilobytes</cell><cell cols="3">gshare bi-mode perceptron</cell></row><row><cell>1</cell><cell>6</cell><cell>7</cell><cell>12</cell></row><row><cell>2</cell><cell>8</cell><cell>9</cell><cell>22</cell></row><row><cell>4</cell><cell>8</cell><cell>11</cell><cell>28</cell></row><row><cell>8</cell><cell>11</cell><cell>13</cell><cell>34</cell></row><row><cell>16</cell><cell>14</cell><cell>14</cell><cell>36</cell></row><row><cell>32</cell><cell>15</cell><cell>15</cell><cell>59</cell></row><row><cell>64</cell><cell>15</cell><cell>16</cell><cell>59</cell></row><row><cell>128</cell><cell>16</cell><cell>17</cell><cell>62</cell></row><row><cell>256</cell><cell>17</cell><cell>17</cell><cell>62</cell></row><row><cell>512</cell><cell>18</cell><cell>19</cell><cell>62</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments. We thank Steve Keckler and Kathryn</head><p>McKinley for many stimulating discussions on this topic, and we thank Steve, Kathryn, and Ibrahim Hur for their comments on earlier drafts of this paper. This research was supported in part by DARPA Contract #F30602-97-1-0150 from the US Air Force Research Laboratory, NSF CAREERS grant ACI-9984660, and by ONR grant N00014-99-1-0402.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Branch prediction for free</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Larus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGPLAN &apos;93 Conference on Programming Language Design and Implementation</title>
				<meeting>the SIGPLAN &apos;93 Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="1993-06">June 1993</date>
			<biblScope unit="page" from="300" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The perceptron: A model for brain functioning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Block</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reviews of Modern Physics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="123" to="135" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evidence-based static branch prediction using machine learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lindsay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mozer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Programming Languages and Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The YAGS branch prediction scheme</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Eden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual ACM/IEEE International Symposium on Microarchitecture</title>
				<meeting>the 31st Annual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1998-11">November 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A language for describing predictors and its application to automatic synthesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gloy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Symposium on Computer Architecture</title>
				<meeting>the 24th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1997-06">June 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using hybrid branch predictors to improve branch prediction accuracy in the presence of context switches</title>
		<author>
			<persName><forename type="first">M</forename><surname>Evers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Symposium on Computer Architecture</title>
				<meeting>the 23rd International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1996-05">May 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An analysis of correlation and predictability: What makes two-level branch predictors work</title>
		<author>
			<persName><forename type="first">M</forename><surname>Evers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Chappell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Annual International Symposium on Computer Architecture</title>
				<meeting>the 25th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1998-07">July 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Faucett</surname></persName>
		</author>
		<title level="m">Fundamentals of Neural Networks: Architectures, Algorithms and Applications</title>
				<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A 2.7ns 0.25um CMOS 54 ¢ 54b multiplier</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hagihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yoshikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nakazato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Iriki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ikeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shibue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Inaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kagamihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yamashina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Solid-State Circuits Conference</title>
				<meeting>the IEEE International Solid-State Circuits Conference</meeting>
		<imprint>
			<date type="published" when="1998-02">February 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Computer Architecture: A Quantitative Approach</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hennessy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Morgan Kaufmann Publishers</publisher>
		</imprint>
	</monogr>
	<note>Second Edition</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Assigning confidence to conditional branch predictions</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rotenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual International Symposium on Microarchitecture</title>
				<meeting>the 29th Annual International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1996-12">December 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamically weighted ensemble neural networks for classification</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Walsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1998 International Joint Conference on Neural Networks</title>
				<meeting>the 1998 International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="1998-05">May 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The impact of delay on the design of branch predictors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calvin</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33th Annual International Symposium on Microarchitecture</title>
				<meeting>the 33th Annual International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2000-12">December 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The Alpha 21264 microprocessor architecture</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Mclellan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Webb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Compaq Computer Corporation</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Artificial Neural Networks for Image Understanding</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Kulkarni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Van Nostrand Reinhold</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The bi-mode branch predictor</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual International Symposium on Microarchitecture</title>
				<meeting>the 30th Annual International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1997-11">November 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Combining branch predictors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mcfarling</surname></persName>
		</author>
		<idno>TN-36m</idno>
		<imprint>
			<date type="published" when="1993-06">June 1993</date>
		</imprint>
		<respStmt>
			<orgName>Digital Western Research Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Trading conflict and capacity aliasing in conditional branch predictors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Michaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Uhlig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Symposium on Computer Architecture</title>
				<meeting>the 24th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1997-06">June 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rosenblatt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1962">1962</date>
			<publisher>Spartan</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Correlation and aliasing in dynamic branch predictors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sechrest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Symposium on Computer Architecture</title>
				<meeting>the 23rd International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1999-05">May 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Understanding neural networks via rule extraction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Setiono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Joint Conference on Artificial Intelligence</title>
				<meeting>the 14th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="480" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Agree predictor: A mechanism for reducing negative branch history interference</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sprangle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Chappell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alsup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Symposium on Computer Architecture</title>
				<meeting>the 24th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1997-06">June 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Variable length path branch prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Evers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the 8th International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="1998-10">October 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Highly accurate data value prediction using hybrid predictors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Franklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual International Symposium on Microarchitecture</title>
				<meeting>the 30th Annual International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adaptive switching circuits</title>
		<author>
			<persName><forename type="first">B</forename><surname>Widrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Hoff</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IRE WESCON Convention Record</title>
				<imprint>
			<date type="published" when="1960">1960</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="96" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Two-level adaptive branch prediction</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24 Ø ACM/IEEE Int&apos;l Symposium on Microarchitecture</title>
				<meeting>the 24 Ø ACM/IEEE Int&apos;l Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1991-11">November 1991</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
