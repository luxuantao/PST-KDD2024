<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sentic Blending: Scalable Multimodal Fusion for the Continuous Interpretation of Semantics and Sentics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><roleName>Member, IEEE</roleName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
							<email>cambria@nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="laboratory">! • E. Cambria is with Temasek Laboratories</orgName>
								<orgName type="institution">National University of Singa-pore</orgName>
								<address>
									<addrLine>5A Engineering Drive 1</addrLine>
									<postCode>117411</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Newton</forename><surname>Howard</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">! • E. Cambria is with Temasek Laboratories</orgName>
								<orgName type="institution">National University of Singa-pore</orgName>
								<address>
									<addrLine>5A Engineering Drive 1</addrLine>
									<postCode>117411</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Media Laboratory</orgName>
								<orgName type="institution">MIT</orgName>
								<address>
									<addrLine>20 Ames Street, Cam-bridge</addrLine>
									<postCode>02139-4307</postCode>
									<region>Massachusetts</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Jane</forename><surname>Hsu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">! • E. Cambria is with Temasek Laboratories</orgName>
								<orgName type="institution">National University of Singa-pore</orgName>
								<address>
									<addrLine>5A Engineering Drive 1</addrLine>
									<postCode>117411</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<postCode>106</postCode>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Amir</forename><surname>Hussain</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">! • E. Cambria is with Temasek Laboratories</orgName>
								<orgName type="institution">National University of Singa-pore</orgName>
								<address>
									<addrLine>5A Engineering Drive 1</addrLine>
									<postCode>117411</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Dept. of Computing Science and Mathematics</orgName>
								<orgName type="institution">University of Stirling</orgName>
								<address>
									<addrLine>Cottrell Building</addrLine>
									<postCode>FK9 4LA</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sentic Blending: Scalable Multimodal Fusion for the Continuous Interpretation of Semantics and Sentics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multimodal fusion</term>
					<term>SenticNet</term>
					<term>Facial expression analysis</term>
					<term>Affective common-sense</term>
					<term>Emotion recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The capability of interpreting the conceptual and affective information associated with natural language through different modalities is a key issue for the enhancement of human-agent interaction. The proposed methodology, termed sentic blending, enables the continuous interpretation of semantics and sentics (i.e., the conceptual and affective information associated with natural language) based on the integration of an affective common-sense knowledge base with any multimodal signal-processing module. In this work, in particular, sentic blending is interfaced with a facial emotional classifier and an opinion mining engine. One of the main distinguishing features of the proposed technique is that it does not simply perform cognitive and affective classification in terms of discrete labels, but it operates in a multidimensional space that enables the generation of a continuous stream characterising user's semantic and sentic progress over time, despite the outputs of the unimodal categorical modules have very different time-scales and output labels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>M ULTIMODALITY is a key aspect when trying to achieve natural interaction in machines. People, in fact, communicate not only through dialogues, but also through many other channels, e.g., facial expressions, gestures, eye contact, posture, and voice tone. Besides semantics, hence, machines also need to be able to recognise, interpret, and process sentics, that is, emotional information. In human cognition, thinking and feeling are mutually present: emotions are often the product of our thoughts, as well as our reflections are often the product of our affective states. Emotions are intrinsically part of our mental activity and play a key role in communication and decision-making processes.</p><p>Emotion is a chain of events made up of feedback loops. Feelings and behaviour can affect cognition, just as cognition can influence feeling. Emotion, cognition, and action interact in feedback loops and emotion can be viewed in a structural model tied to adaptation <ref type="bibr" target="#b0">[1]</ref>. There is actually no fundamental opposition between emotion and reason. In fact, it may be argued that reason consists of basing choices on the perspectives of emotions at some later time. Reason dictates not giving in to one's impulses because doing so may cause greater suffering later <ref type="bibr" target="#b1">[2]</ref>. Reason does not necessarily imply exertion of the voluntary capacities to suppress emotion. It does not necessarily involve depriving certain aspects of reality of their emotive powers. On the contrary, our voluntary capacities allow us to draw more of reality into the sphere of emotion. They allow one's emotions to be elicited not merely by the proximal, or the perceptual, or that which directly interferes with one's actions, but by that which, in fact, touches on one's concerns, whether proximal or distal, whether occurring now or in the future, whether interfering with one's own life or that of others.</p><p>Cognitive functions serve emotions and biological needs. Information from the environment is evaluated in terms of its ability to satisfy or frustrate needs. What is particularly significant is that each new cognitive experience that is biologically important is connected with an emotional reaction such as fear, pleasure, pain, disgust, or depression <ref type="bibr" target="#b2">[3]</ref>. Emotions, in fact, are special states shaped by natural selection to adjust various aspects of our organism in order to make it better face particular situations, e.g., anger evolved for reaction, fear evolved for protection, and affection evolved for reproduction.</p><p>For these reasons, an important strand of emotionrelated research in human-computer interaction is the simulation of emotional expressions made by embodied computer agents. Although several studies prove that multisensory fusion (e.g., audio, visual, physiological responses) improves the robustness and accuracy of machine analysis of semantics and sentics <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, most cognitive and affective recognition works still focus on increasing the success rates in sensing semantics and sentics from a single channel, rather than merging complementary information across different channels <ref type="bibr" target="#b5">[6]</ref>. The multimodal fusion of multiple channels is far from being solved <ref type="bibr" target="#b6">[7]</ref> and represents an active and open research issue. In this context, sentic blending proposes a methodology for integrating and analysing conceptual and affective information coming from different modules. In this work, in particular, it is shown how a scalable semantics and sentics fusion technique allows to integrate different multimodal signals coming from a natural language text analyser and a facial emotional classifier, by presenting user cognitive and affective information in the form of a continuous multidimensional stream that shows user semantic and sentic evolution along the user-agent interaction process.</p><p>The structure of the paper is the following: Section 2 discusses the challenges of cognitive and affective multimodal fusion; Section 3 details how a multidimensional space is used for implementing the multimodal fusion methodology between different channels; Section 4 presents a proof of concept (PoC) and an evaluation of the proposed methodology; Section 5, finally, sets out conclusions and a description of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Cognitive and affective information fusion is essential to achieve successful user-agent interaction, but there are several challenges that make it a particularly difficult task. A major challenge is the definition of a reliable strategy to fuse the cognitive and affective information coming from different sources with very different time scales, metric levels, and temporal structures. Existing fusion strategies follow two main streams: feature-level fusion and decision-level fusion. Feature-level fusion combines the features extracted from each channel in a joint vector before classification. Although several works have reported good performances when fusing different modalities at a feature-level <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, this strategy becomes more challenging as the number of input features increases and they are of very different natures (different timing, metrics, etc.). Adding new modalities implies a big effort to synchronise the different inputs and retrain the whole classification system.</p><p>To overcome such difficulties, most researchers opt for a decision-level fusion, in which the inputs coming from each modality are modelled and classified independently, and their relative unimodal recognition results are integrated at the end of the process by the use of suitable criteria, e.g., expert rules and simple operators such as majority vote, sum, product, and adaptation of weights. Many studies have demonstrated the advantage of decision-level fusion over feature-level fusion, due to the uncorrelated errors from different classifiers <ref type="bibr" target="#b9">[10]</ref> and the fact that time and feature dependence are abstracted. Various (mainly bimodal) decision-level fusion methods have been proposed in the literature <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, but optimal fusion designs are still undefined.</p><p>Most available multimodal recognisers have designed ad hoc solutions for fusing information coming from a set of given modalities, but cannot accept new modalities without re-defining and/or re-training the whole system. Moreover, in general they are not adaptive to input quality changes and, hence, do not consider eventual adjustments in the reliability of the different information channels. In summary, there is not a general consensus when defining multimodal fusion strategies.</p><p>Another key challenge that directly affects multimodal fusion is related to the chosen output description level of semantics and sentics. Cognitive and affective information, in fact, is often classified into a limited set of labels, e.g., Ekman's six universal emotions <ref type="bibr" target="#b13">[14]</ref>, which often fail to describe the wide range and intensities of semantics and sentics that occur in daily communication settings. To overcome such a hurdle, Whissell <ref type="bibr" target="#b14">[15]</ref> and Plutchik <ref type="bibr" target="#b0">[1]</ref> proposed to view affective information not independently, but rather related to one another in a systematic manner. They consider emotions as a continuous 2D space whose dimensions are evaluation and activation. The former measures how a human feels, from positive to negative. The latter measures whether humans are more or less likely to take some action under the emotional state, from active to passive.</p><p>Unlike the categorical approach, the dimensional approach can describe an infinite number of cognitive and affective states and intensities, and is best suited to deal with variations of these over time. It provides an algebra and allows inputs coming from different modalities to be related mathematically, which makes it especially useful when integrating modules with different time-scales. However, compared to category-based description of semantics and sentics, very few works have chosen a dimensional description level and, in the few that have, the 2D space is discretised to a two-class (positive vs negative and active vs passive) <ref type="bibr" target="#b15">[16]</ref> or a four class (space quadrants) classification <ref type="bibr" target="#b16">[17]</ref>, thereby losing the descriptive potential of the bidimensional space. This is mainly due to the current lack of (both unimodal and multimodal) databases annotated in terms of evaluationactivation dimensions.</p><p>Some interesting dimensional databases are publicly available <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> but, in comparison to categorical ones, they are limited in terms of number of modalities (in general, they explore only audio and/or video channels), annotators, subjects, samples, etc. Moreover, manual dimensional annotation of ground truth is very time-consuming and unreliable, since a large labelling variation between different human raters is reported when working with the dimensional approach <ref type="bibr" target="#b19">[20]</ref>.</p><p>For these reasons, although working at dimensional level would be more appropriate to face the problem of multimodal fusion, training and validation of individual modules is performed using databases with categorical annotations. In order to exploit both categorical and dimensional classification techniques, the proposed multimodal fusion methodology exploits a multidimensional space, built by means of sentic computing <ref type="bibr" target="#b20">[21]</ref>, which enables analogical reasoning between affective commonsense concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A METHODOLOGY FOR FUSING SEMANTICS AND SENTICS</head><p>In cognitive science, the term 'blending' refers to a general theory of cognition describing the way humans process and rationalise information through a set of mental operations <ref type="bibr" target="#b21">[22]</ref>. The theory explains the process by which humans assign meaning to incoming information from sensory input, integrate it, and learn and gain knowledge.</p><p>In the same wake of conceptual blending, sentic blending proposes a methodology for fusing multiple unimodal signals in order to obtain a global multidimensional dynamic stream that shows how semantics and sentics evolve over time. In order to let the unimodal signals be defined in a robust and reliable way by means of existing categorical databases, each signal-processing module is assumed to classify in terms of its own list of cognitive and affective labels. Irrespective of the labels used, sentic blending maps each modules output to AffectiveSpace <ref type="bibr" target="#b20">[21]</ref>, a multidimensional vector space of affective common-sense knowledge, fuses the different sources of conceptual and emotional information over time through mathematical formulation, and obtains a dynamic multidimensional stream representing the users cognitive and affective progress as final output. The proposed methodology is sufficiently scalable to add new modules coming from new channels without having to retrain the whole system.</p><p>The first step of sentic blending consists in building a mapping such that the output of each module i at a given time t 0i can be represented as a 100-dimensional coordinate vector</p><formula xml:id="formula_0">s i (t 0i ) = [x 0i (t 0i ), x 1i (t 0i ), ... x 99i (t 0i )]</formula><p>in AffectiveSpace. The majority of categorical modules described in the literature provide as output at the time t 0i (corresponding to the detection of the cognitive or affective stimulus) a list of labels with some associated weights. Irrespective of the categorisation used, each label has a specific location, i.e., an associated 100D point in AffectiveSpace. The components &lt; x 0i (t 0i ), x 1i (t 0i ), ... x 99i (t 0i ) &gt; of the coordinates vector s i (t 0i ) are then calculated as the barycenter of those weighted points. Hence, the users cognitive and affective progress can be viewed as a point (corresponding to the location of a particular state in time t) moving through this space over time.</p><p>The second step of sentic blending aims to compute the ensemble stream by fusing the different s i (t 0i ) vectors obtained from each modality over time. The main difficulty in achieving multimodal fusion is related to the fact that t 0i stimulus arrival times may be known a-priori or not, and may be very different for each module. To overcome such a hurdle, the following equation is proposed to calculate the ensemble stream s(t) = [x 0 (t), x 1 (t), ... x 99 (t)] at any arbitrary time t:</p><formula xml:id="formula_1">s(t) = N i=1 γ i (t)s i (t 0i ) N i=1 γ i (t) (1)</formula><p>where N is the number of fused modalities, t 0i is the arrival time of the last stimulus detected by module i, and γ i (t) are the 0 to 1 weights (or confidences) that can be assigned to each modality i at a given arbitrary time t. In this way, the overall fused response is the sum of each modalitys contribution s i (t 0i ) modulated by the γ i (t) coefficients over time. Therefore, the definition of γ i (t) is especially important given that it governs the temporal behaviour of the fusion. Because human responses are analogous to systems with additive responses with decay, in which, in the absence of input, the response decays back to a baseline, γ i (t) weights are defined as:</p><formula xml:id="formula_2">γ i (t) = b i c i (t 0i )e −di(t−t0) if greater than 0 elsewhere<label>(2)</label></formula><p>where:</p><p>• b i is the general confidence that can be given to module i (e.g., the general recognition success rate of the module)</p><formula xml:id="formula_3">• c i (t 0i</formula><p>) is the temporal confidence that can be assigned to the last output of module i due to external factors (i.e., not classification issues themselves). For instance, due to sensor errors if dealing with physiological signals, or due to facial tracking problems if studying facial expressions (such as occlusions, lighting conditions, etc.) • d i is the rate of decay (in s −1 ) that indicates how quickly a stimulus decreases over time for module i • is the threshold below which the contribution of a module is assumed to disappear. Since exponential functions tend to zero at infinity but never completely disappear, indicates the γ i (t) value below which the contribution of a module is small enough to be considered non-existent. By defining the aforementioned parameters for each module i and applying (1) and ( <ref type="formula" target="#formula_2">2</ref>), the ensemble stream that characterises the users cognitive and affective progress over time can be computed by calculating successive s(t) values with any desired time between samples t. In other words, the ensemble stream is progressively built by adding s(t k ) samples to its trajectory, where t k = k t (with k integer). However, there are two main issues in the stream calculation process:</p><p>1) If the contribution of every fused module is null at a given sample time, i.e., every γ i (t) is null at that time, the denominator in (1) is zero and the stream sample cannot be computed. Examples of cases in which the contribution of a module is null could be the failure of the connection of a sensor of physiological signals, the appearance of an occlusion in the facial/postural tracking system, or simply when the module is not reactivated before its response decays completely. 2) Large jumps in AffectiveSpace can appear if semantic conflicts arise (e.g., if the distance between two close coordinates vectors s i (t 0i ) is long).</p><p>In order to address both issues, a Kalman filtering technique is applied to the computed stream. By definition, Kalman filters estimate a systems state by combining an inexact (noisy) forecast with an inexact measurement of that state, so that the biggest weight is given to the value with the least uncertainty at each time t. In this way, on the one hand, the Kalman filter serves to smooth the ensemble streams trajectory and thus prevent large jumps. On the other hand, situations in which the sum of γ i (t) is null are prevented by letting the filter prediction output be taken as the 100D point position for those samples. In an analogy to classical mechanics, the 'sentic kinematics' of the 100D point moving through AffectiveSpace are modelled as the systems state X k in the Kalman framework, i.e.,</p><formula xml:id="formula_4">X k = [x 0 , x 1 , ... x 99 , v x0 , v x1 , ... v x99 ] T</formula><p>k representing position and velocity in 100 dimensions at time t k . The following stream samples s(t k ) are modelled as the measurement of the systems state in the following way:</p><formula xml:id="formula_5">X k+1 = F X k + w k = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ x 0 x 1 . . . x 99 v x0 v x1 . . . v x99 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ k+1 = = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ 1 0 • • • 0 1 0 • • • 0 0 1 • • • 0 0 1 • • • 0 . . . . . . . . . . . . . . . . . . . . . . . . 0 0 • • • 1 0 0 • • • 1 0 0 • • • 0 1 0 • • • 0 0 0 • • • 0 0 1 • • • 0 . . . . . . . . . . . . . . . . . . . . . . . . 0 0 • • • 0 0 0 • • • 1 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ x 0 x 1 . . . x 99 v x0 v x1 . . . v x99 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ k + w k</formula><p>where F is the transition matrix taking the state X k from time k to time k + 1 (i.e., from one stream sample to the next in the 100-dimensional vector space), and w k is the process noise (which is usually assumed to be additive, white, Gaussian, and with zero mean). The measurement equation, hence, is defined as:</p><formula xml:id="formula_6">Y k = H X k + z k = ⎡ ⎢ ⎢ ⎢ ⎣ x 0m x 1m</formula><p>. . .</p><formula xml:id="formula_7">x 99m ⎤ ⎥ ⎥ ⎥ ⎦ k = = ⎡ ⎢ ⎢ ⎢ ⎣ 1 0 • • • 0 0 0 • • • 0 0 1 • • • 0 0 0 • • • 0 . . . . . . . . . . . . . . . . . . . . . . . . 0 0 • • • 1 0 0 • • • 0 ⎤ ⎥ ⎥ ⎥ ⎦ ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ x 0 x 1 . . . x 99 v x0 v x1 . . . v x99 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ k + z k</formula><p>where Y k is the measurable at time k and H is the measurement matrix. The measurement noise z k is also assumed to be additive, white, Gaussian, and with zero mean (but uncorrelated with the process noise w k ). Once the process and measurement equations are defined, the Kalman iterative estimation process can be applied to the stream, so that each iteration corresponds to a new sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">POC: FUSING NATURAL LANGUAGE AND FACIAL EXPRESSIONS</head><p>As a PoC, sentic blending is employed for fusing natural language text and facial expressions within the design of an embodied conversational agent (ECA) based on a multimodal animation engine <ref type="bibr" target="#b22">[23]</ref>. ECAs are graphical interfaces capable of using verbal and non-verbal modes of communication to interact with users in computerbased environments. These agents are sometimes just as an animated talking face, may be displaying simple facial expressions and, when using speech synthesis, with some kind of lip synchronisation, and sometimes they have sophisticated 3D graphical representation, with complex body movements and facial expressions.</p><p>Besides expressing emotions, ECAs should also be capable of understanding users' emotions and reacting accordingly. Recent research focuses on the psychological impact of affective agents endowed with the ability to behave empathically with the user <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. The findings demonstrate that bringing about empathic agents is important in human-computer interaction. Moreover, addressing user's emotions significantly enhances the believability and lifelikeness of virtual humans. Nevertheless, to date, there are not many examples of agents that can sense in a completely automatic and natural (both verbal and non-verbal) way human emotion and respond realistically.</p><p>The architecture of the proposed ECA consists of four main modules: Perception, Cognitive, Deliberative/Generative, and Motor module (Fig. <ref type="figure" target="#fig_0">1</ref>). The Perception module simply consists of the hardware necessary to gather the multimodal information from the user, i.e., keyboard, microphone, and webcam. The Cognitive module aims to infer the user's cognitive and affective state from the different inputs and integrate it. The Deliberative/Generative module is in charge of processing the extracted semantics and sentics to manage the virtual agent's decisions and reactions, which are finally generated by the Motor module. The Cognitive module, in particular, is in charge of extracting cognitive and affective information from the textual, vocal, and video inputs and integrating them. It consists of three main parts: a facial expression analyser <ref type="bibr" target="#b22">[23]</ref>, for extracting affective information from video; the SenticNet engine <ref type="bibr" target="#b26">[27]</ref>, for inferring semantics and sentics associated with typed-in text and speech-to-text converted contents; and sentic blending, for integrating the outputs coming from the two previous modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Facial Expression Analyser</head><p>The facial expression analyser achieves an automatic classification of the shown facial expressions into discrete emotional categories. It is able to classify the user's emotion in terms of Ekman's six universal emotions (fear, sadness, joy, disgust, surprise, and anger) <ref type="bibr" target="#b13">[14]</ref> plus neutral, giving a membership confidence value to each emotional category. The face modelling selected as input for the facial expression analyser follows a feature-based approach: the inputs are a set of facial distances and angles calculated from feature points of the mouth, eyebrows, and eyes.</p><p>The system intelligently combines the outputs of five different classifiers simultaneously. In this way, the overall risk of making a poor selection with a given classifier for a given input is reduced. The classifier combination chosen follows a weighted majority voting strategy, where the voted weights are assigned depending on the performance of each classifier for each emotion. In order to select the best classifiers to combine, the Waikado Environment for Knowledge Analysis (Weka) tool was used <ref type="bibr" target="#b27">[28]</ref>. This provides a collection of machine learning algorithms for data mining tasks. From this collection, five classifiers were selected after tuning: RIPPER, MLP, SVM, NB, and C4.5. The selection was based on their widespread use as well as on the individual performance of their Weka implementation.</p><p>To train the classifiers and evaluate the performance of the system, two different facial emotion databases were used: the FGNET database <ref type="bibr" target="#b28">[29]</ref>, which provides video sequences of 19 different Caucasian people, and the MMI Facial Expression Database <ref type="bibr" target="#b29">[30]</ref>, which holds 1,280 videos of 43 different subjects from different races (Caucasian, Asian, and Arabic). Both databases are classified according to Ekman's six universal emotions plus neutral. A new database has been built for testing this work with a total of 1,500 static frames carefully selected from the apex of the video sequences from the FG-NET and MMI databases.</p><p>The results obtained when applying the abovementioned strategy for combining the scores of the five classifiers are shown in the form of confusion matrix in Table <ref type="table" target="#tab_1">1</ref> (results have been obtained with a 10-fold cross-validation test over the 1500 database images). As it can be observed, the success rates for neutral, joy, and surprise are very high (84.44%-95.23%). However, the system tends to confuse disgust with fear, anger with disgust, and fear with surprise; therefore, the performance for those emotions is slightly worse. The lowest result of the classification is for sadness: it is confused with neutral on 67.80% of occasions, due to the similarity of the facial expressions. Confusion between these pairs of emotions occurs frequently in the literature and for this reason many classification works do not consider some of them.</p><p>Nevertheless, the results can be considered positive as two incompatible emotions (such as sadness and joy or fear and anger) are confused on less than 0.2% of occasions. Another relevant aspect to be taken into account when evaluating the results is human opinion. The labels provided in the database for training classifiers correspond to the real emotions felt by users although they do not necessarily have to coincide with the perceptions other human beings may have about the facial expressions shown.</p><p>Undertaking this kind of study is very important when dealing with human-computer interaction, since the system is proved to work in a similar way to the human brain. In order to take into account the human factor in the evaluation of the results, 60 persons were told to classify the 1,500 images of the database in terms of emotions. As a result, each one of the frames was classified by 10 different people in 5 sessions of 50 images. The Kappa statistic obtained from raters annotations is equal to 0.74 (calculated following the formula proposed in <ref type="bibr" target="#b30">[31]</ref>), which indicates an adequate inter-rater agreement in the emotional images annotation.</p><p>With this information, the evaluation of the results was repeated: the recognition was marked as good if the decision was consistent with that reached by the majority of the human assessors. The results (confusion matrix) of considering users' assessment are shown in Table <ref type="table" target="#tab_2">2</ref>. As it can be seen, the success ratios have considerably increased. Therefore, it can be concluded that the confusions of the algorithms go in the same direction as those of the users, which means that the adopted classification strategy is consistent with human classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SenticNet Engine</head><p>In order to effectively mine semantics and sentics from text, it is necessary to bridge the gap between unstructured natural language data and structured machineprocessable data. To this end, an intelligent software engine based on SenticNet has been proposed <ref type="bibr" target="#b31">[32]</ref> that aims to extract the conceptual and affective information associated with natural language text. Such an engine consists of four main components: a pre-processing module, which performs a first skim of the input text; a semantic parser, whose aim is to extract concepts from natural language data; the IsaCore <ref type="bibr" target="#b32">[33]</ref> module, for inferring the semantics associated with the given concepts; and the SenticNet API 1 , for the extraction of sentics.</p><p>The pre-processing module firstly exploits linguistic dictionaries to interpret all the affective valence indicators usually contained in text, e.g., special punctuation, complete upper-case words, cross-linguistic onomatopoeias, exclamation words, degree adverbs, and emoticons. Secondly, the module detects negation and spreads it in a way that it can be accordingly associated to concepts during the parsing phase. Handling negation is an important concern in sentiment-related analysis, as it can reverse the meaning of a statement. Such a task, however, is not trivial as not all appearances of explicit negation terms reverse the polarity of the enclosing sentence and that negation can often be expressed in rather subtle ways <ref type="bibr" target="#b33">[34]</ref>. Lastly, the module converts text to lower-case and, after lemmatising it, splits it into single clauses according to grammatical conjunctions and punctuation.</p><p>1. http://sentic.net/api</p><p>The semantic parser deconstructs text into concepts using a lexicon based on sequences of lexemes that represent multiple-word concepts extracted from SenticNet and IsaCore. These n-grams are not used blindly as fixed word patterns but exploited as reference for the module, in order to extract multiple-word concepts from information-rich sentences. So, differently from other shallow parsers, the module can recognise complex concepts also when irregular verbs are used or when these are interspersed with adjective and adverbs, e.g., the concept 'buy christmas present' in the sentence "I bought a lot of very nice Christmas presents". The semantic parser, additionally, provides, for each retrieved concept, its relative frequency, valence, and status, i.e., the concept's occurrence in the text, its positive or negative connotation, and the degree of intensity with which the concept is expressed, respectively. For each clause, the module outputs a small bag of concepts (SBoC), which is later on analysed separately by the IsaCore module and the SenticNet API to infer the conceptual and affective information associated with the input text, respectively.</p><p>Once natural language text is deconstructed into concepts, these are given as input to both the IsaCore module and the SenticNet API. While the former exploits the graph representation of the common and common-sense knowledge base to detect semantics, the latter exploits the SenticNet API to infer sentics. In particular, the IsaCore module applies spectral association for assigning activation to key nodes of the semantic network, which are used as seeds or centroids for classification. Such seeds can simply be the concepts corresponding to the class labels of interest plus their available synonyms and antonyms, if any. Seeds can also be found by applying CF-IOF <ref type="bibr" target="#b34">[35]</ref> on a training corpus, in order to perform a classification that is more relevant to the data under analysis. After seed concepts are identified, the module spreads their values across the IsaCore graph.</p><p>This operation, an approximation of many steps of spreading activation, transfers the most activation to concepts that are connected to the seed concepts by short paths in affective common-sense knowledge. Therefore, the concepts of each SBoC provided by the semantic parser are projected on the matrix resulting from spectral association in order to calculate their semantic relatedness to each seed concept and, hence, their degree of belonging to each different class. Such classification measure is directly proportional to the degree of connectivity between the nodes representing the retrieved concepts and the seed concepts in the IsaCore graph.</p><p>The concepts retrieved by the semantic parser are also given as input to the SenticNet API, which, in turn, exploits an ensemble of graph-mining and dimensionalityreduction techniques <ref type="bibr" target="#b20">[21]</ref> to infer the affective information associated with them. In particular, the SenticNet API provides concept polarity and affective labels in terms of Pleasantness, Attention, Sensitivity, and Aptitude, the dimensions of the Hourglass of Emotion categorisation model <ref type="bibr" target="#b35">[36]</ref>. As well as in the IsaCore module, the categorisation does not consist in simply labelling each concept, but also in assigning a confidence score to each emotional label, which is directly proportional to the degree of belonging to a specific affective cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sentic Blending Module</head><p>The sentic blending module aims to fuse the affective classification performed by the facial expression analyser with the labels produced by the SenticNet engine in realtime. In particular, the module considers a projection of AffectiveSpace on its first two eigenmoods, that is, its first two bases e 0 and e 1 . The most significant eigenmood, e 0 , represents concepts with positive affective valence. That is, the larger a concept's component in the e 0 direction is, the more affectively positive it is likely to be. Concepts with negative e 0 components, then, are likely to have negative affective valence.</p><p>The multimodal fusion of this PoC takes place in two dimensions. Firstly, every output label inferred by the SenticNet engine and the facial expression analyser is projected into the 2D representation of AffectiveSpace. Therefore, s i (t oi ) can be obtained each time a given module i outputs cognitive and affective information at time t oi (with i comprised between 1 and 2). It is interesting to notice that vectors s 1 (t 01 ) coming from the SenticNet engine can arrive at any time t 01 , unknown apriori. However, the facial expression module outputs its s 2 (t 02 ) vectors with a known frequency, determined by the video frame rate f . For this reason, and given that the facial expression module is the fastest acquisition module, the ensemble streams time between samples is assigned to t = 1 f . The next step towards achieving the temporal fusion of the different modules is assigning a value to the parameters that define the γ i (t) weights, namely b i , c i (t 0i ), d and . It should be noted that it is especially difficult to determine the value of the different d i given that there are no works in the literature providing data for this parameter. Therefore, such values were established empirically for this PoC. Once the parameters are assigned, the ensemble stream calculation process can be started following ( <ref type="formula">1</ref>) and ( <ref type="formula" target="#formula_2">2</ref>).</p><p>Finally, the 'sentic kinematics' filtering technique is iteratively applied in real-time each time a new sample is added to the computed stream. In order to demonstrate the potential of the presented fusion methodology, sentic blending has been applied to a conversation in which the text typed by a user and his facial capture video have been analysed and fused. The user narrates an emotional incident: at first, he is excited and happy about having bought a wonderful new car but, shortly afterwards, he becomes sad when telling he has dented it (see Table <ref type="table">3</ref>). Fig. <ref type="figure" target="#fig_1">2</ref> shows the ensemble streams obtained when applying the methodology to each individual module separately (i.e., the modules are not fused, only the contribution of one module is considered) without using 'sentic kinematics' filtering. At first sight, the timing differences between modalities are striking: the facial expressions modules input stimuli are much more numerous than those of the SenticNet engine, making the latters emotional paths look more linear. Another noteworthy aspect is that the facial expression modules stream calculation is interrupted during several seconds (14s approximately) due to the appearance of a short facial occlusion, causing the tracking program to temporarily lose the facial features. Fig. <ref type="figure" target="#fig_2">3</ref> presents the continuous ensemble stream obtained when applying the methodology to fuse both modules, both without (left) and with (right) the 'sentic kinematics' filtering step. As can be seen, the complexity of the users cognitive and affective progress is shown in a simple and efficient way. Different modalities complement each other to obtain a more reliable result. Although the interruption period of the ensemble stream calculation is considerably reduced with respect to the facial expressions modules individual case (from 14s to 6s approximately), it still exists since the text modules decay process reaches the threshold before the end of the facial occlusion, causing the γ 1 (t) and γ 2 (t) weights to be null. Thanks to the use of the 'sentic kinematics' filtering technique, the ensemble stream is smoothed and the temporal input information absence is solved by letting the filter prediction output be taken as the 2D point position for those samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORK</head><p>Sentic blending is a scalable methodology for fusing multiple cognitive and affective recognition modules. This methodology is able to fuse any number of unimodal categorical modules, with very different timescales and output labels. This is possible thanks to the use of a multidimensional vector space that provides the system with mathematical capabilities to deal with temporal issues. The proposed methodology outputs a continuous multidimensional stream that represents in a novel and efficient way the users detected cognitive and affective progress over time. A Kalman filtering technique controls the ensemble stream in real-time through a 'sentic kinematics' model to ensure temporal consistency and robustness. The methodology has been shown effective to fuse two different modalities: natural language data and facial expressions. The first experimental results are promising and the potential of the proposed methodology has been demonstrated. This work brings a new perspective and invites further discussion on the still open issue of multimodal semantic fusion. In general, evaluation issues are largely solved for categorical recognition approaches. Unimodal categorical modules can be exhaustively evaluated thanks to the use of large well-annotated databases and wellknown measures and methodologies (such as percentage of correctly classified instances, cross-validation, etc.). The evaluation of the performance of dimensional approaches is, however, an open and difficult issue to be solved. In the future, our work is expected to focus in depth on evaluation issues applicable to dimensional approaches and multimodality. The proposed fusion methodology will be explored in different application contexts, with different numbers and natures of modalities to be fused.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. ECA's architecture. The system mainly consists of two modules for managing the avatar's inputs and outputs, and two modules for performing affective common-sense reasoning.</figDesc><graphic url="image-6.png" coords="5,57.47,141.53,100.32,91.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Ensemble streams obtained when applying sentic blending to the SenticNet engine (left) and the facial expression analyser (right), without 'sentic kinematics' filtering.</figDesc><graphic url="image-11.png" coords="7,56.87,49.85,501.02,195.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Ensemble stream obtained when applying sentic blending to the proposed conversation, with (right) and without (left) using 'sentic kinematics' filtering.</figDesc><graphic url="image-12.png" coords="8,57.23,49.73,501.02,222.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>Confusion matrix obtained combining the five classifiers. Success rates for neutral, joy, and surprise are very high, but disgust, anger, and fear tend to be confused.</figDesc><table><row><cell></cell><cell>disgust</cell><cell>joy</cell><cell>anger</cell><cell>fear</cell><cell>sadness</cell><cell>neutral</cell><cell>surprise</cell></row><row><cell>disgust</cell><cell>84.24%</cell><cell>0%</cell><cell>2.34%</cell><cell>13.42%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row><row><cell>joy</cell><cell>4.77%</cell><cell>95.23%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row><row><cell>anger</cell><cell>15.49%</cell><cell>0%</cell><cell>77.78%</cell><cell>0%</cell><cell>3.75%</cell><cell>2.98%</cell><cell>0%</cell></row><row><cell>fear</cell><cell>1.12%</cell><cell>0%</cell><cell>0%</cell><cell>92.59%</cell><cell>2.06%</cell><cell>0%</cell><cell>4.23%</cell></row><row><cell>sadness</cell><cell>0.32%</cell><cell>0.20%</cell><cell>1.68%</cell><cell>0%</cell><cell>66.67%</cell><cell>31.13%</cell><cell>0%</cell></row><row><cell>neutral</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0.88%</cell><cell>1.12%</cell><cell>98.00%</cell><cell>0%</cell></row><row><cell>surprise</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>6.86%</cell><cell>0%</cell><cell>2.03%</cell><cell>91.11%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>Confusion matrix obtained after human assessment. Success ratios considerably increase, meaning that the adopted classification strategy is consistent with human classification.</figDesc><table><row><cell># Module</cell><cell>1</cell><cell>2</cell></row><row><cell>Modality</cell><cell>text</cell><cell>video</cell></row><row><cell>Categorisation model</cell><cell>Hourglass of Emotions</cell><cell>Ekman's basic emotions</cell></row><row><cell>Total number of possible</cell><cell>24</cell><cell>6</cell></row><row><cell>output labels</cell><cell></cell><cell></cell></row><row><cell>General confidence</cell><cell>b 1 = 0.65</cell><cell>b 2 = 0.9461</cell></row><row><cell>Temporal confidence</cell><cell>c 1 (t 01 ) = 1</cell><cell>c 2 (t 02 ) is assigned to the</cell></row><row><cell></cell><cell></cell><cell>tracking quality confidence</cell></row><row><cell></cell><cell></cell><cell>weighting, from 0 to 1,</cell></row><row><cell></cell><cell></cell><cell>provided by the facial</cell></row><row><cell>Decay value</cell><cell>d 1 = 0.035 s −1</cell><cell>feature tracking program Irrelevant since the stream</cell></row><row><cell></cell><cell></cell><cell>sample rate is equal to the</cell></row><row><cell></cell><cell></cell><cell>video frame rate</cell></row><row><cell>Threshold value</cell><cell>= 0.1</cell><cell>Irrelevant since the stream</cell></row><row><cell></cell><cell></cell><cell>sample rate is equal to the</cell></row><row><cell></cell><cell></cell><cell>video frame rate</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2013" xml:id="foot_0">IEEE Symposium on Computational Intelligence for Human-like Intelligence (CIHLI)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2013" xml:id="foot_1">IEEE Symposium on Computational Intelligence for Human-like Intelligence (CIHLI)</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The nature of emotions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Plutchik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Scientist</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="344" to="350" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The laws of emotions</title>
		<author>
			<persName><forename type="first">N</forename><surname>Frijda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">U</forename><surname>Neisser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Cognitive Psychology. Appleton Century Crofts</title>
		<imprint>
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emotion recognition based on multimodal information</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Affective Information Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="241" to="265" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic prediction of frustration</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burleson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="724" to="736" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pad-based multimodal affective fusion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gilroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cavazza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niiranen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Andre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Urbain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Benayoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seichter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Billinghurst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACII</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">From the lab to the real world: Affect recognition using multiple cues and modalities</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Piccardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Affective Computing: Focus on Emotion Expression, Synthesis, and Recognition</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="185" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Beyond facial expressions: Learning human emotion from body gestures</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
				<imprint>
			<publisher>Warwick</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Brain-computer interaction research at the computer vision and multimedia laboratory</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Alecu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kronegg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Voloshynovskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Neural Systems and Rehabilitation Engineering</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="213" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Kuncheva</surname></persName>
		</author>
		<title level="m">Combining Pattern Classifiers: Methods and Algorithms</title>
				<imprint>
			<publisher>Wiley &amp; Sons</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Audio-visual affect recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pianfetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="424" to="428" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bi-modal emotion recognition from expressive face and body gestures</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Piccardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network and Computer Applications</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1334" to="1345" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Emotion detection from infant facial expressions and cries</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yantorno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
				<meeting><address><addrLine>Dallas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Handbook of Cognition and Emotion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dalgleish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Power</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Chichester: Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The dictionary of affect in language</title>
		<author>
			<persName><forename type="first">C</forename><surname>Whissell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emotion: Theory, Research, and Experience</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="113" to="131" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modeling naturalistic affective states via facial, vocal and bodily expressions recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Karpouzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Caridakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kessous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raouzaiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Malatesta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kollias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">4451</biblScope>
			<biblScope unit="page" from="92" to="116" />
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">User and context adaptive neural networks for emotion recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Caridakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karpouzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kollias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="2553" to="2562" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The HUMAINE database: Addressing the needs of the affective computing community</title>
		<author>
			<persName><forename type="first">E</forename><surname>Douglas-Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sneddon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lowry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcrorie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">nduced Emotional Data Affective Computing and Intelligent Interaction, ser</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4738</biblScope>
			<biblScope unit="page" from="488" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The vera am mittag german audio-visual emotional speech database</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kroschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="865" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Emotion recognition in humancomputer interaction</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fragopanagos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="389" to="405" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<title level="m">Sentic Computing: Techniques, Tools, and Applications</title>
				<meeting><address><addrLine>Dordrecht, Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The Way We Think: Conceptual Blending and the Mind&apos;s Hidden Complexities</title>
		<author>
			<persName><forename type="first">G</forename><surname>Fauconnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Basic Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sentic avatar: Multimodal affective conversational agent with common sense,&quot; in Toward Autonomous, Adaptive, and Context-Aware Multimodal Interfaces: Theoretical and Practical Issues, ser</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cerezo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baldassarri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>Esposito, A. Hussain, M. Faundez-Zanuy, R. Martone, and N. Melone</editor>
		<imprint>
			<biblScope unit="volume">6456</biblScope>
			<biblScope unit="page" from="82" to="96" />
			<date type="published" when="2011">2011</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Better game characters by design: A psychological approach</title>
		<author>
			<persName><forename type="first">K</forename><surname>Isbister</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The unbearable likeness of being digital: The persistence of nonverbal social norms in online virtual environments</title>
		<author>
			<persName><forename type="first">N</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bailenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Merget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CyberPsychology &amp; Behavior</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="121" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An empathic virtual dialog agent to improve human-machine interaction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pelachaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sadek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Autonomous Agents and Multiagent Systems</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SenticNet 2: A semantic and affective resource for opinion mining and sentiment analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FLAIRS</title>
				<imprint>
			<publisher>Marco Island</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="202" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<title level="m">Data Mining: Practical Machine Learning Tools and Techniques</title>
				<meeting><address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Facial expressions and emotion database</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wallhoff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Web-based database for facial expression analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rademaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
				<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Nonparametric Statistics for the Social Siences</title>
		<author>
			<persName><forename type="first">S</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Castellan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>McGraw-Hill</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Big social data analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rajagopal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Olsher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Big Data Computing</title>
				<editor>
			<persName><forename type="first">R</forename><surname>Akerkar</surname></persName>
		</editor>
		<imprint>
			<publisher>Chapman and Hall/CRC</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semantic multidimensional scaling for open-domain sentiment analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Howard</surname></persName>
		</author>
		<idno type="DOI">10.1109/MIS.2012.118</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Representing and resolving negation for sentiment analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lapponi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ovrelid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM SENTIRE</title>
				<meeting><address><addrLine>Brussels</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="687" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sentic PROMs: Application of sentic computing to the development of a novel unified framework for measuring health-care quality</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Eckl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">543</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The hourglass of emotions</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Livingstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Cognitive Behavioral Systems, ser. Lecture Notes in Computer Science</title>
		<editor>Esposito, A. Vinciarelli, R. Hoffmann, and V. Muller</editor>
		<imprint>
			<biblScope unit="volume">7403</biblScope>
			<biblScope unit="page" from="144" to="157" />
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
