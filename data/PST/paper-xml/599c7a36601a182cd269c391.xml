<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pansharpening by Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-07-14">14 July 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Giuseppe</forename><surname>Masi</surname></persName>
							<email>giuseppe.masi@unina.it</email>
						</author>
						<author>
							<persName><forename type="first">Davide</forename><surname>Cozzolino</surname></persName>
							<email>davide.cozzolino@unina.it</email>
						</author>
						<author>
							<persName><forename type="first">Luisa</forename><surname>Verdoliva</surname></persName>
							<email>luisa.verdoliva@unina.it</email>
						</author>
						<author>
							<persName><forename type="first">Giuseppe</forename><surname>Scarpa</surname></persName>
							<email>giuseppe.scarpa@unina.it</email>
						</author>
						<author>
							<persName><forename type="first">Lizhe</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Guoqing</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Prasad</forename><forename type="middle">S</forename><surname>Thenkabail</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Università di Napoli Federico II</orgName>
								<address>
									<addrLine>Via Claudio 21</addrLine>
									<postCode>80125</postCode>
									<settlement>Napoli</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">PAN Coastal Blue Green Yellow Red Red Edge Nir Nir</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Pansharpening by Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-07-14">14 July 2016</date>
						</imprint>
					</monogr>
					<idno type="MD5">0E596FF3AC960A6E59EDCED207250CC5</idno>
					<idno type="DOI">10.3390/rs8070594</idno>
					<note type="submission">Received: 20 May 2016; Accepted: 8 July 2016;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>multiresolution</term>
					<term>segmentation</term>
					<term>enhancement</term>
					<term>super-resolution</term>
					<term>machine learning</term>
					<term>convolutional neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A new pansharpening method is proposed, based on convolutional neural networks. We adapt a simple and effective three-layer architecture recently proposed for super-resolution to the pansharpening problem. Moreover, to improve performance without increasing complexity, we augment the input by including several maps of nonlinear radiometric indices typical of remote sensing. Experiments on three representative datasets show the proposed method to provide very promising results, largely competitive with the current state of the art in terms of both full-reference and no-reference metrics, and also at a visual inspection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multi-resolution images are widespread in remote sensing as they provide users with images at the highest resolution both in the spatial and in the spectral domains. Since these goals cannot be both achieved by a single sensor, modern systems acquire two images, a panchromatic (PAN) component with high spatial and low spectral resolution, and a multispectral (MS) component with complementary properties. In many cases, one of these components is enough to satisfy the user needs. Sometimes, instead, they are processed jointly <ref type="bibr" target="#b0">[1]</ref> to take full advantage of all available information. More often, the two pieces of information are fused through a pan-sharpening algorithm, generating a datacube at high resolution in both the spatial and spectral domains. Pansharpening is very important for remote sensing scene interpretation, and is also used as a pre-processing step for several image processing tasks, like feature extraction, segmentation and classification. Consequently, it has been the object of intense research, and many methods have been proposed in the last decades, as reported for example in <ref type="bibr" target="#b1">[2]</ref>.</p><p>A classical and simple approach is component substitution (CS) <ref type="bibr" target="#b2">[3]</ref>. It consists in transforming the MS image in a suitable domain where one of the components is replaced by the high-resolution PAN image. After up-sampling the other components, the whole set is back-transformed in the original domain. Clearly, the more correlated the PAN is with the replaced component, the less distortion is introduced. A simple and fast procedure is based on the intensity-hue-saturation (IHS) transform <ref type="bibr" target="#b3">[4]</ref>, which can be used only when three bands are available. However, a generalized IHS transform (GIHS) can be defined <ref type="bibr" target="#b4">[5]</ref> which includes the response of the near-infrared (NIR) band. Other methods use the principal component analysis (PCA) <ref type="bibr" target="#b5">[6]</ref>, the Brovey transform (BT) <ref type="bibr" target="#b6">[7]</ref>, and Gram-Schmidt (GS) spectra sharpening <ref type="bibr" target="#b7">[8]</ref>. Although these techniques preserve spatial information accurately, and are quite robust to co-registration errors, they are often characterized by high spectral distortion, because PAN and MS components are acquired in spectral ranges that overlap only partially. In <ref type="bibr" target="#b9">[9]</ref> two enhanced versions of GIHS and GS are proposed to deal with this problem, while in <ref type="bibr" target="#b10">[10]</ref> an adaptive approach using partial replacement (PRACS) is presented.</p><p>An alternative approach to component substitution, goes through the injection of high-frequency details extracted from the PAN image into the up-sampled version of the MS. In general, methods based on detail injection guarantee a better spectral fidelity than those based on component substitution. They differ in how spatial-details are extracted from the PAN and how they are injected in the MS. In the basic approach, details are obtained as the difference between the PAN image and a smoothed version of itself. The smooth version can be computed by means of a simple low-pass filter on a single level decomposition <ref type="bibr" target="#b11">[11]</ref> or through a multiresolution analysis (MRA) on several decomposition bands <ref type="bibr" target="#b12">[12]</ref>. These last methods rely on redundant representations, characterized by the shift-invariance property, like the á Trous Wavelet transform (ATWT) or the Laplacian pyramid (LP) <ref type="bibr" target="#b13">[13]</ref>. In order to better capture directional information, methods based on non-separable transforms, like curvelets, have also been developed <ref type="bibr" target="#b14">[14]</ref>. Unfortunately, MRA approaches may exhibit spatial distortions (e.g., ringing artifacts) which worsen the visual quality of the fused data.</p><p>However extracted, detail information must be eventually injected into the up-sampled MS bands to improve spatial resolution. To avoid introducing distortion and artifacts, a model of the involved data is necessary <ref type="bibr" target="#b15">[15]</ref>. This can be defined at a coarser scale, where both MS and PAN images are available, and then extended to a finer scale. By so doing, however, it is implicitly assumed that the same model holds at different scales, which is actually not true, especially considering very high resolution data, like highly detailed urban environments <ref type="bibr" target="#b16">[16]</ref>. The problem can be reduced by tuning the filters so as to closely match the modulation transfer functions (MTFs) of the sensors. For example, in <ref type="bibr" target="#b17">[17]</ref> a smoothing-filter-based intensity modulation (SFIM) has been proposed. It modulates spatial details by the ratio between a high resolution image and its low-pass version, since this cancels the spectral and topographical contrast of the higher resolution image and retains the higher resolution edges only. However, performance depends critically on the accuracy of PAN-MS co-registration. To obtain better coregistered upscaled images, <ref type="bibr" target="#b18">[18]</ref> uses the Induction scaling technique, followed by a new fusion technique called Indusion. Other methods following the same philosophy work in a MRA setting, e.g., ATWT <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b19">[19]</ref><ref type="bibr" target="#b20">[20]</ref><ref type="bibr" target="#b21">[21]</ref> or with LP representations <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b22">[22]</ref><ref type="bibr" target="#b23">[23]</ref><ref type="bibr" target="#b24">[24]</ref>. Some recent papers, to avoid improper modeling, recast the problem in an optimization framework. In <ref type="bibr" target="#b25">[25]</ref>, a different MMSE-optimal detail image is extracted for each MS band, by evaluating band-dependent generalized intensities. This method was extended in <ref type="bibr" target="#b26">[26]</ref>, where parameter estimation is performed through nonlocal parameter optimization based on K-means clustering. In <ref type="bibr" target="#b27">[27]</ref> pansharpening is cast as a Bayesian estimation problem, with a suitable joint prior for observed data and unknown pansharpened image. Total variation is used in <ref type="bibr" target="#b28">[28]</ref>, with the pansharpened image obtained by constrained energy minimization.</p><p>In the last few years, apart from component substitution and detail injection, much attention has been devoted to sparse representations. In <ref type="bibr" target="#b29">[29]</ref> a method based on compressive sensing with sparsity-inducing prior information has been proposed, where sparsity is enforced by building a dictionary of image patches randomly sampled from high-resolution MS images. Further developments have been proposed in <ref type="bibr" target="#b30">[30]</ref><ref type="bibr" target="#b31">[31]</ref><ref type="bibr" target="#b32">[32]</ref> with the goal to avoid the cost of dictionary construction. As of today, instead, there has been limited interest on deep learning, with some exception like in <ref type="bibr" target="#b33">[33]</ref>, where a modified sparse denoising autoencoder algorithm is proposed.</p><p>In this paper, motivated by the impressive performance of deep learning in a large number of applicative fields, not least remote sensing <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b35">35]</ref>, a new pansharpening method is proposed, based on convolutional neural networks (CNN). We build upon the architecture recently proposed in <ref type="bibr" target="#b36">[36]</ref> for the closely related super-resolution problem. First, we simply adapt the architecture to the pansharpening case. Then, we improve it, by leveraging on the huge domain-specific knowledge available in the remote sensing field. In particular, we augment the input by including a number of radiometric indices, tailored to features that proved very relevant for applications. Such nonlinear indices could be hardly extracted by a CNN, and only with a much deeper and data-hungry architecture. Performance assessment fully supports our proposal. We carry out experiments on three datasets, comprising images acquired by the Ikonos, GeoEye-1 and WorldView-2 multiresolution sensors and compare results with a score of well-established reference techniques, obtaining significant improvements on all datasets and under all performance metrics, both full-reference and no-reference.</p><p>In the rest of the paper we provide some necessary background on deep learning and CNN-based super-resolution (Section 2), describe the proposed method (Section 3), and present and discuss experimental results (Section 4). Eventually we draw conclusions and outline future research. In addition, a list of the abbreviations used in this paper is provided at the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Deep Learning and Convolutional Neural Networks</head><p>As the name suggests, artificial neural networks (ANN) take inspiration from their biological counterparts, trying to emulate some of their remarkable abilities. In fact, the human visual system can easily solve complex pattern recognition problems that elude the most powerful computers. This happens thanks to a tightly interconnected network of simple processing units, the neurons. Based on this observation, the artificial neuron is designed to perform a very simple task: given a set of inputs, (x 1 , . . . , x K ) it outputs a nonlinear function of their weighted average</p><formula xml:id="formula_0">y = f (b + K ∑ k=1 w k x k )<label>(1)</label></formula><p>By so doing, it matches simple input features, such as edges, lines or blobs, in the case of images. The outputs of a large layer of neurons operating in parallel become the input of a further layer of neurons, which combine basic features to extract features of higher semantic value, and this proceeds through several layers, allowing for a high level of abstraction. This deep layered architecture is therefore responsible for the impressive abstraction ability of such networks. By modifying neuron weights in response to suitable input stimuli, the network learns how to perform all sorts of desired tasks.</p><p>In a traditional fully-connected network, each neuron takes as input the outputs of all neurons of previous layer, and feeds its own output to all neurons of next layer. As a consequence, a deep ANN includes a very large number of weights, which must be learned on a proportionally large training set, calling for an exceeding computational complexity. Convolutional neural networks (CNN) <ref type="bibr" target="#b37">[37]</ref> overcome to a large extent this problem by renouncing full connectivity. In CNN, in fact, each neuron has a limited receptive field, processing features observed only in a local neighborhood of the neuron itself. This makes full sense for many sources, and notably for images, where spatial features are intrinsically local (especially in lower layers) and spatially invariant. Due to this latter property, in particular, one can use the very same set of weights for all neurons of the same layer, by which the output at neuron (i, j) can be expressed as the convolution with the previous layer input</p><formula xml:id="formula_1">y i,j = f (b + K ∑ n=1 K ∑ m=1 w n,m x i+n,j+m )<label>(2)</label></formula><p>(a third summation is required if x i,j is itself a vector) or, in compact matrix notation</p><formula xml:id="formula_2">y = f (b + w * x)<label>(3)</label></formula><p>where * denotes convolution. CNNs reduce drastically the number of connections among neurons, hence the number of free parameters to learn, enabling the use of deep learning in practical applications. As matter of fact, CNNs have become very popular in recent years, thanks to efficient implementations <ref type="bibr" target="#b38">[38]</ref>, with software available online, relatively fast training achievable with cheap and powerful GPUs, and also thanks to the huge mass of labeled visual data available on the web <ref type="bibr" target="#b39">[39]</ref>, essential for training complex networks.</p><p>When dealing with images, input variables are two-dimensional spatially related entities. Spatial dependencies are propagated throughout the network, which justifies why the features output by intermediate (hidden) layers are represented as images as well. The output y of a layer is also referred to as a feature map and, typically, multiple feature maps are extracted at once in a layer, using different convolutional kernels w. Eventually, depending on the specific task required of the net, the output of the network may be itself an image (think of denoising, segmentation, super-resolution), or else a set of spatially unrelated decision variables (detection, classification, recognition).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">CNN-Based Super-Resolution</head><p>To the best of our knowledge, pan-sharpening has never been addressed before by deep learning methods. However, pan-sharpening itself can be regarded as a special form of super-resolution, a very active research area in computer vision, and methods developed in one field can be often applied to another. In fact, we will take inspiration from a recently published paper <ref type="bibr" target="#b36">[36]</ref> on the super-resolution of natural images via CNN, which is briefly recalled in the following.</p><p>In <ref type="bibr" target="#b36">[36]</ref> the authors design a CNN which mimics the behavior of a sparse-coding super-resolution method, and in fact demonstrate that the former generalizes the latter, performing the same processing steps and, possibly, more general tasks. This strong analogy, explicitly pursued at design time, is a very remarkable feature, as it allows one to associate a precise meaning with each layer of the CNN architecture, something not easily obtained in other cases. Recall that sparse-coding super-resolution is a machine learning method that involves three main steps: (a) projection of each image patch on a low-resolution dictionary; (b) mapping between the patches of low-resolution and corresponding high-resolution dictionaries; (c) reconstruction through the combination of patches of the high-resolution dictionary. Accordingly, the CNN proposed in <ref type="bibr" target="#b36">[36]</ref> is composed of three layers that correspond roughly to the above mentioned steps. Before entering the network, the image is up-sampled to the target resolution via bicubic interpolation. Hence, the first layer computes 64 feature maps using a 9 × 9 receptive field and a ReLU (rectified linear unit, max(0, x) <ref type="bibr" target="#b40">[40]</ref>) nonlinearity. The second step computes 32 feature maps using a 1 × 1 receptive field and, again, ReLU. Finally, the third layer, with a 5 × 5 receptive field, and a simple identity activation function, provides the desired high-resolution image. In summary, the three layers compute the following items</p><formula xml:id="formula_3">f 1 (x) = max(0, w 1 * x + b 1 ), w 1 : 64 × (9 × 9 × 3), b 1 : 64 × 1 f 2 (x) = max(0, w 2 * f 1 (x) + b 2 ), w 2 : 32 × (1 × 1 × 64), b 2 : 32 × 1 f 3 (x) = w 3 * f 2 (x) + b 3 , w 3 : 3 × (5 × 5 × 32), b 3 : 3 × 1</formula><p>To gain insight into the network's rationale, let us track a single 9 × 9 input patch x p centered at point p, obtained by the original image through upsampling. This is projected (y p = w 1 * x p ) on 64 different 9 × 9 × 3 patches, a sort of low-resolution dictionary. Next, as w 2 is defined on a 1 × 1 receptive field, a non-linear mapping of the 64-vector y p to a 32-vector z p = f 2 (y p ) follows, reminding of a translation to the high-resolution basis. Finally, through the 5 × 5 convolution in the third layer, z p will contribute to the final reconstruction at p and all neighbors in a 5 × 5 square, reminding of the weighted average of high-resolution patches.</p><p>Starting from this reference architecture, the authors of <ref type="bibr" target="#b36">[36]</ref> test several variations, adding further hidden layers, for example, or varying the receptive fields. Although some such schemes provide a slightly better performance, this comes at the cost of increased complexity, which speaks in favor of the basic architecture. The best trade-off is achieved using a 3 × 3 receptive field in the second layer.</p><p>The various architectures were always designed having in mind the sparse coding paradigm. While this is a precious guideline, it is certainly possible that better results may be obtained, for the same complexity, by removing this constraint. However, understanding the internal behavior of a given network is still an unsolved, and hot, topic in computer vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed CNN-Based Pansharpening</head><p>Given the excellent performance of the super-resolution method proposed in <ref type="bibr" target="#b36">[36]</ref> we decided to apply the same approach to the pansharpening problem. However, we also want to take advantage of the large domain-specific knowledge existing on remote sensing image processing, introducing some reasonable changes aimed at better exploiting the data. Accordingly, in the following, we first provide some details about the targeted datasets, then we describe the basic architecture, directly inherited from the super-resolution method, and finally, based on a first analysis of results, we propose our remote-sensing specific solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>The proposed method, although rather general, it has been conceived for very high resolution data, and in particular it has been tested on data acquired by some of the most advanced systems for remote sensing of optical data. These are Ikonos, GeoEye-1, and WorldView-2, whose main characteristics are summarized in Tables <ref type="table" target="#tab_0">1</ref> and<ref type="table">2</ref>. This sensors selection allowed us to study the robustness of the proposed method with respect to both spectral resolution (Ikonos/GeoEye-1 vs. WorldView-2) and spatial resolution (Ikonos vs. GeoEye-1/WorldView-2). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PAN MS</head><p>Ikonos 0.82 m GSD at nadir 3.28 m GSD at nadir GeoEye-1 0.46 m GSD at nadir 1.84 m GSD at nadir WorldView-2 0.46 m GSD at nadir 1.84 m GSD at nadir Table <ref type="table">2</ref>. Spectral bands of Ikonos, GeoEye-1, and WorldView-2 sensors. When the band is comprised the wavelength range (in nm) is reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Basic Architecture</head><p>Figure <ref type="figure" target="#fig_0">1</ref> provides a pictorial representation of the basic architecture, which follows closely the super-resolution CNN (SRCNN) architecture. The example is tailored on Ikonos and GeoEye-1 data, with 4-band multispectral components and a panchromatic band with 4 × 4 higher spatial resolution, but applies to other multi-resolution data with obvious modifications. The four low-resolution spectral bands are first upsampled (4 × 4) and interpolated. The result is then stacked with the high-resolution panchromatic band to form the 5-component input. Hence, the network will work at the target resolution from the beginning, with no need of up/down-sampling. The output comprises only 4 bands, which correspond to the input multispectral bands, but at the target panchromatic resolution. We keep using the three-layer structure of <ref type="bibr" target="#b36">[36]</ref>, but replace the 1 × 1 kernels of the central layer with 5 × 5 kernels as, again, these provide some performance improvements in preliminary experiments. Table <ref type="table" target="#tab_1">3</ref> summarizes the parameters of the overall architecture. It is worth underlining that this is quite a simple CNN architecture, relatively shallow, and hence easy to train with a limited training set. This is extremely important for the remote sensing field, where training data are often scarce as opposed to the millions images typically available for computer vision applications. We will therefore stick to this three-layer architecture also in the following, adapting it to remote sensing pansharpening based on prior knowledge on the problem, and working only on the input data.  </p><formula xml:id="formula_4">c 1 = B + 1 K 1 × K 1 f 1 (x) c 2 K 2 × K 2 f 2 (x) c 3 K 3 × K 3 f 3 (x) c 4 = B 5 9 × 9 ReLU 64 5 × 5 ReLU 32 5 × 5 x 4</formula><p>Turning to implementation details, the output of the CNN is a B-band multispectral image with the same spatial resolution as the PAN component. This image should be as close as possible to the ideal image acquired by a multispectral sensor operating at the same spatial resolution of the PAN. Of course, this latter image does not exist. This raises problems not only for performance assessment but, in deep learning, also for network training. We will address both problems using the Wald protocol <ref type="bibr" target="#b41">[41]</ref>, which consists in working on a downsampled multi-resolution image, for which the original MS component represents a valid reference.</p><p>Training based on the Wald training protocol is summarized in Figure <ref type="figure" target="#fig_1">2</ref>. In the upper channel, the original image is downsampled, then its MS component is interpolated, and the resulting (B + 1) image is tiled and fed to the CNN. In the lower channel, the original MS component is itself tiled and used as a reference. During training, the CNN parameters are adapted to produce output tiles that match as closely as possible the reference tiles. The learned parameters are eventually used for the pansharpening of the real multiresolution images, at their original resolution. Clearly, the whole procedure rests upon the hypothesis that performance does not depend critically on scale, which is not always the case, as discussed in the Introduction. Therefore, to reduce the possible mismatches, we smooth data before downsampling, following <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">22]</ref>, using a filter that matches the modulation transfer function of the sensor. Likewise, we upsample the MS component using the interpolation kernel proposed in <ref type="bibr" target="#b13">[13]</ref>. In any case, great attention must be devoted to assess performance not only on the low-resolution data but also on the original images.</p><p>Learning has been carried out as proposed in <ref type="bibr" target="#b36">[36]</ref> and we used exactly the same setting, briefly summarized here. It is based on backpropagation with stochastic gradient descent. The updating iteration refers to a batch of 128 input tiles selected at random from the training set. As loss function L, we use the mean square error between the pansharpened tile X and its reference X, averaged on the batch (To account for the border effects of the filtering, suitably cropped versions of X and X are involved in the computation).</p><formula xml:id="formula_5">L(W) = 1 128 128 ∑ n=1 ||X n -Xn (W)|| 2 (4)</formula><p>where W is the set of all parameters, namely, filter weights and biases. Stochastic gradient descent uses a momentum parameter to reduce randomness, therefore, iteration (i + 1) reads as</p><formula xml:id="formula_6">W i+1 = W i + ∆W i = W i + µ • ∆W i-1 -α • ∇L i (5)</formula><p>where µ is the momentum, and α the learning rate. Following the indications given in <ref type="bibr" target="#b36">[36]</ref> we set µ = 0.9 and α = 10 -4 , except for the last layer, where we set α = 10 -5 , since a smaller learning rate is known to speed up convergence. In all cases the total number of iterations has been fixed to 1.12 × 10 6 . Additional details about learning can be found in <ref type="bibr" target="#b36">[36]</ref>. Figure <ref type="figure" target="#fig_2">3</ref> shows a sample result of the proposed pansharpening method. On the left, there is the input multiresolution image, acquired by the GeoEye-1 sensor, with PAN and MS components at their actual scales (all MS images, here and in the following, are projected on a suitable RGB space). On the right, the result of the pansharpening process, and in the center, for reference the interpolated multispectral component. The result looks quite satisfactory: the spatial resolution of the PAN component is fully preserved, and colors, when the comparison makes sense, are very similar to those of the interpolated MS component, suggesting the absence of spectral distortions. In Section 4, this positive judgement will be supported by strong numerical evidence. </p><formula xml:id="formula_7">↓ 4×4 ↑ 4×4 MS only Tiling • • • Tiling • • •</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Remote-Sensing Specific Architecture</head><p>Although the basic architecture provides already very good results, we set to analyze in more depth its behavior over remote sensing imagery with the goal to propose some sensible improvements based on the available prior knowledge. In particular, we focus on the first-layer filters, whose behavior may be interpreted quite easily, while this is much more difficult for layers further away from the input.</p><p>With reference to WorldView-2 images, hence with nine input bands, Figure <ref type="figure" target="#fig_4">4a</ref> plots the energy of the feature maps at the output of the first layer, sorted in decreasing order. A sharp energy drop is visible after feature 48, suggesting that some of the filters are not really capturing valuable information, and could be removed altogether. This interpretation is reinforced by the analysis of the filter responses, some of which are shown in Figure <ref type="figure" target="#fig_5">5</ref>. In each column, we show the 9 × 9 spatial kernels associated with the 8 spectral components of the input plus the panchromatic component, recalled with an abbreviation on the left. For the sake of clarity, we only show some high-energy filters, on the left, and then some filters around the transition highlighted before. Filters beyond #48 exhibit noise-like kernels, confirming that they are not matching relevant features, and are not effectively trained by the network <ref type="bibr" target="#b42">[42]</ref>. On the contrary, the other filters exhibit well-defined patterns. Spatial structures emerge mainly in the last component, associated with the panchromatic band, while kernels associated with spectral bands are spatially smooth. This is clearly due to the different resolutions of the original bands. Instead, the components associated with spectral bands show strong energy variations, suggesting spectral selectivity. In practice, they are extracting spectral signatures associated with the image land covers. For example, filter #7 matches quite well the vegetation. In fact, a well known indicator for vegetation is the normalized difference vegetation index (NDVI), see Equation <ref type="bibr" target="#b6">(7)</ref>, which basically computes a normalized difference between responses in near-infrared and red bands. Accordingly, filter #7 has a strongly positive kernel in the 7th patch, associated with the near-infrared component, and negative in the 5th patch, associated with the red component. Similar considerations apply to filter #48 which correlates to the water signature. In fact it differentiates between the first three spectral bands (coastal, red, and green) and those from 6th to 8th (near infrared). Notably, this matches pretty well the definition of the normalized difference water index (NDWI), see Equation <ref type="bibr" target="#b5">(6)</ref>. (This filter has very small energy because of the scarcity of water samples in the dataset used for training. This is confirmed by the noisy shape of the filter that indicates the need of further training to increase its regularity. However, the network is quite robust anyway, as other filters match the water class, although less obviously than filter #48.)     Beyond the two above mentioned feature maps, many more, both in the first and the second layer, exhibit a remarkable correlation with some class-specific radiometric indexes. This interesting behavior, with the network trying to mimic well-established features, has motivated us to add some such indexes in order to "guide" (and hopefully boost) the learning process. Indeed, this intuition was supported by experimental results. A possible explanation lies in the relative invariance of the normalized indexes with respect to the illumination. Invariance, in fact, improves the network capability to "generalize" and hence to perform well on data which are scarcely (or not at all) represented in the training set. Therefore, based on numerical results and on the above considerations, we augmented the input by adding further planes, corresponding to some well-known indices. In particular, we considered the following nonlinear radiometric indices computed from the multispectral components (see also Table <ref type="table">2</ref>):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Normalized Difference Water Index:</p><formula xml:id="formula_8">NDWI Ikonos/GeoEye-1 = Green -Nir Green + Nir ; NDWI WorldView-2 = Coastal -Nir2 Coastal + Nir2<label>(6)</label></formula><p>• Normalized Difference Vegetation Index:</p><formula xml:id="formula_9">NDVI Ikonos/GeoEye-1 = Nir -Red Nir + Red ; NDVI WorldView-2 = Nir2 -Red Nir2 + Red<label>(7)</label></formula><p>• Normalized Difference Soil Index (applies to WorldView-2 only):</p><formula xml:id="formula_10">NDSI WorldView-2 = Green -Yellow Green + Yellow<label>(8)</label></formula><p>• Non-Homogeneous Feature Difference (applies to WorldView-2 only):</p><formula xml:id="formula_11">NHFD WorldView-2 = Red Edge -Coastal Red Edge + Coastal<label>(9)</label></formula><p>Radiometric indices can be defined in many different ways <ref type="bibr" target="#b43">[43]</ref>, and indeed we use different definitions of NDVI and NDWI for the four-band (Ikonos/GeoEye-1) and 8-band (WorldView-2) images, since number and meaning of the bands change. Moreover, two of the indexes are defined only for the richer WorldView-2 images. More indices proposed in the remote sensing community could be tested, but selecting the most informative of them goes outside the scope of this work.</p><p>Figure <ref type="figure" target="#fig_4">4b</ref> shows the distribution of filter energy after augmenting the input with the nonlinear radiometric indices. The number of inactive filters drops from 16 to 9, and several filters correlate well with the new indices. Moreover, filter energy grows in general, also for filters that were already active. These facts seem to support our conjecture that the remote sensing specific features do serve as a guidance for the network, allowing it to better address the pansharpening task. A stronger support will come from experimental evidence.</p><p>Besides augmenting the input, we tested further minor variations to the basic architecture, modifying the number of filters and their spatial support. Although a wide range of alternative architectures have been tested, we will focus on a meaningful subset of them obtained by changing the hyperparameters B rad (hence c 1 ), K 1 , and c 2 as summarized in Table <ref type="table" target="#tab_2">4</ref>. Changes in the second and the third layers have also been tested but eventually discarded, as ineffective. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Discussion</head><p>Several experiments have been carried out to find the optimal setting for the proposed technique and assess its performance in comparison with several state-of-the-art references. To this end, three datasets have been designed, with images acquired by the Ikonos, GeoEye-1 and WorldView-2 multiresolution sensors, all characterized by a very high spatial resolution, below one meter for the panchromatic band (sensor characteristics are summarized in Tables <ref type="table" target="#tab_0">1</ref> and<ref type="table">2</ref>). In all cases, the MS component has spatial resolution 4 × 4 lower than the PAN. Datasets are divided (see Table <ref type="table" target="#tab_3">5</ref>) in disjoint training, validation and test subsets. The first two subsets are used for CNN training, while the last one is used for performance assessment of all techniques. For performance assessment, we use a number of metrics proposed in the literature, since no single one can be considered by itself as a reliable indicator of quality. In particular, besides several full-reference metrics, which measure performance on the reduced resolution dataset according to the Wald protocol, we consider also the no-reference QNR <ref type="bibr" target="#b44">[44]</ref> and two derived metrics, that work on the full-resolution dataset. In fact, while these latter indicators may be considered less objective than the former, they are insensitive to possible scale-related phenomena. In Table <ref type="table" target="#tab_4">6</ref>, all these metrics are listed together with the corresponding reference for the interested reader. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparing Different Networks</head><p>In this subsection we report the results of a series of experiments aimed at finding optimal settings for the proposed method, both with and without the external features derived from nonlinear radiometric indices.</p><p>Table <ref type="table" target="#tab_5">7</ref> reports the performance achieved on WorldView-2 images as a function of the parameters c 1 , K 1 , and c 2 . In this case, c 1 , the number of input bands, may be only 9 (without external features) or 13 (with them). For the first-layer filters, with support K 1 × K 1 , we tested small, medium and relatively large receptive fields, with K 1 = 5, 9 and 15, respectively. Finally, we have considered c 2 = 48, 56 or 64 features at the output of the first layer.  In both cases, with and without external features, using large filters, K 1 = 15, causes some performance loss. This can be easily explained by looking at Figure <ref type="figure" target="#fig_9">7a</ref>, showing the evolution of the loss function (computed on the validation set) during training. Apparently 10 6 iterations are not enough to reach convergence with K 1 = 15. On the other hand, even at convergence (after four millions iterations), results are just on par with the other cases. This is likely due to overfitting problems, typically occurring when the training dataset is not large enough, as well explained in <ref type="bibr" target="#b42">[42]</ref>. This same limitation observed in width applies in depth as well, as already pointed out in <ref type="bibr" target="#b36">[36]</ref> for the super-resolution problem. It would be certainly interesting to test more complex networks, e.g., with larger supports and/or deeper architectures, as we may observe more complex filter's patterns than those shown in Figure <ref type="figure" target="#fig_5">5</ref>, but lacking a proportionally larger dataset (not available in this work) for training it would not make sense. Therefore, in particular, we decided not to investigate the K 1 = 15 option further. Concerning the comparison between K 1 = 5 and K 1 = 9, results indicate clearly that the former choice improves spectral accuracy (see D λ ), and the latter spatial accuracy (see D S ), as could be expected. As for the number of output features, they can be reduced from 64 to 48 with no significant accuracy loss, especially in case no external features are used. Finally, let us focus on the comparison between c 1 = 9, and c 1 = 13, that is, on the impact of external radiometric indices. To this end, we have highlighted in color the best result for each indicator both for c 1 = 9 (red, in the upper part of the table) and for c 1 = 13 (blue, in the lower part). The injection of external features improves consistently the best performance. This behavior is also confirmed by the analysis of Figure <ref type="figure" target="#fig_9">7</ref>, showing loss function evolution for architectures with different filter support (left) and number of output features (right) with and without radiometric indices. For each basic configuration (solid line), the corresponding architecture with augmented input (dashed line) provides a significant reduction of the loss. </p><formula xml:id="formula_12">Q4 Q SAM ERGAS SCC D λ D S QNR c 1 K 1 c 2 → 1 → 1 → 0 → 0 → 1 → 0 → 0 → 1<label>9</label></formula><formula xml:id="formula_13">c 1 = 9, K 1 = 9 c 1 = 9, K 1 =15 c 1 =13, K 1 = 5 c 1 =13, K 1 = 9 c 1 =13, K 1 =15 0 2 4 6 8<label>10 12 x 10 5 5 6 7 8 x 10 -4</label></formula><p>iterations In Tables <ref type="table" target="#tab_7">8</ref> and<ref type="table" target="#tab_9">9</ref> we report results obtained on the other two datasets. Apart from numerical differences, it seems safe to say that the same phenomena observed for the WorldView-2 dataset emerge also in these cases.    </p><formula xml:id="formula_14">Loss/#pixel c 1 = 9, c 2 =64 c 1 = 9, c 2 =56 c 1 = 9, c 2 =48 c 1 =13, c 2 =64 c 1 =13, c 2 =56 c 1 =13, c 2 =48<label>(a) (b)</label></formula><formula xml:id="formula_15">Q4 Q SAM ERGAS SCC D λ D S QNR c 1 K 1 c 2 → 1 → 1 → 0 → 0 → 1 → 0 → 0 → 1<label>5</label></formula><formula xml:id="formula_16">Q4 Q SAM ERGAS SCC D λ D S QNR c 1 K 1 c 2 → 1 → 1 → 0 → 0 → 1 → 0 → 0 → 1<label>5</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with the State of the Art</head><p>Let us now compare the proposed method (using the acronym PNN in the following, for CNN-based Pansharpening) with a number of state-of-the-art reference techniques. We tested all the techniques analyzed in the recent review paper <ref type="bibr" target="#b1">[2]</ref>, whose implementation has been made available online <ref type="bibr" target="#b50">[50]</ref>. However, in the following tables, we report results only for the most competitive ones, listed below, according to the quality indices adopted. We included the recently proposed C-BDSD technique <ref type="bibr" target="#b26">[26]</ref>, since this is an improvement of BDSD, already among the best references. This has been implemented and configured according to the author's indications. As for the proposed method, we selected for each dataset the architecture which provided the best QNR figure, always with the nonlinear radiometric indices included. With this choice we decided to give more emphasis to full-resolution results, more indicative of real-world operation, although obtained with a no-reference metric. Software and implementation details will be made available online <ref type="bibr">[51]</ref> to ensure full reproducibility. Again, we provide results separately for the three datasets in Tables <ref type="table" target="#tab_11">10</ref><ref type="table" target="#tab_12">11</ref><ref type="table" target="#tab_13">12</ref>. Performance figures are obtained by averaging over all test images of the dataset. Numerical results speak very clearly in favor of the proposed solution. Let us focus, for the time being, on the WorldView-2 dataset, as in Table <ref type="table" target="#tab_11">10</ref>. Under all full-reference metrics, CNN-based pansharpening guarantees a large performance gain with respect to all references, with MTF-GLP-HPM taking the role of the closest challenger. The situation changes somewhat with no-reference full-resolution measures, where the gap is more narrow and in a single case, for metric D S , BDSD exhibits a better performance. Notice also that, with these metrics, MTF-GLP-HPM becomes the worst reference, while BDSD and C-BDSD are very competitive. This sheds further light on the importance of using multiple metrics and of using careful visual inspection to complement numerical indications. Notice also that, with the exception of BDSD and C-BDSD for no-reference metrics, the performance gap between the proposed solution and all references is much wider than the gap among the various tested CNN architectures, testifying on the robustness of this approach versus the selected hyperparameters.</p><p>The analysis of results obtained on the other datasets, reported in Tables <ref type="table" target="#tab_12">11</ref> and<ref type="table" target="#tab_13">12</ref>, highlights some differences in the behavior of reference techniques, but the proposed solution keeps being largely preferable, especially for GeoEye-1 data, where the gap with all competitors, including BDSD and C-BDSD, becomes very large, even for no-reference metrics.</p><p>To add a further piece of information, Table <ref type="table" target="#tab_14">13</ref> reports, for each dataset, and for some of the performance metrics considered before, the ranking of the compared techniques averaged over all test images. Considering full-reference metrics, the proposed method proves almost always best over the images of all datasets. Only with the no-reference metric QNR, and for two datasets out of three, BDSD or C-BDSC happen to be preferable for some images, although the proposed solution keeps being the first choice on average. It also worth noting that the proposed solution is robust with respect to data resolution. Considering the original datasets, and their 4 × 4 subsampling, our experiments spanned a resolution range of three octaves, with the proposed method outperforming consistently all references. A further performance gain could be achieved by relaxing the constraint on the number of iterations (at the cost of increased design time), as obvious by Figure <ref type="figure" target="#fig_9">7</ref> where the loss functions is not yet at convergence. Finally, as for all machine learning methods, the availability of more training data would certainly benefit performance and robustness. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Visual Inspection</head><p>We conclude our analysis with a careful visual inspection of results. Numerical indicators, in fact, represent only proxies for quality, especially useful during development phases. Only visual inspection allows one to discover a number of artifacts and distortions that elude quantitative analyses. Therefore, for each dataset, we selected a sample image including both urban and natural areas, to allow for a meaningful comparison of pansharpening quality among competing techniques. Together with the reference image, we show (Images should be analyzed on a computer screen with suitable zoom) the output of the proposed method and of a subset of references, chosen for their variety and good objective performance.</p><p>First, in Figures 8-10, we show the low-resolution versions, used to compute full-reference metrics by the Wald protocol. For these images, a valid reference exists, namely, the original multispectral component, shown on the left. The pansharpened images produced by the proposed method look very similar to the original MS components, with comparable spatial resolution and without noticeable artifacts or spectral distortions. C-BDSD returns images which look even sharper than the original, with many more high-frequency details, for example in the vegetation areas. Therefore it seems to provide an enhanced version of the target image. It may be debated, though, whether this is desirable for a pansharpening method. For sure, all images, and markedly the GeoEye-1, present a significant spectral distortion, especially visible in the rooftops. Indusion is characterized by a subtle diffused blurring and some strong artifacts, see the highway in the GeoEye-1 image. The behavior of PRACS is not uniform over the images, it works quite well on the WolrdView-2 and GeoEye-1 images, but introduces a very strong blurring in the Ikonos image. ATWT-M3, on the contrary, provides images of consistently very low quality, with intense blurring, arguably worse than the low-resolution original. This, despite the relatively good performance in terms of no-reference metrics. On the other hand, as will be soon obvious, the performance of ATWT-M3 on the actual data is not nearly as bad, raising further alarm on metric reliability, especially in the presence of scale issues.</p><p>In Figures <ref type="figure" target="#fig_0">11</ref><ref type="figure" target="#fig_1">12</ref><ref type="figure" target="#fig_2">13</ref>, we show the actual full-resolution images obtained through pansharpening. This time, no ground truth exists. However, we show on the left as reference the up-sampled MS component with bicubic interpolation. By comparison, one can appreciate the new details introduced by pansharpening and, at the same time, spot possible spectral distortion in homogeneous regions. As a general remark, most pansharpened images exhibit a very good quality, with an impressive improvement with respect to interpolated MS, especially visible in the enlarged box. In particular, the proposed method appears as one of the more effective, with a consistent performance over all sensors. At a closer inspection, however, a number of problems emerge again. For the proposed method, a subtle pattern is visible is some homogeneous areas, probably related to the high-degree interpolation polynomial. C-BDSD, as before, is mainly affected by spectral distortion, with a clear over-saturation, specially for the green and red hues. Indusion exhibits significant spatial distortion (mostly, ringing artifacts) in the urban areas, while PRACS (in the Ikonos image) and ATWT-M3 (always) introduce spatial blurring, although less than in the low-resolution case. All these observations are also supported by the detail images (difference between pansharpened image and interpolated reference) shown in the bottom row of each figure. The colored areas in the C-BDSD detail images testify of the mentioned spectral distortion, while the low energy of the the PRACS and especially ATWT-M3 detail images underline the inability of these techniques to inject all necessary high-resolution details in the pansharpened image. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Implementation Details</head><p>To conclude this section we provide some implementation details for the proposed method. All implemented PNNs have approximatively the same complexity, and actually they take about the same amount of computational burden in both learning and testing phases. In particular, the learning phase takes about half a day and is carried on GPU (Nvidia Tesla K20c with CUDA 7.5 and CuDNN v3) through the deep learning framework Caffe <ref type="bibr" target="#b51">[52]</ref>. The testing of any PNN, instead, performed via CPU (PC with 2GHz Intel Xeon processor and 64 Gb) and implemented in MATLAB by means of MatConvNet <ref type="bibr" target="#b52">[53]</ref>, on a 320 × 320 (in the MS domain) image takes about 20 seconds, which is comparable to the time required by reference algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>Deep learning has proven extremely effective in a large number of image processing and computer vision problems. Based on this observation, we decided to use convolutional neural networks to address the pansharpening task, modifying an architecture recently proposed for super-resolution. To improve performance without resorting to data-intensive deeper architectures, which would call for huge training sets and complexity, we augmented the input by including several maps of nonlinear radiometric indices. This version proved uniformly superior with respect to the basic architecture. We tested the proposed method against a number of state-of-the-art references on three multiresolution datasets obtaining a very good performance under all metrics, both full-reference and no-reference, and also in terms of subjective quality.</p><p>We are already working to improve the proposed method, following the same approach taken in this paper, that is, to use the large body of knowledge available in the remote sensing field to exploit the full potential of deep learning. In particular, leveraging on our own work, we will test the use of further external inputs, such as textural features <ref type="bibr" target="#b53">[54]</ref> or information derived from external segmenter <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b54">55]</ref>. Another line of research concerns training the network with a loss function aligned with typical no-reference metrics used for pansharpening.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Basic CNN architecture for pansharpening.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. CNN training through the Wald protocol.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Sample result of proposed pansharpening method. Form left to right: input multiresolution image acquired by the GeoEye-1 sensor, interpolation of MS, pansharpened image.</figDesc><graphic coords="7,103.39,447.12,121.41,121.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Energy of first-layer filters without (a) and with (b) nonlinear radiometric indices.</figDesc><graphic coords="8,117.64,596.37,237.31,118.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. A subset of first-layer filter responses, column-wise re-scaled to compensate for different gains. Filters #7 and #48, highlighted, correspond roughly to vegetation and water features.</figDesc><graphic coords="8,368.09,596.37,118.66,118.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6</head><label>6</label><figDesc>Figure6provides further insight into this interpretation. It shows, on the left, a color image obtained by combining three suitable bands of a WorldView-2 image, and then the feature maps associated with filter #7 (center) and filter #48 (right). In the first map, all vegetated areas are clearly highlighted, although some "errors" of interpretation can also be spotted in the top-left part of the image. Likewise, the second map highlights very clearly water basins, again, with some scattered "errors".</figDesc><graphic coords="9,243.78,178.03,107.72,107.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Some first-layer feature maps obtained for a sample WorldView-2 image. From left to right, RGB composition, feature map #7, feature map #48. The selected feature maps highlight quite accurately vegetation and water, respectively.</figDesc><graphic coords="9,124.72,178.03,107.72,107.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Evolution of the loss function (Equation (4)) computed on the WorldView-2 validation set during training for various architectures: (a) fixed number of filters, c 2 = 64; (b) fixed filter support, K 1 = 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>•</head><label></label><figDesc>PRACS: Partial Replacement Adaptive Component Substitution [10]; • Indusion: Decimated Wavelet Transform using an additive injection model [18]; • AWLP: Additive Wavelet Luminance Proportional [20], a generalization of AWL [19]; • ATWT-M3: A Trous Wavelet Transform with the injection Model 3 proposed in [15]; • MTF-GLP-HPM: Generalized Laplacian Pyramid with MTF-matched filter and multiplicative injection model [16]; • BDSD: Band-Dependent Spatial-Detail with local parameter estimation [25]; • C-BDSD: A non-local extension of BDSD, proposed in [26].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 .Figure 9 .Figure 10 .Figure 11 .Figure 12 .Figure 13 .</head><label>8910111213</label><figDesc>Figure 8. Pansharpening of a reduced resolution WorldView-2 image. From left to right: reference image (original MS component) output of proposed method, C-BDSD, Indusion, PRACS, ATWT-M3.</figDesc><graphic coords="16,93.61,207.23,64.94,129.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Spatial resolutions for Ikonos, GeoEye-1, and WorldView-2 sensors.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Default CNN architecture in case of a multispectral component with B = 4 bands and without external features. c 1 is the total number of input bands.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Hyper-parameters of the CNN architectures for Pansharpening. B rad : number of nonlinear radiometric indices, K 1 × K 1 : spatial support of first-layer filters, c 2 number of first-layer filters.</figDesc><table><row><cell>Sensor</cell><cell>B</cell><cell>B rad</cell><cell>c 1 = B + B rad + 1</cell><cell>K 1</cell><cell>c 2</cell></row><row><cell cols="3">Ikonos, GeoEye-1 4 {0, 2} WorldView-2 8 {0, 4}</cell><cell>{5, 7} {9, 13}</cell><cell cols="2">{5, 9, 15} {48, 56, 64} {5, 9, 15} {48, 56, 64}</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Datasets partition. (33 × 33) 7168 × (33 × 33) 30 × (320 × 320)</figDesc><table><row><cell>Training</cell><cell>Validation</cell><cell>Test</cell></row><row><cell cols="3">Ikonos GeoEye-1 WorldWiew-2 14400 × 14400 × (33 × 33) 7168 × (33 × 33) 50 × (320 × 320) 14400 × (33 × 33) 7168 × (33 × 33) 70 × (320 × 320)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Full-reference (low resolution) and no-reference (full resolution) performance metrics.</figDesc><table><row><cell></cell><cell>SAM</cell><cell>Spectral Angle Mapper [45]</cell></row><row><cell></cell><cell cols="2">ERGAS Erreur Relative Globale Adimensionnelle de Synthèse [46]</cell></row><row><cell cols="2">full reference SCC</cell><cell>Spatial Correlation Coefficient [47]</cell></row><row><cell></cell><cell>Q</cell><cell>Universal Image Quality index [48] averaged over the bands</cell></row><row><cell></cell><cell>Qx</cell><cell>x-band extension of Q [49]</cell></row><row><cell></cell><cell>QNR</cell><cell>Quality with no-Reference index [44]</cell></row><row><cell>no reference</cell><cell>D λ</cell><cell>Spectral component of QNR</cell></row><row><cell></cell><cell>D S</cell><cell>Spatial component of QNR</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Performance of various configurations of the proposed architecture on WorldView-2 images without (top) and with (bottom) nonlinear radiometric indices. Best result in red and blue, respectively.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Performance of various configurations of the proposed architecture on Ikonos images without (top) and with (bottom) nonlinear radiometric indices. Best result in red and blue, respectively.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 .</head><label>9</label><figDesc>Performance of various configurations of the proposed architecture on GeoEye-1 images without (top) and with (bottom) nonlinear radiometric indices. Best result in red and blue, respectively.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 .</head><label>10</label><figDesc>Performance comparison on the WorldView-2 dataset.</figDesc><table><row><cell></cell><cell>Q4</cell><cell>Q</cell><cell>SAM</cell><cell>ERGAS</cell><cell>SCC</cell><cell>D λ</cell><cell>D S</cell><cell>QNR</cell></row><row><cell></cell><cell>→ 1</cell><cell>→ 1</cell><cell>→ 0</cell><cell>→ 0</cell><cell>→ 1</cell><cell>→ 0</cell><cell>→ 0</cell><cell>→ 1</cell></row><row><cell>PRACS</cell><cell cols="3">0.7908 0.8789 3.6995</cell><cell>2.4102</cell><cell cols="4">0.8522 0.0234 0.0734 0.9050</cell></row><row><cell>Indusion</cell><cell cols="3">0.6928 0.8373 3.7261</cell><cell>3.2022</cell><cell cols="4">0.8401 0.0552 0.0649 0.8839</cell></row><row><cell>AWLP</cell><cell cols="3">0.8127 0.9043 3.4182</cell><cell>2.2560</cell><cell cols="4">0.8974 0.0665 0.0849 0.8549</cell></row><row><cell>ATWT-M3</cell><cell cols="3">0.7039 0.8186 4.0655</cell><cell>3.1609</cell><cell cols="4">0.8398 0.0675 0.0748 0.8628</cell></row><row><cell cols="4">MTF-GLP-HPM 0.8242 0.9083 3.4497</cell><cell>2.0918</cell><cell cols="4">0.9019 0.0755 0.0953 0.8373</cell></row><row><cell>BDSD</cell><cell cols="3">0.8110 0.9052 3.7449</cell><cell>2.2644</cell><cell cols="4">0.8919 0.0483 0.0382 0.9156</cell></row><row><cell>C-BDSD</cell><cell cols="3">0.8004 0.8948 3.9891</cell><cell>2.6363</cell><cell cols="4">0.8940 0.0251 0.0458 0.9304</cell></row><row><cell>PNN</cell><cell cols="3">0.8511 0.9442 2.5767</cell><cell>1.6029</cell><cell cols="4">0.9392 0.0199 0.0485 0.9326</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 .</head><label>11</label><figDesc>Performance comparison on the Ikonos dataset.</figDesc><table><row><cell></cell><cell>Q4</cell><cell>Q</cell><cell>SAM</cell><cell>ERGAS</cell><cell>SCC</cell><cell>D λ</cell><cell>D S</cell><cell>QNR</cell></row><row><cell></cell><cell>→ 1</cell><cell>→ 1</cell><cell>→ 0</cell><cell>→ 0</cell><cell>→ 1</cell><cell>→ 0</cell><cell>→ 0</cell><cell>→ 1</cell></row><row><cell>PRACS</cell><cell cols="3">0.6597 0.8021 2.9938</cell><cell>2.3597</cell><cell cols="4">0.8735 0.0493 0.1148 0.8424</cell></row><row><cell>Indusion</cell><cell cols="3">0.5928 0.7660 3.2800</cell><cell>2.7961</cell><cell cols="4">0.8506 0.1264 0.1619 0.7340</cell></row><row><cell>AWLP</cell><cell cols="3">0.7143 0.8389 2.8426</cell><cell>2.1126</cell><cell cols="4">0.9069 0.1384 0.1955 0.6951</cell></row><row><cell>ATWT-M3</cell><cell cols="3">0.5579 0.7249 3.5807</cell><cell>3.0327</cell><cell cols="4">0.8183 0.1244 0.1452 0.7490</cell></row><row><cell cols="4">MTF-GLP-HPM 0.7178 0.8422 2.8820</cell><cell>2.0550</cell><cell cols="4">0.9072 0.1524 0.2186 0.6646</cell></row><row><cell>BDSD</cell><cell cols="3">0.7199 0.8576 2.9147</cell><cell>1.9852</cell><cell cols="4">0.9084 0.0395 0.0884 0.8761</cell></row><row><cell>C-BDSD</cell><cell cols="3">0.7204 0.8569 2.9101</cell><cell>2.0553</cell><cell cols="4">0.9164 0.0710 0.1218 0.8173</cell></row><row><cell>PNN</cell><cell cols="3">0.7609 0.9006 2.2831</cell><cell>1.6634</cell><cell cols="4">0.9411 0.0514 0.0731 0.8796</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 .</head><label>12</label><figDesc>Performance comparison on the GeoEye-1 dataset.</figDesc><table><row><cell></cell><cell>Q4</cell><cell>Q</cell><cell>SAM</cell><cell>ERGAS</cell><cell>SCC</cell><cell>D λ</cell><cell>D S</cell><cell>QNR</cell></row><row><cell></cell><cell>→ 1</cell><cell>→ 1</cell><cell>→ 0</cell><cell>→ 0</cell><cell>→ 1</cell><cell>→ 0</cell><cell>→ 0</cell><cell>→ 1</cell></row><row><cell>PRACS</cell><cell cols="3">0.6995 0.8568 3.2364</cell><cell>2.4296</cell><cell cols="4">0.8113 0.0470 0.0877 0.8698</cell></row><row><cell>Indusion</cell><cell cols="3">0.5743 0.7771 3.5361</cell><cell>3.5480</cell><cell cols="4">0.7600 0.1270 0.1262 0.7651</cell></row><row><cell>AWLP</cell><cell cols="3">0.7175 0.8615 3.6297</cell><cell>2.6134</cell><cell cols="4">0.7878 0.1247 0.1521 0.7436</cell></row><row><cell>ATWT-M3</cell><cell cols="3">0.6008 0.7907 3.5546</cell><cell>3.0729</cell><cell cols="4">0.7944 0.0712 0.0710 0.8633</cell></row><row><cell cols="4">MTF-GLP-HPM 0.7359 0.8718 3.2205</cell><cell>5.0344</cell><cell cols="4">0.7887 0.1526 0.1815 0.6956</cell></row><row><cell>BDSD</cell><cell cols="3">0.7399 0.8832 3.3384</cell><cell>2.2342</cell><cell cols="4">0.8526 0.0490 0.0994 0.8572</cell></row><row><cell>C-BDSD</cell><cell cols="3">0.7391 0.8784 3.4817</cell><cell>2.4370</cell><cell cols="4">0.8591 0.0832 0.1342 0.7953</cell></row><row><cell>PNN</cell><cell cols="3">0.8094 0.9402 2.1311</cell><cell>1.5661</cell><cell cols="4">0.9152 0.0327 0.0611 0.9084</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 13 .</head><label>13</label><figDesc>Average ranking over test images for a few metrics.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">WorldView-2</cell><cell></cell><cell>Ikonos</cell><cell></cell><cell></cell><cell>GeoEye-1</cell><cell></cell></row><row><cell></cell><cell cols="9">Q4 SAM QNR Q4 SAM QNR Q4 SAM QNR</cell></row><row><cell>PRACS</cell><cell>5.0</cell><cell>4.3</cell><cell>3.7</cell><cell>5.8</cell><cell>4.2</cell><cell>3.0</cell><cell>5.6</cell><cell>3.5</cell><cell>3.1</cell></row><row><cell>Indusion</cell><cell>7.6</cell><cell>4.2</cell><cell>4.5</cell><cell>7.0</cell><cell>4.9</cell><cell>5.6</cell><cell>7.6</cell><cell>4.9</cell><cell>5.9</cell></row><row><cell>AWLP3.7</cell><cell>3.8</cell><cell>2.6</cell><cell>6.5</cell><cell>3.7</cell><cell>2.2</cell><cell>6.7</cell><cell>4.8</cell><cell>3.0</cell><cell>6.7</cell></row><row><cell>ATWT-M3</cell><cell>7.4</cell><cell>6.0</cell><cell>6.4</cell><cell>7.8</cell><cell>7.0</cell><cell>5.3</cell><cell>7.3</cell><cell>5.9</cell><cell>2.9</cell></row><row><cell cols="2">MTF-GLP-HPM 2.5</cell><cell>2.8</cell><cell>7.7</cell><cell>3.2</cell><cell>2.8</cell><cell>8.0</cell><cell>3.1</cell><cell>3.1</cell><cell>8.0</cell></row><row><cell>BDSD</cell><cell>3.7</cell><cell>8.0</cell><cell>3.3</cell><cell>3.7</cell><cell>6.5</cell><cell>2.1</cell><cell>3.1</cell><cell>7.8</cell><cell>3.0</cell></row><row><cell>C-BDSD</cell><cell>5.0</cell><cell>7.0</cell><cell>2.0</cell><cell>3.4</cell><cell>7.2</cell><cell>3.4</cell><cell>3.5</cell><cell>6.8</cell><cell>5.0</cell></row><row><cell>PNN</cell><cell>1.0</cell><cell>1.2</cell><cell>1.9</cell><cell>1.2</cell><cell>1.0</cell><cell>1.9</cell><cell>1.0</cell><cell>1.0</cell><cell>1.4</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Remote Sens. 2016, 8, 594; doi:10.3390/rs8070594 www.mdpi.com/journal/remotesensing</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This research was partially funded by MIUR under grant 100026-2014-PRIN 001 and partially funded through the POR Campania FESR O.O. 2.2, 2007/2013, ref. PON03PE_00112_1.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Author Contributions: Giuseppe Masi and Davide Cozzolino conceived, designed, and performed the experiments; Luisa Verdoliva analyzed the related state of the art, and contributed with Giuseppe Scarpa to write the paper; Giuseppe Scarpa coordinated the activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of Interest:</head><p>The authors declare no conflict of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abbreviations</head><p>The following abbreviations are used in this manuscript: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Software and implementation details will be made available online at [51] to ensure full reproducibility.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Marker controlled watershed based segmentation of multi-resolution remote sensing images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gaetano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verdoliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Giuseppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="1987" to="3004" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A critical comparison among pansharpening algorithms</title>
		<author>
			<persName><forename type="first">G</forename><surname>Vivone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Alparone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Mura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garzelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Licciardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Restaino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="2565" to="2586" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A generalized component substitution technique for spatial enhancement of multispectral images using a higher resolution data set</title>
		<author>
			<persName><forename type="first">V</forename><surname>Shettigara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogramm. Eng. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="561" to="567" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A new look at IHS-like image fusion methods</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Shyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="177" to="186" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A fast intensity hue-saturation fusion technique with spectral adjustment for IKONOS imagery</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="309" to="312" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Extracting spectral contrast in Landsat thematic mapper image data using selective principal component analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chavez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kwarteng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogramm. Eng. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="339" to="348" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Color enhancement of highly correlated images. II. Channel ratio and &quot;chromaticity</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Gillespie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Kahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Walker</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="343" to="365" />
		</imprint>
	</monogr>
	<note>transformation techniques. Remote Sens. Environ. 1987</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Process for Enhancing the Spatial Resolution of Multispectral Imagery Using Pan-Sharpening</title>
		<author>
			<persName><forename type="first">C</forename><surname>Laben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Brower</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">U.S. Patent</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2000-01-04">4 January 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving component substitution pansharpening through multivariate regression of MS + Pan data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Aiazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baronti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Selva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="3230" to="3239" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A new adaptive component-substitution-based satellite image fusion by using partial replacement</title>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="295" to="309" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Comparison of three different methods to merge multiresolution and multispectral data: Landsat TM and SPOT panchromatic</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chavez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogramm. Eng. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="295" to="303" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multiresolution wavelet decomposition image merger of Landsat Thematic Mapper and SPOT panchromatic data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yocky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogramm. Eng. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="1067" to="1074" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Context-driven fusion of high spatial and spectral resolution images based on oversampled multiresolution analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Aiazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Alparone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baronti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garzelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2300" to="2312" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Remote sensing image fusion using the curvelet transform</title>
		<author>
			<persName><forename type="first">L</forename><surname>Alparone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baronti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garzelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nencini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="143" to="156" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fusion of high spatial and spectral resolution images: The ARSIS concept and its implementation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ranchin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogramm. Eng. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="49" to="61" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An MTF-based spectral distortion minimizing model for pan-sharpening of very high resolution multispectral images of urban areas</title>
		<author>
			<persName><forename type="first">B</forename><surname>Aiazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Alparone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baronti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garzelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Selva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd GRSS/ISPRS Joint Workshop on Remote Sensing and Data Fusion over Urban Areas</title>
		<meeting>the 2nd GRSS/ISPRS Joint Workshop on Remote Sensing and Data Fusion over Urban Areas<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-05">May 2003</date>
			<biblScope unit="page" from="22" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Smoothing filter based intensity modulation: A spectral preserve image fusion technique for improving spatial details</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="3461" to="3472" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Indusion: Fusion of multispectral and panchromatic images using the induction scaling technique</title>
		<author>
			<persName><forename type="first">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Condat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Montanvert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="98" to="102" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multiresolution-based image fusion with additive wavelet decomposition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nunez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Otazu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Prades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Arbiol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1204" to="1211" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Introduction of sensor spectral response into image fusion methods. Application to wavelet-based methods</title>
		<author>
			<persName><forename type="first">X</forename><surname>Otazu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gonzalez-Audicana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nunez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="2376" to="2385" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Contrast and error-based fusion schemes for multispectral image pansharpening</title>
		<author>
			<persName><forename type="first">G</forename><surname>Vivone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Restaino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Mura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Licciardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="930" to="934" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MTF-tailored multiscale fusion of high-resolution MS and Pan imagery</title>
		<author>
			<persName><forename type="first">B</forename><surname>Aiazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Alparone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baronti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garzelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Selva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogramm. Eng. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="591" to="596" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Comparison of pansharpening algorithms: Outcome of the 2006 GRS-S data-fusion contest</title>
		<author>
			<persName><forename type="first">L</forename><surname>Alparone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gamba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="3012" to="3021" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast and efficient panchromatic sharpening</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="155" to="163" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Optimal MMSE pan sharpening of very high resolution multispectral images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Garzelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nencini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Capobianco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="228" to="236" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pansharpening of multispectral images based on nonlocal parameter optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Garzelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="2096" to="2107" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bayesian data fusion for adaptable image pansharpening</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fasbender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Radoux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bogaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="1847" to="1857" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A new pansharpening algorithm based on total variation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Palsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sveinsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ulfarsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="318" to="322" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A new pan-sharpening method using a compressed sensing technique</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="738" to="746" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Remote sensing image fusion via sparse representations over learned dictionaries</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="4779" to="4789" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A sparse image fusion algorithm with application to pan-sharpening</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bamler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="2827" to="2836" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sparse representation based pansharpening using trained dictionary</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="293" to="297" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A new pan-sharpening method with deep neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1037" to="1041" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Application of deep learning algorithms to MSTAR data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE International Geoscience and Remote Sensing Symposium</title>
		<meeting>the 2015 IEEE International Geoscience and Remote Sensing Symposium<address><addrLine>Milan, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07">July 2015</date>
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Land Use Classification in Remote Sensing Images by Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Castelluccio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sansone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verdoliva</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1508.00092" />
		<imprint>
			<date type="published" when="2016-07-13">13 July 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol. Cybern</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc.: Red Hook</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on CVPR 2009, Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on CVPR 2009, Computer Vision and Pattern Recognition<address><addrLine>Miami, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06">June 2009</date>
			<biblScope unit="page" from="20" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Joint deep learning for pedestrian detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings fo the 2013 IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>fo the 2013 IEEE International Conference on Computer Vision (ICCV)<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12">December 2013</date>
			<biblScope unit="page" from="3" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fusion of satellite images of different spatial resolution: Assessing the quality of resulting images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ranchin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mangolini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogramm. Eng. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="691" to="699" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1312.6034" />
		<title level="m">Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps</title>
		<imprint>
			<date type="published" when="2016-07-13">13 July 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">High spatial resolution worldview-2 imagery for mapping NDVI and Its relationship to temporal urban landscape evapotranspiration factors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Beecham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nagler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="580" to="602" />
		</imprint>
	</monogr>
	<note>Remote Sens</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multispectral and panchromatic data fusion assessment without reference</title>
		<author>
			<persName><forename type="first">L</forename><surname>Alparone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aiazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baronti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garzelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nencini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Selva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogramm. Eng. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="193" to="200" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Discrimination among semi-arid landscape endmembers using the Spectral AngleMapper (SAM) algorithm</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Yuhas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F H</forename><surname>Goetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Boardman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Summaries of the Third Annual JPL Airborne Geoscience Workshop; AVIRIS Workshop</title>
		<meeting><address><addrLine>Pasadena, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="147" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Data Fusion: Definitions and Architectures-Fusion of Images of Different Spatial Resolutions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<pubPlace>Paris, France</pubPlace>
		</imprint>
	</monogr>
	<note>Presses des Mines</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A wavelet transform method to merge Landsat TM and SPOT panchromatic data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Civco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Silander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="743" to="757" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A universal image quality index</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="81" to="84" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A global quality measurement of pan-sharpened multispectral imagery</title>
		<author>
			<persName><forename type="first">L</forename><surname>Alparone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baronti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garzelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nencini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="313" to="317" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<ptr target="http://openremotesensing.net/" />
		<title level="m">Open Remote Sensing</title>
		<imprint>
			<date type="published" when="2016-07-13">13 July 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName><surname>Caffe</surname></persName>
		</author>
		<ptr target="http://caffe.berkeleyvision.org" />
		<imprint>
			<date type="published" when="2016-07-13">13 July 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">CNNs for MATLAB</title>
		<author>
			<persName><surname>Matconvnet</surname></persName>
		</author>
		<ptr target="http://www.vlfeat.org/matconvnet" />
		<imprint>
			<date type="published" when="2016-07-13">13 July 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Recursive texture fragmentation and reconstruction segmentation algorithm applied to VHR images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gaetano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Scarpa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Poggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 IEEE International Geoscience and Remote Sensing Symposium</title>
		<meeting>the 2009 IEEE International Geoscience and Remote Sensing Symposium<address><addrLine>Cape Town, South Africa</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-07">July 2009</date>
			<biblScope unit="page" from="12" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A marker-controlled watershed segmentation: Edge, mark and fill</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gaetano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Scarpa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Poggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 IEEE International Geoscience and Remote Sensing Symposium</title>
		<meeting>the 2012 IEEE International Geoscience and Remote Sensing Symposium<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07">July 2012</date>
			<biblScope unit="page" from="22" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
